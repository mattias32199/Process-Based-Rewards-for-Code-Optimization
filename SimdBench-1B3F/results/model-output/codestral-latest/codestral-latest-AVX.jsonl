{"task_id": "SimdBench_0_AVX", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n        __m256i mask_vec = _mm256_setr_epi64x(\n            mask[i] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 3] ? -1 : 0\n        );\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n        __m256i mask_vec = _mm256_setr_epi64x(\n            mask[i] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 3] ? -1 : 0\n        );\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n        __m256i mask_vec = _mm256_set_epi64x(\n            mask[i + 3] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 0] ? -1 : 0\n        );\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n        __m256i mask_vec = _mm256_set_epi64x(\n            mask[i + 3] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 0] ? -1 : 0\n        );\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n        __m256i mask_vec = _mm256_set_epi64x(\n            -mask[i + 3], -mask[i + 2], -mask[i + 1], -mask[i]\n        );\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vshift = _mm256_set1_epi32(3);\n    __m256i vdiv = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i vtmp1 = _mm256_sra_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vtmp2 = _mm256_sll_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vresult = _mm256_div_epi32(_mm256_add_epi32(_mm256_mullo_epi32(vsrc, vscale), _mm256_mullo_epi32(vtmp1, vtmp2)), vdiv);\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vshift = _mm256_set1_epi32(3);\n    __m256i vdiv = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i vtmp1 = _mm256_sra_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vtmp2 = _mm256_sll_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vresult = _mm256_div_epi32(_mm256_add_epi32(_mm256_mullo_epi32(vsrc, vscale), _mm256_mullo_epi32(vtmp1, vtmp2)), vdiv);\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i v7 = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i vtmp1 = _mm256_srai_epi32(_mm256_mullo_epi32(vsrc, vscale), 3);\n        __m256i vtmp2 = _mm256_slli_epi32(_mm256_mullo_epi32(vsrc, vscale), 3);\n        __m256i vresult = _mm256_div_epi32(_mm256_add_epi32(_mm256_mullo_epi32(vsrc, vscale), _mm256_mullo_epi32(vtmp1, vtmp2)), v7);\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vshift = _mm256_set1_epi32(3);\n    __m256i vdiv = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i vtmp1 = _mm256_sra_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vtmp2 = _mm256_sll_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vtmp3 = _mm256_add_epi32(_mm256_mullo_epi32(vsrc, vscale), _mm256_mullo_epi32(vtmp1, vtmp2));\n        __m256i vresult = _mm256_div_epi32(vtmp3, vdiv);\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vshift = _mm256_set1_epi32(3);\n    __m256i vdiv = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i vtmp1 = _mm256_sra_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vtmp2 = _mm256_sll_epi32(_mm256_mullo_epi32(vsrc, vscale), vshift);\n        __m256i vresult = _mm256_div_epi32(_mm256_add_epi32(_mm256_mullo_epi32(vsrc, vscale), _mm256_mullo_epi32(vtmp1, vtmp2)), vdiv);\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256d src_vec = _mm256_loadu_pd(&src[r * cols + stride]);\n        _mm256_storeu_pd(&dst[r], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; r += 4) {\n        __m256d src_vec = _mm256_loadu_pd(&src[r * cols + stride]);\n        _mm256_storeu_pd(&dst[r], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256d src_vec = _mm256_loadu_pd(&src[r * cols + stride]);\n        _mm256_storeu_pd(&dst[r], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256d src_vec = _mm256_loadu_pd(&src[r * cols + stride]);\n        _mm256_storeu_pd(&dst[r], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256d src_vec = _mm256_load_pd(&src[r * cols + stride]);\n        _mm256_store_pd(&dst[r], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 src_vals = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], src_vals);\n\n        __m256i dst_idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        _mm256_i32scatter_ps(dst, dst_idx, src_vals, 4);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 src_vals = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], src_vals);\n\n        __m256i dst_idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        _mm256_i32scatter_ps(dst, dst_idx, src_vals, 4);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 src_vals = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], src_vals);\n\n        __m256i dst_idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        _mm256_i32scatter_ps(dst, dst_idx, src_vals, 4);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 src_vals = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], src_vals);\n\n        __m256 dst_vals = _mm256_i32gather_ps(dst, idx, 4);\n        _mm256_i32scatter_ps(dst, idx, dst_vals, 4);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 src_vals = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], src_vals);\n\n        __m256i dst_idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        _mm256_i32scatter_ps(dst, dst_idx, src_vals, 4);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (i < j) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&arr[j]);\n\n        __m256i temp = a;\n        a = b;\n        b = temp;\n\n        _mm256_storeu_si256((__m256i*)&arr[i], a);\n        _mm256_storeu_si256((__m256i*)&arr[j], b);\n\n        i += 16;\n        j -= 16;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (i < j) {\n        __m256i a = _mm256_loadu_si256((__m256i *)&arr[i]);\n        __m256i b = _mm256_loadu_si256((__m256i *)&arr[j]);\n\n        __m256i temp = a;\n        a = b;\n        b = temp;\n\n        _mm256_storeu_si256((__m256i *)&arr[i], a);\n        _mm256_storeu_si256((__m256i *)&arr[j], b);\n\n        i += 16;\n        j -= 16;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n    __m256i temp;\n\n    while (i < j) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&arr[j]);\n\n        temp = a;\n        a = b;\n        b = temp;\n\n        _mm256_storeu_si256((__m256i*)&arr[i], a);\n        _mm256_storeu_si256((__m256i*)&arr[j], b);\n\n        i += 16;\n        j -= 16;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (i < j) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&arr[j - 15]);\n\n        __m256i temp = a;\n        a = b;\n        b = temp;\n\n        _mm256_storeu_si256((__m256i*)&arr[i], a);\n        _mm256_storeu_si256((__m256i*)&arr[j - 15], b);\n\n        i += 16;\n        j -= 16;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (i < j) {\n        __m256i a = _mm256_loadu_si256((__m256i *)&arr[i]);\n        __m256i b = _mm256_loadu_si256((__m256i *)&arr[j - 15]);\n\n        __m256i temp = a;\n        a = _mm256_shuffle_epi8(b, _mm256_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        b = _mm256_shuffle_epi8(temp, _mm256_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        _mm256_storeu_si256((__m256i *)&arr[i], a);\n        _mm256_storeu_si256((__m256i *)&arr[j - 15], b);\n\n        i += 16;\n        j -= 16;\n    }\n\n    if (i == j) {\n        __m256i a = _mm256_loadu_si256((__m256i *)&arr[i]);\n        a = _mm256_shuffle_epi8(a, _mm256_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        _mm256_storeu_si256((__m256i *)&arr[i], a);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *src = tensor + slice_idx * slice_size;\n    uint8_t *dst = slice;\n\n    for (size_t i = 0; i < slice_size; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(src + i));\n        _mm256_storeu_si256((__m256i *)(dst + i), vec);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *src = tensor + slice_idx * slice_size;\n    uint8_t *dst = slice;\n\n    for (size_t i = 0; i < slice_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i));\n        _mm256_storeu_si256((__m256i *)(dst + i), data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *src = tensor + slice_idx * slice_size;\n    uint8_t *dst = slice;\n\n    for (size_t i = 0; i < slice_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i));\n        _mm256_storeu_si256((__m256i *)(dst + i), data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * slice_size;\n    const uint8_t *src = tensor + offset;\n\n    for (size_t i = 0; i < slice_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i));\n        _mm256_storeu_si256((__m256i *)(slice + i), data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *src = tensor + slice_idx * slice_size;\n    uint8_t *dst = slice;\n\n    for (size_t i = 0; i < slice_size; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(src + i));\n        _mm256_storeu_si256((__m256i *)(dst + i), vec);\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t bi = 0; bi < block_size; bi += 8) {\n                for (size_t bj = 0; bj < block_size; bj += 8) {\n                    __m256i a0 = _mm256_loadu_si256((__m256i*)&src[(i + bi) * cols + j + bj]);\n                    __m256i a1 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 1) * cols + j + bj]);\n                    __m256i a2 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 2) * cols + j + bj]);\n                    __m256i a3 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 3) * cols + j + bj]);\n                    __m256i a4 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 4) * cols + j + bj]);\n                    __m256i a5 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 5) * cols + j + bj]);\n                    __m256i a6 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 6) * cols + j + bj]);\n                    __m256i a7 = _mm256_loadu_si256((__m256i*)&src[(i + bi + 7) * cols + j + bj]);\n\n                    __m256i t0 = _mm256_unpacklo_epi16(a0, a1);\n                    __m256i t1 = _mm256_unpackhi_epi16(a0, a1);\n                    __m256i t2 = _mm256_unpacklo_epi16(a2, a3);\n                    __m256i t3 = _mm256_unpackhi_epi16(a2, a3);\n                    __m256i t4 = _mm256_unpacklo_epi16(a4, a5);\n                    __m256i t5 = _mm256_unpackhi_epi16(a4, a5);\n                    __m256i t6 = _mm256_unpacklo_epi16(a6, a7);\n                    __m256i t7 = _mm256_unpackhi_epi16(a6, a7);\n\n                    __m256i u0 = _mm256_unpacklo_epi32(t0, t2);\n                    __m256i u1 = _mm256_unpackhi_epi32(t0, t2);\n                    __m256i u2 = _mm256_unpacklo_epi32(t1, t3);\n                    __m256i u3 = _mm256_unpackhi_epi32(t1, t3);\n                    __m256i u4 = _mm256_unpacklo_epi32(t4, t6);\n                    __m256i u5 = _mm256_unpackhi_epi32(t4, t6);\n                    __m256i u6 = _mm256_unpacklo_epi32(t5, t7);\n                    __m256i u7 = _mm256_unpackhi_epi32(t5, t7);\n\n                    __m256i v0 = _mm256_permute2x128_si256(u0, u4, 0x20);\n                    __m256i v1 = _mm256_permute2x128_si256(u1, u5, 0x20);\n                    __m256i v2 = _mm256_permute2x128_si256(u2, u6, 0x20);\n                    __m256i v3 = _mm256_permute2x128_si256(u3, u7, 0x20);\n                    __m256i v4 = _mm256_permute2x128_si256(u0, u4, 0x31);\n                    __m256i v5 = _mm256_permute2x128_si256(u1, u5, 0x31);\n                    __m256i v6 = _mm256_permute2x128_si256(u2, u6, 0x31);\n                    __m256i v7 = _mm256_permute2x128_si256(u3, u7, 0x31);\n\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj) * rows + i + bi], v0);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj) * rows + i + bi + 1], v1);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj) * rows + i + bi + 2], v2);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj) * rows + i + bi + 3], v3);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj + 4) * rows + i + bi], v4);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj + 4) * rows + i + bi + 1], v5);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj + 4) * rows + i + bi + 2], v6);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj + 4) * rows + i + bi + 3], v7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t bi = 0; bi < block_size; ++bi) {\n                for (size_t bj = 0; bj < block_size; bj += 16) {\n                    __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[(i + bi) * cols + j + bj]);\n                    __m256i perm = _mm256_permute4x64_epi64(src_vec, _MM_SHUFFLE(3, 1, 2, 0));\n                    _mm256_storeu_si256((__m256i *)&dst[(j + bj) * rows + i + bi], perm);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 16) {\n                    __m256i row = _mm256_loadu_si256((__m256i *)&src[ii * cols + jj]);\n                    for (size_t k = 0; k < 16 && jj + k < cols; ++k) {\n                        dst[(jj + k) * rows + ii] = _mm256_extract_epi16(row, k);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t bi = 0; bi < block_size; bi++) {\n                for (size_t bj = 0; bj < block_size; bj += 16) {\n                    __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[(i + bi) * cols + j + bj]);\n                    __m256i perm = _mm256_permute4x64_epi64(src_vec, _MM_SHUFFLE(3, 1, 2, 0));\n                    _mm256_storeu_si256((__m256i*)&dst[(j + bj) * rows + i + bi], perm);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; ++jj) {\n                    __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[ii * cols + jj]);\n                    __m256i dst_vec = _mm256_permute4x64_epi64(src_vec, _MM_SHUFFLE(3, 1, 2, 0));\n                    _mm256_storeu_si256((__m256i *)&dst[jj * rows + ii], dst_vec);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t stride = dim * dim;\n    size_t i;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (i = 0; i < dim; i += 8) {\n        __m256 diag_vec = _mm256_loadu_ps(&array[i * stride + i * dim + i]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n\n    for (i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t stride = dim * dim;\n    size_t num_elements = dim;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < num_elements; i += 8) {\n        __m256 diag_vec = _mm256_loadu_ps(&array[i * (stride + dim + 1)]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t stride = dim * dim;\n    size_t i;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (i = 0; i < dim; i += 8) {\n        __m256 diag_vec = _mm256_loadu_ps(&array[i * stride + i * dim + i]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n\n    for (i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < dim; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        size_t j = 0;\n\n        for (; j <= dim - 8; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&array[index + j]);\n            sum_vec = _mm256_add_ps(sum_vec, vec);\n        }\n\n        for (; j < dim; ++j) {\n            sum += array[index + j];\n        }\n    }\n\n    sum += _mm256_reduce_add_ps(sum_vec);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t stride = dim * dim;\n    size_t num_elements = dim;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < num_elements; i += 8) {\n        __m256 diag_vec = _mm256_loadu_ps(&array[i * (stride + dim + 1)]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t num_elements = rows * cols;\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < num_elements; i += 4) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_dst = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n        _mm256_storeu_pd(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t num_elements = rows * cols;\n    size_t i;\n\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (i = 0; i <= num_elements - 4; i += 4) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n        _mm256_storeu_pd(&dst[i], v_result);\n    }\n\n    for (; i < num_elements; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t num_elements = rows * cols;\n    size_t i;\n\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (i = 0; i <= num_elements - 4; i += 4) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_dst = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n        _mm256_storeu_pd(&dst[i], v_dst);\n    }\n\n    for (; i < num_elements; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_dst = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(&dst[i * cols + j], v_dst);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_dst = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(&dst[i * cols + j], v_dst);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i] * cols;\n        size_t dst_row_index = i * cols;\n        for (j = 0; j < cols; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row_index + j]);\n            _mm256_storeu_pd(&dst[dst_row_index + j], src_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        for (j = 0; j < cols; j += 4) {\n            __m256d row_data = _mm256_loadu_pd(&src[src_row + j]);\n            _mm256_storeu_pd(&dst[dst_row + j], row_data);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    __m256d row_vec;\n\n    for (i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i] * cols;\n        size_t dst_row_index = i * cols;\n\n        for (j = 0; j < cols; j += 4) {\n            row_vec = _mm256_loadu_pd(&src[src_row_index + j]);\n            _mm256_storeu_pd(&dst[dst_row_index + j], row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        for (j = 0; j < cols; j += 4) {\n            __m256d row_data = _mm256_loadu_pd(&src[src_row + j]);\n            _mm256_storeu_pd(&dst[dst_row + j], row_data);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        for (j = 0; j < cols; j += 4) {\n            __m256d row_data = _mm256_loadu_pd(&src[src_row + j]);\n            _mm256_storeu_pd(&dst[dst_row + j], row_data);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i div2_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_div_epi64(sum, div2_vec);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i div2_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_sra_epi64(sum, div2_vec);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i div2_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_div_epi64(sum, div2_vec);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i div2_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_div_epi64(sum, div2_vec);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i div2_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_div_epi64(sum, div2_vec);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i even_elements = _mm256_shuffle_epi8(vec, mask);\n        min_val = _mm256_min_epi32(min_val, even_elements);\n    }\n\n    int min_arr[8];\n    _mm256_storeu_si256((__m256i*)min_arr, min_val);\n    int min_value = min_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_value) {\n            min_value = min_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i even_elements = _mm256_permutevar8x32_epi32(vec, mask);\n        min_val = _mm256_min_epi32(min_val, even_elements);\n    }\n\n    int min_arr[8];\n    _mm256_storeu_si256((__m256i*)min_arr, min_val);\n    int min_even = min_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_even) {\n            min_even = min_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_even) {\n            min_even = src[i];\n        }\n    }\n\n    return min_even;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i even_elements = _mm256_shuffle_epi8(vec, mask);\n        min_val = _mm256_min_epi32(min_val, even_elements);\n    }\n\n    int min_array[8];\n    _mm256_storeu_si256((__m256i *)min_array, min_val);\n    int min_value = min_array[0];\n    for (int j = 1; j < 8; ++j) {\n        if (min_array[j] < min_value) {\n            min_value = min_array[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i even_elements = _mm256_shuffle_epi8(vec, mask);\n        min_val = _mm256_min_epi32(min_val, even_elements);\n    }\n\n    int min_array[8];\n    _mm256_storeu_si256((__m256i *)min_array, min_val);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (min_array[j] < min_value) {\n            min_value = min_array[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i even_elements = _mm256_shuffle_epi8(vec, mask);\n        min_val = _mm256_min_epi32(min_val, even_elements);\n    }\n\n    int min_array[8];\n    _mm256_storeu_si256((__m256i *)min_array, min_val);\n    int min_value = min_array[0];\n    for (int j = 1; j < 8; ++j) {\n        if (min_array[j] < min_value) {\n            min_value = min_array[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i arith_shift = _mm256_sra_epi32(src_vec, shift_vec);\n        __m256i logical_shift = _mm256_srl_epi32(_mm256_castsi256_si256(src_vec), shift_vec);\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (uint32_t)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i arith_shift = _mm256_srav_epi32(src_vec, shift_vec);\n        __m256i logical_shift = _mm256_srlv_epi32(_mm256_castsi256_si256(src_vec), shift_vec);\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        __m256i arith_shift = _mm256_srav_epi32(src_vec, shift_vec);\n        __m256i logical_shift = _mm256_srlv_epi32(_mm256_castsi256_si256(_mm256_cvtepi32_epi64(src_vec)), shift_vec);\n\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((uint32_t)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i arith_shift_vec = _mm256_sra_epi32(src_vec, shift_vec);\n        __m256i logical_shift_vec = _mm256_srl_epi32(_mm256_castsi256_si256(src_vec), shift_vec);\n        __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i arith_shift_vec = _mm256_sra_epi32(src_vec, shift_vec);\n        __m256i logical_shift_vec = _mm256_srl_epi32(_mm256_castsi256_si256(src_vec), shift_vec);\n        __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    int64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    int64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    size_t remaining = size % 8;\n    for (size_t i = size - remaining; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    int64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = size / 8 * 8; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    int64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = size / 8 * 8; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    int64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = size / 8 * 8; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_idx = _mm256_setzero_si256();\n    __m256i idx_inc = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cmp = _mm256_cmpgt_epi8(current, max_val);\n        max_val = _mm256_blendv_epi8(max_val, current, cmp);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_add_epi8(idx_inc, _mm256_set1_epi8(i)), cmp);\n    }\n\n    int8_t max_vals[32];\n    int8_t max_idxs[32];\n    _mm256_storeu_si256((__m256i*)max_vals, max_val);\n    _mm256_storeu_si256((__m256i*)max_idxs, max_idx);\n\n    int8_t max_value = INT8_MIN;\n    size_t max_index = 0;\n    for (int j = 0; j < 32; ++j) {\n        if (max_vals[j] > max_value) {\n            max_value = max_vals[j];\n            max_index = max_idxs[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t max_index = 0;\n    __m256i max_vals = _mm256_set1_epi8(src[0]);\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vals = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cmp = _mm256_cmpgt_epi8(vals, max_vals);\n        __m256i new_max_vals = _mm256_blendv_epi8(max_vals, vals, cmp);\n        __m256i new_indices = _mm256_blendv_epi8(indices, _mm256_add_epi8(indices, _mm256_set1_epi8(32)), cmp);\n\n        max_vals = new_max_vals;\n        indices = new_indices;\n\n        if (i + 32 < length) {\n            indices = _mm256_add_epi8(indices, _mm256_set1_epi8(32));\n        }\n    }\n\n    int8_t max_val = _mm256_extract_epi8(max_vals, 0);\n    int8_t max_idx = _mm256_extract_epi8(indices, 0);\n\n    for (int i = 1; i < 32; ++i) {\n        int8_t val = _mm256_extract_epi8(max_vals, i);\n        if (val > max_val) {\n            max_val = val;\n            max_idx = _mm256_extract_epi8(indices, i);\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    __m256i max_idx = _mm256_set1_epi32(0);\n    size_t max_scalar = 0;\n    size_t max_scalar_idx = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i current_idx = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i cmp = _mm256_cmpgt_epi8(current_vals, max_val);\n        max_val = _mm256_blendv_epi8(max_val, current_vals, cmp);\n        max_idx = _mm256_blendv_epi8(max_idx, current_idx, cmp);\n    }\n\n    int8_t* max_val_ptr = (int8_t*)&max_val;\n    int32_t* max_idx_ptr = (int32_t*)&max_idx;\n\n    for (int j = 0; j < 32; ++j) {\n        if (max_val_ptr[j] > max_scalar) {\n            max_scalar = max_val_ptr[j];\n            max_scalar_idx = max_idx_ptr[j];\n        }\n    }\n\n    return max_scalar_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t max_index = 0;\n    __m256i max_vals = _mm256_set1_epi8(src[0]);\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length; i += 32) {\n        size_t chunk_size = (length - i) < 32 ? (length - i) : 32;\n        __m256i vals = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(vals, max_vals);\n        __m256i new_indices = _mm256_add_epi8(indices, _mm256_set1_epi8(i));\n        __m256i new_max_vals = _mm256_blendv_epi8(max_vals, vals, cmp);\n        __m256i new_indices = _mm256_blendv_epi8(indices, new_indices, cmp);\n\n        max_vals = new_max_vals;\n        indices = new_indices;\n    }\n\n    int8_t max_val = _mm256_extract_epi8(max_vals, 0);\n    int8_t max_idx = _mm256_extract_epi8(indices, 0);\n\n    for (int i = 1; i < 32; ++i) {\n        int8_t val = _mm256_extract_epi8(max_vals, i);\n        int8_t idx = _mm256_extract_epi8(indices, i);\n        if (val > max_val) {\n            max_val = val;\n            max_idx = idx;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t max_index = 0;\n    __m256i max_vals = _mm256_set1_epi8(src[0]);\n    __m256i max_indices = _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length; i += 32) {\n        size_t remaining = length - i;\n        size_t chunk_size = remaining < 32 ? remaining : 32;\n\n        __m256i vals = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cmp = _mm256_cmpgt_epi8(vals, max_vals);\n\n        max_vals = _mm256_blendv_epi8(max_vals, vals, cmp);\n        max_indices = _mm256_blendv_epi8(max_indices, _mm256_set1_epi8(i), cmp);\n    }\n\n    int8_t max_val = src[0];\n    for (int j = 0; j < 32; ++j) {\n        int8_t val = ((int8_t*)&max_vals)[j];\n        if (val > max_val) {\n            max_val = val;\n            max_index = ((int8_t*)&max_indices)[j];\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                            i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i and_result = _mm256_and_si256(idx_vec, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                            i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i and_result = _mm256_and_si256(idx_vec, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                           i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i and_result = _mm256_and_si256(idx_vec, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                            i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i and_result = _mm256_and_si256(idx_vec, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                            i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i and_result = _mm256_and_si256(idx_vec, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i popcnt = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i popcnt = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i popcnt = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i popcnt = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i popcnt = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(a, b);\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(a, b);\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n    const __m256d zero = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask_b = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n\n                a = _mm256_and_pd(a, mask_a);\n                b = _mm256_and_pd(b, mask_b);\n\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n    const __m256d lower_bound_vec = _mm256_set1_pd(lower_bound);\n    const __m256d upper_bound_vec = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vec, lower_bound_vec, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vec, upper_bound_vec, _CMP_LE_OQ)\n                );\n                __m256d mask_b = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vec, lower_bound_vec, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vec, upper_bound_vec, _CMP_LE_OQ)\n                );\n\n                __m256d masked_a = _mm256_and_pd(a_vec, mask_a);\n                __m256d masked_b = _mm256_and_pd(b_vec, mask_b);\n\n                sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(masked_a, masked_b));\n            }\n\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            C[i * p + j] = sum[0] + sum[1] + sum[2] + sum[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask_b = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n\n                a = _mm256_and_pd(a, mask_a);\n                b = _mm256_and_pd(b, mask_b);\n\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ), _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ), _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ));\n\n                a = _mm256_and_pd(a, mask_a);\n                b = _mm256_and_pd(b, mask_b);\n\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask_b = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n\n                __m256d masked_a = _mm256_and_pd(a, mask_a);\n                __m256d masked_b = _mm256_and_pd(b, mask_b);\n\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(masked_a, masked_b));\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    for (size_t i = 0; i < size; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&src1[i]);\n        __m256 b = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&src1[i]);\n        __m256 b = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(sum);\n        __m256 result = _mm256_cvtepi32_ps(rounded);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&src1[i]);\n        __m256 b = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&src1[i]);\n        __m256 b = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(sum);\n        __m256 result = _mm256_cvtepi32_ps(rounded);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i mask = _mm256_setr_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 cmp = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256 mul = _mm256_mul_ps(v1, v2);\n        __m256 round_up = _mm256_round_ps(mul, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n\n        __m256 result = _mm256_blendv_ps(neg_one, round_up, cmp);\n\n        __m256i index_mask = _mm256_set1_epi32(i);\n        __m256i even_mask = _mm256_and_si256(index_mask, mask);\n        __m256i zero_mask = _mm256_setzero_si256();\n        __m256 final_result = _mm256_blendv_ps(neg_one, result, _mm256_castsi256_ps(even_mask));\n\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i index_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), _mm256_sub_ps(v_src1, v_src2));\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256i index = _mm256_set1_epi32(i);\n        __m256i index_mod_2 = _mm256_and_si256(index, index_mask);\n        __m256i index_mask_2 = _mm256_cmpeq_epi32(index_mod_2, _mm256_setzero_si256());\n        __m256 mask_2 = _mm256_castsi256_ps(index_mask_2);\n\n        __m256 combined_mask = _mm256_and_ps(mask, mask_2);\n\n        __m256 result = _mm256_mul_ps(v_src1, v_src2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF);\n\n        __m256 final_result = _mm256_blendv_ps(neg_one, result, combined_mask);\n\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i index_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256i index = _mm256_set1_epi32(i);\n        __m256i index_mod = _mm256_and_si256(index, index_mask);\n        __m256i index_mask_2 = _mm256_set1_epi32(2);\n        __m256i mod_mask = _mm256_cmpeq_epi32(index_mod, _mm256_setzero_si256());\n        __m256 mask2 = _mm256_castsi256_ps(mod_mask);\n\n        __m256 combined_mask = _mm256_and_ps(mask, mask2);\n        __m256 result = _mm256_mul_ps(v_src1, v_src2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF);\n\n        __m256 final_result = _mm256_blendv_ps(neg_one, result, combined_mask);\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i index_mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256i index = _mm256_set1_epi32(i);\n        __m256i index_masked = _mm256_and_si256(index, index_mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i index_mod2 = _mm256_cmpeq_epi32(index_masked, zero);\n        __m256 mask_mod2 = _mm256_castsi256_ps(index_mod2);\n\n        __m256 result = _mm256_mul_ps(v_src1, v_src2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF);\n\n        __m256 final_mask = _mm256_and_ps(mask, mask_mod2);\n        result = _mm256_blendv_ps(neg_one, result, final_mask);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i index_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256i index = _mm256_set1_epi32(i);\n        __m256i index_mod = _mm256_and_si256(index, _mm256_set1_epi32(1));\n        __m256i index_masked = _mm256_cmpeq_epi32(index_mod, _mm256_setzero_si256());\n        __m256 mask2 = _mm256_castsi256_ps(index_masked);\n\n        __m256 combined_mask = _mm256_and_ps(mask, mask2);\n        __m256 result = _mm256_mul_ps(v_src1, v_src2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF);\n\n        __m256 final_result = _mm256_blendv_ps(neg_one, result, combined_mask);\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i = 0;\n\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (; i <= size - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i = 0;\n\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (; i <= size - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i = 0;\n\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (; i <= size - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            sum_vec = _mm256_fmadd_ps(vec, vec, sum_vec);\n        }\n\n        float sum[8];\n        _mm256_storeu_ps(sum, sum_vec);\n        float row_norm = sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n\n        for (; j < n; ++j) {\n            row_norm += row[j] * row[j];\n        }\n\n        row_norm = sqrtf(row_norm);\n\n        __m256 norm_vec = _mm256_set1_ps(row_norm);\n        j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            vec = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(&row[j], vec);\n        }\n\n        for (; j < n; ++j) {\n            row[j] /= row_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            sum_vec = _mm256_fmadd_ps(vec, vec, sum_vec);\n        }\n\n        float sum[8];\n        _mm256_storeu_ps(sum, sum_vec);\n        float row_sum = sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n\n        for (; j < n; ++j) {\n            row_sum += row[j] * row[j];\n        }\n\n        float norm = sqrtf(row_sum);\n\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            vec = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(&row[j], vec);\n        }\n\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 vec_sq = _mm256_mul_ps(vec, vec);\n            sum_vec = _mm256_add_ps(sum_vec, vec_sq);\n        }\n        float sum[8];\n        _mm256_storeu_ps(sum, sum_vec);\n        float row_norm = 0.0f;\n        for (int k = 0; k < 8; ++k) {\n            row_norm += sum[k];\n        }\n        row_norm = sqrtf(row_norm);\n        __m256 norm_vec = _mm256_set1_ps(row_norm);\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 normalized_vec = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            sum_vec = _mm256_fmadd_ps(vec, vec, sum_vec);\n        }\n\n        float sum[8];\n        _mm256_storeu_ps(sum, sum_vec);\n        float row_sum = sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n\n        for (; j < n; ++j) {\n            row_sum += row[j] * row[j];\n        }\n\n        float norm = sqrtf(row_sum);\n        __m256 norm_vec = _mm256_set1_ps(norm);\n\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            vec = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(&row[j], vec);\n        }\n\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_vec = _mm256_fmadd_ps(vec, vec, sum_vec);\n        }\n\n        float sum[8];\n        _mm256_storeu_ps(sum, sum_vec);\n        float norm = sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n\n        for (; j < n; ++j) {\n            norm += row[j] * row[j];\n        }\n\n        norm = sqrtf(norm);\n        float inv_norm = 1.0f / norm;\n\n        __m256 inv_norm_vec = _mm256_set1_ps(inv_norm);\n        j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm_vec);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        for (; j < n; ++j) {\n            row[j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_fmadd_pd(a, b, sum);\n            }\n            __m256d result = _mm256_round_pd(sum, _MM_FROUND_TO_NEAREST_INT);\n            _mm256_storeu_pd(&dst[i * p + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_fmadd_pd(a, b, sum);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            dst[i * p + j] = round(temp[0] + temp[1] + temp[2] + temp[3]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_fmadd_pd(a, b, sum);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            dst[i * p + j] = round(temp[0] + temp[1] + temp[2] + temp[3]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_fmadd_pd(a, b, sum);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            dst[i * p + j] = round(temp[0] + temp[1] + temp[2] + temp[3]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_fmadd_pd(a, b, sum);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            dst[i * p + j] = round(temp[0] + temp[1] + temp[2] + temp[3]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 rounder = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            for (size_t k = 0; k < 8; ++k) {\n                __m256 src_vec = _mm256_loadu_ps(&src[(i + k) * cols + j]);\n                __m256 rounded_vec = _mm256_round_ps(_mm256_add_ps(src_vec, rounder), _MM_FROUND_TO_NEAREST_INT);\n                __m256 result_vec = _mm256_mul_ps(rounded_vec, quarter);\n\n                _mm256_storeu_ps(&dst[j * rows + (i + k)], result_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 round_quarter = _mm256_set1_ps(0.25f);\n    __m256 round_half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            for (size_t k = 0; k < 8; ++k) {\n                __m256 src_vec = _mm256_loadu_ps(&src[(i + k) * cols + j]);\n                __m256 rounded_vec = _mm256_round_ps(_mm256_add_ps(src_vec, round_half), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                rounded_vec = _mm256_mul_ps(rounded_vec, round_quarter);\n\n                _mm256_storeu_ps(&dst[j * rows + (i + k)], rounded_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 rounder = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            for (size_t k = 0; k < 8; ++k) {\n                __m256 row = _mm256_loadu_ps(&src[(i + k) * cols + j]);\n                __m256 rounded = _mm256_round_ps(_mm256_add_ps(row, rounder), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                __m256 result = _mm256_mul_ps(rounded, quarter);\n\n                _mm256_storeu_ps(&dst[j * rows + (i + k)], result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row0 = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 row1 = _mm256_loadu_ps(&src[(i + 1) * cols + j]);\n            __m256 row2 = _mm256_loadu_ps(&src[(i + 2) * cols + j]);\n            __m256 row3 = _mm256_loadu_ps(&src[(i + 3) * cols + j]);\n            __m256 row4 = _mm256_loadu_ps(&src[(i + 4) * cols + j]);\n            __m256 row5 = _mm256_loadu_ps(&src[(i + 5) * cols + j]);\n            __m256 row6 = _mm256_loadu_ps(&src[(i + 6) * cols + j]);\n            __m256 row7 = _mm256_loadu_ps(&src[(i + 7) * cols + j]);\n\n            __m256 col0 = _mm256_setr_ps(row0[0], row1[0], row2[0], row3[0], row4[0], row5[0], row6[0], row7[0]);\n            __m256 col1 = _mm256_setr_ps(row0[1], row1[1], row2[1], row3[1], row4[1], row5[1], row6[1], row7[1]);\n            __m256 col2 = _mm256_setr_ps(row0[2], row1[2], row2[2], row3[2], row4[2], row5[2], row6[2], row7[2]);\n            __m256 col3 = _mm256_setr_ps(row0[3], row1[3], row2[3], row3[3], row4[3], row5[3], row6[3], row7[3]);\n            __m256 col4 = _mm256_setr_ps(row0[4], row1[4], row2[4], row3[4], row4[4], row5[4], row6[4], row7[4]);\n            __m256 col5 = _mm256_setr_ps(row0[5], row1[5], row2[5], row3[5], row4[5], row5[5], row6[5], row7[5]);\n            __m256 col6 = _mm256_setr_ps(row0[6], row1[6], row2[6], row3[6], row4[6], row5[6], row6[6], row7[6]);\n            __m256 col7 = _mm256_setr_ps(row0[7], row1[7], row2[7], row3[7], row4[7], row5[7], row6[7], row7[7]);\n\n            col0 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col0, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col1 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col1, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col2 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col2, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col3 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col3, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col4 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col4, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col5 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col5, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col6 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col6, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n            col7 = _mm256_round_ps(_mm256_add_ps(_mm256_div_ps(col7, quarter), half), _MM_FROUND_TO_NEAREST_INT);\n\n            col0 = _mm256_mul_ps(col0, quarter);\n            col1 = _mm256_mul_ps(col1, quarter);\n            col2 = _mm256_mul_ps(col2, quarter);\n            col3 = _mm256_mul_ps(col3, quarter);\n            col4 = _mm256_mul_ps(col4, quarter);\n            col5 = _mm256_mul_ps(col5, quarter);\n            col6 = _mm256_mul_ps(col6, quarter);\n            col7 = _mm256_mul_ps(col7, quarter);\n\n            _mm256_storeu_ps(&dst[j * rows + i], col0);\n            _mm256_storeu_ps(&dst[(j + 1) * rows + i], col1);\n            _mm256_storeu_ps(&dst[(j + 2) * rows + i], col2);\n            _mm256_storeu_ps(&dst[(j + 3) * rows + i], col3);\n            _mm256_storeu_ps(&dst[(j + 4) * rows + i], col4);\n            _mm256_storeu_ps(&dst[(j + 5) * rows + i], col5);\n            _mm256_storeu_ps(&dst[(j + 6) * rows + i], col6);\n            _mm256_storeu_ps(&dst[(j + 7) * rows + i], col7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 rounder = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row0 = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 row1 = _mm256_loadu_ps(&src[(i + 1) * cols + j]);\n            __m256 row2 = _mm256_loadu_ps(&src[(i + 2) * cols + j]);\n            __m256 row3 = _mm256_loadu_ps(&src[(i + 3) * cols + j]);\n            __m256 row4 = _mm256_loadu_ps(&src[(i + 4) * cols + j]);\n            __m256 row5 = _mm256_loadu_ps(&src[(i + 5) * cols + j]);\n            __m256 row6 = _mm256_loadu_ps(&src[(i + 6) * cols + j]);\n            __m256 row7 = _mm256_loadu_ps(&src[(i + 7) * cols + j]);\n\n            __m256 t0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 t1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 t2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 t3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 t4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 t5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 t6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 t7 = _mm256_unpackhi_ps(row6, row7);\n\n            row0 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(1, 0, 1, 0));\n            row1 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(3, 2, 3, 2));\n            row2 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(1, 0, 1, 0));\n            row3 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(3, 2, 3, 2));\n            row4 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(1, 0, 1, 0));\n            row5 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(3, 2, 3, 2));\n            row6 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(1, 0, 1, 0));\n            row7 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(3, 2, 3, 2));\n\n            t0 = _mm256_permute2f128_ps(row0, row4, 0x20);\n            t1 = _mm256_permute2f128_ps(row1, row5, 0x20);\n            t2 = _mm256_permute2f128_ps(row2, row6, 0x20);\n            t3 = _mm256_permute2f128_ps(row3, row7, 0x20);\n            t4 = _mm256_permute2f128_ps(row0, row4, 0x31);\n            t5 = _mm256_permute2f128_ps(row1, row5, 0x31);\n            t6 = _mm256_permute2f128_ps(row2, row6, 0x31);\n            t7 = _mm256_permute2f128_ps(row3, row7, 0x31);\n\n            row0 = _mm256_add_ps(_mm256_mul_ps(t0, quarter), rounder);\n            row1 = _mm256_add_ps(_mm256_mul_ps(t1, quarter), rounder);\n            row2 = _mm256_add_ps(_mm256_mul_ps(t2, quarter), rounder);\n            row3 = _mm256_add_ps(_mm256_mul_ps(t3, quarter), rounder);\n            row4 = _mm256_add_ps(_mm256_mul_ps(t4, quarter), rounder);\n            row5 = _mm256_add_ps(_mm256_mul_ps(t5, quarter), rounder);\n            row6 = _mm256_add_ps(_mm256_mul_ps(t6, quarter), rounder);\n            row7 = _mm256_add_ps(_mm256_mul_ps(t7, quarter), rounder);\n\n            row0 = _mm256_round_ps(row0, _MM_FROUND_TO_NEAREST_INT);\n            row1 = _mm256_round_ps(row1, _MM_FROUND_TO_NEAREST_INT);\n            row2 = _mm256_round_ps(row2, _MM_FROUND_TO_NEAREST_INT);\n            row3 = _mm256_round_ps(row3, _MM_FROUND_TO_NEAREST_INT);\n            row4 = _mm256_round_ps(row4, _MM_FROUND_TO_NEAREST_INT);\n            row5 = _mm256_round_ps(row5, _MM_FROUND_TO_NEAREST_INT);\n            row6 = _mm256_round_ps(row6, _MM_FROUND_TO_NEAREST_INT);\n            row7 = _mm256_round_ps(row7, _MM_FROUND_TO_NEAREST_INT);\n\n            row0 = _mm256_mul_ps(row0, quarter);\n            row1 = _mm256_mul_ps(row1, quarter);\n            row2 = _mm256_mul_ps(row2, quarter);\n            row3 = _mm256_mul_ps(row3, quarter);\n            row4 = _mm256_mul_ps(row4, quarter);\n            row5 = _mm256_mul_ps(row5, quarter);\n            row6 = _mm256_mul_ps(row6, quarter);\n            row7 = _mm256_mul_ps(row7, quarter);\n\n            _mm256_storeu_ps(&dst[j * rows + i], row0);\n            _mm256_storeu_ps(&dst[(j + 1) * rows + i], row1);\n            _mm256_storeu_ps(&dst[(j + 2) * rows + i], row2);\n            _mm256_storeu_ps(&dst[(j + 3) * rows + i], row3);\n            _mm256_storeu_ps(&dst[(j + 4) * rows + i], row4);\n            _mm256_storeu_ps(&dst[(j + 5) * rows + i], row5);\n            _mm256_storeu_ps(&dst[(j + 6) * rows + i], row6);\n            _mm256_storeu_ps(&dst[(j + 7) * rows + i], row7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(v1, v2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_elements - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = tensor1[i] - tensor2[i];\n        dst[i] = roundf(dst[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(v1, v2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 a = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 b = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(a, b, sum);\n        }\n        sum = _mm256_hadd_ps(sum, sum);\n        sum = _mm256_hadd_ps(sum, sum);\n        dst[i] = _mm256_cvtss_f32(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 acc = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 a = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 b = _mm256_loadu_ps(&vec[j]);\n            acc = _mm256_fmadd_ps(a, b, acc);\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, acc);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 a = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 b = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(a, b, sum);\n        }\n        sum = _mm256_hadd_ps(sum, sum);\n        sum = _mm256_hadd_ps(sum, sum);\n        dst[i] = _mm256_cvtss_f32(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 vec_avx, mat_avx, dst_avx;\n\n    for (i = 0; i < m; ++i) {\n        dst_avx = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            vec_avx = _mm256_loadu_ps(&vec[j]);\n            mat_avx = _mm256_loadu_ps(&mat[i * n + j]);\n            dst_avx = _mm256_fmadd_ps(mat_avx, vec_avx, dst_avx);\n        }\n        dst[i] = _mm256_reduce_add_ps(dst_avx);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 vec_avx;\n    __m256 mat_avx;\n    __m256 result_avx;\n\n    for (i = 0; i < m; ++i) {\n        result_avx = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            vec_avx = _mm256_loadu_ps(&vec[j]);\n            mat_avx = _mm256_loadu_ps(&mat[i * n + j]);\n            result_avx = _mm256_fmadd_ps(mat_avx, vec_avx, result_avx);\n        }\n        dst[i] = _mm256_reduce_add_ps(result_avx);\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t num_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= num_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < num_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / num_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d sum = _mm256_setzero_pd();\n    size_t i;\n\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / size;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d sum = _mm256_setzero_pd();\n    size_t i;\n\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / size;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d sum = _mm256_setzero_pd();\n    size_t i;\n\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / size;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m256d val = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, val);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / size;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        bool sorted = true;\n\n        if (cols >= 8) {\n            __m256i prev = _mm256_loadu_si256((__m256i*)row);\n            for (size_t j = 8; j < cols; j += 8) {\n                __m256i curr = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i cmp = ascending ? _mm256_cmpgt_epi32(prev, curr) : _mm256_cmpgt_epi32(curr, prev);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    sorted = false;\n                    break;\n                }\n                prev = curr;\n            }\n            if (!sorted) continue;\n\n            size_t remaining = cols % 8;\n            if (remaining > 0) {\n                for (size_t j = cols - remaining; j < cols - 1; ++j) {\n                    if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n        } else {\n            for (size_t j = 0; j < cols - 1; ++j) {\n                if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                    sorted = false;\n                    break;\n                }\n            }\n        }\n\n        if (!sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        size_t j = 0;\n\n        if (cols >= 8) {\n            for (; j <= cols - 8; j += 8) {\n                __m256i vec1 = _mm256_loadu_si256((__m256i*)&row[j]);\n                __m256i vec2 = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n\n                __m256i cmp = ascending ? _mm256_cmpgt_epi32(vec1, vec2) : _mm256_cmpgt_epi32(vec2, vec1);\n\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        if (cols >= 8) {\n            __m256i prev = _mm256_loadu_si256((__m256i*)row);\n            for (size_t j = 8; j < cols; j += 8) {\n                __m256i curr = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i cmp = ascending ? _mm256_cmpgt_epi32(prev, curr) : _mm256_cmpgt_epi32(curr, prev);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n                prev = curr;\n            }\n            size_t remaining = cols % 8;\n            if (remaining > 0) {\n                for (size_t j = cols - remaining; j < cols - 1; ++j) {\n                    if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                        return false;\n                    }\n                }\n            }\n        } else {\n            for (size_t j = 0; j < cols - 1; ++j) {\n                if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        size_t j = 0;\n\n        if (cols >= 8) {\n            for (; j <= cols - 8; j += 8) {\n                __m256i vec1 = _mm256_loadu_si256((__m256i*)&row[j]);\n                __m256i vec2 = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n\n                __m256i cmp = ascending ? _mm256_cmpgt_epi32(vec1, vec2) : _mm256_cmpgt_epi32(vec2, vec1);\n\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        bool sorted = true;\n\n        if (cols >= 8) {\n            __m256i prev = _mm256_loadu_si256((__m256i*)row);\n            for (size_t j = 8; j < cols; j += 8) {\n                __m256i curr = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i cmp = ascending ? _mm256_cmpgt_epi32(prev, curr) : _mm256_cmpgt_epi32(curr, prev);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    sorted = false;\n                    break;\n                }\n                prev = curr;\n            }\n            if (!sorted) continue;\n            size_t remaining = cols % 8;\n            if (remaining > 0) {\n                for (size_t j = cols - remaining; j < cols - 1; ++j) {\n                    if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n        } else {\n            for (size_t j = 0; j < cols - 1; ++j) {\n                if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                    sorted = false;\n                    break;\n                }\n            }\n        }\n        if (!sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vone = _mm256_set1_epi16(1);\n    __m256i vhalf_base = _mm256_set1_epi16(base / 2);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n        __m256i vproduct = _mm256_mullo_epi16(vquotient, vbase);\n        __m256i vdiff = _mm256_sub_epi16(vsrc, vproduct);\n        __m256i vadjust = _mm256_and_si256(_mm256_cmpgt_epi16(vdiff, vhalf_base), vone);\n        __m256i vresult = _mm256_sub_epi16(vproduct, _mm256_mullo_epi16(vadjust, vbase));\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vone = _mm256_set1_epi16(1);\n    __m256i vhalf_base = _mm256_set1_epi16(base / 2);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n        __m256i vproduct = _mm256_mullo_epi16(vquotient, vbase);\n        __m256i vdiff = _mm256_sub_epi16(vsrc, vproduct);\n        __m256i vadjust = _mm256_and_si256(_mm256_cmpgt_epi16(vdiff, vhalf_base), vone);\n        __m256i vresult = _mm256_sub_epi16(vproduct, _mm256_mullo_epi16(vadjust, vbase));\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vone = _mm256_set1_epi16(1);\n    __m256i vhalf_base = _mm256_set1_epi16(base / 2);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n        __m256i vproduct = _mm256_mullo_epi16(vquotient, vbase);\n        __m256i vremainder = _mm256_sub_epi16(vsrc, vproduct);\n        __m256i vadjust = _mm256_and_si256(_mm256_cmpgt_epi16(vremainder, vhalf_base), vone);\n        __m256i vresult = _mm256_sub_epi16(vproduct, _mm256_mullo_epi16(vadjust, vbase));\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vone = _mm256_set1_epi16(1);\n    __m256i vhalf_base = _mm256_set1_epi16(base / 2);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n        __m256i vproduct = _mm256_mullo_epi16(vquotient, vbase);\n        __m256i vdiff = _mm256_sub_epi16(vsrc, vproduct);\n        __m256i vadjust = _mm256_and_si256(_mm256_cmpgt_epi16(vdiff, vhalf_base), vone);\n        __m256i vresult = _mm256_sub_epi16(vproduct, _mm256_mullo_epi16(vadjust, vbase));\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vone = _mm256_set1_epi16(1);\n    __m256i vhalf_base = _mm256_set1_epi16(base / 2);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n        __m256i vproduct = _mm256_mullo_epi16(vquotient, vbase);\n        __m256i vremainder = _mm256_sub_epi16(vsrc, vproduct);\n        __m256i vadjust = _mm256_and_si256(_mm256_cmpgt_epi16(vremainder, vhalf_base), vone);\n        __m256i vresult = _mm256_sub_epi16(vproduct, _mm256_mullo_epi16(vadjust, vbase));\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask == 0xF) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = &matrix[i * cols];\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&row[j]);\n            __m256d cmp = _mm256_cmp_pd(vec, mat_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask == 0xF) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d mat_row = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat_row, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask == 0xF) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, row_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask == 0xF) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d mat_row = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat_row, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask == 0xF) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(cmp));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    for (; i <= total_elements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(a, b);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    for (; i <= total_elements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(eq));\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    for (; i <= total_elements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(a, b);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vx, vy, vabs, vresult;\n\n    for (i = 0; i <= length - 4; i += 4) {\n        vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        vx = _mm256_mullo_epi64(vx, va);\n        vabs = _mm256_abs_epi64(vy);\n        vresult = _mm256_sub_epi64(vx, vabs);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vx, vy, vz;\n\n    for (i = 0; i <= length - 4; i += 4) {\n        vx = _mm256_loadu_si256((__m256i *)&x[i]);\n        vy = _mm256_loadu_si256((__m256i *)&y[i]);\n\n        vz = _mm256_mullo_epi64(vx, va);\n        vy = _mm256_abs_epi64(vy);\n        vz = _mm256_sub_epi64(vz, vy);\n\n        _mm256_storeu_si256((__m256i *)&y[i], vz);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i va = _mm256_set1_epi64x(a);\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        __m256i vabs = _mm256_abs_epi64(vy);\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vsub);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vx, vy, vabs, vresult;\n\n    for (i = 0; i <= length - 4; i += 4) {\n        vx = _mm256_loadu_si256((__m256i *)&x[i]);\n        vy = _mm256_loadu_si256((__m256i *)&y[i]);\n\n        vresult = _mm256_mullo_epi64(vx, va);\n        vabs = _mm256_abs_epi64(vy);\n        vresult = _mm256_sub_epi64(vresult, vabs);\n\n        _mm256_storeu_si256((__m256i *)&y[i], vresult);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i va = _mm256_set1_epi64x(a);\n    size_t i = 0;\n\n    for (; i <= length - 4; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        __m256i vabs = _mm256_abs_epi64(vy);\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vsub);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, current_vec);\n        max_vec = _mm256_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = min_vals[0];\n    int16_t max_num = max_vals[0];\n\n    for (int i = 1; i < 16; ++i) {\n        if (min_vals[i] < min_num) {\n            min_num = min_vals[i];\n        }\n        if (max_vals[i] > max_num) {\n            max_num = max_vals[i];\n        }\n    }\n\n    MinMaxPair result;\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, current_vec);\n        max_vec = _mm256_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = min_vals[0];\n    int16_t max_num = max_vals[0];\n\n    for (int i = 1; i < 16; ++i) {\n        if (min_vals[i] < min_num) min_num = min_vals[i];\n        if (max_vals[i] > max_num) max_num = max_vals[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, current_vec);\n        max_vec = _mm256_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = min_vals[0];\n    int16_t max_num = max_vals[0];\n\n    for (int i = 1; i < 16; ++i) {\n        if (min_vals[i] < min_num) {\n            min_num = min_vals[i];\n        }\n        if (max_vals[i] > max_num) {\n            max_num = max_vals[i];\n        }\n    }\n\n    MinMaxPair result;\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, current_vec);\n        max_vec = _mm256_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = min_vals[0];\n    int16_t max_num = max_vals[0];\n\n    for (int i = 1; i < 16; ++i) {\n        if (min_vals[i] < min_num) min_num = min_vals[i];\n        if (max_vals[i] > max_num) max_num = max_vals[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, current_vec);\n        max_vec = _mm256_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = min_vals[0];\n    int16_t max_num = max_vals[0];\n\n    for (int i = 1; i < 16; ++i) {\n        if (min_vals[i] < min_num) min_num = min_vals[i];\n        if (max_vals[i] > max_num) max_num = max_vals[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j + 8 < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j + 8 < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j + 8 < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j + 8 < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const size_t num_blocks_row = rows / block_size;\n    const size_t num_blocks_col = cols / block_size;\n\n    __m256i first_block[block_size];\n    for (size_t i = 0; i < block_size; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i*)&matrix[i * cols]);\n    }\n\n    for (size_t br = 0; br < num_blocks_row; ++br) {\n        for (size_t bc = 0; bc < num_blocks_col; ++bc) {\n            if (br == 0 && bc == 0) continue;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                __m256i current_block = _mm256_loadu_si256((__m256i*)&matrix[(br * block_size + i) * cols + bc * block_size]);\n                __m256i cmp_result = _mm256_cmpeq_epi32(current_block, first_block[i]);\n                int mask = _mm256_movemask_epi8(cmp_result);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const size_t num_blocks_row = rows / block_size;\n    const size_t num_blocks_col = cols / block_size;\n\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            const int* first_block = matrix + block_row * block_size * cols + block_col * block_size;\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i first = _mm256_loadu_si256((__m256i*)(first_block + i * cols + j));\n                    for (size_t b_row = 0; b_row < num_blocks_row; ++b_row) {\n                        for (size_t b_col = 0; b_col < num_blocks_col; ++b_col) {\n                            if (b_row != block_row || b_col != block_col) {\n                                const int* current_block = matrix + b_row * block_size * cols + b_col * block_size;\n                                __m256i current = _mm256_loadu_si256((__m256i*)(current_block + i * cols + j));\n                                if (!_mm256_testc_si256(first, current)) {\n                                    return false;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    __m256i first_block[block_elements / 8];\n\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i*)(matrix + i * 8));\n    }\n\n    for (size_t br = 0; br < num_blocks_row; ++br) {\n        for (size_t bc = 0; bc < num_blocks_col; ++bc) {\n            if (br == 0 && bc == 0) continue;\n\n            const int* current_block = matrix + (br * block_size * cols + bc * block_size);\n\n            for (size_t i = 0; i < block_elements / 8; ++i) {\n                __m256i current = _mm256_loadu_si256((__m256i*)(current_block + i * 8));\n                __m256i cmp = _mm256_cmpeq_epi32(first_block[i], current);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    __m256i first_block[block_elements / 8];\n\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i*)&matrix[i * 8]);\n    }\n\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue;\n\n            size_t block_start = (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_elements / 8; ++i) {\n                __m256i current_block = _mm256_loadu_si256((__m256i*)&matrix[block_start + i * 8]);\n                __m256i cmp_result = _mm256_cmpeq_epi32(current_block, first_block[i]);\n\n                if (!_mm256_testc_si256(cmp_result, _mm256_set1_epi32(-1))) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    __m256i first_block[block_elements / 8];\n\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i*)&matrix[i * 8]);\n    }\n\n    for (size_t br = 0; br < num_blocks_row; ++br) {\n        for (size_t bc = 0; bc < num_blocks_col; ++bc) {\n            if (br == 0 && bc == 0) continue;\n\n            size_t block_start = (br * block_size * cols + bc * block_size);\n\n            for (size_t i = 0; i < block_elements / 8; ++i) {\n                __m256i current_block = _mm256_loadu_si256((__m256i*)&matrix[block_start + i * 8]);\n                __m256i cmp_result = _mm256_cmpeq_epi32(first_block[i], current_block);\n                int mask = _mm256_movemask_epi8(cmp_result);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block[block_size / 4];\n\n    for (size_t i = 0; i < block_size / 4; ++i) {\n        first_block[i] = _mm256_loadu_pd(&vec[i * 4]);\n    }\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size / 4; ++i) {\n            __m256d current_block = _mm256_loadu_pd(&vec[block * block_size + i * 4]);\n            __m256d diff = _mm256_sub_pd(first_block[i], current_block);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            if (!_mm256_testc_pd(cmp, _mm256_set1_pd(-0.0))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block = _mm256_loadu_pd(vec);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* block_ptr = vec + i * block_size;\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d current_block = _mm256_loadu_pd(block_ptr + j);\n            __m256d diff = _mm256_sub_pd(current_block, first_block);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d within_tol = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            if (!_mm256_testz_pd(within_tol, _mm256_set1_pd(-1.0))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size < 4 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block = _mm256_loadu_pd(vec);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d current_vec = _mm256_loadu_pd(current_block + j);\n            __m256d diff = _mm256_sub_pd(first_block, current_vec);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block[block_size / 4];\n\n    for (size_t i = 0; i < block_size / 4; ++i) {\n        first_block[i] = _mm256_loadu_pd(&vec[i * 4]);\n    }\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size / 4; ++i) {\n            __m256d current_block = _mm256_loadu_pd(&vec[block * block_size + i * 4]);\n            __m256d diff = _mm256_sub_pd(current_block, first_block[i]);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n\n            if (!_mm256_testc_pd(cmp, _mm256_set1_pd(-0.0))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block[block_size / 4];\n\n    for (size_t i = 0; i < block_size / 4; ++i) {\n        first_block[i] = _mm256_loadu_pd(&vec[i * 4]);\n    }\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size / 4; ++i) {\n            __m256d current_block = _mm256_loadu_pd(&vec[block * block_size + i * 4]);\n            __m256d diff = _mm256_sub_pd(current_block, first_block[i]);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n\n            if (!_mm256_testc_pd(cmp, _mm256_set1_pd(-1.0))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d threshold_vec;\n    __m256d row_vec;\n    __m256d norm_vec;\n    __m256d mask_vec;\n    __m256d result_vec;\n\n    for (i = 0; i < rows; ++i) {\n        sum_vec = _mm256_setzero_pd();\n        threshold_vec = _mm256_set1_pd(thresholds[i]);\n\n        for (j = 0; j < cols; j += 4) {\n            row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec);\n        }\n\n        norm_vec = _mm256_hadd_pd(sum_vec, sum_vec);\n        norm_vec = _mm256_hadd_pd(norm_vec, norm_vec);\n        norm_vec = _mm256_sqrt_pd(norm_vec);\n\n        mask_vec = _mm256_cmp_pd(norm_vec, threshold_vec, _CMP_GT_OQ);\n        result_vec = _mm256_and_pd(mask_vec, _mm256_set1_pd(1.0));\n\n        result[i] = _mm256_testz_pd(result_vec, result_vec) == 0;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum_vec, val_vec, threshold_vec;\n    double sum[4];\n\n    for (i = 0; i < rows; ++i) {\n        sum_vec = _mm256_setzero_pd();\n        for (j = 0; j < cols; j += 4) {\n            val_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(val_vec, val_vec, sum_vec);\n        }\n        _mm256_storeu_pd(sum, sum_vec);\n        double row_sum = sum[0] + sum[1] + sum[2] + sum[3];\n        threshold_vec = _mm256_set1_pd(thresholds[i]);\n        result[i] = row_sum > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum, temp, row_val, threshold_val;\n    double temp_result[4];\n\n    for (i = 0; i < rows; ++i) {\n        sum = _mm256_setzero_pd();\n        for (j = 0; j < cols; j += 4) {\n            row_val = _mm256_loadu_pd(&matrix[i * cols + j]);\n            temp = _mm256_mul_pd(row_val, row_val);\n            sum = _mm256_add_pd(sum, temp);\n        }\n\n        _mm256_storeu_pd(temp_result, sum);\n        double row_norm = temp_result[0] + temp_result[1] + temp_result[2] + temp_result[3];\n        for (; j < cols; ++j) {\n            row_norm += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n        row_norm = sqrt(row_norm);\n\n        threshold_val = _mm256_set1_pd(thresholds[i]);\n        _mm256_storeu_pd(temp_result, threshold_val);\n        double threshold = temp_result[0];\n\n        result[i] = row_norm > threshold;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum_vec, temp_vec, matrix_vec;\n    double sum[4];\n\n    for (i = 0; i < rows; ++i) {\n        sum_vec = _mm256_setzero_pd();\n        for (j = 0; j < cols; j += 4) {\n            matrix_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            temp_vec = _mm256_mul_pd(matrix_vec, matrix_vec);\n            sum_vec = _mm256_add_pd(sum_vec, temp_vec);\n        }\n        _mm256_storeu_pd(sum, sum_vec);\n        double row_sum = sum[0] + sum[1] + sum[2] + sum[3];\n        for (; j < cols; ++j) {\n            row_sum += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n        result[i] = (row_sum > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum_vec, temp_vec, matrix_vec;\n    double sum[4];\n\n    for (i = 0; i < rows; ++i) {\n        sum_vec = _mm256_setzero_pd();\n        for (j = 0; j < cols; j += 4) {\n            matrix_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            temp_vec = _mm256_mul_pd(matrix_vec, matrix_vec);\n            sum_vec = _mm256_add_pd(sum_vec, temp_vec);\n        }\n        _mm256_storeu_pd(sum, sum_vec);\n        double row_sum = sum[0] + sum[1] + sum[2] + sum[3];\n        for (j = cols - (cols % 4); j < cols; ++j) {\n            row_sum += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n        result[i] = (row_sum > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i ab_product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_squared, b_squared));\n        sum_products = _mm256_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t sum_squares_arr[8];\n    int32_t sum_products_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_squares_arr, sum_squares);\n    _mm256_storeu_si256((__m256i*)sum_products_arr, sum_products);\n\n    int64_t sum_squares_total = 0;\n    int64_t sum_products_total = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum_squares_total += sum_squares_arr[i];\n        sum_products_total += sum_products_arr[i];\n    }\n\n    return sum_squares_total - sum_products_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i ab_product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_squared, b_squared));\n        sum_products = _mm256_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t sum_squares_scalar = _mm256_extract_epi32(sum_squares, 0) + _mm256_extract_epi32(sum_squares, 1) +\n                                  _mm256_extract_epi32(sum_squares, 2) + _mm256_extract_epi32(sum_squares, 3) +\n                                  _mm256_extract_epi32(sum_squares, 4) + _mm256_extract_epi32(sum_squares, 5) +\n                                  _mm256_extract_epi32(sum_squares, 6) + _mm256_extract_epi32(sum_squares, 7);\n\n    int32_t sum_products_scalar = _mm256_extract_epi32(sum_products, 0) + _mm256_extract_epi32(sum_products, 1) +\n                                  _mm256_extract_epi32(sum_products, 2) + _mm256_extract_epi32(sum_products, 3) +\n                                  _mm256_extract_epi32(sum_products, 4) + _mm256_extract_epi32(sum_products, 5) +\n                                  _mm256_extract_epi32(sum_products, 6) + _mm256_extract_epi32(sum_products, 7);\n\n    return sum_squares_scalar - sum_products_scalar;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i ab_product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_squared, b_squared));\n        sum_products = _mm256_add_epi32(sum_products, ab_product);\n    }\n\n    int64_t sum_squares_scalar = _mm256_extract_epi32(sum_squares, 0) + _mm256_extract_epi32(sum_squares, 1) +\n                                  _mm256_extract_epi32(sum_squares, 2) + _mm256_extract_epi32(sum_squares, 3) +\n                                  _mm256_extract_epi32(sum_squares, 4) + _mm256_extract_epi32(sum_squares, 5) +\n                                  _mm256_extract_epi32(sum_squares, 6) + _mm256_extract_epi32(sum_squares, 7);\n\n    int64_t sum_products_scalar = _mm256_extract_epi32(sum_products, 0) + _mm256_extract_epi32(sum_products, 1) +\n                                  _mm256_extract_epi32(sum_products, 2) + _mm256_extract_epi32(sum_products, 3) +\n                                  _mm256_extract_epi32(sum_products, 4) + _mm256_extract_epi32(sum_products, 5) +\n                                  _mm256_extract_epi32(sum_products, 6) + _mm256_extract_epi32(sum_products, 7);\n\n    return sum_squares_scalar - sum_products_scalar;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i ab_product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_squared, b_squared));\n        sum_products = _mm256_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t sum_squares_arr[8];\n    int32_t sum_products_arr[8];\n\n    _mm256_storeu_si256((__m256i*)sum_squares_arr, sum_squares);\n    _mm256_storeu_si256((__m256i*)sum_products_arr, sum_products);\n\n    int64_t sum_squares_total = 0;\n    int64_t sum_products_total = 0;\n\n    for (int i = 0; i < 8; ++i) {\n        sum_squares_total += sum_squares_arr[i];\n        sum_products_total += sum_products_arr[i];\n    }\n\n    return sum_squares_total - sum_products_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_squared, b_squared));\n        sum_products = _mm256_add_epi32(sum_products, product);\n    }\n\n    int64_t sum_squares_total = 0;\n    int64_t sum_products_total = 0;\n\n    for (int i = 0; i < 8; ++i) {\n        sum_squares_total += _mm256_extract_epi32(sum_squares, i);\n        sum_products_total += _mm256_extract_epi32(sum_products, i);\n    }\n\n    return sum_squares_total - sum_products_total;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i pos_mask = _mm256_and_si256(a, b);\n        __m256i neg_mask = _mm256_and_si256(_mm256_sign_epi8(a, a), _mm256_sign_epi8(b, b));\n\n        __m256i pos_result = _mm256_cmpgt_epi8(pos_mask, _mm256_setzero_si256());\n        __m256i neg_result = _mm256_cmpgt_epi8(neg_mask, _mm256_setzero_si256());\n\n        __m256i result = _mm256_blendv_epi8(_mm256_set1_epi8(0), _mm256_set1_epi8(1), pos_result);\n        result = _mm256_blendv_epi8(result, _mm256_set1_epi8(-1), neg_result);\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i pos_mask = _mm256_and_si256(a, b);\n        __m256i neg_mask = _mm256_andnot_si256(a, b);\n\n        __m256i pos_result = _mm256_cmpgt_epi8(pos_mask, _mm256_setzero_si256());\n        __m256i neg_result = _mm256_cmpgt_epi8(neg_mask, _mm256_setzero_si256());\n\n        __m256i result = _mm256_sub_epi8(pos_result, neg_result);\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i pos_mask = _mm256_set1_epi8(0x7F);\n        __m256i a_pos = _mm256_and_si256(a, pos_mask);\n        __m256i b_pos = _mm256_and_si256(b, pos_mask);\n\n        __m256i both_pos = _mm256_cmpeq_epi8(a_pos, b_pos);\n        __m256i both_neg = _mm256_cmpeq_epi8(a, b);\n\n        __m256i result = _mm256_and_si256(both_pos, _mm256_set1_epi8(1));\n        result = _mm256_or_si256(result, _mm256_and_si256(both_neg, _mm256_set1_epi8(-1)));\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i mask_positive = _mm256_set1_epi8(0x7F);\n    __m256i mask_negative = _mm256_set1_epi8(0x80);\n\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_positive = _mm256_and_si256(a, mask_positive);\n        __m256i b_positive = _mm256_and_si256(b, mask_positive);\n\n        __m256i a_negative = _mm256_and_si256(a, mask_negative);\n        __m256i b_negative = _mm256_and_si256(b, mask_negative);\n\n        __m256i both_positive = _mm256_and_si256(a_positive, b_positive);\n        __m256i both_negative = _mm256_and_si256(a_negative, b_negative);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(_mm256_cmpeq_epi8(both_positive, _mm256_setzero_si256()), _mm256_set1_epi8(1)),\n            _mm256_and_si256(_mm256_cmpeq_epi8(both_negative, _mm256_setzero_si256()), _mm256_set1_epi8(-1))\n        );\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_positive = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i b_positive = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n\n        __m256i a_negative = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n        __m256i b_negative = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b);\n\n        __m256i both_positive = _mm256_and_si256(a_positive, b_positive);\n        __m256i both_negative = _mm256_and_si256(a_negative, b_negative);\n\n        __m256i result = _mm256_blendv_epi8(\n            _mm256_set1_epi8(0),\n            _mm256_blendv_epi8(_mm256_set1_epi8(-1), _mm256_set1_epi8(1), both_positive),\n            _mm256_or_si256(both_positive, both_negative)\n        );\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i;\n\n    for (i = 0; i < midpoint; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        _mm256_storeu_si256((__m256i*)&C[i], xor_result);\n        _mm256_storeu_si256((__m256i*)&C[i + 8], and_result);\n    }\n\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n    }\n\n    for (i = midpoint + 1; i < length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        _mm256_storeu_si256((__m256i*)&C[i], xor_result);\n        _mm256_storeu_si256((__m256i*)&C[i + 8], and_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i;\n\n    for (i = 0; i < midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        _mm_storeu_si128((__m128i*)&C[i], xor_result);\n        _mm_storeu_si128((__m128i*)&C[i + 8], and_result);\n    }\n\n    for (; i < length; ++i) {\n        if (i == midpoint) {\n            C[i] = A[i];\n        } else if (i < midpoint) {\n            C[i] = A[i] ^ B[i];\n        } else {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint\n    for (; i + 8 <= midpoint; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n        __m256i result = _mm256_blendv_epi8(xor_result, and_result, _mm256_set1_epi16(0xFFFF));\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process the remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = (A[i] ^ B[i]) & (A[i] | B[i]);\n    }\n\n    // Copy the midpoint element from A to C\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n    }\n\n    // Process elements after the midpoint\n    for (i = midpoint + 1; i + 8 <= length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n        __m256i result = _mm256_blendv_epi8(xor_result, and_result, _mm256_set1_epi16(0xFFFF));\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process the remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = (A[i] ^ B[i]) & (A[i] | B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint\n    for (; i + 8 <= midpoint; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], xor_result);\n        _mm256_storeu_si256((__m256i*)&C[i + 4], and_result);\n    }\n\n    // Process the midpoint element\n    if (midpoint % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = B[midpoint];\n    }\n\n    // Process elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint\n    for (; i + 8 <= midpoint; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], xor_result);\n        _mm256_storeu_si256((__m256i*)&C[i + 8], and_result);\n    }\n\n    // Process the element exactly at the midpoint\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n    }\n\n    // Process elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256i c1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i c2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256i mask1 = _mm256_and_si256(c1, c2);\n        __m256i mask2 = _mm256_andnot_si256(c2, c1);\n        __m256i mask3 = _mm256_andnot_si256(c1, c2);\n        __m256i mask4 = _mm256_andnot_si256(c1, _mm256_andnot_si256(c2, _mm256_set1_epi32(-1)));\n\n        __m256 result = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(mask3));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(mask2));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(mask1));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256i mask1 = _mm256_and_si256(cond1_vec, cond2_vec);\n        __m256i mask2 = _mm256_andnot_si256(cond2_vec, cond1_vec);\n        __m256i mask3 = _mm256_andnot_si256(cond1_vec, cond2_vec);\n        __m256i mask4 = _mm256_andnot_si256(cond1_vec, _mm256_andnot_si256(cond2_vec, _mm256_set1_epi32(-1)));\n\n        __m256 result = _mm256_blendv_ps(_mm256_blendv_ps(b, c, _mm256_castsi256_ps(mask3)), d, _mm256_castsi256_ps(mask4));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(mask1));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(mask2));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256i c1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i c2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256i mask1 = _mm256_and_si256(c1, c2);\n        __m256i mask2 = _mm256_andnot_si256(c2, c1);\n        __m256i mask3 = _mm256_andnot_si256(c1, c2);\n        __m256i mask4 = _mm256_andnot_si256(c1, _mm256_andnot_si256(c2, _mm256_set1_epi32(-1)));\n\n        __m256 result = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(mask3));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(mask2));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(mask1));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256i c1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i c2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256i mask1 = _mm256_and_si256(c1, c2);\n        __m256i mask2 = _mm256_andnot_si256(c2, c1);\n        __m256i mask3 = _mm256_andnot_si256(c1, c2);\n        __m256i mask4 = _mm256_andnot_si256(c1, _mm256_andnot_si256(c2, _mm256_set1_epi32(-1)));\n\n        __m256 result = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(mask3));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(mask2));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(mask1));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256i mask1 = _mm256_and_si256(cond1_vec, cond2_vec);\n        __m256i mask2 = _mm256_andnot_si256(cond2_vec, cond1_vec);\n        __m256i mask3 = _mm256_andnot_si256(cond1_vec, cond2_vec);\n        __m256i mask4 = _mm256_andnot_si256(cond1_vec, _mm256_andnot_si256(cond2_vec, _mm256_set1_epi32(-1)));\n\n        __m256 result = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(mask3));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(mask2));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(mask1));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi8(count, _mm256_sad_epu8(vec, _mm256_setzero_si256()));\n        count = _mm256_slli_epi32(count, 16);\n        count = _mm256_srli_epi32(count, 24);\n\n        _mm256_storeu_si256((__m256i*)&out[i], count);\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n\n        __m256i sum = _mm256_sad_epu8(count, _mm256_setzero_si256());\n        _mm256_storeu_si256((__m256i*)&out[i], sum);\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi8(count, _mm256_and_si256(vecA, _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 1), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 2), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 3), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 4), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 5), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 6), _mm256_set1_epi32(0x55555555)));\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi32(vecA, 7), _mm256_set1_epi32(0x55555555)));\n\n        __m128i low = _mm256_extracti128_si256(count, 0);\n        __m128i high = _mm256_extracti128_si256(count, 1);\n\n        low = _mm_add_epi8(low, high);\n        low = _mm_add_epi8(low, _mm_srli_si128(low, 1));\n        low = _mm_add_epi8(low, _mm_srli_si128(low, 2));\n        low = _mm_add_epi8(low, _mm_srli_si128(low, 4));\n        low = _mm_add_epi8(low, _mm_srli_si128(low, 8));\n\n        uint8_t result[16];\n        _mm_storeu_si128((__m128i*)result, low);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = result[j];\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi8(count, _mm256_sad_epu8(vec, _mm256_setzero_si256()));\n        count = _mm256_srli_epi16(count, 8);\n\n        __m128i low = _mm256_castsi256_si128(count);\n        __m128i high = _mm256_extracti128_si256(count, 1);\n\n        low = _mm_add_epi8(low, high);\n        low = _mm_sad_epu8(low, _mm_setzero_si128());\n        low = _mm_srli_epi16(low, 8);\n\n        out[i] = (uint8_t)_mm_cvtsi128_si32(low);\n        out[i + 1] = (uint8_t)_mm_extract_epi16(low, 1);\n        out[i + 2] = (uint8_t)_mm_extract_epi16(low, 2);\n        out[i + 3] = (uint8_t)_mm_extract_epi16(low, 3);\n        out[i + 4] = (uint8_t)_mm_extract_epi16(low, 4);\n        out[i + 5] = (uint8_t)_mm_extract_epi16(low, 5);\n        out[i + 6] = (uint8_t)_mm_extract_epi16(low, 6);\n        out[i + 7] = (uint8_t)_mm_extract_epi16(low, 7);\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n        vec = _mm256_srli_epi32(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, _mm256_set1_epi32(0x01010101)));\n\n        __m128i lo = _mm256_castsi256_si128(count);\n        __m128i hi = _mm256_extracti128_si256(count, 1);\n        lo = _mm_add_epi8(lo, hi);\n        lo = _mm_sad_epu8(lo, _mm_setzero_si128());\n\n        _mm_storeu_si32((__m128i*)&out[i], lo);\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i;\n\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i rem = _mm256_rem_epi32(a, three);\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i;\n\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i rem = _mm256_rem_epi32(a, three);\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i remainder = _mm256_div_epi32(vecA, three);\n        __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i absA = _mm256_abs_epi32(vecA);\n        __m256i result = _mm256_blendv_epi8(vecA, absA, mask);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i;\n\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i rem = _mm256_rem_epi32(a, three);\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i rem = _mm256_rem_epi32(a, three);\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i rotate_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask = _mm256_set1_epi32(0xF);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, mask);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(src_vec, rotate_vec),\n            _mm256_sllv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_vec))\n        );\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i rotate_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask = _mm256_set1_epi32(0xF);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, mask);\n\n        __m256i rotated = _mm256_rolv_epi32(src_vec, rotate_vec);\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = _rotl(src[i], rotate_bits);\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    __m256i low_4_bits_mask = _mm256_set1_epi32(0xF);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, low_4_bits_mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_4_bits, low_4_bits_mask);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(src_vec, rotate_mask),\n            _mm256_sllv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask))\n        );\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    __m256i low_4_bits_mask = _mm256_set1_epi32(0xF);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, low_4_bits_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, low_4_bits_mask);\n        __m256i rotated = _mm256_or_si256(_mm256_srlv_epi32(src_vec, rotate_mask), _mm256_sllv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask)));\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n        __m256i result = _mm256_blendv_epi8(negated, rotated, all_ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    __m256i low_4_bits_mask = _mm256_set1_epi32(0xF);\n\n    for (i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, low_4_bits_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, low_4_bits_mask);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(src_vec, rotate_mask),\n            _mm256_sllv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask))\n        );\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_andnot_ps(cmp_gt, _mm256_andnot_ps(cmp_lt, _mm256_set1_ps(-0.0f)));\n\n        __m256i result = _mm256_cvtps_epi32(_mm256_or_ps(_mm256_and_ps(cmp_gt, _mm256_set1_ps(1.0f)),\n                                                        _mm256_and_ps(cmp_lt, _mm256_set1_ps(-1.0f))));\n        result = _mm256_packs_epi32(result, result);\n        result = _mm256_packs_epi16(result, result);\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256i cmp_gt = _mm256_castps_si256(_mm256_cmp_ps(diff, eps, _CMP_GT_OQ));\n        __m256i cmp_lt = _mm256_castps_si256(_mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ));\n\n        __m256i result = _mm256_blendv_epi8(zero, one, cmp_gt);\n        result = _mm256_blendv_epi8(result, neg_one, cmp_lt);\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256i result = _mm256_castps_si256(cmp_gt);\n        result = _mm256_or_si256(result, _mm256_and_si256(_mm256_castps_si256(cmp_lt), neg_one));\n        result = _mm256_and_si256(result, _mm256_or_si256(_mm256_castps_si256(cmp_gt), _mm256_castps_si256(cmp_lt)));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps = _mm256_set1_ps(fabsf(epsilon));\n    __m256 neg_eps = _mm256_set1_ps(-fabsf(epsilon));\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_andnot_ps(cmp_gt, _mm256_andnot_ps(cmp_lt, _mm256_set1_ps(-0.0f)));\n\n        __m256i result = _mm256_castps_si256(_mm256_or_ps(_mm256_and_ps(cmp_gt, _mm256_set1_ps(1.0f)),\n                                                          _mm256_and_ps(cmp_lt, _mm256_set1_ps(-1.0f))));\n        result = _mm256_or_si256(result, _mm256_castps_si256(cmp_eq));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_andnot_ps(cmp_gt, _mm256_andnot_ps(cmp_lt, _mm256_set1_ps(-0.0f)));\n\n        __m256i result = _mm256_castps_si256(_mm256_blendv_ps(_mm256_set1_ps(-1.0f), _mm256_set1_ps(1.0f), cmp_gt));\n        result = _mm256_castps_si256(_mm256_blendv_ps(_mm256_castsi256_ps(result), _mm256_setzero_ps(), cmp_eq));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 float_vec = _mm256_loadu_ps(&A[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(float_vec);\n\n        __m256i max_mask = _mm256_cmpgt_epi32(int_vec, int_max);\n        __m256i min_mask = _mm256_cmpgt_epi32(int_min, int_vec);\n\n        int_vec = _mm256_blendv_epi8(int_vec, int_max, max_mask);\n        int_vec = _mm256_blendv_epi8(int_vec, int_min, min_mask);\n\n        _mm256_storeu_si256((__m256i*)&B[i], int_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v = _mm256_loadu_ps(&A[i]);\n\n        __m256i vi = _mm256_cvtps_epi32(v);\n\n        __m256 v_gt_max = _mm256_cmp_ps(v, float_max, _CMP_GT_OQ);\n        __m256 v_lt_min = _mm256_cmp_ps(v, float_min, _CMP_LT_OQ);\n\n        __m256i v_gt_max_i = _mm256_and_si256(_mm256_castps_si256(v_gt_max), int32_max);\n        __m256i v_lt_min_i = _mm256_and_si256(_mm256_castps_si256(v_lt_min), int32_min);\n\n        __m256i v_clamped = _mm256_blendv_epi8(vi, v_gt_max_i, _mm256_castps_si256(v_gt_max));\n        v_clamped = _mm256_blendv_epi8(v_clamped, v_lt_min_i, _mm256_castps_si256(v_lt_min));\n\n        _mm256_storeu_si256((__m256i*)&B[i], v_clamped);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 float_vec = _mm256_loadu_ps(&A[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(float_vec);\n\n        __m256i cmp_max = _mm256_cmpgt_epi32(int_vec, int_max);\n        __m256i cmp_min = _mm256_cmpgt_epi32(int_min, int_vec);\n\n        int_vec = _mm256_blendv_epi8(int_vec, int_max, cmp_max);\n        int_vec = _mm256_blendv_epi8(int_vec, int_min, cmp_min);\n\n        _mm256_storeu_si256((__m256i*)&B[i], int_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 float_vec = _mm256_loadu_ps(&A[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(float_vec);\n\n        __m256i max_mask = _mm256_cmpgt_epi32(int_vec, int_max);\n        __m256i min_mask = _mm256_cmpgt_epi32(int_min, int_vec);\n\n        int_vec = _mm256_blendv_epi8(int_vec, int_max, max_mask);\n        int_vec = _mm256_blendv_epi8(int_vec, int_min, min_mask);\n\n        _mm256_storeu_si256((__m256i*)&B[i], int_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 float_vec = _mm256_loadu_ps(&A[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(float_vec);\n\n        __m256i max_mask = _mm256_cmpgt_epi32(int_vec, int_max);\n        __m256i min_mask = _mm256_cmpgt_epi32(int_min, int_vec);\n\n        int_vec = _mm256_blendv_epi8(int_vec, int_max, max_mask);\n        int_vec = _mm256_blendv_epi8(int_vec, int_min, min_mask);\n\n        _mm256_storeu_si256((__m256i*)&B[i], int_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256 scale_float_vec = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 a_float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 result_vec = _mm256_mul_ps(a_float_vec, scale_float_vec);\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256 scale_float_vec = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 a_float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 result_vec = _mm256_mul_ps(a_float_vec, scale_float_vec);\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256 scale_float_vec = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_float_vec);\n        _mm256_storeu_ps(&B[i], scaled_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256 scale_float_vec = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 a_float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 result_vec = _mm256_mul_ps(a_float_vec, scale_float_vec);\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256 scale_float_vec = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 a_float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 result_vec = _mm256_mul_ps(a_float_vec, scale_float_vec);\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256i rounded_result = _mm256_cvtps_epi32(div_result);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256i rounded_result = _mm256_cvtps_epi32(div_result);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256i rounded_result = _mm256_cvtps_epi32(div_result);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256i rounded_result = _mm256_cvtps_epi32(div_result);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&A[i]);\n        __m256d clamped_vec = _mm256_max_pd(_mm256_min_pd(vec, max_val), min_val);\n        __m128 low = _mm256_cvtpd_ps(clamped_vec);\n        _mm_storeu_ps(&B[i], low);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(a, min_val), max_val);\n        __m128 b = _mm256_cvtpd_ps(clamped);\n        _mm_storeu_ps(&B[i], b);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n    const __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d cmp_max = _mm256_cmp_pd(a, max_val, _CMP_GT_OQ);\n        __m256d cmp_min = _mm256_cmp_pd(a, min_val, _CMP_LT_OQ);\n\n        __m256d saturated = _mm256_blendv_pd(a, max_val, cmp_max);\n        saturated = _mm256_blendv_pd(saturated, min_val, cmp_min);\n\n        __m128 low = _mm256_cvtpd_ps(_mm256_unpacklo_pd(saturated, zero));\n        __m128 high = _mm256_cvtpd_ps(_mm256_unpackhi_pd(saturated, zero));\n\n        _mm_storeu_ps(&B[i], low);\n        _mm_storeu_ps(&B[i + 2], high);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(a, min_val), max_val);\n        __m128 b = _mm256_cvtpd_ps(clamped);\n        _mm_storeu_ps(&B[i], b);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&A[i]);\n        __m256d clamped_vec = _mm256_max_pd(_mm256_min_pd(vec, max_val), min_val);\n        __m128 vec_low = _mm256_cvtpd_ps(_mm256_castpd256_pd128(clamped_vec));\n        __m128 vec_high = _mm256_cvtpd_ps(_mm256_extractf128_pd(clamped_vec, 1));\n        _mm_storeu_ps(&B[i], vec_low);\n        _mm_storeu_ps(&B[i + 4], vec_high);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_set1_ps(0.0f);\n    __m256 v_one = _mm256_set1_ps(1.0f);\n    __m256 v_range = _mm256_sub_ps(v_max_val, v_min_val);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v_A = _mm256_loadu_ps(&A[i]);\n        __m256i v_control = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 v_normalized = _mm256_div_ps(_mm256_sub_ps(v_A, v_min_val), v_range);\n        v_normalized = _mm256_max_ps(v_normalized, v_zero);\n        v_normalized = _mm256_min_ps(v_normalized, v_one);\n\n        __m256i v_control_mask = _mm256_cmpgt_epi32(v_control, _mm256_setzero_si256());\n        __m256 v_result = _mm256_blendv_ps(v_A, v_normalized, _mm256_castsi256_ps(v_control_mask));\n\n        _mm256_storeu_ps(&B[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_set1_ps(0.0f);\n    __m256 v_one = _mm256_set1_ps(1.0f);\n    __m256 v_range = _mm256_sub_ps(v_max_val, v_min_val);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v_A = _mm256_loadu_ps(&A[i]);\n        __m256i v_control = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 v_normalized = _mm256_div_ps(_mm256_sub_ps(v_A, v_min_val), v_range);\n        v_normalized = _mm256_max_ps(v_normalized, v_zero);\n        v_normalized = _mm256_min_ps(v_normalized, v_one);\n\n        __m256i v_mask = _mm256_cmpgt_epi32(v_control, _mm256_setzero_si256());\n        __m256 v_result = _mm256_blendv_ps(v_A, v_normalized, _mm256_castsi256_ps(v_mask));\n\n        _mm256_storeu_ps(&B[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_set1_ps(0.0f);\n    __m256 v_one = _mm256_set1_ps(1.0f);\n    __m256 v_range = _mm256_sub_ps(v_max_val, v_min_val);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v_A = _mm256_loadu_ps(&A[i]);\n        __m256i v_control = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 v_normalized = _mm256_div_ps(_mm256_sub_ps(v_A, v_min_val), v_range);\n        v_normalized = _mm256_max_ps(v_normalized, v_zero);\n        v_normalized = _mm256_min_ps(v_normalized, v_one);\n\n        __m256i v_control_mask = _mm256_cmpgt_epi32(v_control, _mm256_setzero_si256());\n        __m256 v_result = _mm256_blendv_ps(v_A, v_normalized, _mm256_castsi256_ps(v_control_mask));\n\n        _mm256_storeu_ps(&B[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_one = _mm256_set1_ps(1.0f);\n    __m256 v_zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v_A = _mm256_loadu_ps(&A[i]);\n        __m256i v_control = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 v_normalized = _mm256_div_ps(_mm256_sub_ps(v_A, v_min_val), _mm256_sub_ps(v_max_val, v_min_val));\n        v_normalized = _mm256_max_ps(v_normalized, v_zero);\n        v_normalized = _mm256_min_ps(v_normalized, v_one);\n\n        __m256i v_control_mask = _mm256_cmpgt_epi32(v_control, _mm256_setzero_si256());\n        __m256 v_result = _mm256_blendv_ps(v_A, v_normalized, _mm256_castsi256_ps(v_control_mask));\n\n        _mm256_storeu_ps(&B[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_val_vec = _mm256_set1_ps(min_val);\n    const __m256 max_val_vec = _mm256_set1_ps(max_val);\n    const __m256 zero_vec = _mm256_set1_ps(0.0f);\n    const __m256 one_vec = _mm256_set1_ps(1.0f);\n    const __m256i zero_vec_i = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256i mask = _mm256_cmpgt_epi32(control_vec, zero_vec_i);\n        __m256 mask_f = _mm256_castsi256_ps(mask);\n\n        __m256 diff = _mm256_sub_ps(a_vec, min_val_vec);\n        __m256 range = _mm256_sub_ps(max_val_vec, min_val_vec);\n        __m256 normalized = _mm256_div_ps(diff, range);\n\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask_f);\n\n        _mm256_storeu_ps(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i < size; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high16 = _mm256_srli_epi32(vecA, 16);\n        __m256 vecB = _mm256_cvtepi32_ps(high16);\n        __m256 sqrtB = _mm256_sqrt_ps(vecB);\n        _mm256_storeu_ps(&B[i], sqrtB);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high_bits = _mm256_srli_epi32(a, 16);\n        __m256 b = _mm256_cvtepi32_ps(high_bits);\n        __m256 sqrt_b = _mm256_sqrt_ps(b);\n        _mm256_storeu_ps(&B[i], sqrt_b);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high_bits = _mm256_srli_epi32(a, 16);\n        __m256 b = _mm256_cvtepi32_ps(high_bits);\n        __m256 sqrt_b = _mm256_sqrt_ps(b);\n        _mm256_storeu_ps(&B[i], sqrt_b);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high_bits = _mm256_srli_epi32(a, 16);\n        __m256 b = _mm256_cvtepi32_ps(high_bits);\n        __m256 sqrt_b = _mm256_sqrt_ps(b);\n        _mm256_storeu_ps(&B[i], sqrt_b);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i < size; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high16 = _mm256_srli_epi32(vecA, 16);\n        __m256 vecF = _mm256_cvtepi32_ps(high16);\n        __m256 sqrtF = _mm256_sqrt_ps(vecF);\n        _mm256_storeu_ps(&B[i], sqrtF);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        _mm256_storeu_ps(&B[i], _mm256_castsi256_ps(vec));\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 b = _mm256_castsi256_ps(a);\n        _mm256_storeu_ps(&B[i], b);\n    }\n    for (; i < size; ++i) {\n        B[i] = _mm_cvtsi32_ss(B[i], A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vecB = _mm256_castsi256_ps(vecA);\n        _mm256_storeu_ps(&B[i], vecB);\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256 zero_f = _mm256_set1_ps(0.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n        cond_mask = _mm256_and_si256(cond_mask, mask);\n\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_int);\n        __m256 result = _mm256_blendv_ps(zero_f, diff_f, _mm256_castsi256_ps(cond_mask));\n\n        _mm256_storeu_ps(&diff[i], result);\n        prev = curr;\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256i zero = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_set1_ps(0.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n\n        __m256i diff_int = _mm256_sub_epi32(next, prev);\n        __m256i cond_vec = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        __m256i mask = _mm256_cmpeq_epi32(cond_vec, _mm256_set1_epi32(1));\n        __m256i masked_diff = _mm256_blendv_epi8(zero, diff_int, mask);\n\n        __m256 diff_float = _mm256_cvtepi32_ps(masked_diff);\n        _mm256_storeu_ps(&diff[i], diff_float);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256i zero = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_set1_ps(0.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n        __m256i cond_vec = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        __m256i diff_int = _mm256_sub_epi32(next, prev);\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(cond_vec, zero));\n        __m256 result = _mm256_blendv_ps(zero_f, diff_float, mask);\n\n        _mm256_storeu_ps(&diff[i], result);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256i zero = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_set1_ps(0.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n\n        __m256i diff_int = _mm256_sub_epi32(next, prev);\n        __m256i cond_vec = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        __m256i mask = _mm256_cmpeq_epi32(cond_vec, _mm256_set1_epi32(1));\n        __m256i masked_diff = _mm256_and_si256(diff_int, mask);\n\n        __m256 diff_f = _mm256_cvtepi32_ps(masked_diff);\n        __m256 masked_diff_f = _mm256_blendv_ps(zero_f, diff_f, _mm256_castsi256_ps(mask));\n\n        _mm256_storeu_ps(&diff[i], masked_diff_f);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256i zero = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_set1_ps(0.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n        __m256i cond_vec = _mm256_loadu_si256((__m256i*)&cond[i]);\n        __m256i mask = _mm256_cmpeq_epi32(cond_vec, _mm256_set1_epi32(-1));\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_int);\n        __m256 result = _mm256_blendv_ps(zero_f, diff_f, _mm256_castsi256_ps(mask));\n        _mm256_storeu_ps(&diff[i], result);\n        prev = curr;\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i high_bits = _mm256_slli_epi64(src_vec, 32);\n        __m256i result = _mm256_or_si256(high_bits, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i] << 32) | 0xFFFFFFFF;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i high_bits = _mm256_slli_epi64(src_vec, 32);\n        __m256i result = _mm256_or_si256(high_bits, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i high_bits = _mm256_slli_epi64(src_vec, 32);\n        __m256i result = _mm256_or_si256(high_bits, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i] << 32) | 0xFFFFFFFF;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i high_bits = _mm256_slli_epi64(src_vec, 32);\n        __m256i result = _mm256_or_si256(high_bits, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i high_bits = _mm256_slli_epi64(src_vec, 32);\n        __m256i result = _mm256_or_si256(high_bits, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i] << 32) | 0xFFFFFFFF;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i indices = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d values = _mm256_setzero_pd();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi16(indices, j);\n            if (idx >= 0 && idx < length) {\n                values = _mm256_add_pd(values, _mm256_set1_pd(vec[idx]));\n            }\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        int idx = index[i];\n        if (idx >= 0 && idx < length) {\n            total_sum += vec[idx];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c++\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256d values = _mm256_setzero_pd();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm_extract_epi16(indices, j);\n            if (idx >= 0 && idx < length) {\n                values = _mm256_add_pd(values, _mm256_set1_pd(vec[idx]));\n            }\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        int idx = index[i];\n        if (idx >= 0 && idx < length) {\n            total_sum += vec[idx];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c++\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m256i indices = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d values = _mm256_setzero_pd();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi16(indices, j);\n            if (idx >= 0 && idx < length) {\n                values = _mm256_insertf128_pd(values, _mm_load_sd(&vec[idx]), j / 2);\n            }\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        int idx = index[i];\n        if (idx >= 0 && idx < length) {\n            total_sum += vec[idx];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i indices = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d values = _mm256_setzero_pd();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi16(indices, j);\n            if (idx >= 0 && idx < length) {\n                values = _mm256_insertf128_pd(values, _mm_load_sd(&vec[idx]), j / 2);\n            }\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        int idx = index[i];\n        if (idx >= 0 && idx < length) {\n            total_sum += vec[idx];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i indices = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d values = _mm256_setzero_pd();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi16(indices, j);\n            if (idx >= 0 && idx < length) {\n                values = _mm256_insert_pd(values, vec[idx], j);\n            }\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        int idx = index[i];\n        if (idx >= 0 && idx < length) {\n            total_sum += vec[idx];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_index = 0;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_index = (i + ki) * input_size + (j + kj);\n                    size_t kernel_index = ki * kernel_size + kj;\n\n                    __m256d in_val = _mm256_set1_pd(input[input_index]);\n                    __m256d ker_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(in_val, ker_val));\n                }\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n            output[output_index++] = result > 0 ? result : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_index = 0;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_index = (i + ki) * input_size + (j + kj);\n                    size_t kernel_index = ki * kernel_size + kj;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            output[output_index++] = result > 0 ? result : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_index = 0;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_index = (i + ki) * input_size + (j + kj);\n                    size_t kernel_index = ki * kernel_size + kj;\n\n                    __m256d in_val = _mm256_set1_pd(input[input_index]);\n                    __m256d ker_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(in_val, ker_val));\n                }\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            output[output_index++] = result > 0 ? result : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t kernel_center = kernel_size / 2;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_row = i + ki;\n                    size_t input_col = j + kj;\n                    size_t input_index = input_row * input_size + input_col;\n                    size_t kernel_index = ki * kernel_size + kj;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m256d zero = _mm256_setzero_pd();\n            sum = _mm256_max_pd(sum, zero);\n            _mm256_storeu_pd(&output[i * output_size + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t kernel_center = kernel_size / 2;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_index = (i + ki) * input_size + (j + kj);\n                    size_t kernel_index = ki * kernel_size + kj;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_fmadd_pd(input_val, kernel_val, sum);\n                }\n            }\n\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            double final_sum = result[0] + result[1] + result[2] + result[3];\n\n            output[i * output_size + j] = final_sum > 0 ? final_sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int size = numbers.size();\n    const int vec_size = 4; // AVX operates on 256-bit registers, which is 4 doubles\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < size; ++i) {\n        __m256d vec1 = _mm256_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < size; j += vec_size) {\n            __m256d vec2 = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int size = numbers.size();\n    const int step = 4; // AVX processes 4 doubles at a time\n\n    __m256d thresh = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < size; i += step) {\n        __m256d a = _mm256_loadu_pd(&numbers[i]);\n\n        for (int j = i + step; j < size; j += step) {\n            __m256d b = _mm256_loadu_pd(&numbers[j]);\n\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int size = numbers.size();\n    const int vec_size = 4; // AVX operates on 256-bit registers, which hold 4 doubles\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < size; ++i) {\n        __m256d vec1 = _mm256_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < size; j += vec_size) {\n            __m256d vec2 = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int size = numbers.size();\n    const int step = 4; // AVX operates on 4 doubles at a time\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < size; i += step) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n\n        for (int j = i + step; j < size; j += step) {\n            __m256d vec2 = _mm256_loadu_pd(&numbers[j]);\n\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int size = numbers.size();\n    const int step = 4; // AVX processes 4 doubles at a time\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < size; i += step) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n        for (int j = i + step; j < size; j += step) {\n            __m256d vec2 = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m256i balance = _mm256_setzero_si256();\n    int size = operations.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    int current_balance = 0;\n    for (; i < size; ++i) {\n        current_balance += operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\nbool below_zero_simd(std::vector<int> operations) {\n    __m256i balance = _mm256_setzero_si256();\n    int size = operations.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    int scalar_balance = 0;\n    for (; i < size; ++i) {\n        scalar_balance += operations[i];\n        if (scalar_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int balance = 0;\n    __m256i balance_vec = _mm256_set1_epi32(balance);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < operations.size(); i += 8) {\n        __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n        if (_mm256_movemask_epi8(below_zero_mask) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\nbool below_zero_simd(std::vector<int> operations) {\n    __m256i balance = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < operations.size(); i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int balance = 0;\n    __m256i balance_vec = _mm256_set1_epi32(balance);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < operations.size(); i += 8) {\n        __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n        __m256i mask = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n        if (_mm256_testz_si256(mask, mask) == 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum = _mm256_setzero_pd();\n    __m256d count = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, v);\n        count = _mm256_add_pd(count, _mm256_set1_pd(1.0));\n    }\n\n    double total_sum[4];\n    _mm256_storeu_pd(total_sum, sum);\n    double total_count[4];\n    _mm256_storeu_pd(total_count, count);\n\n    double mean = (total_sum[0] + total_sum[1] + total_sum[2] + total_sum[3]) /\n                  (total_count[0] + total_count[1] + total_count[2] + total_count[3]);\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_total[4];\n    _mm256_storeu_pd(mad_total, mad_sum);\n\n    double mad = (mad_total[0] + mad_total[1] + mad_total[2] + mad_total[3]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum = _mm256_setzero_pd();\n    __m256d count = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, v);\n        count = _mm256_add_pd(count, _mm256_set1_pd(1.0));\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    double total_count = _mm256_reduce_add_pd(count);\n    double mean = total_sum / total_count;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_diff_sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n        abs_diff_sum = _mm256_add_pd(abs_diff_sum, abs_diff);\n    }\n\n    double total_abs_diff = _mm256_reduce_add_pd(abs_diff_sum);\n    return total_abs_diff / total_count;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, v);\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    double mean = total_sum / n;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum = _mm256_setzero_pd();\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum = _mm256_reduce_add_pd(mad_sum);\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum = _mm256_setzero_pd();\n    __m256d count = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, v);\n        count = _mm256_add_pd(count, _mm256_set1_pd(1.0));\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    double total_count = _mm256_reduce_add_pd(count);\n    double mean = total_sum / total_count;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_diff_sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n        abs_diff_sum = _mm256_add_pd(abs_diff_sum, abs_diff);\n    }\n\n    double total_abs_diff = _mm256_reduce_add_pd(abs_diff_sum);\n    return total_abs_diff / total_count;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    __m256d sum = _mm256_setzero_pd();\n    __m256d abs_diff_sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double total_sum[4];\n    _mm256_storeu_pd(total_sum, sum);\n    double mean = (total_sum[0] + total_sum[1] + total_sum[2] + total_sum[3]) / n;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n        abs_diff_sum = _mm256_add_pd(abs_diff_sum, abs_diff);\n    }\n\n    double total_abs_diff[4];\n    _mm256_storeu_pd(total_abs_diff, abs_diff_sum);\n    double mad = (total_abs_diff[0] + total_abs_diff[1] + total_abs_diff[2] + total_abs_diff[3]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) return {};\n\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&numbers[i + 4]);\n\n        __m256i interleaved1 = _mm256_unpacklo_epi32(vec1, delim_vec);\n        __m256i interleaved2 = _mm256_unpackhi_epi32(vec1, delim_vec);\n        __m256i interleaved3 = _mm256_unpacklo_epi32(vec2, delim_vec);\n        __m256i interleaved4 = _mm256_unpackhi_epi32(vec2, delim_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved1);\n        result.push_back(_mm256_extract_epi32(interleaved2, 0));\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved3);\n        result.push_back(_mm256_extract_epi32(interleaved4, 0));\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) return {};\n\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&numbers[i + 4]);\n\n        __m256i interleaved1 = _mm256_unpacklo_epi32(vec1, delim_vec);\n        __m256i interleaved2 = _mm256_unpackhi_epi32(vec1, delim_vec);\n        __m256i interleaved3 = _mm256_unpacklo_epi32(vec2, delim_vec);\n        __m256i interleaved4 = _mm256_unpackhi_epi32(vec2, delim_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved1);\n        result.push_back(_mm256_extract_epi32(interleaved2, 0));\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved3);\n        result.push_back(_mm256_extract_epi32(interleaved4, 0));\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) return {};\n\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&numbers[i + 4]);\n\n        __m256i interleaved1 = _mm256_unpacklo_epi32(vec1, delim_vec);\n        __m256i interleaved2 = _mm256_unpackhi_epi32(vec1, delim_vec);\n        __m256i interleaved3 = _mm256_unpacklo_epi32(vec2, delim_vec);\n        __m256i interleaved4 = _mm256_unpackhi_epi32(vec2, delim_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved1);\n        result.push_back(_mm256_extract_epi32(interleaved2, 0));\n        _mm256_storeu_si256((__m256i*)&result[result.size()], interleaved3);\n        result.push_back(_mm256_extract_epi32(interleaved4, 0));\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) return {};\n\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i + 4]));\n\n        __m256i interleaved1 = _mm256_unpacklo_epi32(vec1, delim_vec);\n        __m256i interleaved2 = _mm256_unpackhi_epi32(vec1, delim_vec);\n        __m256i interleaved3 = _mm256_unpacklo_epi32(vec2, delim_vec);\n        __m256i interleaved4 = _mm256_unpackhi_epi32(vec2, delim_vec);\n\n        __m256i result1 = _mm256_permute2x128_si256(interleaved1, interleaved2, 0x20);\n        __m256i result2 = _mm256_permute2x128_si256(interleaved3, interleaved4, 0x20);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[result.size()]), result1);\n        result.resize(result.size() + 8);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[result.size()]), result2);\n        result.resize(result.size() + 8);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) return {};\n\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i vec2 = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7));\n        __m256i vec3 = _mm256_blend_epi32(vec2, delim_vec, 0xAA);\n\n        _mm256_storeu_si256((__m256i*)&result[result.size()], vec3);\n        result.resize(result.size() + 8);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    size_t size = numbers.size();\n    size_t i = 0;\n\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi64x(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&numbers[i + 8]);\n\n        sum_vec = _mm256_add_epi16(sum_vec, a);\n        sum_vec = _mm256_add_epi16(sum_vec, b);\n\n        product_vec = _mm256_mullo_epi16(product_vec, a);\n        product_vec = _mm256_mullo_epi16(product_vec, b);\n    }\n\n    alignas(32) int16_t sum_array[16];\n    alignas(32) int16_t product_array[16];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    _mm256_store_si256((__m256i*)product_array, product_vec);\n\n    for (int j = 0; j < 16; ++j) {\n        sum += sum_array[j];\n        product *= product_array[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    size_t size = numbers.size();\n    size_t i = 0;\n\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi64x(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&numbers[i + 8]);\n\n        sum_vec = _mm256_add_epi16(sum_vec, a);\n        sum_vec = _mm256_add_epi16(sum_vec, b);\n\n        product_vec = _mm256_mullo_epi16(product_vec, a);\n        product_vec = _mm256_mullo_epi16(product_vec, b);\n    }\n\n    alignas(32) int16_t sum_array[16];\n    alignas(32) int16_t product_array[16];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    _mm256_store_si256((__m256i*)product_array, product_vec);\n\n    for (int j = 0; j < 16; ++j) {\n        sum += sum_array[j];\n        product *= product_array[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    size_t size = numbers.size();\n\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < size; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&numbers[i + 8]);\n\n        sum_vec = _mm256_add_epi16(sum_vec, a);\n        sum_vec = _mm256_add_epi16(sum_vec, b);\n\n        product_vec = _mm256_mullo_epi16(product_vec, a);\n        product_vec = _mm256_mullo_epi16(product_vec, b);\n    }\n\n    alignas(32) int16_t sum_array[16];\n    alignas(32) int16_t product_array[16];\n\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    _mm256_store_si256((__m256i*)product_array, product_vec);\n\n    for (int i = 0; i < 16; ++i) {\n        sum += sum_array[i];\n        product *= product_array[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    size_t size = numbers.size();\n\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < size; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum_vec = _mm256_add_epi16(sum_vec, vec);\n        prod_vec = _mm256_mullo_epi16(prod_vec, vec);\n    }\n\n    int16_t sum_arr[16];\n    int16_t prod_arr[16];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    _mm256_storeu_si256((__m256i*)prod_arr, prod_vec);\n\n    for (int i = 0; i < 16; ++i) {\n        sum += sum_arr[i];\n        product *= prod_arr[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    size_t size = numbers.size();\n    size_t i = 0;\n\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi32(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&numbers[i + 8]);\n\n        sum_vec = _mm256_add_epi16(sum_vec, a);\n        sum_vec = _mm256_add_epi16(sum_vec, b);\n\n        prod_vec = _mm256_mullo_epi16(prod_vec, a);\n        prod_vec = _mm256_mullo_epi16(prod_vec, b);\n    }\n\n    alignas(32) int16_t sum_array[16];\n    alignas(32) int16_t prod_array[16];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    _mm256_store_si256((__m256i*)prod_array, prod_vec);\n\n    for (int j = 0; j < 16; ++j) {\n        sum += sum_array[j];\n        product *= prod_array[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    if (numbers.empty()) return result;\n\n    __m256i max_val = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        __m256i current_val = _mm256_set1_epi32(numbers[i]);\n        max_val = _mm256_max_epi32(max_val, current_val);\n        result[i] = _mm256_extract_epi32(max_val, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int size = numbers.size();\n    if (size == 0) return {};\n\n    std::vector<int> result(size);\n    __m256i max_val = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_val = _mm256_max_epi32(max_val, current);\n        _mm256_storeu_si256((__m256i*)&result[i], max_val);\n    }\n\n    int max_scalar = result[size - 1];\n    for (int i = size - 1; i >= 0; --i) {\n        max_scalar = std::max(max_scalar, numbers[i]);\n        result[i] = max_scalar;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m256i max_val = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_val = _mm256_max_epi32(max_val, current);\n\n        for (int j = 0; j < 8 && i + j < n; ++j) {\n            result[i + j] = _mm256_extract_epi32(max_val, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    if (numbers.empty()) return result;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    for (size_t i = 0; i < len / 32; ++i) {\n        __m256i va = _mm256_loadu_si256(pa + i);\n        __m256i vb = _mm256_loadu_si256(pb + i);\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr + i, vx);\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    for (size_t i = 0; i < len / 32; ++i) {\n        __m256i va = _mm256_loadu_si256(pa + i);\n        __m256i vb = _mm256_loadu_si256(pb + i);\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr + i, vx);\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    for (size_t i = 0; i < len / 32; ++i) {\n        __m256i va = _mm256_loadu_si256(pa + i);\n        __m256i vb = _mm256_loadu_si256(pb + i);\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr + i, vx);\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    for (size_t i = 0; i < len / 32; ++i) {\n        __m256i va = _mm256_loadu_si256(pa + i);\n        __m256i vb = _mm256_loadu_si256(pb + i);\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr + i, vx);\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_length = 0;\n    std::string longest_string = strings[0];\n\n    const size_t num_strings = strings.size();\n    const size_t num_chunks = num_strings / 4;\n    const size_t remainder = num_strings % 4;\n\n    for (size_t i = 0; i < num_chunks; ++i) {\n        __m256i lengths = _mm256_set_epi32(\n            strings[i * 4 + 3].size(),\n            strings[i * 4 + 2].size(),\n            strings[i * 4 + 1].size(),\n            strings[i * 4].size(),\n            strings[i * 4 + 3].size(),\n            strings[i * 4 + 2].size(),\n            strings[i * 4 + 1].size(),\n            strings[i * 4].size()\n        );\n\n        __m256i max_lengths = _mm256_set1_epi32(max_length);\n        __m256i mask = _mm256_cmpgt_epi32(lengths, max_lengths);\n\n        if (_mm256_movemask_epi8(mask) != 0) {\n            for (size_t j = 0; j < 4; ++j) {\n                size_t current_length = strings[i * 4 + j].size();\n                if (current_length > max_length) {\n                    max_length = current_length;\n                    longest_string = strings[i * 4 + j];\n                }\n            }\n        }\n    }\n\n    for (size_t i = num_chunks * 4; i < num_strings; ++i) {\n        size_t current_length = strings[i].size();\n        if (current_length > max_length) {\n            max_length = current_length;\n            longest_string = strings[i];\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    __m256i max_lengths = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        for (int j = 0; j < 8 && i + j < strings.size(); ++j) {\n            lengths = _mm256_insert_epi32(lengths, strings[i + j].size(), j);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_lengths);\n        max_lengths = _mm256_blendv_epi8(max_lengths, lengths, cmp);\n        indices = _mm256_blendv_epi8(indices, _mm256_set1_epi32(i), cmp);\n    }\n\n    for (int j = 0; j < 8; ++j) {\n        size_t length = _mm256_extract_epi32(max_lengths, j);\n        if (length > max_length) {\n            max_length = length;\n            max_index = _mm256_extract_epi32(indices, j);\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    __m256i max_lengths = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        for (int j = 0; j < 8 && i + j < strings.size(); ++j) {\n            lengths = _mm256_insert_epi32(lengths, strings[i + j].size(), j);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_lengths);\n        max_lengths = _mm256_blendv_epi8(max_lengths, lengths, cmp);\n        indices = _mm256_blendv_epi8(indices, _mm256_set1_epi32(i), cmp);\n    }\n\n    alignas(32) int lengths_array[8];\n    alignas(32) int indices_array[8];\n    _mm256_store_si256((__m256i*)lengths_array, max_lengths);\n    _mm256_store_si256((__m256i*)indices_array, indices);\n\n    for (int j = 0; j < 8; ++j) {\n        if (lengths_array[j] > max_length) {\n            max_length = lengths_array[j];\n            max_index = indices_array[j] + j;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_length = 0;\n    std::string longest_string = strings[0];\n\n    const size_t num_strings = strings.size();\n    const size_t chunk_size = 4; // AVX2 processes 4 elements at a time\n\n    for (size_t i = 0; i < num_strings; i += chunk_size) {\n        __m256i lengths = _mm256_setzero_si256();\n\n        for (size_t j = 0; j < chunk_size && i + j < num_strings; ++j) {\n            lengths = _mm256_insert_epi32(lengths, strings[i + j].size(), j);\n        }\n\n        __m256i max_lengths = _mm256_set1_epi32(max_length);\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_lengths);\n\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            for (size_t j = 0; j < chunk_size && i + j < num_strings; ++j) {\n                if (strings[i + j].size() > max_length) {\n                    max_length = strings[i + j].size();\n                    longest_string = strings[i + j];\n                }\n            }\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_length = 0;\n    std::string longest_string = strings[0];\n\n    for (size_t i = 0; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_set_epi32(i + 3, i + 2, i + 1, i, i + 3, i + 2, i + 1, i);\n\n        for (int j = 0; j < 4 && i + j < strings.size(); ++j) {\n            lengths = _mm256_insert_epi32(lengths, strings[i + j].size(), j);\n        }\n\n        __m256i max_lengths = _mm256_set1_epi32(max_length);\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_lengths);\n\n        if (_mm256_testz_si256(cmp, cmp) == 0) {\n            int mask = _mm256_movemask_epi8(cmp);\n            int index = _bit_scan_forward(mask);\n            max_length = strings[i + index].size();\n            longest_string = strings[i + index];\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n\n    while (!_mm256_testz_si256(vec_b, vec_b)) {\n        __m256i temp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(vec_b, _mm256_set1_epi64x(a / b)));\n        vec_a = temp;\n    }\n\n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n\n    while (!_mm256_testz_si256(vec_b, vec_b)) {\n        __m256i temp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(vec_b, _mm256_div_epu64(vec_a, vec_b)));\n        vec_a = temp;\n    }\n\n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (!_mm256_testz_si256(vb, vb)) {\n        __m256i temp = vb;\n        vb = _mm256_sub_epi64(va, _mm256_and_si256(vb, _mm256_sub_epi64(va, vb)));\n        va = temp;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n\n    while (!_mm256_testz_si256(vec_b, vec_b)) {\n        __m256i temp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(vec_b, _mm256_div_epu64(vec_a, vec_b)));\n        vec_a = temp;\n    }\n\n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n\n    while (!_mm256_testz_si256(vec_b, vec_b)) {\n        __m256i temp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(vec_b, _mm256_div_epu64(vec_a, vec_b)));\n        vec_a = temp;\n    }\n\n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    std::vector<float> closest_pair(2);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (size_t j = i + 8; j < n; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n            float diff_arr[8];\n            _mm256_storeu_ps(diff_arr, abs_diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diff_arr[k] < min_diff) {\n                    min_diff = diff_arr[k];\n                    closest_pair[0] = std::min(numbers[i + k], numbers[j]);\n                    closest_pair[1] = std::max(numbers[i + k], numbers[j]);\n                }\n            }\n        }\n    }\n\n    return closest_pair;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    std::vector<float> closest_pair(2);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (size_t j = i + 8; j < n; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            float diff_arr[8];\n            _mm256_storeu_ps(diff_arr, abs_diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diff_arr[k] < min_diff) {\n                    min_diff = diff_arr[k];\n                    closest_pair[0] = std::min(numbers[i + k], numbers[j]);\n                    closest_pair[1] = std::max(numbers[i + k], numbers[j]);\n                }\n            }\n        }\n    }\n\n    return closest_pair;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    std::vector<float> closest_pair(2);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (size_t j = i + 8; j < n; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            diff = _mm256_and_ps(diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n\n            float diff_arr[8];\n            _mm256_storeu_ps(diff_arr, diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diff_arr[k] < min_diff) {\n                    min_diff = diff_arr[k];\n                    closest_pair[0] = std::min(numbers[i + k], numbers[j]);\n                    closest_pair[1] = std::max(numbers[i + k], numbers[j]);\n                }\n            }\n        }\n    }\n\n    return closest_pair;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    std::vector<float> result(2);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (size_t j = i + 8; j < n; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            float diff_arr[8];\n            _mm256_storeu_ps(diff_arr, abs_diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diff_arr[k] < min_diff) {\n                    min_diff = diff_arr[k];\n                    result[0] = std::min(numbers[i + k], numbers[j + k]);\n                    result[1] = std::max(numbers[i + k], numbers[j + k]);\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    std::vector<float> closest_pair(2);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (size_t j = i + 8; j < n; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n            float temp_diff[8];\n            _mm256_storeu_ps(temp_diff, abs_diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (temp_diff[k] < min_diff) {\n                    min_diff = temp_diff[k];\n                    closest_pair[0] = std::min(numbers[i + k], numbers[j]);\n                    closest_pair[1] = std::max(numbers[i + k], numbers[j]);\n                }\n            }\n        }\n    }\n\n    return closest_pair;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float min_float = _mm256_reduce_min_ps(min_val);\n    float max_float = _mm256_reduce_max_ps(max_val);\n\n    __m256 range = _mm256_set1_ps(max_float - min_float);\n    __m256 min_vec = _mm256_set1_ps(min_float);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_sub_ps(vec, min_vec);\n        vec = _mm256_div_ps(vec, range);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float min_float = _mm256_reduce_min_ps(min_val);\n    float max_float = _mm256_reduce_max_ps(max_val);\n\n    __m256 scale = _mm256_set1_ps(1.0f / (max_float - min_float));\n    __m256 offset = _mm256_set1_ps(-min_float);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_add_ps(vec, offset);\n        vec = _mm256_mul_ps(vec, scale);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float min_float = _mm256_reduce_min_ps(min_val);\n    float max_float = _mm256_reduce_max_ps(max_val);\n\n    __m256 range = _mm256_set1_ps(max_float - min_float);\n    __m256 min_vec = _mm256_set1_ps(min_float);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_sub_ps(vec, min_vec);\n        vec = _mm256_div_ps(vec, range);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t size = numbers.size();\n    std::vector<float> result(size);\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float min_float = _mm256_reduce_min_ps(min_val);\n    float max_float = _mm256_reduce_max_ps(max_val);\n\n    __m256 scale = _mm256_set1_ps(1.0f / (max_float - min_float));\n    __m256 offset = _mm256_set1_ps(-min_float);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_add_ps(vec, offset);\n        vec = _mm256_mul_ps(vec, scale);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n\n    __m256 min_vec = _mm256_set1_ps(numbers[0]);\n    __m256 max_vec = _mm256_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_vec = _mm256_min_ps(min_vec, vec);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float min_val = _mm256_reduce_min_ps(min_vec);\n    float max_val = _mm256_reduce_max_ps(max_vec);\n\n    __m256 scale_vec = _mm256_set1_ps(1.0f / (max_val - min_val));\n    __m256 offset_vec = _mm256_set1_ps(-min_val);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_add_ps(_mm256_mul_ps(vec, scale_vec), offset_vec);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_result = _mm256_set1_epi64x(0);\n\n    for (uint64_t i = 1; i <= n / 2; i += 4) {\n        __m256i vec_div = _mm256_add_epi64(vec_i, _mm256_set1_epi64x(i - 1));\n        __m256i vec_mod = _mm256_sub_epi64(vec_n, _mm256_mullo_epi64(vec_div, _mm256_div_epu64(vec_n, vec_div)));\n        __m256i vec_mask = _mm256_cmpeq_epi64(vec_mod, vec_ones);\n        vec_result = _mm256_blendv_epi8(vec_result, vec_div, vec_mask);\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, vec_result);\n    return result[3];\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_setr_epi64x(n / 2, n / 2 - 1, n / 2 - 2, n / 2 - 3);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_set1_epi64x(0);\n\n    while (true) {\n        __m256i remainder = _mm256_sub_epi64(vec_n, _mm256_mullo_epi64(vec_i, _mm256_div_epi64(vec_n, vec_i)));\n        __m256i mask = _mm256_cmpeq_epi64(remainder, vec_zero);\n\n        if (_mm256_testz_si256(mask, mask)) {\n            vec_i = _mm256_sub_epi64(vec_i, vec_one);\n        } else {\n            break;\n        }\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, vec_i);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_setr_epi64x(1, 2, 3, 4);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_result = vec_zero;\n\n    for (uint64_t i = 1; i <= n / 4; i += 4) {\n        __m256i vec_div = _mm256_div_epi64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mullo_epi64(vec_div, vec_i);\n        __m256i vec_eq = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        __m256i vec_mask = _mm256_and_si256(vec_eq, vec_i);\n        vec_result = _mm256_max_epi64(vec_result, vec_mask);\n        vec_i = _mm256_add_epi64(vec_i, vec_one);\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, vec_result);\n    return result[3];\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_result = vec_zero;\n\n    for (uint64_t i = 1; i <= n / 4; i += 4) {\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mullo_epi64(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        __m256i vec_mask = _mm256_and_si256(vec_cmp, vec_ones);\n        __m256i vec_max = _mm256_max_epu64(vec_result, vec_mask);\n        vec_result = _mm256_blendv_epi8(vec_result, vec_i, vec_mask);\n        vec_i = _mm256_add_epi64(vec_i, _mm256_set1_epi64x(4));\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, vec_result);\n    uint64_t max_divisor = 0;\n    for (int i = 0; i < 4; ++i) {\n        if (result[i] > max_divisor && result[i] < n) {\n            max_divisor = result[i];\n        }\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n, n - 1, n - 2, n - 3);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_result = vec_zero;\n\n    while (_mm256_movemask_epi8(_mm256_cmpeq_epi64(vec_i, vec_zero)) == 0) {\n        __m256i vec_div = _mm256_div_epi64(vec_n, vec_i);\n        __m256i vec_mod = _mm256_sub_epi64(vec_n, _mm256_mullo_epi64(vec_div, vec_i));\n        __m256i vec_mask = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n        vec_result = _mm256_blendv_epi8(vec_result, vec_i, vec_mask);\n\n        vec_i = _mm256_sub_epi64(vec_i, vec_one);\n    }\n\n    return _mm256_extract_epi64(vec_result, 0);\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 2);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_zero = _mm256_set1_epi32(0);\n\n    while (n > 1) {\n        __m256i vec_div = _mm256_div_epi32(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mullo_epi32(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mul, vec_n);\n\n        int mask = _mm256_movemask_epi8(vec_cmp);\n        if (mask != 0) {\n            int idx = __builtin_ctz(mask) / 4;\n            int factor = vec_i[idx];\n            factors.push_back(factor);\n            n /= factor;\n            vec_n = _mm256_set1_epi32(n);\n        } else {\n            vec_i = _mm256_add_epi32(vec_i, vec_one);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_zero = _mm256_set1_epi32(0);\n\n    while (n > 1) {\n        __m256i v_div = _mm256_div_epi32(v_n, v_i);\n        __m256i v_mul = _mm256_mullo_epi32(v_div, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_mul, v_n);\n        int mask = _mm256_movemask_epi8(v_cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    int factor = _mm256_extract_epi32(v_i, j);\n                    while (n % factor == 0) {\n                        factors.push_back(factor);\n                        n /= factor;\n                    }\n                    v_n = _mm256_set1_epi32(n);\n                }\n            }\n        } else {\n            v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(20));\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_zero = _mm256_set1_epi32(0);\n\n    while (n > 1) {\n        __m256i v_div = _mm256_div_epi32(v_n, v_i);\n        __m256i v_mul = _mm256_mullo_epi32(v_div, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_mul, v_n);\n\n        int mask = _mm256_movemask_epi8(v_cmp);\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    int factor = v_i.m256i_i32[j];\n                    while (n % factor == 0) {\n                        factors.push_back(factor);\n                        n /= factor;\n                    }\n                    v_n = _mm256_set1_epi32(n);\n                }\n            }\n        } else {\n            v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(2));\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i vec_ones = _mm256_set1_epi32(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (n > 1) {\n        __m256i vec_div = _mm256_div_epi32(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mullo_epi32(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mul, vec_n);\n\n        int mask = _mm256_movemask_epi8(vec_cmp);\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    int factor = _mm256_extract_epi32(vec_i, j);\n                    while (n % factor == 0) {\n                        factors.push_back(factor);\n                        n /= factor;\n                    }\n                    vec_n = _mm256_set1_epi32(n);\n                }\n            }\n        } else {\n            vec_i = _mm256_add_epi32(vec_i, vec_ones);\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (n > 1) {\n        __m256i vec_div = _mm256_div_epi32(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mullo_epi32(vec_div, vec_i);\n        __m256i vec_eq = _mm256_cmpeq_epi32(vec_mul, vec_n);\n        int mask = _mm256_movemask_epi8(vec_eq);\n\n        if (mask != 0) {\n            int idx = __builtin_ctz(mask) / 4;\n            int factor = vec_i[idx];\n            factors.push_back(factor);\n            n /= factor;\n            vec_n = _mm256_set1_epi32(n);\n        } else {\n            vec_i = _mm256_add_epi32(vec_i, vec_one);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    std::vector<float> result;\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8 && i + j < l.size(); ++j) {\n            if (mask.m256_f32[j]) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    std::vector<float> result;\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        int mask_bits = _mm256_movemask_ps(mask);\n\n        for (int j = 0; j < 8 && i + j < l.size(); ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    std::vector<float> result;\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        int mask_bits = _mm256_movemask_ps(mask);\n\n        for (int j = 0; j < 8 && i + j < l.size(); ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    __m256 zero = _mm256_set1_ps(0.0f);\n    size_t i = 0;\n\n    for (; i + 8 <= l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask.m256_f32[j]) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    std::vector<float> result;\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        int mask_bits = _mm256_movemask_ps(mask);\n\n        for (int j = 0; j < 8 && i + j < l.size(); ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i step = _mm256_set1_epi64x(30);\n    __m256i i_vec = _mm256_set1_epi64x(i);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    while (i * i <= n) {\n        __m256i mod1 = _mm256_set1_epi64x(n % i);\n        __m256i mod2 = _mm256_set1_epi64x(n % (i + 4));\n        __m256i mod3 = _mm256_set1_epi64x(n % (i + 6));\n        __m256i mod4 = _mm256_set1_epi64x(n % (i + 30));\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask1 = _mm256_cmpeq_epi64(mod1, zero);\n        __m256i mask2 = _mm256_cmpeq_epi64(mod2, zero);\n        __m256i mask3 = _mm256_cmpeq_epi64(mod3, zero);\n        __m256i mask4 = _mm256_cmpeq_epi64(mod4, zero);\n\n        __m256i or_mask = _mm256_or_si256(_mm256_or_si256(mask1, mask2), _mm256_or_si256(mask3, mask4));\n        int mask = _mm256_movemask_epi8(or_mask);\n\n        if (mask != 0) return false;\n\n        i_vec = _mm256_add_epi64(i_vec, step);\n        i = _mm256_extract_epi64(i_vec, 0);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i step = _mm256_set1_epi64x(30);\n    __m256i i_vec = _mm256_set1_epi64x(i);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    while (i * i <= n) {\n        __m256i i_plus_0 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(0));\n        __m256i i_plus_4 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(4));\n        __m256i i_plus_8 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(8));\n        __m256i i_plus_12 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(12));\n        __m256i i_plus_16 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(16));\n        __m256i i_plus_20 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(20));\n        __m256i i_plus_24 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(24));\n        __m256i i_plus_28 = _mm256_add_epi64(i_vec, _mm256_set1_epi64x(28));\n\n        __m256i mod_0 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_0));\n        __m256i mod_4 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_4));\n        __m256i mod_8 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_8));\n        __m256i mod_12 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_12));\n        __m256i mod_16 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_16));\n        __m256i mod_20 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_20));\n        __m256i mod_24 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_24));\n        __m256i mod_28 = _mm256_cvtepi64_epi32(_mm256_div_epi64(n_vec, i_plus_28));\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask_0 = _mm256_cmpeq_epi64(mod_0, zero);\n        __m256i mask_4 = _mm256_cmpeq_epi64(mod_4, zero);\n        __m256i mask_8 = _mm256_cmpeq_epi64(mod_8, zero);\n        __m256i mask_12 = _mm256_cmpeq_epi64(mod_12, zero);\n        __m256i mask_16 = _mm256_cmpeq_epi64(mod_16, zero);\n        __m256i mask_20 = _mm256_cmpeq_epi64(mod_20, zero);\n        __m256i mask_24 = _mm256_cmpeq_epi64(mod_24, zero);\n        __m256i mask_28 = _mm256_cmpeq_epi64(mod_28, zero);\n\n        __m256i or_mask = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(_mm256_or_si256(mask_0, mask_4), _mm256_or_si256(mask_8, mask_12)), _mm256_or_si256(_mm256_or_si256(mask_16, mask_20), _mm256_or_si256(mask_24, mask_28))));\n\n        if (_mm256_testz_si256(or_mask, or_mask) == 0) {\n            return false;\n        }\n\n        i_vec = _mm256_add_epi64(i_vec, step);\n        i += 30;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_set_epi64x(15, 11, 7, 3);\n    __m256i v_step = _mm256_set1_epi64x(8);\n\n    while (i * i <= n) {\n        __m256i v_mod = _mm256_set1_epi64x(0);\n        v_mod = _mm256_add_epi64(v_mod, _mm256_cmpeq_epi64(_mm256_set1_epi64x(0), _mm256_div_epi64(v_n, v_i)));\n        if (_mm256_movemask_epi8(v_mod) != 0) return false;\n\n        v_i = _mm256_add_epi64(v_i, v_step);\n        i += 8;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i step = _mm256_set1_epi64x(30);\n    __m256i i_vec = _mm256_set1_epi64x(i);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (i * i <= n) {\n        __m256i mod1 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, i_vec));\n        __m256i mod2 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(4))));\n        __m256i mod3 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(6))));\n        __m256i mod4 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(10))));\n        __m256i mod5 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(12))));\n        __m256i mod6 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(16))));\n        __m256i mod7 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(18))));\n        __m256i mod8 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(22))));\n        __m256i mod9 = _mm256_cvtepi64_epi32(_mm256_rem_epi64(n_vec, _mm256_add_epi64(i_vec, _mm256_set1_epi64x(24))));\n\n        __m256i mask1 = _mm256_cmpeq_epi32(mod1, zero);\n        __m256i mask2 = _mm256_cmpeq_epi32(mod2, zero);\n        __m256i mask3 = _mm256_cmpeq_epi32(mod3, zero);\n        __m256i mask4 = _mm256_cmpeq_epi32(mod4, zero);\n        __m256i mask5 = _mm256_cmpeq_epi32(mod5, zero);\n        __m256i mask6 = _mm256_cmpeq_epi32(mod6, zero);\n        __m256i mask7 = _mm256_cmpeq_epi32(mod7, zero);\n        __m256i mask8 = _mm256_cmpeq_epi32(mod8, zero);\n        __m256i mask9 = _mm256_cmpeq_epi32(mod9, zero);\n\n        __m256i combined_mask = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(_mm256_or_si256(_mm256_or_si256(mask1, mask2), mask3), mask4), mask5), _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(mask6, mask7), mask8), mask9));\n\n        if (_mm256_testz_si256(combined_mask, combined_mask) == 0) {\n            return false;\n        }\n\n        i_vec = _mm256_add_epi64(i_vec, step);\n        i += 30;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_set_epi64x(15, 11, 7, 3);\n    __m256i v_step = _mm256_set1_epi64x(8);\n\n    while (i * i <= n) {\n        __m256i v_mod = _mm256_set1_epi64x(0);\n        v_mod = _mm256_add_epi64(v_mod, _mm256_cmpeq_epi64(_mm256_set1_epi64x(0), _mm256_div_epi64(v_n, v_i)));\n        int mask = _mm256_movemask_epi8(v_mod);\n        if (mask != 0) return false;\n\n        v_i = _mm256_add_epi64(v_i, v_step);\n        i += 8;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool has_letters = false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_mask = _mm256_set1_epi8(0xDF);\n    __m256i letter_mask = _mm256_set1_epi8(0x80);\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i is_lower = _mm256_and_si256(vec, letter_mask);\n        __m256i is_upper = _mm256_andnot_si256(letter_mask, vec);\n\n        __m256i lower_to_upper = _mm256_and_si256(is_lower, lower_mask);\n        __m256i upper_to_lower = _mm256_andnot_si256(lower_mask, is_upper);\n\n        __m256i reversed_case = _mm256_or_si256(lower_to_upper, upper_to_lower);\n        __m256i no_change = _mm256_andnot_si256(letter_mask, vec);\n\n        __m256i final_vec = _mm256_or_si256(reversed_case, no_change);\n\n        _mm256_storeu_si256((__m256i*)&result[i], final_vec);\n\n        has_letters |= !_mm256_testz_si256(is_lower, is_lower) || !_mm256_testz_si256(is_upper, is_upper);\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool has_letters = false;\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower = _mm256_set1_epi8('a');\n        __m256i upper = _mm256_set1_epi8('A');\n        __m256i diff = _mm256_set1_epi8('a' - 'A');\n        __m256i mask = _mm256_set1_epi8(0x20);\n\n        __m256i is_lower = _mm256_cmpeq_epi8(_mm256_and_si256(_mm256_subs_epu8(vec, lower), mask), _mm256_setzero_si256());\n        __m256i is_upper = _mm256_cmpeq_epi8(_mm256_and_si256(_mm256_subs_epu8(vec, upper), mask), _mm256_setzero_si256());\n\n        __m256i to_upper = _mm256_and_si256(is_lower, _mm256_sub_epi8(vec, diff));\n        __m256i to_lower = _mm256_and_si256(is_upper, _mm256_add_epi8(vec, diff));\n        __m256i unchanged = _mm256_andnot_si256(_mm256_or_si256(is_lower, is_upper), vec);\n\n        __m256i res = _mm256_or_si256(_mm256_or_si256(to_upper, to_lower), unchanged);\n\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n\n        has_letters |= _mm256_movemask_epi8(is_lower) | _mm256_movemask_epi8(is_upper);\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const size_t len = s.length();\n    std::string result(len, '\\0');\n    bool has_letters = false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_mask = _mm256_set1_epi8(0xDF);\n    __m256i ascii_a = _mm256_set1_epi8('a');\n    __m256i ascii_z = _mm256_set1_epi8('z');\n    __m256i ascii_A = _mm256_set1_epi8('A');\n    __m256i ascii_Z = _mm256_set1_epi8('Z');\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_sub_epi8(ascii_a, _mm256_set1_epi8(1))),\n                                            _mm256_cmpgt_epi8(_mm256_add_epi8(ascii_z, _mm256_set1_epi8(1)), vec));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_sub_epi8(ascii_A, _mm256_set1_epi8(1))),\n                                            _mm256_cmpgt_epi8(_mm256_add_epi8(ascii_Z, _mm256_set1_epi8(1)), vec));\n\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        has_letters |= _mm256_movemask_epi8(is_letter) != 0;\n\n        __m256i lower_to_upper = _mm256_andnot_si256(lower_mask, vec);\n        __m256i upper_to_lower = _mm256_or_si256(lower_mask, vec);\n\n        __m256i converted = _mm256_blendv_epi8(vec, _mm256_blendv_epi8(lower_to_upper, upper_to_lower, is_upper), is_letter);\n\n        _mm256_storeu_si256((__m256i*)&result[i], converted);\n    }\n\n    if (!has_letters) {\n        for (size_t i = 0; i < len / 2; ++i) {\n            std::swap(result[i], result[len - 1 - i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool has_letters = false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_mask = _mm256_set1_epi8(0xDF);\n    __m256i letter_mask = _mm256_set1_epi8(0x80);\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i is_lower = _mm256_and_si256(vec, lower_mask);\n        __m256i is_upper = _mm256_andnot_si256(lower_mask, vec);\n        __m256i is_letter = _mm256_and_si256(vec, letter_mask);\n\n        __m256i lower_to_upper = _mm256_and_si256(is_lower, upper_mask);\n        __m256i upper_to_lower = _mm256_andnot_si256(upper_mask, is_upper);\n\n        __m256i reversed_case = _mm256_or_si256(lower_to_upper, upper_to_lower);\n        __m256i final_vec = _mm256_blendv_epi8(vec, reversed_case, is_letter);\n\n        _mm256_storeu_si256((__m256i*)&result[i], final_vec);\n\n        has_letters |= _mm256_movemask_epi8(is_letter) != 0;\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool has_letters = false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_mask = _mm256_set1_epi8(0xDF);\n    __m256i lower_bound = _mm256_set1_epi8('a');\n    __m256i upper_bound = _mm256_set1_epi8('z');\n    __m256i upper_bound_cap = _mm256_set1_epi8('Z');\n    __m256i lower_bound_cap = _mm256_set1_epi8('A');\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1))), _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), vec));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_sub_epi8(lower_bound_cap, _mm256_set1_epi8(1))), _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound_cap, _mm256_set1_epi8(1)), vec));\n\n        __m256i to_upper = _mm256_andnot_si256(is_lower, is_upper);\n        __m256i to_lower = _mm256_andnot_si256(is_upper, is_lower);\n\n        __m256i upper = _mm256_and_si256(vec, upper_mask);\n        __m256i lower = _mm256_or_si256(vec, lower_mask);\n\n        __m256i res = _mm256_blendv_epi8(vec, upper, to_upper);\n        res = _mm256_blendv_epi8(res, lower, to_lower);\n\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n\n        has_letters |= _mm256_movemask_epi8(to_upper) | _mm256_movemask_epi8(to_lower);\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_thirds;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i mod_mask = _mm256_set1_epi32(3);\n        __m256i mod_result = _mm256_div_epi32(mask, mod_mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(mod_result, zero);\n\n        int cmp_mask = _mm256_movemask_epi8(cmp);\n        for (int j = 0; j < 8; ++j) {\n            if (cmp_mask & (1 << (j * 4))) {\n                sorted_thirds.push_back(l[i + j]);\n            }\n        }\n    }\n\n    std::sort(sorted_thirds.begin(), sorted_thirds.end());\n\n    int sorted_index = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i mod_mask = _mm256_set1_epi32(3);\n        __m256i mod_result = _mm256_div_epi32(mask, mod_mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(mod_result, zero);\n\n        int cmp_mask = _mm256_movemask_epi8(cmp);\n        for (int j = 0; j < 8; ++j) {\n            if (cmp_mask & (1 << (j * 4))) {\n                l[i + j] = sorted_thirds[sorted_index++];\n            }\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int size = l.size();\n    int remainder = size % 8;\n    int aligned_size = size - remainder;\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i mod_mask = _mm256_set1_epi32(3);\n        __m256i mod_result = _mm256_div_epi32(mask, mod_mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(mod_result, zero);\n\n        int cmp_mask = _mm256_movemask_epi8(cmp);\n        if (cmp_mask != 0) {\n            std::vector<int> to_sort;\n            for (int j = 0; j < 8; ++j) {\n                if (cmp_mask & (1 << (j * 4))) {\n                    to_sort.push_back(l[i + j]);\n                }\n            }\n            std::sort(to_sort.begin(), to_sort.end());\n            int k = 0;\n            for (int j = 0; j < 8; ++j) {\n                if (cmp_mask & (1 << (j * 4))) {\n                    l[i + j] = to_sort[k++];\n                }\n            }\n        }\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 3 == 0) {\n            std::vector<int> to_sort;\n            for (int j = i; j < size; j += 3) {\n                to_sort.push_back(l[j]);\n            }\n            std::sort(to_sort.begin(), to_sort.end());\n            int k = 0;\n            for (int j = i; j < size; j += 3) {\n                l[j] = to_sort[k++];\n            }\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_thirds;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                sorted_thirds.push_back(temp[j]);\n            }\n        }\n    }\n\n    std::sort(sorted_thirds.begin(), sorted_thirds.end());\n\n    int sorted_index = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                temp[j] = sorted_thirds[sorted_index++];\n            }\n        }\n\n        __m256i sorted_vec = _mm256_load_si256((__m256i*)temp);\n        _mm256_storeu_si256((__m256i*)&l[i], sorted_vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_thirds;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                sorted_thirds.push_back(temp[j]);\n            }\n        }\n    }\n\n    std::sort(sorted_thirds.begin(), sorted_thirds.end());\n\n    int sorted_index = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                temp[j] = sorted_thirds[sorted_index++];\n            }\n        }\n\n        _mm256_storeu_si256((__m256i*)&l[i], _mm256_load_si256((__m256i*)temp));\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_thirds;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int thirds_array[8];\n        _mm256_store_si256((__m256i*)thirds_array, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (thirds_array[j] != 0) {\n                sorted_thirds.push_back(thirds_array[j]);\n            }\n        }\n    }\n\n    std::sort(sorted_thirds.begin(), sorted_thirds.end());\n\n    int sorted_index = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i thirds = _mm256_and_si256(vec, mask);\n\n        alignas(32) int thirds_array[8];\n        _mm256_store_si256((__m256i*)thirds_array, thirds);\n\n        for (int j = 0; j < 8; ++j) {\n            if (thirds_array[j] != 0) {\n                thirds_array[j] = sorted_thirds[sorted_index++];\n            }\n        }\n\n        __m256i sorted_thirds_vec = _mm256_load_si256((__m256i*)thirds_array);\n        __m256i result = _mm256_or_si256(_mm256_andnot_si256(mask, vec), sorted_thirds_vec);\n\n        _mm256_storeu_si256((__m256i*)&l[i], result);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\nfloat max_element_simd(std::vector<float> l) {\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n    int i;\n\n    for (i = 0; i <= (int)l.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_vals[8];\n    _mm256_storeu_ps(max_vals, max_vec);\n    float max_val = max_vals[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_vals[j] > max_val) {\n            max_val = max_vals[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\nfloat max_element_simd(std::vector<float> l) {\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n    int i = 0;\n\n    for (; i <= (int)l.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_arr[8];\n    _mm256_storeu_ps(max_arr, max_vec);\n    float max_val = max_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_arr[j] > max_val) {\n            max_val = max_arr[j];\n        }\n    }\n\n    for (; i < (int)l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\nfloat max_element_simd(std::vector<float> l) {\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n    int i;\n\n    for (i = 0; i <= (int)l.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_vals[8];\n    _mm256_storeu_ps(max_vals, max_vec);\n\n    float max_element = max_vals[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_vals[j] > max_element) {\n            max_element = max_vals[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_element) {\n            max_element = l[i];\n        }\n    }\n\n    return max_element;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l) {\n    const int size = l.size();\n    const int aligned_size = size & ~7;\n    const int remainder = size & 7;\n\n    __m256 max_vec = _mm256_set1_ps(-std::numeric_limits<float>::infinity());\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_vals[8];\n    _mm256_storeu_ps(max_vals, max_vec);\n    float max_val = *std::max_element(max_vals, max_vals + 8);\n\n    for (int i = aligned_size; i < size; ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\nfloat max_element_simd(std::vector<float> l) {\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n    int size = l.size();\n    int i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_vals[8];\n    _mm256_storeu_ps(max_vals, max_vec);\n\n    float max_element = max_vals[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_vals[j] > max_element) {\n            max_element = max_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > max_element) {\n            max_element = l[i];\n        }\n    }\n\n    return max_element;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_add_epi32(_mm256_set1_epi32(i), _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i vmod11 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_div_epi32(vi, v11), v0), _mm256_cmpgt_epi32(vi, v0));\n        __m256i vmod13 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_div_epi32(vi, v13), v0), _mm256_cmpgt_epi32(vi, v0));\n        __m256i vdivisible = _mm256_or_si256(vmod11, vmod13);\n\n        __m256i vcount = _mm256_setzero_si256();\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(vi, j);\n            if (_mm256_extract_epi32(vdivisible, j)) {\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        vcount = _mm256_add_epi32(vcount, v1);\n                    }\n                    num /= 10;\n                }\n            }\n        }\n        count += _mm256_extract_epi32(vcount, 0) + _mm256_extract_epi32(vcount, 1) +\n                 _mm256_extract_epi32(vcount, 2) + _mm256_extract_epi32(vcount, 3) +\n                 _mm256_extract_epi32(vcount, 4) + _mm256_extract_epi32(vcount, 5) +\n                 _mm256_extract_epi32(vcount, 6) + _mm256_extract_epi32(vcount, 7);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v0 = _mm256_set1_epi32(0);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i vcount = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vdiv11 = _mm256_div_epi32(vi, v11);\n        __m256i vdiv13 = _mm256_div_epi32(vi, v13);\n        __m256i vmod11 = _mm256_sub_epi32(vi, _mm256_mullo_epi32(vdiv11, v11));\n        __m256i vmod13 = _mm256_sub_epi32(vi, _mm256_mullo_epi32(vdiv13, v13));\n        __m256i vcond11 = _mm256_cmpeq_epi32(vmod11, v0);\n        __m256i vcond13 = _mm256_cmpeq_epi32(vmod13, v0);\n        __m256i vcond = _mm256_or_si256(vcond11, vcond13);\n\n        __m256i vtemp = vi;\n        __m256i vcount7 = _mm256_set1_epi32(0);\n        while (_mm256_testz_si256(vtemp, vtemp) == 0) {\n            __m256i vmod10 = _mm256_sub_epi32(vtemp, _mm256_mullo_epi32(_mm256_div_epi32(vtemp, v10), v10));\n            __m256i vcond7 = _mm256_cmpeq_epi32(vmod10, v7);\n            vcount7 = _mm256_add_epi32(vcount7, vcond7);\n            vtemp = _mm256_div_epi32(vtemp, v10);\n        }\n\n        vcount = _mm256_add_epi32(vcount, _mm256_and_si256(vcond, vcount7));\n    }\n\n    int* pcount = (int*)&vcount;\n    for (int i = 0; i < 8; ++i) {\n        count += pcount[i];\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v0 = _mm256_set1_epi32(0);\n    __m256i v1 = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i vdiv11 = _mm256_div_epi32(vi, v11);\n        __m256i vdiv13 = _mm256_div_epi32(vi, v13);\n        __m256i vmod11 = _mm256_sub_epi32(vi, _mm256_mullo_epi32(vdiv11, v11));\n        __m256i vmod13 = _mm256_sub_epi32(vi, _mm256_mullo_epi32(vdiv13, v13));\n        __m256i vcond11 = _mm256_cmpeq_epi32(vmod11, v0);\n        __m256i vcond13 = _mm256_cmpeq_epi32(vmod13, v0);\n        __m256i vcond = _mm256_or_si256(vcond11, vcond13);\n\n        __m256i vcount = _mm256_set1_epi32(0);\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(vi, j);\n            if (_mm256_extract_epi32(vcond, j)) {\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        vcount = _mm256_add_epi32(vcount, v1);\n                    }\n                    num /= 10;\n                }\n            }\n        }\n        count += _mm256_extract_epi32(vcount, 0);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v0 = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vindex = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vmod11 = _mm256_div_epi32(vindex, v11);\n        __m256i vmod13 = _mm256_div_epi32(vindex, v13);\n        __m256i vdiv11 = _mm256_mullo_epi32(vmod11, v11);\n        __m256i vdiv13 = _mm256_mullo_epi32(vmod13, v13);\n        __m256i vrem11 = _mm256_sub_epi32(vindex, vdiv11);\n        __m256i vrem13 = _mm256_sub_epi32(vindex, vdiv13);\n        __m256i vcond11 = _mm256_cmpeq_epi32(vrem11, v0);\n        __m256i vcond13 = _mm256_cmpeq_epi32(vrem13, v0);\n        __m256i vcond = _mm256_or_si256(vcond11, vcond13);\n\n        __m256i vdiv10 = _mm256_div_epi32(vindex, v10);\n        __m256i vrem10 = _mm256_sub_epi32(vindex, _mm256_mullo_epi32(vdiv10, v10));\n        __m256i vcond7 = _mm256_cmpeq_epi32(vrem10, v7);\n        __m256i vfinal = _mm256_and_si256(vcond, vcond7);\n\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(vfinal, v0));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v10 = _mm256_set1_epi32(10);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_add_epi32(_mm256_set1_epi32(i), _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i vmod11 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_div_epi32(vi, v11), v0), _mm256_cmpgt_epi32(vi, v0));\n        __m256i vmod13 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_div_epi32(vi, v13), v0), _mm256_cmpgt_epi32(vi, v0));\n        __m256i vdivisible = _mm256_or_si256(vmod11, vmod13);\n\n        __m256i vcount = _mm256_setzero_si256();\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(vi, j);\n            if (_mm256_extract_epi32(vdivisible, j)) {\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        vcount = _mm256_add_epi32(vcount, v1);\n                    }\n                    num /= 10;\n                }\n            }\n        }\n        count += _mm256_extract_epi32(vcount, 0) + _mm256_extract_epi32(vcount, 1) + _mm256_extract_epi32(vcount, 2) + _mm256_extract_epi32(vcount, 3) +\n                 _mm256_extract_epi32(vcount, 4) + _mm256_extract_epi32(vcount, 5) + _mm256_extract_epi32(vcount, 6) + _mm256_extract_epi32(vcount, 7);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t size = l.size();\n    if (size < 2) return l;\n\n    __m256 even_mask = _mm256_setr_ps(1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f);\n    __m256 odd_mask = _mm256_setr_ps(0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 even = _mm256_and_ps(vec, even_mask);\n        __m256 odd = _mm256_and_ps(vec, odd_mask);\n\n        float even_arr[8];\n        _mm256_storeu_ps(even_arr, even);\n        std::sort(even_arr, even_arr + 8, [](float a, float b) { return a != 0.0f && (b == 0.0f || a < b); });\n\n        __m256 sorted_even = _mm256_loadu_ps(even_arr);\n        __m256 result = _mm256_or_ps(sorted_even, odd);\n        _mm256_storeu_ps(&l[i], result);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    __m256 even_mask = _mm256_setr_ps(0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f);\n    __m256 odd_mask = _mm256_setr_ps(1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 even = _mm256_and_ps(vec, even_mask);\n        __m256 odd = _mm256_and_ps(vec, odd_mask);\n\n        float even_arr[8];\n        _mm256_storeu_ps(even_arr, even);\n\n        std::sort(even_arr, even_arr + 4);\n        std::sort(even_arr + 4, even_arr + 8);\n\n        __m256 sorted_even = _mm256_loadu_ps(even_arr);\n        __m256 result = _mm256_or_ps(sorted_even, odd);\n\n        _mm256_storeu_ps(&l[i], result);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    __m256 even_mask = _mm256_setr_ps(0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f);\n    __m256 odd_mask = _mm256_setr_ps(1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 even_vals = _mm256_and_ps(vec, even_mask);\n        __m256 odd_vals = _mm256_and_ps(vec, odd_mask);\n\n        float even_arr[8];\n        _mm256_storeu_ps(even_arr, even_vals);\n\n        std::sort(even_arr, even_arr + 8, [](float a, float b) {\n            return a != 0.0f && (b == 0.0f || a < b);\n        });\n\n        __m256 sorted_even_vals = _mm256_loadu_ps(even_arr);\n        __m256 result = _mm256_or_ps(sorted_even_vals, odd_vals);\n        _mm256_storeu_ps(&l[i], result);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    std::vector<float> even_elements;\n    even_elements.reserve((n + 1) / 2);\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    for (size_t i = 0; i < even_elements.size(); ++i) {\n        l[2 * i] = even_elements[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    __m256 even_mask = _mm256_setr_ps(0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f);\n    __m256 odd_mask = _mm256_setr_ps(1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 even = _mm256_and_ps(vec, even_mask);\n        __m256 odd = _mm256_and_ps(vec, odd_mask);\n\n        float even_arr[8];\n        _mm256_storeu_ps(even_arr, even);\n\n        std::sort(even_arr, even_arr + 4);\n        std::sort(even_arr + 4, even_arr + 8);\n\n        __m256 sorted_even = _mm256_loadu_ps(even_arr);\n        __m256 result = _mm256_or_ps(sorted_even, odd);\n\n        _mm256_storeu_ps(&l[i], result);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i a = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i b = _mm256_set1_epi32(l[j]);\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i c = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(a, _mm256_add_epi32(b, c));\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i a = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i b = _mm256_set1_epi32(l[j]);\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i c = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i a = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i b = _mm256_set1_epi32(l[j]);\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i c = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_set1_epi32(-(l[i] + l[j]));\n\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vk_chunk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i cmp = _mm256_cmpeq_epi32(vk_chunk, vk);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_set1_epi32(-(l[i] + l[j]));\n\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vk_chunk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i cmp = _mm256_cmpeq_epi32(vk_chunk, vk);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int size = l.size();\n    int remainder = size % 8;\n    int aligned_size = size - remainder;\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i result = _mm256_add_epi32(vec, ones);\n        _mm256_storeu_si256((__m256i*)&l[i], result);\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m256i increment = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i inc_vec = _mm256_add_epi32(vec, increment);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), inc_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m256i increment = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i res = _mm256_add_epi32(vec, increment);\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    const int size = l.size();\n    const int aligned_size = size & ~7;\n    const int remainder = size & 7;\n\n    __m256i increment = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        vec = _mm256_add_epi32(vec, increment);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[i]), vec);\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int size = l.size();\n    int aligned_size = (size + 7) & ~7;\n    l.resize(aligned_size, 0);\n\n    __m256i increment = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, increment);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    l.resize(size);\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    const int size = l.size();\n    if (size < 2) return false;\n\n    __m256i vec1, vec2, neg_vec2, result;\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = 0; j < size; j += 8) {\n            if (i == j) continue;\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            neg_vec2 = _mm256_sub_epi32(_mm256_setzero_si256(), vec2);\n            result = _mm256_cmpeq_epi32(vec1, neg_vec2);\n            if (_mm256_movemask_epi8(result) != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    const int* data = l.data();\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        for (size_t j = 0; j < 8; ++j) {\n            int val = data[i + j];\n            __m256i neg_val = _mm256_set1_epi32(-val);\n            __m256i cmp = _mm256_cmpeq_epi32(vec, neg_val);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n    }\n\n    for (; i < size; ++i) {\n        for (size_t j = i + 1; j < size; ++j) {\n            if (data[i] + data[j] == 0) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    const int size = l.size();\n    if (size < 2) return false;\n\n    __m256i vec = _mm256_set1_epi32(0);\n    for (int i = 0; i < size; ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        __m256i neg_current = _mm256_sub_epi32(vec, current);\n\n        for (int j = i + 1; j < size; j += 8) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i cmp = _mm256_cmpeq_epi32(chunk, neg_current);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    const int size = l.size();\n    if (size < 2) return false;\n\n    for (int i = 0; i < size; ++i) {\n        __m256i vec = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < size; j += 8) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i neg_chunk = _mm256_sub_epi32(_mm256_setzero_si256(), chunk);\n            __m256i cmp = _mm256_cmpeq_epi32(vec, neg_chunk);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    const int size = l.size();\n    if (size < 2) return false;\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = i + 8; j < size; j += 8) {\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i sum = _mm256_add_epi32(vec1, vec2);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t rem = _mm256_extract_epi64(rem_vec, 0);\n        x = _mm256_extract_epi64(div_vec, 0);\n\n        result = std::to_string(rem) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t rem = _mm256_extract_epi64(rem_vec, 0);\n        x = _mm256_extract_epi64(div_vec, 0);\n\n        result = std::to_string(rem) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t rem = _mm256_extract_epi64(rem_vec, 0);\n        x = _mm256_extract_epi64(div_vec, 0);\n\n        result = std::to_string(rem) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t rem = _mm256_extract_epi64(rem_vec, 0);\n        x = _mm256_extract_epi64(div_vec, 0);\n\n        result = std::to_string(rem) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t rem = _mm256_extract_epi64(rem_vec, 0);\n        x = _mm256_extract_epi64(div_vec, 0);\n\n        result = std::to_string(rem) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&text[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&text[len - i - 32]);\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    for (; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&text[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&text[len - i - 32]);\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n    }\n\n    for (; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&text[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&text[len - i - 32]);\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    for (; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&text[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&text[len - i - 32]);\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    for (; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&text[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&text[len - i - 32]);\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    for (; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    int len = s.length();\n    int i;\n\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    for (i = 0; i <= len - 32; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i decoded = _mm256_sub_epi8(input, shift);\n        decoded = _mm256_sub_epi8(decoded, base);\n        decoded = _mm256_add_epi8(decoded, mod);\n        decoded = _mm256_and_si256(decoded, _mm256_set1_epi8(25));\n        decoded = _mm256_add_epi8(decoded, base);\n        _mm256_storeu_si256((__m256i*)&out[i], decoded);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    int i;\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    for (i = 0; i <= s.length() - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i decoded = _mm256_sub_epi8(vec, shift);\n        decoded = _mm256_sub_epi8(decoded, base);\n        decoded = _mm256_add_epi8(decoded, mod);\n        decoded = _mm256_and_si256(decoded, _mm256_set1_epi8(25));\n        decoded = _mm256_add_epi8(decoded, base);\n        _mm256_storeu_si256((__m256i*)&out[i], decoded);\n    }\n\n    for (; i < s.length(); i++) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    int i;\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    for (i = 0; i <= s.length() - 32; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i decoded = _mm256_sub_epi8(input, shift);\n        decoded = _mm256_sub_epi8(decoded, base);\n        decoded = _mm256_add_epi8(decoded, mod);\n        decoded = _mm256_and_si256(decoded, _mm256_set1_epi8(25));\n        decoded = _mm256_add_epi8(decoded, base);\n        _mm256_storeu_si256((__m256i*)&out[i], decoded);\n    }\n\n    for (; i < s.length(); i++) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    int len = s.length();\n    int i;\n\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    for (i = 0; i <= len - 32; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i decoded = _mm256_sub_epi8(input, shift);\n        decoded = _mm256_sub_epi8(decoded, base);\n        decoded = _mm256_add_epi8(decoded, mod);\n        decoded = _mm256_and_si256(decoded, _mm256_set1_epi8(25));\n        decoded = _mm256_add_epi8(decoded, base);\n        _mm256_storeu_si256((__m256i*)&out[i], decoded);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    int len = s.length();\n    int i;\n\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    for (i = 0; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i decoded = _mm256_sub_epi8(vec, shift);\n        decoded = _mm256_sub_epi8(decoded, base);\n        decoded = _mm256_add_epi8(decoded, mod);\n        decoded = _mm256_and_si256(decoded, _mm256_set1_epi8(25));\n        decoded = _mm256_add_epi8(decoded, base);\n        _mm256_storeu_si256((__m256i*)&out[i], decoded);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_testz_si256(cmp, cmp) == 0) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_testz_si256(cmp, cmp) == 0) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 registers are 256 bits (32 bytes)\n    int balance = 0;\n\n    // Process the string in chunks of 32 bytes\n    for (size_t i = 0; i < len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_bracket = _mm256_set1_epi8('<');\n        __m256i close_bracket = _mm256_set1_epi8('>');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        // Count the number of set bits in the masks\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    // Process the remaining bytes\n    for (size_t i = len / vec_size * vec_size; i < len; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 registers are 256 bits (32 bytes)\n    __m256i open_bracket = _mm256_set1_epi8('<');\n    __m256i close_bracket = _mm256_set1_epi8('>');\n    __m256i zero = _mm256_setzero_si256();\n    int balance = 0;\n\n    for (size_t i = 0; i < len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    __m256i open_bracket = _mm256_set1_epi8('<');\n    __m256i close_bracket = _mm256_set1_epi8('>');\n    int balance = 0;\n\n    for (size_t i = 0; i < len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    int balance = 0;\n\n    for (size_t i = 0; i < len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    size_t i = 0;\n    int balance = 0;\n\n    // Process full vectors\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_bracket = _mm256_set1_epi8('<');\n        __m256i close_bracket = _mm256_set1_epi8('>');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    size_t i = 0;\n    int balance = 0;\n\n    // Process the string in chunks of 32 bytes\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i open_bracket = _mm256_set1_epi8('(');\n        __m256i close_bracket = _mm256_set1_epi8(')');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    // Process the remaining part of the string\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    if (len % 32 != 0) {\n        return false;\n    }\n\n    __m256i open = _mm256_set1_epi8('(');\n    __m256i close = _mm256_set1_epi8(')');\n    __m256i count = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, open);\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, close);\n\n        __m256i open_count = _mm256_and_si256(open_mask, _mm256_set1_epi8(1));\n        __m256i close_count = _mm256_and_si256(close_mask, _mm256_set1_epi8(-1));\n\n        count = _mm256_add_epi8(count, open_count);\n        count = _mm256_add_epi8(count, close_count);\n    }\n\n    alignas(32) int8_t count_array[32];\n    _mm256_store_si256((__m256i*)count_array, count);\n\n    int total_count = 0;\n    for (int i = 0; i < 32; ++i) {\n        total_count += count_array[i];\n    }\n\n    return total_count == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    size_t i = 0;\n    int balance = 0;\n\n    // Process the string in chunks of 32 bytes\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i open_bracket = _mm256_set1_epi8('(');\n        __m256i close_bracket = _mm256_set1_epi8(')');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    // Process the remaining bytes\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    size_t i = 0;\n    int balance = 0;\n\n    // Process the string in chunks of 32 bytes\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i open_bracket = _mm256_set1_epi8('(');\n        __m256i close_bracket = _mm256_set1_epi8(')');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(vec, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(vec, close_bracket);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n        if (balance < 0) return false;\n    }\n\n    // Process the remaining bytes\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const size_t len = brackets.size();\n    if (len % 32 != 0) {\n        return false;\n    }\n\n    __m256i open_bracket = _mm256_set1_epi8('(');\n    __m256i close_bracket = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = zero;\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, open_bracket);\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, close_bracket);\n\n        __m256i open_count = _mm256_and_si256(open_mask, _mm256_set1_epi8(1));\n        __m256i close_count = _mm256_and_si256(close_mask, _mm256_set1_epi8(-1));\n\n        balance = _mm256_add_epi8(balance, open_count);\n        balance = _mm256_add_epi8(balance, close_count);\n    }\n\n    int32_t balance_sum = 0;\n    for (int i = 0; i < 32; ++i) {\n        balance_sum += ((int8_t*)&balance)[i];\n    }\n\n    return balance_sum == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 multipliers = _mm256_cvtepi32_ps(_mm256_add_epi32(indices, _mm256_set1_epi32(i + 1)));\n        __m256 derivatives = _mm256_mul_ps(coeffs, multipliers);\n\n        _mm256_storeu_ps(&result[i], derivatives);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256 scale = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 scaled_indices = _mm256_cvtepi32_ps(_mm256_add_epi32(indices, _mm256_set1_epi32(i)));\n        __m256 derivatives = _mm256_mul_ps(coeffs, scaled_indices);\n        _mm256_storeu_ps(&result[i], derivatives);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n    int n = xs.size() - 1;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);\n        __m256 products = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], products);\n    }\n\n    for (int i = (n / 8) * 8; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n    int n = xs.size() - 1;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);\n        __m256 products = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], products);\n    }\n\n    for (int i = (n / 8) * 8; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n    int n = xs.size();\n\n    for (int i = 0; i < n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_set_ps(8.0f, 7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f);\n        __m256 prod = _mm256_mul_ps(coeffs, indices);\n\n        _mm256_storeu_ps(&result[i], prod);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i count = _mm256_setzero_si256();\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower = _mm256_set1_epi8(0x20);\n        chunk = _mm256_or_si256(chunk, lower);\n\n        __m256i mask = _mm256_setzero_si256();\n        for (int j = 0; j < 10; ++j) {\n            __m256i cmp = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(vowels[j]));\n            mask = _mm256_or_si256(mask, cmp);\n        }\n\n        count = _mm256_add_epi8(count, mask);\n    }\n\n    int total_count = 0;\n    for (int j = 0; j < 32; ++j) {\n        total_count += _mm256_extract_epi8(count, j) & 1;\n    }\n\n    for (; i < length; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == length - 1)) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\nint vowels_count_simd(std::string s) {\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i count = _mm256_setzero_si256();\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower_chunk = _mm256_or_si256(chunk, _mm256_set1_epi8(0x20)); // Convert to lowercase\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('a'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('e'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('i'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('o'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('u'));\n        __m256i cmp6 = _mm256_and_si256(_mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('y')), _mm256_cmpeq_epi8(_mm256_set1_epi8(s[i + 31]), _mm256_set1_epi8(0)));\n\n        __m256i mask = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)), cmp5), cmp6);\n        count = _mm256_sub_epi8(count, mask);\n    }\n\n    int result = _mm256_extract_epi8(count, 0) + _mm256_extract_epi8(count, 8) + _mm256_extract_epi8(count, 16) + _mm256_extract_epi8(count, 24);\n\n    for (; i < length; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == length - 1)) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\nint vowels_count_simd(std::string s) {\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i count = _mm256_setzero_si256();\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower_chunk = _mm256_or_si256(chunk, _mm256_set1_epi8(0x20)); // Convert to lowercase\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('a'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('e'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('i'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('o'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('u'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('y'));\n\n        __m256i mask = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)), cmp5), cmp6);\n        count = _mm256_sub_epi8(count, mask);\n    }\n\n    int result = _mm256_movemask_epi8(count);\n    result = _mm_popcnt_u32(result);\n\n    for (; i < length; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == length - 1)) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\nint vowels_count_simd(std::string s) {\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i count = _mm256_setzero_si256();\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower_chunk = _mm256_or_si256(chunk, _mm256_set1_epi8(0x20));\n        __m256i cmp = _mm256_cmpeq_epi8(lower_chunk, vowels);\n        count = _mm256_add_epi8(count, cmp);\n    }\n\n    int result = _mm256_extract_epi8(count, 0) + _mm256_extract_epi8(count, 1) + _mm256_extract_epi8(count, 2) + _mm256_extract_epi8(count, 3) + _mm256_extract_epi8(count, 4);\n\n    for (; i < length; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == length - 1)) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\nint vowels_count_simd(std::string s) {\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i count = _mm256_setzero_si256();\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i lower = _mm256_and_si256(chunk, _mm256_set1_epi8(0xDF)); // Convert to lowercase\n        __m256i cmp = _mm256_cmpeq_epi8(lower, vowels);\n        count = _mm256_sub_epi8(count, cmp);\n    }\n\n    int result = _mm256_movemask_epi8(count);\n    result = _mm_popcnt_u32(result);\n\n    for (; i < length; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == length - 1)) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\nint digitSum_simd(std::string s) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0x20);\n    __m256i ascii_mask = _mm256_set1_epi8(0x7F);\n\n    int len = s.length();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper_chunk = _mm256_andnot_si256(upper_mask, chunk);\n        __m256i is_upper = _mm256_cmpeq_epi8(upper_chunk, chunk);\n        __m256i ascii_values = _mm256_and_si256(chunk, ascii_mask);\n        __m256i masked_values = _mm256_and_si256(ascii_values, is_upper);\n        sum = _mm256_add_epi8(sum, masked_values);\n    }\n\n    int result = 0;\n    for (; i < len; ++i) {\n        if (isupper(s[i])) {\n            result += s[i];\n        }\n    }\n\n    int temp[32];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    for (int j = 0; j < 32; ++j) {\n        result += temp[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\nint digitSum_simd(std::string s) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_bound = _mm256_set1_epi8('A' - 1);\n    __m256i lower_bound = _mm256_set1_epi8('Z' + 1);\n\n    int length = s.length();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_bound), _mm256_cmpgt_epi8(lower_bound, chunk));\n        __m256i masked_chunk = _mm256_and_si256(chunk, is_upper);\n        sum = _mm256_add_epi8(sum, masked_chunk);\n    }\n\n    int result = 0;\n    for (int j = 0; j < 32; ++j) {\n        result += ((char*)&sum)[j];\n    }\n\n    for (; i < length; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            result += s[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\nint digitSum_simd(std::string s) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_bound = _mm256_set1_epi8('Z');\n    __m256i lower_bound = _mm256_set1_epi8('A');\n    __m256i mask = _mm256_set1_epi8(0x80);\n\n    int len = s.length();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, lower_bound), _mm256_cmpgt_epi8(upper_bound, vec));\n        __m256i filtered = _mm256_and_si256(vec, upper_mask);\n        sum = _mm256_add_epi8(sum, filtered);\n    }\n\n    alignas(32) int buffer[8] = {0};\n    _mm256_store_si256((__m256i*)buffer, sum);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += buffer[j];\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            total_sum += s[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\nint digitSum_simd(std::string s) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_bound = _mm256_set1_epi8('Z');\n    __m256i lower_bound = _mm256_set1_epi8('A' - 1);\n\n    int len = s.length();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, lower_bound), _mm256_cmpgt_epi8(upper_bound, vec));\n        __m256i filtered = _mm256_and_si256(vec, mask);\n        sum = _mm256_add_epi8(sum, filtered);\n    }\n\n    alignas(32) char buffer[32] = {0};\n    for (; i < len; ++i) {\n        buffer[i % 32] = s[i];\n    }\n    __m256i vec = _mm256_load_si256((__m256i*)buffer);\n    __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, lower_bound), _mm256_cmpgt_epi8(upper_bound, vec));\n    __m256i filtered = _mm256_and_si256(vec, mask);\n    sum = _mm256_add_epi8(sum, filtered);\n\n    int result = 0;\n    for (int j = 0; j < 32; ++j) {\n        result += ((char*)&sum)[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\nint digitSum_simd(std::string s) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8('A' - 1);\n    __m256i lower_mask = _mm256_set1_epi8('Z' + 1);\n\n    size_t len = s.length();\n    size_t i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper = _mm256_cmpgt_epi8(vec, upper_mask);\n        __m256i lower = _mm256_cmpgt_epi8(lower_mask, vec);\n        __m256i mask = _mm256_and_si256(upper, lower);\n        __m256i masked_vec = _mm256_and_si256(vec, mask);\n        sum = _mm256_add_epi8(sum, masked_vec);\n    }\n\n    alignas(32) char buffer[32] = {0};\n    for (; i < len; ++i) {\n        buffer[i % 32] = s[i];\n    }\n    __m256i vec = _mm256_load_si256((__m256i*)buffer);\n    __m256i upper = _mm256_cmpgt_epi8(vec, upper_mask);\n    __m256i lower = _mm256_cmpgt_epi8(lower_mask, vec);\n    __m256i mask = _mm256_and_si256(upper, lower);\n    __m256i masked_vec = _mm256_and_si256(vec, mask);\n    sum = _mm256_add_epi8(sum, masked_vec);\n\n    int result = 0;\n    alignas(32) int sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum);\n    for (int j = 0; j < 8; ++j) {\n        result += sum_array[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    const int size = arr.size();\n    const int vec_size = 8; // AVX2 operates on 256-bit registers, which is 8 integers\n    const int max_index = std::numeric_limits<int>::max();\n\n    __m256i min_val = _mm256_set1_epi32(max_index);\n    __m256i min_idx = _mm256_set1_epi32(max_index);\n\n    for (int i = 0; i < size; i += vec_size) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i even_mask = _mm256_and_si256(values, _mm256_set1_epi32(1));\n        even_mask = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        __m256i masked_values = _mm256_blendv_epi8(_mm256_set1_epi32(max_index), values, even_mask);\n        __m256i masked_indices = _mm256_blendv_epi8(_mm256_set1_epi32(max_index), indices, even_mask);\n\n        __m256i cmp = _mm256_cmpgt_epi32(min_val, masked_values);\n        min_val = _mm256_blendv_epi8(min_val, masked_values, cmp);\n        min_idx = _mm256_blendv_epi8(min_idx, masked_indices, cmp);\n    }\n\n    alignas(32) int min_vals[8];\n    alignas(32) int min_idxs[8];\n    _mm256_store_si256((__m256i*)min_vals, min_val);\n    _mm256_store_si256((__m256i*)min_idxs, min_idx);\n\n    int smallest_val = max_index;\n    int smallest_idx = max_index;\n    for (int i = 0; i < vec_size; ++i) {\n        if (min_vals[i] < smallest_val) {\n            smallest_val = min_vals[i];\n            smallest_idx = min_idxs[i];\n        }\n    }\n\n    if (smallest_val == max_index) return {};\n    return {smallest_val, smallest_idx};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    const int size = arr.size();\n    const int vec_size = 8; // AVX2 operates on 256-bit registers, which is 8 integers\n    const int mask = 0xAA; // Mask to identify even numbers (0xAA = 10101010 in binary)\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i min_idx = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    for (int i = 0; i < size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm256_set1_epi32(mask);\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n\n        __m256i cmp = _mm256_cmpeq_epi32(even_vec, _mm256_setzero_si256());\n        __m256i idx = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i + 0);\n\n        __m256i new_min_val = _mm256_min_epi32(min_val, vec);\n        __m256i new_min_idx = _mm256_blendv_epi8(min_idx, idx, _mm256_cmpeq_epi32(new_min_val, vec));\n\n        min_val = _mm256_blendv_epi8(min_val, new_min_val, cmp);\n        min_idx = _mm256_blendv_epi8(min_idx, new_min_idx, cmp);\n    }\n\n    int min_val_arr[8];\n    int min_idx_arr[8];\n    _mm256_storeu_si256((__m256i*)min_val_arr, min_val);\n    _mm256_storeu_si256((__m256i*)min_idx_arr, min_idx);\n\n    int smallest_val = std::numeric_limits<int>::max();\n    int smallest_idx = std::numeric_limits<int>::max();\n\n    for (int i = 0; i < vec_size; ++i) {\n        if (min_val_arr[i] < smallest_val && (min_val_arr[i] & 1) == 0) {\n            smallest_val = min_val_arr[i];\n            smallest_idx = min_idx_arr[i];\n        }\n    }\n\n    if (smallest_val == std::numeric_limits<int>::max()) {\n        return {};\n    }\n\n    return {smallest_val, smallest_idx};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even_value = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    __m256i min_even_value_vec = _mm256_set1_epi32(min_even_value);\n    __m256i min_index_vec = _mm256_set1_epi32(-1);\n    __m256i even_mask = _mm256_set1_epi32(1);\n\n    int size = arr.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i is_even = _mm256_and_si256(values, even_mask);\n        is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        __m256i even_values = _mm256_and_si256(values, is_even);\n        __m256i even_indices = _mm256_and_si256(indices, is_even);\n\n        __m256i cmp = _mm256_cmpgt_epi32(min_even_value_vec, even_values);\n        min_even_value_vec = _mm256_blendv_epi8(min_even_value_vec, even_values, cmp);\n        min_index_vec = _mm256_blendv_epi8(min_index_vec, even_indices, cmp);\n    }\n\n    alignas(32) int min_even_values[8];\n    alignas(32) int min_indices[8];\n    _mm256_store_si256((__m256i*)min_even_values, min_even_value_vec);\n    _mm256_store_si256((__m256i*)min_indices, min_index_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (min_even_values[j] < min_even_value) {\n            min_even_value = min_even_values[j];\n            min_index = min_indices[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    const int size = arr.size();\n    const int vec_size = 8; // AVX2 operates on 256-bit registers, which is 8 integers\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    __m256i min_vec = _mm256_set1_epi32(min_even);\n    __m256i index_vec = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm256_and_si256(data, mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        __m256i current_indices = _mm256_add_epi32(index_vec, _mm256_set1_epi32(i));\n        __m256i masked_data = _mm256_and_si256(data, is_even);\n        __m256i masked_indices = _mm256_and_si256(current_indices, is_even);\n\n        __m256i min_data = _mm256_min_epi32(min_vec, masked_data);\n        __m256i min_indices = _mm256_blendv_epi8(masked_indices, current_indices, _mm256_cmpeq_epi32(min_data, min_vec));\n\n        min_vec = min_data;\n        index_vec = min_indices;\n    }\n\n    alignas(32) int indices[8];\n    alignas(32) int values[8];\n    _mm256_store_si256((__m256i*)indices, index_vec);\n    _mm256_store_si256((__m256i*)values, min_vec);\n\n    for (int i = 0; i < vec_size; ++i) {\n        if (values[i] < min_even && values[i] != 0) {\n            min_even = values[i];\n            min_index = indices[i];\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even_value = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    __m256i min_even_value_vec = _mm256_set1_epi32(min_even_value);\n    __m256i min_index_vec = _mm256_set1_epi32(min_index);\n    __m256i even_mask = _mm256_set1_epi32(1);\n\n    int size = arr.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i is_even = _mm256_and_si256(values, even_mask);\n        is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        __m256i even_values = _mm256_and_si256(values, is_even);\n        __m256i even_indices = _mm256_and_si256(indices, is_even);\n\n        __m256i min_even_values = _mm256_min_epi32(min_even_value_vec, even_values);\n        __m256i min_even_indices = _mm256_blendv_epi8(min_index_vec, even_indices, _mm256_cmpeq_epi32(min_even_value_vec, min_even_values));\n\n        min_even_value_vec = min_even_values;\n        min_index_vec = min_even_indices;\n    }\n\n    alignas(32) int min_even_values[8];\n    alignas(32) int min_even_indices[8];\n    _mm256_store_si256((__m256i*)min_even_values, min_even_value_vec);\n    _mm256_store_si256((__m256i*)min_even_indices, min_index_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (min_even_values[j] < min_even_value) {\n            min_even_value = min_even_values[j];\n            min_index = min_even_indices[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even_value, min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int vec_size = 8;\n    const int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_val + 1, 0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < vec_size && i + j < lst.size(); ++j) {\n            freq[_mm256_extract_epi32(vec, j)]++;\n        }\n    }\n\n    for (int i = max_val; i > 0; --i) {\n        if (freq[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int vec_size = 8;\n    const int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> count(max_val + 1, 0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < vec_size; ++j) {\n            int val = _mm256_extract_epi32(vec, j);\n            if (val > 0 && val <= max_val) {\n                count[val]++;\n            }\n        }\n    }\n\n    for (int i = max_val; i > 0; --i) {\n        if (count[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int SIMD_WIDTH = 8;\n    const int MAX_VALUE = 256;\n    std::vector<int> count(MAX_VALUE, 0);\n\n    for (size_t i = 0; i < lst.size(); i += SIMD_WIDTH) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < SIMD_WIDTH; ++j) {\n            int val = _mm256_extract_epi32(vec, j);\n            if (val < MAX_VALUE) {\n                count[val]++;\n            }\n        }\n    }\n\n    int result = -1;\n    for (int i = 1; i < MAX_VALUE; ++i) {\n        if (count[i] >= i) {\n            result = std::max(result, i);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int vec_size = 8;\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> count(max_val + 1, 0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < vec_size; ++j) {\n            int val = _mm256_extract_epi32(vec, j);\n            if (val > 0) {\n                count[val]++;\n            }\n        }\n    }\n\n    for (int i = max_val; i > 0; --i) {\n        if (count[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int vec_size = 8;\n    const int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> count(max_val + 1, 0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < vec_size; ++j) {\n            int val = _mm256_extract_epi32(vec, j);\n            if (val > 0) {\n                count[val]++;\n            }\n        }\n    }\n\n    for (int i = max_val; i > 0; --i) {\n        if (count[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int sum = 0;\n    bool is_palindrome = true;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i left_vec, right_vec;\n\n    for (int i = 0; i < n / 8; ++i) {\n        left_vec = _mm256_loadu_si256((__m256i*)&q[i * 8]);\n        right_vec = _mm256_loadu_si256((__m256i*)&q[n - (i + 1) * 8]);\n\n        sum_vec = _mm256_add_epi32(sum_vec, left_vec);\n\n        if (!_mm256_testz_si256(left_vec, right_vec)) {\n            is_palindrome = false;\n        }\n    }\n\n    sum_vec = _mm256_hadd_epi32(sum_vec, sum_vec);\n    sum_vec = _mm256_hadd_epi32(sum_vec, sum_vec);\n    sum = _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 4);\n\n    for (int i = (n / 8) * 8; i < n; ++i) {\n        sum += q[i];\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n        }\n    }\n\n    return is_palindrome && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i left_vec = _mm256_setzero_si256();\n    __m256i right_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        if (i + 8 < n) {\n            __m256i rev_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n            left_vec = _mm256_or_si256(left_vec, _mm256_slli_epi32(vec, 16));\n            right_vec = _mm256_or_si256(right_vec, _mm256_slli_epi32(rev_vec, 16));\n        }\n    }\n\n    int sum = _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 1) +\n              _mm256_extract_epi32(sum_vec, 2) + _mm256_extract_epi32(sum_vec, 3) +\n              _mm256_extract_epi32(sum_vec, 4) + _mm256_extract_epi32(sum_vec, 5) +\n              _mm256_extract_epi32(sum_vec, 6) + _mm256_extract_epi32(sum_vec, 7);\n\n    bool is_balanced = _mm256_testz_si256(left_vec, right_vec);\n\n    return is_balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i left = _mm256_setzero_si256();\n    __m256i right = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum = _mm256_add_epi32(sum, vec);\n\n        if (i + 8 < n) {\n            __m256i rev = _mm256_permute2x128_si256(vec, vec, 1);\n            rev = _mm256_shuffle_epi32(rev, _MM_SHUFFLE(0, 1, 2, 3));\n            left = _mm256_blend_epi32(left, vec, 0xF0);\n            right = _mm256_blend_epi32(right, rev, 0x0F);\n        }\n    }\n\n    int total_sum = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                    _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                    _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                    _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n    bool is_balanced = true;\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_balanced = false;\n            break;\n        }\n    }\n\n    return is_balanced && total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i reverse_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        __m256i rev_vec = _mm256_permute2x128_si256(vec, vec, 1);\n        rev_vec = _mm256_shuffle_epi32(rev_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        reverse_vec = _mm256_or_si256(reverse_vec, rev_vec);\n    }\n\n    int sum = _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 1) +\n              _mm256_extract_epi32(sum_vec, 2) + _mm256_extract_epi32(sum_vec, 3) +\n              _mm256_extract_epi32(sum_vec, 4) + _mm256_extract_epi32(sum_vec, 5) +\n              _mm256_extract_epi32(sum_vec, 6) + _mm256_extract_epi32(sum_vec, 7);\n\n    if (sum > w) return false;\n\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i left_vec = _mm256_setzero_si256();\n    __m256i right_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        if (i < n / 2) {\n            left_vec = _mm256_add_epi32(left_vec, vec);\n        }\n        if (i >= n / 2) {\n            right_vec = _mm256_add_epi32(right_vec, vec);\n        }\n    }\n\n    int sum = _mm256_reduce_add_epi32(sum_vec);\n    int left_sum = _mm256_reduce_add_epi32(left_vec);\n    int right_sum = _mm256_reduce_add_epi32(right_vec);\n\n    bool is_balanced = (left_sum == right_sum);\n    bool is_within_weight = (sum <= w);\n\n    return is_balanced && is_within_weight;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    for (; i <= n - 1 - i; ++i) {\n        __m256i left = _mm256_set1_epi32(arr[i]);\n        __m256i right = _mm256_set1_epi32(arr[n - 1 - i]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0xFFFFFFFF) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    for (; i <= n - 1 - i; i++) {\n        __m256i vec1 = _mm256_set1_epi32(arr[i]);\n        __m256i vec2 = _mm256_set1_epi32(arr[n - 1 - i]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0xFFFFFFFF) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    for (; i <= n - 1 - i; ++i) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0xFFFFFFFF) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i++) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        changes += __builtin_popcount(~mask & 0xFFFFFFFF);\n    }\n\n    if (n % 2 != 0) {\n        changes += (arr[n / 2] != arr[n / 2 - 1]);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    for (; i <= n - 1 - i; ++i) {\n        __m256i left = _mm256_set1_epi32(arr[i]);\n        __m256i right = _mm256_set1_epi32(arr[n - 1 - i]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0xFFFFFFFF) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int total1 = 0, total2 = 0;\n\n    for (const auto& str : lst1) {\n        total1 += str.size();\n    }\n\n    for (const auto& str : lst2) {\n        total2 += str.size();\n    }\n\n    return (total1 <= total2) ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int total1 = 0, total2 = 0;\n\n    for (const auto& str : lst1) {\n        total1 += str.size();\n    }\n\n    for (const auto& str : lst2) {\n        total2 += str.size();\n    }\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int total1 = 0, total2 = 0;\n\n    for (const auto& str : lst1) {\n        total1 += str.size();\n    }\n\n    for (const auto& str : lst2) {\n        total2 += str.size();\n    }\n\n    return total1 <= total2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto count_chars = [](const std::vector<std::string>& lst) {\n        __m256i total = _mm256_setzero_si256();\n        for (const auto& str : lst) {\n            size_t len = str.size();\n            __m256i len_vec = _mm256_set1_epi32(static_cast<int>(len));\n            total = _mm256_add_epi32(total, len_vec);\n        }\n        int sum[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum), total);\n        return sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n    };\n\n    int total1 = count_chars(lst1);\n    int total2 = count_chars(lst2);\n\n    return total1 <= total2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int total1 = 0, total2 = 0;\n\n    for (const auto& str : lst1) {\n        total1 += str.size();\n    }\n\n    for (const auto& str : lst2) {\n        total2 += str.size();\n    }\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 1) return x == 1;\n    if (x == 1) return n == 1;\n    if (n == 0) return x == 1;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_result = vec_one;\n\n    while (_mm256_testz_si256(vec_result, vec_result) == 0) {\n        __m256i vec_temp = _mm256_mul_epu32(vec_result, vec_n);\n        vec_result = _mm256_blend_epi32(vec_result, vec_temp, 0xAA);\n        vec_result = _mm256_blend_epi32(vec_result, _mm256_srli_epi64(vec_temp, 32), 0x55);\n        vec_result = _mm256_cmpeq_epi64(vec_result, vec_x);\n    }\n\n    return _mm256_testz_si256(vec_result, vec_result) == 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true;\n    if (n == 1) return x == 1;\n    if (n == 0) return x == 1;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_result = vec_one;\n\n    while (true) {\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n        __m256i cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask == 0xFFFFFFFF) return true;\n        if (_mm256_testz_si256(cmp, cmp)) return false;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 1) return x == 1;\n    if (x == 1) return n == 1;\n    if (n == 0) return x == 1;\n    if (x == 0) return false;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i ones = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n\n    __m256i current = ones;\n    __m256i mask = _mm256_cmpeq_epi64(vx, current);\n\n    while (_mm256_testz_si256(mask, mask) && _mm256_movemask_epi8(_mm256_cmpgt_epi64(current, vx)) == 0) {\n        current = _mm256_mullo_epi64(current, vn);\n        mask = _mm256_cmpeq_epi64(vx, current);\n    }\n\n    return _mm256_movemask_epi8(mask) != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true;\n    if (n == 1) return x == 1;\n    if (n == 0) return x == 1;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_power = _mm256_set1_epi64x(1);\n\n    while (true) {\n        vec_power = _mm256_mullo_epi64(vec_power, vec_n);\n        __m256i cmp = _mm256_cmpeq_epi64(vec_power, vec_x);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask == 0xFFFFFFFFFFFFFFFF) return true;\n        if (_mm256_testz_si256(vec_power, vec_power)) return false;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true;\n    if (n == 1) return x == 1;\n    if (n == 0) return x == 1;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_power = _mm256_set1_epi64x(1);\n\n    while (true) {\n        __m256i vec_mul = _mm256_mullo_epi64(vec_power, vec_n);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_x);\n\n        if (_mm256_testz_si256(vec_cmp, vec_cmp) == 0) {\n            return true;\n        }\n\n        vec_power = vec_mul;\n        __m256i vec_overflow = _mm256_cmpgt_epi64(vec_power, vec_x);\n        if (_mm256_testz_si256(vec_overflow, vec_overflow) == 0) {\n            return false;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m256i prime_mask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0\n    );\n\n    int count = 0;\n    int len = num.size();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&num[i]);\n        __m256i mask = _mm256_cmpeq_epi8(vec, prime_mask);\n        count += _mm256_movemask_epi8(mask);\n    }\n\n    for (; i < len; ++i) {\n        if (num[i] == '2' || num[i] == '3' || num[i] == '5' || num[i] == '7' ||\n            num[i] == 'B' || num[i] == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m256i prime_mask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    int count = 0;\n    int length = num.size();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&num[i]);\n        __m256i mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('2'));\n        mask = _mm256_or_si256(mask, _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('3')));\n        mask = _mm256_or_si256(mask, _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('5')));\n        mask = _mm256_or_si256(mask, _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('7')));\n        mask = _mm256_or_si256(mask, _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('B')));\n        mask = _mm256_or_si256(mask, _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('D')));\n\n        mask = _mm256_and_si256(mask, prime_mask);\n\n        count += _mm256_movemask_epi8(mask);\n    }\n\n    for (; i < length; ++i) {\n        if ((num[i] == '2' || num[i] == '3' || num[i] == '5' || num[i] == '7' ||\n             num[i] == 'B' || num[i] == 'D') && (i % 2 == 0 || num[i] != '0')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m256i prime_mask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    int count = 0;\n    int length = num.size();\n    int i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&num[i]);\n        __m256i mask = _mm256_set1_epi8(0x0F);\n        __m256i low_nibble = _mm256_and_si256(vec, mask);\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi64(vec, 4), mask);\n\n        __m256i low_result = _mm256_and_si256(_mm256_shuffle_epi8(prime_mask, low_nibble), _mm256_cmpeq_epi8(low_nibble, low_nibble));\n        __m256i high_result = _mm256_and_si256(_mm256_shuffle_epi8(prime_mask, high_nibble), _mm256_cmpeq_epi8(high_nibble, high_nibble));\n\n        __m256i result = _mm256_or_si256(low_result, high_result);\n        count += _mm256_movemask_epi8(result);\n    }\n\n    for (; i < length; ++i) {\n        char c = num[i];\n        if (c >= '0' && c <= '9') {\n            if (c == '2' || c == '3' || c == '5' || c == '7') {\n                ++count;\n            }\n        } else if (c >= 'A' && c <= 'F') {\n            if (c == 'B' || c == 'D') {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m256i prime_mask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 0, 0, 1,\n        0, 0, 1, 0, 1, 0, 0, 0,\n        0, 0, 1, 1, 0, 0, 0, 1,\n        0, 0, 1, 0, 1, 0, 0, 0\n    );\n\n    int count = 0;\n    int len = num.size();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&num[i]);\n        __m256i mask = _mm256_cmpeq_epi8(vec, prime_mask);\n        count += _mm256_movemask_epi8(mask);\n    }\n\n    for (; i < len; ++i) {\n        if (num[i] == '2' || num[i] == '3' || num[i] == '5' || num[i] == '7' ||\n            num[i] == 'B' || num[i] == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m256i primes = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i zero = _mm256_setzero_si256();\n    int count = 0;\n\n    for (size_t i = 0; i < num.size(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&num[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, primes);\n        __m256i mask = _mm256_cmpgt_epi8(cmp, zero);\n        count += _mm256_movemask_epi8(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) {\n        return false;\n    }\n\n    const size_t len = s.length();\n    const size_t vec_size = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    const size_t num_chunks = len / vec_size;\n    const size_t remainder = len % vec_size;\n\n    for (size_t i = 0; i < num_chunks; ++i) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size]));\n        for (size_t j = 0; j < vec_size - 2; ++j) {\n            __m256i vec1 = _mm256_slli_si256(vec, j);\n            __m256i vec2 = _mm256_slli_si256(vec, j + 1);\n            __m256i vec3 = _mm256_slli_si256(vec, j + 2);\n            __m256i cmp1 = _mm256_cmpeq_epi8(vec1, vec2);\n            __m256i cmp2 = _mm256_cmpeq_epi8(vec1, vec3);\n            __m256i cmp3 = _mm256_cmpeq_epi8(vec2, vec3);\n            __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n            if (_mm256_testz_si256(result, result) == 0) {\n                return false;\n            }\n        }\n    }\n\n    for (size_t i = num_chunks * vec_size; i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) {\n        return false;\n    }\n\n    const size_t len = s.length();\n    const size_t vec_size = 32; // AVX2 registers are 256 bits (32 bytes)\n    const size_t num_vectors = (len - 2) / vec_size + 1;\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i * vec_size));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i * vec_size + 1));\n        __m256i vec3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i * vec_size + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(vec1, vec2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(vec1, vec3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(vec2, vec3);\n\n        __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        int mask = _mm256_movemask_epi8(result);\n\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) {\n        return false;\n    }\n\n    const size_t len = s.length();\n    const size_t vec_size = 32; // AVX2 registers are 256 bits (32 bytes)\n    const size_t num_vectors = len / vec_size;\n    const size_t remainder = len % vec_size;\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size]));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size + 1]));\n        __m256i vec3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size + 2]));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(vec1, vec2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(vec1, vec3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(vec2, vec3);\n\n        __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        if (_mm256_testz_si256(result, result) == 0) {\n            return false;\n        }\n    }\n\n    for (size_t i = num_vectors * vec_size; i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) {\n        return false;\n    }\n\n    const size_t len = s.length();\n    const size_t vec_size = 32; // AVX2 registers are 256 bits wide, 32 bytes\n    const size_t num_vecs = len / vec_size;\n    const size_t remainder = len % vec_size;\n\n    for (size_t i = 0; i < num_vecs; ++i) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size]));\n        for (size_t j = 0; j < vec_size - 2; ++j) {\n            __m256i a = _mm256_set1_epi8(s[i * vec_size + j]);\n            __m256i b = _mm256_set1_epi8(s[i * vec_size + j + 1]);\n            __m256i c = _mm256_set1_epi8(s[i * vec_size + j + 2]);\n\n            __m256i cmp1 = _mm256_cmpeq_epi8(a, b);\n            __m256i cmp2 = _mm256_cmpeq_epi8(a, c);\n            __m256i cmp3 = _mm256_cmpeq_epi8(b, c);\n\n            __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n            if (_mm256_movemask_epi8(result) != 0) {\n                return false;\n            }\n        }\n    }\n\n    for (size_t i = num_vecs * vec_size; i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) {\n        return false;\n    }\n\n    const int len = s.length();\n    const int vec_size = 32; // AVX2 registers are 256 bits wide, so 32 bytes\n    const int num_vecs = (len + vec_size - 1) / vec_size;\n\n    for (int i = 0; i < num_vecs; ++i) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i * vec_size]);\n\n        for (int j = 0; j < vec_size - 2; ++j) {\n            __m256i vec1 = _mm256_slli_si256(vec, j);\n            __m256i vec2 = _mm256_slli_si256(vec, j + 1);\n            __m256i vec3 = _mm256_slli_si256(vec, j + 2);\n\n            __m256i cmp1 = _mm256_cmpeq_epi8(vec1, vec2);\n            __m256i cmp2 = _mm256_cmpeq_epi8(vec1, vec3);\n            __m256i cmp3 = _mm256_cmpeq_epi8(vec2, vec3);\n\n            __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n\n            if (_mm256_testz_si256(result, result) == 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if (length % 2 == 0) return false;\n\n    int limit = static_cast<int>(std::sqrt(length)) + 1;\n    __m256i vec = _mm256_set1_epi32(1);\n    __m256i mask = _mm256_set1_epi32(0);\n\n    for (int i = 3; i <= limit; i += 8) {\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mod = _mm256_div_epi32(_mm256_set1_epi32(length), indices);\n        __m256i is_zero = _mm256_cmpeq_epi32(mod, _mm256_setzero_si256());\n        mask = _mm256_or_si256(mask, is_zero);\n    }\n\n    int mask_result = _mm256_movemask_epi8(mask);\n    return mask_result == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if (length % 2 == 0) return false;\n\n    __m256i vec = _mm256_set1_epi32(length);\n    __m256i div = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i mod = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < 8; ++i) {\n        __m256i temp = _mm256_div_epi32(vec, div);\n        temp = _mm256_mullo_epi32(temp, div);\n        temp = _mm256_sub_epi32(vec, temp);\n        mod = _mm256_or_si256(mod, temp);\n    }\n\n    int mask = _mm256_movemask_epi8(mod);\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if (length % 2 == 0) return false;\n\n    int limit = static_cast<int>(std::sqrt(length)) + 1;\n    __m256i vec = _mm256_set1_epi32(1);\n    __m256i mask = _mm256_set1_epi32(0);\n\n    for (int i = 3; i <= limit; i += 8) {\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mod = _mm256_div_epi32(_mm256_set1_epi32(length), indices);\n        __m256i is_zero = _mm256_cmpeq_epi32(mod, _mm256_setzero_si256());\n        mask = _mm256_or_si256(mask, is_zero);\n    }\n\n    int mask_result = _mm256_movemask_epi8(mask);\n    return mask_result == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2 || length == 3) return true;\n    if (length % 2 == 0 || length % 3 == 0) return false;\n\n    __m256i v_length = _mm256_set1_epi32(length);\n    __m256i v_i = _mm256_set_epi32(15, 13, 11, 9, 7, 5, 3, 1);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (int i = 1; i <= length / 16; ++i) {\n        __m256i v_rem = _mm256_div_epi32(v_length, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_rem, v_zero);\n        int mask = _mm256_movemask_epi8(v_cmp);\n        if (mask != 0) return false;\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(16));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if (length % 2 == 0) return false;\n\n    __m256i v = _mm256_set1_epi32(length);\n    __m256i i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i inc = _mm256_set1_epi32(8);\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (int j = 0; j < length; j += 8) {\n        __m256i mod = _mm256_div_epi32(v, _mm256_add_epi32(i, _mm256_set1_epi32(j)));\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(mod, zero);\n        int mask_result = _mm256_movemask_epi8(cmp);\n        if (mask_result != 0) return false;\n        i = _mm256_add_epi32(i, inc);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vec = _mm256_set1_epi64x(N);\n    __m256i mask = _mm256_set1_epi64x(0x0101010101010101);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < 64; i += 8) {\n        __m256i shifted = _mm256_srli_epi64(vec, i);\n        __m256i anded = _mm256_and_si256(shifted, mask);\n        sum = _mm256_add_epi8(sum, anded);\n    }\n\n    uint64_t result = 0;\n    for (int i = 0; i < 32; i += 8) {\n        result += _mm256_extract_epi64(sum, i / 8);\n    }\n\n    std::string binary;\n    while (result) {\n        binary = (result % 2 ? \"1\" : \"0\") + binary;\n        result /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vec = _mm256_set1_epi64x(N);\n    __m256i mask = _mm256_set1_epi64x(0x0101010101010101);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < 64; i += 8) {\n        __m256i digits = _mm256_and_si256(vec, mask);\n        sum = _mm256_add_epi8(sum, digits);\n        vec = _mm256_srli_epi64(vec, 8);\n    }\n\n    uint64_t total_sum = 0;\n    for (int i = 0; i < 4; ++i) {\n        total_sum += _mm256_extract_epi64(sum, i);\n    }\n\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = (total_sum % 2 == 0 ? \"0\" : \"1\") + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x0F);\n\n    while (N) {\n        __m256i digits = _mm256_set1_epi64x(N);\n        __m256i low = _mm256_and_si256(digits, mask);\n        __m256i high = _mm256_and_si256(_mm256_srli_epi64(digits, 4), mask);\n        sum = _mm256_add_epi8(sum, low);\n        sum = _mm256_add_epi8(sum, high);\n        N >>= 8;\n    }\n\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    uint64_t total_sum = 0;\n    for (int i = 0; i < 32; ++i) {\n        total_sum += temp[i];\n    }\n\n    std::string binary;\n    while (total_sum) {\n        binary = (total_sum % 2 ? \"1\" : \"0\") + binary;\n        total_sum /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x0F);\n\n    while (N) {\n        __m256i digits = _mm256_set1_epi64x(N);\n        digits = _mm256_and_si256(digits, mask);\n        sum = _mm256_add_epi8(sum, digits);\n        N >>= 4;\n    }\n\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n\n    uint64_t total_sum = 0;\n    for (int i = 0; i < 32; ++i) {\n        total_sum += temp[i];\n    }\n\n    std::string binary_sum;\n    while (total_sum) {\n        binary_sum = (total_sum % 2 ? \"1\" : \"0\") + binary_sum;\n        total_sum /= 2;\n    }\n\n    return binary_sum.empty() ? \"0\" : binary_sum;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vec = _mm256_set1_epi64x(N);\n    __m256i mask = _mm256_set1_epi64x(0x0101010101010101);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < 64; i += 8) {\n        __m256i shifted = _mm256_srli_epi64(vec, i);\n        __m256i anded = _mm256_and_si256(shifted, mask);\n        sum = _mm256_add_epi8(sum, anded);\n    }\n\n    uint64_t result = _mm256_extract_epi64(sum, 0) + _mm256_extract_epi64(sum, 1) +\n                      _mm256_extract_epi64(sum, 2) + _mm256_extract_epi64(sum, 3);\n\n    result = (result & 0xFF) + ((result >> 8) & 0xFF) + ((result >> 16) & 0xFF) + ((result >> 24) & 0xFF) +\n             ((result >> 32) & 0xFF) + ((result >> 40) & 0xFF) + ((result >> 48) & 0xFF) + ((result >> 56) & 0xFF);\n\n    std::string binary;\n    while (result) {\n        binary = (result & 1 ? \"1\" : \"0\") + binary;\n        result >>= 1;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_and_si256(vec, mask);\n        __m256i odd_indices = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i result = _mm256_and_si256(even_mask, odd_indices);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i even_mask = _mm256_set1_epi32(1);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n        __m256i odd_indices = _mm256_and_si256(mask, even_elements);\n        sum_vec = _mm256_add_epi32(sum_vec, odd_indices);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_and_si256(vec, mask);\n        __m256i even_elements = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(vec, even_elements);\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int buffer[8];\n    _mm256_store_si256((__m256i*)buffer, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += buffer[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_and_si256(vec, mask);\n        __m256i odd_indices = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i result = _mm256_and_si256(even_mask, odd_indices);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(buffer), sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += buffer[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_and_si256(vec, mask);\n        __m256i even_elements = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(vec, even_elements);\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions (2 * 2)\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions (2 * 2)\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions (2 * 2)\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    size_t aligned_len = len & ~31; // Align to 32 bytes\n\n    for (size_t i = 0; i < aligned_len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256((__m256i*)&result[i], encrypted);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = aligned_len; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions (2 * 2)\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result = message;\n    size_t len = message.length();\n    size_t i = 0;\n\n    __m256i swap_case_mask = _mm256_set1_epi8(0x20);\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n        __m256i swapped = _mm256_xor_si256(chunk, swap_case_mask);\n\n        __m256i mask = _mm256_set1_epi8(0);\n        for (int j = 0; j < 10; ++j) {\n            __m256i v = _mm256_set1_epi8(_mm256_extract_epi8(vowels, j));\n            __m256i r = _mm256_set1_epi8(_mm256_extract_epi8(replacements, j));\n            __m256i cmp = _mm256_cmpeq_epi8(chunk, v);\n            swapped = _mm256_blendv_epi8(swapped, r, cmp);\n            mask = _mm256_or_si256(mask, cmp);\n        }\n\n        _mm256_storeu_si256((__m256i*)&result[i], swapped);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20;\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20;\n        }\n\n        if (c == 'a') c = 'c';\n        else if (c == 'e') c = 'g';\n        else if (c == 'i') c = 'k';\n        else if (c == 'o') c = 'q';\n        else if (c == 'u') c = 'w';\n        else if (c == 'A') c = 'C';\n        else if (c == 'E') c = 'G';\n        else if (c == 'I') c = 'K';\n        else if (c == 'O') c = 'Q';\n        else if (c == 'U') c = 'W';\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result = message;\n    size_t len = message.size();\n    size_t i = 0;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i vowel_mask = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i vowel_replace = _mm256_setr_epi8('c', 'g', 'k', 'q', 's', 'C', 'G', 'K', 'Q', 'S', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n        __m256i swapped = _mm256_xor_si256(chunk, lower_mask);\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(_mm256_shuffle_epi8(vowel_mask, chunk), chunk);\n        __m256i replaced = _mm256_blendv_epi8(swapped, _mm256_shuffle_epi8(vowel_replace, chunk), is_vowel);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), replaced);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20;\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20;\n        }\n\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            c += 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result = message;\n    size_t len = message.size();\n    size_t i = 0;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i vowel_mask = _mm256_set1_epi8('a' | 'e' | 'i' | 'o' | 'u' | 'A' | 'E' | 'I' | 'O' | 'U');\n    __m256i vowel_shift = _mm256_set1_epi8(2);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n        __m256i swapped_case = _mm256_xor_si256(chunk, lower_mask);\n        __m256i is_vowel = _mm256_and_si256(swapped_case, vowel_mask);\n        __m256i shifted_vowels = _mm256_add_epi8(is_vowel, vowel_shift);\n        __m256i encoded_chunk = _mm256_blendv_epi8(swapped_case, shifted_vowels, is_vowel);\n        _mm256_storeu_si256((__m256i*)&result[i], encoded_chunk);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c -= 0x20;\n        } else if (c >= 'A' && c <= 'Z') {\n            c += 0x20;\n        }\n\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            c += 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result = message;\n    size_t len = message.length();\n    size_t i = 0;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i vowel_mask = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i vowel_replacement = _mm256_setr_epi8('c', 'g', 'k', 'q', 's', 'C', 'G', 'K', 'Q', 'S', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n        __m256i swapped = _mm256_xor_si256(chunk, lower_mask);\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(_mm256_shuffle_epi8(vowel_mask, chunk), chunk);\n        __m256i replaced = _mm256_blendv_epi8(swapped, _mm256_shuffle_epi8(vowel_replacement, chunk), is_vowel);\n\n        _mm256_storeu_si256((__m256i*)&result[i], replaced);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20;\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20;\n        }\n\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            c += 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result = message;\n    size_t len = message.size();\n    size_t i = 0;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i vowel_mask = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i vowel_replace = _mm256_setr_epi8('c', 'g', 'k', 'q', 's', 'C', 'G', 'K', 'Q', 'S', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n        __m256i swapped = _mm256_xor_si256(chunk, lower_mask);\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(_mm256_shuffle_epi8(vowel_mask, chunk), chunk);\n        __m256i replaced = _mm256_blendv_epi8(swapped, _mm256_shuffle_epi8(vowel_replace, chunk), is_vowel);\n\n        _mm256_storeu_si256((__m256i*)&result[i], replaced);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20;\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20;\n        }\n\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            c += 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('Z');\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n            __m256i lower_check = _mm256_cmpgt_epi8(vec, lower_mask);\n            __m256i upper_check = _mm256_cmpgt_epi8(upper_mask, vec);\n\n            all_lower &= _mm256_testz_si256(lower_check, lower_check) == 0;\n            all_upper &= _mm256_testz_si256(upper_check, upper_check) == 0;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            all_lower &= (c >= 'a' && c <= 'z');\n            all_upper &= (c >= 'A' && c <= 'Z');\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_mask = _mm256_set1_epi8(0xDF);\n        __m256i lower_check = _mm256_set1_epi8(0x00);\n        __m256i upper_check = _mm256_set1_epi8(0xFF);\n\n        size_t len = key.size();\n        size_t i = 0;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&key[i]);\n            __m256i lower_vec = _mm256_or_si256(vec, lower_mask);\n            __m256i upper_vec = _mm256_and_si256(vec, upper_mask);\n\n            lower_check = _mm256_cmpeq_epi8(lower_vec, vec);\n            upper_check = _mm256_cmpeq_epi8(upper_vec, vec);\n\n            if (!_mm256_testz_si256(lower_check, lower_check)) all_lower = false;\n            if (!_mm256_testz_si256(upper_check, upper_check)) all_upper = false;\n        }\n\n        for (; i < len; ++i) {\n            if (islower(key[i])) all_upper = false;\n            if (isupper(key[i])) all_lower = false;\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('Z' + 1);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n            __m256i lower_check = _mm256_cmpgt_epi8(vec, lower_mask);\n            __m256i upper_check = _mm256_cmplt_epi8(vec, upper_mask);\n\n            all_lower &= _mm256_testz_si256(lower_check, lower_check) == 0;\n            all_upper &= _mm256_testz_si256(upper_check, upper_check) == 0;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            all_lower &= (c >= 'a' && c <= 'z');\n            all_upper &= (c >= 'A' && c <= 'Z');\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_mask = _mm256_set1_epi8(0xDF);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&key[i]);\n            __m256i lower_vec = _mm256_or_si256(vec, lower_mask);\n            __m256i upper_vec = _mm256_and_si256(vec, upper_mask);\n\n            all_lower &= _mm256_movemask_epi8(_mm256_cmpeq_epi8(vec, lower_vec)) == 0xFFFFFFFF;\n            all_upper &= _mm256_movemask_epi8(_mm256_cmpeq_epi8(vec, upper_vec)) == 0xFFFFFFFF;\n        }\n\n        for (; i < len; ++i) {\n            all_lower &= (key[i] >= 'a' && key[i] <= 'z');\n            all_upper &= (key[i] >= 'A' && key[i] <= 'Z');\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) continue;\n\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_mask = _mm256_set1_epi8(0xDF);\n\n        for (size_t i = 0; i < len; i += 32) {\n            size_t chunk_size = std::min(len - i, static_cast<size_t>(32));\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n\n            __m256i lower_vec = _mm256_or_si256(vec, lower_mask);\n            __m256i upper_vec = _mm256_and_si256(vec, upper_mask);\n\n            __m256i lower_check = _mm256_cmpeq_epi8(lower_vec, vec);\n            __m256i upper_check = _mm256_cmpeq_epi8(upper_vec, vec);\n\n            int lower_result = _mm256_movemask_epi8(lower_check);\n            int upper_result = _mm256_movemask_epi8(upper_check);\n\n            int lower_mask_result = (1 << chunk_size) - 1;\n            int upper_mask_result = (1 << chunk_size) - 1;\n\n            if ((lower_result & lower_mask_result) != lower_mask_result) {\n                all_lower = false;\n            }\n            if ((upper_result & upper_mask_result) != upper_mask_result) {\n                all_upper = false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            __m256i vi = _mm256_set1_epi32(i);\n            __m256i vstep = _mm256_set1_epi32(8);\n            __m256i vstart = _mm256_set_epi32(i * i + 28, i * i + 24, i * i + 20, i * i + 16, i * i + 12, i * i + 8, i * i + 4, i * i);\n\n            for (int j = i * i; j < n; j += 32) {\n                __m256i vj = _mm256_add_epi32(vstart, vstep);\n                __m256i vmask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), vj);\n                __m256i vindex = _mm256_and_si256(vmask, _mm256_div_epi32(vj, vi));\n                __m256i vresult = _mm256_cmpeq_epi32(vindex, _mm256_setzero_si256());\n\n                int mask = _mm256_movemask_epi8(vresult);\n                while (mask) {\n                    int bitpos = _tzcnt_u32(mask);\n                    is_prime[j + bitpos] = false;\n                    mask &= mask - 1;\n                }\n\n                vstart = _mm256_add_epi32(vstart, _mm256_set1_epi32(32));\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            __m256i vi = _mm256_set1_epi32(i);\n            __m256i vstep = _mm256_set1_epi32(8);\n            __m256i vstart = _mm256_set_epi32(i * i + 28, i * i + 24, i * i + 20, i * i + 16, i * i + 12, i * i + 8, i * i + 4, i * i);\n\n            for (int j = i * i; j < n; j += 32) {\n                __m256i vj = _mm256_add_epi32(vstart, _mm256_mullo_epi32(vi, vstep));\n                vstart = _mm256_add_epi32(vstart, _mm256_set1_epi32(32));\n\n                int mask = _mm256_movemask_epi8(_mm256_cmpgt_epi32(_mm256_set1_epi32(n), vj));\n                for (int k = 0; k < 8; ++k) {\n                    if (mask & (1 << k)) {\n                        is_prime[vj.m256i_i32[k]] = false;\n                    }\n                }\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            __m256i vi = _mm256_set1_epi32(i);\n            __m256i vstep = _mm256_set1_epi32(8);\n            __m256i vstart = _mm256_set_epi32(i * i + 28, i * i + 24, i * i + 20, i * i + 16, i * i + 12, i * i + 8, i * i + 4, i * i);\n\n            for (int j = i * i; j < n; j += 32) {\n                __m256i vj = _mm256_add_epi32(vstart, vstep);\n                __m256i vmask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), vj);\n                __m256i vindex = _mm256_and_si256(vmask, _mm256_div_epi32(vj, vi));\n                int mask = _mm256_movemask_epi8(vindex);\n\n                for (int k = 0; k < 32; k += 4) {\n                    if (mask & (0xF << k)) {\n                        is_prime[j + k] = false;\n                    }\n                }\n\n                vstart = _mm256_add_epi32(vstart, _mm256_set1_epi32(32));\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    const __m256i upper_vowels = _mm256_set1_epi8('E');\n    const __m256i mask = _mm256_set1_epi8(0xDF); // Mask to convert to uppercase\n    int count = 0;\n    int length = s.length();\n\n    for (int i = 0; i < length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper_chunk = _mm256_and_si256(chunk, mask);\n        __m256i cmp = _mm256_cmpeq_epi8(upper_chunk, upper_vowels);\n\n        // Count the number of 0xFF bytes in cmp\n        uint32_t mask0 = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask0);\n\n        // Check even indices\n        __m256i even_mask = _mm256_setr_epi8(0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0);\n        __m256i even_cmp = _mm256_and_si256(cmp, even_mask);\n        uint32_t even_mask0 = _mm256_movemask_epi8(even_cmp);\n        count += _mm_popcnt_u32(even_mask0);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\nint count_upper_simd(const std::string & s) {\n    const __m256i upper_vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i mask = _mm256_set1_epi8(0x01);\n    int count = 0;\n    int len = s.length();\n\n    for (int i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i even_mask = _mm256_setr_epi8(0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00);\n        __m256i even_chars = _mm256_and_si256(chunk, even_mask);\n\n        __m256i cmp = _mm256_cmpeq_epi8(even_chars, upper_vowels);\n        __m256i result = _mm256_and_si256(cmp, mask);\n\n        count += _mm256_movemask_epi8(result);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    const char* data = s.c_str();\n    size_t len = s.length();\n    size_t i = 0;\n    int count = 0;\n\n    __m256i mask = _mm256_set1_epi8('A' - 1);\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i even_mask = _mm256_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_chars = _mm256_and_si256(chunk, even_mask);\n\n        __m256i cmp = _mm256_cmpgt_epi8(even_chars, mask);\n        __m256i is_vowel = _mm256_and_si256(cmp, vowels);\n\n        count += _mm256_movemask_epi8(is_vowel) & 0x55555555;\n    }\n\n    for (; i < len; i += 2) {\n        if (data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    const __m256i upper_vowels = _mm256_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n    const __m256i mask = _mm256_set1_epi8(0x01);\n    int count = 0;\n    int len = s.length();\n\n    for (int i = 0; i < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i even_mask = _mm256_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_chars = _mm256_and_si256(vec, even_mask);\n        __m256i is_upper_vowel = _mm256_cmpeq_epi8(_mm256_and_si256(even_chars, upper_vowels), upper_vowels);\n        __m256i result = _mm256_and_si256(is_upper_vowel, mask);\n        count += _mm256_movemask_epi8(result) & 0x55555555;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    const __m256i upper_vowels = _mm256_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n    const __m256i mask = _mm256_set1_epi8(0x20);\n    int count = 0;\n    int length = s.length();\n\n    for (int i = 0; i < length; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i even_mask = _mm256_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n        __m256i upper_vec = _mm256_andnot_si256(mask, even_vec);\n        __m256i cmp = _mm256_cmpeq_epi8(upper_vec, upper_vowels);\n        int mask_result = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask_result);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n    __m256i current = _mm256_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 8) {\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n    __m256i index = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i base = _mm256_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i stones = _mm256_add_epi32(base, _mm256_mullo_epi32(index, increment));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), stones);\n        index = _mm256_add_epi32(index, _mm256_set1_epi32(8));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i index = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i index = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (int i = 0; i < n; i += 8) {\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n    __m256i index = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i base = _mm256_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i stones = _mm256_add_epi32(base, _mm256_mullo_epi32(index, increment));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), stones);\n\n        index = _mm256_add_epi32(index, _mm256_set1_epi32(8));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n\n    __m256i sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i avg = _mm256_div_epi64(_mm256_add_epi64(sum, vec_one), vec_two);\n\n    int64_t avg_val = _mm256_extract_epi64(avg, 0);\n\n    std::string binary_str;\n    while (avg_val > 0) {\n        binary_str = (avg_val % 2 == 0 ? \"0\" : \"1\") + binary_str;\n        avg_val /= 2;\n    }\n\n    return binary_str.empty() ? \"0\" : binary_str;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_diff = _mm256_sub_epi64(vec_m, vec_n);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n    __m256i vec_sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i vec_avg = _mm256_div_epi64(vec_sum, vec_two);\n\n    int64_t avg = _mm256_extract_epi64(vec_avg, 0);\n\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n\n    __m256i sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i avg = _mm256_div_epi64(_mm256_add_epi64(sum, vec_one), vec_two);\n\n    int64_t avg_val = _mm256_extract_epi64(avg, 0);\n\n    std::string binary_str;\n    while (avg_val > 0) {\n        binary_str = (avg_val % 2 == 0 ? \"0\" : \"1\") + binary_str;\n        avg_val /= 2;\n    }\n\n    return binary_str.empty() ? \"0\" : binary_str;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n\n    __m256i sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i avg = _mm256_div_epi64(_mm256_add_epi64(sum, vec_one), vec_two);\n\n    int64_t avg_val = _mm256_extract_epi64(avg, 0);\n\n    std::string binary_str;\n    while (avg_val > 0) {\n        binary_str = (avg_val % 2 == 0 ? \"0\" : \"1\") + binary_str;\n        avg_val /= 2;\n    }\n\n    return binary_str.empty() ? \"0\" : binary_str;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n\n    __m256i sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i avg = _mm256_div_epi64(_mm256_add_epi64(sum, vec_one), vec_two);\n\n    int64_t result = _mm256_extract_epi64(avg, 0);\n\n    std::string binary_result;\n    while (result > 0) {\n        binary_result = (result % 2 == 0 ? \"0\" : \"1\") + binary_result;\n        result /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i vec_idx = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i vec_step = _mm256_set1_epi32(8);\n    __m256i vec_mod = _mm256_set1_epi32(10000);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_two = _mm256_set1_epi32(2);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_add_epi32(vec_idx, _mm256_set1_epi32(i));\n        __m256i vec_even = _mm256_and_si256(vec_i, vec_one);\n        __m256i vec_fact = vec_one;\n        __m256i vec_sum = vec_one;\n\n        for (int j = 2; j <= 8; ++j) {\n            __m256i vec_j = _mm256_set1_epi32(j);\n            vec_fact = _mm256_blendv_epi8(vec_fact, _mm256_mullo_epi32(vec_fact, vec_j), _mm256_cmpeq_epi32(vec_even, vec_one));\n            vec_sum = _mm256_blendv_epi8(vec_sum, _mm256_add_epi32(vec_sum, vec_j), _mm256_cmpeq_epi32(vec_even, vec_zero));\n        }\n\n        vec_fact = _mm256_blendv_epi8(vec_sum, _mm256_and_si256(vec_fact, vec_mod), vec_even);\n\n        _mm256_storeu_si256((__m256i*)&result[i], vec_fact);\n        vec_idx = _mm256_add_epi32(vec_idx, vec_step);\n    }\n\n    for (int i = n - (n % 8); i < n; ++i) {\n        int sum = 0;\n        int fact = 1;\n        for (int j = 1; j <= i; ++j) {\n            if (i % 2 == 0) {\n                fact = (fact * j) % 10000;\n            } else {\n                sum += j;\n            }\n        }\n        result[i] = (i % 2 == 0) ? fact : sum;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i vec_idx = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i vec_step = _mm256_set1_epi32(8);\n    __m256i vec_mod = _mm256_set1_epi32(10000);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_add_epi32(vec_idx, _mm256_set1_epi32(i));\n        __m256i vec_even = _mm256_and_si256(vec_i, vec_one);\n        __m256i vec_fact = vec_one;\n        __m256i vec_sum = vec_zero;\n\n        for (int j = 0; j < 8; ++j) {\n            __m256i vec_j = _mm256_set1_epi32(j + 1);\n            vec_fact = _mm256_mullo_epi32(vec_fact, vec_j);\n            vec_sum = _mm256_add_epi32(vec_sum, vec_j);\n        }\n\n        __m256i vec_result = _mm256_blendv_epi8(vec_sum, _mm256_mod_epi32(vec_fact, vec_mod), vec_even);\n        _mm256_storeu_si256((__m256i*)&result[i], vec_result);\n        vec_idx = _mm256_add_epi32(vec_idx, vec_step);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i mod = _mm256_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7, i + 8);\n        __m256i mask = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_mask = _mm256_and_si256(indices, mask);\n\n        __m256i fact = _mm256_set1_epi32(1);\n        __m256i sum = _mm256_set1_epi32(0);\n\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(indices, j);\n            if (idx > n) break;\n\n            if (_mm256_extract_epi32(even_mask, j)) {\n                int f = 1;\n                for (int k = 1; k <= idx; ++k) {\n                    f = (f * k) % 10000;\n                }\n                fact = _mm256_insert_epi32(fact, f, j);\n            } else {\n                int s = 0;\n                for (int k = 1; k <= idx; ++k) {\n                    s += k;\n                }\n                sum = _mm256_insert_epi32(sum, s, j);\n            }\n        }\n\n        __m256i res = _mm256_blendv_epi8(sum, fact, even_mask);\n        for (int j = 0; j < 8 && i + j < n; ++j) {\n            result[i + j] = _mm256_extract_epi32(res, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i vec_idx = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i vec_step = _mm256_set1_epi32(8);\n    __m256i vec_mod = _mm256_set1_epi32(10000);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_add_epi32(vec_idx, _mm256_set1_epi32(i));\n        __m256i vec_even = _mm256_and_si256(vec_i, vec_one);\n        __m256i vec_fact = vec_one;\n        __m256i vec_sum = vec_zero;\n\n        for (int j = 0; j < 8; ++j) {\n            __m256i vec_j = _mm256_set1_epi32(j + 1);\n            vec_fact = _mm256_mullo_epi32(vec_fact, vec_j);\n            vec_sum = _mm256_add_epi32(vec_sum, vec_j);\n        }\n\n        __m256i vec_result = _mm256_blendv_epi8(vec_sum, _mm256_mod_epi32(vec_fact, vec_mod), vec_even);\n        _mm256_storeu_si256((__m256i*)&result[i], vec_result);\n        vec_idx = _mm256_add_epi32(vec_idx, vec_step);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i vec_idx = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i vec_step = _mm256_set1_epi32(8);\n    __m256i vec_mod = _mm256_set1_epi32(10000);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_two = _mm256_set1_epi32(2);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_add_epi32(vec_idx, _mm256_set1_epi32(i));\n        __m256i vec_even = _mm256_and_si256(vec_i, vec_one);\n        __m256i vec_fact = vec_one;\n        __m256i vec_sum = vec_one;\n\n        for (int j = 2; j <= 8; ++j) {\n            __m256i vec_j = _mm256_set1_epi32(j);\n            __m256i vec_cond = _mm256_cmpgt_epi32(vec_i, vec_j);\n            vec_fact = _mm256_blendv_epi8(vec_fact, _mm256_mullo_epi32(vec_fact, vec_j), vec_cond);\n            vec_sum = _mm256_blendv_epi8(vec_sum, _mm256_add_epi32(vec_sum, vec_j), vec_cond);\n        }\n\n        __m256i vec_result = _mm256_blendv_epi8(vec_sum, _mm256_and_si256(vec_fact, vec_mod), vec_even);\n        _mm256_storeu_si256((__m256i*)&result[i], vec_result);\n        vec_idx = _mm256_add_epi32(vec_idx, vec_step);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i reversed = _mm256_shuffle_epi8(numbers, _mm256_setr_epi8(12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3, 12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3));\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(numbers, reversed);\n        __m256i even_numbers = _mm256_and_si256(numbers, even_mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_numbers, zero);\n\n        __m256i even_palindromes = _mm256_and_si256(is_palindrome, is_even);\n        __m256i odd_palindromes = _mm256_andnot_si256(is_even, is_palindrome);\n\n        even_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_palindromes, zero));\n        odd_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(odd_palindromes, zero));\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i reversed = _mm256_set_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(numbers, reversed);\n        __m256i even_numbers = _mm256_and_si256(numbers, even_mask);\n        __m256i odd_numbers = _mm256_and_si256(numbers, odd_mask);\n\n        __m256i even_palindromes = _mm256_and_si256(is_palindrome, even_numbers);\n        __m256i odd_palindromes = _mm256_and_si256(is_palindrome, odd_numbers);\n\n        even_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_palindromes, _mm256_setzero_si256()));\n        odd_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(odd_palindromes, _mm256_setzero_si256()));\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i reversed = _mm256_shuffle_epi8(numbers, _mm256_setr_epi8(\n            12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3,\n            28, 29, 30, 31, 24, 25, 26, 27, 20, 21, 22, 23, 16, 17, 18, 19\n        ));\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(numbers, reversed);\n        __m256i even_numbers = _mm256_and_si256(numbers, even_mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_numbers, zero);\n\n        __m256i even_palindromes = _mm256_andnot_si256(is_even, is_palindrome);\n        __m256i odd_palindromes = _mm256_and_si256(is_even, is_palindrome);\n\n        even_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_palindromes, zero)) >> 1;\n        odd_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(odd_palindromes, zero)) >> 1;\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i reversed = _mm256_shuffle_epi8(numbers, _mm256_setr_epi8(\n            12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3,\n            28, 29, 30, 31, 24, 25, 26, 27, 20, 21, 22, 23, 16, 17, 18, 19\n        ));\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(numbers, reversed);\n        __m256i even_numbers = _mm256_and_si256(numbers, even_mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_numbers, zero);\n\n        __m256i even_palindromes = _mm256_and_si256(is_palindrome, is_even);\n        __m256i odd_palindromes = _mm256_andnot_si256(is_even, is_palindrome);\n\n        even_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_palindromes, zero)) >> 1;\n        odd_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(odd_palindromes, zero)) >> 1;\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i reversed = _mm256_shuffle_epi8(numbers, _mm256_setr_epi8(\n            12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3,\n            28, 29, 30, 31, 24, 25, 26, 27, 20, 21, 22, 23, 16, 17, 18, 19\n        ));\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(numbers, reversed);\n        __m256i even_numbers = _mm256_and_si256(numbers, even_mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_numbers, zero);\n\n        __m256i even_palindromes = _mm256_and_si256(is_palindrome, is_even);\n        __m256i odd_palindromes = _mm256_andnot_si256(is_even, is_palindrome);\n\n        even_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_palindromes, zero)) >> 1;\n        odd_count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(odd_palindromes, zero)) >> 1;\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(abs_vec, j);\n            int sum = 0;\n            while (num > 0) {\n                sum += num % 10;\n                num /= 10;\n            }\n            sum_vec = _mm256_insert_epi32(sum_vec, sum, j);\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_vec, _mm256_setzero_si256());\n        count += _mm256_movemask_epi8(_mm256_cvtepi32_epi8(mask));\n    }\n\n    for (; i < size; ++i) {\n        int num = abs(n[i]);\n        int sum = 0;\n        while (num > 0) {\n            sum += num % 10;\n            num /= 10;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n.size(); i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i mask = _mm256_cmpgt_epi32(v, zero);\n        __m256i abs_v = _mm256_abs_epi32(v);\n\n        __m256i digits = _mm256_set1_epi32(0);\n        for (int j = 0; j < 8; ++j) {\n            __m256i digit = _mm256_set1_epi32(abs_v.m256i_i32[j] % 10);\n            digits = _mm256_add_epi32(digits, digit);\n        }\n\n        __m256i digit_sum = _mm256_hadd_epi32(digits, digits);\n        digit_sum = _mm256_hadd_epi32(digit_sum, digit_sum);\n        digit_sum = _mm256_hadd_epi32(digit_sum, digit_sum);\n\n        __m256i result = _mm256_cmpgt_epi32(digit_sum, zero);\n        count += _mm256_movemask_ps(_mm256_castsi256_ps(result));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n.size(); i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i mask = _mm256_cmpgt_epi32(v, zero);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m256i digits = _mm256_set1_epi32(0);\n\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(abs_v, j);\n            int digit_sum = 0;\n            while (num > 0) {\n                digit_sum += num % 10;\n                num /= 10;\n            }\n            digits = _mm256_insert_epi32(digits, digit_sum, j);\n        }\n\n        __m256i valid = _mm256_and_si256(mask, _mm256_cmpgt_epi32(digits, zero));\n        count += _mm256_movemask_epi8(valid) & 0x11111111;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n.size(); i += 8) {\n        __m256i num_vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(num_vec);\n        __m256i digit_sum_vec = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(abs_vec, j);\n            int digit_sum = 0;\n            while (num > 0) {\n                digit_sum += num % 10;\n                num /= 10;\n            }\n            digit_sum_vec = _mm256_insert_epi32(digit_sum_vec, digit_sum, j);\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(digit_sum_vec, zero_vec);\n        sum_vec = _mm256_add_epi32(sum_vec, mask);\n    }\n\n    int sums[8];\n    _mm256_storeu_si256((__m256i*)sums, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        count += sums[i];\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n.size(); i += 8) {\n        __m256i num_vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(num_vec);\n\n        __m256i digit_sum_vec = _mm256_setzero_si256();\n        for (int j = 0; j < 4; ++j) {\n            __m256i digit_vec = _mm256_set1_epi32(10);\n            __m256i temp_vec = abs_vec;\n            __m256i sum_digits = _mm256_setzero_si256();\n\n            for (int k = 0; k < 10; ++k) {\n                __m256i digit = _mm256_div_epi32(temp_vec, digit_vec);\n                sum_digits = _mm256_add_epi32(sum_digits, digit);\n                temp_vec = _mm256_mullo_epi32(temp_vec, digit_vec);\n            }\n\n            digit_sum_vec = _mm256_add_epi32(digit_sum_vec, sum_digits);\n            abs_vec = _mm256_srli_epi32(abs_vec, 8);\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(digit_sum_vec, zero_vec);\n        count += _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int shift = 0; shift < N; ++shift) {\n        bool is_sorted = true;\n        for (int i = 0; i < N; i += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[(i + shift) % N]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[i]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    int min_index = 0;\n    __m256i min_val = _mm256_set1_epi32(arr[0]);\n\n    for (int i = 0; i < N; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(min_val, vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (0x01010101 << (4 * j))) {\n                    min_index = i + j;\n                    min_val = _mm256_set1_epi32(arr[min_index]);\n                    break;\n                }\n            }\n        }\n    }\n\n    std::vector<int> rotated(N);\n    for (int i = 0; i < N; ++i) {\n        rotated[i] = arr[(min_index + i) % N];\n    }\n\n    for (int i = 0; i < N - 1; ++i) {\n        if (rotated[i] > rotated[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int shift = 0; shift < N; ++shift) {\n        bool is_sorted = true;\n        for (int i = 0; i < N; i += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[(i + shift) % N]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[i]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    __m256i min_val = _mm256_set1_epi32(arr[0]);\n    __m256i max_val = _mm256_set1_epi32(arr[0]);\n\n    for (int i = 0; i < N; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        min_val = _mm256_min_epi32(min_val, vec);\n        max_val = _mm256_max_epi32(max_val, vec);\n    }\n\n    int min_val_scalar = _mm256_extract_epi32(min_val, 0);\n    int max_val_scalar = _mm256_extract_epi32(max_val, 0);\n\n    for (int i = 1; i < 8; ++i) {\n        min_val_scalar = std::min(min_val_scalar, _mm256_extract_epi32(min_val, i));\n        max_val_scalar = std::max(max_val_scalar, _mm256_extract_epi32(max_val, i));\n    }\n\n    int min_index = std::distance(arr.begin(), std::find(arr.begin(), arr.end(), min_val_scalar));\n    int max_index = std::distance(arr.begin(), std::find(arr.begin(), arr.end(), max_val_scalar));\n\n    if (min_index > max_index) {\n        return false;\n    }\n\n    for (int i = 0; i < N - 1; ++i) {\n        if (arr[(min_index + i) % N] > arr[(min_index + i + 1) % N]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int shift = 0; shift < N; ++shift) {\n        bool is_sorted = true;\n        for (int i = 0; i < N; i += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[(i + shift) % N]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[i]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const int size = lst1.size();\n    __m256i vec1, vec2, mask_even, mask_odd;\n    mask_even = _mm256_set1_epi32(1);\n    mask_odd = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i even_mask1 = _mm256_and_si256(vec1, mask_even);\n        __m256i odd_mask1 = _mm256_andnot_si256(mask_even, vec1);\n        __m256i even_mask2 = _mm256_and_si256(vec2, mask_even);\n        __m256i odd_mask2 = _mm256_andnot_si256(mask_even, vec2);\n\n        if (_mm256_testz_si256(odd_mask1, odd_mask1)) {\n            continue;\n        }\n\n        if (_mm256_testz_si256(even_mask2, even_mask2)) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const int size = lst1.size();\n    __m256i vec1, vec2, mask, even_mask, odd_mask;\n\n    even_mask = _mm256_set1_epi32(1);\n    odd_mask = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        mask = _mm256_and_si256(vec1, even_mask);\n        mask = _mm256_cmpeq_epi32(mask, odd_mask);\n\n        if (_mm256_testz_si256(mask, mask) == 0) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const int size = lst1.size();\n    __m256i vec1, vec2, mask1, mask2, even_mask;\n\n    even_mask = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        mask1 = _mm256_and_si256(vec1, even_mask);\n        mask2 = _mm256_and_si256(vec2, even_mask);\n\n        if (!_mm256_testz_si256(mask1, mask1) || !_mm256_testz_si256(mask2, mask2)) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const int size = lst1.size();\n    __m256i vec1, vec2, mask, even_mask;\n\n    even_mask = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        mask = _mm256_and_si256(vec1, even_mask);\n        if (!_mm256_testz_si256(mask, mask)) {\n            mask = _mm256_and_si256(vec2, even_mask);\n            if (_mm256_testz_si256(mask, mask)) {\n                return \"NO\";\n            }\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const int size = lst1.size();\n    __m256i vec1, vec2, mask1, mask2, even_mask;\n\n    even_mask = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < size; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        mask1 = _mm256_and_si256(vec1, even_mask);\n        mask2 = _mm256_and_si256(vec2, even_mask);\n\n        if (!_mm256_testz_si256(mask1, mask1) || !_mm256_testz_si256(mask2, mask2)) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int odd_count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i odd_mask = _mm256_set1_epi8(1);\n            __m256i and_result = _mm256_and_si256(vec, odd_mask);\n            __m256i cmp_result = _mm256_cmpeq_epi8(and_result, odd_mask);\n            int mask = _mm256_movemask_epi8(cmp_result);\n            odd_count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" + std::to_string(odd_count) + \"ng \" + std::to_string(odd_count) + \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int odd_count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        __m256i odd_mask = _mm256_set1_epi8(1);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i odd_digits = _mm256_and_si256(vec, odd_mask);\n            odd_count += _mm_popcnt_u32(_mm256_movemask_epi8(odd_digits));\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) +\n                             \"n the str\" + std::to_string(odd_count) +\n                             \"ng \" + std::to_string(odd_count) +\n                             \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int odd_count = 0;\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i odd_mask = _mm256_set1_epi8(1);\n            __m256i and_result = _mm256_and_si256(vec, odd_mask);\n            __m256i cmp_result = _mm256_cmpeq_epi8(and_result, odd_mask);\n            int mask = _mm256_movemask_epi8(cmp_result);\n            odd_count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) +\n                             \"n the str\" + std::to_string(odd_count) +\n                             \"ng \" + std::to_string(odd_count) +\n                             \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int odd_count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        __m256i odd_mask = _mm256_set1_epi8(1);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&str[i]);\n            vec = _mm256_and_si256(vec, odd_mask);\n            __m256i odd_vec = _mm256_cmpeq_epi8(vec, odd_mask);\n            odd_count += _mm256_movemask_epi8(odd_vec);\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" + std::to_string(odd_count) + \"ng \" + std::to_string(odd_count) + \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int odd_count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i odd_mask = _mm256_set1_epi8(1);\n            __m256i and_result = _mm256_and_si256(vec, odd_mask);\n            __m256i cmp_result = _mm256_cmpeq_epi8(and_result, odd_mask);\n            int mask = _mm256_movemask_epi8(cmp_result);\n            odd_count += _mm_popcnt_u32(mask);\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" + std::to_string(odd_count) + \"ng \" + std::to_string(odd_count) + \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const int64_t* data = nums.data();\n    size_t size = nums.size();\n    size_t i = 0;\n\n    __m256i min_vec = _mm256_set1_epi64x(INT64_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        min_vec = _mm256_min_epi64(min_vec, vec);\n    }\n\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    int64_t min_val = min_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (min_vals[j] < min_val) {\n            min_val = min_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < min_val) {\n            min_val = data[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const int64_t* data = nums.data();\n    size_t size = nums.size();\n    size_t i = 0;\n\n    __m256i min_vec = _mm256_set1_epi64x(INT64_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        min_vec = _mm256_min_epi64(min_vec, vec);\n    }\n\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    int64_t min_val = min_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (min_vals[j] < min_val) {\n            min_val = min_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < min_val) {\n            min_val = data[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const int64_t* data = nums.data();\n    size_t size = nums.size();\n    size_t i = 0;\n\n    __m256i min_vec = _mm256_set1_epi64x(INT64_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        min_vec = _mm256_min_epi64(min_vec, vec);\n    }\n\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    int64_t min_val = min_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (min_vals[j] < min_val) {\n            min_val = min_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < min_val) {\n            min_val = data[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const int64_t* data = nums.data();\n    size_t size = nums.size();\n    size_t i = 0;\n\n    __m256i min_vec = _mm256_set1_epi64x(INT64_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        min_vec = _mm256_min_epi64(min_vec, vec);\n    }\n\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    int64_t min_val = min_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (min_vals[j] < min_val) {\n            min_val = min_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < min_val) {\n            min_val = data[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const int64_t* data = nums.data();\n    size_t size = nums.size();\n    size_t i = 0;\n\n    __m256i min_vec = _mm256_set1_epi64x(INT64_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        min_vec = _mm256_min_epi64(min_vec, vec);\n    }\n\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    int64_t min_val = min_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (min_vals[j] < min_val) {\n            min_val = min_vals[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < min_val) {\n            min_val = data[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int total_water = 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n\n    for (int i = 0; i < num_rows; ++i) {\n        const int* row = grid[i].data();\n        int j = 0;\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i water = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i sum = _mm256_dpwssd_epi32(ones, water, 0xF1);\n            total_water += _mm256_extract_epi32(sum, 0);\n        }\n\n        for (; j < num_cols; ++j) {\n            total_water += row[j];\n        }\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int total_water = 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n\n    for (int i = 0; i < num_rows; ++i) {\n        const int* row = grid[i].data();\n        int j = 0;\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i water = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i sum = _mm256_dpwssd_epi32(water, ones, 0xF1);\n            total_water += _mm256_extract_epi32(sum, 0);\n        }\n\n        for (; j < num_cols; ++j) {\n            total_water += row[j];\n        }\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int total_water = 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n\n    for (int i = 0; i < num_rows; ++i) {\n        const int* row = grid[i].data();\n        int j = 0;\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i water = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j));\n            __m256i sum = _mm256_sad_epu8(water, _mm256_setzero_si256());\n            total_water += _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 4);\n        }\n\n        for (; j < num_cols; ++j) {\n            total_water += row[j];\n        }\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int total_water = 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n\n    for (int i = 0; i < num_rows; ++i) {\n        const int* row = grid[i].data();\n        int j = 0;\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i water = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i mask = _mm256_cmpeq_epi32(water, ones);\n            int count = _mm256_movemask_epi8(mask) & 0x01010101;\n            count = (count & 0x0F0F0F0F) + ((count >> 4) & 0x0F0F0F0F);\n            count = (count & 0x00FF00FF) + ((count >> 8) & 0x00FF00FF);\n            count = (count & 0x0000FFFF) + ((count >> 16) & 0x0000FFFF);\n            total_water += count;\n        }\n\n        for (; j < num_cols; ++j) {\n            total_water += grid[i][j];\n        }\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int total_water = 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n\n    for (int i = 0; i < num_rows; ++i) {\n        const int* row = grid[i].data();\n        int j = 0;\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i water = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i sum = _mm256_dpwssd_epi32(water, ones, 0xF1);\n            total_water += _mm256_extract_epi32(sum, 0);\n        }\n\n        for (; j < num_cols; ++j) {\n            total_water += row[j];\n        }\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i mask_even = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i mask_odd = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_pos = _mm256_and_si256(vec, mask_even);\n        __m256i odd_vals = _mm256_and_si256(vec, mask_odd);\n        __m256i result = _mm256_and_si256(even_pos, odd_vals);\n\n        alignas(32) int16_t temp[16];\n        _mm256_store_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 16; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t size = lst.size();\n    size_t i = 0;\n\n    __m256i odd_mask = _mm256_set1_epi16(1);\n    __m256i even_pos_mask = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i odd_elements = _mm256_and_si256(vec, odd_mask);\n        __m256i even_pos_elements = _mm256_and_si256(vec, even_pos_mask);\n        __m256i result = _mm256_and_si256(odd_elements, even_pos_elements);\n\n        int16_t temp[16];\n        _mm256_storeu_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 16; j += 2) {\n            sum += temp[j];\n        }\n    }\n\n    for (; i < size; i += 2) {\n        if (lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i mask_even = _mm256_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n    __m256i mask_odd = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_and_si256(vec, mask_odd);\n        __m256i result = _mm256_and_si256(even_masked, odd_masked);\n\n        alignas(32) int16_t temp[16];\n        _mm256_store_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 16; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i mask = _mm256_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n    __m256i odd_mask = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_pos = _mm256_and_si256(vec, mask);\n        __m256i odd_elements = _mm256_and_si256(even_pos, odd_mask);\n        __m256i sum_vec = _mm256_hadd_epi16(odd_elements, odd_elements);\n        sum_vec = _mm256_hadd_epi16(sum_vec, sum_vec);\n        sum_vec = _mm256_hadd_epi16(sum_vec, sum_vec);\n        sum += _mm256_extract_epi16(sum_vec, 0) + _mm256_extract_epi16(sum_vec, 8);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i mask = _mm256_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n    __m256i odd_mask = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_pos = _mm256_and_si256(vec, mask);\n        __m256i odd_elements = _mm256_and_si256(even_pos, odd_mask);\n        __m256i sum_vec = _mm256_hadd_epi16(odd_elements, odd_elements);\n        sum_vec = _mm256_hadd_epi16(sum_vec, sum_vec);\n        sum_vec = _mm256_hadd_epi16(sum_vec, sum_vec);\n        sum += _mm256_extract_epi16(sum_vec, 0) + _mm256_extract_epi16(sum_vec, 8);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(mask, vec);\n        __m256i masked_vec = _mm256_and_si256(vec, cmp);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int sum = 0;\n    int* sum_arr = (int*)&sum_vec;\n    for (int i = 0; i < 8; ++i) {\n        sum += sum_arr[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i threshold = _mm256_set1_epi32(99);\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, vec);\n        __m256i masked = _mm256_and_si256(vec, cmp);\n        sum = _mm256_add_epi32(sum, masked);\n    }\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(mask, vec);\n        __m256i masked_vec = _mm256_and_si256(vec, cmp);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int sum[8];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(mask, vec);\n        __m256i masked_vec = _mm256_and_si256(vec, cmp);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum += ((int*)&sum_vec)[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n\n    for (; i <= k - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi32(vec, _mm256_set1_epi32(9)), _mm256_cmpgt_epi32(_mm256_set1_epi32(100), vec));\n        __m256i masked_vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int sum = 0;\n    for (; i < k; ++i) {\n        if (arr[i] >= 10 && arr[i] < 100) {\n            sum += arr[i];\n        }\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() < 2) return true;\n\n    const int *data = lst.data();\n    size_t size = lst.size();\n    size_t i = 0;\n\n    // Check for duplicates using AVX2\n    for (; i + 8 <= size; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i v_next = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(v, v_next);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for duplicates in the remaining elements\n    for (; i < size - 1; ++i) {\n        if (data[i] == data[i + 1]) return false;\n    }\n\n    // Check for sorted order using AVX2\n    i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i v_next = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(v, v_next);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for sorted order in the remaining elements\n    for (; i < size - 1; ++i) {\n        if (data[i] > data[i + 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() < 2) return true;\n\n    const int *data = lst.data();\n    size_t size = lst.size();\n    size_t i = 0;\n\n    // Check for duplicates and sorted order using AVX2\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next_vec = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(vec, next_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) return false;\n\n        // Check for duplicates within the current vector\n        __m256i dup_cmp = _mm256_cmpeq_epi32(vec, _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0)));\n        int dup_mask = _mm256_movemask_epi8(dup_cmp);\n\n        if (dup_mask != 0) return false;\n    }\n\n    // Check remaining elements\n    for (; i < size - 1; ++i) {\n        if (data[i] > data[i + 1]) return false;\n        if (i > 0 && data[i] == data[i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() < 2) return true;\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n    size_t i = 0;\n\n    // Check for duplicates using AVX2\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next_vec = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(vec, next_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for duplicates in the remaining elements\n    for (; i + 1 < size; ++i) {\n        if (data[i] == data[i + 1]) return false;\n    }\n\n    // Check for sorted order using AVX2\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next_vec = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(vec, next_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for sorted order in the remaining elements\n    for (; i + 1 < size; ++i) {\n        if (data[i] > data[i + 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    const int size = lst.size();\n    if (size < 2) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current;\n\n    for (int i = 1; i < size; i += 8) {\n        int limit = std::min(i + 8, size);\n        int values[8] = {0};\n        for (int j = i; j < limit; ++j) {\n            values[j - i] = lst[j];\n        }\n        current = _mm256_loadu_si256((__m256i*)values);\n\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n        if (_mm256_movemask_epi8(cmp) != 0) return false;\n\n        __m256i dup = _mm256_cmpeq_epi32(prev, current);\n        if (_mm256_movemask_epi8(dup) != 0) return false;\n\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() < 2) return true;\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n    size_t i = 0;\n\n    // Check for duplicates using AVX2\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next_vec = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(vec, next_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for duplicates in the remaining elements\n    for (; i + 1 < size; ++i) {\n        if (data[i] == data[i + 1]) return false;\n    }\n\n    // Check for ascending order using AVX2\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next_vec = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(vec, next_vec);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        if (mask != 0) return false;\n    }\n\n    // Check for ascending order in the remaining elements\n    for (; i + 1 < size; ++i) {\n        if (data[i] > data[i + 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod_signs = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(vec, vec);\n\n        sum = _mm256_add_epi32(sum, abs_vec);\n        prod_signs = _mm256_mullo_epi32(prod_signs, signs);\n    }\n\n    int result_sum = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                     _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                     _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                     _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n    int result_prod_signs = _mm256_extract_epi32(prod_signs, 0) * _mm256_extract_epi32(prod_signs, 1) *\n                            _mm256_extract_epi32(prod_signs, 2) * _mm256_extract_epi32(prod_signs, 3) *\n                            _mm256_extract_epi32(prod_signs, 4) * _mm256_extract_epi32(prod_signs, 5) *\n                            _mm256_extract_epi32(prod_signs, 6) * _mm256_extract_epi32(prod_signs, 7);\n\n    return result_sum * result_prod_signs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod_signs = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i signs = _mm256_sign_epi32(vec, vec);\n        prod_signs = _mm256_mullo_epi32(prod_signs, signs);\n    }\n\n    int result_sum = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                    _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                    _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                    _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n    int result_prod_signs = _mm256_extract_epi32(prod_signs, 0) * _mm256_extract_epi32(prod_signs, 1) *\n                           _mm256_extract_epi32(prod_signs, 2) * _mm256_extract_epi32(prod_signs, 3) *\n                           _mm256_extract_epi32(prod_signs, 4) * _mm256_extract_epi32(prod_signs, 5) *\n                           _mm256_extract_epi32(prod_signs, 6) * _mm256_extract_epi32(prod_signs, 7);\n\n    return result_sum * result_prod_signs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod_signs = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(vec, vec);\n\n        sum = _mm256_add_epi32(sum, abs_vec);\n        prod_signs = _mm256_mullo_epi32(prod_signs, signs);\n    }\n\n    int result_sum = 0;\n    int result_prod_signs = 1;\n\n    for (int i = 0; i < 8; ++i) {\n        result_sum += ((int*)&sum)[i];\n        result_prod_signs *= ((int*)&prod_signs)[i];\n    }\n\n    return result_sum * result_prod_signs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod_signs = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i signs = _mm256_sign_epi32(vec, vec);\n        prod_signs = _mm256_mullo_epi32(prod_signs, signs);\n    }\n\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum);\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_arr[i];\n    }\n\n    int prod_signs_arr[8];\n    _mm256_storeu_si256((__m256i*)prod_signs_arr, prod_signs);\n    int total_prod_signs = 1;\n    for (int i = 0; i < 8; ++i) {\n        total_prod_signs *= prod_signs_arr[i];\n    }\n\n    return total_sum * total_prod_signs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_vec = _mm256_set1_epi32(1);\n\n    int size = arr.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i sign = _mm256_and_si256(sign_mask, _mm256_set1_epi32(-1));\n        sign = _mm256_add_epi32(sign, _mm256_set1_epi32(1));\n        sign_vec = _mm256_mullo_epi32(sign_vec, sign);\n    }\n\n    int sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += ((int*)&sum_vec)[j];\n    }\n\n    int sign = 1;\n    for (int j = 0; j < 8; ++j) {\n        sign *= ((int*)&sign_vec)[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += abs(arr[i]);\n        sign *= (arr[i] > 0) - (arr[i] < 0);\n    }\n\n    return sum * sign;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n    std::vector<int> currentPath(k);\n\n    for (int startRow = 0; startRow < N; ++startRow) {\n        for (int startCol = 0; startCol < N; ++startCol) {\n            currentPath[0] = grid[startRow][startCol];\n            for (int step = 1; step < k; ++step) {\n                int row = (step - 1) / N;\n                int col = (step - 1) % N;\n                int nextRow = row + (step % 2 == 0 ? 1 : -1);\n                int nextCol = col + (step % 2 == 0 ? -1 : 1);\n\n                if (nextRow >= 0 && nextRow < N && nextCol >= 0 && nextCol < N) {\n                    currentPath[step] = grid[nextRow][nextCol];\n                } else {\n                    currentPath[step] = INT_MAX;\n                }\n            }\n\n            if (std::lexicographical_compare(currentPath.begin(), currentPath.end(), minPath.begin(), minPath.end())) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, std::numeric_limits<int>::max());\n    std::vector<int> currentPath(k);\n\n    auto isValid = [&](int x, int y) {\n        return x >= 0 && x < N && y >= 0 && y < N;\n    };\n\n    auto dfs = [&](auto&& dfs, int x, int y, int step) -> void {\n        if (step == k) {\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n            return;\n        }\n\n        currentPath[step] = grid[x][y];\n\n        int dx[] = {-1, 1, 0, 0};\n        int dy[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int nx = x + dx[i];\n            int ny = y + dy[i];\n            if (isValid(nx, ny)) {\n                dfs(dfs, nx, ny, step + 1);\n            }\n        }\n    };\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dfs(dfs, i, j, 0);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, std::numeric_limits<int>::max());\n    std::vector<int> currentPath(k);\n\n    auto isValid = [&](int x, int y) {\n        return x >= 0 && x < N && y >= 0 && y < N;\n    };\n\n    auto dfs = [&](auto&& self, int x, int y, int step) -> void {\n        if (step == k) {\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n            return;\n        }\n\n        int dx[] = {-1, 1, 0, 0};\n        int dy[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int nx = x + dx[i];\n            int ny = y + dy[i];\n            if (isValid(nx, ny)) {\n                currentPath[step] = grid[nx][ny];\n                self(self, nx, ny, step + 1);\n            }\n        }\n    };\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            currentPath[0] = grid[i][j];\n            dfs(dfs, i, j, 1);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, std::numeric_limits<int>::max());\n    std::vector<int> currentPath(k);\n\n    auto isValid = [&](int x, int y) {\n        return x >= 0 && x < N && y >= 0 && y < N;\n    };\n\n    auto dfs = [&](auto&& self, int x, int y, int step) -> void {\n        if (step == k) {\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n            return;\n        }\n\n        currentPath[step] = grid[x][y];\n\n        int dx[] = {-1, 1, 0, 0};\n        int dy[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int nx = x + dx[i];\n            int ny = y + dy[i];\n            if (isValid(nx, ny)) {\n                self(self, nx, ny, step + 1);\n            }\n        }\n    };\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dfs(dfs, i, j, 0);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n    std::vector<int> currentPath(k);\n\n    for (int startRow = 0; startRow < N; ++startRow) {\n        for (int startCol = 0; startCol < N; ++startCol) {\n            currentPath[0] = grid[startRow][startCol];\n            std::vector<std::pair<int, int>> stack = {{startRow, startCol}};\n\n            while (!stack.empty()) {\n                auto [row, col] = stack.back();\n                stack.pop_back();\n\n                for (int step = 1; step < k; ++step) {\n                    std::vector<std::pair<int, int>> neighbors;\n                    if (row > 0) neighbors.emplace_back(row - 1, col);\n                    if (row < N - 1) neighbors.emplace_back(row + 1, col);\n                    if (col > 0) neighbors.emplace_back(row, col - 1);\n                    if (col < N - 1) neighbors.emplace_back(row, col + 1);\n\n                    for (const auto& [nRow, nCol] : neighbors) {\n                        currentPath[step] = grid[nRow][nCol];\n                        stack.emplace_back(nRow, nCol);\n                    }\n                }\n\n                if (std::lexicographical_compare(currentPath.begin(), currentPath.end(), minPath.begin(), minPath.end())) {\n                    minPath = currentPath;\n                }\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n) {\n    __m256i mask = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_set1_epi64x(n);\n        __m256i odd_digits = _mm256_and_si256(digits, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi8(odd_digits, odd_mask);\n        __m256i odd_product = _mm256_and_si256(digits, is_odd);\n\n        product = _mm256_mullo_epi64(product, odd_product);\n        n /= 10;\n    }\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; ++i) {\n        result *= ((uint64_t*)&product)[i];\n    }\n\n    return result == 1 ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n) {\n    __m256i mask = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i product = _mm256_set1_epi8(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_set1_epi64x(n);\n        __m256i odd_digits = _mm256_and_si256(digits, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi8(odd_digits, odd_mask);\n        __m256i odd_product = _mm256_and_si256(digits, is_odd);\n        product = _mm256_mullo_epi8(product, odd_product);\n\n        n /= 10;\n    }\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; ++i) {\n        result *= ((uint64_t*)(&product))[i];\n    }\n\n    return result == 1 ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n) {\n    __m256i mask = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x1);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i digits = _mm256_setzero_si256();\n\n    for (int i = 0; i < 4; ++i) {\n        if (n == 0) break;\n        digits = _mm256_insert_epi8(digits, n % 10, i);\n        n /= 10;\n    }\n\n    __m256i odd_digits = _mm256_and_si256(digits, odd_mask);\n    __m256i is_odd = _mm256_cmpgt_epi8(odd_digits, zero);\n    __m256i odd_product = _mm256_and_si256(is_odd, digits);\n\n    for (int i = 0; i < 4; ++i) {\n        product = _mm256_mullo_epi64(product, odd_product);\n        odd_product = _mm256_srli_epi64(odd_product, 64);\n    }\n\n    uint64_t result = _mm256_extract_epi64(product, 0);\n    return result == 1 ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n) {\n    __m256i mask = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    while (n > 0) {\n        __m256i digits = _mm256_set1_epi64x(n);\n        __m256i odd_digits = _mm256_and_si256(digits, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi8(odd_digits, odd_mask);\n        __m256i odd_product = _mm256_and_si256(digits, is_odd);\n        product = _mm256_mullo_epi64(product, odd_product);\n\n        n /= 10;\n    }\n\n    uint64_t result = _mm256_extract_epi64(product, 0);\n    for (int i = 1; i < 4; ++i) {\n        result *= _mm256_extract_epi64(product, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n) {\n    __m256i mask = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    while (n > 0) {\n        uint8_t digit = n % 10;\n        n /= 10;\n\n        __m256i current_digit = _mm256_set1_epi8(digit);\n        __m256i is_odd = _mm256_and_si256(current_digit, odd_mask);\n        is_odd = _mm256_cmpeq_epi8(is_odd, odd_mask);\n\n        __m256i masked_digit = _mm256_and_si256(current_digit, is_odd);\n        masked_digit = _mm256_blendv_epi8(zero, masked_digit, is_odd);\n\n        product = _mm256_mullo_epi32(product, masked_digit);\n    }\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; ++i) {\n        result *= ((uint64_t*)&product)[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n        __m256i squared_vec = _mm256_mullo_epi32(int_vec, int_vec);\n        __m256i high_half = _mm256_srli_epi64(squared_vec, 32);\n        __m256i sum_half = _mm256_add_epi32(squared_vec, high_half);\n        __m256i sum_vec_i = _mm256_cvtepi32_ps(sum_half);\n        sum_vec = _mm256_add_ps(sum_vec, sum_vec_i);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i <= lst.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        vec = _mm256_ceil_ps(vec);\n        vec = _mm256_mul_ps(vec, vec);\n        sum_vec = _mm256_add_ps(sum_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (; i < lst.size(); ++i) {\n        float val = std::ceil(lst[i]);\n        sum += static_cast<uint64_t>(val * val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const int size = lst.size();\n    const int aligned_size = size & ~7;\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 one = _mm256_set1_ps(1.0f);\n    __m256 sum = _mm256_setzero_ps();\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 squared_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum = _mm256_add_ps(sum, squared_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum);\n    float total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (int i = aligned_size; i < size; ++i) {\n        float ceil_val = ceilf(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n        __m256i squared_vec = _mm256_mullo_epi32(int_vec, int_vec);\n        __m256i abs_squared_vec = _mm256_and_si256(squared_vec, mask);\n        __m256 abs_squared_float_vec = _mm256_cvtepi32_ps(abs_squared_vec);\n        sum_vec = _mm256_add_ps(sum_vec, abs_squared_float_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256 sum_vec = _mm256_setzero_ps();\n    int i = 0;\n\n    for (; i <= lst.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        vec = _mm256_ceil_ps(vec);\n        vec = _mm256_mul_ps(vec, vec);\n        sum_vec = _mm256_add_ps(sum_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < lst.size(); ++i) {\n        float val = ceilf(lst[i]);\n        sum += val * val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        mask = _mm256_and_si256(mask, cmp);\n\n        prev = curr;\n    }\n\n    int result = -1;\n    for (int i = 0; i < 8; ++i) {\n        if (_mm256_extract_epi32(mask, i) != 0) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        mask = _mm256_and_si256(mask, cmp);\n\n        prev = curr;\n    }\n\n    int result = _mm256_movemask_epi8(mask);\n    if (result == 0) return -1;\n\n    int index = __builtin_ctz(result) / 4;\n    return index + n - 8;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        mask = _mm256_and_si256(mask, cmp);\n\n        prev = curr;\n    }\n\n    int result = -1;\n    for (int i = 0; i < 8; ++i) {\n        if (_mm256_extract_epi32(mask, i) != 0) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    const int size = arr.size();\n    if (size < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for (int i = 1; i < size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        mask = _mm256_and_si256(mask, cmp);\n\n        prev = curr;\n    }\n\n    int result = _mm256_movemask_epi8(mask);\n    if (result == 0) return -1;\n\n    int index = __builtin_ctz(result) / 4;\n    return index + 1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n        mask = _mm256_and_si256(mask, cmp);\n\n        prev = current;\n    }\n\n    int mask_result = _mm256_movemask_epi8(mask);\n    if (mask_result == 0) return -1;\n\n    for (int i = n - 1; i >= 0; i--) {\n        if (arr[i] < arr[i - 1]) return i;\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n    __m256i largest_neg_vec = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos_vec = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_values);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_values);\n    }\n\n    alignas(32) int neg_buffer[8];\n    alignas(32) int pos_buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(neg_buffer), largest_neg_vec);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(pos_buffer), smallest_pos_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (neg_buffer[j] > largest_negative) {\n            largest_negative = neg_buffer[j];\n        }\n        if (pos_buffer[j] < smallest_positive && pos_buffer[j] != 0) {\n            smallest_positive = pos_buffer[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && (lst[i] < smallest_positive || smallest_positive == 0)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i largest_neg_vec = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos_vec = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_values);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_values);\n    }\n\n    alignas(32) int neg_buffer[8];\n    alignas(32) int pos_buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(neg_buffer), largest_neg_vec);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(pos_buffer), smallest_pos_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (neg_buffer[j] > largest_negative) {\n            largest_negative = neg_buffer[j];\n        }\n        if (pos_buffer[j] < smallest_positive && pos_buffer[j] != 0) {\n            smallest_positive = pos_buffer[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && (lst[i] < smallest_positive || smallest_positive == 0)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n    __m256i largest_neg_vec = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos_vec = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t size = lst.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_values);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_values);\n    }\n\n    alignas(32) int neg_buffer[8];\n    alignas(32) int pos_buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(neg_buffer), largest_neg_vec);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(pos_buffer), smallest_pos_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (neg_buffer[j] > largest_negative) {\n            largest_negative = neg_buffer[j];\n        }\n        if (pos_buffer[j] < smallest_positive && pos_buffer[j] != 0) {\n            smallest_positive = pos_buffer[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && (lst[i] < smallest_positive || smallest_positive == 0)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    __m256i largest_neg_vec = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos_vec = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(0), vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(0));\n\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_values);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_values);\n    }\n\n    alignas(32) int neg_buffer[8];\n    alignas(32) int pos_buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(neg_buffer), largest_neg_vec);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(pos_buffer), smallest_pos_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (neg_buffer[j] > largest_negative) {\n            largest_negative = neg_buffer[j];\n        }\n        if (pos_buffer[j] < smallest_positive && pos_buffer[j] != 0) {\n            smallest_positive = pos_buffer[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && (lst[i] < smallest_positive || smallest_positive == 0)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i largest_neg_vec = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos_vec = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_values);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_values);\n    }\n\n    alignas(32) int neg_buffer[8];\n    alignas(32) int pos_buffer[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(neg_buffer), largest_neg_vec);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(pos_buffer), smallest_pos_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (neg_buffer[j] > largest_negative) {\n            largest_negative = neg_buffer[j];\n        }\n        if (pos_buffer[j] < smallest_positive && pos_buffer[j] != 0) {\n            smallest_positive = pos_buffer[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && (lst[i] < smallest_positive || smallest_positive == 0)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m256i mul_vec = _mm256_set1_epi64x(i);\n        vec = _mm256_mullo_epi64(vec, mul_vec);\n        result *= i;\n    }\n\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec);\n    result *= temp[0] * temp[1] * temp[2] * temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m256i mul_vec = _mm256_set1_epi64x(i);\n        vec = _mm256_mullo_epi64(vec, mul_vec);\n        result *= i;\n    }\n\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec);\n    result *= temp[0] * temp[1] * temp[2] * temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m256i mul_vec = _mm256_set1_epi64x(i);\n        vec = _mm256_mullo_epi64(vec, mul_vec);\n        result *= i;\n    }\n\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec);\n    result *= temp[0] * temp[1] * temp[2] * temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec = _mm256_set1_epi64x(1);\n    __m256i ones = _mm256_set1_epi64x(1);\n    __m256i result = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m256i current = _mm256_set1_epi64x(i);\n        vec = _mm256_mullo_epi64(vec, current);\n        result = _mm256_mullo_epi64(result, vec);\n    }\n\n    uint64_t res[4];\n    _mm256_storeu_si256((__m256i*)res, result);\n    return res[0];\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m256i mul_vec = _mm256_set1_epi64x(i);\n        vec = _mm256_mullo_epi64(vec, mul_vec);\n        result *= i;\n    }\n\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec);\n    result *= temp[0] * temp[1] * temp[2] * temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    const int size = lst.size();\n    int sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i + 0);\n\n        __m256i mask_multiple_of_3 = _mm256_set1_epi32(3);\n        __m256i mask_multiple_of_4 = _mm256_set1_epi32(4);\n\n        __m256i mod_3 = _mm256_div_epi32(indices, mask_multiple_of_3);\n        __m256i mod_4 = _mm256_div_epi32(indices, mask_multiple_of_4);\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod_3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_cmpeq_epi32(mod_4, _mm256_setzero_si256());\n        __m256i is_multiple_of_4_not_3 = _mm256_andnot_si256(is_multiple_of_3, is_multiple_of_4);\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(vec, vec), vec);\n\n        __m256i result = _mm256_blendv_epi8(vec, squared, is_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, is_multiple_of_4_not_3);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int sum_array[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(sum_array), sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += sum_array[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    const int size = lst.size();\n    int sum = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i mask_multiple_of_3 = _mm256_setr_epi32(\n            (i + 0) % 3 == 0, (i + 1) % 3 == 0, (i + 2) % 3 == 0, (i + 3) % 3 == 0,\n            (i + 4) % 3 == 0, (i + 5) % 3 == 0, (i + 6) % 3 == 0, (i + 7) % 3 == 0\n        );\n\n        __m256i mask_multiple_of_4 = _mm256_setr_epi32(\n            (i + 0) % 4 == 0, (i + 1) % 4 == 0, (i + 2) % 4 == 0, (i + 3) % 4 == 0,\n            (i + 4) % 4 == 0, (i + 5) % 4 == 0, (i + 6) % 4 == 0, (i + 7) % 4 == 0\n        );\n\n        __m256i mask_multiple_of_3_not_4 = _mm256_andnot_si256(mask_multiple_of_4, mask_multiple_of_3);\n        __m256i mask_multiple_of_4_not_3 = _mm256_andnot_si256(mask_multiple_of_3, mask_multiple_of_4);\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(vec, vec), vec);\n\n        __m256i result = _mm256_blendv_epi8(vec, squared, mask_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, mask_multiple_of_4_not_3);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (int i = size / 8 * 8; i < size; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    const int size = lst.size();\n    const int vec_size = 8; // AVX2 processes 8 integers at a time\n    int sum = 0;\n\n    __m256i vec_sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i mask_multiple_of_3 = _mm256_set1_epi32(3);\n        __m256i mask_multiple_of_4 = _mm256_set1_epi32(4);\n\n        __m256i mod_3 = _mm256_div_epi32(indices, mask_multiple_of_3);\n        __m256i mod_4 = _mm256_div_epi32(indices, mask_multiple_of_4);\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod_3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_cmpeq_epi32(mod_4, _mm256_setzero_si256());\n        __m256i is_not_multiple_of_3 = _mm256_andnot_si256(is_multiple_of_3, _mm256_set1_epi32(-1));\n\n        __m256i mask_square = _mm256_and_si256(is_multiple_of_3, vec);\n        __m256i mask_cube = _mm256_and_si256(is_multiple_of_4, is_not_multiple_of_3);\n\n        __m256i squared = _mm256_mullo_epi32(mask_square, mask_square);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(mask_cube, mask_cube), mask_cube);\n\n        __m256i result = _mm256_add_epi32(_mm256_add_epi32(vec, squared), cubed);\n\n        vec_sum = _mm256_add_epi32(vec_sum, result);\n    }\n\n    alignas(32) int buffer[8];\n    _mm256_store_si256((__m256i*)buffer, vec_sum);\n    for (int i = 0; i < 8; ++i) {\n        sum += buffer[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    const int size = lst.size();\n    int sum = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i mask_mul_3 = _mm256_set1_epi32(3);\n        __m256i mask_mul_4 = _mm256_set1_epi32(4);\n\n        __m256i mod_3 = _mm256_div_epi32(indices, mask_mul_3);\n        __m256i mod_4 = _mm256_div_epi32(indices, mask_mul_4);\n\n        __m256i is_mul_3 = _mm256_cmpeq_epi32(mod_3, _mm256_setzero_si256());\n        __m256i is_mul_4 = _mm256_cmpeq_epi32(mod_4, _mm256_setzero_si256());\n        __m256i not_mul_3 = _mm256_andnot_si256(is_mul_3, _mm256_set1_epi32(-1));\n\n        __m256i mask = _mm256_and_si256(is_mul_3, vec);\n        mask = _mm256_or_si256(mask, _mm256_and_si256(is_mul_4, _mm256_and_si256(not_mul_3, vec)));\n\n        __m256i squares = _mm256_mullo_epi32(mask, mask);\n        __m256i cubes = _mm256_mullo_epi32(_mm256_mullo_epi32(mask, mask), mask);\n\n        __m256i result = _mm256_blendv_epi8(vec, squares, is_mul_3);\n        result = _mm256_blendv_epi8(result, cubes, _mm256_and_si256(is_mul_4, not_mul_3));\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] + sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    for (int i = size / 8 * 8; i < size; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i mask_multiple_of_3 = _mm256_set1_epi32(3);\n        __m256i mask_multiple_of_4 = _mm256_set1_epi32(4);\n\n        __m256i multiple_of_3 = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_multiple_of_3), _mm256_setzero_si256());\n        __m256i multiple_of_4 = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_multiple_of_4), _mm256_setzero_si256());\n        __m256i multiple_of_4_not_3 = _mm256_andnot_si256(multiple_of_3, multiple_of_4);\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(vec, vec), vec);\n\n        __m256i result = _mm256_blendv_epi8(vec, squared, multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, multiple_of_4_not_3);\n\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, result);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\nint specialFilter_simd(const std::vector<int>& nums) {\n    const int threshold = 10;\n    const __m256i threshold_vec = _mm256_set1_epi32(threshold);\n    const __m256i odd_digits_mask = _mm256_set1_epi32(0x55555555); // 01010101... in binary\n\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp = _mm256_cmpgt_epi32(abs_vec, threshold_vec);\n\n        __m256i first_digit = _mm256_srli_epi32(abs_vec, 28); // Shift right to get the first digit\n        __m256i last_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(0xF)); // Mask to get the last digit\n\n        __m256i first_digit_odd = _mm256_and_si256(first_digit, odd_digits_mask);\n        __m256i last_digit_odd = _mm256_and_si256(last_digit, odd_digits_mask);\n\n        __m256i both_odd = _mm256_and_si256(first_digit_odd, last_digit_odd);\n        __m256i result = _mm256_and_si256(cmp, both_odd);\n\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_setzero_si256())) >> 2;\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        int abs_num = abs(num);\n        int first_digit = abs_num / 1000000000;\n        int last_digit = abs_num % 10;\n\n        if (abs_num > threshold && (first_digit & 1) && (last_digit & 1)) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\nint specialFilter_simd(const std::vector<int> & nums) {\n    const int threshold = 10;\n    const __m256i threshold_vec = _mm256_set1_epi32(threshold);\n    const __m256i odd_digits_mask = _mm256_set1_epi32(0x55555555); // 01010101... in binary\n    const __m256i odd_digits = _mm256_set1_epi32(0x11111111); // 00010001... in binary\n\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp = _mm256_cmpgt_epi32(abs_vec, threshold_vec);\n\n        __m256i first_digit = _mm256_and_si256(abs_vec, odd_digits_mask);\n        __m256i last_digit = _mm256_and_si256(_mm256_srli_epi32(abs_vec, 28), odd_digits_mask);\n\n        __m256i first_odd = _mm256_cmpeq_epi32(first_digit, odd_digits);\n        __m256i last_odd = _mm256_cmpeq_epi32(last_digit, odd_digits);\n\n        __m256i result = _mm256_and_si256(cmp, _mm256_and_si256(first_odd, last_odd));\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_set1_epi32(-1))) >> 2;\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        int abs_num = abs(num);\n        if (abs_num > threshold &&\n            ((abs_num % 10) & 1) &&\n            (((abs_num / 1000000000) % 10) & 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\nint specialFilter_simd(const std::vector<int> & nums) {\n    const int threshold = 10;\n    const __m256i threshold_vec = _mm256_set1_epi32(threshold);\n    const __m256i odd_digits_mask = _mm256_set1_epi32(0x55555555); // 01010101... in binary\n    const __m256i odd_digits = _mm256_set1_epi32(0x11111111); // 00010001... in binary\n\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp = _mm256_cmpgt_epi32(abs_vec, threshold_vec);\n\n        __m256i first_digit = _mm256_and_si256(abs_vec, odd_digits_mask);\n        __m256i last_digit = _mm256_and_si256(_mm256_srli_epi32(abs_vec, 28), odd_digits_mask);\n\n        __m256i first_odd = _mm256_cmpeq_epi32(first_digit, odd_digits);\n        __m256i last_odd = _mm256_cmpeq_epi32(last_digit, odd_digits);\n\n        __m256i result = _mm256_and_si256(cmp, _mm256_and_si256(first_odd, last_odd));\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_set1_epi32(-1))) >> 2;\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        int abs_num = abs(num);\n        if (abs_num > threshold &&\n            ((abs_num % 10) & 1) &&\n            (((abs_num / 1000000000) % 10) & 1)) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\nint specialFilter_simd(const std::vector<int>& nums) {\n    const int threshold = 10;\n    const __m256i threshold_vec = _mm256_set1_epi32(threshold);\n    const __m256i odd_first_digit_mask = _mm256_set1_epi32(0x88888888);\n    const __m256i odd_last_digit_mask = _mm256_set1_epi32(0x11111111);\n    const __m256i odd_digits = _mm256_set1_epi32(0x66666666);\n\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i first_digit = _mm256_srli_epi32(abs_vec, 28);\n        __m256i last_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(0xF));\n\n        __m256i first_digit_odd = _mm256_and_si256(first_digit, odd_digits);\n        __m256i last_digit_odd = _mm256_and_si256(last_digit, odd_digits);\n\n        __m256i mask = _mm256_and_si256(\n            _mm256_cmpgt_epi32(vec, threshold_vec),\n            _mm256_and_si256(first_digit_odd, last_digit_odd)\n        );\n\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(mask, _mm256_set1_epi32(-1)));\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > threshold && ((num / 1000000000) % 10) % 2 == 1 && (num % 10) % 2 == 1) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int>& nums) {\n    const int threshold = 10;\n    const __m256i threshold_vec = _mm256_set1_epi32(threshold);\n    const __m256i odd_digits_mask = _mm256_set1_epi32(0x55555555); // Mask for odd digits (1, 3, 5, 7, 9)\n    const __m256i sign_mask = _mm256_set1_epi32(0x80000000); // Mask for sign bit\n\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        __m256i first_digit = _mm256_srli_epi32(abs_vec, 28); // Shift right to get the first digit\n        __m256i last_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(0xF)); // Mask to get the last digit\n\n        __m256i first_digit_odd = _mm256_and_si256(first_digit, odd_digits_mask);\n        __m256i last_digit_odd = _mm256_and_si256(last_digit, odd_digits_mask);\n\n        __m256i both_odd = _mm256_and_si256(first_digit_odd, last_digit_odd);\n        __m256i greater_than_threshold = _mm256_cmpgt_epi32(vec, threshold_vec);\n\n        __m256i result = _mm256_and_si256(both_odd, greater_than_threshold);\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_setzero_si256())) >> 2;\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        int abs_num = abs(num);\n        int first_digit = abs_num / 1000000000;\n        int last_digit = abs_num % 10;\n\n        if (num > threshold && (first_digit & 1) && (last_digit & 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i vindex = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vthree = _mm256_set1_epi32(3);\n\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_add_epi32(vindex, _mm256_set1_epi32(i));\n        __m256i vsq = _mm256_mullo_epi32(vi, vi);\n        __m256i va = _mm256_sub_epi32(vsq, vi);\n        va = _mm256_add_epi32(va, vone);\n\n        for (uint64_t j = i + 1; j < n; j += 8) {\n            __m256i vj = _mm256_add_epi32(vindex, _mm256_set1_epi32(j));\n            __m256i vsqj = _mm256_mullo_epi32(vj, vj);\n            __m256i vaj = _mm256_sub_epi32(vsqj, vj);\n            vaj = _mm256_add_epi32(vaj, vone);\n\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_add_epi32(vindex, _mm256_set1_epi32(k));\n                __m256i vsqk = _mm256_mullo_epi32(vk, vk);\n                __m256i vak = _mm256_sub_epi32(vsqk, vk);\n                vak = _mm256_add_epi32(vak, vone);\n\n                __m256i sum = _mm256_add_epi32(va, vaj);\n                sum = _mm256_add_epi32(sum, vak);\n                __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(sum, vthree), _mm256_setzero_si256());\n                count += _mm256_movemask_epi8(mask) >> 24;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i vec_i = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i vec_j = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n    __m256i vec_k = _mm256_setr_epi32(3, 4, 5, 6, 7, 8, 9, 10);\n    __m256i vec_ones = _mm256_set1_epi32(1);\n    __m256i vec_threes = _mm256_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n - 2; i += 8) {\n        __m256i vec_a_i = _mm256_mullo_epi32(vec_i, vec_i);\n        vec_a_i = _mm256_sub_epi32(vec_a_i, vec_i);\n        vec_a_i = _mm256_add_epi32(vec_a_i, vec_ones);\n\n        __m256i vec_a_j = _mm256_mullo_epi32(vec_j, vec_j);\n        vec_a_j = _mm256_sub_epi32(vec_a_j, vec_j);\n        vec_a_j = _mm256_add_epi32(vec_a_j, vec_ones);\n\n        __m256i vec_a_k = _mm256_mullo_epi32(vec_k, vec_k);\n        vec_a_k = _mm256_sub_epi32(vec_a_k, vec_k);\n        vec_a_k = _mm256_add_epi32(vec_a_k, vec_ones);\n\n        __m256i sum_ijk = _mm256_add_epi32(vec_a_i, vec_a_j);\n        sum_ijk = _mm256_add_epi32(sum_ijk, vec_a_k);\n\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(sum_ijk, vec_threes), _mm256_setzero_si256());\n        count += _mm256_movemask_epi8(mask) & 0x01010101;\n\n        vec_i = _mm256_add_epi32(vec_i, vec_ones);\n        vec_j = _mm256_add_epi32(vec_j, vec_ones);\n        vec_k = _mm256_add_epi32(vec_k, vec_ones);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i vindex = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vthree = _mm256_set1_epi32(3);\n\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_add_epi32(vindex, _mm256_set1_epi32(i));\n        __m256i vsq = _mm256_mullo_epi32(vi, vi);\n        __m256i va = _mm256_sub_epi32(vsq, vi);\n        va = _mm256_add_epi32(va, vone);\n\n        for (uint64_t j = i + 1; j < n; j += 8) {\n            __m256i vj = _mm256_add_epi32(vindex, _mm256_set1_epi32(j));\n            __m256i vsqj = _mm256_mullo_epi32(vj, vj);\n            __m256i vaj = _mm256_sub_epi32(vsqj, vj);\n            vaj = _mm256_add_epi32(vaj, vone);\n\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_add_epi32(vindex, _mm256_set1_epi32(k));\n                __m256i vsqk = _mm256_mullo_epi32(vk, vk);\n                __m256i vak = _mm256_sub_epi32(vsqk, vk);\n                vak = _mm256_add_epi32(vak, vone);\n\n                __m256i vsum = _mm256_add_epi32(va, vaj);\n                vsum = _mm256_add_epi32(vsum, vak);\n                __m256i vmod = _mm256_and_si256(_mm256_sub_epi32(vsum, vone), _mm256_set1_epi32(2));\n                __m256i vcmp = _mm256_cmpeq_epi32(vmod, _mm256_setzero_si256());\n\n                count += _mm256_movemask_epi8(vcmp) >> 24;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i vindex = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vthree = _mm256_set1_epi32(3);\n\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_add_epi32(vindex, _mm256_set1_epi32(i));\n        __m256i vsq = _mm256_mullo_epi32(vi, vi);\n        __m256i va = _mm256_sub_epi32(vsq, vi);\n        va = _mm256_add_epi32(va, vone);\n\n        for (uint64_t j = i + 1; j < n; j += 8) {\n            __m256i vj = _mm256_add_epi32(vindex, _mm256_set1_epi32(j));\n            __m256i vsqj = _mm256_mullo_epi32(vj, vj);\n            __m256i vaj = _mm256_sub_epi32(vsqj, vj);\n            vaj = _mm256_add_epi32(vaj, vone);\n\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_add_epi32(vindex, _mm256_set1_epi32(k));\n                __m256i vsqk = _mm256_mullo_epi32(vk, vk);\n                __m256i vak = _mm256_sub_epi32(vsqk, vk);\n                vak = _mm256_add_epi32(vak, vone);\n\n                __m256i sum = _mm256_add_epi32(va, vaj);\n                sum = _mm256_add_epi32(sum, vak);\n                __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(sum, vthree), _mm256_setzero_si256());\n                count += _mm256_movemask_epi8(mask) >> 24;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i threes = _mm256_set1_epi32(3);\n\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i i_vec = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n        __m256i a_i = _mm256_sub_epi32(_mm256_add_epi32(_mm256_mullo_epi32(i_vec, i_vec), ones), i_vec);\n\n        for (uint64_t j = i + 1; j < n; j += 8) {\n            __m256i j_vec = _mm256_add_epi32(indices, _mm256_set1_epi32(j));\n            __m256i a_j = _mm256_sub_epi32(_mm256_add_epi32(_mm256_mullo_epi32(j_vec, j_vec), ones), j_vec);\n\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i k_vec = _mm256_add_epi32(indices, _mm256_set1_epi32(k));\n                __m256i a_k = _mm256_sub_epi32(_mm256_add_epi32(_mm256_mullo_epi32(k_vec, k_vec), ones), k_vec);\n\n                __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a_i, a_j), a_k);\n                __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(sum, threes), _mm256_setzero_si256());\n\n                count += _mm256_movemask_epi8(mask) & 0x01010101;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n\n    __m256i vec_prime = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i vec_prime2 = _mm256_setr_epi32(23, 29, 31, 37, 41, 43, 47, 53);\n\n    __m256i cmp1 = _mm256_cmpeq_epi32(vec_n, vec_prime);\n    __m256i cmp2 = _mm256_cmpeq_epi32(vec_n, vec_prime2);\n\n    __m256i cmp_result = _mm256_or_si256(cmp1, cmp2);\n\n    int mask = _mm256_movemask_epi8(cmp_result);\n\n    return (mask != 0) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n\n    __m256i vec_2 = _mm256_set1_epi32(2);\n    __m256i vec_3 = _mm256_set1_epi32(3);\n    __m256i vec_5 = _mm256_set1_epi32(5);\n    __m256i vec_7 = _mm256_set1_epi32(7);\n\n    __m256i div_2 = _mm256_div_epi32(vec_n, vec_2);\n    __m256i div_3 = _mm256_div_epi32(vec_n, vec_3);\n    __m256i div_5 = _mm256_div_epi32(vec_n, vec_5);\n    __m256i div_7 = _mm256_div_epi32(vec_n, vec_7);\n\n    __m256i mul_2 = _mm256_mullo_epi32(div_2, vec_2);\n    __m256i mul_3 = _mm256_mullo_epi32(div_3, vec_3);\n    __m256i mul_5 = _mm256_mullo_epi32(div_5, vec_5);\n    __m256i mul_7 = _mm256_mullo_epi32(div_7, vec_7);\n\n    __m256i cmp_2 = _mm256_cmpeq_epi32(mul_2, vec_n);\n    __m256i cmp_3 = _mm256_cmpeq_epi32(mul_3, vec_n);\n    __m256i cmp_5 = _mm256_cmpeq_epi32(mul_5, vec_n);\n    __m256i cmp_7 = _mm256_cmpeq_epi32(mul_7, vec_n);\n\n    __m256i or_cmp = _mm256_or_si256(_mm256_or_si256(cmp_2, cmp_3), _mm256_or_si256(cmp_5, cmp_7));\n    int mask = _mm256_movemask_epi8(or_cmp);\n\n    return (mask == 0) ? y : x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n\n    __m256i vec_2 = _mm256_set1_epi32(2);\n    __m256i vec_3 = _mm256_set1_epi32(3);\n    __m256i vec_5 = _mm256_set1_epi32(5);\n    __m256i vec_7 = _mm256_set1_epi32(7);\n\n    __m256i div_2 = _mm256_div_epi32(vec_n, vec_2);\n    __m256i div_3 = _mm256_div_epi32(vec_n, vec_3);\n    __m256i div_5 = _mm256_div_epi32(vec_n, vec_5);\n    __m256i div_7 = _mm256_div_epi32(vec_n, vec_7);\n\n    __m256i mod_2 = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(div_2, vec_2));\n    __m256i mod_3 = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(div_3, vec_3));\n    __m256i mod_5 = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(div_5, vec_5));\n    __m256i mod_7 = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(div_7, vec_7));\n\n    __m256i is_prime = _mm256_and_si256(\n        _mm256_and_si256(\n            _mm256_cmpeq_epi32(mod_2, _mm256_setzero_si256()),\n            _mm256_cmpeq_epi32(mod_3, _mm256_setzero_si256())\n        ),\n        _mm256_and_si256(\n            _mm256_cmpeq_epi32(mod_5, _mm256_setzero_si256()),\n            _mm256_cmpeq_epi32(mod_7, _mm256_setzero_si256())\n        )\n    );\n\n    __m256i result = _mm256_blendv_epi8(vec_y, vec_x, is_prime);\n\n    return _mm256_extract_epi32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n\n    __m256i vec_prime = _mm256_set1_epi32(1);\n    __m256i vec_non_prime = _mm256_set1_epi32(0);\n\n    __m256i vec_result = _mm256_blendv_epi8(vec_y, vec_x, vec_prime);\n\n    int result = _mm256_extract_epi32(vec_result, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n\n    __m256i vec_prime = _mm256_set1_epi32(1);\n    __m256i vec_non_prime = _mm256_set1_epi32(0);\n\n    __m256i vec_result = _mm256_blendv_epi8(vec_y, vec_x, vec_prime);\n\n    int result = _mm256_extract_epi32(vec_result, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(vec);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 mask = _mm256_cmp_ps(vec, float_vec, _CMP_EQ_OQ);\n        __m256i odd_mask = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i odd_int_vec = _mm256_and_si256(int_vec, _mm256_cmpeq_epi32(odd_mask, _mm256_set1_epi32(1)));\n        __m256i square_vec = _mm256_mullo_epi32(odd_int_vec, odd_int_vec);\n        __m256i masked_square_vec = _mm256_and_si256(square_vec, _mm256_castps_si256(mask));\n        sum_vec = _mm256_add_epi32(sum_vec, masked_square_vec);\n    }\n\n    alignas(32) int32_t sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_array[j];\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] > 0 && lst[i] == static_cast<int>(lst[i]) && static_cast<int>(lst[i]) % 2 != 0) {\n            sum += static_cast<int>(lst[i]) * static_cast<int>(lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sum = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(vec);\n        __m256i odd_mask = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i non_negative_mask = _mm256_cmpgt_epi32(int_vec, zero);\n        __m256i valid_mask = _mm256_and_si256(odd_mask, non_negative_mask);\n\n        __m256i squares = _mm256_mullo_epi32(int_vec, int_vec);\n        __m256i valid_squares = _mm256_and_si256(squares, valid_mask);\n\n        sum = _mm256_add_epi32(sum, valid_squares);\n    }\n\n    int32_t temp[vec_size];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    uint64_t result = 0;\n    for (int i = 0; i < vec_size; ++i) {\n        result += temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i int_vec = _mm256_cvtps_epi32(vec);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 mask = _mm256_cmp_ps(vec, float_vec, _CMP_EQ_OQ);\n        __m256i odd_mask = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i odd_vec = _mm256_and_si256(int_vec, _mm256_cmpeq_epi32(odd_mask, _mm256_set1_epi32(1)));\n        __m256i square_vec = _mm256_mullo_epi32(odd_vec, odd_vec);\n        __m256i masked_square_vec = _mm256_and_si256(square_vec, _mm256_castps_si256(mask));\n        sum_vec = _mm256_add_epi32(sum_vec, masked_square_vec);\n    }\n\n    alignas(32) int32_t sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_array[j];\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] > 0 && lst[i] == static_cast<int>(lst[i]) && static_cast<int>(lst[i]) % 2 != 0) {\n            sum += static_cast<int>(lst[i]) * static_cast<int>(lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i veci = _mm256_cvtps_epi32(vec);\n\n        __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi32(veci, zero), _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_and_si256(veci, one), one), _mm256_cmpgt_epi32(veci, zero)));\n\n        __m256i squares = _mm256_mullo_epi32(veci, veci);\n        __m256i result = _mm256_and_si256(squares, mask);\n\n        alignas(32) int32_t temp[8];\n        _mm256_store_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vectorSize = 8;\n    __m256 sumVec = _mm256_setzero_ps();\n    __m256i zeroVec = _mm256_set1_epi32(0);\n    __m256i oneVec = _mm256_set1_epi32(1);\n    __m256i twoVec = _mm256_set1_epi32(2);\n\n    for (size_t i = 0; i < lst.size(); i += vectorSize) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i intVec = _mm256_cvtps_epi32(vec);\n        __m256i oddMask = _mm256_and_si256(_mm256_cmpgt_epi32(intVec, zeroVec), _mm256_cmpeq_epi32(_mm256_and_si256(intVec, oneVec), oneVec));\n        __m256i oddSquares = _mm256_mullo_epi32(intVec, intVec);\n        oddSquares = _mm256_and_si256(oddSquares, oddMask);\n        __m256 oddSquaresFloat = _mm256_cvtepi32_ps(oddSquares);\n        sumVec = _mm256_add_ps(sumVec, oddSquaresFloat);\n    }\n\n    float sumArray[vectorSize];\n    _mm256_storeu_ps(sumArray, sumVec);\n    float sum = 0.0f;\n    for (int i = 0; i < vectorSize; ++i) {\n        sum += sumArray[i];\n    }\n\n    return static_cast<uint64_t>(sum);\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff_vec);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff_vec);\n    }\n\n    return result;\n}\n```"}

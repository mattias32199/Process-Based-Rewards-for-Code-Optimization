{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const*)(dst + i));\n        \n        __m128i mask_vec = _mm_set_epi8(\n            mask[i+3], mask[i+3], mask[i+3], mask[i+3], mask[i+3], mask[i+3], mask[i+3], mask[i+3],\n            mask[i+2], mask[i+2], mask[i+2], mask[i+2], mask[i+2], mask[i+2], mask[i+2], mask[i+2],\n            mask[i+1], mask[i+1], mask[i+1], mask[i+1], mask[i+1], mask[i+1], mask[i+1], mask[i+1],\n            mask[i], mask[i], mask[i], mask[i], mask[i], mask[i], mask[i], mask[i]\n        );\n        __m256i mask_256 = _mm256_cvtepi8_epi64(mask_vec);\n        \n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_256);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const*)(dst + i));\n        \n        __m128i mask_vec = _mm_set_epi32(\n            mask[i+3] ? -1 : 0,\n            mask[i+2] ? -1 : 0,\n            mask[i+1] ? -1 : 0,\n            mask[i] ? -1 : 0\n        );\n        __m256i mask_256 = _mm256_cvtepi32_epi64(mask_vec);\n        \n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_256);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const*)(dst + i));\n        \n        __m128i mask_vec = _mm_set_epi32(\n            mask[i+3] ? -1 : 0,\n            mask[i+2] ? -1 : 0,\n            mask[i+1] ? -1 : 0,\n            mask[i] ? -1 : 0\n        );\n        __m256i mask_256 = _mm256_cvtepi32_epi64(mask_vec);\n        \n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_256);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const*)(dst + i));\n        __m128i mask_vec = _mm_loadu_si128((__m128i const*)(mask + i));\n        \n        mask_vec = _mm_cmpeq_epi8(mask_vec, _mm_set1_epi8(1));\n        __m256i mask_256 = _mm256_cvtepi8_epi64(mask_vec);\n        \n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_256);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const*)(dst + i));\n        __m128i mask_vec = _mm_loadu_si128((__m128i const*)(mask + i));\n        \n        mask_vec = _mm_cmplt_epi8(mask_vec, _mm_setzero_si128());\n        __m256i mask_256 = _mm256_cvtepi8_epi64(mask_vec);\n        \n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_256);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_shift_right = _mm256_set1_epi32(3);\n    __m256i v_shift_left = _mm256_set1_epi32(3);\n    __m256i v_divisor = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n        \n        __m256i v_tmp1 = _mm256_sra_epi32(v_scaled, v_shift_right);\n        __m256i v_tmp2 = _mm256_sll_epi32(v_scaled, v_shift_left);\n        \n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        \n        __m256i v_result = _mm256_div_epi32(v_sum, v_divisor);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256i seven_vec = _mm256_set1_epi32(7);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i scaled = _mm256_mullo_epi32(src_vec, scale_vec);\n        \n        __m256i tmp1 = _mm256_srai_epi32(scaled, 3);\n        __m256i tmp2 = _mm256_slli_epi32(scaled, 3);\n        \n        __m256i tmp1_mul_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(scaled, tmp1_mul_tmp2);\n        \n        __m256i result = _mm256_div_epi32(sum, seven_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n        \n        __m256i v_tmp1 = _mm256_srai_epi32(v_scaled, 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n        \n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        \n        __m256i v_result = _mm256_div_epi32(v_sum, v_seven);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n    \n    for (; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256i seven_vec = _mm256_set1_epi32(7);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i scaled = _mm256_mullo_epi32(src_vec, scale_vec);\n        \n        __m256i tmp1 = _mm256_srai_epi32(scaled, 3);\n        __m256i tmp2 = _mm256_slli_epi32(scaled, 3);\n        \n        __m256i tmp1_mul_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(scaled, tmp1_mul_tmp2);\n        \n        __m256i result = _mm256_div_epi32(sum, seven_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256i seven_vec = _mm256_set1_epi32(7);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i scaled = _mm256_mullo_epi32(src_vec, scale_vec);\n        \n        __m256i tmp1 = _mm256_srai_epi32(scaled, 3);\n        __m256i tmp2 = _mm256_slli_epi32(scaled, 3);\n        \n        __m256i tmp1_mul_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(scaled, tmp1_mul_tmp2);\n        \n        __m256i result = _mm256_div_epi32(sum, seven_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 3 < rows; i += 4) {\n        __m256d src_vec = _mm256_set_pd(\n            src[i * cols + stride],\n            src[(i + 1) * cols + stride],\n            src[(i + 2) * cols + stride],\n            src[(i + 3) * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], src_vec);\n    }\n    \n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    size_t remaining = rows % 4;\n    \n    for (i = 0; i < rows - remaining; i += 4) {\n        __m256d v_src0 = _mm256_set1_pd(src[i * cols + stride]);\n        __m256d v_src1 = _mm256_set1_pd(src[(i + 1) * cols + stride]);\n        __m256d v_src2 = _mm256_set1_pd(src[(i + 2) * cols + stride]);\n        __m256d v_src3 = _mm256_set1_pd(src[(i + 3) * cols + stride]);\n        \n        __m256d v_dst = _mm256_unpacklo_pd(\n            _mm256_unpacklo_pd(v_src0, v_src1),\n            _mm256_unpacklo_pd(v_src2, v_src3)\n        );\n        \n        _mm256_storeu_pd(&dst[i], v_dst);\n    }\n    \n    for (; i < rows; i++) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    const size_t vec_size = 4; // AVX can process 4 doubles at once\n    size_t vec_end = rows - (rows % vec_size);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256d vec = _mm256_set_pd(\n            src[(i+3) * cols + stride],\n            src[(i+2) * cols + stride],\n            src[(i+1) * cols + stride],\n            src[i * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    for (; i < rows; i++) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    const size_t vec_size = 4; // AVX can process 4 doubles at once\n    size_t remaining = rows % vec_size;\n    \n    for (r = 0; r < rows - remaining; r += vec_size) {\n        __m256d v_src0 = _mm256_set_pd(\n            src[(r + 3) * cols + stride],\n            src[(r + 2) * cols + stride],\n            src[(r + 1) * cols + stride],\n            src[r * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[r], v_src0);\n    }\n    \n    for (; r < rows; r++) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    const size_t vec_size = 4; // AVX can process 4 doubles at once\n    size_t remaining = rows % vec_size;\n    \n    for (r = 0; r < rows - remaining; r += vec_size) {\n        __m256d v_src0 = _mm256_set_pd(\n            src[(r + 3) * cols + stride],\n            src[(r + 2) * cols + stride],\n            src[(r + 1) * cols + stride],\n            src[r * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[r], v_src0);\n    }\n    \n    for (; r < rows; r++) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 src_data = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, src_data);\n        \n        __m256 src_vec = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, idx, src_vec, sizeof(float));\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 src_data = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, src_data);\n\n        __m256 src_vec = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, idx, src_vec, 4);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 src_data = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, src_data);\n\n        __m256 src_vec = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, idx, src_vec, sizeof(float));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 src_data = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, src_data);\n\n        __m256 src_vec = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, idx, src_vec, 4);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 src_data = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, src_data);\n        \n        __m256 src_vec = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, idx, src_vec, sizeof(float));\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(arr + start));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(arr + end - 15));\n        \n        vec1 = _mm256_shuffle_epi8(vec1, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        vec2 = _mm256_shuffle_epi8(vec2, _mm256_set_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        \n        vec1 = _mm256_permute2x128_si256(vec1, vec1, 0x01);\n        vec2 = _mm256_permute2x128_si256(vec2, vec2, 0x01);\n        \n        _mm256_storeu_si256((__m256i*)(arr + start), vec2);\n        _mm256_storeu_si256((__m256i*)(arr + end - 15), vec1);\n        \n        start += 16;\n        end -= 16;\n    }\n    \n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        // Load 16 elements from start and end positions\n        __m256i vec_start = _mm256_loadu_si256((__m256i*)(arr + start));\n        __m256i vec_end = _mm256_loadu_si256((__m256i*)(arr + end - 15));\n\n        // Reverse the order of elements in the vectors\n        vec_start = _mm256_permute4x64_epi64(_mm256_shuffle_epi32(vec_start, _MM_SHUFFLE(2, 3, 0, 1)), _MM_SHUFFLE(0, 1, 2, 3));\n        vec_end = _mm256_permute4x64_epi64(_mm256_shuffle_epi32(vec_end, _MM_SHUFFLE(2, 3, 0, 1)), _MM_SHUFFLE(0, 1, 2, 3));\n\n        // Swap the vectors\n        _mm256_storeu_si256((__m256i*)(arr + start), vec_end);\n        _mm256_storeu_si256((__m256i*)(arr + end - 15), vec_start);\n\n        start += 16;\n        end -= 16;\n    }\n\n    // Handle remaining elements (less than 16) with scalar operations\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        // Load 16 elements from start and end positions\n        __m256i vec_start = _mm256_loadu_si256((__m256i*)(arr + start));\n        __m256i vec_end = _mm256_loadu_si256((__m256i*)(arr + end - 15));\n\n        // Reverse the order of elements in the vectors\n        vec_start = _mm256_permute4x64_epi64(vec_start, 0x1B);\n        vec_end = _mm256_permute4x64_epi64(vec_end, 0x1B);\n\n        // Swap the vectors\n        _mm256_storeu_si256((__m256i*)(arr + start), vec_end);\n        _mm256_storeu_si256((__m256i*)(arr + end - 15), vec_start);\n\n        start += 16;\n        end -= 16;\n    }\n\n    // Handle remaining elements (less than 16) with scalar code\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(arr + start));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(arr + end - 15));\n        \n        vec1 = _mm256_permute4x64_epi64(vec1, 0x1B);\n        vec2 = _mm256_permute4x64_epi64(vec2, 0x1B);\n        \n        vec1 = _mm256_shuffle_epi8(vec1, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        vec2 = _mm256_shuffle_epi8(vec2, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        \n        _mm256_storeu_si256((__m256i*)(arr + start), vec2);\n        _mm256_storeu_si256((__m256i*)(arr + end - 15), vec1);\n        \n        start += 16;\n        end -= 16;\n    }\n    \n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(arr + start));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(arr + end - 15));\n        \n        vec1 = _mm256_permute4x64_epi64(vec1, 0x1B);\n        vec2 = _mm256_permute4x64_epi64(vec2, 0x1B);\n        \n        vec1 = _mm256_shuffle_epi8(vec1, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        vec2 = _mm256_shuffle_epi8(vec2, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        \n        _mm256_storeu_si256((__m256i*)(arr + start), vec2);\n        _mm256_storeu_si256((__m256i*)(arr + end - 15), vec1);\n        \n        start += 16;\n        end -= 16;\n    }\n    \n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t aligned_size = slice_size & ~31;\n    \n    for (; i < aligned_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(src + i));\n        _mm256_storeu_si256((__m256i*)(dst + i), data);\n    }\n    \n    for (; i < slice_size; ++i) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + simd_size <= slice_size; i += simd_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(src + i));\n        _mm256_storeu_si256((__m256i*)(dst + i), data);\n    }\n    \n    // Handle remaining elements\n    for (; i < slice_size; i++) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = slice_size / 32;\n    \n    for (; i < simd_size * 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        _mm256_storeu_si256((__m256i*)(dst + i), data);\n    }\n    \n    for (; i < slice_size; ++i) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + simd_size <= slice_size; i += simd_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(src + i));\n        _mm256_storeu_si256((__m256i*)(dst + i), data);\n    }\n    \n    // Handle remaining elements\n    for (; i < slice_size; i++) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + simd_size <= slice_size; i += simd_size) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        _mm256_storeu_si256((__m256i*)(dst + i), data);\n    }\n    \n    // Handle remaining elements\n    for (; i < slice_size; i++) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i tmp0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i tmp1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i tmp2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i tmp3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i tmp4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i tmp5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i tmp6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i tmp7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i res0 = _mm_unpacklo_epi32(tmp0, tmp1);\n                    __m128i res1 = _mm_unpackhi_epi32(tmp0, tmp1);\n                    __m128i res2 = _mm_unpacklo_epi32(tmp2, tmp3);\n                    __m128i res3 = _mm_unpackhi_epi32(tmp2, tmp3);\n                    __m128i res4 = _mm_unpacklo_epi32(tmp4, tmp5);\n                    __m128i res5 = _mm_unpackhi_epi32(tmp4, tmp5);\n                    __m128i res6 = _mm_unpacklo_epi32(tmp6, tmp7);\n                    __m128i res7 = _mm_unpackhi_epi32(tmp6, tmp7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), res0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), res1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), res2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), res3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), res4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), res5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), res6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), res7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 16) {\n                for (size_t jj = j; jj < max_j; jj++) {\n                    __m256i row_data = _mm256_loadu_si256((__m256i*)(src + ii * cols + jj));\n                    for (int k = 0; k < 16; k++) {\n                        dst[(jj + k) * rows + ii] = _mm256_extract_epi16(row_data, k);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 16) {\n                size_t remaining_rows = max_i - ii;\n                size_t simd_rows = remaining_rows < 16 ? remaining_rows : 16;\n                \n                for (size_t jj = j; jj < max_j; ++jj) {\n                    __m256i row_data = _mm256_loadu_si256((__m256i*)(src + ii * cols + jj));\n                    \n                    for (size_t k = 0; k < simd_rows; ++k) {\n                        dst[jj * rows + ii + k] = src[(ii + k) * cols + jj];\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    float sum = 0.0f;\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n    \n    // Process 8 elements at a time\n    size_t i;\n    for (i = 0; i + 8 <= dim; i += 8) {\n        size_t index = i * stride;\n        __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n        sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n    }\n    \n    // Horizontal sum of the vector\n    __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum += _mm_cvtss_f32(sum128);\n    \n    // Process remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * stride;\n        sum += array[index];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    float sum = 0.0f;\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n    \n    // Process elements in chunks of 8\n    size_t i;\n    for (i = 0; i + 8 <= dim; i += 8) {\n        size_t index = i * stride;\n        __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n        sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n    }\n    \n    // Horizontal sum of the vector\n    __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum += _mm_cvtss_f32(sum128);\n    \n    // Process remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * stride;\n        sum += array[index];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    float sum = 0.0f;\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n    \n    // Process 8 elements at a time\n    size_t i;\n    for (i = 0; i + 8 <= dim; i += 8) {\n        size_t index = i * stride;\n        __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n        sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n    }\n    \n    // Sum the elements in the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; j++) {\n        sum += temp[j];\n    }\n    \n    // Process remaining elements\n    for (; i < dim; i++) {\n        size_t index = i * stride;\n        sum += array[index];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    float sum = 0.0f;\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i + 8 <= dim; i += 8) {\n        size_t index = i * stride;\n        __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n        sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n    }\n\n    // Sum the elements in the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; j++) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < dim; i++) {\n        size_t index = i * stride;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    float sum = 0.0f;\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n    \n    // Process elements in chunks of 8\n    size_t i;\n    for (i = 0; i + 8 <= dim; i += 8) {\n        size_t index = i * stride;\n        __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n        sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n    }\n    \n    // Horizontal sum of the vector\n    __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum += _mm_cvtss_f32(sum128);\n    \n    // Process remaining elements\n    for (; i < dim; i++) {\n        size_t index = i * stride;\n        sum += array[index];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d scl = _mm256_set1_pd(scale);\n    size_t total_elements = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh, _CMP_GT_OQ);\n        __m256d scaled = _mm256_mul_pd(data, scl);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d scl = _mm256_set1_pd(scale);\n    size_t total_elements = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh, _CMP_GT_OQ);\n        __m256d scaled = _mm256_mul_pd(data, scl);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d scl = _mm256_set1_pd(scale);\n    size_t total_elements = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh, _CMP_GT_OQ);\n        __m256d scaled = _mm256_mul_pd(data, scl);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d scl = _mm256_set1_pd(scale);\n    size_t total_elements = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh, _CMP_GT_OQ);\n        __m256d scaled = _mm256_mul_pd(data, scl);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d scl = _mm256_set1_pd(scale);\n    size_t total_elements = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh, _CMP_GT_OQ);\n        __m256d scaled = _mm256_mul_pd(data, scl);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_chunks = cols / 4;\n    size_t col_remainder = cols % 4;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n\n        for (size_t j = 0; j < col_chunks; ++j) {\n            __m256d data = _mm256_loadu_pd(src_ptr + j * 4);\n            _mm256_storeu_pd(dst_ptr + j * 4, data);\n        }\n\n        for (size_t j = 0; j < col_remainder; ++j) {\n            dst_ptr[col_chunks * 4 + j] = src_ptr[col_chunks * 4 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_chunks = cols / 4;\n    size_t col_remainder = cols % 4;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        double *dst_ptr = dst + i * cols;\n        const double *src_ptr = src + src_row * cols;\n\n        for (size_t j = 0; j < col_chunks; ++j) {\n            __m256d data = _mm256_loadu_pd(src_ptr + j * 4);\n            _mm256_storeu_pd(dst_ptr + j * 4, data);\n        }\n\n        for (size_t j = 0; j < col_remainder; ++j) {\n            dst_ptr[col_chunks * 4 + j] = src_ptr[col_chunks * 4 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_chunks = cols / 4;\n    size_t col_remainder = cols % 4;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n\n        for (size_t j = 0; j < col_chunks; ++j) {\n            __m256d row_data = _mm256_loadu_pd(src_ptr + j * 4);\n            _mm256_storeu_pd(dst_ptr + j * 4, row_data);\n        }\n\n        for (size_t j = col_chunks * 4; j < cols; ++j) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_chunks = cols / 4;\n    size_t col_remainder = cols % 4;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        double *dst_ptr = dst + i * cols;\n        const double *src_ptr = src + src_row * cols;\n\n        for (size_t j = 0; j < col_chunks; ++j) {\n            __m256d row_data = _mm256_loadu_pd(src_ptr + j * 4);\n            _mm256_storeu_pd(dst_ptr + j * 4, row_data);\n        }\n\n        for (size_t j = 0; j < col_remainder; ++j) {\n            dst_ptr[col_chunks * 4 + j] = src_ptr[col_chunks * 4 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_chunks = cols / 4;\n    size_t col_remainder = cols % 4;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        double *dst_ptr = dst + i * cols;\n        const double *src_ptr = src + src_row * cols;\n\n        for (size_t j = 0; j < col_chunks; ++j) {\n            __m256d row_data = _mm256_loadu_pd(src_ptr + j * 4);\n            _mm256_storeu_pd(dst_ptr + j * 4, row_data);\n        }\n\n        for (size_t j = 0; j < col_remainder; ++j) {\n            dst_ptr[col_chunks * 4 + j] = src_ptr[col_chunks * 4 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n        v_sum = _mm256_div_epi64(v_sum, v_two);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), v_sum);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n        v_sum = _mm256_div_epi64(v_sum, v_two);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), v_sum);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i;\n    __m256i vec_scalar = _mm256_set1_epi64x(scalar);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i vec_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vec_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        __m256i vec_sum = _mm256_add_epi64(vec_src1, vec_src2);\n        vec_sum = _mm256_add_epi64(vec_sum, vec_scalar);\n        vec_sum = _mm256_srai_epi64(vec_sum, 1);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), vec_sum);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        __m256i v_sum_scalar = _mm256_add_epi64(v_sum, v_scalar);\n        __m256i v_result = _mm256_srai_epi64(v_sum_scalar, 1);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n        v_sum = _mm256_div_epi64(v_sum, v_two);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_sum);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i evens = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, mask);\n        min_vec = _mm256_min_epi32(min_vec, evens);\n    }\n\n    int min_arr[8];\n    _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n    int min_val = INT_MAX;\n    for (int j = 0; j < 8; j++) {\n        if (min_arr[j] < min_val) {\n            min_val = min_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, mask), _mm256_setzero_si256());\n        __m256i masked_data = _mm256_blendv_epi8(data, _mm256_set1_epi32(INT_MAX), even_mask);\n        min_vec = _mm256_min_epi32(min_vec, masked_data);\n    }\n\n    int min_val = INT_MAX;\n    int temp_min[8];\n    _mm256_storeu_si256((__m256i*)temp_min, min_vec);\n    for (int j = 0; j < 8; j++) {\n        if (temp_min[j] < min_val) {\n            min_val = temp_min[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == INT_MAX ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i evens = _mm256_and_si256(data, mask);\n        min_vec = _mm256_min_epi32(min_vec, evens);\n    }\n\n    int min_val = INT_MAX;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, min_vec);\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask = _mm256_setr_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i evens = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, mask);\n        min_vec = _mm256_min_epi32(min_vec, evens);\n    }\n\n    int min_val = INT_MAX;\n    int temp_min[8];\n    _mm256_storeu_si256((__m256i*)temp_min, min_vec);\n\n    for (int j = 0; j < 8; j++) {\n        if (temp_min[j] < min_val) {\n            min_val = temp_min[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i even_vec = _mm256_and_si256(vec, mask);\n        min_vec = _mm256_min_epi32(min_vec, even_vec);\n    }\n\n    int min_val = INT_MAX;\n    int temp_min[8];\n    _mm256_storeu_si256((__m256i*)temp_min, min_vec);\n\n    for (int j = 0; j < 8; j++) {\n        if (temp_min[j] < min_val) {\n            min_val = temp_min[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == INT_MAX ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i arith_shift = _mm256_srai_epi32(src_vec, shift);\n        __m256i logical_shift = _mm256_srli_epi32(src_vec, shift);\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_srai_epi32(src_vec, shift);\n        \n        // Logical right shift (convert to uint, shift, then cast back)\n        __m256i logical_shift = _mm256_srli_epi32(src_vec, shift);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        \n        // Store result\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_srai_epi32(src_vec, shift);\n        \n        // Logical right shift (convert to uint, shift, then cast back)\n        __m256i logical_shift = _mm256_srli_epi32(src_vec, shift);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        \n        // Store result\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_srai_epi32(data, shift);\n        \n        // Logical right shift (convert to uint, shift, then cast back)\n        __m256i logical_shift = _mm256_srli_epi32(data, shift);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int val = src[i];\n        int arith_shift = val >> shift;\n        int logical_shift = (unsigned int)val >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_srai_epi32(src_vec, shift);\n        \n        // Logical right shift (convert to uint, shift, then cast back)\n        __m256i logical_shift = _mm256_srli_epi32(src_vec, shift);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shift, logical_shift);\n        \n        // Store result\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t sum = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp_sum[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)temp_sum, sum_vec);\n\n    for (size_t j = 0; j < 8; ++j) {\n        sum += temp_sum[j];\n    }\n\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n    \n    int32_t sum_array[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    \n    int64_t sum = 0;\n    for (size_t j = 0; j < 8; j++) {\n        sum += sum_array[j];\n    }\n    \n    for (; i < total_elements; i++) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n    \n    int32_t sum_array[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                  sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n    \n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t sum_array[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                  sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t sum = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp_sum[8] __attribute__((aligned(32)));\n    _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n    for (size_t j = 0; j < 8; ++j) {\n        sum += temp_sum[j];\n    }\n\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    const size_t vec_size = 32;\n    const size_t vec_end = length - (length % vec_size);\n\n    __m256i max_vec = _mm256_set1_epi8(src[0]);\n    __m256i idx_vec = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i max_idx_vec = _mm256_setzero_si256();\n    __m256i inc_vec = _mm256_set1_epi8(32);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(data, max_vec);\n        max_vec = _mm256_max_epi8(max_vec, data);\n        max_idx_vec = _mm256_blendv_epi8(max_idx_vec, idx_vec, mask);\n        idx_vec = _mm256_add_epi8(idx_vec, inc_vec);\n    }\n\n    alignas(32) int8_t max_values[32];\n    alignas(32) int8_t max_indices[32];\n    _mm256_store_si256((__m256i*)max_values, max_vec);\n    _mm256_store_si256((__m256i*)max_indices, max_idx_vec);\n\n    for (size_t j = 0; j < 32; ++j) {\n        if (max_values[j] > max_val) {\n            max_val = max_values[j];\n            max_index = i - vec_size + j;\n        } else if (max_values[j] == max_val) {\n            size_t current_index = i - vec_size + j;\n            if (current_index < max_index) {\n                max_index = current_index;\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    const size_t vec_size = 32;\n    const size_t num_vecs = length / vec_size;\n    const size_t remainder = length % vec_size;\n\n    __m256i current_max = _mm256_set1_epi8(src[0]);\n    __m256i current_indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    for (; i < num_vecs * vec_size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(vec, current_max);\n        current_max = _mm256_max_epi8(vec, current_max);\n\n        __m256i indices = _mm256_add_epi32(_mm256_set1_epi32(i), current_indices);\n        current_indices = _mm256_blendv_epi8(current_indices, indices, cmp);\n\n        current_indices = _mm256_add_epi32(current_indices, increment);\n    }\n\n    alignas(32) int8_t max_values[32];\n    alignas(32) int32_t max_indices[8];\n    _mm256_store_si256((__m256i*)max_values, current_max);\n    _mm256_store_si256((__m256i*)max_indices, current_indices);\n\n    for (size_t j = 0; j < 32; ++j) {\n        if (max_values[j] > max_val) {\n            max_val = max_values[j];\n            max_index = i - vec_size + j;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Compare with current max\n        __m256i max_vec = _mm256_set1_epi8(max_val);\n        __m256i cmp = _mm256_cmpgt_epi8(vec, max_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            int pos = __builtin_ctz(mask);\n            max_val = src[i + pos];\n            max_index = i + pos;\n\n            // Check remaining elements in the current vector\n            for (int j = pos + 1; j < 32; ++j) {\n                if (src[i + j] > max_val) {\n                    max_val = src[i + j];\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    const size_t vec_size = 32;\n    const size_t num_vecs = length / vec_size;\n    const size_t remainder = length % vec_size;\n\n    __m256i max_vals = _mm256_set1_epi8(src[0]);\n    __m256i max_indices = _mm256_setzero_si256();\n\n    for (; i < num_vecs; ++i) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(src + i * vec_size));\n        __m256i indices = _mm256_add_epi8(_mm256_set1_epi8(i * vec_size), \n                                         _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                                          16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31));\n        \n        __m256i cmp = _mm256_cmpgt_epi8(current, max_vals);\n        max_vals = _mm256_max_epi8(max_vals, current);\n        max_indices = _mm256_blendv_epi8(max_indices, indices, cmp);\n    }\n\n    alignas(32) int8_t max_vals_arr[32];\n    alignas(32) int8_t max_indices_arr[32];\n    _mm256_store_si256((__m256i*)max_vals_arr, max_vals);\n    _mm256_store_si256((__m256i*)max_indices_arr, max_indices);\n\n    for (size_t j = 0; j < 32; ++j) {\n        if (max_vals_arr[j] > max_val) {\n            max_val = max_vals_arr[j];\n            max_index = max_indices_arr[j];\n        }\n    }\n\n    for (i = num_vecs * vec_size; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    const size_t vec_size = 32;\n    const size_t num_vecs = length / vec_size;\n    const size_t remainder = length % vec_size;\n\n    __m256i current_max = _mm256_set1_epi8(src[0]);\n    __m256i current_indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    for (i = 0; i < num_vecs; ++i) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(src + i * vec_size));\n        __m256i mask = _mm256_cmpgt_epi8(data, current_max);\n        current_max = _mm256_max_epi8(data, current_max);\n        current_indices = _mm256_add_epi32(current_indices, increment);\n    }\n\n    alignas(32) int8_t max_values[32];\n    alignas(32) int32_t max_indices[8];\n    _mm256_store_si256((__m256i*)max_values, current_max);\n    _mm256_store_si256((__m256i*)max_indices, current_indices);\n\n    for (i = 0; i < 32; ++i) {\n        if (max_values[i] > max_val) {\n            max_val = max_values[i];\n            max_index = i + (num_vecs * vec_size);\n        }\n    }\n\n    for (i = num_vecs * vec_size; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    \n    for (; i + 15 < length; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+14, i+12, i+10, i+8, i+6, i+4, i+2, i);\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        __m256i cmp_result = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        \n        __m256i src1_data = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i src2_data = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        __m256i blended = _mm256_blendv_epi8(src1_data, src2_data, cmp_result);\n        _mm256_storeu_si256((__m256i*)(dst + i), blended);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 16; // AVX2 can process 16 uint16_t elements at once\n    \n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    \n    for (; i + vec_size <= length; i += vec_size) {\n        // Generate indices [i, i+1, ..., i+15]\n        __m256i indices = _mm256_setr_epi16(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7,\n                                           i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        \n        // Compute (indices & mask)\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        \n        // Create mask by checking if (indices & mask) != 0\n        __m256i blend_mask = _mm256_cmpgt_epi16(and_result, _mm256_setzero_si256());\n        \n        // Load data from src1 and src2\n        __m256i src1_data = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i src2_data = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        // Blend based on the mask\n        __m256i result = _mm256_blendv_epi8(src2_data, src1_data, blend_mask);\n        \n        // Store the result\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 16; // AVX2 can process 16 uint16_t elements at once\n    \n    // Create a vector of mask values for comparison\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    \n    for (; i + vec_size <= length; i += vec_size) {\n        // Load 16 elements from src1 and src2\n        __m256i src1_vec = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        \n        // Generate indices 0..15\n        __m256i indices = _mm256_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n        __m256i current_indices = _mm256_add_epi16(indices, _mm256_set1_epi16(i));\n        \n        // Convert to 32-bit for AND operation\n        __m256i current_indices32 = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(current_indices, 0));\n        __m256i mask_result_lo = _mm256_and_si256(current_indices32, mask_vec);\n        current_indices32 = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(current_indices, 1));\n        __m256i mask_result_hi = _mm256_and_si256(current_indices32, mask_vec);\n        \n        // Compare with zero\n        __m256i cmp_lo = _mm256_cmpgt_epi32(mask_result_lo, _mm256_setzero_si256());\n        __m256i cmp_hi = _mm256_cmpgt_epi32(mask_result_hi, _mm256_setzero_si256());\n        \n        // Convert comparison results to 16-bit masks\n        __m128i cmp_lo16 = _mm_packs_epi32(_mm256_extracti128_si256(cmp_lo, 0), _mm256_extracti128_si256(cmp_lo, 1));\n        __m128i cmp_hi16 = _mm_packs_epi32(_mm256_extracti128_si256(cmp_hi, 0), _mm256_extracti128_si256(cmp_hi, 1));\n        __m256i blend_mask = _mm256_set_m128i(cmp_hi16, cmp_lo16);\n        \n        // Blend the vectors based on the mask\n        __m256i result = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        \n        // Store the result\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i vmask = _mm256_set1_epi32(mask);\n    \n    for (; i + 15 < length; i += 16) {\n        __m256i vidx = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8);\n        __m256i vidx2 = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i+0);\n        \n        __m256i vres1 = _mm256_and_si256(vidx, vmask);\n        __m256i vres2 = _mm256_and_si256(vidx2, vmask);\n        \n        __m256i vcmp1 = _mm256_cmpeq_epi32(vres1, _mm256_setzero_si256());\n        __m256i vcmp2 = _mm256_cmpeq_epi32(vres2, _mm256_setzero_si256());\n        \n        __m256i vsrc1_lo = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vsrc2_lo = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i vsrc1_hi = _mm256_loadu_si256((__m256i const*)(src1 + i + 8));\n        __m256i vsrc2_hi = _mm256_loadu_si256((__m256i const*)(src2 + i + 8));\n        \n        __m256i vblend_lo = _mm256_blendv_epi8(vsrc1_lo, vsrc2_lo, vcmp2);\n        __m256i vblend_hi = _mm256_blendv_epi8(vsrc1_hi, vsrc2_hi, vcmp1);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), vblend_lo);\n        _mm256_storeu_si256((__m256i*)(dst + i + 8), vblend_hi);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i indices = _mm256_setr_epi16(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7,\n                                           i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        __m256i mask_vec = _mm256_set1_epi32(mask);\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        __m256i cmp = _mm256_cmpgt_epi16(and_result, _mm256_setzero_si256());\n        \n        __m256i src1_vec = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i res = _mm256_blendv_epi8(src2_vec, src1_vec, cmp);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n    const __m256i mask = _mm256_set1_epi8(0x0F);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i lo = _mm256_and_si256(data, mask);\n        __m256i hi = _mm256_and_si256(_mm256_srli_epi16(data, 4), mask);\n        __m256i popcnt_lo = _mm256_shuffle_epi8(lookup, lo);\n        __m256i popcnt_hi = _mm256_shuffle_epi8(lookup, hi);\n        __m256i popcnt = _mm256_add_epi8(popcnt_lo, popcnt_hi);\n        popcnt = _mm256_sad_epu8(popcnt, _mm256_setzero_si256());\n        __m128i sum = _mm_add_epi32(_mm256_extracti128_si256(popcnt, 0), _mm256_extracti128_si256(popcnt, 1));\n        _mm_storeu_si128((__m128i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n    const __m256i mask = _mm256_set1_epi8(0x0F);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i lo = _mm256_and_si256(data, mask);\n        __m256i hi = _mm256_and_si256(_mm256_srli_epi16(data, 4), mask);\n        __m256i popcnt_lo = _mm256_shuffle_epi8(lookup, lo);\n        __m256i popcnt_hi = _mm256_shuffle_epi8(lookup, hi);\n        __m256i popcnt = _mm256_add_epi8(popcnt_lo, popcnt_hi);\n        popcnt = _mm256_sad_epu8(popcnt, _mm256_setzero_si256());\n        __m128i sum = _mm_add_epi32(_mm256_extracti128_si256(popcnt, 0), \n                                   _mm256_extracti128_si256(popcnt, 1));\n        _mm_storeu_si128((__m128i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mask4 = _mm256_set1_epi32(0x01010101);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        __m256i tmp = _mm256_sub_epi32(vec, _mm256_and_si256(_mm256_srli_epi32(vec, 1), mask1));\n        tmp = _mm256_add_epi32(_mm256_and_si256(tmp, mask2), _mm256_and_si256(_mm256_srli_epi32(tmp, 2), mask2));\n        tmp = _mm256_add_epi32(_mm256_and_si256(tmp, mask3), _mm256_and_si256(_mm256_srli_epi32(tmp, 4), mask3));\n        tmp = _mm256_mullo_epi32(tmp, mask4);\n        tmp = _mm256_srli_epi32(tmp, 24);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), tmp);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n        __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n        __m256i x = data;\n        x = _mm256_sub_epi32(x, _mm256_and_si256(_mm256_srli_epi32(x, 1), mask1));\n        x = _mm256_add_epi32(_mm256_and_si256(x, mask2), _mm256_and_si256(_mm256_srli_epi32(x, 2), mask2));\n        x = _mm256_add_epi32(_mm256_and_si256(x, mask3), _mm256_and_si256(_mm256_srli_epi32(x, 4), mask3));\n        x = _mm256_add_epi32(_mm256_and_si256(x, mask4), _mm256_and_si256(_mm256_srli_epi32(x, 8), mask4));\n        x = _mm256_add_epi32(_mm256_and_si256(x, mask5), _mm256_and_si256(_mm256_srli_epi32(x, 16), mask5));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), x);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        dst[i] = x & 0x0000003F;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX2 processes 8 uint32_t at a time\n\n    for (; i + vec_size <= length; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i popcnt = _mm256_setzero_si256();\n\n        // Mask and shift approach for population count\n        __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n        __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n        __m256i temp = data;\n        temp = _mm256_sub_epi32(temp, _mm256_and_si256(_mm256_srli_epi32(temp, 1), mask1));\n        temp = _mm256_add_epi32(_mm256_and_si256(temp, mask2), _mm256_and_si256(_mm256_srli_epi32(temp, 2), mask2));\n        temp = _mm256_add_epi32(_mm256_and_si256(temp, mask3), _mm256_and_si256(_mm256_srli_epi32(temp, 4), mask3));\n        temp = _mm256_add_epi32(_mm256_and_si256(temp, mask4), _mm256_and_si256(_mm256_srli_epi32(temp, 8), mask4));\n        temp = _mm256_add_epi32(_mm256_and_si256(temp, mask5), _mm256_and_si256(_mm256_srli_epi32(temp, 16), mask5));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), temp);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x + (x >> 8));\n        x = (x + (x >> 16)) & 0x0000003F;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t aligned_length = length - (length % 16);\n\n    for (; i < aligned_length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t aligned_length = length - (length % 16);\n\n    for (; i < aligned_length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t aligned_length = length & ~15;\n\n    for (; i < aligned_length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t aligned_length = length & ~15;\n\n    for (; i < aligned_length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 16;\n    const size_t simd_iterations = length / simd_elements;\n\n    for (; i < simd_iterations * simd_elements; i += simd_elements) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(A + i * n + k);\n                __m256d b = _mm256_loadu_pd(B + k * p + j);\n                \n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n                \n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            \n            double temp_sum = _mm256_reduce_add_pd(sum);\n            for (; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    temp_sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = temp_sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(A + i * n + k);\n                __m256d b = _mm256_loadu_pd(B + j * n + k);\n\n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n\n                __m256d a_masked = _mm256_and_pd(a, mask);\n                __m256d b_masked = _mm256_and_pd(b, mask);\n                __m256d prod = _mm256_mul_pd(a_masked, b_masked);\n\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(A + i * n + k);\n                __m256d b = _mm256_loadu_pd(B + j * n + k);\n                \n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n                \n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(A + i * n + k);\n                __m256d b = _mm256_loadu_pd(B + k * p + j);\n                \n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n                \n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            \n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            \n            for (; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    res += a_val * b_val;\n                }\n            }\n            \n            C[i * p + j] = res;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(A + i * n + k);\n                __m256d b = _mm256_loadu_pd(B + k * p + j);\n                \n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b, upper_bound, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n                \n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            \n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            \n            for (; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    res += a_val * b_val;\n                }\n            }\n            \n            C[i * p + j] = res;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), \n                            _mm_add_ss(_mm_load_ss(src1 + i), _mm_load_ss(src2 + i)), \n                            _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), \n                            _mm_add_ss(_mm_load_ss(src1 + i), _mm_load_ss(src2 + i)), \n                            _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), \n                                _mm_add_ss(_mm_load_ss(src1 + i), \n                                _mm_load_ss(src2 + i)), \n                                _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), \n                           _mm_add_ss(_mm_load_ss(src1 + i), _mm_load_ss(src2 + i)), \n                           _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_NINT);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), \n                            _mm_add_ss(_mm_load_ss(src1 + i), _mm_load_ss(src2 + i)), \n                            _MM_FROUND_NINT));\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256i index_mask = _mm256_set1_epi32(1);\n    __m256 abs_diff_threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one = _mm256_set1_ps(-1.0f);\n    __m256 zero = _mm256_setzero_ps();\n\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        \n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        \n        __m256 cmp_diff = _mm256_cmp_ps(abs_diff, abs_diff_threshold, _CMP_LE_OQ);\n        \n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, index_mask), zero);\n        __m256 cmp_index = _mm256_castsi256_ps(even_mask);\n        \n        __m256 mask = _mm256_and_ps(cmp_diff, cmp_index);\n        \n        __m256 mul = _mm256_mul_ps(v_src1, v_src2);\n        __m256 rounded = _mm256_round_ps(mul, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n        \n        __m256 res = _mm256_blendv_ps(minus_one, rounded, mask);\n        \n        _mm256_storeu_ps(dst + i, res);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i even_mask = _mm256_set1_epi32(0xFFFFFFFE);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        // Calculate absolute difference\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // abs(diff)\n        \n        // Check condition 1: abs(diff) <= 50\n        __m256 cond1 = _mm256_cmp_ps(diff, threshold, _CMP_LE_OS);\n        \n        // Check condition 2: index is multiple of 2\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i cond2 = _mm256_cmpeq_epi32(_mm256_and_si256(indices, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        \n        // Combine conditions\n        __m256 mask = _mm256_and_ps(cond1, _mm256_castsi256_ps(cond2));\n        \n        // Calculate multiplication for elements that meet conditions\n        __m256 mul = _mm256_mul_ps(v1, v2);\n        mul = _mm256_ceil_ps(mul);\n        \n        // Select results based on conditions\n        __m256 res = _mm256_blendv_ps(neg_one, mul, mask);\n        \n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256i even_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        // Calculate absolute difference\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // abs(diff)\n        \n        // Check condition 1: abs(diff) <= 50\n        __m256 cond1 = _mm256_cmp_ps(diff, threshold, _CMP_LE_OS);\n        \n        // Check condition 2: index is multiple of 2\n        __m256i indices = _mm256_add_epi32(_mm256_set1_epi32(i), _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256i mod2 = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256 cond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mod2, _mm256_setzero_si256()));\n        \n        // Combine conditions\n        __m256 mask = _mm256_and_ps(cond1, cond2);\n        \n        // Calculate multiplication for elements that meet conditions\n        __m256 mul = _mm256_mul_ps(v1, v2);\n        mul = _mm256_round_ps(mul, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n        \n        // Select results based on conditions\n        __m256 res = _mm256_blendv_ps(neg_one, mul, mask);\n        \n        _mm256_storeu_ps(dst + i, res);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256i index_mask = _mm256_set1_epi32(1);\n    __m256 abs_diff_threshold = _mm256_set1_ps(50.0f);\n    __m256 default_value = _mm256_set1_ps(-1.0f);\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        // Calculate absolute difference\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        \n        // Check condition 1: abs_diff <= 50\n        __m256 cond1_mask = _mm256_cmp_ps(abs_diff, abs_diff_threshold, _CMP_LE_OS);\n        \n        // Check condition 2: index is multiple of 2\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod2 = _mm256_and_si256(indices, index_mask);\n        __m256 cond2_mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mod2, _mm256_setzero_si256()));\n        \n        // Combine conditions\n        __m256 mask = _mm256_and_ps(cond1_mask, cond2_mask);\n        \n        // Calculate multiplication and round up\n        __m256 mul = _mm256_mul_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(mul, _MM_FROUND_CEIL);\n        \n        // Select result based on conditions\n        __m256 res = _mm256_blendv_ps(default_value, rounded, mask);\n        \n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256i index_mask = _mm256_set1_epi32(1);\n    __m256 abs_diff_threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one = _mm256_set1_ps(-1.0f);\n    __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, index_mask), zero);\n\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 abs_diff_mask = _mm256_cmp_ps(abs_diff, abs_diff_threshold, _CMP_LE_OQ);\n\n        __m256 mul = _mm256_mul_ps(v1, v2);\n        mul = _mm256_ceil_ps(mul);\n\n        __m256 mask = _mm256_and_ps(_mm256_castsi256_ps(even_mask), abs_diff_mask);\n        __m256 res = _mm256_blendv_ps(minus_one, mul, mask);\n\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res_vec);\n    }\n\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res_vec);\n    }\n\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res_vec);\n    }\n\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n        \n        // Horizontal sum of the vector\n        __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n        \n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        \n        // Normalize 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n        \n        // Horizontal sum of the vector\n        __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f;\n        \n        __m256 norm_vec = _mm256_set1_ps(1.0f / norm);\n        j = 0;\n        \n        // Normalize 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            row_vec = _mm256_mul_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(row + j, row_vec);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n        \n        // Horizontal sum of the vector\n        __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 1), _mm256_castps256_ps128(sum_vec));\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n        \n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        \n        // Normalize 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Process 8 elements at a time\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n        \n        // Horizontal sum of the vector\n        __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), \n                               _mm256_extractf128_ps(sum_vec, 1));\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n        \n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        \n        // Normalize 8 elements at a time\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n        \n        // Horizontal sum of the vector\n        __m128 sum128 = _mm_add_ps(_mm256_extractf128_ps(sum_vec, 0), _mm256_extractf128_ps(sum_vec, 1));\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n        \n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        \n        // Normalize 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d prod = _mm256_mul_pd(a, b);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            for (; k < n; ++k) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = (double)((long long)(res + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d prod = _mm256_mul_pd(a, b);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            for (; k < n; ++k) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = (double)((long long)(res + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d prod = _mm256_mul_pd(a, b);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            for (; k < n; ++k) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = (double)((long long)(res + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + j + k * p);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            for (; k < n; ++k) {\n                res += mat1[i * n + k] * mat2[j + k * p];\n            }\n            dst[i * p + j] = (double)((long long)(res + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + j + k * p);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1] + temp[2] + temp[3];\n            for (; k < n; ++k) {\n                res += mat1[i * n + k] * mat2[j + k * p];\n            }\n            dst[i * p + j] = (double)((long long)(res + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n    const __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_data = _mm256_loadu_ps(src + i * cols + j);\n            __m256 scaled = _mm256_mul_ps(row_data, _mm256_set1_ps(4.0f));\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 result = _mm256_mul_ps(rounded, quarter);\n            \n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < rows) {\n                    dst[j * rows + i + k] = ((float*)&result)[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n    const __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j++) {\n            __m256 row_data = _mm256_loadu_ps(src + i * cols + j);\n            __m256 scaled = _mm256_mul_ps(row_data, _mm256_set1_ps(4.0f));\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 result = _mm256_mul_ps(rounded, quarter);\n            \n            for (size_t k = 0; k < 8; k++) {\n                if (i + k < rows) {\n                    dst[j * rows + i + k] = ((float*)&result)[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n    const __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_data = _mm256_loadu_ps(src + i * cols + j);\n            __m256 scaled = _mm256_mul_ps(row_data, _mm256_set1_ps(4.0f));\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 result = _mm256_mul_ps(rounded, quarter);\n            \n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < rows) {\n                    dst[j * rows + i + k] = ((float*)&result)[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 mul = _mm256_set1_ps(4.0f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n    \n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row = _mm256_loadu_ps(src + i * cols + j);\n            row = _mm256_mul_ps(row, mul);\n            row = _mm256_add_ps(row, half);\n            row = _mm256_floor_ps(row);\n            row = _mm256_div_ps(row, mul);\n            \n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < rows) {\n                    dst[j * rows + i + k] = _mm256_extract_ps(row, k);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n    const __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_data = _mm256_loadu_ps(src + i * cols + j);\n            \n            // Round to nearest quarter\n            __m256 scaled = _mm256_mul_ps(row_data, _mm256_set1_ps(4.0f));\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 result = _mm256_mul_ps(rounded, quarter);\n            \n            // Transpose by storing columns into rows\n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < rows) {\n                    dst[j * rows + i + k] = _mm256_extract_ps(result, k);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(vec1, vec2);\n        res = _mm256_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = val >= 0 ? floorf(val + 0.5f) : ceilf(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(vec1, vec2);\n        res = _mm256_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val) : ceilf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(vec1, vec2);\n        res = _mm256_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val + 0.5f) : ceilf(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(vec1, vec2);\n        res = _mm256_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val + 0.5f) : ceilf(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(vec1, vec2);\n        res = _mm256_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val) : ceilf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_val = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_row, vec_val));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / (double)total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        size_t j;\n        for (j = 0; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (direction) {\n                cmp = _mm256_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n        \n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (direction) {\n                cmp = _mm256_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n        \n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (direction) {\n                cmp = _mm256_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n        \n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (direction) {\n                cmp = _mm256_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n        \n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (direction) {\n                cmp = _mm256_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n        \n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i div = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul = _mm256_mullo_epi16(div, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), mul);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i div = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul = _mm256_mullo_epi16(div, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), mul);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i div = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul = _mm256_mullo_epi16(div, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), mul);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i div = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul = _mm256_mullo_epi16(div, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), mul);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i div = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul = _mm256_mullo_epi16(div, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), mul);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(vector + j);\n            __m256d mat = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            if (vector[j] != row[j]) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(vector + j);\n            __m256d mat = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            if (vector[j] != row[j]) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec_row = _mm256_loadu_pd(row + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(vec_row, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec_row = _mm256_loadu_pd(row + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(vec_row, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(vector + j);\n            __m256d mat = _mm256_loadu_pd(row + j);\n            __m256d cmp = _mm256_cmp_pd(vec, mat, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            if (vector[j] != row[j]) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_ones = _mm256_set1_epi8(1);\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        __m256i vec_cmp = _mm256_cmpeq_epi8(vec_a, vec_b);\n        __m256i vec_sum = _mm256_sad_epu8(vec_cmp, vec_zero);\n        \n        equal_count += _mm256_extract_epi16(vec_sum, 0) + _mm256_extract_epi16(vec_sum, 4);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    const __m256i zero = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a, b);\n        __m256i mask = _mm256_cmpeq_epi8(cmp, zero);\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i sum = _mm256_sub_epi8(ones, _mm256_sign_epi8(ones, mask));\n        count_vec = _mm256_add_epi8(count_vec, sum);\n    }\n\n    uint8_t count_arr[32];\n    _mm256_storeu_si256((__m256i*)count_arr, count_vec);\n    for (int j = 0; j < 32; j++) {\n        equal_count += count_arr[j];\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    const __m256i zero = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a_vec, b_vec);\n        __m256i mask = _mm256_cmpeq_epi8(cmp, zero);\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i sum = _mm256_sub_epi8(ones, _mm256_sign_epi8(ones, mask));\n        count_vec = _mm256_add_epi8(count_vec, sum);\n    }\n\n    alignas(32) uint8_t count_arr[32];\n    _mm256_store_si256((__m256i*)count_arr, count_vec);\n\n    for (int j = 0; j < 32; ++j) {\n        equal_count += count_arr[j];\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t count = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a, b);\n        __m256i mask = _mm256_and_si256(cmp, ones);\n        __m256i sum = _mm256_sad_epu8(mask, zero);\n        count += _mm256_extract_epi16(sum, 0) + _mm256_extract_epi16(sum, 4);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m256i vcount = _mm256_setzero_si256();\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vcmp = _mm256_cmpeq_epi8(va, vb);\n        int mask = _mm256_movemask_epi8(vcmp);\n        equal_count += __builtin_popcount(mask);\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n        \n        __m256i ax_vec = _mm256_mullo_epi64(a_vec, x_vec);\n        __m256i abs_y_vec = _mm256_abs_epi64(y_vec);\n        __m256i result = _mm256_sub_epi64(ax_vec, abs_y_vec);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    \n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n        \n        __m256i ax = _mm256_mullo_epi64(a_vec, x_vec);\n        __m256i abs_y = _mm256_abs_epi64(y_vec);\n        __m256i result = _mm256_sub_epi64(ax, abs_y);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    __m256i zero = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n        \n        __m256i ax = _mm256_mullo_epi64(a_vec, x_vec);\n        __m256i abs_y = _mm256_abs_epi64(y_vec);\n        __m256i res = _mm256_sub_epi64(ax, abs_y);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    __m256i zero = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n        \n        __m256i ax = _mm256_mullo_epi64(a_vec, x_vec);\n        __m256i abs_y = _mm256_abs_epi64(y_vec);\n        __m256i res = _mm256_sub_epi64(ax, abs_y);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    __m256i zero = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n        \n        __m256i ax = _mm256_mullo_epi64(a_vec, x_vec);\n        __m256i abs_y = _mm256_abs_epi64(y_vec);\n        __m256i result = _mm256_sub_epi64(ax, abs_y);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i max_vec = _mm256_set1_epi16(vec[0]);\n    __m256i min_vec = _mm256_set1_epi16(vec[0]);\n    \n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(vec + i));\n        max_vec = _mm256_max_epi16(max_vec, data);\n        min_vec = _mm256_min_epi16(min_vec, data);\n    }\n    \n    int16_t max_arr[16], min_arr[16];\n    _mm256_storeu_si256((__m256i*)max_arr, max_vec);\n    _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n    \n    int16_t max_val = max_arr[0];\n    int16_t min_val = min_arr[0];\n    \n    for (size_t j = 1; j < 16; ++j) {\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n    }\n    \n    for (; i < length; ++i) {\n        if (vec[i] > max_val) max_val = vec[i];\n        if (vec[i] < min_val) min_val = vec[i];\n    }\n    \n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(32767);\n    __m256i max_vec = _mm256_set1_epi16(-32768);\n    size_t i;\n\n    for (i = 0; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_arr[16], max_arr[16];\n    _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n    _mm256_storeu_si256((__m256i*)max_arr, max_vec);\n\n    int16_t min_val = 32767;\n    int16_t max_val = -32768;\n    for (size_t j = 0; j < 16; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; i++) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    \n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(vec + i));\n        max_vec = _mm256_max_epi16(max_vec, data);\n        min_vec = _mm256_min_epi16(min_vec, data);\n    }\n    \n    int16_t max_arr[16] __attribute__((aligned(32)));\n    int16_t min_arr[16] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)max_arr, max_vec);\n    _mm256_store_si256((__m256i*)min_arr, min_vec);\n    \n    int16_t max_val = INT16_MIN;\n    int16_t min_val = INT16_MAX;\n    for (size_t j = 0; j < 16; ++j) {\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n    }\n    \n    for (; i < length; ++i) {\n        if (vec[i] > max_val) max_val = vec[i];\n        if (vec[i] < min_val) min_val = vec[i];\n    }\n    \n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(32767);\n    __m256i max_vec = _mm256_set1_epi16(-32768);\n    \n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n    \n    int16_t min_arr[16] __attribute__((aligned(32)));\n    int16_t max_arr[16] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)min_arr, min_vec);\n    _mm256_store_si256((__m256i*)max_arr, max_vec);\n    \n    int16_t min_val = 32767;\n    int16_t max_val = -32768;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n    \n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n    \n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_arr[16] __attribute__((aligned(32)));\n    int16_t max_arr[16] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)min_arr, min_vec);\n    _mm256_store_si256((__m256i*)max_arr, max_vec);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 16; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    size_t simd_cols = cols - 1;\n    size_t vec_size = 8;\n    size_t remaining = simd_cols % vec_size;\n    size_t vec_iters = simd_cols / vec_size;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        const int* next_ptr = row_start + 1;\n\n        for (size_t j = 0; j < vec_iters; ++j) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row_start + j * vec_size));\n            __m256i next = _mm256_loadu_si256((__m256i*)(next_ptr + j * vec_size));\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                return false;\n            }\n        }\n\n        for (size_t j = vec_iters * vec_size; j < simd_cols; ++j) {\n            if (row_start[j] >= row_start[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 8 <= cols - 1; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 8 <= cols - 1; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 8 <= cols - 1; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n    if (cols == 1) return true;\n\n    size_t vec_elements = 8;\n    size_t remaining = cols - 1;\n    size_t vec_steps = remaining / vec_elements;\n    size_t vec_remain = remaining % vec_elements;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        __m256i prev = _mm256_loadu_si256((__m256i*)row);\n        bool row_valid = true;\n\n        for (size_t j = 1; j <= vec_steps; ++j) {\n            __m256i curr = _mm256_loadu_si256((__m256i*)(row + j * vec_elements));\n            __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                row_valid = false;\n                break;\n            }\n            prev = curr;\n        }\n\n        if (row_valid && vec_remain > 0) {\n            __m256i curr = _mm256_loadu_si256((__m256i*)(row + vec_steps * vec_elements + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                row_valid = false;\n            }\n        }\n\n        if (!row_valid) return false;\n\n        // Check the remaining elements\n        for (size_t j = vec_steps * vec_elements + 1 + vec_remain; j < cols; ++j) {\n            if (row[j - 1] >= row[j]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n    __m256i first_block_vec;\n\n    for (size_t i = 0; i < block_elements; i += 8) {\n        if (i + 8 <= block_elements) {\n            first_block_vec = _mm256_loadu_si256((__m256i*)(first_block + i));\n        } else {\n            __m256i mask = _mm256_set_epi32(\n                i + 7 < block_elements ? first_block[i + 7] : 0,\n                i + 6 < block_elements ? first_block[i + 6] : 0,\n                i + 5 < block_elements ? first_block[i + 5] : 0,\n                i + 4 < block_elements ? first_block[i + 4] : 0,\n                i + 3 < block_elements ? first_block[i + 3] : 0,\n                i + 2 < block_elements ? first_block[i + 2] : 0,\n                i + 1 < block_elements ? first_block[i + 1] : 0,\n                i < block_elements ? first_block[i] : 0\n            );\n            first_block_vec = mask;\n        }\n\n        for (size_t br = 0; br < num_blocks_rows; ++br) {\n            for (size_t bc = 0; bc < num_blocks_cols; ++bc) {\n                if (br == 0 && bc == 0) continue;\n\n                const int* current_block = matrix + (br * block_size * cols) + (bc * block_size * block_size);\n                __m256i current_block_vec;\n\n                if (i + 8 <= block_elements) {\n                    current_block_vec = _mm256_loadu_si256((__m256i*)(current_block + i));\n                } else {\n                    __m256i mask = _mm256_set_epi32(\n                        i + 7 < block_elements ? current_block[i + 7] : 0,\n                        i + 6 < block_elements ? current_block[i + 6] : 0,\n                        i + 5 < block_elements ? current_block[i + 5] : 0,\n                        i + 4 < block_elements ? current_block[i + 4] : 0,\n                        i + 3 < block_elements ? current_block[i + 3] : 0,\n                        i + 2 < block_elements ? current_block[i + 2] : 0,\n                        i + 1 < block_elements ? current_block[i + 1] : 0,\n                        i < block_elements ? current_block[i] : 0\n                    );\n                    current_block_vec = mask;\n                }\n\n                __m256i cmp_result = _mm256_cmpeq_epi32(first_block_vec, current_block_vec);\n                int mask = _mm256_movemask_epi8(cmp_result);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    const int* first_block = matrix;\n    for (size_t br = 0; br < num_blocks_rows; ++br) {\n        for (size_t bc = 0; bc < num_blocks_cols; ++bc) {\n            if (br == 0 && bc == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + (br * block_size * cols + bc * block_size);\n            size_t i = 0;\n            for (; i + 8 <= block_elements; i += 8) {\n                __m256i first = _mm256_loadu_si256((__m256i*)(first_block + i));\n                __m256i current = _mm256_loadu_si256((__m256i*)(current_block + i));\n                __m256i cmp = _mm256_cmpeq_epi32(first, current);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n\n            for (; i < block_elements; ++i) {\n                if (first_block[i] != current_block[i]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n    const int* current_block = matrix;\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            if (i == 0 && j == 0) {\n                current_block += block_elements;\n                continue;\n            }\n\n            size_t k = 0;\n            for (; k + 8 <= block_elements; k += 8) {\n                __m256i first = _mm256_loadu_si256((__m256i*)(first_block + k));\n                __m256i current = _mm256_loadu_si256((__m256i*)(current_block + k));\n                __m256i cmp = _mm256_cmpeq_epi32(first, current);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n\n            for (; k < block_elements; ++k) {\n                if (first_block[k] != current_block[k]) {\n                    return false;\n                }\n            }\n\n            current_block += block_elements;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n    const int* current_block = matrix;\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            if (i == 0 && j == 0) {\n                continue;\n            }\n\n            current_block = matrix + (i * block_size * cols) + (j * block_size);\n\n            size_t k = 0;\n            for (; k + 8 <= block_elements; k += 8) {\n                __m256i first_vec = _mm256_loadu_si256((__m256i*)(first_block + k));\n                __m256i current_vec = _mm256_loadu_si256((__m256i*)(current_block + k));\n                __m256i cmp = _mm256_cmpeq_epi32(first_vec, current_vec);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n\n            for (; k < block_elements; ++k) {\n                if (first_block[k] != current_block[k]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n    const int* current_block = matrix;\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            if (i == 0 && j == 0) {\n                current_block += block_elements;\n                continue;\n            }\n\n            for (size_t k = 0; k < block_elements; k += 8) {\n                size_t remaining = block_elements - k;\n                if (remaining >= 8) {\n                    __m256i first = _mm256_loadu_si256((__m256i*)(first_block + k));\n                    __m256i current = _mm256_loadu_si256((__m256i*)(current_block + k));\n                    __m256i cmp = _mm256_cmpeq_epi32(first, current);\n                    if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                        return false;\n                    }\n                } else {\n                    for (size_t l = 0; l < remaining; ++l) {\n                        if (first_block[k + l] != current_block[k + l]) {\n                            return false;\n                        }\n                    }\n                }\n            }\n            current_block += block_elements;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return true;\n    }\n\n    __m256d tol = _mm256_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size; i += 4) {\n            size_t remaining = block_size - i;\n            if (remaining >= 4) {\n                __m256d a = _mm256_loadu_pd(first_block + i);\n                __m256d b = _mm256_loadu_pd(current_block + i);\n                __m256d diff = _mm256_sub_pd(a, b);\n                __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n                __m256d cmp = _mm256_cmp_pd(abs_diff, tol, _CMP_GT_OQ);\n                if (_mm256_movemask_pd(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                for (size_t j = 0; j < remaining; ++j) {\n                    double diff = first_block[i + j] - current_block[i + j];\n                    if (diff < -tolerance || diff > tolerance) {\n                        return false;\n                    }\n                }\n            }\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return true;\n    }\n\n    __m256d tol = _mm256_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            size_t remaining = block_size - j;\n            if (remaining >= 4) {\n                __m256d a = _mm256_loadu_pd(first_block + j);\n                __m256d b = _mm256_loadu_pd(current_block + j);\n                __m256d diff = _mm256_sub_pd(a, b);\n                __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n                __m256d cmp = _mm256_cmp_pd(abs_diff, tol, _CMP_GT_OQ);\n                if (_mm256_movemask_pd(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                for (size_t k = 0; k < remaining; ++k) {\n                    double diff = first_block[j + k] - current_block[j + k];\n                    if (fabs(diff) > tolerance) {\n                        return false;\n                    }\n                }\n            }\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    const double* first_block = vec;\n    __m256d tol = _mm256_set1_pd(tolerance);\n    __m256d neg_tol = _mm256_set1_pd(-tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n        size_t j = 0;\n\n        for (; j + 4 <= block_size; j += 4) {\n            __m256d first = _mm256_loadu_pd(first_block + j);\n            __m256d current = _mm256_loadu_pd(current_block + j);\n            __m256d diff = _mm256_sub_pd(first, current);\n            __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j < block_size; ++j) {\n            double diff = first_block[j] - current_block[j];\n            if (diff < -tolerance || diff > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m256d tol = _mm256_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size; i += 4) {\n            if (i + 4 > block_size) {\n                for (size_t j = i; j < block_size; ++j) {\n                    double diff = fabs(first_block[j] - current_block[j]);\n                    if (diff > tolerance) {\n                        return false;\n                    }\n                }\n                break;\n            }\n\n            __m256d a = _mm256_loadu_pd(first_block + i);\n            __m256d b = _mm256_loadu_pd(current_block + i);\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m256d tol = _mm256_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        size_t i = 0;\n        for (; i + 4 <= block_size; i += 4) {\n            __m256d a = _mm256_loadu_pd(first_block + i);\n            __m256d b = _mm256_loadu_pd(current_block + i);\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; i < block_size; ++i) {\n            double diff = first_block[i] - current_block[i];\n            if (diff < 0) diff = -diff;\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d squared = _mm256_mul_pd(row_vec, row_vec);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n        double sum = 0.0;\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        sum = ((double*)&temp)[0] + ((double*)&temp)[2];\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n        double norm = sqrt(sum);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d squared = _mm256_mul_pd(row_vec, row_vec);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n        double sum = 0.0;\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        sum = ((double*)&temp)[0] + ((double*)&temp)[2];\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n        result[i] = (sqrt(sum) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d squared = _mm256_mul_pd(row_vec, row_vec);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n        double sum = 0.0;\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        sum += ((double*)&temp)[0] + ((double*)&temp)[2];\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n        result[i] = (sum > thresholds[i] * thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec);\n        }\n        double sum = 0.0;\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        sum = _mm256_cvtsd_f64(_mm256_add_pd(temp, _mm256_permute4x64_pd(temp, 0b01001110)));\n        \n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n        result[i] = (sqrt(sum) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d squared = _mm256_mul_pd(row_vec, row_vec);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n        double sum = 0.0;\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        sum += ((double*)&temp)[0] + ((double*)&temp)[2];\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n        double norm = sqrt(sum);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t end = length - (length % 8);\n\n    for (; i < end; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_end = length - (length % 8);\n\n    for (; i < simd_end; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c_vec = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)&C[i], c_vec);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_end = length - (length % 8);\n\n    for (; i < simd_end; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t end = length - (length % 8);\n\n    for (; i < end; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t end = length - (length % 8);\n\n    for (; i < end; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_maddubs_epi16(a, a);\n        __m256i b_sq = _mm256_maddubs_epi16(b, b);\n        __m256i prod = _mm256_maddubs_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(a_sq, _mm256_set1_epi16(1)));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(b_sq, _mm256_set1_epi16(1)));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_madd_epi16(prod, _mm256_set1_epi16(1)));\n    }\n\n    int32_t sq_arr[8], prod_arr[8];\n    _mm256_storeu_si256((__m256i*)sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)prod_arr, sum_prod);\n\n    int64_t sum_squares = 0;\n    int64_t sum_products = 0;\n    for (int j = 0; j < 8; j++) {\n        sum_squares += sq_arr[j];\n        sum_products += prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        sum_squares += A[i] * A[i] + B[i] * B[i];\n        sum_products += A[i] * B[i];\n    }\n\n    return sum_squares - 2 * sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_maddubs_epi16(a, a);\n        __m256i b_sq = _mm256_maddubs_epi16(b, b);\n        __m256i prod = _mm256_maddubs_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(a_sq, _mm256_set1_epi16(1)));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(b_sq, _mm256_set1_epi16(1)));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_madd_epi16(prod, _mm256_set1_epi16(1)));\n    }\n\n    int32_t sq_arr[8], prod_arr[8];\n    _mm256_storeu_si256((__m256i*)sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)prod_arr, sum_prod);\n\n    int64_t sum_squares = 0, sum_products = 0;\n    for (int j = 0; j < 8; j++) {\n        sum_squares += sq_arr[j];\n        sum_products += prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        sum_squares += A[i] * A[i] + B[i] * B[i];\n        sum_products += A[i] * B[i];\n    }\n\n    return sum_squares - 2 * sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_maddubs_epi16(a, a);\n        __m256i b_sq = _mm256_maddubs_epi16(b, b);\n        __m256i prod = _mm256_maddubs_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(a_sq, _mm256_set1_epi16(1)));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(b_sq, _mm256_set1_epi16(1)));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_madd_epi16(prod, _mm256_set1_epi16(1)));\n    }\n\n    int32_t sum_sq_arr[8] = {0};\n    int32_t sum_prod_arr[8] = {0};\n    _mm256_storeu_si256((__m256i*)sum_sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = 0;\n    int64_t total_sum_prod = 0;\n    for (int j = 0; j < 8; j++) {\n        total_sum_sq += sum_sq_arr[j];\n        total_sum_prod += sum_prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        total_sum_sq += a * a + b * b;\n        total_sum_prod += a * b;\n    }\n\n    return total_sum_sq - 2 * total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_maddubs_epi16(a, a);\n        __m256i b_sq = _mm256_maddubs_epi16(b, b);\n        __m256i prod = _mm256_maddubs_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(a_sq, _mm256_set1_epi16(1)));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(b_sq, _mm256_set1_epi16(1)));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_madd_epi16(prod, _mm256_set1_epi16(1)));\n    }\n\n    int32_t sq_arr[8], prod_arr[8];\n    _mm256_storeu_si256((__m256i*)sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)prod_arr, sum_prod);\n\n    int64_t sum_squares = 0, sum_products = 0;\n    for (int j = 0; j < 8; j++) {\n        sum_squares += sq_arr[j];\n        sum_products += prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        sum_squares += a * a + b * b;\n        sum_products += a * b;\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_lo = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a, 0));\n        __m256i a_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a, 1));\n        __m256i b_lo = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b, 0));\n        __m256i b_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b, 1));\n\n        __m256i sq_lo = _mm256_mullo_epi16(a_lo, a_lo);\n        __m256i sq_hi = _mm256_mullo_epi16(a_hi, a_hi);\n        __m256i prod_lo = _mm256_mullo_epi16(a_lo, b_lo);\n        __m256i prod_hi = _mm256_mullo_epi16(a_hi, b_hi);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sq_lo, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sq_lo, 1))));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sq_hi, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sq_hi, 1))));\n\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_lo, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_lo, 1))));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_hi, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_hi, 1))));\n    }\n\n    int32_t sum_sq_arr[8] = {0};\n    int32_t sum_prod_arr[8] = {0};\n    _mm256_storeu_si256((__m256i*)sum_sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = 0;\n    int64_t total_sum_prod = 0;\n    for (int j = 0; j < 8; j++) {\n        total_sum_sq += sum_sq_arr[j];\n        total_sum_prod += sum_prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        total_sum_sq += A[i] * A[i];\n        total_sum_prod += A[i] * B[i];\n    }\n\n    return total_sum_sq - total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 32; // AVX2 can process 32 int8_t elements at once\n    \n    for (; i + vec_size <= length; i += vec_size) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a_vec, _mm256_setzero_si256());\n        __m256i b_pos = _mm256_cmpgt_epi8(b_vec, _mm256_setzero_si256());\n        __m256i a_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a_vec);\n        __m256i b_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b_vec);\n        \n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i res = _mm256_or_si256(\n            _mm256_and_si256(both_pos, _mm256_set1_epi8(1)),\n            _mm256_and_si256(both_neg, _mm256_set1_epi8(-1))\n        );\n        \n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 32; // AVX2 can process 32 int8_t elements at once\n    \n    for (; i + vec_size <= length; i += vec_size) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a_vec, _mm256_setzero_si256());\n        __m256i b_pos = _mm256_cmpgt_epi8(b_vec, _mm256_setzero_si256());\n        __m256i a_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a_vec);\n        __m256i b_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b_vec);\n        \n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i res = _mm256_or_si256(\n            _mm256_and_si256(both_pos, _mm256_set1_epi8(1)),\n            _mm256_and_si256(both_neg, _mm256_set1_epi8(-1))\n        );\n        \n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i b_pos = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n\n        __m256i a_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n        __m256i b_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(both_pos, _mm256_set1_epi8(1)),\n            _mm256_and_si256(both_neg, _mm256_set1_epi8(-1))\n        );\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vec_size = 32; // AVX2 processes 32 int8_t elements at a time\n\n    for (; i + vec_size <= length; i += vec_size) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a_vec, _mm256_setzero_si256());\n        __m256i b_pos = _mm256_cmpgt_epi8(b_vec, _mm256_setzero_si256());\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n\n        __m256i a_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a_vec);\n        __m256i b_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b_vec);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(both_pos, _mm256_set1_epi8(1)),\n            _mm256_and_si256(both_neg, _mm256_set1_epi8(-1))\n        );\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i b_pos = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n\n        __m256i a_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n        __m256i b_neg = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(both_pos, _mm256_set1_epi8(1)),\n            _mm256_and_si256(both_neg, _mm256_set1_epi8(-1))\n        );\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements after midpoint (AND)\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Process remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    \n    // Process remaining elements after midpoint (AND)\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Handle remaining elements before midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Store midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n\n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Handle remaining elements after midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i mask1 = _mm256_loadu_si256((__m256i const*)(cond1 + i));\n        __m256i mask2 = _mm256_loadu_si256((__m256i const*)(cond2 + i));\n        \n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 c = _mm256_loadu_ps(C + i);\n        __m256 d = _mm256_loadu_ps(D + i);\n        \n        __m256i cond1_true = _mm256_cmpeq_epi32(mask1, _mm256_set1_epi32(1));\n        __m256i cond1_false = _mm256_cmpeq_epi32(mask1, _mm256_set1_epi32(0));\n        __m256i cond2_true = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(1));\n        __m256i cond2_false = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(0));\n        \n        __m256i case1_mask = _mm256_and_si256(cond1_true, cond2_true);\n        __m256i case2_mask = _mm256_and_si256(cond1_true, cond2_false);\n        __m256i case3_mask = _mm256_and_si256(cond1_false, cond2_true);\n        __m256i case4_mask = _mm256_and_si256(cond1_false, cond2_false);\n        \n        __m256 res = _mm256_setzero_ps();\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case1_mask), a));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case2_mask), b));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case3_mask), c));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case4_mask), d));\n        \n        _mm256_storeu_ps(out + i, res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i mask1 = _mm256_loadu_si256((__m256i const*)(cond1 + i));\n        __m256i mask2 = _mm256_loadu_si256((__m256i const*)(cond2 + i));\n        \n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 c = _mm256_loadu_ps(C + i);\n        __m256 d = _mm256_loadu_ps(D + i);\n        \n        __m256i cond1_true = _mm256_cmpeq_epi32(mask1, _mm256_set1_epi32(1));\n        __m256i cond1_false = _mm256_cmpeq_epi32(mask1, _mm256_set1_epi32(0));\n        __m256i cond2_true = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(1));\n        __m256i cond2_false = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(0));\n        \n        __m256i case1_mask = _mm256_and_si256(cond1_true, cond2_true);\n        __m256i case2_mask = _mm256_and_si256(cond1_true, cond2_false);\n        __m256i case3_mask = _mm256_and_si256(cond1_false, cond2_true);\n        __m256i case4_mask = _mm256_and_si256(cond1_false, cond2_false);\n        \n        __m256 res = _mm256_setzero_ps();\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case1_mask), a));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case2_mask), b));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case3_mask), c));\n        res = _mm256_or_ps(res, _mm256_and_ps(_mm256_castsi256_ps(case4_mask), d));\n        \n        _mm256_storeu_ps(out + i, res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n        \n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n        \n        __m256i mask1 = _mm256_cmpeq_epi32(cond1_vec, _mm256_set1_epi32(1));\n        __m256i mask2 = _mm256_cmpeq_epi32(cond2_vec, _mm256_set1_epi32(1));\n        \n        __m256 res1 = _mm256_blendv_ps(b, a, _mm256_castsi256_ps(_mm256_and_si256(mask1, mask2)));\n        __m256 res2 = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(_mm256_andnot_si256(mask1, mask2)));\n        __m256 res = _mm256_blendv_ps(res2, res1, _mm256_castsi256_ps(mask1));\n        \n        _mm256_storeu_ps(&out[i], res);\n    }\n    \n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n        \n        __m256i mask1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i mask2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n        \n        __m256 res1 = _mm256_blendv_ps(b, a, _mm256_castsi256_ps(_mm256_and_si256(mask1, mask2)));\n        __m256 res2 = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(_mm256_andnot_si256(mask1, mask2)));\n        __m256 res = _mm256_blendv_ps(res2, res1, _mm256_castsi256_ps(mask1));\n        \n        _mm256_storeu_ps(&out[i], res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n        \n        __m256i mask1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i mask2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n        \n        __m256 res1 = _mm256_blendv_ps(b, a, _mm256_castsi256_ps(_mm256_and_si256(mask1, mask2)));\n        __m256 res2 = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(_mm256_andnot_si256(mask1, mask2)));\n        __m256 res = _mm256_blendv_ps(res2, res1, _mm256_castsi256_ps(mask1));\n        \n        _mm256_storeu_ps(&out[i], res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i]) {\n            out[i] = cond2[i] ? A[i] : B[i];\n        } else {\n            out[i] = cond2[i] ? C[i] : D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n        __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n        __m256i v1 = _mm256_sub_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1));\n        __m256i v2 = _mm256_add_epi32(_mm256_and_si256(v1, mask2), _mm256_and_si256(_mm256_srli_epi32(v1, 2), mask2));\n        __m256i v3 = _mm256_add_epi32(_mm256_and_si256(v2, mask3), _mm256_and_si256(_mm256_srli_epi32(v2, 4), mask3));\n        __m256i v4 = _mm256_add_epi32(_mm256_and_si256(v3, mask4), _mm256_and_si256(_mm256_srli_epi32(v3, 8), mask4));\n        __m256i v5 = _mm256_add_epi32(_mm256_and_si256(v4, mask5), _mm256_and_si256(_mm256_srli_epi32(v4, 16), mask5));\n\n        __m128i low = _mm256_castsi256_si128(v5);\n        __m128i high = _mm256_extracti128_si256(v5, 1);\n        \n        __m128i result = _mm_packus_epi32(low, high);\n        result = _mm_packus_epi16(result, result);\n        \n        _mm_storel_epi64((__m128i*)(out + i), result);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val + (val >> 8)) & 0x00FF00FF;\n        val = (val + (val >> 16)) & 0x0000FFFF;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n        __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n        __m256i v1 = _mm256_sub_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1));\n        __m256i v2 = _mm256_add_epi32(_mm256_and_si256(v1, mask2), _mm256_and_si256(_mm256_srli_epi32(v1, 2), mask2));\n        __m256i v3 = _mm256_add_epi32(_mm256_and_si256(v2, mask3), _mm256_and_si256(_mm256_srli_epi32(v2, 4), mask3));\n        __m256i v4 = _mm256_add_epi32(_mm256_and_si256(v3, mask4), _mm256_and_si256(_mm256_srli_epi32(v3, 8), mask4));\n        __m256i v5 = _mm256_add_epi32(_mm256_and_si256(v4, mask5), _mm256_and_si256(_mm256_srli_epi32(v4, 16), mask5));\n\n        __m128i lo = _mm256_castsi256_si128(v5);\n        __m128i hi = _mm256_extracti128_si256(v5, 1);\n        __m128i packed = _mm_packus_epi32(lo, hi);\n        packed = _mm_packus_epi16(packed, packed);\n\n        _mm_storel_epi64((__m128i*)(out + i), packed);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        out[i] = x & 0x3F;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        __m256i v1 = _mm256_srli_epi32(v, 1);\n        v1 = _mm256_and_si256(v1, _mm256_set1_epi32(0x55555555));\n        v = _mm256_sub_epi32(v, v1);\n        \n        __m256i v2 = _mm256_srli_epi32(v, 2);\n        v2 = _mm256_and_si256(v2, _mm256_set1_epi32(0x33333333));\n        v1 = _mm256_and_si256(v, _mm256_set1_epi32(0x33333333));\n        v = _mm256_add_epi32(v1, v2);\n        \n        __m256i v4 = _mm256_srli_epi32(v, 4);\n        v4 = _mm256_add_epi32(v4, v);\n        v4 = _mm256_and_si256(v4, _mm256_set1_epi32(0x0F0F0F0F));\n        \n        __m256i v8 = _mm256_srli_epi32(v4, 8);\n        v8 = _mm256_add_epi32(v8, v4);\n        v8 = _mm256_and_si256(v8, _mm256_set1_epi32(0x00FF00FF));\n        \n        __m256i v16 = _mm256_srli_epi32(v8, 16);\n        v16 = _mm256_add_epi32(v16, v8);\n        v16 = _mm256_and_si256(v16, _mm256_set1_epi32(0x0000FFFF));\n        \n        __m128i lo = _mm256_castsi256_si128(v16);\n        __m128i hi = _mm256_extracti128_si256(v16, 1);\n        \n        __m128i result = _mm_packus_epi32(lo, hi);\n        result = _mm_packus_epi16(result, result);\n        \n        _mm_storel_epi64((__m128i*)(out + i), result);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x + (x >> 8)) & 0x00FF00FF;\n        x = (x + (x >> 16)) & 0x0000FFFF;\n        out[i] = (uint8_t)x;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        __m256i v1 = _mm256_srli_epi32(v, 1);\n        v1 = _mm256_and_si256(v1, _mm256_set1_epi32(0x55555555));\n        v = _mm256_sub_epi32(v, v1);\n        \n        __m256i v2 = _mm256_srli_epi32(v, 2);\n        v2 = _mm256_and_si256(v2, _mm256_set1_epi32(0x33333333));\n        v1 = _mm256_and_si256(v, _mm256_set1_epi32(0x33333333));\n        v = _mm256_add_epi32(v1, v2);\n        \n        __m256i v4 = _mm256_srli_epi32(v, 4);\n        v4 = _mm256_add_epi32(v, v4);\n        v4 = _mm256_and_si256(v4, _mm256_set1_epi32(0x0F0F0F0F));\n        \n        __m256i v8 = _mm256_srli_epi32(v4, 8);\n        v8 = _mm256_add_epi32(v4, v8);\n        \n        __m256i v16 = _mm256_srli_epi32(v8, 16);\n        v16 = _mm256_add_epi32(v8, v16);\n        \n        __m256i mask = _mm256_set1_epi32(0x0000003F);\n        v16 = _mm256_and_si256(v16, mask);\n        \n        __m128i lo = _mm256_castsi256_si128(v16);\n        __m128i hi = _mm256_extracti128_si256(v16, 1);\n        \n        __m128i packed = _mm_packus_epi32(lo, hi);\n        packed = _mm_packus_epi16(packed, packed);\n        \n        _mm_storel_epi64((__m128i*)(out + i), packed);\n    }\n    \n    for (; i < total_elements; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        out[i] = x & 0x3F;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n        __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n        __m256i v1 = _mm256_sub_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1));\n        __m256i v2 = _mm256_add_epi32(_mm256_and_si256(v1, mask2), _mm256_and_si256(_mm256_srli_epi32(v1, 2), mask2));\n        __m256i v3 = _mm256_add_epi32(_mm256_and_si256(v2, mask3), _mm256_and_si256(_mm256_srli_epi32(v2, 4), mask3));\n        __m256i v4 = _mm256_add_epi32(_mm256_and_si256(v3, mask4), _mm256_and_si256(_mm256_srli_epi32(v3, 8), mask4));\n        __m256i v5 = _mm256_add_epi32(_mm256_and_si256(v4, mask5), _mm256_and_si256(_mm256_srli_epi32(v4, 16), mask5));\n\n        __m128i lo = _mm256_castsi256_si128(v5);\n        __m128i hi = _mm256_extracti128_si256(v5, 1);\n        __m128i result = _mm_packus_epi32(lo, hi);\n        result = _mm_packus_epi16(result, result);\n\n        _mm_storel_epi64((__m128i*)(out + i), result);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val + (val >> 8)) & 0x00FF00FF;\n        val = (val + (val >> 16)) & 0x0000FFFF;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i remainder = _mm256_abs_epi32(a);\n        remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(_mm256_div_epi32(remainder, three), three));\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i abs_a = _mm256_and_si256(a, mask);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, cmp);\n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n\n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i remainder = _mm256_abs_epi32(a);\n        remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(_mm256_div_epi32(remainder, three), three));\n        \n        __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        \n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i remainder = _mm256_abs_epi32(a);\n        remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(_mm256_div_epi32(remainder, three), three));\n        \n        __m256i cmp = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i abs_a = _mm256_and_si256(a, mask);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, cmp);\n        \n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        B[i] = (val % 3 == 0) ? (val < 0 ? -val : val) : val;\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i remainder = _mm256_abs_epi32(a);\n        remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(_mm256_div_epi32(remainder, three), three));\n        \n        __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i abs_a = _mm256_abs_epi32(a);\n        __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n        \n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_neg = _mm256_set1_epi32(0x80000000);\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i remainder = _mm256_abs_epi32(a);\n        remainder = _mm256_sub_epi32(remainder, _mm256_and_si256(_mm256_srai_epi32(remainder, 30), remainder));\n        remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(_mm256_div_epi32(remainder, three), three));\n        \n        __m256i cmp = _mm256_cmpeq_epi32(remainder, zero);\n        __m256i abs_val = _mm256_andnot_si256(mask_neg, a);\n        __m256i result = _mm256_blendv_epi8(a, abs_val, cmp);\n        \n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n\n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF);\n    __m256i mask_all1 = _mm256_set1_epi32(0xF);\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    __m256i rotate_inv = _mm256_set1_epi32(32 - rotate_bits);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cond = _mm256_cmpeq_epi32(low4, mask_all1);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srl_epi32(src_vec, rotate_mask),\n            _mm256_sll_epi32(src_vec, rotate_inv)\n        );\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n        __m256i result = _mm256_blendv_epi8(negated, rotated, cond);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF);\n    __m256i all_ones = _mm256_set1_epi32(0xF);\n    __m256i rotate_val = _mm256_set1_epi32(rotate_bits);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cond = _mm256_cmpeq_epi32(low4, all_ones);\n        \n        __m256i rotated = _mm256_or_si256(\n            _mm256_srl_epi32(src_vec, rotate_val),\n            _mm256_sll_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_val))\n        );\n        \n        __m256i negated = _mm256_xor_si256(src_vec, neg_one);\n        \n        __m256i res = _mm256_blendv_epi8(negated, rotated, cond);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF);\n    __m256i mask_all_ones = _mm256_set1_epi32(0xF);\n    __m256i rotate_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cond = _mm256_cmpeq_epi32(low4, mask_all_ones);\n        \n        __m256i rotated = _mm256_or_si256(\n            _mm256_srl_epi32(src_vec, rotate_vec),\n            _mm256_sll_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_vec))\n        );\n        \n        __m256i negated = _mm256_xor_si256(src_vec, neg_one);\n        \n        __m256i res = _mm256_blendv_epi8(negated, rotated, cond);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF);\n    __m256i mask_all1 = _mm256_set1_epi32(0xF);\n    __m256i rotate_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i rotate_comp = _mm256_set1_epi32(32 - rotate_bits);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cond = _mm256_cmpeq_epi32(low4, mask_all1);\n        \n        __m256i rotated = _mm256_or_si256(\n            _mm256_srl_epi32(src_vec, rotate_vec),\n            _mm256_sll_epi32(src_vec, rotate_comp)\n        );\n        \n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n        \n        __m256i res = _mm256_blendv_epi8(negated, rotated, cond);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF);\n    __m256i mask_all1 = _mm256_set1_epi32(0xF);\n    __m256i rotate_val = _mm256_set1_epi32(rotate_bits);\n    __m256i rotate_shift = _mm256_set1_epi32(32 - rotate_bits);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cond = _mm256_cmpeq_epi32(low4, mask_all1);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srl_epi32(src_vec, rotate_val),\n            _mm256_sll_epi32(src_vec, rotate_shift)\n        );\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n        __m256i res = _mm256_blendv_epi8(negated, rotated, cond);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    eps = _mm256_and_ps(eps, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF))); // abs(epsilon)\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 gt_mask = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 lt_mask = _mm256_cmp_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), eps), _CMP_LT_OQ);\n        \n        __m256i result = _mm256_setzero_si256();\n        result = _mm256_sub_epi8(result, _mm256_castps_si256(lt_mask));\n        result = _mm256_add_epi8(result, _mm256_castps_si256(gt_mask));\n        \n        __m128i res_lo = _mm256_extracti128_si256(result, 0);\n        __m128i res_hi = _mm256_extracti128_si256(result, 1);\n        \n        _mm_storel_epi64((__m128i*)(out + i), res_lo);\n        _mm_storel_epi64((__m128i*)(out + i + 4), res_hi);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    eps = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), eps); // abs(epsilon)\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 gt_mask = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 lt_mask = _mm256_cmp_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), eps), _CMP_LT_OQ);\n        \n        __m256i gt = _mm256_cvtps_epi32(_mm256_and_ps(gt_mask, _mm256_set1_ps(1.0f)));\n        __m256i lt = _mm256_cvtps_epi32(_mm256_and_ps(lt_mask, _mm256_set1_ps(-1.0f)));\n        \n        __m256i result = _mm256_add_epi32(gt, lt);\n        __m128i result_low = _mm256_extracti128_si256(result, 0);\n        __m128i result_high = _mm256_extracti128_si256(result, 1);\n        \n        __m128i packed = _mm_packs_epi32(result_low, result_high);\n        packed = _mm_packs_epi16(packed, packed);\n        \n        _mm_storel_epi64((__m128i*)(out + i), packed);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    eps = _mm256_and_ps(eps, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF))); // abs(epsilon)\n    __m256 neg_eps = _mm256_sub_ps(_mm256_setzero_ps(), eps);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 mask_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 mask_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256i res = _mm256_setzero_si256();\n        res = _mm256_sub_epi32(res, _mm256_castps_si256(mask_lt)); // -1 where LT\n        res = _mm256_add_epi32(res, _mm256_castps_si256(mask_gt)); // +1 where GT\n\n        // Pack 32-bit results to 8-bit\n        __m128i res_lo = _mm256_castsi256_si128(res);\n        __m128i res_hi = _mm256_extracti128_si256(res, 1);\n        __m128i packed = _mm_packs_epi32(res_lo, res_hi);\n        packed = _mm_packs_epi16(packed, packed);\n        _mm_storel_epi64((__m128i*)(out + i), packed);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 abs_eps = _mm256_and_ps(eps, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n    __m256 neg_abs_eps = _mm256_sub_ps(_mm256_setzero_ps(), abs_eps);\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 mask_gt = _mm256_cmp_ps(diff, abs_eps, _CMP_GT_OS);\n        __m256 mask_lt = _mm256_cmp_ps(diff, neg_abs_eps, _CMP_LT_OS);\n        \n        __m256 res = _mm256_blendv_ps(_mm256_setzero_ps(), _mm256_set1_ps(1.0f), mask_gt);\n        res = _mm256_blendv_ps(res, _mm256_set1_ps(-1.0f), mask_lt);\n        \n        __m128i res8 = _mm256_cvtps_epi32(res);\n        res8 = _mm_packs_epi32(_mm256_extractf128_si256(res8, 0), _mm256_extractf128_si256(res8, 1));\n        res8 = _mm_packs_epi16(res8, res8);\n        \n        _mm_storel_epi64((__m128i*)(out + i), res8);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 abs_eps = _mm256_and_ps(eps, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 gt_mask = _mm256_cmp_ps(diff, abs_eps, _CMP_GT_OQ);\n        __m256 lt_mask = _mm256_cmp_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), abs_eps), _CMP_LT_OQ);\n        \n        __m256 gt_result = _mm256_and_ps(gt_mask, _mm256_set1_ps(1.0f));\n        __m256 lt_result = _mm256_and_ps(lt_mask, _mm256_set1_ps(-1.0f));\n        __m256 result = _mm256_or_ps(gt_result, lt_result);\n        \n        __m128i result_i32 = _mm256_cvtps_epi32(result);\n        __m128i result_i8 = _mm_packs_epi32(result_i32, result_i32);\n        result_i8 = _mm_packs_epi16(result_i8, result_i8);\n        \n        *(int64_t*)(out + i) = _mm_cvtsi128_si64(result_i8);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float32_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float32_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, float32_min), float32_max);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float32_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float32_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, float32_min), float32_max);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float32_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float32_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, float32_min), float32_max);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float32_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float32_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, float32_min), float32_max);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 float32_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 float32_min = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, float32_min), float32_max);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    \n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256i rounded = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m256d a = _mm256_loadu_pd(A + i);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm256_min_pd(a, flt_max);\n        a = _mm256_max_pd(a, neg_flt_max);\n        \n        // Convert to float\n        __m128 b = _mm256_cvtpd_ps(a);\n        _mm_storeu_ps(B + i, b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m256d a = _mm256_loadu_pd(A + i);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm256_min_pd(a, flt_max);\n        a = _mm256_max_pd(a, neg_flt_max);\n        \n        // Convert to float\n        __m128 b = _mm256_cvtpd_ps(a);\n        _mm_storeu_ps(B + i, b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m256d a = _mm256_loadu_pd(A + i);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm256_min_pd(a, flt_max);\n        a = _mm256_max_pd(a, neg_flt_max);\n        \n        // Convert to float\n        __m128 b = _mm256_cvtpd_ps(a);\n        _mm_storeu_ps(B + i, b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m256d a = _mm256_loadu_pd(A + i);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm256_min_pd(a, flt_max);\n        a = _mm256_max_pd(a, neg_flt_max);\n        \n        // Convert to float\n        __m128 b = _mm256_cvtpd_ps(a);\n        _mm_storeu_ps(B + i, b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m256d a = _mm256_loadu_pd(A + i);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm256_min_pd(a, flt_max);\n        a = _mm256_max_pd(a, neg_flt_max);\n        \n        // Convert double to float\n        __m128 b = _mm256_cvtpd_ps(a);\n        \n        _mm_storeu_ps(B + i, b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)(B + i), b);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)(B + i), b);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)(B + i), b);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)(B + i), b);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m256i b = _mm256_cvtepi16_epi32(a);\n        _mm256_storeu_si256((__m256i*)(B + i), b);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min = _mm256_set1_ps(min_val);\n    __m256 v_max = _mm256_set1_ps(max_val);\n    __m256 v_range = _mm256_sub_ps(v_max, v_min);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(c, _mm256_setzero_si256()));\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a, v_min), v_range);\n        normalized = _mm256_max_ps(normalized, v_zero);\n        normalized = _mm256_min_ps(normalized, v_one);\n\n        __m256 result = _mm256_blendv_ps(a, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vmax = _mm256_set1_ps(max_val);\n    __m256 vrange = _mm256_sub_ps(vmax, vmin);\n    __m256 vzero = _mm256_setzero_ps();\n    __m256 vone = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 mask = _mm256_cvtepi32_ps(_mm256_cmpgt_epi32(c, _mm256_setzero_si256()));\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a, vmin), vrange);\n        normalized = _mm256_max_ps(normalized, vzero);\n        normalized = _mm256_min_ps(normalized, vone);\n        \n        __m256 result = _mm256_blendv_ps(a, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vmax = _mm256_set1_ps(max_val);\n    __m256 vrange = _mm256_sub_ps(vmax, vmin);\n    __m256 vzero = _mm256_setzero_ps();\n    __m256 vone = _mm256_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 mask = _mm256_cvtepi32_ps(_mm256_cmpgt_epi32(c, _mm256_setzero_si256()));\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a, vmin), vrange);\n        normalized = _mm256_max_ps(normalized, vzero);\n        normalized = _mm256_min_ps(normalized, vone);\n        \n        __m256 result = _mm256_blendv_ps(a, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vmax = _mm256_set1_ps(max_val);\n    __m256 vrange = _mm256_sub_ps(vmax, vmin);\n    __m256 vzero = _mm256_set1_ps(0.0f);\n    __m256 vone = _mm256_set1_ps(1.0f);\n    __m256i vzero_int = _mm256_set1_epi32(0);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256i vctrl = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256i vmask = _mm256_cmpgt_epi32(vctrl, vzero_int);\n        __m256 vnorm = _mm256_div_ps(_mm256_sub_ps(va, vmin), vrange);\n        vnorm = _mm256_max_ps(vnorm, vzero);\n        vnorm = _mm256_min_ps(vnorm, vone);\n        __m256 vres = _mm256_blendv_ps(va, vnorm, _mm256_castsi256_ps(vmask));\n        _mm256_storeu_ps(&B[i], vres);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vmax = _mm256_set1_ps(max_val);\n    __m256 vrange = _mm256_sub_ps(vmax, vmin);\n    __m256 vzero = _mm256_setzero_ps();\n    __m256 vone = _mm256_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 mask = _mm256_cvtepi32_ps(_mm256_cmpgt_epi32(c, _mm256_setzero_si256()));\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a, vmin), vrange);\n        normalized = _mm256_max_ps(normalized, vzero);\n        normalized = _mm256_min_ps(normalized, vone);\n        \n        __m256 result = _mm256_blendv_ps(a, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = _mm_cvtss_f32(_mm_sqrt_ss(_mm_cvtsi32_ss(_mm_setzero_ps(), high16)));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union {\n            uint32_t u;\n            float f;\n        } pun;\n        pun.u = A[i];\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union { uint32_t u; float f; } pun = { .u = A[i] };\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    for (; i < size; ++i) {\n        union { uint32_t u; float f; } pun = { .u = A[i] };\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    for (; i < size; ++i) {\n        union { uint32_t u; float f; } pun = { .u = A[i] };\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union { uint32_t u; float f; } pun = { .u = A[i] };\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a_curr = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i a_prev = _mm256_loadu_si256((__m256i*)(A + i - 1));\n        __m256i diff_int = _mm256_sub_epi32(a_curr, a_prev);\n\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n        __m256i zero_mask = _mm256_cmpeq_epi32(cond_mask, _mm256_setzero_si256());\n        \n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        __m256 result = _mm256_blendv_ps(diff_float, _mm256_setzero_ps(), _mm256_castsi256_ps(zero_mask));\n\n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n        __m256i cond_ext = _mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1));\n        \n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((__m256i*)(A + i - 1));\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n        \n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        __m256 zeros = _mm256_setzero_ps();\n        __m256 result = _mm256_blendv_ps(zeros, diff_float, _mm256_castsi256_ps(cond_ext));\n        \n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n        __m256i cond_mask_shifted = _mm256_srli_epi32(cond_mask, 31);\n        __m256i a_curr = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i a_prev = _mm256_loadu_si256((__m256i*)(A + i - 1));\n        __m256i diff_int = _mm256_sub_epi32(a_curr, a_prev);\n        __m256i masked_diff = _mm256_and_si256(diff_int, cond_mask_shifted);\n        __m256 diff_float = _mm256_cvtepi32_ps(masked_diff);\n        _mm256_storeu_ps(diff + i, diff_float);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n        __m256i cond_mask_shift = _mm256_srli_epi32(cond_mask, 31);\n        \n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((__m256i*)(A + i - 1));\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n        \n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        __m256 zeros = _mm256_setzero_ps();\n        \n        __m256 result = _mm256_blendv_ps(zeros, diff_float, _mm256_castsi256_ps(cond_mask_shift));\n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n        __m256i cond_ext = _mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1));\n        \n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((__m256i*)(A + i - 1));\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n        \n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        __m256 zeros = _mm256_setzero_ps();\n        __m256 result = _mm256_blendv_ps(zeros, diff_float, _mm256_castsi256_ps(cond_ext));\n        \n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i src_ext = _mm256_cvtepu32_epi64(src_vec);\n        __m256i high_bits = _mm256_slli_epi64(ones, 32);\n        __m256i result = _mm256_or_si256(src_ext, high_bits);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i src_lo = _mm256_cvtepu32_epi64(src_vec);\n        __m256i src_hi = _mm256_slli_epi64(src_lo, 32);\n        __m256i result = _mm256_or_si256(src_hi, _mm256_set1_epi64x(0xFFFFFFFF00000000));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | (0xFFFFFFFF00000000);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i src_ext = _mm256_cvtepu32_epi64(src_vec);\n        __m256i dst_vec = _mm256_or_si256(src_ext, _mm256_slli_epi64(ones, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i extended = _mm256_cvtepu32_epi64(src_vec);\n        __m256i result = _mm256_or_si256(extended, _mm256_slli_epi64(ones, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i extended = _mm256_cvtepu32_epi64(src_vec);\n        __m256i result = _mm256_or_si256(extended, _mm256_slli_epi64(mask, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m256i idx_ext = _mm256_cvtepi16_epi32(idx);\n        __m128i idx_lo = _mm256_extracti128_si256(idx_ext, 0);\n        __m128i idx_hi = _mm256_extracti128_si256(idx_ext, 1);\n\n        __m256d val_lo = _mm256_i32gather_pd(vec, idx_lo, 8);\n        __m256d val_hi = _mm256_i32gather_pd(vec, idx_hi, 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, val_lo);\n        sum_vec = _mm256_add_pd(sum_vec, val_hi);\n    }\n\n    double sum = 0.0;\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    __m256d hsum = _mm256_hadd_pd(sum_vec, sum_vec);\n    sum += ((double*)&hsum)[0] + ((double*)&hsum)[2];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m256i idx_ext = _mm256_cvtepi16_epi32(idx);\n        __m128i idx_lo = _mm256_extracti128_si256(idx_ext, 0);\n        __m128i idx_hi = _mm256_extracti128_si256(idx_ext, 1);\n\n        __m256d val_lo = _mm256_i32gather_pd(vec, idx_lo, 8);\n        __m256d val_hi = _mm256_i32gather_pd(vec, idx_hi, 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, val_lo);\n        sum_vec = _mm256_add_pd(sum_vec, val_hi);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m256d v0 = _mm256_i32gather_pd(vec, _mm_cvtepi16_epi32(idx), 8);\n        sum_vec = _mm256_add_pd(sum_vec, v0);\n    }\n\n    double sum = 0.0;\n    __m256d temp = sum_vec;\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m256i idx_256 = _mm256_cvtepi16_epi32(idx);\n        __m128i idx_low = _mm256_extracti128_si256(idx_256, 0);\n        __m128i idx_high = _mm256_extracti128_si256(idx_256, 1);\n\n        __m256d val_low = _mm256_i32gather_pd(vec, idx_low, 8);\n        __m256d val_high = _mm256_i32gather_pd(vec, idx_high, 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, val_low);\n        sum_vec = _mm256_add_pd(sum_vec, val_high);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m256i idx_ext = _mm256_cvtepi16_epi32(idx);\n        __m128i idx_lo = _mm256_extracti128_si256(idx_ext, 0);\n        __m128i idx_hi = _mm256_extracti128_si256(idx_ext, 1);\n\n        __m256d val_lo = _mm256_i32gather_pd(vec, idx_lo, 8);\n        __m256d val_hi = _mm256_i32gather_pd(vec, idx_hi, 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, val_lo);\n        sum_vec = _mm256_add_pd(sum_vec, val_hi);\n    }\n\n    double sum = _mm256_reduce_add_pd(sum_vec);\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t kernel_elements = kernel_size * kernel_size;\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t kernel_idx = 0;\n            \n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t remaining = kernel_size - kj;\n                    if (remaining >= 4) {\n                        __m256d input_vec = _mm256_loadu_pd(input + (i + ki) * input_size + j + kj);\n                        __m256d kernel_vec = _mm256_loadu_pd(kernel + ki * kernel_size + kj);\n                        sum = _mm256_fmadd_pd(input_vec, kernel_vec, sum);\n                    } else {\n                        for (size_t k = 0; k < remaining; ++k) {\n                            double input_val = input[(i + ki) * input_size + j + kj + k];\n                            double kernel_val = kernel[ki * kernel_size + kj + k];\n                            sum = _mm256_add_pd(sum, _mm256_set1_pd(input_val * kernel_val));\n                        }\n                    }\n                }\n            }\n            \n            double result = 0.0;\n            double* sum_arr = (double*)&sum;\n            for (size_t k = 0; k < 4; ++k) {\n                result += sum_arr[k];\n            }\n            \n            output[i * output_size + j] = result > 0.0 ? result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t vec_size = 4; // AVX2 can process 4 doubles at once\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += vec_size) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    __m256d input_val = _mm256_loadu_pd(input + (i + ki) * input_size + (j + kj));\n                    __m256d kernel_val = _mm256_set1_pd(kernel[ki * kernel_size + kj]);\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n            // Apply ReLU activation\n            __m256d zero = _mm256_setzero_pd();\n            sum = _mm256_max_pd(sum, zero);\n            \n            // Store result\n            _mm256_storeu_pd(output + i * output_size + j, sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t vec_size = 4; // AVX2 can process 4 doubles at once\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += vec_size) {\n                    size_t remaining = kernel_size - kj;\n                    if (remaining >= vec_size) {\n                        __m256d input_vec = _mm256_loadu_pd(input + (i + ki) * input_size + j + kj);\n                        __m256d kernel_vec = _mm256_loadu_pd(kernel + ki * kernel_size + kj);\n                        sum = _mm256_fmadd_pd(input_vec, kernel_vec, sum);\n                    } else {\n                        for (size_t k = 0; k < remaining; ++k) {\n                            double input_val = input[(i + ki) * input_size + j + kj + k];\n                            double kernel_val = kernel[ki * kernel_size + kj + k];\n                            sum = _mm256_add_pd(sum, _mm256_set1_pd(input_val * kernel_val));\n                        }\n                    }\n                }\n            }\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            double total = result[0] + result[1] + result[2] + result[3];\n            output[i * output_size + j] = total > 0 ? total : 0; // ReLU\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t kernel_elements = kernel_size * kernel_size;\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            size_t kernel_idx = 0;\n            \n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t remaining = kernel_size - kj;\n                    if (remaining >= 4) {\n                        __m256d input_vec = _mm256_loadu_pd(input + (i + ki) * input_size + j + kj);\n                        __m256d kernel_vec = _mm256_loadu_pd(kernel + ki * kernel_size + kj);\n                        sum = _mm256_fmadd_pd(input_vec, kernel_vec, sum);\n                    } else {\n                        for (size_t k = 0; k < remaining; ++k) {\n                            double input_val = input[(i + ki) * input_size + j + kj + k];\n                            double kernel_val = kernel[ki * kernel_size + kj + k];\n                            sum = _mm256_add_pd(sum, _mm256_set1_pd(input_val * kernel_val));\n                        }\n                    }\n                }\n            }\n            \n            double result = 0.0;\n            double* sum_arr = (double*)&sum;\n            for (size_t k = 0; k < 4; ++k) {\n                result += sum_arr[k];\n            }\n            \n            output[i * output_size + j] = result > 0.0 ? result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t kernel_elements = kernel_size * kernel_size;\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t kernel_idx = 0;\n            \n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t remaining = kernel_size - kj;\n                    if (remaining >= 4) {\n                        __m256d input_vec = _mm256_loadu_pd(input + (i + ki) * input_size + j + kj);\n                        __m256d kernel_vec = _mm256_loadu_pd(kernel + ki * kernel_size + kj);\n                        sum_vec = _mm256_fmadd_pd(input_vec, kernel_vec, sum_vec);\n                    } else {\n                        for (size_t k = 0; k < remaining; ++k) {\n                            double input_val = input[(i + ki) * input_size + j + kj + k];\n                            double kernel_val = kernel[ki * kernel_size + kj + k];\n                            sum_vec = _mm256_add_pd(sum_vec, _mm256_set1_pd(input_val * kernel_val));\n                        }\n                    }\n                }\n            }\n            \n            double sum = _mm256_reduce_add_pd(sum_vec);\n            output[i * output_size + j] = sum > 0 ? sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    size_t vec_size = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d zero_vec = _mm256_setzero_pd();\n\n    for (; i + 4 <= vec_size - 1; i += 4) {\n        __m256d current_vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d next_vec = _mm256_loadu_pd(&numbers[i + 1]);\n        __m256d diff_vec = _mm256_sub_pd(next_vec, current_vec);\n        __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec);\n        __m256d cmp_mask = _mm256_cmp_pd(abs_diff_vec, thresh_vec, _CMP_LT_OS);\n        if (_mm256_movemask_pd(cmp_mask) != 0) {\n            return true;\n        }\n    }\n\n    for (; i < vec_size - 1; ++i) {\n        double diff = fabs(numbers[i + 1] - numbers[i]);\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    size_t size = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d zero_vec = _mm256_setzero_pd();\n\n    for (; i + 4 <= size - 1; i += 4) {\n        __m256d current = _mm256_loadu_pd(&numbers[i]);\n        __m256d next = _mm256_loadu_pd(&numbers[i + 1]);\n        __m256d diff = _mm256_sub_pd(next, current);\n        __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(zero_vec, diff), diff);\n        __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < size - 1; ++i) {\n        double diff = numbers[i + 1] - numbers[i];\n        if (diff < 0) diff = -diff;\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    size_t vec_size = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d neg_thresh_vec = _mm256_set1_pd(-threshold);\n\n    for (; i + 4 <= vec_size - 1; i += 4) {\n        __m256d current_vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d next_vec = _mm256_loadu_pd(&numbers[i + 1]);\n        __m256d diff_vec = _mm256_sub_pd(next_vec, current_vec);\n        __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec);\n        \n        __m256d cmp_result = _mm256_cmp_pd(abs_diff_vec, thresh_vec, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp_result);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < vec_size - 1; ++i) {\n        double diff = numbers[i + 1] - numbers[i];\n        if (diff < 0) diff = -diff;\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    size_t size = numbers.size();\n    __m256d thresh = _mm256_set1_pd(threshold);\n    __m256d sign_mask = _mm256_set1_pd(-0.0);\n\n    for (; i + 4 <= size - 1; i += 4) {\n        __m256d current = _mm256_loadu_pd(&numbers[i]);\n        __m256d next = _mm256_loadu_pd(&numbers[i + 1]);\n\n        __m256d diff = _mm256_sub_pd(next, current);\n        __m256d abs_diff = _mm256_andnot_pd(sign_mask, diff);\n\n        __m256d cmp = _mm256_cmp_pd(abs_diff, thresh, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < size - 1; ++i) {\n        double diff = fabs(numbers[i + 1] - numbers[i]);\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    size_t vec_size = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d zero_vec = _mm256_setzero_pd();\n\n    for (; i + 4 <= vec_size - 1; i += 4) {\n        __m256d current_vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d next_vec = _mm256_loadu_pd(&numbers[i + 1]);\n        __m256d diff_vec = _mm256_sub_pd(next_vec, current_vec);\n        __m256d abs_diff_vec = _mm256_max_pd(diff_vec, _mm256_sub_pd(zero_vec, diff_vec));\n        __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, thresh_vec, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp_vec);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < vec_size - 1; ++i) {\n        double diff = fabs(numbers[i + 1] - numbers[i]);\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    if (size == 0) return false;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i has_negative = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        has_negative = _mm256_or_si256(has_negative, cmp);\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (int j = i; j < size; ++j) {\n        remaining_balance += operations[j];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    // Check SIMD results\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, has_negative);\n    for (int k = 0; k < 8; ++k) {\n        if (result[k] != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    if (size == 0) return false;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i has_negative = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op);\n        __m256i mask = _mm256_cmpgt_epi32(zero, balance);\n        has_negative = _mm256_or_si256(has_negative, mask);\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    int* balance_arr = (int*)&balance;\n    for (int j = 0; j < 8; ++j) {\n        remaining_balance += balance_arr[j];\n    }\n\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    // Check if any lane had negative balance\n    int* has_negative_arr = (int*)&has_negative;\n    for (int j = 0; j < 8; ++j) {\n        if (has_negative_arr[j] != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    if (size == 0) return false;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i has_negative = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op);\n        __m256i mask = _mm256_cmpgt_epi32(zero, balance);\n        has_negative = _mm256_or_si256(has_negative, mask);\n    }\n\n    // Check the remaining elements\n    int remaining_balance = 0;\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    // Check SIMD results\n    int results[8];\n    _mm256_storeu_si256((__m256i*)results, has_negative);\n    for (int j = 0; j < 8; ++j) {\n        if (results[j] != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    int size = operations.size();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i has_negative = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        has_negative = _mm256_or_si256(has_negative, cmp);\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (int j = i; j < size; ++j) {\n        remaining_balance += operations[j];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    // Check SIMD results\n    alignas(32) int balance_arr[8];\n    alignas(32) int has_negative_arr[8];\n    _mm256_store_si256((__m256i*)balance_arr, balance);\n    _mm256_store_si256((__m256i*)has_negative_arr, has_negative);\n\n    for (int k = 0; k < 8; ++k) {\n        if (has_negative_arr[k] != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    int size = operations.size();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i has_negative = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        has_negative = _mm256_or_si256(has_negative, cmp);\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    // Check SIMD results\n    alignas(32) int balance_arr[8];\n    alignas(32) int has_negative_arr[8];\n    _mm256_store_si256((__m256i*)balance_arr, balance);\n    _mm256_store_si256((__m256i*)has_negative_arr, has_negative);\n\n    for (int j = 0; j < 8; ++j) {\n        if (has_negative_arr[j]) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum = _mm256_setzero_pd();\n    double* data = numbers.data();\n    size_t i;\n\n    // Calculate mean\n    for (i = 0; i + 3 < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(data + i);\n        sum = _mm256_add_pd(sum, vec);\n    }\n    double mean = _mm256_reduce_add_pd(sum);\n    for (; i < n; ++i) {\n        mean += data[i];\n    }\n    mean /= n;\n\n    // Calculate MAD\n    __m256d mad_sum = _mm256_setzero_pd();\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    for (i = 0; i + 3 < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(data + i);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n    }\n    double mad = _mm256_reduce_add_pd(mad_sum);\n    for (; i < n; ++i) {\n        mad += fabs(data[i] - mean);\n    }\n    mad /= n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i;\n    for (i = 0; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    double sum = _mm256_reduce_add_pd(sum_vec);\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n    double mean = sum / n;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    for (i = 0; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n    double mad_sum = _mm256_reduce_add_pd(mad_sum_vec);\n    for (; i < n; ++i) {\n        mad_sum += fabs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    double mad_sum = 0.0;\n    i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(temp, mad_sum_vec);\n    mad_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        mad_sum += fabs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    \n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    double mad_sum = 0.0;\n\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(temp_sum, mad_sum_vec);\n    mad_sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < n; ++i) {\n        mad_sum += fabs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    \n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    double mad_sum = 0.0;\n\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(temp_sum, mad_sum_vec);\n    mad_sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < n; ++i) {\n        mad_sum += fabs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m256i delimiter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= original_size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i shuffled_lo = _mm256_unpacklo_epi32(data, delimiter_vec);\n        __m256i shuffled_hi = _mm256_unpackhi_epi32(data, delimiter_vec);\n\n        __m256i permuted_lo = _mm256_permute4x64_epi64(shuffled_lo, 0b11011000);\n        __m256i permuted_hi = _mm256_permute4x64_epi64(shuffled_hi, 0b11011000);\n\n        _mm256_storeu_si256((__m256i*)&result[i * 2], permuted_lo);\n        _mm256_storeu_si256((__m256i*)&result[i * 2 + 8], permuted_hi);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i * 2 + 1 < new_size) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m256i delimiter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= original_size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i shuffled_lo = _mm256_unpacklo_epi32(data, delimiter_vec);\n        __m256i shuffled_hi = _mm256_unpackhi_epi32(data, delimiter_vec);\n\n        __m256i permuted_lo = _mm256_permute4x64_epi64(shuffled_lo, 0b11011000);\n        __m256i permuted_hi = _mm256_permute4x64_epi64(shuffled_hi, 0b11011000);\n\n        _mm256_storeu_si256((__m256i*)&result[i * 2], permuted_lo);\n        _mm256_storeu_si256((__m256i*)&result[i * 2 + 8], permuted_hi);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i * 2 + 1 < new_size) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m256i delimiter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= original_size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&numbers[i]);\n        \n        __m256i even_mask = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        __m256i odd_mask = _mm256_set_epi32(15, 14, 13, 12, 11, 10, 9, 8);\n        \n        __m256i shuffled1 = _mm256_permutevar8x32_epi32(data, even_mask);\n        __m256i shuffled2 = _mm256_permutevar8x32_epi32(delimiter_vec, odd_mask);\n        \n        __m256i interleaved_lo = _mm256_unpacklo_epi32(shuffled1, shuffled2);\n        __m256i interleaved_hi = _mm256_unpackhi_epi32(shuffled1, shuffled2);\n        \n        _mm256_storeu_si256((__m256i*)&result[i * 2], interleaved_lo);\n        _mm256_storeu_si256((__m256i*)&result[i * 2 + 8], interleaved_hi);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != original_size - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m256i delimiter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= original_size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i shuffled_lo = _mm256_unpacklo_epi32(data, delimiter_vec);\n        __m256i shuffled_hi = _mm256_unpackhi_epi32(data, delimiter_vec);\n\n        __m256i permuted_lo = _mm256_permute4x64_epi64(shuffled_lo, 0b11011000);\n        __m256i permuted_hi = _mm256_permute4x64_epi64(shuffled_hi, 0b11011000);\n\n        __m256i interleaved = _mm256_blend_epi32(permuted_lo, permuted_hi, 0b10101010);\n        __m256i final = _mm256_permutevar8x32_epi32(interleaved, _mm256_setr_epi32(0, 4, 1, 5, 2, 6, 3, 7));\n\n        _mm256_storeu_si256((__m256i*)&result[i * 2], final);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i * 2 + 1 < new_size) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m256i delimiter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 8 <= original_size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i shuffled_lo = _mm256_unpacklo_epi32(data, delimiter_vec);\n        __m256i shuffled_hi = _mm256_unpackhi_epi32(data, delimiter_vec);\n\n        __m256i permuted_lo = _mm256_permute4x64_epi64(shuffled_lo, 0b11011000);\n        __m256i permuted_hi = _mm256_permute4x64_epi64(shuffled_hi, 0b11011000);\n\n        _mm256_storeu_si256((__m256i*)&result[i * 2], permuted_lo);\n        _mm256_storeu_si256((__m256i*)&result[i * 2 + 8], permuted_hi);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i * 2 + 1 < new_size) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n    int i = 0;\n    int size = numbers.size();\n    \n    for (; i + 15 < size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        \n        // Sum calculation\n        __m256i sum_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i sum_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        sum_vec = _mm256_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m256i prod_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i prod_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t sum = _mm_extract_epi32(sum128, 0);\n    \n    // Horizontal product of prod_vec\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_vec, 0), _mm256_extracti128_si256(prod_vec, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t prod = _mm_extract_epi32(prod128, 0);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        prod *= numbers[i];\n    }\n    \n    return {(int64_t)sum, (int64_t)prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n    int i = 0;\n    int size = numbers.size();\n    \n    for (; i + 15 < size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        \n        // Sum calculation\n        __m256i sum_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i sum_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        sum_vec = _mm256_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m256i prod_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i prod_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_high);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t sum = _mm_extract_epi32(sum128, 0);\n\n    // Horizontal product of prod_vec\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_vec, 0), _mm256_extracti128_si256(prod_vec, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t prod = _mm_extract_epi32(prod128, 0);\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        prod *= numbers[i];\n    }\n\n    return {(int64_t)sum, (int64_t)prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n    \n    size_t i = 0;\n    for (; i + 16 <= numbers.size(); i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        \n        // Sum calculation\n        __m256i sum_lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i sum_hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        sum_vec = _mm256_add_epi32(sum_vec, sum_lo);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_hi);\n        \n        // Product calculation\n        __m256i prod_lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i prod_hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_lo);\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_hi);\n    }\n    \n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), \n                                   _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    int32_t sum = _mm_extract_epi32(sum128, 0);\n    \n    // Horizontal product of prod_vec\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_vec, 0), \n                                     _mm256_extracti128_si256(prod_vec, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    int32_t prod = _mm_extract_epi32(prod128, 0);\n    \n    // Process remaining elements\n    for (; i < numbers.size(); ++i) {\n        sum += numbers[i];\n        prod *= numbers[i];\n    }\n    \n    return {static_cast<int64_t>(sum), static_cast<int64_t>(prod)};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n    \n    size_t size = numbers.size();\n    if (size == 0) {\n        return {0, 1};\n    }\n\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi16(1);\n\n    // Process 16 elements at a time (since each __m256i can hold 16 int16_t)\n    for (; i + 15 < size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&numbers[i]);\n        \n        // Sum calculation\n        __m256i sum_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i sum_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        sum_vec = _mm256_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m256i prod_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i prod_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        product_vec = _mm256_mullo_epi32(product_vec, prod_low);\n        product_vec = _mm256_mullo_epi32(product_vec, prod_high);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum += _mm_extract_epi32(sum128, 0);\n\n    // Horizontal product of product_vec\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(product_vec, 0), _mm256_extracti128_si256(product_vec, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    product *= _mm_extract_epi32(prod128, 0);\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n    int i = 0;\n    int size = numbers.size();\n    \n    for (; i + 15 < size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        \n        // Sum calculation\n        __m256i sum_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i sum_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        sum_vec = _mm256_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m256i prod_low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 0));\n        __m256i prod_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(data, 1));\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm256_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t sum_result = _mm_extract_epi32(sum128, 0);\n    \n    // Horizontal product of prod_vec\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_vec, 0), _mm256_extracti128_si256(prod_vec, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int32_t prod_result = _mm_extract_epi32(prod128, 0);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum_result += numbers[i];\n        prod_result *= numbers[i];\n    }\n    \n    return {(int64_t)sum_result, (int64_t)prod_result};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    size_t i = 1;\n    __m256i current_max = _mm256_set1_epi32(result[0]);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i max_vec = _mm256_max_epi32(vec, current_max);\n        current_max = _mm256_set1_epi32(_mm256_extract_epi32(max_vec, 7));\n        _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = std::max(numbers[i], result[i-1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_set1_epi32(result[i-1]);\n        \n        __m256i max_val = _mm256_max_epi32(current, prev_max);\n        _mm256_storeu_si256((__m256i*)&result[i], max_val);\n        \n        for (int j = 0; j < 8; ++j) {\n            if (result[i+j] < result[i+j-1]) {\n                result[i+j] = result[i+j-1];\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = numbers[i] > result[i-1] ? numbers[i] : result[i-1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_set1_epi32(result[i-1]);\n        \n        __m256i max_vals = _mm256_max_epi32(current, prev_max);\n        \n        __m256i scan_max = _mm256_set_epi32(\n            _mm256_extract_epi32(max_vals, 6),\n            _mm256_extract_epi32(max_vals, 5),\n            _mm256_extract_epi32(max_vals, 4),\n            _mm256_extract_epi32(max_vals, 3),\n            _mm256_extract_epi32(max_vals, 2),\n            _mm256_extract_epi32(max_vals, 1),\n            _mm256_extract_epi32(max_vals, 0),\n            result[i-1]\n        );\n        \n        scan_max = _mm256_max_epi32(scan_max, _mm256_permutevar8x32_epi32(scan_max, _mm256_setr_epi32(0, 0, 1, 2, 3, 4, 5, 6)));\n        scan_max = _mm256_max_epi32(scan_max, _mm256_permutevar8x32_epi32(scan_max, _mm256_setr_epi32(0, 0, 0, 1, 2, 3, 4, 5)));\n        scan_max = _mm256_max_epi32(scan_max, _mm256_permutevar8x32_epi32(scan_max, _mm256_setr_epi32(0, 0, 0, 0, 1, 2, 3, 4)));\n        \n        _mm256_storeu_si256((__m256i*)&result[i], scan_max);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = std::max(numbers[i], result[i-1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_set1_epi32(result[i-1]);\n        \n        __m256i max_val = _mm256_max_epi32(current, prev_max);\n        \n        __m256i shifted_max = _mm256_permutevar8x32_epi32(max_val, _mm256_setr_epi32(0, 0, 1, 2, 3, 4, 5, 6));\n        shifted_max = _mm256_max_epi32(shifted_max, _mm256_setr_epi32(result[i-1], numbers[i], numbers[i+1], numbers[i+2], \n                                     numbers[i+3], numbers[i+4], numbers[i+5], numbers[i+6]));\n        \n        _mm256_storeu_si256((__m256i*)&result[i], max_val);\n        \n        for (int j = 0; j < 7; ++j) {\n            result[i+j] = std::max(result[i+j], result[i+j-1]);\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = std::max(numbers[i], result[i-1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_set1_epi32(result[i-1]);\n        \n        __m256i max_val = _mm256_max_epi32(current, prev_max);\n        \n        __m256i shifted1 = _mm256_alignr_epi8(max_val, max_val, 4);\n        shifted1 = _mm256_blend_epi32(max_val, shifted1, 0xFE);\n        max_val = _mm256_max_epi32(max_val, shifted1);\n        \n        __m256i shifted2 = _mm256_alignr_epi8(max_val, max_val, 8);\n        shifted2 = _mm256_blend_epi32(max_val, shifted2, 0xFC);\n        max_val = _mm256_max_epi32(max_val, shifted2);\n        \n        __m256i shifted4 = _mm256_permute2x128_si256(max_val, max_val, 0x08);\n        shifted4 = _mm256_alignr_epi8(max_val, shifted4, 12);\n        shifted4 = _mm256_blend_epi32(max_val, shifted4, 0xF0);\n        max_val = _mm256_max_epi32(max_val, shifted4);\n        \n        _mm256_storeu_si256((__m256i*)&result[i], max_val);\n        \n        // Update the last element for next iteration\n        result[i+7] = _mm256_extract_epi32(max_val, 7);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        result[i] = std::max(numbers[i], result[i-1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    a.resize(len, '0');\n    b.resize(len, '0');\n    std::string result(len, '0');\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), res);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    a.resize(len, '0');\n    b.resize(len, '0');\n    std::string result(len, '0');\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), res);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    a.resize(len, '0');\n    b.resize(len, '0');\n    std::string result(len, '0');\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), res);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    a.resize(len, '0');\n    b.resize(len, '0');\n    std::string result(len, '0');\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&a[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&b[i]);\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n    \n    size_t i = 0;\n    const size_t simd_size = 32; // AVX2 can process 256 bits (32 bytes) at a time\n    \n    for (; i + simd_size <= len; i += simd_size) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        \n        __m256i xor_result = _mm256_xor_si256(a_vec, b_vec);\n        \n        _mm256_storeu_si256((__m256i*)(result.data() + i), xor_result);\n    }\n    \n    // Process remaining elements\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 8 strings at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        \n        // Load lengths of 8 strings\n        for (int j = 0; j < 8; ++j) {\n            lengths = _mm256_insert_epi64(lengths, strings[i + j].length(), j);\n        }\n\n        // Find maximum length in the current 8 strings\n        __m256i max_vec = _mm256_max_epu64(lengths, _mm256_permute4x64_epi64(lengths, 0b01001110));\n        max_vec = _mm256_max_epu64(max_vec, _mm256_permute4x64_epi64(max_vec, 0b10110001));\n        uint64_t current_max = _mm256_extract_epi64(max_vec, 0);\n\n        if (current_max > max_len) {\n            max_len = current_max;\n            // Find the first occurrence of max_len in current 8 strings\n            for (int j = 0; j < 8; ++j) {\n                if (strings[i + j].length() == max_len) {\n                    max_idx = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = strings[0].length();\n    size_t max_idx = 0;\n\n    size_t i = 0;\n    const size_t vec_size = 32; // AVX2 can process 256 bits (32 bytes) at a time\n    const size_t num_strings = strings.size();\n\n    for (; i + 8 <= num_strings; i += 8) {\n        __m256i len_vec = _mm256_setzero_si256();\n        \n        // Load 8 string lengths\n        for (size_t j = 0; j < 8; ++j) {\n            size_t len = strings[i + j].length();\n            len_vec = _mm256_insert_epi32(len_vec, len, j);\n        }\n\n        // Compare with current max\n        __m256i max_vec = _mm256_set1_epi32(max_len);\n        __m256i cmp = _mm256_cmpgt_epi32(len_vec, max_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first string longer than current max\n            for (size_t j = 0; j < 8; ++j) {\n                size_t len = strings[i + j].length();\n                if (len > max_len) {\n                    max_len = len;\n                    max_idx = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < num_strings; ++i) {\n        size_t len = strings[i].length();\n        if (len > max_len) {\n            max_len = len;\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 8 strings at a time using AVX2\n    const size_t simd_width = 8;\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load lengths of 8 strings into AVX2 registers\n        __m256i lengths = _mm256_set_epi32(\n            strings[i+7].length(),\n            strings[i+6].length(),\n            strings[i+5].length(),\n            strings[i+4].length(),\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        // Find maximum length in current batch\n        __m256i max_vec = _mm256_max_epu32(lengths, _mm256_permutevar8x32_epi32(lengths, _mm256_set_epi32(3,2,1,0,7,6,5,4)));\n        max_vec = _mm256_max_epu32(max_vec, _mm256_permutevar8x32_epi32(max_vec, _mm256_set_epi32(1,0,3,2,5,4,7,6)));\n        max_vec = _mm256_max_epu32(max_vec, _mm256_permutevar8x32_epi32(max_vec, _mm256_set_epi32(0,1,2,3,4,5,6,7)));\n        uint32_t current_max = _mm256_extract_epi32(max_vec, 0);\n\n        // Update global maximum if needed\n        if (current_max > max_len) {\n            max_len = current_max;\n            // Find the first occurrence of max_len in current batch\n            for (size_t j = 0; j < simd_width; ++j) {\n                if (strings[i+j].length() == max_len) {\n                    max_idx = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 8 strings at a time using AVX2\n    const size_t simd_width = 8;\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load lengths of 8 strings into AVX registers\n        __m256i lengths = _mm256_set_epi32(\n            strings[i+7].length(),\n            strings[i+6].length(),\n            strings[i+5].length(),\n            strings[i+4].length(),\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        // Find maximum length in current batch\n        __m256i max_vec = _mm256_set1_epi32(max_len);\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // There are longer strings in this batch\n            alignas(32) int len_arr[8];\n            _mm256_store_si256((__m256i*)len_arr, lengths);\n\n            for (int j = 0; j < 8; ++j) {\n                if (len_arr[j] > max_len) {\n                    max_len = len_arr[j];\n                    max_idx = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    const size_t size = strings.size();\n    size_t i = 0;\n\n    // Process 8 strings at a time using AVX2\n    if (size >= 8) {\n        __m256i max_lens = _mm256_setzero_si256();\n        __m256i max_idxs = _mm256_setzero_si256();\n        __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n        for (; i + 8 <= size; i += 8) {\n            // Load lengths of 8 strings\n            __m256i lens = _mm256_setr_epi32(\n                strings[i].length(),\n                strings[i+1].length(),\n                strings[i+2].length(),\n                strings[i+3].length(),\n                strings[i+4].length(),\n                strings[i+5].length(),\n                strings[i+6].length(),\n                strings[i+7].length()\n            );\n\n            // Compare with current max\n            __m256i cmp = _mm256_cmpgt_epi32(lens, max_lens);\n            max_lens = _mm256_max_epi32(max_lens, lens);\n            max_idxs = _mm256_blendv_epi8(max_idxs, indices, cmp);\n            indices = _mm256_add_epi32(indices, _mm256_set1_epi32(8));\n        }\n\n        // Extract max from SIMD registers\n        alignas(32) int32_t lens_arr[8];\n        alignas(32) int32_t idxs_arr[8];\n        _mm256_store_si256((__m256i*)lens_arr, max_lens);\n        _mm256_store_si256((__m256i*)idxs_arr, max_idxs);\n\n        for (int j = 0; j < 8; ++j) {\n            if (lens_arr[j] > max_len) {\n                max_len = lens_arr[j];\n                max_idx = idxs_arr[j];\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n    \n    while (b != 0) {\n        __m256i vec_t = _mm256_set1_epi64x(b);\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(_mm256_set1_epi64x(a / b), vec_b));\n        vec_a = vec_t;\n        b = _mm256_extract_epi64(vec_b, 0);\n    }\n    \n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n    \n    while (b != 0) {\n        __m256i vec_tmp = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(_mm256_set1_epi64x(a / b), vec_b));\n        vec_a = vec_b;\n        vec_b = vec_tmp;\n        \n        uint64_t tmp = b;\n        b = a % b;\n        a = tmp;\n    }\n    \n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n    \n    while (b != 0) {\n        __m256i vec_tmp = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(_mm256_set1_epi64x(a/b), vec_b));\n        vec_a = vec_b;\n        vec_b = vec_tmp;\n        \n        uint64_t tmp = b;\n        b = a % b;\n        a = tmp;\n    }\n    \n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n    \n    while (_mm256_extract_epi64(vec_b, 0) != 0) {\n        __m256i vec_tmp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mullo_epi64(_mm256_div_epu64(vec_a, vec_b), vec_b));\n        vec_a = vec_tmp;\n    }\n    \n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n    \n    while (_mm256_extract_epi64(vec_b, 0) != 0) {\n        __m256i vec_tmp = vec_b;\n        vec_b = _mm256_sub_epi64(vec_a, _mm256_mul_epu32(vec_b, _mm256_div_epu32(vec_a, vec_b)));\n        vec_a = vec_tmp;\n    }\n    \n    return _mm256_extract_epi64(vec_a, 0);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = std::numeric_limits<float>::max();\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    size_t size = numbers.size();\n    for (; i + 8 <= size; i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n        \n        float diffs[8];\n        _mm256_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < size; ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = INFINITY;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    size_t size = numbers.size();\n    for (; i + 8 <= size; i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n\n        __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n        __m256 cmp = _mm256_cmp_ps(diff, min_diff_vec, _CMP_LT_OS);\n\n        int mask = _mm256_movemask_ps(cmp);\n        if (mask != 0) {\n            float temp_diff[8];\n            _mm256_storeu_ps(temp_diff, diff);\n            for (int j = 0; j < 8; ++j) {\n                if (temp_diff[j] < min_diff) {\n                    min_diff = temp_diff[j];\n                    num1 = numbers[i + j];\n                    num2 = numbers[i + j + 1];\n                }\n            }\n        }\n    }\n\n    for (; i + 1 < size; ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = INFINITY;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        float diffs[8];\n        _mm256_storeu_ps(diffs, abs_diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = std::numeric_limits<float>::max();\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n        \n        float diffs[8];\n        _mm256_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = std::numeric_limits<float>::max();\n    float num1 = 0.0f, num2 = 0.0f;\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n        \n        float diffs[8];\n        _mm256_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using SIMD\n    __m256 min_vec = _mm256_set1_ps(numbers[0]);\n    __m256 max_vec = _mm256_set1_ps(numbers[0]);\n    \n    size_t i;\n    for (i = 0; i + 8 <= numbers.size(); i += 8) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        min_vec = _mm256_min_ps(min_vec, data);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n\n    // Horizontal min/max\n    float min_val = numbers[0], max_val = numbers[0];\n    float temp_min[8], temp_max[8];\n    _mm256_storeu_ps(temp_min, min_vec);\n    _mm256_storeu_ps(temp_max, max_vec);\n    \n    for (int j = 0; j < 8; ++j) {\n        if (temp_min[j] < min_val) min_val = temp_min[j];\n        if (temp_max[j] > max_val) max_val = temp_max[j];\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        if (numbers[i] < min_val) min_val = numbers[i];\n        if (numbers[i] > max_val) max_val = numbers[i];\n    }\n\n    // Avoid division by zero\n    if (min_val == max_val) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    // Compute scale factors\n    __m256 scale = _mm256_set1_ps(1.0f / (max_val - min_val));\n    __m256 offset = _mm256_set1_ps(-min_val);\n\n    // Apply transformation\n    for (i = 0; i + 8 <= numbers.size(); i += 8) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        data = _mm256_add_ps(data, offset);\n        data = _mm256_mul_ps(data, scale);\n        _mm256_storeu_ps(&numbers[i], data);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    float inv_range = 1.0f / range;\n    __m256 v_min = _mm256_set1_ps(min_val);\n    __m256 v_inv_range = _mm256_set1_ps(inv_range);\n\n    size_t i = 0;\n    size_t aligned_size = numbers.size() & ~7;\n    for (; i < aligned_size; i += 8) {\n        __m256 v_data = _mm256_loadu_ps(&numbers[i]);\n        v_data = _mm256_sub_ps(v_data, v_min);\n        v_data = _mm256_mul_ps(v_data, v_inv_range);\n        _mm256_storeu_ps(&numbers[i], v_data);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) * inv_range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    float inv_range = 1.0f / range;\n    __m256 v_min = _mm256_set1_ps(min_val);\n    __m256 v_inv_range = _mm256_set1_ps(inv_range);\n\n    size_t i = 0;\n    size_t aligned_size = numbers.size() & ~7; // Process in chunks of 8 floats\n\n    for (; i < aligned_size; i += 8) {\n        __m256 v_data = _mm256_loadu_ps(&numbers[i]);\n        v_data = _mm256_sub_ps(v_data, v_min);\n        v_data = _mm256_mul_ps(v_data, v_inv_range);\n        _mm256_storeu_ps(&numbers[i], v_data);\n    }\n\n    // Process remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) * inv_range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    float inv_range = 1.0f / range;\n    __m256 v_min = _mm256_set1_ps(min_val);\n    __m256 v_inv_range = _mm256_set1_ps(inv_range);\n\n    size_t i = 0;\n    size_t aligned_size = numbers.size() & ~7; // Process in chunks of 8 floats\n\n    for (; i < aligned_size; i += 8) {\n        __m256 v_data = _mm256_loadu_ps(&numbers[i]);\n        v_data = _mm256_sub_ps(v_data, v_min);\n        v_data = _mm256_mul_ps(v_data, v_inv_range);\n        _mm256_storeu_ps(&numbers[i], v_data);\n    }\n\n    // Process remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) * inv_range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    for (float num : numbers) {\n        min_val = std::min(min_val, num);\n        max_val = std::max(max_val, num);\n    }\n\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    float inv_range = 1.0f / range;\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 inv_range_vec = _mm256_set1_ps(inv_range);\n\n    size_t i = 0;\n    size_t n = numbers.size();\n    for (; i + 8 <= n; i += 8) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        data = _mm256_sub_ps(data, min_vec);\n        data = _mm256_mul_ps(data, inv_range_vec);\n        _mm256_storeu_ps(&numbers[i], data);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        numbers[i] = (numbers[i] - min_val) * inv_range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) return 1;\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n    \n    // Process potential divisors in parallel using AVX2\n    for (uint64_t i = 2; i <= sqrt_n; i += 4) {\n        __m256i divisors = _mm256_set_epi64x(i+3, i+2, i+1, i);\n        __m256i remainders = _mm256_set1_epi64x(n);\n        \n        // Calculate n % divisors\n        __m256i quotients = _mm256_div_epu64(remainders, divisors);\n        __m256i products = _mm256_mul_epu32(divisors, quotients);\n        __m256i rem = _mm256_sub_epi64(remainders, products);\n        \n        // Check which divisors divide n evenly\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpeq_epi64(rem, zero);\n        \n        // Extract results\n        uint64_t rem_arr[4];\n        _mm256_storeu_si256((__m256i*)rem_arr, rem);\n        \n        uint64_t div_arr[4];\n        _mm256_storeu_si256((__m256i*)div_arr, divisors);\n        \n        for (int j = 0; j < 4; ++j) {\n            if (rem_arr[j] == 0 && div_arr[j] < n) {\n                if (div_arr[j] > max_divisor) {\n                    max_divisor = div_arr[j];\n                }\n                uint64_t counterpart = n / div_arr[j];\n                if (counterpart > max_divisor && counterpart < n) {\n                    max_divisor = counterpart;\n                }\n            }\n        }\n    }\n    \n    // Handle remaining divisors not processed by SIMD\n    for (uint64_t i = sqrt_n + 1; i < n; ++i) {\n        if (n % i == 0) {\n            max_divisor = i;\n            break;\n        }\n    }\n    \n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_set1_epi64x(0);\n\n    for (uint64_t i = 2; i <= sqrt_n; i += 4) {\n        __m256i v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n        __m256i v_rem = _mm256_sub_epi64(v_n, _mm256_mul_epu32(_mm256_div_epu32(v_n, v_i), v_i));\n        __m256i v_mask = _mm256_cmpeq_epi64(v_rem, v_zero);\n\n        int mask = _mm256_movemask_epi8(v_mask);\n        if (mask != 0) {\n            uint64_t divisors[4];\n            _mm256_storeu_si256((__m256i*)divisors, v_i);\n\n            uint64_t rems[4];\n            _mm256_storeu_si256((__m256i*)rems, v_rem);\n\n            for (int j = 0; j < 4; ++j) {\n                if (rems[j] == 0) {\n                    if (divisors[j] > max_divisor) {\n                        max_divisor = divisors[j];\n                    }\n                    uint64_t counterpart = n / divisors[j];\n                    if (counterpart != divisors[j] && counterpart > max_divisor) {\n                        max_divisor = counterpart;\n                    }\n                }\n            }\n        }\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 4 candidates at a time using AVX2\n    for (uint64_t i = 2; i <= sqrt_n; i += 4) {\n        __m256i divisors = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i remainders = _mm256_set1_epi64x(n);\n        remainders = _mm256_rem_epu64(remainders, divisors);\n\n        // Check which divisors are valid (remainder == 0)\n        __m256i mask = _mm256_cmpeq_epi64(remainders, _mm256_setzero_si256());\n        int valid = _mm256_movemask_epi8(mask);\n\n        // Process each candidate in the vector\n        for (int j = 0; j < 4; j++) {\n            if (valid & (1 << (j * 8))) {\n                uint64_t candidate = i + j;\n                if (candidate > sqrt_n) continue;\n                if (candidate > max_divisor) {\n                    max_divisor = candidate;\n                }\n                uint64_t pair = n / candidate;\n                if (pair != candidate && pair > max_divisor) {\n                    max_divisor = pair;\n                }\n            }\n        }\n    }\n\n    // Handle remaining candidates not processed by SIMD\n    for (uint64_t i = (sqrt_n / 4) * 4 + 1; i <= sqrt_n; i++) {\n        if (n % i == 0) {\n            max_divisor = i;\n            uint64_t pair = n / i;\n            if (pair != i && pair > max_divisor) {\n                max_divisor = pair;\n            }\n        }\n    }\n\n    return max_divisor == 1 ? 1 : (max_divisor == n ? 1 : max_divisor);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 4 candidates at a time using AVX2\n    for (uint64_t i = 2; i <= sqrt_n; i += 4) {\n        __m256i divisors = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i remainders = _mm256_set1_epi64x(n);\n        remainders = _mm256_rem_epu64(remainders, divisors);\n\n        // Check which divisors are valid (remainder == 0)\n        __m256i mask = _mm256_cmpeq_epi64(remainders, _mm256_setzero_si256());\n        int valid = _mm256_movemask_epi8(mask);\n\n        // Process each candidate in the vector\n        for (int j = 0; j < 4; j++) {\n            if (valid & (1 << (j * 8))) {\n                uint64_t candidate = i + j;\n                if (candidate > sqrt_n) {\n                    continue;\n                }\n                if (candidate > max_divisor) {\n                    max_divisor = candidate;\n                }\n                uint64_t pair = n / candidate;\n                if (pair > max_divisor && pair < n) {\n                    max_divisor = pair;\n                }\n            }\n        }\n    }\n\n    return max_divisor == 1 ? 1 : (max_divisor == n ? 1 : max_divisor);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 4 candidates at a time using AVX2\n    for (uint64_t i = 2; i <= sqrt_n; i += 4) {\n        __m256i divisors = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i remainders = _mm256_set1_epi64x(n);\n        __m256i quotients = _mm256_setzero_si256();\n\n        // Calculate n % divisor for each candidate\n        for (int j = 0; j < 4; ++j) {\n            __m256i div = _mm256_srli_epi64(divisors, j * 64);\n            __m256i rem = _mm256_srli_epi64(remainders, j * 64);\n            quotients = _mm256_or_si256(quotients, _mm256_sllv_epi64(_mm256_div_epu64(rem, div), _mm256_set1_epi64x(j * 64)));\n        }\n\n        __m256i mod_results = _mm256_sub_epi64(remainders, _mm256_mul_epu32(quotients, divisors));\n\n        // Check which divisors evenly divide n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpeq_epi64(mod_results, zero);\n\n        // Extract results\n        uint64_t results[4];\n        _mm256_storeu_si256((__m256i*)results, mask);\n\n        for (int j = 0; j < 4; ++j) {\n            if (results[j] != 0 && (i + j) <= sqrt_n) {\n                uint64_t candidate = i + j;\n                if (n % candidate == 0) {\n                    max_divisor = candidate;\n                    uint64_t pair = n / candidate;\n                    if (pair != candidate && pair > max_divisor) {\n                        max_divisor = pair;\n                    }\n                }\n            }\n        }\n    }\n\n    // Check remaining candidates not processed by SIMD\n    for (uint64_t i = (sqrt_n / 4) * 4 + 1; i <= sqrt_n; ++i) {\n        if (n % i == 0) {\n            max_divisor = i;\n            uint64_t pair = n / i;\n            if (pair != i && pair > max_divisor) {\n                max_divisor = pair;\n            }\n        }\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle even numbers using SIMD\n    __m256i zeros = _mm256_setzero_si256();\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Check for odd divisors up to sqrt(n)\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i divisors = _mm256_set_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i zeros = _mm256_setzero_si256();\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    int i = 3;\n    while (i * i <= n) {\n        __m256i current = _mm256_set1_epi32(i);\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_set1_epi32(n % i), zeros);\n        int mask_bits = _mm256_movemask_epi8(mask);\n\n        if (mask_bits) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i += 2;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle even numbers using SIMD\n    __m256i v_n = _mm256_set1_epi32(n);\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Check for odd divisors up to sqrt(n)\n    for (int i = 3; i * i <= n; i += 2) {\n        __m256i v_i = _mm256_set1_epi32(i);\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    // Handle even numbers using SIMD\n    __m256i zeros = _mm256_setzero_si256();\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Check for odd divisors up to sqrt(n)\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle even numbers using SIMD\n    __m256i zeros = _mm256_setzero_si256();\n    __m256i twos = _mm256_set1_epi32(2);\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Check for odd divisors up to sqrt(n)\n    int max_divisor = sqrt(n);\n    int i = 3;\n    while (i <= max_divisor) {\n        // Process 8 divisors at a time using AVX2\n        __m256i divisors = _mm256_set_epi32(i+14, i+12, i+10, i+8, i+6, i+4, i+2, i);\n        __m256i n_vec = _mm256_set1_epi32(n);\n        __m256i remainders = _mm256_rem_epi32(n_vec, divisors);\n\n        // Check which divisors are factors\n        __m256i mask = _mm256_cmpeq_epi32(remainders, zeros);\n        int mask_bits = _mm256_movemask_epi8(mask);\n\n        if (mask_bits != 0) {\n            // Process each potential factor\n            for (int j = 0; j < 8; ++j) {\n                if (mask_bits & (1 << (j*4))) {\n                    int divisor = i + j*2;\n                    while (n % divisor == 0) {\n                        factors.push_back(divisor);\n                        n /= divisor;\n                    }\n                }\n            }\n            max_divisor = sqrt(n);\n        }\n        i += 16; // Skip ahead by 16 (since we checked 8 odd numbers)\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    int i = 0;\n    const int simd_size = 8; // AVX can process 8 floats at a time\n    const int n = l.size();\n    \n    for (; i <= n - simd_size; i += simd_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        \n        int mask_bits = _mm256_movemask_ps(mask);\n        if (mask_bits == 0) continue;\n        \n        for (int j = 0; j < simd_size; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    int i = 0;\n    const int simd_width = 8; // AVX can process 8 floats at a time\n    const int n = l.size();\n    \n    for (; i <= n - simd_width; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        int mask_bits = _mm256_movemask_ps(mask);\n        if (mask_bits == 0) continue;\n        \n        for (int j = 0; j < simd_width; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    int i = 0;\n    const int simd_size = 8; // AVX can process 8 floats at once\n    const int n = l.size();\n    \n    for (; i <= n - simd_size; i += simd_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        int mask_bits = _mm256_movemask_ps(mask);\n        if (mask_bits == 0) continue;\n        \n        for (int j = 0; j < simd_size; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    int i = 0;\n    const int simd_size = 8; // AVX can process 8 floats at a time\n    const int n = l.size();\n    \n    for (; i <= n - simd_size; i += simd_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        int mask_bits = _mm256_movemask_ps(mask);\n        \n        if (mask_bits == 0) continue;\n        \n        for (int j = 0; j < simd_size; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    int i = 0;\n    const int simd_size = 8; // AVX can process 8 floats at a time\n    \n    for (; i + simd_size <= l.size(); i += simd_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        int mask_bits = _mm256_movemask_ps(mask);\n        if (mask_bits == 0) continue;\n        \n        for (int j = 0; j < simd_size; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisibility up to sqrt(n) using SIMD\n    int64_t max_divisor = (int64_t)sqrt(n) + 1;\n    __m256i divisors = _mm256_setr_epi64x(5, 7, 11, 13);\n    __m256i increments = _mm256_set1_epi64x(6);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        // Check current divisors\n        __m256i remainders = _mm256_rem_epi64(n_vec, divisors);\n        __m256i cmp = _mm256_cmpeq_epi64(remainders, _mm256_setzero_si256());\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        // Update divisors for next iteration\n        divisors = _mm256_add_epi64(divisors, increments);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m256i divisors = _mm256_setr_epi64x(5, 7, 11, 13);\n    __m256i increments = _mm256_set1_epi64x(6);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    \n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        // Check current divisors\n        __m256i remainders = _mm256_rem_epi64(n_vec, divisors);\n        __m256i mask = _mm256_cmpeq_epi64(remainders, _mm256_setzero_si256());\n        if (!_mm256_testz_si256(mask, mask)) {\n            return false;\n        }\n        \n        // Update divisors for next iteration\n        divisors = _mm256_add_epi64(divisors, increments);\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m256i divisors = _mm256_setr_epi64x(5, 7, 11, 13);\n    __m256i increments = _mm256_set1_epi64x(6);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    \n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        __m256i current_divisors = _mm256_add_epi64(divisors, _mm256_set1_epi64x(i - 5));\n        \n        // Check if any divisor divides n\n        __m256i remainders = _mm256_rem_epu64(n_vec, current_divisors);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(remainders, zero);\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(cmp));\n        \n        if (mask != 0) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m256i divisors = _mm256_setr_epi64x(5, 7, 11, 13);\n    __m256i increments = _mm256_set1_epi64x(6);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    \n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        __m256i current_divisors = _mm256_add_epi64(divisors, _mm256_set1_epi64x(i - 5));\n        __m256i remainders = _mm256_rem_epu64(n_vec, current_divisors);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(remainders, zero);\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(cmp));\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisibility up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    int64_t i = 5;\n    int64_t step = 8; // Process 8 candidates at a time (AVX2 can handle 4x64-bit ints)\n\n    __m256i divisors = _mm256_setr_epi64x(i, i+2, i+4, i+6);\n    __m256i steps = _mm256_set1_epi64x(step);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i <= max_divisor; i += step) {\n        // Check divisors i, i+2, i+4, i+6\n        __m256i remainders = _mm256_sub_epi64(n_vec, _mm256_mul_epu32(_mm256_div_epu32(n_vec, divisors), divisors));\n        __m256i mask = _mm256_cmpeq_epi64(remainders, zero);\n        if (!_mm256_testz_si256(mask, mask)) {\n            return false;\n        }\n        divisors = _mm256_add_epi64(divisors, steps);\n    }\n\n    // Check remaining divisors\n    while (i <= max_divisor) {\n        if (n % i == 0 || n % (i + 2) == 0) {\n            return false;\n        }\n        i += 6;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letter = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    constexpr size_t vec_size = 32;\n    size_t vec_end = len / vec_size * vec_size;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_a = _mm256_set1_epi8('a');\n    __m256i upper_z = _mm256_set1_epi8('z');\n    __m256i lower_a = _mm256_set1_epi8('A');\n    __m256i lower_z = _mm256_set1_epi8('Z');\n\n    for (i = 0; i < vec_end; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n\n        // Check for lowercase letters\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(upper_a, _mm256_set1_epi8(1))),\n            _mm256_cmpgt_epi8(_mm256_add_epi8(upper_z, _mm256_set1_epi8(1)), chunk)\n        );\n\n        // Check for uppercase letters\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(lower_a, _mm256_set1_epi8(1))),\n            _mm256_cmpgt_epi8(_mm256_add_epi8(lower_z, _mm256_set1_epi8(1)), chunk)\n        );\n\n        // Toggle case\n        __m256i toggled = _mm256_xor_si256(\n            chunk,\n            _mm256_or_si256(\n                _mm256_and_si256(is_lower, lower_mask),\n                _mm256_and_si256(is_upper, lower_mask)\n            )\n        );\n\n        _mm256_storeu_si256((__m256i*)(result.data() + i), toggled);\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (isupper(c)) {\n            result[i] = tolower(c);\n        } else if (islower(c)) {\n            result[i] = toupper(c);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letter = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in AVX2 chunks\n    constexpr size_t vec_size = 32; // AVX2 register size in bytes\n    size_t vec_end = len - (len % vec_size);\n\n    __m256i lower_a = _mm256_set1_epi8('a');\n    __m256i lower_z = _mm256_set1_epi8('z');\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_z = _mm256_set1_epi8('Z');\n    __m256i flip_mask = _mm256_set1_epi8(0x20);\n\n    for (i = 0; i < vec_end; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i const*)(s.data() + i));\n\n        // Check for lowercase letters\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk)\n        );\n\n        // Check for uppercase letters\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk)\n        );\n\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        __m256i to_flip = _mm256_and_si256(is_letter, flip_mask);\n        __m256i flipped = _mm256_xor_si256(chunk, to_flip);\n\n        _mm256_storeu_si256((__m256i*)(result.data() + i), flipped);\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (isalpha(c)) {\n            result[i] = islower(c) ? toupper(c) : tolower(c);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; i++) {\n        if (isalpha(s[i])) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    constexpr size_t vec_size = 32;\n    size_t vec_end = len - (len % vec_size);\n    i = 0;\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        \n        // Create masks for uppercase and lowercase letters\n        __m256i upper_mask = _mm256_or_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk)\n        );\n        __m256i lower_mask = _mm256_or_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk)\n        );\n        __m256i letter_mask = _mm256_and_si256(upper_mask, lower_mask);\n        letter_mask = _mm256_xor_si256(letter_mask, _mm256_set1_epi8(-1));\n\n        // Toggle case for letters\n        __m256i toggled = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n        \n        // Blend original and toggled based on letter mask\n        __m256i res = _mm256_blendv_epi8(chunk, toggled, letter_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), res);\n    }\n\n    // Process remaining elements\n    for (; i < len; i++) {\n        if (isalpha(s[i])) {\n            result[i] = islower(s[i]) ? toupper(s[i]) : tolower(s[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letter = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; i++) {\n        if (isalpha(s[i])) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in AVX2 chunks\n    constexpr size_t vec_size = 32; // AVX2 register size in bytes\n    size_t vec_end = len - (len % vec_size);\n\n    for (i = 0; i < vec_end; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create masks for uppercase and lowercase letters\n        __m256i uppercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk)\n        );\n        __m256i lowercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk)\n        );\n        __m256i letter_mask = _mm256_or_si256(uppercase_mask, lowercase_mask);\n\n        // Toggle case: uppercase becomes lowercase and vice versa\n        __m256i toggled = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n        \n        // Blend the toggled letters with original characters\n        __m256i res = _mm256_blendv_epi8(chunk, toggled, letter_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n\n    // Process remaining elements\n    for (; i < len; i++) {\n        if (isalpha(s[i])) {\n            result[i] ^= 0x20;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letter = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes (AVX2 register size)\n    constexpr size_t vec_size = 32;\n    size_t vec_end = len - (len % vec_size);\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_a = _mm256_set1_epi8('a');\n    __m256i upper_z = _mm256_set1_epi8('z');\n    __m256i lower_a = _mm256_set1_epi8('A');\n    __m256i lower_z = _mm256_set1_epi8('Z');\n\n    for (i = 0; i < vec_end; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        \n        // Check for lowercase letters\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(upper_a, _mm256_set1_epi8(1))),\n            _mm256_cmpgt_epi8(_mm256_add_epi8(upper_z, _mm256_set1_epi8(1)), chunk)\n        );\n        \n        // Check for uppercase letters\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(lower_a, _mm256_set1_epi8(1))),\n            _mm256_cmpgt_epi8(_mm256_add_epi8(lower_z, _mm256_set1_epi8(1)), chunk)\n        );\n        \n        // Toggle case\n        __m256i toggled = _mm256_xor_si256(chunk, _mm256_or_si256(\n            _mm256_and_si256(is_lower, lower_mask),\n            _mm256_and_si256(is_upper, lower_mask)\n        ));\n        \n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), toggled);\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (isalpha(c)) {\n            result[i] = islower(c) ? toupper(c) : tolower(c);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_3;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_3.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_3.begin(), divisible_by_3.end());\n\n    // Put them back using SIMD for better performance\n    size_t simd_size = divisible_by_3.size() - divisible_by_3.size() % 8;\n    size_t i = 0;\n    for (; i < simd_size; i += 8) {\n        __m256i vals = _mm256_loadu_si256((__m256i*)&divisible_by_3[i]);\n        _mm256_storeu_si256((__m256i*)&l[i * 3], vals);\n    }\n\n    // Handle remaining elements\n    for (; i < divisible_by_3.size(); ++i) {\n        l[i * 3] = divisible_by_3[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_3;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_3.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_3.begin(), divisible_by_3.end());\n\n    // Put them back using SIMD\n    size_t vec_size = divisible_by_3.size();\n    size_t i = 0;\n    for (; i + 8 <= vec_size; i += 8) {\n        __m256i sorted_values = _mm256_loadu_si256((__m256i const*)&divisible_by_3[i]);\n        __m256i original_indices = _mm256_setr_epi32(i*3, (i+1)*3, (i+2)*3, (i+3)*3, \n                                                    (i+4)*3, (i+5)*3, (i+6)*3, (i+7)*3);\n        \n        // Scatter the sorted values back to their positions\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(original_indices, j);\n            if (idx < l.size()) {\n                l[idx] = _mm256_extract_epi32(sorted_values, j);\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < vec_size; ++i) {\n        int idx = i * 3;\n        if (idx < l.size()) {\n            l[idx] = divisible_by_3[i];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_3;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_3.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_3.begin(), divisible_by_3.end());\n\n    // Put them back in sorted order\n    for (size_t i = 0, j = 0; i < l.size(); i += 3, j++) {\n        l[i] = divisible_by_3[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Put them back using SIMD\n    size_t i = 0;\n    size_t j = 0;\n    for (; i + 8 <= l.size(); i += 9) {\n        // Load 9 elements (3 groups of 3)\n        __m256i vec0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&l[i + 8]);\n        \n        // Create mask for elements divisible by 3 (positions 0, 3, 6)\n        __m256i mask0 = _mm256_set_epi32(0, 0, 0, 0, 0, 1, 0, 1);\n        __m256i mask1 = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 1);\n        \n        // Blend with sorted values\n        __m256i sorted0 = _mm256_set_epi32(0, 0, 0, 0, 0, divisible_by_three[j+1], 0, divisible_by_three[j]);\n        __m256i sorted1 = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, divisible_by_three[j+2]);\n        \n        vec0 = _mm256_blendv_epi8(vec0, sorted0, mask0);\n        vec1 = _mm256_blendv_epi8(vec1, sorted1, mask1);\n        \n        // Store back\n        _mm256_storeu_si256((__m256i*)&l[i], vec0);\n        _mm256_storeu_si256((__m256i*)&l[i + 8], vec1);\n        \n        j += 3;\n    }\n\n    // Handle remaining elements\n    for (; i < l.size(); i += 3) {\n        if (j < divisible_by_three.size()) {\n            l[i] = divisible_by_three[j++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_3;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_3.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_3.begin(), divisible_by_3.end());\n\n    // Put them back into the original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 3, j++) {\n        if (j < divisible_by_3.size()) {\n            l[i] = divisible_by_3[j];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    float max_val = l[0];\n    int i = 0;\n    int size = l.size();\n    \n    __m256 max_vec = _mm256_set1_ps(max_val);\n    \n    for (; i <= size - 8; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n    \n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return std::numeric_limits<float>::quiet_NaN();\n    \n    float max_val = l[0];\n    int i = 0;\n    int size = l.size();\n    \n    __m256 max_vec = _mm256_set1_ps(max_val);\n    \n    for (; i <= size - 8; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n    \n    float temp_max[8];\n    _mm256_storeu_ps(temp_max, max_vec);\n    \n    for (int j = 0; j < 8; ++j) {\n        if (temp_max[j] > max_val) {\n            max_val = temp_max[j];\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    float max_val = l[0];\n    int i = 0;\n    int size = l.size();\n    \n    __m256 max_vec = _mm256_set1_ps(max_val);\n    \n    for (; i + 8 <= size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n    \n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n    \n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int size = l.size();\n    float max_val = l[0];\n    int i = 0;\n\n    if (size >= 8) {\n        __m256 max_vec = _mm256_loadu_ps(&l[0]);\n        for (i = 8; i <= size - 8; i += 8) {\n            __m256 current_vec = _mm256_loadu_ps(&l[i]);\n            max_vec = _mm256_max_ps(max_vec, current_vec);\n        }\n\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, max_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > max_val) {\n                max_val = temp[j];\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return 0.0f;\n    \n    float max_val = l[0];\n    int i = 0;\n    int size = l.size();\n    \n    __m256 max_vec = _mm256_set1_ps(max_val);\n    \n    for (; i <= size - 8; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n    \n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n    \n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    int i;\n    __m256i vcount = _mm256_setzero_si256();\n    __m256i vseven = _mm256_set1_epi32('7');\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (i = 0; i <= n - 8; i += 8) {\n        __m256i vnums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vdiv11 = _mm256_or_si256(\n            _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(vnums, _mm256_set1_epi32(11))),\n            _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(vnums, _mm256_set1_epi32(13)))\n        );\n        \n        __m256i vmask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), vnums);\n        vdiv11 = _mm256_and_si256(vdiv11, vmask);\n        \n        if (_mm256_testz_si256(vdiv11, vdiv11)) {\n            continue;\n        }\n        \n        for (int j = 0; j < 8; j++) {\n            int num = i + j;\n            if (num >= n) break;\n            \n            if ((num % 11 == 0 || num % 13 == 0)) {\n                int tmp = num;\n                while (tmp > 0) {\n                    if (tmp % 10 == 7) {\n                        count++;\n                    }\n                    tmp /= 10;\n                }\n            }\n        }\n    }\n    \n    for (; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int tmp = i;\n            while (tmp > 0) {\n                if (tmp % 10 == 7) {\n                    count++;\n                }\n                tmp /= 10;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v_sevens = _mm256_set1_epi32(7);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_eleven = _mm256_set1_epi32(11);\n    __m256i v_thirteen = _mm256_set1_epi32(13);\n    \n    for (int i = 0; i < n; i += 8) {\n        __m256i v_nums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i v_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), v_nums);\n        \n        // Check divisible by 11 or 13\n        __m256i v_rem11 = _mm256_sub_epi32(v_nums, _mm256_mullo_epi32(_mm256_div_epi32(v_nums, v_eleven), v_eleven));\n        __m256i v_rem13 = _mm256_sub_epi32(v_nums, _mm256_mullo_epi32(_mm256_div_epi32(v_nums, v_thirteen), v_thirteen));\n        __m256i v_div = _mm256_or_si256(_mm256_cmpeq_epi32(v_rem11, v_zero), _mm256_cmpeq_epi32(v_rem13, v_zero));\n        \n        // Count 7s in each number\n        __m256i v_count7 = _mm256_setzero_si256();\n        for (int j = 0; j < 8; ++j) {\n            int num = i + j;\n            if (num >= n) continue;\n            \n            int tmp = num;\n            int digit7 = 0;\n            while (tmp > 0) {\n                if (tmp % 10 == 7) digit7++;\n                tmp /= 10;\n            }\n            v_count7 = _mm256_insert_epi32(v_count7, digit7, j);\n        }\n        \n        // Combine conditions\n        __m256i v_res = _mm256_and_si256(_mm256_and_si256(v_mask, v_div), v_count7);\n        \n        // Sum results\n        int res[8];\n        _mm256_storeu_si256((__m256i*)res, v_res);\n        for (int j = 0; j < 8; ++j) {\n            if (i + j < n) count += res[j];\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    int i;\n    __m256i v_sevens = _mm256_set1_epi32(7);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_eleven = _mm256_set1_epi32(11);\n    __m256i v_thirteen = _mm256_set1_epi32(13);\n    \n    for (i = 0; i <= n - 8; i += 8) {\n        __m256i v_nums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i v_div11 = _mm256_sub_epi32(v_zero, _mm256_cmpeq_epi32(v_zero, _mm256_rem_epi32(v_nums, v_eleven)));\n        __m256i v_div13 = _mm256_sub_epi32(v_zero, _mm256_cmpeq_epi32(v_zero, _mm256_rem_epi32(v_nums, v_thirteen)));\n        __m256i v_div = _mm256_or_si256(v_div11, v_div13);\n        \n        __m256i v_temp = v_nums;\n        __m256i v_digit_count = _mm256_setzero_si256();\n        \n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_temp, v_zero))) {\n            __m256i v_digit = _mm256_rem_epi32(v_temp, _mm256_set1_epi32(10));\n            v_digit_count = _mm256_add_epi32(v_digit_count, \n                _mm256_and_si256(v_div, _mm256_cmpeq_epi32(v_digit, v_sevens)));\n            v_temp = _mm256_div_epi32(v_temp, _mm256_set1_epi32(10));\n        }\n        \n        int counts[8];\n        _mm256_storeu_si256((__m256i*)counts, v_digit_count);\n        for (int j = 0; j < 8; j++) {\n            count += counts[j];\n        }\n    }\n    \n    for (; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int num = i;\n            while (num > 0) {\n                if (num % 10 == 7) {\n                    count++;\n                }\n                num /= 10;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v_sevens = _mm256_set1_epi32(7);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_eleven = _mm256_set1_epi32(11);\n    __m256i v_thirteen = _mm256_set1_epi32(13);\n    \n    for (int i = 0; i < n; i += 8) {\n        __m256i v_nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i v_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), v_nums);\n        \n        // Check divisible by 11 or 13\n        __m256i v_rem11 = _mm256_sub_epi32(v_nums, _mm256_mullo_epi32(_mm256_div_epi32(v_nums, v_eleven), v_eleven));\n        __m256i v_rem13 = _mm256_sub_epi32(v_nums, _mm256_mullo_epi32(_mm256_div_epi32(v_nums, v_thirteen), v_thirteen));\n        __m256i v_div = _mm256_or_si256(_mm256_cmpeq_epi32(v_rem11, v_zero), _mm256_cmpeq_epi32(v_rem13, v_zero));\n        \n        // Count 7s in each number\n        __m256i v_temp = v_nums;\n        __m256i v_digit_count = _mm256_setzero_si256();\n        for (int j = 0; j < 10; j++) {\n            __m256i v_rem = _mm256_sub_epi32(v_temp, _mm256_mullo_epi32(_mm256_div_epi32(v_temp, _mm256_set1_epi32(10)), _mm256_set1_epi32(10)));\n            v_digit_count = _mm256_add_epi32(v_digit_count, _mm256_and_si256(_mm256_cmpeq_epi32(v_rem, v_sevens), _mm256_set1_epi32(1)));\n            v_temp = _mm256_div_epi32(v_temp, _mm256_set1_epi32(10));\n        }\n        \n        // Combine conditions\n        __m256i v_result = _mm256_and_si256(_mm256_and_si256(v_div, v_mask), v_digit_count);\n        \n        // Sum the results\n        int res[8];\n        _mm256_storeu_si256((__m256i*)res, v_result);\n        for (int k = 0; k < 8; k++) {\n            count += res[k];\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    int i;\n    __m256i vcount = _mm256_setzero_si256();\n    __m256i v7 = _mm256_set1_epi32('7');\n    __m256i v11 = _mm256_set1_epi32(11);\n    __m256i v13 = _mm256_set1_epi32(13);\n    __m256i vzero = _mm256_setzero_si256();\n\n    for (i = 0; i <= n - 8; i += 8) {\n        __m256i vi = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i vrem11 = _mm256_rem_epi32(vi, v11);\n        __m256i vrem13 = _mm256_rem_epi32(vi, v13);\n        __m256i vdiv = _mm256_or_si256(_mm256_cmpeq_epi32(vrem11, vzero), \n                                      _mm256_cmpeq_epi32(vrem13, vzero));\n        \n        // Convert numbers to digits and count '7's\n        __m256i vnum = vi;\n        __m256i vdigits = _mm256_setzero_si256();\n        for (int j = 0; j < 5; j++) {\n            __m256i vdigit = _mm256_rem_epi32(vnum, _mm256_set1_epi32(10));\n            vdigits = _mm256_add_epi32(vdigits, _mm256_cmpeq_epi32(vdigit, v7));\n            vnum = _mm256_div_epi32(vnum, _mm256_set1_epi32(10));\n        }\n        \n        // Mask with divisible condition\n        vdigits = _mm256_and_si256(vdigits, vdiv);\n        count += _mm256_extract_epi32(vdigits, 0) >> 31;\n        count += _mm256_extract_epi32(vdigits, 1) >> 31;\n        count += _mm256_extract_epi32(vdigits, 2) >> 31;\n        count += _mm256_extract_epi32(vdigits, 3) >> 31;\n        count += _mm256_extract_epi32(vdigits, 4) >> 31;\n        count += _mm256_extract_epi32(vdigits, 5) >> 31;\n        count += _mm256_extract_epi32(vdigits, 6) >> 31;\n        count += _mm256_extract_epi32(vdigits, 7) >> 31;\n    }\n\n    // Handle remaining elements\n    for (; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int num = i;\n            while (num > 0) {\n                if (num % 10 == 7) {\n                    count++;\n                }\n                num /= 10;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.empty()) return l;\n\n    // Separate even and odd elements\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    // Sort even elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Merge back sorted even elements with original odd elements\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = even_elements[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.empty()) return l;\n\n    // Separate even and odd elements\n    std::vector<float> evens, odds;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 2 == 0) evens.push_back(l[i]);\n        else odds.push_back(l[i]);\n    }\n\n    // Sort the even elements\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back with AVX\n    size_t size = l.size();\n    std::vector<float> result(size);\n    size_t even_idx = 0, odd_idx = 0;\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 even_vec = _mm256_set_ps(\n            (i + 7 < size && (i + 7) % 2 == 0) ? evens[even_idx++] : 0.0f,\n            (i + 6 < size && (i + 6) % 2 == 0) ? evens[even_idx++] : 0.0f,\n            (i + 5 < size && (i + 5) % 2 == 0) ? evens[even_idx++] : 0.0f,\n            (i + 4 < size && (i + 4) % 2 == 0) ? evens[even_idx++] : 0.0f,\n            (i + 3 < size && (i + 3) % 2 == 0) ? evens[even_idx++] : 0.0f),\n            (i + 2 < size && (i + 2) % 2 == 0) ? evens[even_idx++] : 0.0f,\n            (i + 1 < size && (i + 1) % 2 == 0) ? evens[even_idx++] : 0.0f),\n            (i < size && i % 2 == 0) ? evens[even_idx++] : 0.0f\n        );\n\n        __m256 odd_vec = _mm256_set_ps(\n            (i + 7 < size && (i + 7) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 6 < size && (i + 6) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 5 < size && (i + 5) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 4 < size && (i + 4) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 3 < size && (i + 3) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 2 < size && (i + 2) % 2 != 0) ? odds[odd_idx++] : 0.0f,\n            (i + 1 < size && (i + 1) % 2 != 0) ? odds[odd_idx++] : 0.0f),\n            (i < size && i % 2 != 0) ? odds[odd_idx++] : 0.0f\n        );\n\n        __m256 mask = _mm256_set_ps(0.0f, -1.0f, 0.0f, -1.0f, 0.0f, -1.0f, 0.0f, -1.0f);\n        __m256 res = _mm256_blendv_ps(even_vec, odd_vec, mask);\n\n        _mm256_storeu_ps(&result[i], res);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    size_t even_idx = 0;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        l[i] = evens[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    size_t even_idx = 0;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        l[i] = evens[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Separate even and odd elements\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even elements\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back sorted evens with original odds\n    size_t even_idx = 0;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        l[i] = evens[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n    \n    std::sort(l.begin(), l.end());\n    size_t n = l.size();\n    \n    for (size_t i = 0; i < n - 2; ++i) {\n        int target = -l[i];\n        size_t left = i + 1;\n        size_t right = n - 1;\n        \n        while (left < right) {\n            int sum = l[left] + l[right];\n            if (sum == target) {\n                return true;\n            } else if (sum < target) {\n                left++;\n            } else {\n                right--;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n    \n    std::sort(l.begin(), l.end());\n    size_t n = l.size();\n    \n    for (size_t i = 0; i < n - 2; ++i) {\n        int a = l[i];\n        size_t left = i + 1;\n        size_t right = n - 1;\n        \n        while (left < right) {\n            int b = l[left];\n            int c = l[right];\n            int sum = a + b + c;\n            \n            if (sum == 0) {\n                return true;\n            } else if (sum < 0) {\n                left++;\n            } else {\n                right--;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n    \n    std::sort(l.begin(), l.end());\n    size_t n = l.size();\n    \n    for (size_t i = 0; i < n - 2; ++i) {\n        int a = l[i];\n        size_t left = i + 1;\n        size_t right = n - 1;\n        \n        while (left < right) {\n            int sum = a + l[left] + l[right];\n            if (sum == 0) {\n                return true;\n            } else if (sum < 0) {\n                left++;\n            } else {\n                right--;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n    \n    std::sort(l.begin(), l.end());\n    size_t n = l.size();\n    \n    for (size_t i = 0; i < n - 2; ++i) {\n        int target = -l[i];\n        size_t left = i + 1;\n        size_t right = n - 1;\n        \n        while (left < right) {\n            int sum = l[left] + l[right];\n            if (sum == target) {\n                return true;\n            } else if (sum < target) {\n                left++;\n            } else {\n                right--;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n    \n    std::sort(l.begin(), l.end());\n    size_t n = l.size();\n    \n    for (size_t i = 0; i < n - 2; ++i) {\n        size_t left = i + 1;\n        size_t right = n - 1;\n        \n        while (left < right) {\n            int sum = l[i] + l[left] + l[right];\n            if (sum == 0) {\n                return true;\n            } else if (sum < 0) {\n                left++;\n            } else {\n                right--;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    size_t size = l.size();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t vec_size = l.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (; i < vec_size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t vec_size = l.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (; i < vec_size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t vec_size = l.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (; i < vec_size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t vec_size = l.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (; i < vec_size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        size_t j = i + 1;\n        \n        for (; j + 7 < l.size(); j += 8) {\n            __m256i elements = _mm256_loadu_si256((__m256i const*)&l[j]);\n            __m256i sum = _mm256_add_epi32(current, elements);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 7 < l.size(); j += 8) {\n            __m256i next = _mm256_loadu_si256((__m256i const*)&l[j]);\n            __m256i sum = _mm256_add_epi32(current, next);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 7 < l.size(); j += 8) {\n            __m256i next = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i sum = _mm256_add_epi32(current, next);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        size_t j = i + 1;\n        \n        for (; j + 7 < l.size(); j += 8) {\n            __m256i elements = _mm256_loadu_si256((__m256i const*)&l[j]);\n            __m256i sum = _mm256_add_epi32(current, elements);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m256i current = _mm256_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 7 < l.size(); j += 8) {\n            __m256i elements = _mm256_loadu_si256((__m256i const*)&l[j]);\n            __m256i sum = _mm256_add_epi32(current, elements);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    // Process 32 bytes at a time using AVX2\n    while (i + 32 <= j) {\n        __m256i left = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i right = _mm256_loadu_si256((__m256i*)(str + j - 31));\n        \n        // Reverse the right vector\n        right = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n        \n        i += 32;\n        j -= 32;\n    }\n    \n    // Handle remaining characters\n    while (i < j) {\n        if (str[i] != str[j]) return false;\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 32 <= j) {\n        __m256i front = _mm256_loadu_si256((__m256i const*)(str + i));\n        __m256i back = _mm256_loadu_si256((__m256i const*)(str + j - 31));\n        \n        __m256i rev_back = _mm256_permute4x64_epi64(back, 0x1B);\n        rev_back = _mm256_shuffle_epi8(rev_back, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        \n        __m256i cmp = _mm256_cmpeq_epi8(front, rev_back);\n        int mask = _mm256_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFFFFFF) return false;\n        \n        i += 32;\n        j -= 32;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) return false;\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    // Process 32 bytes at a time using AVX2\n    while (i + 32 <= j) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(str + i));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(str + j - 31));\n        \n        // Reverse the right vector\n        right = _mm256_permute2x128_si256(right, right, 0x01);\n        right = _mm256_shuffle_epi8(right, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n        \n        i += 32;\n        j -= 32;\n    }\n    \n    // Process remaining characters\n    while (i < j) {\n        if (str[i] != str[j]) return false;\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    // Process 32 bytes at a time using AVX2\n    while (i + 32 <= j) {\n        __m256i front = _mm256_loadu_si256((__m256i const*)(str + i));\n        __m256i back = _mm256_loadu_si256((__m256i const*)(str + j - 31));\n        \n        // Reverse the back vector\n        back = _mm256_permute2x128_si256(back, back, 0x01);\n        back = _mm256_shuffle_epi8(back, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        \n        __m256i cmp = _mm256_cmpeq_epi8(front, back);\n        int mask = _mm256_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n        \n        i += 32;\n        j -= 32;\n    }\n    \n    // Handle remaining characters\n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    // Process 32 bytes at a time using AVX2\n    for (; i + 31 < j; i += 32, j -= 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i right = _mm256_loadu_si256((__m256i*)(str + j - 31));\n        \n        // Reverse the right vector\n        right = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n    }\n    \n    // Process remaining characters\n    for (; i < j; ++i, --j) {\n        if (str[i] != str[j]) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    \n    const __m256i a_vec = _mm256_set1_epi8('a');\n    const __m256i five_vec = _mm256_set1_epi8(5);\n    const __m256i twenty_six_vec = _mm256_set1_epi8(26);\n    const __m256i twenty_one_vec = _mm256_set1_epi8(21); // 26-5\n    \n    size_t i = 0;\n    for (; i + 32 <= s.size(); i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Subtract 'a' and 5, then add 26 to avoid negative values\n        __m256i shifted = _mm256_sub_epi8(chars, a_vec);\n        shifted = _mm256_sub_epi8(shifted, five_vec);\n        shifted = _mm256_add_epi8(shifted, twenty_one_vec);\n        \n        // Modulo 26\n        shifted = _mm256_and_si256(shifted, _mm256_set1_epi8(0x1F)); // Fast mod 32\n        __m256i mask = _mm256_cmpgt_epi8(shifted, _mm256_set1_epi8(25));\n        shifted = _mm256_sub_epi8(shifted, _mm256_and_si256(mask, twenty_six_vec));\n        \n        // Add 'a' back\n        shifted = _mm256_add_epi8(shifted, a_vec);\n        \n        _mm256_storeu_si256((__m256i*)&out[i], shifted);\n    }\n    \n    // Process remaining characters\n    for (; i < s.size(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    \n    size_t i = 0;\n    const size_t vec_size = 32;\n    const size_t len = s.size();\n    \n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Subtract 5 (shift back)\n        __m256i shifted = _mm256_sub_epi8(chars, _mm256_set1_epi8(5));\n        \n        // Handle wrap-around for characters below 'a'\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a' - 1), shifted);\n        __m256i wrapped = _mm256_add_epi8(shifted, _mm256_set1_epi8(26));\n        shifted = _mm256_blendv_epi8(shifted, wrapped, mask);\n        \n        _mm256_storeu_si256((__m256i*)&out[i], shifted);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    \n    size_t i = 0;\n    const size_t vec_size = 32;\n    const size_t len = s.size();\n    \n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Subtract 5 from each character\n        __m256i shifted = _mm256_sub_epi8(chars, _mm256_set1_epi8(5));\n        \n        // Handle wrap-around for characters below 'a'\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a' - 1), shifted);\n        __m256i wrapped = _mm256_add_epi8(shifted, _mm256_set1_epi8(26));\n        shifted = _mm256_blendv_epi8(shifted, wrapped, mask);\n        \n        _mm256_storeu_si256((__m256i*)&out[i], shifted);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    \n    size_t i = 0;\n    const size_t vec_size = 32;\n    const size_t len = s.size();\n    \n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Subtract 5, handling wrap-around for 'a'-'z'\n        __m256i shifted = _mm256_sub_epi8(chunk, _mm256_set1_epi8(5));\n        \n        // Compare with 'a'-1 to find underflows\n        __m256i underflow_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a' - 1), shifted);\n        \n        // Add 26 to underflowed characters\n        __m256i wrapped = _mm256_add_epi8(shifted, _mm256_set1_epi8(26));\n        \n        // Blend based on underflow mask\n        __m256i result = _mm256_blendv_epi8(shifted, wrapped, underflow_mask);\n        \n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n    \n    // Handle remaining elements\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    \n    const __m256i a_vec = _mm256_set1_epi8('a');\n    const __m256i mod_vec = _mm256_set1_epi8(26);\n    const __m256i shift_vec = _mm256_set1_epi8(21); // 26 - 5 = 21\n    \n    size_t i = 0;\n    for (; i + 32 <= s.size(); i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Subtract 'a' and add 21 (equivalent to subtracting 5 mod 26)\n        __m256i shifted = _mm256_sub_epi8(data, a_vec);\n        shifted = _mm256_add_epi8(shifted, shift_vec);\n        \n        // Mod 26\n        shifted = _mm256_and_si256(shifted, _mm256_set1_epi8(0x1F)); // Fast mod 32\n        __m256i mask = _mm256_cmpgt_epi8(shifted, _mm256_set1_epi8(25));\n        shifted = _mm256_sub_epi8(shifted, _mm256_and_si256(mask, mod_vec));\n        \n        // Add 'a' back\n        shifted = _mm256_add_epi8(shifted, a_vec);\n        \n        _mm256_storeu_si256((__m256i*)&out[i], shifted);\n    }\n    \n    // Process remaining elements\n    for (; i < s.size(); ++i) {\n        int w = ((int)s[i] - 'a' + 21) % 26 + 'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(-1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, data);\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(cmp, all_ones)) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(-1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i t_vec = _mm256_set1_epi32(t);\n    __m256i max_vec = _mm256_set1_epi32(0x80000000); // Initialize to smallest possible int\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(data, t_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    int balance = 0;\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i cmp_open = _mm256_cmpeq_epi8(chunk, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(chunk, close);\n        int mask_open = _mm256_movemask_epi8(cmp_open);\n        int mask_close = _mm256_movemask_epi8(cmp_close);\n\n        balance += _mm_popcnt_u32(mask_open);\n        balance -= _mm_popcnt_u32(mask_close);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n    \n    const int simd_width = 32; // AVX2 can process 32 bytes at a time\n    int i = 0;\n    int balance = 0;\n    \n    for (; i + simd_width <= len; i += simd_width) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        \n        __m256i gt_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n        __m256i lt_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n\n        int gt_count = _mm_popcnt_u32(_mm256_movemask_epi8(gt_mask));\n        int lt_count = _mm_popcnt_u32(_mm256_movemask_epi8(lt_mask));\n\n        balance += lt_count - gt_count;\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i min = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i cmp_open = _mm256_cmpeq_epi8(chunk, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(chunk, close);\n        __m256i diff = _mm256_sub_epi8(\n            _mm256_sign_epi8(_mm256_set1_epi8(1), cmp_open),\n            _mm256_sign_epi8(_mm256_set1_epi8(1), cmp_close)\n        );\n        sum = _mm256_add_epi8(sum, diff);\n        min = _mm256_min_epi8(min, sum);\n    }\n\n    // Horizontal sum and min\n    __m128i sum128 = _mm_add_epi8(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    __m128i min128 = _mm_min_epi8(_mm256_extracti128_si256(min, 0), _mm256_extracti128_si256(min, 1));\n    sum128 = _mm_add_epi8(sum128, _mm_srli_si128(sum128, 8));\n    sum128 = _mm_add_epi8(sum128, _mm_srli_si128(sum128, 4));\n    sum128 = _mm_add_epi8(sum128, _mm_srli_si128(sum128, 2));\n    sum128 = _mm_add_epi8(sum128, _mm_srli_si128(sum128, 1));\n    min128 = _mm_min_epi8(min128, _mm_srli_si128(min128, 8));\n    min128 = _mm_min_epi8(min128, _mm_srli_si128(min128, 4));\n    min128 = _mm_min_epi8(min128, _mm_srli_si128(min128, 2));\n    min128 = _mm_min_epi8(min128, _mm_srli_si128(min128, 1));\n\n    int total = _mm_extract_epi8(sum128, 0);\n    int min_val = _mm_extract_epi8(min128, 0);\n\n    // Process remaining elements\n    int balance = 0;\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else {\n            --balance;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return (total + balance) == 0 && min_val >= 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    int balance = 0;\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i cmp_open = _mm256_cmpeq_epi8(chunk, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(chunk, close);\n        int mask_open = _mm256_movemask_epi8(cmp_open);\n        int mask_close = _mm256_movemask_epi8(cmp_close);\n\n        balance += __builtin_popcount(mask_open);\n        balance -= __builtin_popcount(mask_close);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum = zero;\n    __m256i mask;\n\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i cmp_open = _mm256_cmpeq_epi8(chunk, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(chunk, close);\n        __m256i diff = _mm256_sub_epi8(\n            _mm256_sign_epi8(_mm256_set1_epi8(1), cmp_open),\n            _mm256_sign_epi8(_mm256_set1_epi8(1), cmp_close)\n        );\n        sum = _mm256_add_epi8(sum, diff);\n        mask = _mm256_cmpgt_epi8(zero, sum);\n        if (!_mm256_testz_si256(mask, mask)) {\n            return false;\n        }\n    }\n\n    int balance = 0;\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len == 0) return true;\n    if (len % 2 != 0) return false;\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    int balance = 0;\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i opens = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi8(chunk, close_mask);\n        opens = _mm256_and_si256(opens, one);\n        closes = _mm256_and_si256(closes, neg_one);\n        __m256i delta = _mm256_add_epi8(opens, closes);\n        __m256i sum = _mm256_sad_epu8(delta, zero);\n        balance += _mm256_extract_epi16(sum, 0) - _mm256_extract_epi16(sum, 4);\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        balance += (brackets[i] == '(') ? 1 : -1;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        \n        // Compare with '('\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        int open_count = _mm_popcnt_u32(_mm256_movemask_epi8(open_mask));\n        \n        // Compare with ')'\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n        int close_count = _mm_popcnt_u32(_mm256_movemask_epi8(close_mask));\n        \n        balance += open_count - close_count;\n        \n        // Early exit if balance goes negative\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    int balance = 0;\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_cmp = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i close_cmp = _mm256_cmpeq_epi8(chunk, close_mask);\n        \n        __m256i open_count = _mm256_sub_epi8(zero, open_cmp);\n        __m256i close_count = _mm256_sub_epi8(zero, close_cmp);\n        \n        int opens = _mm256_movemask_epi8(open_cmp);\n        int closes = _mm256_movemask_epi8(close_cmp);\n        \n        balance += __builtin_popcount(opens) - __builtin_popcount(closes);\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        balance += (brackets[i] == '(') - (brackets[i] == ')');\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n\n    int balance = 0;\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i opens = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi8(chunk, close_mask);\n        \n        opens = _mm256_sign_epi8(ones, opens);\n        closes = _mm256_sign_epi8(neg_ones, closes);\n        \n        __m256i delta = _mm256_add_epi8(opens, closes);\n        __m256i sum = _mm256_sad_epu8(delta, zero);\n        \n        int local_balance = _mm256_extract_epi16(sum, 0) + _mm256_extract_epi16(sum, 4);\n        balance += local_balance;\n        \n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        balance += (brackets[i] == '(') ? 1 : -1;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    constexpr size_t simd_width = 32;\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n\n    for (; i + simd_width <= len; i += simd_width) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i opens = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi8(chunk, close_mask);\n\n        int open_count = _mm_popcnt_u32(_mm256_movemask_epi8(opens));\n        int close_count = _mm_popcnt_u32(_mm256_movemask_epi8(closes));\n\n        balance += open_count - close_count;\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return {};\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX can process 8 floats at once\n    const size_t n = result.size();\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 derivative = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.empty()) {\n        return {};\n    }\n    \n    size_t n = xs.size() - 1;\n    std::vector<float> result(n);\n    \n    size_t i = 0;\n    const size_t vec_size = 8; // AVX can process 8 floats at once\n    \n    for (; i + vec_size <= n; i += vec_size) {\n        __m256 indices = _mm256_set_ps(i + 7.0f, i + 6.0f, i + 5.0f, i + 4.0f,\n                                      i + 3.0f, i + 2.0f, i + 1.0f, i + 0.0f);\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 derivative = _mm256_mul_ps(indices, coeffs);\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return {};\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    const size_t aligned_size = (xs.size() - 1) & ~7;\n\n    for (; i < aligned_size; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256 coeffs = _mm256_loadu_ps(&xs[i+1]);\n        __m256 derivative = _mm256_mul_ps(coeffs, _mm256_cvtepi32_ps(indices));\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n\n    for (; i < xs.size() - 1; ++i) {\n        result[i] = xs[i+1] * (i+1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return {};\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX can process 8 floats at a time\n    const size_t limit = (xs.size() - 1) / vec_size * vec_size;\n\n    for (; i < limit; i += vec_size) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 derivative = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n\n    for (; i < xs.size() - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.empty()) {\n        return {};\n    }\n\n    size_t n = xs.size() - 1;\n    std::vector<float> result(n);\n\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX can process 8 floats at once\n    __m256i indices = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 factors = _mm256_cvtepi32_ps(indices);\n        __m256 derivative = _mm256_mul_ps(coeffs, factors);\n        _mm256_storeu_ps(&result[i], derivative);\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(vec_size));\n    }\n\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n    \n    // Check if 'y' at the end is a vowel\n    if (tolower(str[len - 1]) == 'y') {\n        count++;\n    }\n\n    __m256i vowels = _mm256_set1_epi8(0);\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('a'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('e'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('i'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('o'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('u'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('A'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('E'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('I'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('O'));\n    vowels = _mm256_or_si256(vowels, _mm256_set1_epi8('U'));\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i eq = _mm256_cmpeq_epi8(chunk, vowels);\n        int mask = _mm256_movemask_epi8(eq);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i++) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    size_t i = 0;\n    __m256i vowels = _mm256_set1_epi8('a');\n    __m256i vowels_upper = _mm256_set1_epi8('A');\n    __m256i mask_a = _mm256_set1_epi8('a' ^ 'A');\n    __m256i mask_e = _mm256_set1_epi8('e' ^ 'E');\n    __m256i mask_i = _mm256_set1_epi8('i' ^ 'I');\n    __m256i mask_o = _mm256_set1_epi8('o' ^ 'O');\n    __m256i mask_u = _mm256_set1_epi8('u' ^ 'U');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n\n        // Check for lowercase vowels\n        __m256i eq_a = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i eq_e = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e'));\n        __m256i eq_i = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i'));\n        __m256i eq_o = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o'));\n        __m256i eq_u = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u'));\n\n        // Check for uppercase vowels by XORing with case mask\n        __m256i upper_chunk = _mm256_xor_si256(chunk, mask_a);\n        __m256i eq_A = _mm256_cmpeq_epi8(upper_chunk, vowels_upper);\n        upper_chunk = _mm256_xor_si256(chunk, mask_e);\n        __m256i eq_E = _mm256_cmpeq_epi8(upper_chunk, _mm256_set1_epi8('E'));\n        upper_chunk = _mm256_xor_si256(chunk, mask_i);\n        __m256i eq_I = _mm256_cmpeq_epi8(upper_chunk, _mm256_set1_epi8('I'));\n        upper_chunk = _mm256_xor_si256(chunk, mask_o);\n        __m256i eq_O = _mm256_cmpeq_epi8(upper_chunk, _mm256_set1_epi8('O'));\n        upper_chunk = _mm256_xor_si256(chunk, mask_u);\n        __m256i eq_U = _mm256_cmpeq_epi8(upper_chunk, _mm256_set1_epi8('U'));\n\n        // Combine all comparisons\n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(eq_a, eq_e),\n                _mm256_or_si256(eq_i, eq_o)\n            ),\n            _mm256_or_si256(\n                _mm256_or_si256(eq_u, eq_A),\n                _mm256_or_si256(eq_E, _mm256_or_si256(eq_I, _mm256_or_si256(eq_O, eq_U)))\n            )\n        );\n\n        // Count the number of set bits in the mask\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(vowel_mask));\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            ++count;\n        }\n    }\n\n    // Check for 'y' at the end\n    if (len > 0) {\n        char last = tolower(str[len - 1]);\n        if (last == 'y') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    size_t i = 0;\n    const size_t block_size = 32;\n    __m256i vowels = _mm256_set1_epi8('a');\n    __m256i vowels_upper = _mm256_set1_epi8('A');\n    __m256i mask_a = _mm256_set1_epi8('a' ^ 'A');\n    __m256i mask_e = _mm256_set1_epi8('e' ^ 'E');\n    __m256i mask_i = _mm256_set1_epi8('i' ^ 'I');\n    __m256i mask_o = _mm256_set1_epi8('o' ^ 'O');\n    __m256i mask_u = _mm256_set1_epi8('u' ^ 'U');\n\n    for (; i + block_size <= len; i += block_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n\n        // Check for 'a' or 'A'\n        __m256i eq_a = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, vowels),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_a), vowels_upper)\n        );\n\n        // Check for 'e' or 'E'\n        __m256i eq_e = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_e), _mm256_set1_epi8('E'))\n        );\n\n        // Check for 'i' or 'I'\n        __m256i eq_i = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_i), _mm256_set1_epi8('I'))\n        );\n\n        // Check for 'o' or 'O'\n        __m256i eq_o = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_o), _mm256_set1_epi8('O'))\n        );\n\n        // Check for 'u' or 'U'\n        __m256i eq_u = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_u), _mm256_set1_epi8('U'))\n        );\n\n        // Combine all vowel checks\n        __m256i any_vowel = _mm256_or_si256(\n            _mm256_or_si256(_mm256_or_si256(eq_a, eq_e), \n            _mm256_or_si256(_mm256_or_si256(eq_i, eq_o), eq_u)\n        );\n\n        // Count the number of set bits in the mask\n        int mask = _mm256_movemask_epi8(any_vowel);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    // Check for 'y' at the end\n    if (len > 0) {\n        char last = tolower(str[len - 1]);\n        if (last == 'y') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    int len = s.size();\n    int count = 0;\n    int i = 0;\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    const int chunk_size = 32;\n    __m256i vowel_mask = _mm256_set1_epi8(0);\n    __m256i a = _mm256_set1_epi8('a');\n    __m256i e = _mm256_set1_epi8('e');\n    __m256i i_char = _mm256_set1_epi8('i');\n    __m256i o = _mm256_set1_epi8('o');\n    __m256i u = _mm256_set1_epi8('u');\n    __m256i A = _mm256_set1_epi8('A');\n    __m256i E = _mm256_set1_epi8('E');\n    __m256i I = _mm256_set1_epi8('I');\n    __m256i O = _mm256_set1_epi8('O');\n    __m256i U = _mm256_set1_epi8('U');\n\n    for (; i + chunk_size <= len; i += chunk_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        __m256i eq_a = _mm256_cmpeq_epi8(data, a);\n        __m256i eq_e = _mm256_cmpeq_epi8(data, e);\n        __m256i eq_i = _mm256_cmpeq_epi8(data, i_char);\n        __m256i eq_o = _mm256_cmpeq_epi8(data, o);\n        __m256i eq_u = _mm256_cmpeq_epi8(data, u);\n        __m256i eq_A = _mm256_cmpeq_epi8(data, A);\n        __m256i eq_E = _mm256_cmpeq_epi8(data, E);\n        __m256i eq_I = _mm256_cmpeq_epi8(data, I);\n        __m256i eq_O = _mm256_cmpeq_epi8(data, O);\n        __m256i eq_U = _mm256_cmpeq_epi8(data, U);\n        \n        __m256i vowels = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(eq_a, eq_e),\n                _mm256_or_si256(eq_i, eq_o)\n            ),\n            _mm256_or_si256(\n                _mm256_or_si256(eq_u, eq_A),\n                _mm256_or_si256(eq_E, _mm256_or_si256(eq_I, _mm256_or_si256(eq_O, eq_U)))\n            )\n        );\n        \n        int mask = _mm256_movemask_epi8(vowels);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len - 1; ++i) {\n        char c = tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            ++count;\n        }\n    }\n\n    // Check last character for 'y' or 'Y'\n    if (len > 0) {\n        char last = tolower(s[len - 1]);\n        if (last == 'y') {\n            ++count;\n        } else if (last == 'a' || last == 'e' || last == 'i' || last == 'o' || last == 'u') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n\n    // Process the string in chunks of 32 bytes (AVX2)\n    size_t i = 0;\n    const size_t block_size = 32;\n    __m256i vowels = _mm256_set1_epi8('a');\n    __m256i vowels_upper = _mm256_set1_epi8('A');\n    __m256i mask_a = _mm256_set1_epi8('a' ^ 'A');\n    __m256i mask_e = _mm256_set1_epi8('e' ^ 'E');\n    __m256i mask_i = _mm256_set1_epi8('i' ^ 'I');\n    __m256i mask_o = _mm256_set1_epi8('o' ^ 'O');\n    __m256i mask_u = _mm256_set1_epi8('u' ^ 'U');\n\n    for (; i + block_size <= len; i += block_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Check for 'a' or 'A'\n        __m256i eq_a = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, vowels),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_a), vowels)\n        );\n        \n        // Check for 'e' or 'E'\n        __m256i eq_e = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_e), _mm256_set1_epi8('E'))\n        );\n        \n        // Check for 'i' or 'I'\n        __m256i eq_i = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_i), _mm256_set1_epi8('I'))\n        );\n        \n        // Check for 'o' or 'O'\n        __m256i eq_o = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_o), _mm256_set1_epi8('O'))\n        );\n        \n        // Check for 'u' or 'U'\n        __m256i eq_u = _mm256_or_si256(\n            _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u')),\n            _mm256_cmpeq_epi8(_mm256_xor_si256(chunk, mask_u), _mm256_set1_epi8('U'))\n        );\n        \n        // Combine all vowel checks\n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(eq_a, eq_e),\n            _mm256_or_si256(_mm256_or_si256(eq_i, eq_o), eq_u)\n        );\n        \n        // Count the number of set bits in the mask\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(vowel_mask));\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            ++count;\n        }\n    }\n\n    // Check for 'y' at the end\n    if (len > 0) {\n        char last = tolower(str[len - 1]);\n        if (last == 'y') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.length();\n    int sum = 0;\n\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_Z = _mm256_set1_epi8('Z');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        __m256i mask_ge_A = _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(upper_A, _mm256_set1_epi8(1)));\n        __m256i mask_le_Z = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_Z, _mm256_set1_epi8(1)), chunk);\n        __m256i mask = _mm256_and_si256(mask_ge_A, mask_le_Z);\n        \n        __m256i upper_chars = _mm256_and_si256(chunk, mask);\n        \n        __m256i sum_lo = _mm256_unpacklo_epi8(upper_chars, zero);\n        __m256i sum_hi = _mm256_unpackhi_epi8(upper_chars, zero);\n        sum_vec = _mm256_add_epi16(sum_vec, sum_lo);\n        sum_vec = _mm256_add_epi16(sum_vec, sum_hi);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi16(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 8));\n    sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 4));\n    sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 2));\n    sum += _mm_extract_epi16(sum128, 0);\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            sum += c;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_Z = _mm256_set1_epi8('Z');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        __m256i mask_ge_A = _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(upper_A, _mm256_set1_epi8(1)));\n        __m256i mask_le_Z = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_Z, _mm256_set1_epi8(1)), chunk);\n        __m256i mask = _mm256_and_si256(mask_ge_A, mask_le_Z);\n        \n        __m256i upper_chars = _mm256_and_si256(chunk, mask);\n        \n        __m256i sum_lo = _mm256_unpacklo_epi8(upper_chars, zero);\n        __m256i sum_hi = _mm256_unpackhi_epi8(upper_chars, zero);\n        __m256i sum_16 = _mm256_add_epi16(sum_lo, sum_hi);\n        \n        __m256i sum_32 = _mm256_add_epi32(\n            _mm256_unpacklo_epi16(sum_16, zero),\n            _mm256_unpackhi_epi16(sum_16, zero)\n        );\n        \n        sum_vec = _mm256_add_epi32(sum_vec, sum_32);\n    }\n\n    alignas(32) int sum_arr[8];\n    _mm256_store_si256((__m256i*)sum_arr, sum_vec);\n    sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n          sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.length();\n    int sum = 0;\n\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0x20);\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i upper_chars = _mm256_and_si256(chunk, upper_mask);\n        __m256i is_upper = _mm256_cmpeq_epi8(upper_chars, _mm256_setzero_si256());\n        __m256i masked = _mm256_and_si256(chunk, is_upper);\n        \n        __m256i sum_lo = _mm256_unpacklo_epi8(masked, _mm256_setzero_si256());\n        __m256i sum_hi = _mm256_unpackhi_epi8(masked, _mm256_setzero_si256());\n        sum_vec = _mm256_add_epi32(sum_vec, sum_lo);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_hi);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            sum += c;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.length();\n    int sum = 0;\n\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        __m256i upper_mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1));\n        __m256i lower_mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n        __m256i is_upper = _mm256_andnot_si256(lower_mask, upper_mask);\n        \n        __m256i masked = _mm256_and_si256(chunk, is_upper);\n        \n        __m256i sum_lo = _mm256_sad_epu8(masked, _mm256_setzero_si256());\n        __m256i sum_hi = _mm256_shuffle_epi32(sum_lo, _MM_SHUFFLE(3, 3, 1, 1));\n        __m256i sum32 = _mm256_add_epi32(sum_lo, sum_hi);\n        \n        sum_vec = _mm256_add_epi32(sum_vec, sum32);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            sum += c;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_Z = _mm256_set1_epi8('Z');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, _mm256_sub_epi8(upper_A, _mm256_set1_epi8(1))),\n            _mm256_cmpgt_epi8(upper_Z, chunk)\n        );\n        __m256i upper_chars = _mm256_and_si256(chunk, mask);\n        \n        __m256i sum_lo = _mm256_cvtepu8_epi16(_mm256_castsi256_si128(upper_chars));\n        __m256i sum_hi = _mm256_cvtepu8_epi16(_mm256_extracti128_si256(upper_chars, 1));\n        \n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_castsi256_si128(sum_lo)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sum_lo, 1))\n        ));\n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_castsi256_si128(sum_hi)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sum_hi, 1))\n        ));\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum128 = _mm_add_epi32(\n        _mm256_castsi256_si128(sum_vec),\n        _mm256_extracti128_si256(sum_vec, 1)\n    );\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 8));\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 4));\n    sum += _mm_extract_epi32(sum128, 0);\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t vec_size = arr.size();\n    const size_t vec_end = vec_size - (vec_size % 8);\n\n    __m256i min_even_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i min_index_vec = _mm256_set1_epi32(-1);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    for (; i < vec_end; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i even_mask = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        __m256i current_even = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, even_mask);\n        __m256i current_indices = _mm256_add_epi32(_mm256_set1_epi32(i), indices);\n\n        __m256i cmp = _mm256_cmpgt_epi32(min_even_vec, current_even);\n        min_even_vec = _mm256_blendv_epi8(min_even_vec, current_even, cmp);\n        min_index_vec = _mm256_blendv_epi8(min_index_vec, current_indices, cmp);\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    int min_even_arr[8], min_index_arr[8];\n    _mm256_storeu_si256((__m256i*)min_even_arr, min_even_vec);\n    _mm256_storeu_si256((__m256i*)min_index_arr, min_index_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (min_even_arr[j] < min_even) {\n            min_even = min_even_arr[j];\n            min_index = min_index_arr[j];\n        } else if (min_even_arr[j] == min_even && min_index_arr[j] < min_index && min_index != -1) {\n            min_index = min_index_arr[j];\n        }\n    }\n\n    for (; i < vec_size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            } else if (arr[i] == min_even && i < min_index) {\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t n = arr.size();\n    const size_t simd_width = 8; // AVX2 can process 8 ints at a time\n\n    // Process elements in SIMD chunks\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        int mask_bits = _mm256_movemask_epi8(mask);\n\n        if (mask_bits != 0) {\n            // There are even numbers in this chunk\n            alignas(32) int extracted[simd_width];\n            _mm256_store_si256((__m256i*)extracted, vec);\n\n            for (size_t j = 0; j < simd_width; ++j) {\n                if ((mask_bits >> (j * 4)) & 1) { // Check if the number is even\n                    int val = extracted[j];\n                    if (val < min_even) {\n                        min_even = val;\n                        min_index = i + j;\n                    } else if (val == min_even && (i + j) < min_index) {\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            } else if (arr[i] == min_even && i < min_index) {\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t n = arr.size();\n    const size_t simd_width = 8;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec, mask), _mm256_setzero_si256());\n        \n        // Extract even elements\n        __m256i even_values = _mm256_and_si256(vec, even_mask);\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        \n        // Find minimum even value in current chunk\n        __m256i min_even_vec = _mm256_set1_epi32(min_even);\n        __m256i cmp = _mm256_and_si256(_mm256_cmpgt_epi32(min_even_vec, even_values), even_mask);\n        int mask_bits = _mm256_movemask_epi8(cmp);\n        \n        if (mask_bits != 0) {\n            // There are smaller even values in this chunk\n            alignas(32) int values[8];\n            alignas(32) int idxs[8];\n            _mm256_store_si256((__m256i*)values, even_values);\n            _mm256_store_si256((__m256i*)idxs, indices);\n            \n            for (int j = 0; j < 8; ++j) {\n                if (values[j] < min_even && (values[j] & 1) == 0) {\n                    min_even = values[j];\n                    min_index = idxs[j];\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t vec_size = arr.size();\n    const size_t vec_end = vec_size - (vec_size % 8);\n\n    __m256i min_even_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i min_index_vec = _mm256_set1_epi32(-1);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (; i < vec_end; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i masked_data = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, mask);\n        __m256i masked_indices = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), indices, mask);\n\n        __m256i cmp = _mm256_cmpgt_epi32(min_even_vec, masked_data);\n        min_even_vec = _mm256_blendv_epi8(min_even_vec, masked_data, cmp);\n        min_index_vec = _mm256_blendv_epi8(min_index_vec, masked_indices, cmp);\n\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(8));\n    }\n\n    int min_even_arr[8], min_index_arr[8];\n    _mm256_storeu_si256((__m256i*)min_even_arr, min_even_vec);\n    _mm256_storeu_si256((__m256i*)min_index_arr, min_index_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (min_even_arr[j] < min_even) {\n            min_even = min_even_arr[j];\n            min_index = min_index_arr[j];\n        } else if (min_even_arr[j] == min_even && min_index_arr[j] < min_index) {\n            min_index = min_index_arr[j];\n        }\n    }\n\n    for (; i < vec_size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            } else if (arr[i] == min_even && i < min_index) {\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t n = arr.size();\n    const size_t simd_width = 8; // AVX2 can process 8 ints at a time\n\n    // Process the array in chunks of 8 elements using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i even_mask = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        \n        // Extract the even elements\n        __m256i evens = _mm256_and_si256(vec, even_mask);\n        __m256i indices = _mm256_add_epi32(_mm256_set1_epi32(i), _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        \n        // Find the minimum even value in this chunk\n        __m256i min_even_vec = _mm256_set1_epi32(min_even);\n        __m256i cmp = _mm256_cmpgt_epi32(min_even_vec, evens);\n        __m256i new_min_even = _mm256_blendv_epi8(min_even_vec, evens, cmp);\n        \n        // Update the min_even and min_index\n        int evens_arr[simd_width];\n        int indices_arr[simd_width];\n        _mm256_storeu_si256((__m256i*)evens_arr, evens);\n        _mm256_storeu_si256((__m256i*)indices_arr, indices);\n        \n        for (int j = 0; j < simd_width; ++j) {\n            if (evens_arr[j] < min_even && (evens_arr[j] & 1) == 0) {\n                min_even = evens_arr[j];\n                min_index = indices_arr[j];\n            } else if (evens_arr[j] == min_even && (evens_arr[j] & 1) == 0 && indices_arr[j] < min_index) {\n                min_index = indices_arr[j];\n            }\n        }\n    }\n\n    // Process the remaining elements\n    for (; i < n; ++i) {\n        if ((arr[i] & 1) == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            } else if (arr[i] == min_even && i < min_index) {\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process the map in chunks of 8 for AVX\n    auto it = freq.begin();\n    int size = freq.size();\n    int remainder = size % 8;\n    int chunks = size / 8;\n\n    for (int i = 0; i < chunks; ++i) {\n        __m256i keys = _mm256_setzero_si256();\n        __m256i vals = _mm256_setzero_si256();\n\n        // Load 8 keys and values\n        int j = 0;\n        for (; j < 8; ++j, ++it) {\n            keys = _mm256_setr_epi32(\n                j == 0 ? it->first : 0,\n                j == 1 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 2 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 3 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 4 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 5 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 6 ? (it != freq.end() ? (++it)->first : 0) : 0,\n                j == 7 ? (it != freq.end() ? (++it)->first : 0) : 0\n            );\n            vals = _mm256_setr_epi32(\n                j == 0 ? it->second : 0,\n                j == 1 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 2 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 3 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 4 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 5 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 6 ? (it != freq.end() ? it->second : 0) : 0,\n                j == 7 ? (it != freq.end() ? it->second : 0) : 0\n            );\n        }\n\n        // Compare keys and values\n        __m256i cmp = _mm256_cmpgt_epi32(vals, keys);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Check each element in the vector\n            int* key_arr = (int*)&keys;\n            int* val_arr = (int*)&vals;\n            for (int k = 0; k < 8; ++k) {\n                if (val_arr[k] >= key_arr[k] && key_arr[k] > max_val) {\n                    max_val = key_arr[k];\n                    max_key = key_arr[k];\n                }\n            }\n        }\n    }\n\n    // Process remainder\n    for (int i = 0; i < remainder; ++i, ++it) {\n        if (it->second >= it->first && it->first > max_val) {\n            max_val = it->first;\n            max_key = it->first;\n        }\n    }\n\n    return max_key;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process the map in chunks of 8 for AVX2\n    auto it = freq.begin();\n    int size = freq.size();\n    int remaining = size;\n\n    while (remaining >= 8) {\n        __m256i keys = _mm256_set_epi32(\n            (it + 7)->first, (it + 6)->first, (it + 5)->first, (it + 4)->first,\n            (it + 3)->first, (it + 2)->first, (it + 1)->first, it->first\n        );\n        __m256i values = _mm256_set_epi32(\n            (it + 7)->second, (it + 6)->second, (it + 5)->second, (it + 4)->second,\n            (it + 3)->second, (it + 2)->second, (it + 1)->second, it->second\n        );\n\n        __m256i cmp = _mm256_cmpgt_epi32(values, keys);\n        __m256i mask = _mm256_cmpgt_epi32(keys, _mm256_setzero_si256());\n        __m256i valid = _mm256_and_si256(cmp, mask);\n\n        int mask_result = _mm256_movemask_epi8(valid);\n        if (mask_result != 0) {\n            for (int i = 0; i < 8; ++i) {\n                int key = (it + i)->first;\n                int val = (it + i)->second;\n                if (key > 0 && val >= key && key > max_val) {\n                    max_val = key;\n                }\n            }\n        }\n\n        it += 8;\n        remaining -= 8;\n    }\n\n    // Process remaining elements\n    for (; it != freq.end(); ++it) {\n        int key = it->first;\n        int val = it->second;\n        if (key > 0 && val >= key && key > max_val) {\n            max_val = key;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process keys in chunks of 8 for AVX\n    auto keys = std::vector<int>();\n    for (const auto& pair : freq) {\n        keys.push_back(pair.first);\n    }\n\n    size_t i = 0;\n    for (; i + 8 <= keys.size(); i += 8) {\n        __m256i key_vec = _mm256_loadu_si256((__m256i*)&keys[i]);\n        __m256i freq_vec = _mm256_setzero_si256();\n\n        // Load frequencies\n        for (int j = 0; j < 8; j++) {\n            int key = keys[i + j];\n            int f = freq[key];\n            freq_vec = _mm256_insert_epi32(freq_vec, f, j);\n        }\n\n        // Compare key <= freq\n        __m256i cmp = _mm256_cmpgt_epi32(freq_vec, key_vec);\n        __m256i mask = _mm256_cmpeq_epi32(cmp, _mm256_setzero_si256());\n        __m256i filtered_keys = _mm256_andnot_si256(mask, key_vec);\n\n        // Find max in filtered keys\n        alignas(32) int filtered_arr[8];\n        _mm256_store_si256((__m256i*)filtered_arr, filtered_keys);\n\n        for (int j = 0; j < 8; j++) {\n            if (filtered_arr[j] > 0 && filtered_arr[j] > max_val) {\n                max_val = filtered_arr[j];\n                max_key = filtered_arr[j];\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < keys.size(); i++) {\n        int key = keys[i];\n        int f = freq[key];\n        if (f >= key && key > max_val) {\n            max_val = key;\n            max_key = key;\n        }\n    }\n\n    return max_key;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process the map in chunks of 8 for AVX\n    auto it = freq.begin();\n    int size = freq.size();\n    int remainder = size % 8;\n    int chunks = size / 8;\n\n    for (int i = 0; i < chunks; ++i) {\n        __m256i keys = _mm256_setzero_si256();\n        __m256i vals = _mm256_setzero_si256();\n        __m256i cmp_result = _mm256_setzero_si256();\n\n        // Load 8 keys and values\n        int j = 0;\n        int tmp_keys[8], tmp_vals[8];\n        for (; j < 8 && it != freq.end(); ++j, ++it) {\n            tmp_keys[j] = it->first;\n            tmp_vals[j] = it->second;\n        }\n\n        keys = _mm256_loadu_si256((__m256i*)tmp_keys);\n        vals = _mm256_loadu_si256((__m256i*)tmp_vals);\n\n        // Compare values >= keys\n        cmp_result = _mm256_cmpgt_epi32(vals, keys);\n        __m256i eq_result = _mm256_cmpeq_epi32(vals, keys);\n        __m256i mask = _mm256_or_si256(cmp_result, eq_result);\n\n        // Extract mask and process\n        int mask_result = _mm256_movemask_epi8(mask);\n        if (mask_result != 0) {\n            for (int k = 0; k < 8; ++k) {\n                if ((mask_result & (1 << (k * 4))) { // Check each 32-bit lane\n                    if (tmp_vals[k] >= tmp_keys[k] && tmp_keys[k] > max_val) {\n                        max_val = tmp_keys[k];\n                        max_key = tmp_keys[k];\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remainder\n    for (int i = 0; i < remainder; ++i, ++it) {\n        if (it->second >= it->first && it->first > max_val) {\n            max_val = it->first;\n            max_key = it->first;\n        }\n    }\n\n    return max_key;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_num = -1;\n\n    // Process elements in chunks of 8 for AVX\n    size_t i = 0;\n    size_t size = freq.size();\n    const int* keys = nullptr;\n    const int* counts = nullptr;\n    std::vector<int> keys_vec, counts_vec;\n    \n    keys_vec.reserve(size);\n    counts_vec.reserve(size);\n    for (const auto& pair : freq) {\n        keys_vec.push_back(pair.first);\n        counts_vec.push_back(pair.second);\n    }\n    keys = keys_vec.data();\n    counts = counts_vec.data();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i key_vec = _mm256_loadu_si256((__m256i*)(keys + i));\n        __m256i count_vec = _mm256_loadu_si256((__m256i*)(counts + i));\n        __m256i cmp = _mm256_cmpgt_epi32(count_vec, key_vec);\n        __m256i eq = _mm256_cmpeq_epi32(count_vec, key_vec);\n        __m256i mask = _mm256_or_si256(cmp, eq);\n        int mask_bits = _mm256_movemask_epi8(mask);\n\n        if (mask_bits != 0) {\n            alignas(32) int key_arr[8];\n            alignas(32) int count_arr[8];\n            _mm256_store_si256((__m256i*)key_arr, key_vec);\n            _mm256_store_si256((__m256i*)count_arr, count_vec);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((count_arr[j] >= key_arr[j]) && key_arr[j] > 0) {\n                    if (key_arr[j] > max_val) {\n                        max_val = key_arr[j];\n                        max_num = key_arr[j];\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int key = keys[i];\n        int count = counts[i];\n        if (count >= key && key > 0) {\n            if (key > max_val) {\n                max_val = key;\n                max_num = key;\n            }\n        }\n    }\n\n    return max_num;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    // Check if the vector is a palindrome\n    size_t n = q.size();\n    for (size_t i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i;\n    for (i = 0; i + 7 < n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Sum the remaining elements\n    int sum = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n    for (; i < n; ++i) {\n        sum += q[i];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    // Check if the vector is empty (though description says non-empty)\n    if (q.empty()) {\n        return false;\n    }\n\n    // Calculate sum of elements\n    int sum = 0;\n    int size = q.size();\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n\n    // Process elements in chunks of 8 (AVX2 can handle 8 ints at once)\n    for (; i + 7 < size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Sum the elements of the sum_vec\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += q[i];\n    }\n\n    // Check sum condition\n    if (sum > w) {\n        return false;\n    }\n\n    // Check palindrome condition\n    int left = 0;\n    int right = size - 1;\n    while (left < right) {\n        if (q[left] != q[right]) {\n            return false;\n        }\n        left++;\n        right--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) {\n        return false;\n    }\n\n    // Check if the vector is a palindrome\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t j = n - 1;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + 7 < j - 7; i += 8, j -= 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&q[j - 7]);\n        __m256i rev_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i cmp = _mm256_cmpeq_epi32(left, rev_right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < j; ++i, --j) {\n        if (q[i] != q[j]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) {\n        return false;\n    }\n\n    // Calculate the sum of elements using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t k = 0;\n    for (; k + 7 < n; k += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&q[k]);\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Sum the elements of the vector\n    int sum_array[8];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n    int sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n              sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    // Add remaining elements\n    for (; k < n; ++k) {\n        sum += q[k];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    // Check if the vector is a palindrome\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t j = n - 1;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + 7 < j - 7; i += 8, j -= 8) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(q.data() + i));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(q.data() + j - 7));\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < j; ++i, --j) {\n        if (q[i] != q[j]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum of elements using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t k = 0;\n    for (; k + 7 < n; k += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(q.data() + k));\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Horizontal sum of the vector\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 8));\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 4));\n    int sum = _mm_extract_epi32(sum128, 0);\n\n    // Sum remaining elements\n    for (; k < n; ++k) {\n        sum += q[k];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    // Check if palindromic\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t end = n / 2;\n\n    // Use AVX2 for vectorized comparison\n    for (; i + 8 <= end; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&q[n - 1 - i - 7]);\n        right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < end; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t j = 0;\n    for (; j + 8 <= n; j += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&q[j]);\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Horizontal sum\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n              sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n    // Sum remaining elements\n    for (; j < n; ++j) {\n        sum += q[j];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        int block_size = 8;\n        int remaining = j - i + 1;\n        if (remaining >= block_size) {\n            __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i right = _mm256_loadu_si256((__m256i*)&arr[j - 7]);\n            right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n            __m256i cmp = _mm256_cmpeq_epi32(left, right);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != -1) {\n                for (int k = 0; k < block_size; ++k) {\n                    if (arr[i + k] != arr[j - (7 - k)]) {\n                        changes++;\n                    }\n                }\n            }\n            i += block_size;\n            j -= block_size;\n        } else {\n            if (arr[i] != arr[j]) {\n                changes++;\n            }\n            i++;\n            j--;\n        }\n    }\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t count1 = 0;\n    size_t count2 = 0;\n    \n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        count1 += len;\n    }\n    \n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        count2 += len;\n    }\n    \n    if (count1 <= count2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t count1 = 0;\n    size_t count2 = 0;\n    \n    // Process lst1\n    for (const auto& s : lst1) {\n        size_t len = s.size();\n        count1 += len;\n    }\n    \n    // Process lst2\n    for (const auto& s : lst2) {\n        size_t len = s.size();\n        count2 += len;\n    }\n    \n    if (count1 <= count2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n    \n    // Process lst1\n    for (const auto& s : lst1) {\n        size_t len = s.length();\n        size_t i = 0;\n        __m256i vec_sum = _mm256_setzero_si256();\n        \n        // Process 32 bytes at a time\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            vec_sum = _mm256_add_epi64(vec_sum, _mm256_sad_epu8(chunk, _mm256_setzero_si256()));\n        }\n        \n        // Process remaining bytes\n        uint64_t tail_sum = 0;\n        for (; i < len; ++i) {\n            tail_sum += s[i];\n        }\n        __m256i tail_vec = _mm256_set_epi64x(0, 0, 0, tail_sum);\n        vec_sum = _mm256_add_epi64(vec_sum, tail_vec);\n        \n        // Horizontal sum\n        uint64_t tmp[4];\n        _mm256_storeu_si256((__m256i*)tmp, vec_sum);\n        sum1 = _mm256_add_epi64(sum1, _mm256_set_epi64x(0, 0, 0, tmp[0] + tmp[2]));\n    }\n    \n    // Process lst2\n    for (const auto& s : lst2) {\n        size_t len = s.length();\n        size_t i = 0;\n        __m256i vec_sum = _mm256_setzero_si256();\n        \n        // Process 32 bytes at a time\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            vec_sum = _mm256_add_epi64(vec_sum, _mm256_sad_epu8(chunk, _mm256_setzero_si256()));\n        }\n        \n        // Process remaining bytes\n        uint64_t tail_sum = 0;\n        for (; i < len; ++i) {\n            tail_sum += s[i];\n        }\n        __m256i tail_vec = _mm256_set_epi64x(0, 0, 0, tail_sum);\n        vec_sum = _mm256_add_epi64(vec_sum, tail_vec);\n        \n        // Horizontal sum\n        uint64_t tmp[4];\n        _mm256_storeu_si256((__m256i*)tmp, vec_sum);\n        sum2 = _mm256_add_epi64(sum2, _mm256_set_epi64x(0, 0, 0, tmp[0] + tmp[2]));\n    }\n    \n    // Extract final sums\n    uint64_t final_sum1 = _mm256_extract_epi64(sum1, 0);\n    uint64_t final_sum2 = _mm256_extract_epi64(sum2, 0);\n    \n    if (final_sum1 < final_sum2) {\n        return lst1;\n    } else if (final_sum2 < final_sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n    \n    // Process lst1\n    for (const auto& s : lst1) {\n        size_t len = s.length();\n        size_t i = 0;\n        __m256i vec_sum = _mm256_setzero_si256();\n        \n        // Process string in chunks of 32 bytes (AVX2 register size)\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n            vec_sum = _mm256_add_epi64(vec_sum, _mm256_sad_epu8(chunk, _mm256_setzero_si256()));\n        }\n        \n        // Horizontal sum of the vector\n        uint64_t sum_arr[4];\n        _mm256_storeu_si256((__m256i*)sum_arr, vec_sum);\n        uint64_t partial_sum = sum_arr[0] + sum_arr[2];\n        \n        // Process remaining bytes\n        for (; i < len; ++i) {\n            partial_sum += s[i];\n        }\n        \n        // Add to total sum\n        sum1 = _mm256_add_epi64(sum1, _mm256_set1_epi64x(partial_sum));\n    }\n    \n    // Process lst2 similarly\n    for (const auto& s : lst2) {\n        size_t len = s.length();\n        size_t i = 0;\n        __m256i vec_sum = _mm256_setzero_si256();\n        \n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n            vec_sum = _mm256_add_epi64(vec_sum, _mm256_sad_epu8(chunk, _mm256_setzero_si256()));\n        }\n        \n        uint64_t sum_arr[4];\n        _mm256_storeu_si256((__m256i*)sum_arr, vec_sum);\n        uint64_t partial_sum = sum_arr[0] + sum_arr[2];\n        \n        for (; i < len; ++i) {\n            partial_sum += s[i];\n        }\n        \n        sum2 = _mm256_add_epi64(sum2, _mm256_set1_epi64x(partial_sum));\n    }\n    \n    // Extract the total sums\n    uint64_t total1_arr[4], total2_arr[4];\n    _mm256_storeu_si256((__m256i*)total1_arr, sum1);\n    _mm256_storeu_si256((__m256i*)total2_arr, sum2);\n    uint64_t total1 = total1_arr[0] + total1_arr[1] + total1_arr[2] + total1_arr[3];\n    uint64_t total2 = total2_arr[0] + total2_arr[1] + total2_arr[2] + total2_arr[3];\n    \n    if (total1 < total2) {\n        return lst1;\n    } else if (total2 < total1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n    \n    // Process lst1 in chunks of 8 strings\n    for (size_t i = 0; i < lst1.size(); i += 8) {\n        __m256i chunk = _mm256_setzero_si256();\n        for (size_t j = 0; j < 8 && i + j < lst1.size(); ++j) {\n            chunk = _mm256_add_epi32(chunk, _mm256_set1_epi32(lst1[i + j].size()));\n        }\n        sum1 = _mm256_add_epi32(sum1, chunk);\n    }\n    \n    // Process lst2 in chunks of 8 strings\n    for (size_t i = 0; i < lst2.size(); i += 8) {\n        __m256i chunk = _mm256_setzero_si256();\n        for (size_t j = 0; j < 8 && i + j < lst2.size(); ++j) {\n            chunk = _mm256_add_epi32(chunk, _mm256_set1_epi32(lst2[i + j].size()));\n        }\n        sum2 = _mm256_add_epi32(sum2, chunk);\n    }\n    \n    // Horizontal sum for sum1\n    __m128i sum1_low = _mm256_extracti128_si256(sum1, 0);\n    __m128i sum1_high = _mm256_extracti128_si256(sum1, 1);\n    sum1_low = _mm_add_epi32(sum1_low, sum1_high);\n    sum1_low = _mm_hadd_epi32(sum1_low, sum1_low);\n    sum1_low = _mm_hadd_epi32(sum1_low, sum1_low);\n    int total1 = _mm_extract_epi32(sum1_low, 0);\n    \n    // Horizontal sum for sum2\n    __m128i sum2_low = _mm256_extracti128_si256(sum2, 0);\n    __m128i sum2_high = _mm256_extracti128_si256(sum2, 1);\n    sum2_low = _mm_add_epi32(sum2_low, sum2_high);\n    sum2_low = _mm_hadd_epi32(sum2_low, sum2_low);\n    sum2_low = _mm_hadd_epi32(sum2_low, sum2_low);\n    int total2 = _mm_extract_epi32(sum2_low, 0);\n    \n    if (total1 < total2) {\n        return lst1;\n    } else if (total2 < total1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return false;\n    if (x == 1) return true;\n    if (n == 1) return false;\n\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (!_mm256_testz_si256(vec_cmp, vec_cmp)) {\n            return true;\n        }\n\n        __m256i vec_overflow = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (!_mm256_testz_si256(vec_overflow, vec_overflow)) {\n            return false;\n        }\n\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    if (x == 0) return false;\n\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_set1_epi64x(0);\n\n    while (1) {\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(vec_cmp) != 0) {\n            return true;\n        }\n\n        __m256i vec_overflow = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(vec_overflow) != 0) {\n            return false;\n        }\n\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vresult = _mm256_set1_epi64x(1);\n    __m256i vone = _mm256_set1_epi64x(1);\n    __m256i vzero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(vresult, vx);\n        if (!_mm256_testz_si256(vcmp, vcmp)) {\n            return true;\n        }\n\n        __m256i vmul = _mm256_mul_epu32(vresult, vn);\n        if (_mm256_testz_si256(_mm256_cmpgt_epi64(vmul, vresult), _mm256_set1_epi64x(-1)) == 0) {\n            break;\n        }\n\n        vresult = vmul;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_set1_epi64x(0);\n\n    while (1) {\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(vec_cmp)) return true;\n\n        __m256i vec_overflow = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(vec_overflow)) return false;\n\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    if (x == 0) return false;\n\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_set1_epi64x(0);\n\n    while (1) {\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (!_mm256_testz_si256(vec_cmp, vec_cmp)) {\n            return true;\n        }\n\n        __m256i vec_overflow = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (!_mm256_testz_si256(vec_overflow, vec_overflow)) {\n            return false;\n        }\n\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n    }\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Mask for prime digits: 2, 3, 5, 7, B, D\n    const __m256i prime_mask = _mm256_set1_epi8(0x1A); // Binary: 00011010 (bits 1,3,5,7,11,13 set)\n    const __m256i ascii_offset = _mm256_set1_epi8(0x30);\n    const __m256i nine = _mm256_set1_epi8(9);\n    const __m256i a_offset = _mm256_set1_epi8(0x37);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        \n        // Convert ASCII to values: '0'-'9' -> 0-9, 'A'-'F' -> 10-15\n        __m256i digit_values = _mm256_sub_epi8(chunk, ascii_offset);\n        __m256i gt_nine = _mm256_cmpgt_epi8(digit_values, nine);\n        __m256i adjusted = _mm256_sub_epi8(chunk, a_offset);\n        digit_values = _mm256_blendv_epi8(digit_values, adjusted, gt_nine);\n\n        // Check if digit is prime (2,3,5,7,11,13)\n        __m256i shifted = _mm256_sll_epi32(_mm256_set1_epi32(1), digit_values);\n        __m256i masked = _mm256_and_si256(shifted, prime_mask);\n        __m256i cmp = _mm256_cmpgt_epi8(masked, _mm256_setzero_si256());\n\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(cmp));\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val = (c >= 'A') ? (c - 'A' + 10) : (c - '0');\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Mask for prime digits: 2, 3, 5, 7, B, D\n    const __m256i prime_mask = _mm256_set1_epi8(0x00);\n    const __m256i digit_2 = _mm256_set1_epi8('2');\n    const __m256i digit_3 = _mm256_set1_epi8('3');\n    const __m256i digit_5 = _mm256_set1_epi8('5');\n    const __m256i digit_7 = _mm256_set1_epi8('7');\n    const __m256i digit_B = _mm256_set1_epi8('B');\n    const __m256i digit_D = _mm256_set1_epi8('D');\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        \n        __m256i eq_2 = _mm256_cmpeq_epi8(chunk, digit_2);\n        __m256i eq_3 = _mm256_cmpeq_epi8(chunk, digit_3);\n        __m256i eq_5 = _mm256_cmpeq_epi8(chunk, digit_5);\n        __m256i eq_7 = _mm256_cmpeq_epi8(chunk, digit_7);\n        __m256i eq_B = _mm256_cmpeq_epi8(chunk, digit_B);\n        __m256i eq_D = _mm256_cmpeq_epi8(chunk, digit_D);\n\n        __m256i any = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(eq_2, eq_3),\n                _mm256_or_si256(eq_5, eq_7)\n            ),\n            _mm256_or_si256(eq_B, eq_D)\n        );\n\n        int mask = _mm256_movemask_epi8(any);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Create a mask for prime digits: 2,3,5,7,B,D\n    const __m256i primes = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    // Process 32 characters at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        \n        // Convert ASCII to hex values\n        __m256i hex_digits = _mm256_sub_epi8(chunk, _mm256_set1_epi8('0'));\n        __m256i mask_letters = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('9'));\n        __m256i adjust_letters = _mm256_sub_epi8(_mm256_set1_epi8('A' - '0' - 10), _mm256_set1_epi8(0));\n        hex_digits = _mm256_add_epi8(hex_digits, _mm256_and_si256(mask_letters, adjust_letters));\n        \n        // Lookup in primes mask\n        __m256i prime_mask = _mm256_shuffle_epi8(primes, hex_digits);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(prime_mask));\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val = (c <= '9') ? (c - '0') : (c - 'A' + 10);\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Mask for prime digits: 2, 3, 5, 7, B, D\n    const __m256i prime_mask = _mm256_set1_epi8(0x00);\n    const __m256i primes = _mm256_setr_epi8(\n        0x02, 0x03, 0x05, 0x07, 0x0B, 0x0D, 0x00, 0x00,\n        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n        0x02, 0x03, 0x05, 0x07, 0x0B, 0x0D, 0x00, 0x00,\n        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00\n    );\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, primes);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Create a mask for prime digits: 2,3,5,7,B,D\n    const __m256i primes = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    // Process 32 characters at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        \n        // Convert ASCII to hex values\n        __m256i hex_digits = _mm256_sub_epi8(chunk, _mm256_set1_epi8('0'));\n        __m256i mask_letters = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('9'));\n        __m256i adjust_letters = _mm256_and_si256(mask_letters, _mm256_set1_epi8('A' - '0' - 10));\n        hex_digits = _mm256_add_epi8(hex_digits, adjust_letters);\n        \n        // Lookup in primes mask\n        __m256i prime_mask = _mm256_shuffle_epi8(primes, hex_digits);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(prime_mask));\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val = (c <= '9') ? (c - '0') : (c - 'A' + 10);\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    const char* str = s.c_str();\n    \n    // Process 32 bytes at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Shift right by 1 byte\n        __m256i chunk_r1 = _mm256_alignr_epi8(chunk, _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)), 1);\n        // Shift right by 2 bytes\n        __m256i chunk_r2 = _mm256_alignr_epi8(chunk, _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)), 2);\n        \n        // Compare with next and next-next characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, chunk_r1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, chunk_r2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk_r1, chunk_r2);\n        \n        // Combine comparisons\n        __m256i res = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        \n        // Check if any byte is not zero\n        if (!_mm256_testz_si256(res, res)) {\n            return false;\n        }\n    }\n    \n    // Handle remaining characters\n    for (; i + 3 <= len; ++i) {\n        if (str[i] == str[i+1] || str[i] == str[i+2] || str[i+1] == str[i+2]) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    const char* str = s.c_str();\n    \n    // Process 32 bytes at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Compare each byte with the next one\n        __m256i next1 = _mm256_loadu_si256((__m256i*)(str + i + 1));\n        __m256i eq1 = _mm256_cmpeq_epi8(chunk, next1);\n        \n        // Compare each byte with the one after next\n        __m256i next2 = _mm256_loadu_si256((__m256i*)(str + i + 2));\n        __m256i eq2 = _mm256_cmpeq_epi8(chunk, next2);\n        \n        // OR the results to find any duplicates in any of the comparisons\n        __m256i or_result = _mm256_or_si256(eq1, eq2);\n        \n        // Check if any byte in the result is non-zero\n        if (!_mm256_testz_si256(or_result, or_result)) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i + 2 < len; ++i) {\n        if (str[i] == str[i+1] || str[i] == str[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    const char* str = s.c_str();\n\n    // Process 32 bytes at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Shift left by 1 byte\n        __m256i shifted1 = _mm256_alignr_epi8(chunk, _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)), 1);\n        // Shift left by 2 bytes\n        __m256i shifted2 = _mm256_alignr_epi8(chunk, _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)), 2);\n\n        // Compare with next and next-next characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, shifted1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, shifted2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(shifted1, shifted2);\n\n        // Combine comparisons\n        __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n\n        if (!_mm256_testz_si256(result, result)) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters\n    for (; i + 3 <= len; ++i) {\n        if (str[i] == str[i+1] || str[i] == str[i+2] || str[i+1] == str[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    const char* str = s.c_str();\n    \n    // Process 32 bytes at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Check first 3 characters\n        if (i == 0) {\n            if (str[0] == str[1] || str[1] == str[2] || str[0] == str[2]) {\n                return false;\n            }\n        }\n        \n        // Check overlapping between chunks\n        if (i > 0) {\n            if (str[i] == str[i-1] || str[i] == str[i-2] || str[i+1] == str[i-1]) {\n                return false;\n            }\n        }\n        \n        // Check within the chunk\n        __m256i shifted1 = _mm256_alignr_epi8(chunk, chunk, 1);\n        __m256i shifted2 = _mm256_alignr_epi8(chunk, chunk, 2);\n        \n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, shifted1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, shifted2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(shifted1, shifted2);\n        \n        __m256i or_result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        if (!_mm256_testz_si256(or_result, or_result)) {\n            return false;\n        }\n    }\n    \n    // Handle remaining characters\n    for (; i + 2 < len; ++i) {\n        if (str[i] == str[i+1] || str[i] == str[i+2] || str[i+1] == str[i+2]) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    const char* str = s.c_str();\n    \n    // Process 32 bytes at a time\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        \n        // Shift left by 1 byte\n        __m256i chunk_shift1 = _mm256_alignr_epi8(\n            _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)),\n            chunk, 1\n        );\n        \n        // Shift left by 2 bytes\n        __m256i chunk_shift2 = _mm256_alignr_epi8(\n            _mm256_permute2x128_si256(chunk, chunk, _MM_SHUFFLE(0, 0, 2, 0)),\n            chunk, 2\n        );\n        \n        // Compare with next and next-next characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, chunk_shift1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, chunk_shift2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk_shift1, chunk_shift2);\n        \n        // Combine comparisons\n        __m256i or_result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        \n        if (!_mm256_testz_si256(or_result, or_result)) {\n            return false;\n        }\n    }\n    \n    // Handle remaining characters\n    for (; i + 3 <= len; ++i) {\n        if (str[i] == str[i+1] || str[i] == str[i+2] || str[i+1] == str[i+2]) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i len_vec = _mm256_set1_epi32(len);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (size_t i = 3; i * i <= len; i += 32) {\n        __m256i current_divisors = _mm256_add_epi32(divisors, _mm256_set1_epi32(i - 3));\n        __m256i remainders = _mm256_rem_epu32(len_vec, current_divisors);\n        __m256i cmp = _mm256_cmpeq_epi32(remainders, zeros);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    // Initialize SIMD vectors for checking divisors\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i len_vec = _mm256_set1_epi32(len);\n    __m256i zeros = _mm256_setzero_si256();\n\n    // Check divisors up to sqrt(len) using SIMD\n    size_t sqrt_len = sqrt(len);\n    for (size_t i = 3; i <= sqrt_len; i += 16) {\n        // Load next 8 divisors\n        __m256i rem = _mm256_rem_epu32(len_vec, divisors);\n        __m256i cmp = _mm256_cmpeq_epi32(rem, zeros);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n        // Increment divisors by 16 for next iteration\n        divisors = _mm256_add_epi32(divisors, _mm256_set1_epi32(16));\n    }\n\n    // Check remaining divisors sequentially\n    size_t start = (sqrt_len / 16) * 16 + 3;\n    for (size_t i = start; i <= sqrt_len; i += 2) {\n        if (len % i == 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m256i divisors = _mm256_set_epi32(9, 7, 5, 3, 0, 0, 0, 0);\n    __m256i len_vec = _mm256_set1_epi32(len);\n    __m256i remainders = _mm256_rem_epu32(len_vec, divisors);\n\n    alignas(32) int rem[8];\n    _mm256_store_si256((__m256i*)rem, remainders);\n\n    for (int i = 3; i * i <= len; i += 2) {\n        if (len % i == 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    // Vectorized prime check\n    __m256i divisors = _mm256_set_epi32(7, 5, 3, 0, 0, 0, 0, 0);\n    __m256i len_vec = _mm256_set1_epi32(len);\n    __m256i remainders = _mm256_rem_epu32(len_vec, divisors);\n    int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(remainders, _mm256_setzero_si256()));\n    if (mask != 0) {\n        return false;\n    }\n\n    // Scalar check for remaining divisors\n    for (size_t i = 3; i * i <= len; i += 2) {\n        if (len % i == 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len == 2) return true;\n    if (len % 2 == 0) return false;\n\n    __m256i divisors = _mm256_set_epi32(7, 5, 3, 0, 0, 0, 0, 0);\n    int max_divisor = (int)sqrt(len) + 1;\n    \n    for (int i = 3; i <= max_divisor; i += 8) {\n        __m256i current = _mm256_set_epi32(i+6, i+4, i+2, i, 0, 0, 0, 0);\n        __m256i remainders = _mm256_set1_epi32(len);\n        __m256i divs = _mm256_add_epi32(divisors, current);\n        remainders = _mm256_rem_epu32(remainders, divs);\n        \n        int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(remainders, _mm256_setzero_si256()));\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    while (N > 0) {\n        uint64_t digit = N % 10;\n        sum += digit;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (sum > 0) {\n        binary.push_back((sum % 2) + '0');\n        sum /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    \n    // Process digits in parallel using SIMD\n    while (N > 0) {\n        // Extract 8 digits at a time (each digit is 0-9, fits in 8 bits)\n        uint64_t chunk = N % 100000000;\n        N /= 100000000;\n        \n        // Load the chunk into a 256-bit vector\n        __m256i v = _mm256_set1_epi64x(chunk);\n        \n        // Mask to extract each digit (0x0F0F0F0F0F0F0F0F)\n        __m256i mask = _mm256_set1_epi8(0x0F);\n        \n        // Extract low nibbles (digits)\n        __m256i digits = _mm256_and_si256(v, mask);\n        \n        // Sum the digits horizontally\n        __m256i sum8 = _mm256_sad_epu8(digits, _mm256_setzero_si256());\n        \n        // Extract the sum\n        uint64_t partial_sum = _mm256_extract_epi64(sum8, 0) + \n                              _mm256_extract_epi64(sum8, 1) + \n                              _mm256_extract_epi64(sum8, 2) + \n                              _mm256_extract_epi64(sum8, 3);\n        \n        sum += partial_sum;\n    }\n    \n    // Convert sum to binary string\n    std::string binary;\n    if (sum == 0) {\n        binary = \"0\";\n    } else {\n        while (sum > 0) {\n            binary += (sum % 2) ? '1' : '0';\n            sum /= 2;\n        }\n        std::reverse(binary.begin(), binary.end());\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    \n    // Process digits in parallel using SIMD\n    while (N > 0) {\n        // Extract 8 digits at a time (0-9 each)\n        uint64_t chunk = N % 100000000;\n        N /= 100000000;\n        \n        // Split into individual digits\n        __m256i digits = _mm256_setr_epi32(\n            (chunk / 10000000) % 10,\n            (chunk / 1000000) % 10,\n            (chunk / 100000) % 10,\n            (chunk / 10000) % 10,\n            (chunk / 1000) % 10,\n            (chunk / 100) % 10,\n            (chunk / 10) % 10,\n            chunk % 10\n        );\n        \n        // Horizontal sum of the digits\n        __m256i sum_vec = _mm256_hadd_epi32(digits, digits);\n        sum_vec = _mm256_hadd_epi32(sum_vec, sum_vec);\n        sum_vec = _mm256_hadd_epi32(sum_vec, sum_vec);\n        \n        // Extract the sum\n        sum += _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 4);\n    }\n    \n    // Handle special case when input is 0\n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    // Convert sum to binary string\n    std::string binary;\n    while (sum > 0) {\n        binary.push_back((sum % 2) + '0');\n        sum /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    \n    // Process 4 digits at a time using AVX2\n    constexpr int digits_per_step = 4;\n    uint64_t mask = 0xF;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    while (N > 0) {\n        uint64_t digits = N % 10000; // Extract last 4 digits\n        N /= 10000;\n        \n        // Extract individual digits\n        uint64_t d0 = digits % 10;\n        digits /= 10;\n        uint64_t d1 = digits % 10;\n        digits /= 10;\n        uint64_t d2 = digits % 10;\n        digits /= 10;\n        uint64_t d3 = digits % 10;\n        \n        // Load digits into AVX vector\n        __m256i digit_vec = _mm256_set_epi64x(0, 0, d1, d0);\n        __m256i digit_vec2 = _mm256_set_epi64x(0, 0, d3, d2);\n        \n        // Horizontal add\n        __m256i combined = _mm256_add_epi64(digit_vec, digit_vec2);\n        sum_vec = _mm256_add_epi64(sum_vec, combined);\n    }\n    \n    // Extract horizontal sum from vector\n    uint64_t sum_parts[4];\n    _mm256_storeu_si256((__m256i*)sum_parts, sum_vec);\n    sum = sum_parts[0] + sum_parts[1] + sum_parts[2] + sum_parts[3];\n    \n    // Handle remaining digits (if any)\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n    \n    // Convert sum to binary string\n    std::string binary;\n    if (sum == 0) {\n        binary = \"0\";\n    } else {\n        while (sum > 0) {\n            binary += (sum % 2) ? '1' : '0';\n            sum /= 2;\n        }\n        std::reverse(binary.begin(), binary.end());\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    \n    // Process 8 digits at a time using AVX2\n    while (N > 0) {\n        uint64_t chunk = N % 100000000; // Process up to 8 digits\n        N /= 100000000;\n        \n        // Convert chunk to individual digits\n        uint32_t digits[8] = {0};\n        for (int i = 0; i < 8 && chunk > 0; ++i) {\n            digits[i] = chunk % 10;\n            chunk /= 10;\n        }\n        \n        // Load digits into AVX2 register\n        __m256i v_digits = _mm256_loadu_si256((__m256i*)digits);\n        // Sum the digits\n        __m256i v_sum = _mm256_hadd_epi32(v_digits, v_digits);\n        v_sum = _mm256_hadd_epi32(v_sum, v_sum);\n        // Extract the sum\n        sum += _mm256_extract_epi32(v_sum, 0) + _mm256_extract_epi32(v_sum, 4);\n    }\n    \n    // Convert sum to binary string\n    std::string binary;\n    if (sum == 0) {\n        binary = \"0\";\n    } else {\n        while (sum > 0) {\n            binary += (sum % 2) ? '1' : '0';\n            sum /= 2;\n        }\n        std::reverse(binary.begin(), binary.end());\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(0xFFFFFFFE); // Mask to check even numbers (bitwise AND with 1)\n    __m256i mask_odd_index = _mm256_set_epi32(6, 4, 2, 0, 6, 4, 2, 0); // Mask for odd indices (0-based)\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        \n        // Check if index is odd (index & 1)\n        __m256i index_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256i odd_index = _mm256_cmpeq_epi32(index_mask, _mm256_set1_epi32(1));\n        \n        // Check if value is even (value & 1 == 0)\n        __m256i even_value = _mm256_and_si256(data, _mm256_set1_epi32(1));\n        even_value = _mm256_cmpeq_epi32(even_value, _mm256_setzero_si256());\n        \n        // Combine conditions (odd index AND even value)\n        __m256i condition = _mm256_and_si256(odd_index, even_value);\n        \n        // Mask the values that meet the condition\n        __m256i masked_values = _mm256_and_si256(data, condition);\n        \n        // Accumulate the sum\n        sum_vec = _mm256_add_epi32(sum_vec, masked_values);\n    }\n\n    // Horizontal sum of the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if ((i % 2 == 1) && (lst[i] % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(0xFFFFFFFE);\n    __m256i mask_odd_index = _mm256_set_epi32(6, 4, 2, 0, 6, 4, 2, 0);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i index_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256i odd_index = _mm256_cmpeq_epi32(index_mask, _mm256_set1_epi32(1));\n        __m256i even_data = _mm256_and_si256(data, mask_even);\n        __m256i cmp_result = _mm256_cmpeq_epi32(even_data, data);\n        __m256i valid = _mm256_and_si256(odd_index, cmp_result);\n        __m256i masked_data = _mm256_and_si256(data, valid);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < size; ++i) {\n        if ((i % 2 == 1) && (lst[i] % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(0xFFFFFFFE);\n    __m256i mask_odd_index = _mm256_set_epi32(6, 4, 2, 0, 6, 4, 2, 0);\n\n    for (; i <= size - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i index_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256i odd_index = _mm256_cmpeq_epi32(index_mask, _mm256_set1_epi32(1));\n        __m256i even_data = _mm256_and_si256(data, mask_even);\n        __m256i is_zero = _mm256_cmpeq_epi32(even_data, data);\n        __m256i cond = _mm256_and_si256(odd_index, is_zero);\n        __m256i masked_data = _mm256_and_si256(data, cond);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < size; ++i) {\n        if ((i % 2 == 1) && (lst[i] % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(1);\n    __m256i mask_odd_index = _mm256_set_epi32(6, 4, 2, 0, 6, 4, 2, 0);\n    \n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        \n        // Check if elements are even\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, mask_even), _mm256_setzero_si256());\n        \n        // Check if indices are odd\n        __m256i odd_index_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_even), _mm256_set1_epi32(1));\n        \n        // Combine masks\n        __m256i combined_mask = _mm256_and_si256(even_mask, odd_index_mask);\n        \n        // Extract elements that meet the condition\n        __m256i masked_data = _mm256_and_si256(data, combined_mask);\n        \n        // Accumulate the sum\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n    \n    // Horizontal sum of the vector\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 8));\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 4));\n    sum = _mm_extract_epi32(sum128, 0);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 0;\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(0xFFFFFFFE); // Mask to check even numbers\n    __m256i mask_odd_indices = _mm256_set_epi32(6, 4, 2, 0, 6, 4, 2, 0); // Indices 1,3,5,7 etc. (0-based)\n    \n    for (; i <= size - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_add_epi32(_mm256_set1_epi32(i), mask_odd_indices);\n        \n        // Check if indices are within bounds\n        __m256i valid_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(size), indices);\n        \n        // Load elements at odd indices (even indices in 0-based)\n        __m256i odd_elements = _mm256_maskload_epi32(&lst[0], valid_mask);\n        \n        // Check if elements are even\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(odd_elements, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        even_mask = _mm256_xor_si256(even_mask, _mm256_set1_epi32(-1)); // Invert mask\n        \n        // Add even elements\n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_and_si256(odd_elements, even_mask));\n    }\n    \n    // Horizontal sum\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum += _mm_extract_epi32(sum128, 0);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + block_size <= s.size(); i += block_size) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), shifted);\n    }\n    \n    // Handle remaining characters\n    for (; i < s.size(); ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + block_size <= s.size(); i += block_size) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_add_epi8(input, _mm256_set1_epi8(4)); // Add 4 to each byte\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), shifted);\n    }\n    \n    // Handle remaining characters\n    for (; i < s.size(); ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + block_size <= s.size(); i += block_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_add_epi8(data, _mm256_set1_epi8(4)); // Add 4 to each byte\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), shifted);\n    }\n    \n    // Handle remaining characters\n    for (; i < s.size(); ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32; // AVX2 can process 32 bytes at a time\n    \n    for (; i + block_size <= s.size(); i += block_size) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), shifted);\n    }\n    \n    // Handle remaining characters\n    for (; i < s.size(); ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32;\n    \n    for (; i + block_size <= s.size(); i += block_size) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shifted = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n        _mm256_storeu_si256((__m256i*)&result[i], shifted);\n    }\n    \n    for (; i < s.size(); ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result(message.size(), '\\0');\n    size_t i = 0;\n    const size_t len = message.size();\n    const size_t simd_size = len - len % 32;\n\n    // Process 32 characters at a time using AVX2\n    for (; i < simd_size; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&message[i]);\n        \n        // Check for lowercase letters (a-z)\n        __m256i lowercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(input, _mm256_set1_epi8('a' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), input)\n        );\n        \n        // Check for uppercase letters (A-Z)\n        __m256i uppercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(input, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), input)\n        );\n        \n        // Swap case: lowercase to uppercase and vice versa\n        __m256i case_swapped = _mm256_xor_si256(\n            input,\n            _mm256_and_si256(\n                _mm256_or_si256(lowercase_mask, uppercase_mask),\n                _mm256_set1_epi8(0x20)\n            )\n        );\n        \n        // Identify vowels (case insensitive)\n        __m256i vowels_mask = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(\n                    _mm256_cmpeq_epi8(_mm256_and_si256(case_swapped, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('A')),\n                    _mm256_cmpeq_epi8(_mm256_and_si256(case_swapped, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('E'))\n                ),\n                _mm256_or_si256(\n                    _mm256_or_si256(\n                        _mm256_cmpeq_epi8(_mm256_and_si256(case_swapped, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('I')),\n                        _mm256_cmpeq_epi8(_mm256_and_si256(case_swapped, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('O'))\n                    ),\n                    _mm256_cmpeq_epi8(_mm256_and_si256(case_swapped, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('U'))\n                )\n            )\n        );\n        \n        // Add 2 to vowels (wrapping around if needed)\n        __m256i vowels_plus_2 = _mm256_add_epi8(\n            case_swapped,\n            _mm256_and_si256(\n                vowels_mask,\n                _mm256_set1_epi8(2)\n            )\n        );\n        \n        // Handle wrap-around for 'Y' and 'y' (Y+2 becomes A, y+2 becomes a)\n        __m256i wrap_mask = _mm256_cmpgt_epi8(\n            _mm256_sub_epi8(vowels_plus_2, _mm256_set1_epi8('Z')),\n            _mm256_setzero_si256()\n        );\n        vowels_plus_2 = _mm256_sub_epi8(\n            vowels_plus_2,\n            _mm256_and_si256(\n                wrap_mask,\n                _mm256_set1_epi8(26)\n            )\n        );\n        \n        // Combine results: vowels get +2, others get case swapped\n        __m256i final_result = _mm256_blendv_epi8(\n            case_swapped,\n            vowels_plus_2,\n            vowels_mask\n        );\n        \n        _mm256_storeu_si256((__m256i*)&result[i], final_result);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c = c - 'a' + 'A';\n        } else if (c >= 'A' && c <= 'Z') {\n            c = c - 'A' + 'a';\n        }\n        \n        switch (c) {\n            case 'A': case 'a': c += 2; break;\n            case 'E': case 'e': c += 2; break;\n            case 'I': case 'i': c += 2; break;\n            case 'O': case 'o': c += 2; break;\n            case 'U': case 'u': c += 2; break;\n            case 'Y': c = 'A'; break;\n            case 'y': c = 'a'; break;\n            default: break;\n        }\n        \n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const int simd_size = 32;\n    size_t i = 0;\n    size_t len = message.size();\n    \n    for (; i + simd_size <= len; i += simd_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n        \n        // Swap case\n        __m256i case_mask = _mm256_set1_epi8(0x20);\n        __m256i swapped_case = _mm256_xor_si256(chunk, case_mask);\n        \n        // Identify vowels (both lowercase and uppercase)\n        __m256i a_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('a'));\n        __m256i e_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e'));\n        __m256i i_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i'));\n        __m256i o_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o'));\n        __m256i u_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u'));\n        __m256i A_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('A'));\n        __m256i E_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('E'));\n        __m256i I_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('I'));\n        __m256i O_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('O'));\n        __m256i U_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('U'));\n        \n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(a_mask, e_mask),\n                _mm256_or_si256(i_mask, o_mask)\n            ),\n            _mm256_or_si256(\n                _mm256_or_si256(u_mask, A_mask),\n                _mm256_or_si256(E_mask, I_mask)\n            )\n        );\n        vowel_mask = _mm256_or_si256(vowel_mask, _mm256_or_si256(O_mask, U_mask));\n        \n        // Add 2 to vowels (wrapping around if needed)\n        __m256i vowel_plus_2 = _mm256_add_epi8(chunk, _mm256_set1_epi8(2));\n        \n        // Handle wrap-around for 'y'/'Y' and 'z'/'Z'\n        __m256i wrap_mask = _mm256_cmpgt_epi8(vowel_plus_2, _mm256_set1_epi8('z'));\n        __m256i wrap_corr = _mm256_and_si256(wrap_mask, _mm256_set1_epi8(-26));\n        vowel_plus_2 = _mm256_add_epi8(vowel_plus_2, wrap_corr);\n        \n        // Combine results\n        __m256i encoded = _mm256_blendv_epi8(swapped_case, vowel_plus_2, vowel_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encoded);\n    }\n    \n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = message[i];\n        bool is_vowel = false;\n        \n        switch (c) {\n            case 'a': case 'e': case 'i': case 'o': case 'u':\n            case 'A': case 'E': case 'I': case 'O': case 'U':\n                is_vowel = true;\n                break;\n        }\n        \n        if (is_vowel) {\n            c += 2;\n            if (c > 'z' || (c > 'Z' && c < 'a')) {\n                c -= 26;\n            }\n        }\n        \n        // Swap case\n        c ^= 0x20;\n        \n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const int simd_size = 32;\n    int i = 0;\n    int len = message.size();\n    \n    for (; i + simd_size <= len; i += simd_size) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n        \n        // Create masks for uppercase and lowercase letters\n        __m256i uppercase_mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1));\n        __m256i lowercase_mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1));\n        __m256i letter_mask = _mm256_and_si256(uppercase_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk));\n        letter_mask = _mm256_or_si256(letter_mask, \n                                     _mm256_and_si256(lowercase_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk)));\n        \n        // Swap case\n        __m256i case_swapped = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n        \n        // Identify vowels (both cases)\n        __m256i vowel_a = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('a'));\n        __m256i vowel_e = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e'));\n        __m256i vowel_i = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i'));\n        __m256i vowel_o = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o'));\n        __m256i vowel_u = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u'));\n        __m256i vowel_A = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('A'));\n        __m256i vowel_E = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('E'));\n        __m256i vowel_I = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('I'));\n        __m256i vowel_O = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('O'));\n        __m256i vowel_U = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('U'));\n        \n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(vowel_a, vowel_e),\n                _mm256_or_si256(vowel_i, vowel_o)\n            ),\n            _mm256_or_si256(\n                _mm256_or_si256(vowel_u, vowel_A),\n                _mm256_or_si256(vowel_E, _mm256_or_si256(vowel_I, _mm256_or_si256(vowel_O, vowel_U)))\n        );\n        \n        // Add 2 to vowels (wrapping around if needed)\n        __m256i vowel_shifted = _mm256_add_epi8(chunk, _mm256_set1_epi8(2));\n        \n        // Handle wrap-around for lowercase vowels\n        __m256i wrap_mask_lower = _mm256_cmpgt_epi8(vowel_shifted, _mm256_set1_epi8('z'));\n        __m256i wrapped_lower = _mm256_sub_epi8(vowel_shifted, _mm256_set1_epi8(26));\n        vowel_shifted = _mm256_blendv_epi8(vowel_shifted, wrapped_lower, wrap_mask_lower);\n        \n        // Handle wrap-around for uppercase vowels\n        __m256i wrap_mask_upper = _mm256_cmpgt_epi8(vowel_shifted, _mm256_set1_epi8('Z'));\n        __m256i wrapped_upper = _mm256_sub_epi8(vowel_shifted, _mm256_set1_epi8(26));\n        vowel_shifted = _mm256_blendv_epi8(vowel_shifted, wrapped_upper, wrap_mask_upper);\n        \n        // Apply vowel transformation\n        __m256i transformed = _mm256_blendv_epi8(case_swapped, vowel_shifted, vowel_mask);\n        \n        // Store result\n        _mm256_storeu_si256((__m256i*)&result[i], transformed);\n    }\n    \n    // Process remaining elements\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (isalpha(c)) {\n            // Swap case\n            c ^= 0x20;\n            \n            // Check if vowel\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c += 2;\n                // Handle wrap-around\n                if ((c > 'z' && c < 'a') || c > 'Z') {\n                    c -= 26;\n                }\n            }\n        }\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const char* src = message.data();\n    char* dst = result.data();\n    size_t len = message.size();\n    \n    const __m256i vowels = _mm256_set1_epi8(0x20 | 0x40); // Mask for vowels (case-insensitive)\n    const __m256i vowel_a = _mm256_set1_epi8('a');\n    const __m256i vowel_e = _mm256_set1_epi8('e');\n    const __m256i vowel_i = _mm256_set1_epi8('i');\n    const __m256i vowel_o = _mm256_set1_epi8('o');\n    const __m256i vowel_u = _mm256_set1_epi8('u');\n    const __m256i two = _mm256_set1_epi8(2);\n    const __m256i case_mask = _mm256_set1_epi8(0x20);\n    \n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        // Swap case\n        __m256i swapped = _mm256_xor_si256(chunk, case_mask);\n        \n        // Identify vowels (case-insensitive)\n        __m256i lower = _mm256_or_si256(chunk, case_mask);\n        __m256i is_a = _mm256_cmpeq_epi8(lower, vowel_a);\n        __m256i is_e = _mm256_cmpeq_epi8(lower, vowel_e);\n        __m256i is_i = _mm256_cmpeq_epi8(lower, vowel_i);\n        __m256i is_o = _mm256_cmpeq_epi8(lower, vowel_o);\n        __m256i is_u = _mm256_cmpeq_epi8(lower, vowel_u);\n        __m256i is_vowel = _mm256_or_si256(\n            _mm256_or_si256(is_a, is_e),\n            _mm256_or_si256(is_i, _mm256_or_si256(is_o, is_u))\n        );\n        \n        // Replace vowels with +2\n        __m256i vowel_plus2 = _mm256_add_epi8(swapped, two);\n        __m256i encoded = _mm256_blendv_epi8(swapped, vowel_plus2, is_vowel);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), encoded);\n    }\n    \n    // Handle remaining elements\n    for (; i < len; ++i) {\n        char c = src[i];\n        char swapped = c ^ 0x20;\n        \n        char lower = c | 0x20;\n        if (lower == 'a' || lower == 'e' || lower == 'i' || lower == 'o' || lower == 'u') {\n            dst[i] = swapped + 2;\n        } else {\n            dst[i] = swapped;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result(message.size(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 32; // AVX2 processes 32 bytes at a time\n\n    for (; i + block_size <= message.size(); i += block_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Check for lowercase letters (a-z)\n        __m256i lowercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(data, _mm256_set1_epi8('a' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), data)\n        );\n        // Check for uppercase letters (A-Z)\n        __m256i uppercase_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(data, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), data)\n        );\n\n        // Swap case: lowercase to uppercase and vice versa\n        __m256i swapped_case = _mm256_xor_si256(\n            data,\n            _mm256_and_si256(\n                _mm256_or_si256(lowercase_mask, uppercase_mask),\n                _mm256_set1_epi8(0x20)\n            )\n        );\n\n        // Identify vowels (a, e, i, o, u) in both cases\n        __m256i vowel_a = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('a'));\n        __m256i vowel_e = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('e'));\n        __m256i vowel_i = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('i'));\n        __m256i vowel_o = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('o'));\n        __m256i vowel_u = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('u'));\n        __m256i vowel_A = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('A'));\n        __m256i vowel_E = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('E'));\n        __m256i vowel_I = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('I'));\n        __m256i vowel_O = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('O'));\n        __m256i vowel_U = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8('U'));\n\n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(vowel_a, vowel_e),\n                _mm256_or_si256(vowel_i, vowel_o)\n            ),\n            _mm256_or_si256(\n                _mm256_or_si256(vowel_u, vowel_A),\n                _mm256_or_si256(vowel_E, vowel_I)\n            )\n        );\n        vowel_mask = _mm256_or_si256(vowel_mask, _mm256_or_si256(vowel_O, vowel_U));\n\n        // Replace vowels with vowel + 2\n        __m256i replaced_vowels = _mm256_add_epi8(swapped_case, _mm256_set1_epi8(2));\n        __m256i final_data = _mm256_blendv_epi8(swapped_case, replaced_vowels, vowel_mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_data);\n    }\n\n    // Process remaining characters\n    for (; i < message.size(); ++i) {\n        char c = message[i];\n        // Swap case\n        if (c >= 'a' && c <= 'z') {\n            c -= 32;\n        } else if (c >= 'A' && c <= 'Z') {\n            c += 32;\n        }\n        // Replace vowels\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_z = _mm256_set1_epi8('Z');\n    __m256i lower_a = _mm256_set1_epi8('a');\n    __m256i lower_z = _mm256_set1_epi8('z');\n\n    bool has_lower = false;\n    bool has_upper = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            \n            __m256i upper_check = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, _mm256_sub_epi8(upper_a, _mm256_set1_epi8(1))),\n                _mm256_cmpgt_epi8(upper_z, chars)\n            );\n            __m256i lower_check = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, _mm256_sub_epi8(lower_a, _mm256_set1_epi8(1))),\n                _mm256_cmpgt_epi8(lower_z, chars)\n            );\n            __m256i alpha_check = _mm256_or_si256(upper_check, lower_check);\n            \n            if (!_mm256_testz_si256(alpha_check, alpha_check)) {\n                __m256i is_upper = _mm256_andnot_si256(_mm256_cmpeq_epi8(_mm256_and_si256(chars, lower_mask), _mm256_setzero_si256()), upper_check);\n                __m256i is_lower = _mm256_and_si256(lower_check, _mm256_cmpeq_epi8(_mm256_and_si256(chars, lower_mask), lower_mask));\n                \n                if (!_mm256_testz_si256(is_upper, is_upper)) {\n                    has_upper = true;\n                }\n                if (!_mm256_testz_si256(is_lower, is_lower)) {\n                    has_lower = true;\n                }\n                if (has_upper && has_lower) {\n                    return false;\n                }\n            }\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (c >= 'A' && c <= 'Z') {\n                has_upper = true;\n            } else if (c >= 'a' && c <= 'z') {\n                has_lower = true;\n            }\n            if (has_upper && has_lower) {\n                return false;\n            }\n        }\n    }\n\n    return (has_upper || has_lower);\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_bound = _mm256_set1_epi8('Z');\n        __m256i lower_bound = _mm256_set1_epi8('A');\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            \n            // Check if all are letters\n            __m256i is_letter = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n                _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chars)\n            );\n            if (_mm256_movemask_epi8(is_letter) != 0xFFFFFFFF) {\n                return false;\n            }\n\n            // Check case\n            __m256i upper_chars = _mm256_or_si256(chars, lower_mask);\n            __m256i is_upper = _mm256_cmpeq_epi8(chars, upper_chars);\n            __m256i lower_chars = _mm256_and_si256(chars, _mm256_set1_epi8(0xDF));\n            __m256i is_lower = _mm256_cmpeq_epi8(chars, lower_chars);\n\n            if (_mm256_movemask_epi8(is_upper) != 0xFFFFFFFF) {\n                all_lower = false;\n            }\n            if (_mm256_movemask_epi8(is_lower) != 0xFFFFFFFF) {\n                all_upper = false;\n            }\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            if (islower(c)) {\n                all_upper = false;\n            } else {\n                all_lower = false;\n            }\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_a = _mm256_set1_epi8('a');\n        __m256i upper_z = _mm256_set1_epi8('z');\n        __m256i lower_a = _mm256_set1_epi8('A');\n        __m256i lower_z = _mm256_set1_epi8('Z');\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            \n            // Check for lowercase\n            __m256i lower_check = _mm256_or_si256(\n                _mm256_cmpgt_epi8(upper_a, chars),\n                _mm256_cmpgt_epi8(chars, upper_z)\n            );\n            int lower_mask_result = _mm256_movemask_epi8(lower_check);\n            if (lower_mask_result != 0) {\n                all_lower = false;\n            }\n\n            // Check for uppercase\n            __m256i upper_check = _mm256_or_si256(\n                _mm256_cmpgt_epi8(lower_a, chars),\n                _mm256_cmpgt_epi8(chars, lower_z)\n            );\n            int upper_mask_result = _mm256_movemask_epi8(upper_check);\n            if (upper_mask_result != 0) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!islower(c)) {\n                all_lower = false;\n            }\n            if (!isupper(c)) {\n                all_upper = false;\n            }\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_bound = _mm256_set1_epi8('Z');\n        __m256i lower_bound = _mm256_set1_epi8('A');\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk;\n            if (i + 32 <= len) {\n                chunk = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            } else {\n                char buffer[32] = {0};\n                std::memcpy(buffer, key.data() + i, len - i);\n                chunk = _mm256_loadu_si256((const __m256i*)buffer);\n            }\n\n            __m256i upper_check = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('z'));\n            __m256i lower_check = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n            __m256i digit_check = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('/')),\n                _mm256_cmpgt_epi8(_mm256_set1_epi8(':'), chunk)\n            );\n\n            if (!_mm256_testz_si256(upper_check, upper_check) ||\n                !_mm256_testz_si256(lower_check, lower_check) ||\n                !_mm256_testz_si256(digit_check, digit_check)) {\n                return false;\n            }\n\n            __m256i is_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('@')),\n                _mm256_cmpgt_epi8(_mm256_set1_epi8('['), chunk)\n            );\n            __m256i is_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('`')),\n                _mm256_cmpgt_epi8(_mm256_set1_epi8('{'), chunk)\n            );\n\n            all_upper &= _mm256_testc_si256(is_upper, _mm256_set1_epi8(-1));\n            all_lower &= _mm256_testc_si256(is_lower, _mm256_set1_epi8(-1));\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            all_upper &= isupper(c);\n            all_lower &= islower(c);\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_z = _mm256_set1_epi8('Z');\n    __m256i lower_a = _mm256_set1_epi8('a');\n    __m256i lower_z = _mm256_set1_epi8('z');\n\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            \n            __m256i is_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, _mm256_sub_epi8(upper_a, _mm256_set1_epi8(1))),\n                _mm256_cmpgt_epi8(_mm256_add_epi8(upper_z, _mm256_set1_epi8(1)), chars)\n            );\n            \n            __m256i is_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, _mm256_sub_epi8(lower_a, _mm256_set1_epi8(1))),\n                _mm256_cmpgt_epi8(_mm256_add_epi8(lower_z, _mm256_set1_epi8(1)), chars)\n            );\n\n            __m256i is_alpha = _mm256_or_si256(is_upper, is_lower);\n            __m256i non_alpha = _mm256_andnot_si256(is_alpha, _mm256_set1_epi8(0xFF));\n\n            if (_mm256_movemask_epi8(non_alpha)) {\n                return false;\n            }\n\n            all_upper &= (_mm256_movemask_epi8(is_upper) == 0xFFFFFFFF;\n            all_lower &= (_mm256_movemask_epi8(is_lower) == 0xFFFFFFFF;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            all_upper &= isupper(c);\n            all_lower &= islower(c);\n        }\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility using AVX2\n        __m256i divisors = _mm256_set1_epi32(i);\n        int j;\n        for (j = 3; j <= sqrt_i; j += 8) {\n            __m256i current = _mm256_set_epi32(j+7, j+6, j+5, j+4, j+3, j+2, j+1, j);\n            __m256i remainders = _mm256_rem_epi32(divisors, current);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(remainders, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        // Check remaining divisors\n        if (is_prime) {\n            for (; j <= sqrt_i; ++j) {\n                if (i % j == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility using AVX2\n        __m256i divisors = _mm256_set1_epi32(i);\n        int j;\n        for (j = 3; j <= sqrt_i; j += 8) {\n            __m256i current = _mm256_set_epi32(j+7, j+6, j+5, j+4, j+3, j+2, j+1, j);\n            __m256i remainders = _mm256_rem_epi32(divisors, current);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(remainders, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        // Check remaining divisors\n        if (is_prime) {\n            for (; j <= sqrt_i; j += 2) {\n                if (i % j == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility using AVX2\n        __m256i divisors = _mm256_set1_epi32(0);\n        __m256i i_vec = _mm256_set1_epi32(i);\n        int j;\n        for (j = 3; j <= sqrt_i; j += 8) {\n            // Load 8 potential divisors\n            divisors = _mm256_setr_epi32(j, j+1, j+2, j+3, j+4, j+5, j+6, j+7);\n            // Compute i % divisors\n            __m256i quot = _mm256_set1_epi32(0);\n            __m256i rem = _mm256_set1_epi32(0);\n            for (int k = 0; k < 32; ++k) {\n                __m256i mask = _mm256_cmpgt_epi32(divisors, rem);\n                rem = _mm256_slli_epi32(rem, 1);\n                rem = _mm256_or_epi32(rem, _mm256_srli_epi32(_mm256_and_si256(i_vec, _mm256_set1_epi32(1 << (31 - k))), 31 - k));\n                quot = _mm256_slli_epi32(quot, 1);\n                __m256i cmp = _mm256_cmpgt_epi32(divisors, rem);\n                quot = _mm256_sub_epi32(quot, _mm256_andnot_si256(cmp, _mm256_set1_epi32(1)));\n                rem = _mm256_sub_epi32(rem, _mm256_andnot_si256(cmp, divisors));\n            }\n            // Check if any remainder is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n            if (!_mm256_testz_si256(mask, mask)) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        // Check remaining divisors\n        for (; j <= sqrt_i; ++j) {\n            if (i % j == 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility using AVX2\n        __m256i divisors = _mm256_set1_epi32(i);\n        int j;\n        for (j = 3; j <= sqrt_i; j += 8) {\n            __m256i candidates = _mm256_setr_epi32(j, j+1, j+2, j+3, j+4, j+5, j+6, j+7);\n            __m256i remainders = _mm256_rem_epi32(divisors, candidates);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(remainders, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        // Check remaining divisors\n        if (is_prime) {\n            for (; j <= sqrt_i; ++j) {\n                if (i % j == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility using AVX2\n        __m256i divisors = _mm256_set1_epi32(0);\n        __m256i i_vec = _mm256_set1_epi32(i);\n        __m256i zeros = _mm256_setzero_si256();\n\n        for (int j = 0; j < primes.size(); j += 8) {\n            int remaining = primes.size() - j;\n            if (remaining > 8) remaining = 8;\n\n            // Load primes into AVX register\n            int prime_arr[8] = {0};\n            for (int k = 0; k < remaining; ++k) {\n                if (primes[j + k] > sqrt_i) {\n                    is_prime = true;\n                    break;\n                }\n                prime_arr[k] = primes[j + k];\n            }\n\n            divisors = _mm256_loadu_si256((__m256i*)prime_arr);\n            __m256i remainders = _mm256_rem_epi32(i_vec, divisors);\n            __m256i cmp = _mm256_cmpeq_epi32(remainders, zeros);\n\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Process 32 characters at a time (AVX2)\n    const size_t vec_size = 32;\n    __m256i vowels = _mm256_set1_epi8('A');\n    __m256i vowels_mask = _mm256_setr_epi8(\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        \n        // Check for uppercase vowels\n        __m256i vowel_cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i vowel_mask = _mm256_shuffle_epi8(vowels_mask, chunk);\n        __m256i vowel_match = _mm256_cmpeq_epi8(vowel_mask, vowels);\n        __m256i vowel_result = _mm256_or_si256(vowel_cmp, vowel_match);\n        \n        // Create mask for even indices (0, 2, 4...)\n        __m256i indices = _mm256_setr_epi8(\n            0, -1, 2, -1, 4, -1, 6, -1,\n            8, -1, 10, -1, 12, -1, 14, -1,\n            16, -1, 18, -1, 20, -1, 22, -1,\n            24, -1, 26, -1, 28, -1, 30, -1\n        );\n        __m256i even_mask = _mm256_cmpgt_epi8(indices, _mm256_set1_epi8(-1));\n        \n        // Combine vowel match with even indices\n        __m256i result = _mm256_and_si256(vowel_result, even_mask);\n        \n        // Count matches\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Process 32 characters at a time (AVX2)\n    const __m256i vowel_mask = _mm256_set1_epi8(0x20); // Mask for case insensitivity\n    const __m256i vowels = _mm256_set1_epi8('a' | 0x20); // 'a' or 'A'\n    const __m256i vowels_end = _mm256_set1_epi8('u' | 0x20); // 'u' or 'U'\n    const __m256i even_mask = _mm256_set1_epi16(0x00FF); // Mask for even indices\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        \n        // Convert to lowercase for comparison\n        __m256i lower_chunk = _mm256_or_si256(chunk, vowel_mask);\n        \n        // Check if characters are between 'a' and 'u'\n        __m256i vowel_range = _mm256_cmpgt_epi8(lower_chunk, _mm256_sub_epi8(vowels, _mm256_set1_epi8(1)));\n        __m256i vowel_range_end = _mm256_cmpgt_epi8(vowels_end, lower_chunk);\n        __m256i vowel_cmp = _mm256_and_si256(vowel_range, vowel_range_end);\n        \n        // Check for exact vowels (A, E, I, O, U)\n        __m256i is_a = _mm256_cmpeq_epi8(lower_chunk, vowels);\n        __m256i is_e = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('e' | 0x20));\n        __m256i is_i = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('i' | 0x20));\n        __m256i is_o = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('o' | 0x20));\n        __m256i is_u = _mm256_cmpeq_epi8(lower_chunk, _mm256_set1_epi8('u' | 0x20));\n        \n        __m256i vowel_match = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(is_a, is_e), _mm256_or_si256(is_i, is_o)), is_u);\n        \n        // Combine with vowel range check\n        vowel_match = _mm256_and_si256(vowel_match, vowel_cmp);\n        \n        // Check for uppercase (original character is not lowercase)\n        __m256i is_upper = _mm256_cmpgt_epi8(_mm256_set1_epi8('A' - 1), chunk);\n        is_upper = _mm256_or_si256(is_upper, _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z')));\n        is_upper = _mm256_cmpeq_epi8(is_upper, _mm256_setzero_si256());\n        \n        // Combine vowel and uppercase checks\n        __m256i result = _mm256_and_si256(vowel_match, is_upper);\n        \n        // Create mask for even indices (0, 2, 4, ...)\n        __m256i indices = _mm256_set_epi8(31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,\n                                         15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0);\n        __m256i even = _mm256_and_si256(indices, _mm256_set1_epi8(1));\n        even = _mm256_cmpeq_epi8(even, _mm256_setzero_si256());\n        \n        result = _mm256_and_si256(result, even);\n        \n        // Count the number of set bits in the result\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if ((c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Process 32 characters at a time (AVX2)\n    const size_t vec_size = 32;\n    __m256i vowels_mask = _mm256_set1_epi8(0x20); // Mask for lowercase conversion\n    __m256i vowels = _mm256_set1_epi8('a'); // Base for vowel comparison\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_E = _mm256_set1_epi8('E');\n    __m256i upper_I = _mm256_set1_epi8('I');\n    __m256i upper_O = _mm256_set1_epi8('O');\n    __m256i upper_U = _mm256_set1_epi8('U');\n    __m256i even_mask = _mm256_set_epi8(\n        0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n    );\n\n    for (; i + vec_size <= len; i += vec_size) {\n        // Load 32 characters\n        __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        \n        // Check for uppercase vowels\n        __m256i is_A = _mm256_cmpeq_epi8(chars, upper_A);\n        __m256i is_E = _mm256_cmpeq_epi8(chars, upper_E);\n        __m256i is_I = _mm256_cmpeq_epi8(chars, upper_I);\n        __m256i is_O = _mm256_cmpeq_epi8(chars, upper_O);\n        __m256i is_U = _mm256_cmpeq_epi8(chars, upper_U);\n        \n        // Combine vowel checks\n        __m256i vowel_match = _mm256_or_si256(\n            _mm256_or_si256(is_A, is_E),\n            _mm256_or_si256(is_I, _mm256_or_si256(is_O, is_U))\n        );\n        \n        // Apply even index mask\n        __m256i even_vowel_match = _mm256_and_si256(vowel_match, even_mask);\n        \n        // Count matches\n        int mask = _mm256_movemask_epi8(even_vowel_match);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i vowels = _mm256_set1_epi8('A' | ('E' << 8) | ('I' << 16) | ('O' << 24) | ('U' << 32));\n    __m256i mask_even = _mm256_set1_epi16(0x00FF);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        \n        // Check for uppercase vowels\n        __m256i eq_A = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('A'));\n        __m256i eq_E = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('E'));\n        __m256i eq_I = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('I'));\n        __m256i eq_O = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('O'));\n        __m256i eq_U = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('U'));\n        __m256i vowel_mask = _mm256_or_si256(\n            _mm256_or_si256(eq_A, eq_E),\n            _mm256_or_si256(_mm256_or_si256(eq_I, eq_O), eq_U)\n        );\n\n        // Mask even indices\n        __m256i even_mask = _mm256_set_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n        __m256i result = _mm256_and_si256(vowel_mask, even_mask);\n        \n        count += _mm_popcnt_u32(_mm256_movemask_epi8(result));\n    }\n\n    // Handle remaining elements\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Mask for uppercase vowels (A, E, I, O, U)\n    const __m256i vowels_mask = _mm256_set1_epi8(0x20); // For case-insensitive check\n    const __m256i upper_A = _mm256_set1_epi8('A');\n    const __m256i upper_E = _mm256_set1_epi8('E');\n    const __m256i upper_I = _mm256_set1_epi8('I');\n    const __m256i upper_O = _mm256_set1_epi8('O');\n    const __m256i upper_U = _mm256_set1_epi8('U');\n\n    // Process 32 characters at a time (AVX2)\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        \n        // Check for uppercase vowels\n        __m256i is_A = _mm256_cmpeq_epi8(chunk, upper_A);\n        __m256i is_E = _mm256_cmpeq_epi8(chunk, upper_E);\n        __m256i is_I = _mm256_cmpeq_epi8(chunk, upper_I);\n        __m256i is_O = _mm256_cmpeq_epi8(chunk, upper_O);\n        __m256i is_U = _mm256_cmpeq_epi8(chunk, upper_U);\n        \n        // Combine all vowel checks\n        __m256i vowel_match = _mm256_or_si256(\n            _mm256_or_si256(is_A, is_E),\n            _mm256_or_si256(_mm256_or_si256(is_I, is_O), is_U)\n        );\n        \n        // Create a mask for even indices (0, 2, 4, ...)\n        __m256i index_mask = _mm256_set_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n        \n        // Apply index mask to vowel matches\n        __m256i result = _mm256_and_si256(vowel_match, index_mask);\n        \n        // Count the number of set bits in the result\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    int size = n;\n    result.resize(size);\n\n    int i = 0;\n    int vec_size = 8; // AVX2 can process 8 ints at a time\n    int remainder = size % vec_size;\n    int main_part = size - remainder;\n\n    __m256i increment = _mm256_set1_epi32(2);\n    __m256i base = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i start = _mm256_set1_epi32(n);\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i < main_part; i += vec_size) {\n        __m256i current = _mm256_add_epi32(start, _mm256_mullo_epi32(base, increment));\n        __m256i parity = _mm256_and_si256(start, mask);\n        __m256i adjust = _mm256_sub_epi32(parity, one);\n        current = _mm256_add_epi32(current, adjust);\n\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        start = _mm256_add_epi32(start, _mm256_set1_epi32(vec_size * 2));\n    }\n\n    for (; i < size; ++i) {\n        result[i] = n;\n        n += (n % 2 == 0) ? 2 : 2;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    int size = n;\n    result.resize(size);\n\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at a time\n    __m256i increment = _mm256_set1_epi32(2);\n    __m256i current = _mm256_set_epi32(\n        n + 6, n + 4, n + 2, n,\n        n + 6, n + 4, n + 2, n\n    );\n    __m256i mask = _mm256_set1_epi32(n % 2 == 0 ? 1 : 0);\n\n    for (; i + vec_size <= size; i += vec_size) {\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = n + 2 * i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    int size = n;\n    result.resize(size);\n\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at a time\n\n    // Process in chunks of 8 using AVX2\n    for (; i + vec_size <= size; i += vec_size) {\n        __m256i base = _mm256_setr_epi32(\n            n + 2 * i,\n            n + 2 * (i + 1),\n            n + 2 * (i + 2),\n            n + 2 * (i + 3),\n            n + 2 * (i + 4),\n            n + 2 * (i + 5),\n            n + 2 * (i + 6),\n            n + 2 * (i + 7)\n        );\n\n        // If n is even, we need to subtract 1 from odd positions (0-based)\n        if (n % 2 == 0) {\n            __m256i mask = _mm256_setr_epi32(1, -1, 1, -1, 1, -1, 1, -1);\n            base = _mm256_add_epi32(base, mask);\n        }\n\n        _mm256_storeu_si256((__m256i*)&result[i], base);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        result[i] = n + 2 * i;\n        if (n % 2 == 0) {\n            result[i] += (i % 2 == 0) ? 1 : -1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int remainder = n % 2;\n    int i = 0;\n    const int vec_size = 8;\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i base = _mm256_setr_epi32(\n            n + i * 2,\n            n + (i + 1) * 2,\n            n + (i + 2) * 2,\n            n + (i + 3) * 2,\n            n + (i + 4) * 2,\n            n + (i + 5) * 2,\n            n + (i + 6) * 2,\n            n + (i + 7) * 2\n        );\n\n        if (remainder == 1) {\n            __m256i offset = _mm256_set1_epi32(1);\n            base = _mm256_sub_epi32(base, offset);\n        }\n\n        _mm256_storeu_si256((__m256i*)&result[i], base);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = n + i * 2;\n        if (remainder == 1) {\n            result[i] -= 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int* data = result.data();\n\n    int remainder = n % 8;\n    int vec_size = n - remainder;\n\n    __m256i increment = _mm256_set1_epi32(2);\n    __m256i base = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i start = _mm256_set1_epi32(n);\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i one = _mm256_set1_epi32(1);\n\n    __m256i parity = _mm256_and_si256(start, mask);\n    __m256i step = _mm256_add_epi32(base, base);\n    step = _mm256_add_epi32(step, parity);\n\n    for (int i = 0; i < vec_size; i += 8) {\n        _mm256_storeu_si256((__m256i*)(data + i), start);\n        start = _mm256_add_epi32(start, step);\n        step = _mm256_add_epi32(step, _mm256_slli_epi32(increment, 3));\n    }\n\n    for (int i = vec_size; i < n; ++i) {\n        data[i] = n + (i * 2);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    // Vectorized sum calculation\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t i = n;\n    int64_t end = m - (m - n + 1) % 4;\n\n    for (; i <= end; i += 4) {\n        __m256i nums = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        sum_vec = _mm256_add_epi64(sum_vec, nums);\n    }\n\n    // Horizontal sum of the vector\n    int64_t temp_sum[4];\n    _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n    sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    // Handle remaining elements\n    for (; i <= m; ++i) {\n        sum += i;\n    }\n\n    int64_t avg = sum / count;\n    if (sum % count >= (count + 1) / 2) {\n        avg += 1;\n    }\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back('0' + (avg % 2));\n        avg /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i sum = _mm256_add_epi64(vec_n, vec_m);\n    __m256i one = _mm256_set1_epi64x(1);\n    __m256i avg = _mm256_srli_epi64(_mm256_add_epi64(sum, one), 1);\n\n    int64_t res_arr[4];\n    _mm256_storeu_si256((__m256i*)res_arr, avg);\n    int64_t res = res_arr[0];\n\n    if (res == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (res > 0) {\n        binary.push_back((res & 1) ? '1' : '0');\n        res >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    // Process 4 elements at a time using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi64x(3, 2, 1, 0);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i increment = _mm256_set1_epi64x(4);\n\n    int64_t i = 0;\n    for (; i + 3 <= m - n; i += 4) {\n        __m256i current = _mm256_add_epi64(n_vec, indices);\n        sum_vec = _mm256_add_epi64(sum_vec, current);\n        n_vec = _mm256_add_epi64(n_vec, increment);\n    }\n\n    // Sum the elements of the vector\n    int64_t temp_sum[4];\n    _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n    sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    // Process remaining elements\n    for (; i <= m - n; i++) {\n        sum += n + i;\n    }\n\n    int64_t avg = sum / count;\n    if (sum % count >= (count + 1) / 2) {\n        avg++;\n    }\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary += (avg % 2) ? '1' : '0';\n        avg /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    // Process 4 elements at a time using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi64x(3, 2, 1, 0);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i increment = _mm256_set1_epi64x(4);\n\n    int64_t i = 0;\n    for (; i + 3 <= m - n; i += 4) {\n        __m256i current = _mm256_add_epi64(n_vec, indices);\n        sum_vec = _mm256_add_epi64(sum_vec, current);\n        n_vec = _mm256_add_epi64(n_vec, increment);\n    }\n\n    // Sum the vector elements\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i <= m - n; ++i) {\n        sum += n + i;\n    }\n\n    int64_t avg = sum / count;\n    if (sum % count >= (count + 1) / 2) {\n        avg += 1;\n    }\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary += (avg % 2) ? '1' : '0';\n        avg /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    // Vectorized sum calculation\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t i = n;\n    int64_t end = m - 3;\n\n    for (; i <= end; i += 4) {\n        __m256i nums = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        sum_vec = _mm256_add_epi64(sum_vec, nums);\n    }\n\n    // Sum the vector elements\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (; i <= m; ++i) {\n        sum += i;\n    }\n\n    int64_t avg = sum / count;\n    if (sum % count >= (count + 1) / 2) {\n        avg += 1;\n    }\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back((avg % 2) + '0');\n        avg /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask), _mm256_setzero_si256());\n        \n        // Compute factorial for even indices\n        __m256i fact = _mm256_set1_epi32(1);\n        __m256i counter = _mm256_set1_epi32(1);\n        __m256i ones = _mm256_set1_epi32(1);\n        \n        for (int j = 1; j <= i+8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i cmp = _mm256_cmpgt_epi32(j_vec, indices);\n            if (_mm256_testz_si256(cmp, cmp)) break;\n            \n            fact = _mm256_mullo_epi32(fact, _mm256_blendv_epi8(ones, j_vec, cmp));\n        }\n        fact = _mm256_rem_epi32(fact, _mm256_set1_epi32(10000));\n        \n        // Compute sum for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, ones));\n        sum = _mm256_srli_epi32(sum, 1);\n        \n        // Blend results based on even/odd\n        __m256i res = _mm256_blendv_epi8(sum, fact, even_mask);\n        \n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n    \n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if ((i+1) % 2 == 0) {\n            // Factorial mod 10000\n            int fact = 1;\n            for (int j = 1; j <= i+1; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            // Sum 1..i+1\n            result[i] = (i+1)*(i+2)/2;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_and_si256(indices, mask);\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n        \n        // Compute factorial for even indices\n        __m256i fact = _mm256_set1_epi32(1);\n        __m256i current = _mm256_set1_epi32(1);\n        __m256i one = _mm256_set1_epi32(1);\n        \n        for (int j = 1; j <= i+8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i cmp = _mm256_cmpgt_epi32(j_vec, indices);\n            __m256i update_mask = _mm256_andnot_si256(cmp, is_even);\n            \n            fact = _mm256_blendv_epi8(fact, _mm256_mullo_epi32(fact, j_vec), update_mask);\n        }\n        fact = _mm256_and_si256(fact, _mm256_set1_epi32(0x00002710));\n        \n        // Compute sum for odd indices\n        __m256i sum = _mm256_set1_epi32(0);\n        for (int j = 1; j <= i+8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i cmp = _mm256_cmpgt_epi32(j_vec, indices);\n            __m256i update_mask = _mm256_andnot_si256(cmp, _mm256_xor_si256(is_even, _mm256_set1_epi32(-1)));\n            \n            sum = _mm256_blendv_epi8(sum, _mm256_add_epi32(sum, j_vec), update_mask);\n        }\n        \n        // Blend results\n        __m256i res = _mm256_blendv_epi8(sum, fact, is_even);\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n    \n    // Handle remaining elements\n    for (; i < n; ++i) {\n        int idx = i + 1;\n        if (idx % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= idx; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= idx; ++j) {\n                sum += j;\n            }\n            result[i] = sum;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    \n    for (int i = 0; i < n; i += 8) {\n        int remaining = std::min(8, n - i);\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        \n        // Calculate factorial for even indices and sum for odd indices\n        __m256i fact_result = _mm256_set1_epi32(1);\n        __m256i sum_result = _mm256_set1_epi32(0);\n        __m256i current = _mm256_set1_epi32(1);\n        \n        __m256i max_val = indices;\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_set1_epi32(1);\n        \n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(current, max_val)) != 0xFFFF) {\n            // Calculate factorial part\n            __m256i fact_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask), _mm256_setzero_si256());\n            __m256i fact_update = _mm256_and_si256(fact_mask, current);\n            fact_result = _mm256_mullo_epi32(fact_result, _mm256_blendv_epi8(one, current, fact_mask));\n            fact_result = _mm256_rem_epi32(fact_result, _mm256_set1_epi32(10000));\n            \n            // Calculate sum part\n            __m256i sum_mask = _mm256_cmpgt_epi32(mask, _mm256_and_si256(indices, mask));\n            sum_result = _mm256_add_epi32(sum_result, _mm256_and_si256(sum_mask, current));\n            \n            current = _mm256_add_epi32(current, one);\n        }\n        \n        // Combine results\n        __m256i res = _mm256_blendv_epi8(sum_result, fact_result, \n            _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask), _mm256_setzero_si256()));\n        \n        // Store results\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, res);\n        \n        for (int j = 0; j < remaining; j++) {\n            result[i + j] = temp[j];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask), _mm256_setzero_si256());\n        \n        // Compute factorial for even indices\n        __m256i fact = _mm256_set1_epi32(1);\n        __m256i counter = _mm256_set1_epi32(1);\n        __m256i one = _mm256_set1_epi32(1);\n        \n        for (int j = 1; j <= i+8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i cmp = _mm256_cmpgt_epi32(j_vec, indices);\n            if (_mm256_testz_si256(cmp, cmp)) {\n                break;\n            }\n            fact = _mm256_mullo_epi32(fact, _mm256_blendv_epi8(one, j_vec, cmp));\n        }\n        fact = _mm256_rem_epi32(fact, _mm256_set1_epi32(10000));\n        \n        // Compute sum for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, one));\n        sum = _mm256_srli_epi32(sum, 1);\n        \n        // Blend results based on even/odd\n        __m256i res = _mm256_blendv_epi8(sum, fact, even_mask);\n        \n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n    \n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if ((i+1) % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i+1; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            result[i] = (i+1)*(i+2)/2;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 0;\n    \n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256i mask = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask), _mm256_setzero_si256());\n        \n        // Compute factorial for even indices\n        __m256i fact = _mm256_set1_epi32(1);\n        __m256i counter = _mm256_set1_epi32(1);\n        for (int j = 1; j <= i+8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i cmp = _mm256_cmpgt_epi32(j_vec, indices);\n            fact = _mm256_blendv_epi8(_mm256_mullo_epi32(fact, j_vec), fact, cmp);\n        }\n        fact = _mm256_rem_epi32(fact, _mm256_set1_epi32(10000));\n        \n        // Compute sum for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, _mm256_set1_epi32(1)));\n        sum = _mm256_srli_epi32(sum, 1);\n        \n        // Blend results based on even/odd\n        __m256i res = _mm256_blendv_epi8(sum, fact, even_mask);\n        \n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((i+1) % 2 == 0) {\n            // Factorial mod 10000\n            int fact = 1;\n            for (int j = 1; j <= i+1; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            // Sum 1..i\n            result[i] = (i+1)*(i+2)/2;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i even_counts = _mm256_setzero_si256();\n    __m256i odd_counts = _mm256_setzero_si256();\n    int even_total = 0;\n    int odd_total = 0;\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i max_n = _mm256_set1_epi32(n);\n        __m256i mask = _mm256_cmpgt_epi32(max_n, _mm256_sub_epi32(nums, _mm256_set1_epi32(1)));\n        \n        __m256i original = nums;\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i temp = nums;\n        \n        // Reverse digits for each number in the vector\n        for (int j = 0; j < 8; ++j) {\n            int num = _mm256_extract_epi32(temp, j);\n            int rev = 0;\n            int orig_num = num;\n            while (num > 0) {\n                rev = rev * 10 + num % 10;\n                num /= 10;\n            }\n            reversed = _mm256_insert_epi32(reversed, rev, j);\n        }\n        \n        __m256i is_palindrome = _mm256_cmpeq_epi32(original, reversed);\n        __m256i is_valid = _mm256_and_si256(is_palindrome, mask);\n        \n        __m256i parity = _mm256_and_si256(original, _mm256_set1_epi32(1));\n        __m256i is_even = _mm256_cmpeq_epi32(parity, _mm256_setzero_si256());\n        __m256i is_odd = _mm256_cmpeq_epi32(parity, _mm256_set1_epi32(1));\n        \n        __m256i even_mask = _mm256_and_si256(is_valid, is_even);\n        __m256i odd_mask = _mm256_and_si256(is_valid, is_odd);\n        \n        even_counts = _mm256_sub_epi32(even_counts, even_mask);\n        odd_counts = _mm256_sub_epi32(odd_counts, odd_mask);\n    }\n    \n    // Sum the counts\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, even_counts);\n    _mm256_store_si256((__m256i*)odd_arr, odd_counts);\n    \n    for (int i = 0; i < 8; ++i) {\n        even_total += even_arr[i];\n        odd_total += odd_arr[i];\n    }\n    \n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i even_counts = _mm256_setzero_si256();\n    __m256i odd_counts = _mm256_setzero_si256();\n    int even_total = 0;\n    int odd_total = 0;\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(\n            i, i+1, i+2, i+3, i+4, i+5, i+6, i+7\n        );\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n+1), nums);\n        \n        __m256i original = nums;\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i temp = nums;\n        \n        // Reverse digits\n        for (__m256i ten = _mm256_set1_epi32(10); \n             _mm256_movemask_epi8(_mm256_cmpgt_epi32(temp, _mm256_setzero_si256())) != 0;\n             temp = _mm256_div_epi32(temp, ten)) {\n            __m256i remainder = _mm256_rem_epi32(temp, ten);\n            reversed = _mm256_add_epi32(_mm256_mullo_epi32(reversed, ten), remainder);\n        }\n        \n        __m256i is_palindrome = _mm256_cmpeq_epi32(original, reversed);\n        is_palindrome = _mm256_and_si256(is_palindrome, mask);\n        \n        __m256i parity = _mm256_and_si256(original, _mm256_set1_epi32(1));\n        __m256i is_even = _mm256_cmpeq_epi32(parity, _mm256_setzero_si256());\n        __m256i is_odd = _mm256_cmpeq_epi32(parity, _mm256_set1_epi32(1));\n        \n        __m256i even_pals = _mm256_and_si256(is_palindrome, is_even);\n        __m256i odd_pals = _mm256_and_si256(is_palindrome, is_odd);\n        \n        even_counts = _mm256_sub_epi32(even_counts, even_pals);\n        odd_counts = _mm256_sub_epi32(odd_counts, odd_pals);\n    }\n    \n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, even_counts);\n    _mm256_store_si256((__m256i*)odd_arr, odd_counts);\n    \n    for (int i = 0; i < 8; ++i) {\n        even_total += even_arr[i];\n        odd_total += odd_arr[i];\n    }\n    \n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i even_counts = _mm256_setzero_si256();\n    __m256i odd_counts = _mm256_setzero_si256();\n    int even_total = 0;\n    int odd_total = 0;\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_set_epi32(\n            std::min(i + 7, n), std::min(i + 6, n), std::min(i + 5, n), std::min(i + 4, n),\n            std::min(i + 3, n), std::min(i + 2, n), std::min(i + 1, n), i\n        );\n\n        __m256i original = nums;\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i temp = nums;\n\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(temp, _mm256_setzero_si256()))) {\n            __m256i digit = _mm256_rem_epi32(temp, _mm256_set1_epi32(10));\n            reversed = _mm256_add_epi32(_mm256_mullo_epi32(reversed, _mm256_set1_epi32(10)), digit);\n            temp = _mm256_div_epi32(temp, _mm256_set1_epi32(10));\n        }\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(original, reversed);\n        __m256i is_even = _mm256_and_si256(is_palindrome, _mm256_cmpeq_epi32(_mm256_and_si256(original, _mm256_set1_epi32(1)), _mm256_setzero_si256()));\n        __m256i is_odd = _mm256_and_si256(is_palindrome, _mm256_cmpgt_epi32(_mm256_and_si256(original, _mm256_set1_epi32(1)), _mm256_setzero_si256()));\n\n        even_counts = _mm256_add_epi32(even_counts, _mm256_and_si256(is_even, _mm256_set1_epi32(1)));\n        odd_counts = _mm256_add_epi32(odd_counts, _mm256_and_si256(is_odd, _mm256_set1_epi32(1)));\n    }\n\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, even_counts);\n    _mm256_store_si256((__m256i*)odd_arr, odd_counts);\n\n    for (int i = 0; i < 8; ++i) {\n        even_total += even_arr[i];\n        odd_total += odd_arr[i];\n    }\n\n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i even_counts = _mm256_setzero_si256();\n    __m256i odd_counts = _mm256_setzero_si256();\n    int even_total = 0;\n    int odd_total = 0;\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_set_epi32(\n            std::min(i + 7, n),\n            std::min(i + 6, n),\n            std::min(i + 5, n),\n            std::min(i + 4, n),\n            std::min(i + 3, n),\n            std::min(i + 2, n),\n            std::min(i + 1, n),\n            i\n        );\n\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i temp = nums;\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(temp, _mm256_setzero_si256())) != 0) {\n            __m256i digit = _mm256_sub_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(temp, _mm256_set1_epi32(10)));\n            reversed = _mm256_add_epi32(_mm256_mullo_epi32(reversed, _mm256_set1_epi32(10)), digit);\n            temp = _mm256_div_epi32(temp, _mm256_set1_epi32(10));\n        }\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(nums, reversed);\n        __m256i is_even = _mm256_and_si256(is_palindrome, _mm256_cmpeq_epi32(_mm256_and_si256(nums, _mm256_set1_epi32(1)), _mm256_setzero_si256()));\n        __m256i is_odd = _mm256_and_si256(is_palindrome, _mm256_cmpgt_epi32(_mm256_and_si256(nums, _mm256_set1_epi32(1)), _mm256_setzero_si256()));\n\n        even_counts = _mm256_sub_epi32(even_counts, is_even);\n        odd_counts = _mm256_sub_epi32(odd_counts, is_odd);\n    }\n\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, even_counts);\n    _mm256_store_si256((__m256i*)odd_arr, odd_counts);\n\n    for (int i = 0; i < 8; ++i) {\n        even_total += even_arr[i];\n        odd_total += odd_arr[i];\n    }\n\n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    // Process 8 integers at a time using AVX2\n    const int stride = 8;\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i count_even = _mm256_setzero_si256();\n    __m256i count_odd = _mm256_setzero_si256();\n\n    int i = 1;\n    for (; i + stride <= n; i += stride) {\n        __m256i nums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i temp = nums;\n\n        // Reverse digits for each number in the vector\n        for (int j = 0; j < 10; ++j) {\n            __m256i digit = _mm256_rem_epu32(temp, _mm256_set1_epi32(10));\n            reversed = _mm256_add_epi32(_mm256_mullo_epi32(reversed, _mm256_set1_epi32(10)), digit);\n            temp = _mm256_div_epu32(temp, _mm256_set1_epi32(10));\n        }\n\n        // Check palindrome (nums == reversed)\n        __m256i pal_mask = _mm256_cmpeq_epi32(nums, reversed);\n        \n        // Check even/odd (nums & 1)\n        __m256i parity = _mm256_and_si256(nums, even_mask);\n        __m256i is_even = _mm256_cmpeq_epi32(parity, _mm256_setzero_si256());\n        \n        // Count even palindromes\n        __m256i even_pals = _mm256_and_si256(pal_mask, is_even);\n        count_even = _mm256_sub_epi32(count_even, even_pals);\n        \n        // Count odd palindromes\n        __m256i odd_pals = _mm256_andnot_si256(is_even, pal_mask);\n        count_odd = _mm256_sub_epi32(count_odd, odd_pals);\n    }\n\n    // Extract counts from SIMD registers\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, count_even);\n    _mm256_store_si256((__m256i*)odd_arr, count_odd);\n    \n    for (int j = 0; j < 8; ++j) {\n        even_count += even_arr[j];\n        odd_count += odd_arr[j];\n    }\n\n    // Process remaining numbers sequentially\n    for (; i <= n; ++i) {\n        int num = i;\n        int reversed = 0;\n        int temp = num;\n        \n        while (temp != 0) {\n            reversed = reversed * 10 + temp % 10;\n            temp /= 10;\n        }\n        \n        if (num == reversed) {\n            if (num % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum_mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (; i <= size - 8; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum = _mm256_setzero_si256();\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sign = _mm256_cmpgt_epi32(zero, nums);\n\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(abs_nums, zero))) {\n            __m256i digit = _mm256_and_si256(abs_nums, _mm256_set1_epi32(0xF));\n            sum = _mm256_add_epi32(sum, digit);\n            abs_nums = _mm256_srli_epi32(abs_nums, 4);\n        }\n\n        __m256i first_digit = _mm256_and_si256(_mm256_srli_epi32(_mm256_abs_epi32(nums), 28), _mm256_set1_epi32(0xF));\n        __m256i first_sign = _mm256_and_si256(sign, first_digit);\n        sum = _mm256_sub_epi32(sum, first_sign);\n\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask) / 4;\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int abs_num = abs(num);\n        while (abs_num > 0) {\n            sum += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (num < 0) {\n            int first_digit = abs(num);\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            sum -= 2 * first_digit;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum_mask = _mm256_set1_epi32(0x0F0F0F0F);\n    __m256i sign_mask = _mm256_set1_epi32(0x80808080);\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum = _mm256_setzero_si256();\n        __m256i num = nums;\n\n        // Handle negative numbers\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, num);\n        __m256i neg_num = _mm256_sub_epi32(zero, num);\n        num = _mm256_blendv_epi8(num, neg_num, is_neg);\n\n        // Sum digits\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(num, zero))) {\n            __m256i digits = _mm256_and_si256(num, sum_mask);\n            sum = _mm256_add_epi32(sum, digits);\n            num = _mm256_srli_epi32(num, 8);\n        }\n\n        // Add the negative sign digit if needed\n        sum = _mm256_sub_epi32(sum, _mm256_and_si256(is_neg, one));\n\n        // Compare sum > 0\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(cmp)) >> 2;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int s = 0;\n        int neg = num < 0;\n        num = neg ? -num : num;\n\n        while (num > 0) {\n            s += num % 10;\n            num /= 10;\n        }\n        if (neg) {\n            s -= 1;\n        }\n        if (s > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    __m256i sum_mask = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i <= size - 8; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum = _mm256_setzero_si256();\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sign_mask = _mm256_cmpgt_epi32(zero, nums);\n\n        // Process each digit\n        __m256i t = abs_nums;\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(t, zero))) {\n            __m256i digit = _mm256_sub_epi32(t, _mm256_mullo_epi32(_mm256_srli_epi32(t, 1), _mm256_set1_epi32(0xCCCCCCCD)));\n            digit = _mm256_srli_epi32(_mm256_mullo_epi32(digit, _mm256_set1_epi32(0x1999999A)), 28);\n            digit = _mm256_add_epi32(digit, _mm256_and_si256(_mm256_srli_epi32(t, 28), one));\n            t = _mm256_srli_epi32(t, 4);\n\n            // For negative numbers, subtract the first digit\n            __m256i first_digit_mask = _mm256_and_si256(_mm256_cmpeq_epi32(t, zero), sign_mask);\n            digit = _mm256_sub_epi32(digit, _mm256_and_si256(digit, first_digit_mask));\n            sum = _mm256_add_epi32(sum, digit);\n        }\n\n        // Compare sum > 0\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask) / 4;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int t = abs(num);\n        int sum = 0;\n        bool first_digit = true;\n\n        while (t > 0) {\n            int digit = t % 10;\n            if (first_digit && num < 0) {\n                sum -= digit;\n                first_digit = false;\n            } else {\n                sum += digit;\n            }\n            t /= 10;\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum_mask = _mm256_set1_epi32(0x0F0F0F0F);\n    __m256i ten = _mm256_set1_epi32(10);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sign_mask = _mm256_cmpgt_epi32(zero, nums);\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sum = _mm256_setzero_si256();\n\n        __m256i t = abs_nums;\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(t, zero))) {\n            __m256i digit = _mm256_and_si256(t, sum_mask);\n            sum = _mm256_add_epi32(sum, digit);\n            t = _mm256_div_epi32(t, ten);\n        }\n\n        __m256i first_digit = _mm256_and_si256(abs_nums, sum_mask);\n        __m256i adjust = _mm256_and_si256(sign_mask, first_digit);\n        adjust = _mm256_sub_epi32(zero, adjust);\n        sum = _mm256_add_epi32(sum, adjust);\n\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask) / 4;\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int abs_num = abs(num);\n        while (abs_num > 0) {\n            sum += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (num < 0) {\n            int first_digit = abs(n[i]);\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            sum -= 2 * first_digit;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    __m256i sum_mask = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum = _mm256_setzero_si256();\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sign = _mm256_cmpgt_epi32(zero, nums);\n\n        // Process each digit\n        __m256i t = abs_nums;\n        while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(t, zero))) {\n            __m256i digit = _mm256_sub_epi32(t, _mm256_mullo_epi32(_mm256_srli_epi32(t, 4), _mm256_set1_epi32(0x0F0F0F0F));\n            digit = _mm256_add_epi32(digit, _mm256_srli_epi32(digit, 8));\n            digit = _mm256_add_epi32(digit, _mm256_srli_epi32(digit, 16));\n            digit = _mm256_and_si256(digit, _mm256_set1_epi32(0x000000FF));\n\n            // Adjust for negative numbers (first digit is negative)\n            __m256i first_digit_mask = _mm256_cmpeq_epi32(t, abs_nums);\n            __m256i neg_digit = _mm256_sub_epi32(zero, digit);\n            digit = _mm256_blendv_epi8(digit, neg_digit, _mm256_and_si256(sign, first_digit_mask));\n\n            sum = _mm256_add_epi32(sum, digit);\n            t = _mm256_srli_epi32(t, 8);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        __m256i mask = _mm256_and_si256(cmp, one);\n        sum_mask = _mm256_add_epi32(sum_mask, mask);\n    }\n\n    // Horizontal sum of the mask\n    alignas(32) int mask_arr[8];\n    _mm256_store_si256((__m256i*)mask_arr, sum_mask);\n    for (int j = 0; j < 8; ++j) {\n        count += mask_arr[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int t = abs(num);\n        int sum = 0;\n        int is_first_digit = 1;\n\n        while (t > 0) {\n            int digit = t % 10;\n            if (is_first_digit && num < 0) {\n                digit = -digit;\n            }\n            sum += digit;\n            t /= 10;\n            is_first_digit = 0;\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    int count = 0;\n    int vec_size = 8; // AVX2 can process 8 integers at a time\n    int remaining = n % vec_size;\n    int iterations = n / vec_size;\n\n    __m256i prev_vec, curr_vec;\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    // Process the vector in chunks of 8 elements\n    for (int i = 0; i < iterations; ++i) {\n        int offset = i * vec_size;\n        curr_vec = _mm256_loadu_si256((__m256i const*)&arr[offset]);\n        \n        if (i > 0) {\n            __m256i cmp = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            __m256i masked = _mm256_and_si256(cmp, mask);\n            sum_vec = _mm256_add_epi32(sum_vec, masked);\n        }\n        prev_vec = curr_vec;\n    }\n\n    // Handle the remaining elements\n    if (remaining > 0) {\n        int offset = iterations * vec_size;\n        __m256i last_vec = _mm256_loadu_si256((__m256i const*)&arr[offset - vec_size]);\n        __m256i curr_remaining = _mm256_loadu_si256((__m256i const*)&arr[offset - vec_size + remaining]);\n        \n        __m256i cmp = _mm256_cmpgt_epi32(last_vec, curr_remaining);\n        __m256i masked = _mm256_and_si256(cmp, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked);\n    }\n\n    // Check the first and last elements\n    if (arr.back() > arr.front()) {\n        count++;\n    }\n\n    // Sum up the results from the SIMD operations\n    int simd_sum[8];\n    _mm256_storeu_si256((__m256i*)simd_sum, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        count += simd_sum[i];\n    }\n\n    return count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n\n    // Process elements in chunks of 8 using AVX2\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[(i + 1) % n]);\n        \n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        int mask = _mm256_movemask_epi8(cmp);\n        \n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (arr[(i + j) % n] > arr[(i + j + 1) % n]) {\n                    count++;\n                    pos = (i + j) % n;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i % n] > arr[(i + 1) % n]) {\n            count++;\n            pos = i % n;\n        }\n    }\n\n    if (count == 0) {\n        return true;\n    } else if (count == 1) {\n        // Check if the array is sorted after rotation\n        std::vector<int> rotated(arr.begin() + pos + 1, arr.end());\n        rotated.insert(rotated.end(), arr.begin(), arr.begin() + pos + 1);\n        \n        for (int i = 0; i < n - 1; ++i) {\n            if (rotated[i] > rotated[i + 1]) {\n                return false;\n            }\n        }\n        return true;\n    } else {\n        return false;\n    }\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one__ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n    \n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n    \n    // Find the position where the order breaks\n    for (int i = 0; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            count++;\n            pos = i;\n        }\n    }\n    \n    // If more than one break, not possible\n    if (count > 1) return false;\n    \n    // If no break, already sorted\n    if (count == 0) return true;\n    \n    // Check if last element <= first element\n    return (arr.back() <= arr.front());\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n\n    // Find the position where the order breaks\n    for (int i = 0; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            count++;\n            pos = i;\n        }\n    }\n\n    // If more than one break, not possible\n    if (count > 1) {\n        return false;\n    }\n\n    // If no break, already sorted\n    if (count == 0) {\n        return true;\n    }\n\n    // Check if last element is <= first element\n    if (arr.back() <= arr.front()) {\n        return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n\n    // Process 8 elements at a time using AVX2\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[(i + 1) % n]);\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        int mask = _mm256_movemask_epi8(cmp);\n        \n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (arr[(i + j) % n] > arr[(i + j + 1) % n]) {\n                    count++;\n                    pos = (i + j) % n;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i % n] > arr[(i + 1) % n]) {\n            count++;\n            pos = i % n;\n        }\n    }\n\n    if (count == 0) {\n        return true;\n    } else if (count == 1) {\n        // Check if the array can be sorted by shifting\n        std::vector<int> sorted = arr;\n        std::sort(sorted.begin(), sorted.end());\n        std::rotate(arr.begin(), arr.begin() + pos + 1, arr.end());\n        return arr == sorted;\n    } else {\n        return false;\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int i = 0;\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    int lst1_odd_count = 0;\n    int lst2_even_count = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)&lst1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)&lst2[i]);\n        \n        __m256i mask1 = _mm256_and_si256(v1, ones);\n        __m256i mask2 = _mm256_and_si256(v2, ones);\n        \n        __m256i cmp1 = _mm256_cmpeq_epi32(mask1, zero);\n        __m256i cmp2 = _mm256_cmpeq_epi32(mask2, zero);\n        \n        lst1_odd_count += 8 - _mm256_popcnt_epi32(_mm256_movemask_epi8(cmp1)) / 4;\n        lst2_even_count += _mm256_popcnt_epi32(_mm256_movemask_epi8(cmp2)) / 4;\n    }\n\n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            lst1_odd_count++;\n        }\n        if (lst2[i] % 2 == 0) {\n            lst2_even_count++;\n        }\n    }\n\n    return (lst1_odd_count <= lst2_even_count) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int i = 0;\n    int lst1_odd = 0;\n    int lst2_even = 0;\n\n    // Process in chunks of 8 integers (256 bits)\n    for (; i + 7 < size; i += 8) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)&lst1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)&lst2[i]);\n        \n        // Mask for odd numbers in lst1 (lst1 & 1)\n        __m256i mask1 = _mm256_and_si256(v1, _mm256_set1_epi32(1));\n        // Count odd numbers in lst1\n        lst1_odd += _mm_popcnt_u32(_mm256_movemask_ps(_mm256_castsi256_ps(mask1)));\n        \n        // Mask for even numbers in lst2 (~(lst2 & 1))\n        __m256i mask2 = _mm256_and_si256(v2, _mm256_set1_epi32(1));\n        lst2_even += 8 - _mm_popcnt_u32(_mm256_movemask_ps(_mm256_castsi256_ps(mask2)));\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            lst1_odd++;\n        }\n        if (lst2[i] % 2 == 0) {\n            lst2_even++;\n        }\n    }\n\n    return (lst1_odd <= lst2_even) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int odd_count1 = 0;\n    int even_count2 = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)&lst1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)&lst2[i]);\n        \n        // Mask for odd numbers in lst1 (lst1[i] & 1)\n        __m256i mask1 = _mm256_and_si256(v1, _mm256_set1_epi32(1));\n        // Mask for even numbers in lst2 (~lst2[i] & 1)\n        __m256i mask2 = _mm256_andnot_si256(_mm256_and_si256(v2, _mm256_set1_epi32(1)), _mm256_set1_epi32(1));\n        \n        // Count odd numbers in lst1\n        __m256i cmp1 = _mm256_cmpeq_epi32(mask1, _mm256_set1_epi32(1));\n        odd_count1 += _mm_popcnt_u32(_mm256_movemask_ps(_mm256_castsi256_ps(cmp1))));\n        \n        // Count even numbers in lst2\n        __m256i cmp2 = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(1));\n        even_count2 += _mm_popcnt_u32(_mm256_movemask_ps(_mm256_castsi256_ps(cmp2))));\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    return (odd_count1 <= even_count2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int i = 0;\n    __m256i mask = _mm256_set1_epi32(1);\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i const*)&lst2[i]);\n        \n        __m256i and1 = _mm256_and_si256(vec1, mask);\n        __m256i and2 = _mm256_and_si256(vec2, mask);\n        \n        __m256i cmp1 = _mm256_cmpeq_epi32(and1, _mm256_setzero_si256());\n        __m256i cmp2 = _mm256_cmpeq_epi32(and2, _mm256_setzero_si256());\n        \n        int mask1 = _mm256_movemask_epi8(cmp1);\n        int mask2 = _mm256_movemask_epi8(cmp2);\n        \n        if ((mask1 & mask2) != mask1) {\n            return \"NO\";\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if ((lst1[i] % 2 != 0) && (lst2[i] % 2 != 0)) {\n            return \"NO\";\n        }\n    }\n    \n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int i = 0;\n    __m256i mask = _mm256_set1_epi32(1);\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i const*)&lst2[i]);\n        \n        __m256i and1 = _mm256_and_si256(vec1, mask);\n        __m256i and2 = _mm256_and_si256(vec2, mask);\n        \n        __m256i cmp1 = _mm256_cmpeq_epi32(and1, _mm256_setzero_si256());\n        __m256i cmp2 = _mm256_cmpeq_epi32(and2, _mm256_setzero_si256());\n        \n        int mask1 = _mm256_movemask_epi8(cmp1);\n        int mask2 = _mm256_movemask_epi8(cmp2);\n        \n        if ((mask1 | mask2) != 0xFFFFFFFF) {\n            return \"NO\";\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if ((lst1[i] % 2 != 0) && (lst2[i] % 2 != 0)) {\n            return \"NO\";\n        }\n    }\n    \n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t len = s.size();\n        size_t count = 0;\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 31 < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i ascii_0 = _mm256_set1_epi8('0');\n            __m256i digits = _mm256_sub_epi8(chunk, ascii_0);\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n            __m256i cmp = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1));\n            int mask = _mm256_movemask_epi8(cmp);\n            count += __builtin_popcount(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n\n        // Create the output string\n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t len = s.size();\n        size_t count = 0;\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 31 < len; i += 32) {\n            __m256i chars = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0'));\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n            __m256i cmp = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1));\n            int mask = _mm256_movemask_epi8(cmp);\n            count += __builtin_popcount(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        // Create the output string\n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t len = s.length();\n        size_t count = 0;\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            // Load 32 characters\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            \n            // Subtract '0' to convert to digit values\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0'));\n            \n            // Mask to check odd digits (digits & 1)\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n            \n            // Compare with 1 to find odd digits\n            __m256i cmp = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1));\n            \n            // Get mask of comparison results\n            int mask = _mm256_movemask_epi8(cmp);\n            \n            // Count set bits in mask\n            count += __builtin_popcount(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n\n        // Create the output string\n        std::string out = \"the number of odd elements \";\n        out += std::to_string(count);\n        out += \"n the str\";\n        out += std::to_string(count);\n        out += \"ng \";\n        out += std::to_string(count);\n        out += \" of the \";\n        out += std::to_string(count);\n        out += \"nput.\";\n        result.push_back(out);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t len = s.length();\n        size_t count = 0;\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i ascii_0 = _mm256_set1_epi8('0');\n            __m256i digits = _mm256_sub_epi8(chunk, ascii_0);\n            __m256i odd_mask = _mm256_set1_epi8(1);\n            __m256i odd_bits = _mm256_and_si256(digits, odd_mask);\n            __m256i cmp = _mm256_cmpeq_epi8(odd_bits, odd_mask);\n            int mask = _mm256_movemask_epi8(cmp);\n            count += __builtin_popcount(mask);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n\n        // Create the output string\n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t len = s.size();\n        size_t count = 0;\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 31 < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i ascii_zero = _mm256_set1_epi8('0');\n            __m256i digits = _mm256_sub_epi8(chunk, ascii_zero);\n            __m256i mask = _mm256_set1_epi8(1);\n            __m256i odd = _mm256_and_si256(digits, mask);\n            __m256i cmp = _mm256_cmpeq_epi8(odd, mask);\n            int mask_bits = _mm256_movemask_epi8(cmp);\n            count += __builtin_popcount(mask_bits);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n\n        // Create the output string\n        std::string out = \"the number of odd elements \";\n        out += std::to_string(count);\n        out += \"n the str\";\n        out += std::to_string(count);\n        out += \"ng \";\n        out += std::to_string(count);\n        out += \" of the \";\n        out += std::to_string(count);\n        out += \"nput.\";\n        result.push_back(out);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 4 (AVX2 can handle 4 int64_t elements)\n    const size_t vec_size = 4;\n    size_t vec_end = n - (n % vec_size);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i current_vec = _mm256_set1_epi64x(current_sum);\n        \n        // Calculate cumulative sums\n        __m256i sum_vec = _mm256_add_epi64(current_vec, vec);\n        \n        // Calculate running minimums\n        __m256i min_vec = _mm256_min_epi64(sum_vec, vec);\n        \n        // Extract values to find the new current_sum and min_sum\n        int64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        _mm256_storeu_si256((__m256i*)(temp + 2), min_vec);\n        \n        current_sum = temp[3]; // Last element of sum_vec\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j + 4] < min_sum) {\n                min_sum = temp[j + 4];\n            }\n        }\n        if (current_sum < min_sum) {\n            min_sum = current_sum;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 4 (AVX2 can handle 4 int64_t elements)\n    const size_t vec_size = 4;\n    size_t vec_end = n - (n % vec_size);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i current_vec = _mm256_set1_epi64x(current_sum);\n        \n        // Calculate cumulative sum\n        __m256i sum_vec = _mm256_add_epi64(current_vec, vec);\n        \n        // Calculate running minimum sum\n        __m256i min_vec = _mm256_min_epi64(sum_vec, vec);\n        \n        // Extract values to find the new current_sum and min_sum\n        int64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        current_sum = temp[3]; // Last element is the new current_sum\n        \n        _mm256_storeu_si256((__m256i*)temp, min_vec);\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] < min_sum) {\n                min_sum = temp[j];\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = (nums[i] < current_sum + nums[i]) ? nums[i] : current_sum + nums[i];\n        if (current_sum < min_sum) {\n            min_sum = current_sum;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    \n    // Process elements in chunks of 4 (AVX2 can handle 4 int64_t elements)\n    const size_t vec_size = 4;\n    size_t n = nums.size();\n    \n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i current_vec = _mm256_set1_epi64x(current_sum);\n        \n        // Calculate prefix sums\n        __m256i prefix = _mm256_add_epi64(current_vec, vec);\n        \n        // Calculate running minimum\n        __m256i min_prefix = _mm256_min_epi64(prefix, vec);\n        \n        // Update current_sum with the last element of the prefix\n        current_sum = _mm256_extract_epi64(prefix, 3);\n        \n        // Find the minimum in min_prefix\n        __m256i min_tmp = _mm256_min_epi64(min_prefix, _mm256_permute4x64_epi64(min_prefix, 0b01001110));\n        min_tmp = _mm256_min_epi64(min_tmp, _mm256_permute4x64_epi64(min_tmp, 0b00000001));\n        int64_t local_min = _mm256_extract_epi64(min_tmp, 0);\n        \n        if (local_min < min_sum) {\n            min_sum = local_min;\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        if (current_sum < min_sum) {\n            min_sum = current_sum;\n        }\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 4 (AVX2 can handle 4 int64_t elements)\n    const size_t vec_size = 4;\n    size_t vec_end = n - (n % vec_size);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i current_vec = _mm256_set1_epi64x(current_sum);\n\n        // Calculate cumulative sums\n        __m256i sum_vec = _mm256_add_epi64(current_vec, vec);\n        \n        // Calculate running minimums\n        __m256i min_vec = _mm256_min_epi64(sum_vec, vec);\n        \n        // Extract values to update current_sum and min_sum\n        int64_t sums[4], mins[4];\n        _mm256_storeu_si256((__m256i*)sums, sum_vec);\n        _mm256_storeu_si256((__m256i*)mins, min_vec);\n\n        // Update current_sum and min_sum\n        current_sum = sums[3];\n        for (int j = 0; j < 4; ++j) {\n            if (mins[j] < min_sum) {\n                min_sum = mins[j];\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        if (current_sum < min_sum) {\n            min_sum = current_sum;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 4 (AVX2 can handle 4 int64_t elements)\n    const size_t vec_size = 4;\n    size_t vec_end = n - (n % vec_size);\n\n    for (; i < vec_end; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i current_vec = _mm256_set1_epi64x(current_sum);\n        \n        // Calculate prefix sums\n        __m256i sum_vec = _mm256_add_epi64(current_vec, vec);\n        \n        // Calculate running minimum\n        __m256i min_vec = _mm256_min_epi64(sum_vec, vec);\n        \n        // Extract horizontal minimum\n        int64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, min_vec);\n        int64_t local_min = temp[0];\n        for (int j = 1; j < 4; ++j) {\n            if (temp[j] < local_min) {\n                local_min = temp[j];\n            }\n        }\n        \n        // Update current_sum and min_sum\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        current_sum = temp[3];\n        if (local_min < min_sum) {\n            min_sum = local_min;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        if (current_sum < min_sum) {\n            min_sum = current_sum;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_operations = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& well = grid[i];\n        int sum = 0;\n        int j = 0;\n\n        // Process 8 elements at a time using AVX2\n        __m256i sum_vec = _mm256_setzero_si256();\n        for (; j + 7 < cols; j += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&well[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Sum the elements of the vector\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            sum += temp[k];\n        }\n\n        // Process remaining elements\n        for (; j < cols; ++j) {\n            sum += well[j];\n        }\n\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    for (const auto &row : grid) {\n        int i = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n        int vec_size = 8;\n\n        for (; i + vec_size <= cols; i += vec_size) {\n            __m256i data_vec = _mm256_loadu_si256((__m256i*)&row[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data_vec);\n        }\n\n        alignas(32) int sum_arr[8];\n        _mm256_store_si256((__m256i*)sum_arr, sum_vec);\n        int row_sum = 0;\n        for (int j = 0; j < 8; ++j) {\n            row_sum += sum_arr[j];\n        }\n\n        for (; i < cols; ++i) {\n            row_sum += row[i];\n        }\n\n        total_water += row_sum;\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int j = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; j + 31 < cols; j += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i const*)(&row[j]));\n            __m256i vec2 = _mm256_loadu_si256((__m256i const*)(&row[j + 8]));\n            __m256i vec3 = _mm256_loadu_si256((__m256i const*)(&row[j + 16]));\n            __m256i vec4 = _mm256_loadu_si256((__m256i const*)(&row[j + 24]));\n\n            __m256i sum1 = _mm256_add_epi32(vec1, vec2);\n            __m256i sum2 = _mm256_add_epi32(vec3, vec4);\n            __m256i sum = _mm256_add_epi32(sum1, sum2);\n            sum_vec = _mm256_add_epi32(sum_vec, sum);\n        }\n\n        alignas(32) int sum_arr[8];\n        _mm256_store_si256((__m256i*)sum_arr, sum_vec);\n        int row_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n                      sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n        for (; j < cols; ++j) {\n            row_sum += row[j];\n        }\n\n        total += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (capacity <= 0) return 0;\n    \n    int total = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n    \n    for (const auto& row : grid) {\n        int col = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n        \n        for (; col + 31 < cols; col += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&row[col]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&row[col + 8]);\n            __m256i vec3 = _mm256_loadu_si256((__m256i*)&row[col + 16]);\n            __m256i vec4 = _mm256_loadu_si256((__m256i*)&row[col + 24]);\n            \n            __m256i sum1 = _mm256_add_epi32(vec1, vec2);\n            __m256i sum2 = _mm256_add_epi32(vec3, vec4);\n            __m256i sum_total = _mm256_add_epi32(sum1, sum2);\n            sum_vec = _mm256_add_epi32(sum_vec, sum_total);\n        }\n        \n        int sum_arr[8] = {0};\n        _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n        int row_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n                      sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n        \n        for (; col < cols; ++col) {\n            row_sum += row[col];\n        }\n        \n        total += (row_sum + capacity - 1) / capacity;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int j = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; j + 31 < cols; j += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i const*)(&row[j]));\n            __m256i vec2 = _mm256_loadu_si256((__m256i const*)(&row[j + 8]));\n            __m256i vec3 = _mm256_loadu_si256((__m256i const*)(&row[j + 16]));\n            __m256i vec4 = _mm256_loadu_si256((__m256i const*)(&row[j + 24]));\n\n            sum_vec = _mm256_add_epi32(sum_vec, vec1);\n            sum_vec = _mm256_add_epi32(sum_vec, vec2);\n            sum_vec = _mm256_add_epi32(sum_vec, vec3);\n            sum_vec = _mm256_add_epi32(sum_vec, vec4);\n        }\n\n        int sum = 0;\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            sum += temp[k];\n        }\n\n        for (; j < cols; ++j) {\n            sum += row[j];\n        }\n\n        total += (sum + capacity - 1) / capacity;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    int64_t sum = 0;\n    size_t i = 0;\n\n    __m256i odd_mask = _mm256_set1_epi16(1);\n    __m256i even_pos_mask = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        \n        // Check odd elements (data & 1)\n        __m256i odd_elements = _mm256_and_si256(data, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi16(odd_elements, odd_mask);\n        \n        // Check even positions (alternating mask)\n        __m256i pos_mask = _mm256_cmpeq_epi16(even_pos_mask, _mm256_setzero_si256());\n        \n        // Combine conditions\n        __m256i mask = _mm256_and_si256(is_odd, pos_mask);\n        \n        // Extract elements matching both conditions\n        __m256i masked_data = _mm256_and_si256(data, mask);\n        \n        // Sum the masked elements\n        sum_vec = _mm256_add_epi16(sum_vec, masked_data);\n    }\n\n    // Horizontal sum of the vector\n    int16_t sum_arr[16];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    for (int j = 0; j < 16; j++) {\n        sum += sum_arr[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    // Process 16 elements at a time (256 bits / 16 bits per element)\n    const __m256i even_mask = _mm256_set1_epi16(0x0001);\n    const __m256i odd_mask = _mm256_set1_epi16(0x0001);\n    const __m256i zero = _mm256_setzero_si256();\n    \n    for (; i + 16 <= size; i += 16) {\n        // Load 16 elements\n        __m256i data = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        \n        // Create position mask (even positions: 0, 2, 4, ...)\n        __m256i pos_mask = _mm256_set_epi16(\n            1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0\n        );\n        \n        // Check if elements are in even positions\n        __m256i even_pos = _mm256_cmpeq_epi16(pos_mask, zero);\n        \n        // Check if elements are odd\n        __m256i odd_elements = _mm256_and_si256(data, odd_mask);\n        odd_elements = _mm256_cmpeq_epi16(odd_elements, odd_mask);\n        \n        // Combine conditions\n        __m256i mask = _mm256_and_si256(even_pos, odd_elements);\n        \n        // Extract matching elements\n        __m256i selected = _mm256_and_si256(data, mask);\n        \n        // Sum horizontally\n        __m256i sum_hi = _mm256_unpackhi_epi16(selected, zero);\n        __m256i sum_lo = _mm256_unpacklo_epi16(selected, zero);\n        __m256i sum256 = _mm256_add_epi32(sum_hi, sum_lo);\n        \n        // Reduce to 128-bit\n        __m128i sum128 = _mm_add_epi32(\n            _mm256_extracti128_si256(sum256, 1),\n            _mm256_castsi256_si128(sum256)\n        );\n        \n        // Reduce to 64-bit\n        sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n        sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n        \n        sum += _mm_extract_epi32(sum128, 0);\n    }\n    \n    // Process remaining elements\n    for (; i < size; i++) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    __m256i mask_odd = _mm256_set1_epi16(1);\n    __m256i mask_even_pos = _mm256_set_epi16(\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0,\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0\n    );\n    \n    for (; i + 16 <= size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        __m256i odd_mask = _mm256_and_si256(data, mask_odd);\n        odd_mask = _mm256_cmpeq_epi16(odd_mask, mask_odd);\n        \n        __m256i even_pos_mask = _mm256_and_si256(data, mask_even_pos);\n        even_pos_mask = _mm256_cmpeq_epi16(even_pos_mask, _mm256_setzero_si256());\n        even_pos_mask = _mm256_xor_si256(even_pos_mask, _mm256_set1_epi16(0xFFFF));\n        \n        __m256i combined_mask = _mm256_and_si256(odd_mask, even_pos_mask);\n        __m256i masked_data = _mm256_and_si256(data, combined_mask);\n        \n        __m128i low = _mm256_extracti128_si256(masked_data, 0);\n        __m128i high = _mm256_extracti128_si256(masked_data, 1);\n        \n        __m128i sum128 = _mm_add_epi16(low, high);\n        sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 8));\n        sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 4));\n        sum128 = _mm_add_epi16(sum128, _mm_srli_si128(sum128, 2));\n        \n        sum += _mm_extract_epi16(sum128, 0);\n    }\n    \n    for (; i < size; i += 2) {\n        if (i < size && (lst[i] & 1)) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    int64_t sum = 0;\n    size_t i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_odd = _mm256_set1_epi16(1);\n    __m256i mask_even_pos = _mm256_set_epi16(\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0,\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0\n    );\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        \n        // Check odd elements\n        __m256i odd_mask = _mm256_and_si256(data, mask_odd);\n        odd_mask = _mm256_cmpeq_epi16(odd_mask, mask_odd);\n        \n        // Check even positions\n        __m256i pos_mask = _mm256_and_si256(mask_even_pos, _mm256_set1_epi16(1));\n        pos_mask = _mm256_cmpeq_epi16(pos_mask, _mm256_set1_epi16(1));\n        \n        // Combine masks\n        __m256i combined_mask = _mm256_and_si256(odd_mask, pos_mask);\n        \n        // Apply mask and sum\n        __m256i masked = _mm256_and_si256(data, combined_mask);\n        sum_vec = _mm256_add_epi16(sum_vec, masked);\n    }\n\n    // Horizontal sum of sum_vec\n    int16_t sum_arr[16];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    for (int j = 0; j < 16; ++j) {\n        sum += sum_arr[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_odd = _mm256_set1_epi16(1);\n    __m256i mask_even_pos = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    \n    for (; i + 16 <= size; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        \n        __m256i odd_mask = _mm256_and_si256(data, mask_odd);\n        odd_mask = _mm256_cmpeq_epi16(odd_mask, mask_odd);\n        \n        __m256i even_pos_mask = _mm256_cmpeq_epi16(mask_even_pos, _mm256_setzero_si256());\n        \n        __m256i combined_mask = _mm256_and_si256(odd_mask, even_pos_mask);\n        __m256i masked_data = _mm256_and_si256(data, combined_mask);\n        \n        __m256i sum_lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(masked_data, 0));\n        __m256i sum_hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(masked_data, 1));\n        \n        sum_vec = _mm256_add_epi32(sum_vec, sum_lo);\n        sum_vec = _mm256_add_epi32(sum_vec, sum_hi);\n    }\n    \n    int32_t temp_sum[8];\n    _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp_sum[j];\n    }\n    \n    for (; i < size; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at a time\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i hundred = _mm256_set1_epi32(100);\n\n    for (; i + vec_size <= k; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        // Check if elements are >= -99 and <= 99 (two digits)\n        __m256i abs_data = _mm256_abs_epi32(data);\n        __m256i cmp_lt_100 = _mm256_cmpgt_epi32(hundred, abs_data);\n        __m256i mask = _mm256_and_si256(cmp_lt_100, _mm256_cmpgt_epi32(abs_data, zero));\n        \n        // Mask elements with more than two digits to zero\n        __m256i masked_data = _mm256_and_si256(data, mask);\n        \n        // Add to sum vector\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n\n    // Horizontal sum of the vector\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        int abs_val = val < 0 ? -val : val;\n        if (abs_val < 100 && abs_val >= 10) {\n            sum += val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at a time\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i hundred = _mm256_set1_epi32(100);\n\n    for (; i + vec_size <= k; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        // Check if elements are >= -99 and <= 99 (two digits)\n        __m256i abs_data = _mm256_abs_epi32(data);\n        __m256i cmp_lt_100 = _mm256_cmpgt_epi32(hundred, abs_data);\n        __m256i mask = _mm256_and_si256(cmp_lt_100, _mm256_cmpgt_epi32(abs_data, zero));\n        \n        // Mask elements that don't meet criteria to zero\n        __m256i masked_data = _mm256_and_si256(data, mask);\n        \n        // Add to sum vector\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n\n    // Horizontal sum of the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        int abs_val = val < 0 ? -val : val;\n        if (abs_val < 100 && abs_val >= 10) {\n            sum += val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at a time\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + vec_size <= k; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        // Check if elements have at most two digits (<=99 and >=-99)\n        __m256i mask_positive = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), data);\n        __m256i mask_negative = _mm256_cmpgt_epi32(data, _mm256_set1_epi32(-100));\n        __m256i mask = _mm256_and_si256(mask_positive, mask_negative);\n        \n        // Apply mask and accumulate\n        __m256i masked_data = _mm256_and_si256(data, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_data);\n    }\n    \n    // Horizontal sum of the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; j++) {\n        sum += temp[j];\n    }\n    \n    // Process remaining elements\n    for (; i < k; i++) {\n        int val = arr[i];\n        if (val > -100 && val < 100) {\n            sum += val;\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at once\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i hundred = _mm256_set1_epi32(100);\n\n    for (; i + vec_size <= k; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        // Check if elements are >= -99 and <= 99 (two digits)\n        __m256i abs_data = _mm256_abs_epi32(data);\n        __m256i cmp_lt_100 = _mm256_cmpgt_epi32(hundred, abs_data);\n        __m256i cmp_ge_10 = _mm256_cmpgt_epi32(abs_data, _mm256_sub_epi32(ten, _mm256_set1_epi32(1)));\n        __m256i mask = _mm256_and_si256(cmp_lt_100, cmp_ge_10);\n        \n        // Blend with zero if condition not met\n        __m256i filtered = _mm256_and_si256(data, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, filtered);\n    }\n\n    // Horizontal sum of the vector\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), \n                               _mm256_extracti128_si256(sum_vec, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(0, 1, 2, 3)));\n    sum += _mm_extract_epi32(sum128, 0);\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        int abs_val = val < 0 ? -val : val;\n        if (abs_val >= 10 && abs_val < 100) {\n            sum += val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    const int vec_size = 8; // AVX2 can process 8 ints at once\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_vec = _mm256_set1_epi32(0xFFFFFFC0); // Mask for numbers > 99 (0xFFFFFFC0 is -64)\n    \n    for (; i + vec_size <= k; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_data = _mm256_abs_epi32(data);\n        __m256i cmp = _mm256_cmpgt_epi32(abs_data, _mm256_set1_epi32(99));\n        __m256i masked = _mm256_andnot_si256(cmp, data);\n        sum_vec = _mm256_add_epi32(sum_vec, masked);\n    }\n\n    // Horizontal sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        if (val > -100 && val < 100) {\n            sum += val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t vec_end = vec_size - 1;\n    const size_t simd_end = vec_end - (vec_end % 8);\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current, cmp, shifted;\n\n    for (; i < simd_end; i += 8) {\n        current = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        shifted = _mm256_alignr_epi8(current, prev, 12);\n        cmp = _mm256_cmpgt_epi32(shifted, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n        prev = current;\n    }\n\n    for (; i < vec_end; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < vec_size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i > 1 && lst[i] == lst[i - 2]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t vec_end = vec_size - 1;\n    const size_t simd_end = vec_end - (vec_end % 8);\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current, cmp, shifted;\n\n    for (; i < simd_end; i += 8) {\n        current = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        shifted = _mm256_alignr_epi8(current, prev, 12);\n        cmp = _mm256_cmpgt_epi32(shifted, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n        prev = current;\n    }\n\n    for (; i < vec_end; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < vec_size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i < vec_size - 1 && lst[i] == lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t vec_end = vec_size - 1;\n    const size_t simd_end = vec_end - (vec_end % 8);\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current, cmp, shifted;\n\n    for (; i < simd_end; i += 8) {\n        current = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        shifted = _mm256_alignr_epi8(current, prev, 12);\n        cmp = _mm256_cmpgt_epi32(prev, shifted);\n\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        prev = _mm256_permutevar8x32_epi32(current, _mm256_set_epi32(6, 5, 4, 3, 2, 1, 0, 7));\n    }\n\n    for (; i < vec_end; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < vec_size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i > 1 && lst[i] == lst[i - 2]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t vec_end = vec_size - 1;\n    const size_t simd_end = vec_end - (vec_end % 8);\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current, cmp, shifted;\n\n    for (; i < simd_end; i += 8) {\n        current = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        shifted = _mm256_alignr_epi8(current, prev, 12);\n        cmp = _mm256_cmpgt_epi32(shifted, current);\n\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        prev = _mm256_permutevar8x32_epi32(current, _mm256_set_epi32(6, 5, 4, 3, 2, 1, 0, 7));\n    }\n\n    for (; i < vec_end; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < vec_size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i + 1 < vec_size && lst[i] == lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t vec_end = vec_size - 1;\n    const size_t simd_end = vec_end - (vec_end % 8);\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i current, cmp, shifted;\n\n    for (; i < simd_end; i += 8) {\n        current = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        shifted = _mm256_alignr_epi8(current, prev, 12);\n        shifted = _mm256_permute2x128_si256(prev, current, 0x21);\n        cmp = _mm256_cmpgt_epi32(shifted, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n        prev = current;\n    }\n\n    for (; i < vec_end; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < vec_size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i < vec_size - 1 && lst[i] == lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        \n        // Compute absolute values for sum\n        __m256i abs_data = _mm256_abs_epi32(data);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_data);\n        \n        // Compute signs\n        __m256i sign_mask = _mm256_cmpgt_epi32(data, zero_vec);\n        __m256i neg_sign_mask = _mm256_cmpgt_epi32(zero_vec, data);\n        __m256i sign = _mm256_sub_epi32(\n            _mm256_and_si256(sign_mask, _mm256_set1_epi32(1)),\n            _mm256_and_si256(neg_sign_mask, _mm256_set1_epi32(1))\n        );\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, sign);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int sum = _mm_extract_epi32(sum_low, 0);\n\n    // Horizontal product of sign_prod_vec\n    __m128i sign_prod_low = _mm256_extracti128_si256(sign_prod_vec, 0);\n    __m128i sign_prod_high = _mm256_extracti128_si256(sign_prod_vec, 1);\n    sign_prod_low = _mm_mullo_epi32(sign_prod_low, sign_prod_high);\n    sign_prod_low = _mm_mullo_epi32(sign_prod_low, _mm_shuffle_epi32(sign_prod_low, 0xB1));\n    sign_prod_low = _mm_mullo_epi32(sign_prod_low, _mm_shuffle_epi32(sign_prod_low, 0x4E));\n    int sign_prod = _mm_extract_epi32(sign_prod_low, 0);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        sum += abs(arr[i]);\n        if (arr[i] > 0) {\n            sign_prod *= 1;\n        } else if (arr[i] < 0) {\n            sign_prod *= -1;\n        } else {\n            sign_prod = 0;\n        }\n    }\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_mag = _mm256_setzero_si256();\n    __m256i prod_sign = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i minus_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        \n        // Compute magnitudes\n        __m256i abs_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i mag = _mm256_add_epi32(_mm256_xor_si256(vec, abs_mask), _mm256_and_si256(abs_mask, one));\n        sum_mag = _mm256_add_epi32(sum_mag, mag);\n        \n        // Compute signs\n        __m256i sign = _mm256_or_si256(\n            _mm256_cmpgt_epi32(vec, zero),\n            _mm256_cmpgt_epi32(zero, vec)\n        );\n        __m256i sign_val = _mm256_blendv_epi8(\n            zero,\n            _mm256_blendv_epi8(minus_one, one, _mm256_cmpgt_epi32(vec, zero)),\n            sign\n        );\n        prod_sign = _mm256_mullo_epi32(prod_sign, sign_val);\n    }\n\n    // Horizontal sum of magnitudes\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_mag, 0), _mm256_extracti128_si256(sum_mag, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int sum = _mm_cvtsi128_si32(sum128);\n\n    // Horizontal product of signs\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_sign, 0), _mm256_extracti128_si256(prod_sign, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int prod = _mm_cvtsi128_si32(prod128);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        sum += abs(val);\n        if (val > 0) {\n            prod *= 1;\n        } else if (val < 0) {\n            prod *= -1;\n        } else {\n            prod = 0;\n        }\n    }\n\n    return sum * prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        \n        // Compute absolute values for sum\n        __m256i abs_data = _mm256_abs_epi32(data);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_data);\n        \n        // Compute signs\n        __m256i sign_mask = _mm256_cmpgt_epi32(data, zero_vec);\n        __m256i neg_sign_mask = _mm256_cmpgt_epi32(zero_vec, data);\n        __m256i sign = _mm256_sub_epi32(\n            _mm256_and_si256(sign_mask, _mm256_set1_epi32(1)),\n            _mm256_and_si256(neg_sign_mask, _mm256_set1_epi32(1))\n        );\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, sign);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int sum = _mm_extract_epi32(sum_low, 0);\n\n    // Horizontal product of sign_prod_vec\n    __m128i sign_low = _mm256_extracti128_si256(sign_prod_vec, 0);\n    __m128i sign_high = _mm256_extracti128_si256(sign_prod_vec, 1);\n    sign_low = _mm_mullo_epi32(sign_low, sign_high);\n    sign_low = _mm_mullo_epi32(sign_low, _mm_shuffle_epi32(sign_low, _MM_SHUFFLE(2, 3, 0, 1)));\n    sign_low = _mm_mullo_epi32(sign_low, _mm_shuffle_epi32(sign_low, _MM_SHUFFLE(1, 0, 3, 2)));\n    int sign_prod = _mm_extract_epi32(sign_low, 0);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        sum += abs(arr[i]);\n        if (arr[i] > 0) {\n            sign_prod *= 1;\n        } else if (arr[i] < 0) {\n            sign_prod *= -1;\n        } else {\n            sign_prod = 0;\n        }\n    }\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_mag = _mm256_setzero_si256();\n    __m256i prod_sign = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i minus_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        \n        // Compute magnitudes\n        __m256i abs_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i mag = _mm256_add_epi32(_mm256_xor_si256(vec, abs_mask), _mm256_and_si256(abs_mask, one));\n        sum_mag = _mm256_add_epi32(sum_mag, mag);\n        \n        // Compute signs\n        __m256i sign = _mm256_sign_epi32(one, vec);\n        prod_sign = _mm256_mullo_epi32(prod_sign, sign);\n    }\n\n    // Horizontal sum of magnitudes\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum_mag, 0), _mm256_extracti128_si256(sum_mag, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int sum = _mm_extract_epi32(sum128, 0);\n\n    // Horizontal product of signs\n    __m128i prod128 = _mm_mullo_epi32(_mm256_extracti128_si256(prod_sign, 0), _mm256_extracti128_si256(prod_sign, 1));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(2, 3, 0, 1)));\n    prod128 = _mm_mullo_epi32(prod128, _mm_shuffle_epi32(prod128, _MM_SHUFFLE(1, 0, 3, 2)));\n    int prod = _mm_extract_epi32(prod128, 0);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        sum += abs(val);\n        if (val > 0) {\n            prod *= 1;\n        } else if (val < 0) {\n            prod *= -1;\n        } else {\n            prod = 0;\n        }\n    }\n\n    return sum * prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i;\n    for (i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        \n        // Compute absolute values for sum\n        __m256i abs_data = _mm256_abs_epi32(data);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_data);\n        \n        // Compute signs\n        __m256i sign_mask = _mm256_cmpgt_epi32(data, zero_vec);\n        __m256i neg_sign_mask = _mm256_cmpgt_epi32(zero_vec, data);\n        __m256i sign = _mm256_sub_epi32(\n            _mm256_and_si256(sign_mask, _mm256_set1_epi32(1)),\n            _mm256_and_si256(neg_sign_mask, _mm256_set1_epi32(1))\n        );\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, sign);\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int sum = _mm_extract_epi32(sum_low, 0);\n\n    // Horizontal product of sign_prod_vec\n    __m128i sign_low = _mm256_extracti128_si256(sign_prod_vec, 0);\n    __m128i sign_high = _mm256_extracti128_si256(sign_prod_vec, 1);\n    sign_low = _mm_mullo_epi32(sign_low, sign_high);\n    sign_low = _mm_mullo_epi32(sign_low, _mm_shuffle_epi32(sign_low, _MM_SHUFFLE(2, 3, 0, 1)));\n    sign_low = _mm_mullo_epi32(sign_low, _mm_shuffle_epi32(sign_low, _MM_SHUFFLE(1, 0, 3, 2)));\n    int sign_prod = _mm_extract_epi32(sign_low, 0);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        sum += abs(arr[i]);\n        if (arr[i] > 0) {\n            sign_prod *= 1;\n        } else if (arr[i] < 0) {\n            sign_prod *= -1;\n        } else {\n            sign_prod = 0;\n        }\n    }\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    std::vector<std::vector<int>> positions(N * N + 1);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            positions[grid[i][j]] = {i, j};\n        }\n    }\n\n    std::vector<int> current_path;\n    current_path.push_back(1);\n    for (int step = 1; step < k; ++step) {\n        int last_val = current_path.back();\n        int i = positions[last_val][0];\n        int j = positions[last_val][1];\n\n        int min_next_val = INT_MAX;\n        int di[] = {-1, 1, 0, 0};\n        int dj[] = {0, 0, -1, 1};\n\n        __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n        for (int d = 0; d < 4; d += 8) {\n            int valid_directions = std::min(4 - d, 8);\n            __m256i i_vec = _mm256_set1_epi32(i);\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i di_vec = _mm256_set_epi32(\n                d + 7 < 4 ? di[d + 7] : 0,\n                d + 6 < 4 ? di[d + 6] : 0,\n                d + 5 < 4 ? di[d + 5] : 0,\n                d + 4 < 4 ? di[d + 4] : 0,\n                d + 3 < 4 ? di[d + 3] : 0,\n                d + 2 < 4 ? di[d + 2] : 0,\n                d + 1 < 4 ? di[d + 1] : 0,\n                di[d]\n            );\n            __m256i dj_vec = _mm256_set_epi32(\n                d + 7 < 4 ? dj[d + 7] : 0,\n                d + 6 < 4 ? dj[d + 6] : 0,\n                d + 5 < 4 ? dj[d + 5] : 0,\n                d + 4 < 4 ? dj[d + 4] : 0,\n                d + 3 < 4 ? dj[d + 3] : 0,\n                d + 2 < 4 ? dj[d + 2] : 0,\n                d + 1 < 4 ? dj[d + 1] : 0,\n                dj[d]\n            );\n\n            __m256i new_i_vec = _mm256_add_epi32(i_vec, di_vec);\n            __m256i new_j_vec = _mm256_add_epi32(j_vec, dj_vec);\n\n            __m256i mask_i = _mm256_and_si256(\n                _mm256_cmpgt_epi32(new_i_vec, _mm256_set1_epi32(-1)),\n                _mm256_cmpgt_epi32(_mm256_set1_epi32(N), new_i_vec)\n            );\n            __m256i mask_j = _mm256_and_si256(\n                _mm256_cmpgt_epi32(new_j_vec, _mm256_set1_epi32(-1)),\n                _mm256_cmpgt_epi32(_mm256_set1_epi32(N), new_j_vec)\n            );\n            __m256i mask = _mm256_and_si256(mask_i, mask_j);\n\n            alignas(32) int new_i_arr[8], new_j_arr[8];\n            _mm256_store_si256((__m256i*)new_i_arr, new_i_vec);\n            _mm256_store_si256((__m256i*)new_j_arr, new_j_vec);\n\n            __m256i val_vec = _mm256_set1_epi32(INT_MAX);\n            for (int idx = 0; idx < valid_directions; ++idx) {\n                if (new_i_arr[idx] >= 0 && new_i_arr[idx] < N && new_j_arr[idx] >= 0 && new_j_arr[idx] < N) {\n                    int val = grid[new_i_arr[idx]][new_j_arr[idx]];\n                    val_vec = _mm256_insert_epi32(val_vec, val, idx);\n                }\n            }\n\n            min_vec = _mm256_min_epi32(min_vec, val_vec);\n        }\n\n        alignas(32) int min_arr[8];\n        _mm256_store_si256((__m256i*)min_arr, min_vec);\n        for (int idx = 0; idx < 8; ++idx) {\n            if (min_arr[idx] < min_next_val) {\n                min_next_val = min_arr[idx];\n            }\n        }\n\n        current_path.push_back(min_next_val);\n    }\n\n    return current_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = grid[0][0];\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (grid[i][j] < min_val) {\n                    min_val = grid[i][j];\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    std::vector<std::vector<int>> positions(N * N + 1);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            positions[grid[i][j]] = {i, j};\n        }\n    }\n\n    std::vector<int> current_path;\n    current_path.push_back(1);\n    for (int step = 1; step < k; ++step) {\n        int last_val = current_path.back();\n        int x = positions[last_val][0];\n        int y = positions[last_val][1];\n\n        int min_next_val = N * N + 1;\n        int dx[] = {-1, 1, 0, 0};\n        int dy[] = {0, 0, -1, 1};\n\n        __m256i min_vec = _mm256_set1_epi32(N * N + 1);\n        for (int i = 0; i < 4; i += 8) {\n            __m256i x_vec = _mm256_set1_epi32(x);\n            __m256i y_vec = _mm256_set1_epi32(y);\n            __m256i dx_vec = _mm256_set_epi32(\n                i + 7 < 4 ? dx[i + 7] : 0,\n                i + 6 < 4 ? dx[i + 6] : 0,\n                i + 5 < 4 ? dx[i + 5] : 0,\n                i + 4 < 4 ? dx[i + 4] : 0,\n                dx[i + 3],\n                dx[i + 2],\n                dx[i + 1],\n                dx[i]\n            );\n            __m256i dy_vec = _mm256_set_epi32(\n                i + 7 < 4 ? dy[i + 7] : 0,\n                i + 6 < 4 ? dy[i + 6] : 0,\n                i + 5 < 4 ? dy[i + 5] : 0,\n                i + 4 < 4 ? dy[i + 4] : 0,\n                dy[i + 3],\n                dy[i + 2],\n                dy[i + 1],\n                dy[i]\n            );\n\n            __m256i new_x_vec = _mm256_add_epi32(x_vec, dx_vec);\n            __m256i new_y_vec = _mm256_add_epi32(y_vec, dy_vec);\n\n            __m256i mask_x = _mm256_and_si256(\n                _mm256_cmpgt_epi32(new_x_vec, _mm256_set1_epi32(-1)),\n                _mm256_cmpgt_epi32(_mm256_set1_epi32(N), new_x_vec)\n            );\n            __m256i mask_y = _mm256_and_si256(\n                _mm256_cmpgt_epi32(new_y_vec, _mm256_set1_epi32(-1)),\n                _mm256_cmpgt_epi32(_mm256_set1_epi32(N), new_y_vec)\n            );\n            __m256i mask = _mm256_and_si256(mask_x, mask_y);\n\n            __m256i grid_vals = _mm256_set1_epi32(N * N + 1);\n            for (int j = 0; j < 8; ++j) {\n                if (_mm256_extract_epi32(mask, j)) {\n                    int nx = _mm256_extract_epi32(new_x_vec, j);\n                    int ny = _mm256_extract_epi32(new_y_vec, j);\n                    grid_vals = _mm256_insert_epi32(grid_vals, grid[nx][ny], j);\n                }\n            }\n\n            min_vec = _mm256_min_epi32(min_vec, grid_vals);\n        }\n\n        int min_arr[8];\n        _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n        min_next_val = *std::min_element(min_arr, min_arr + 8);\n\n        current_path.push_back(min_next_val);\n    }\n\n    return current_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = grid[0][0];\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (grid[i][j] < min_val) {\n                    min_val = grid[i][j];\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    std::vector<std::vector<int>> positions(N * N + 1);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            positions[grid[i][j]] = {i, j};\n        }\n    }\n\n    std::vector<int> current_path;\n    current_path.push_back(1);\n    for (int step = 1; step < k; ++step) {\n        int last_val = current_path.back();\n        int i = positions[last_val][0];\n        int j = positions[last_val][1];\n\n        int min_next = N * N + 1;\n        int di[] = {-1, 1, 0, 0};\n        int dj[] = {0, 0, -1, 1};\n\n        __m256i min_vec = _mm256_set1_epi32(N * N + 1);\n        for (int d = 0; d < 4; d += 4) {\n            int ni[4], nj[4];\n            __m256i valid_mask = _mm256_setzero_si256();\n            __m256i next_vals = _mm256_set1_epi32(N * N + 1);\n\n            for (int dd = 0; dd < 4; ++dd) {\n                ni[dd] = i + di[d + dd];\n                nj[dd] = j + dj[d + dd];\n                if (ni[dd] >= 0 && ni[dd] < N && nj[dd] >= 0 && nj[dd] < N) {\n                    valid_mask = _mm256_or_si256(valid_mask, _mm256_set1_epi32(1 << dd));\n                    next_vals = _mm256_insert_epi32(next_vals, grid[ni[dd]][nj[dd]], dd);\n                }\n            }\n\n            __m256i cmp = _mm256_cmpgt_epi32(min_vec, next_vals);\n            min_vec = _mm256_blendv_epi8(min_vec, next_vals, cmp);\n        }\n\n        alignas(32) int min_arr[8];\n        _mm256_store_si256((__m256i*)min_arr, min_vec);\n        for (int dd = 0; dd < 4; ++dd) {\n            if (min_arr[dd] < min_next) {\n                min_next = min_arr[dd];\n            }\n        }\n\n        current_path.push_back(min_next);\n    }\n\n    return current_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = grid[0][0];\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible starting positions (positions with the smallest values)\n    std::vector<std::pair<int, int>> starts;\n    int min_val = N * N + 1;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (grid[i][j] < min_val) {\n                min_val = grid[i][j];\n                starts.clear();\n                starts.emplace_back(i, j);\n            } else if (grid[i][j] == min_val) {\n                starts.emplace_back(i, j);\n            }\n        }\n    }\n\n    // Directions: up, down, left, right\n    const int dirs[4][2] = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};\n\n    // Initialize the best path with the first starting position\n    std::vector<int> best_path;\n    best_path.push_back(min_val);\n\n    for (int step = 1; step < k; ++step) {\n        std::vector<std::pair<int, int>> next_starts;\n        int next_min_val = N * N + 1;\n\n        // Process each current position in parallel using AVX\n        for (const auto &pos : starts) {\n            int i = pos.first;\n            int j = pos.second;\n\n            // Load neighbor values into SIMD registers\n            __m256i min_vec = _mm256_set1_epi32(N * N + 1);\n            int neighbor_vals[4] = {N * N + 1, N * N + 1, N * N + 1, N * N + 1};\n\n            for (int d = 0; d < 4; ++d) {\n                int ni = i + dirs[d][0];\n                int nj = j + dirs[d][1];\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    neighbor_vals[d] = grid[ni][nj];\n                }\n            }\n\n            __m256i neighbor_vec = _mm256_loadu_si256((__m256i*)neighbor_vals);\n            min_vec = _mm256_min_epi32(min_vec, neighbor_vec);\n\n            // Extract the minimum value from the SIMD register\n            int min_neighbor = N * N + 1;\n            for (int d = 0; d < 4; ++d) {\n                if (neighbor_vals[d] < min_neighbor) {\n                    min_neighbor = neighbor_vals[d];\n                }\n            }\n\n            if (min_neighbor < next_min_val) {\n                next_min_val = min_neighbor;\n                next_starts.clear();\n                for (int d = 0; d < 4; ++d) {\n                    int ni = i + dirs[d][0];\n                    int nj = j + dirs[d][1];\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N && grid[ni][nj] == min_neighbor) {\n                        next_starts.emplace_back(ni, nj);\n                    }\n                }\n            } else if (min_neighbor == next_min_val) {\n                for (int d = 0; d < 4; ++d) {\n                    int ni = i + dirs[d][0];\n                    int nj = j + dirs[d][1];\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N && grid[ni][nj] == min_neighbor) {\n                        next_starts.emplace_back(ni, nj);\n                    }\n                }\n            }\n        }\n\n        best_path.push_back(next_min_val);\n        starts = std::move(next_starts);\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = grid[0][0];\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible starting positions (all cells)\n    std::vector<std::pair<int, int>> positions;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            positions.emplace_back(i, j);\n        }\n    }\n\n    // Initialize the best path with the first cell's path\n    std::vector<int> best_path;\n    bool first = true;\n\n    // Process positions in batches using AVX\n    for (size_t p = 0; p < positions.size(); p += 8) {\n        // Load positions into SIMD registers (batch of 8)\n        __m256i x_coords = _mm256_setzero_si256();\n        __m256i y_coords = _mm256_setzero_si256();\n        for (int i = 0; i < 8 && p + i < positions.size(); ++i) {\n            x_coords = _mm256_insert_epi32(x_coords, positions[p + i].first, i);\n            y_coords = _mm256_insert_epi32(y_coords, positions[p + i].second, i);\n        }\n\n        // Simulate BFS for each position in the batch\n        for (int step = 0; step < k; ++step) {\n            // For simplicity, we'll process each position in the batch sequentially\n            // (This is a placeholder - actual SIMD BFS would be more complex)\n            for (int i = 0; i < 8 && p + i < positions.size(); ++i) {\n                int x = _mm256_extract_epi32(x_coords, i);\n                int y = _mm256_extract_epi32(y_coords, i);\n                // Generate neighbors (this part would need proper SIMD implementation)\n                // For demonstration, we'll just find the minimal neighbor\n                int min_neighbor = grid[x][y];\n                int dx[] = {-1, 1, 0, 0};\n                int dy[] = {0, 0, -1, 1};\n                for (int dir = 0; dir < 4; ++dir) {\n                    int nx = x + dx[dir];\n                    int ny = y + dy[dir];\n                    if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                        if (grid[nx][ny] < min_neighbor) {\n                            min_neighbor = grid[nx][ny];\n                            x = nx;\n                            y = ny;\n                        }\n                    }\n                }\n                // Update position in the SIMD register\n                x_coords = _mm256_insert_epi32(x_coords, x, i);\n                y_coords = _mm256_insert_epi32(y_coords, y, i);\n            }\n        }\n\n        // Compare paths (this would need proper SIMD comparison)\n        for (int i = 0; i < 8 && p + i < positions.size(); ++i) {\n            int x = positions[p + i].first;\n            int y = positions[p + i].second;\n            std::vector<int> current_path;\n            current_path.push_back(grid[x][y]);\n            for (int step = 1; step < k; ++step) {\n                // Find minimal neighbor (same logic as above)\n                int min_neighbor = grid[x][y];\n                int min_x = x, min_y = y;\n                int dx[] = {-1, 1, 0, 0};\n                int dy[] = {0, 0, -1, 1};\n                for (int dir = 0; dir < 4; ++dir) {\n                    int nx = x + dx[dir];\n                    int ny = y + dy[dir];\n                    if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                        if (grid[nx][ny] < min_neighbor) {\n                            min_neighbor = grid[nx][ny];\n                            min_x = nx;\n                            min_y = ny;\n                        }\n                    }\n                }\n                x = min_x;\n                y = min_y;\n                current_path.push_back(grid[x][y]);\n            }\n\n            if (first || current_path < best_path) {\n                best_path = current_path;\n                first = false;\n            }\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i product_vec = _mm256_set1_epi64x(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i two_vec = _mm256_set1_epi64x(2);\n\n    while (n > 0) {\n        uint64_t digits[4] = {0};\n        for (int i = 0; i < 4 && n > 0; i++) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digits);\n        __m256i mask = _mm256_and_si256(digit_vec, one_vec);\n        __m256i is_odd = _mm256_cmpeq_epi64(mask, one_vec);\n        __m256i masked_digits = _mm256_and_si256(digit_vec, is_odd);\n        __m256i default_one = _mm256_blendv_epi8(one_vec, masked_digits, is_odd);\n        product_vec = _mm256_mullo_epi64(product_vec, default_one);\n    }\n\n    uint64_t products[4];\n    _mm256_storeu_si256((__m256i*)products, product_vec);\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; i++) {\n        if (products[i] != 1) {\n            result *= products[i];\n        }\n    }\n\n    return (result == 1) ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i product_vec = _mm256_set1_epi64x(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i two_vec = _mm256_set1_epi64x(2);\n\n    while (n > 0) {\n        uint64_t digits[4] = {0};\n        for (int i = 0; i < 4 && n > 0; ++i) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digits);\n        __m256i mask = _mm256_cmpeq_epi64(_mm256_and_si256(digit_vec, one_vec), one_vec);\n        __m256i masked_digits = _mm256_and_si256(digit_vec, mask);\n        masked_digits = _mm256_blendv_epi8(one_vec, masked_digits, mask);\n\n        product_vec = _mm256_mullo_epi64(product_vec, masked_digits);\n    }\n\n    uint64_t products[4];\n    _mm256_storeu_si256((__m256i*)products, product_vec);\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; ++i) {\n        if (products[i] != 1) {\n            result *= products[i];\n        }\n    }\n\n    if (result == 1) {\n        return 0;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i product_vec = _mm256_set1_epi64x(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i two_vec = _mm256_set1_epi64x(2);\n\n    while (n > 0) {\n        uint64_t digits[4] = {0};\n        for (int i = 0; i < 4 && n > 0; i++) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digits);\n        __m256i mask = _mm256_and_si256(digit_vec, one_vec);\n        mask = _mm256_cmpeq_epi64(mask, one_vec);\n        __m256i masked_digits = _mm256_and_si256(digit_vec, mask);\n        masked_digits = _mm256_blendv_epi8(one_vec, masked_digits, mask);\n        product_vec = _mm256_mullo_epi64(product_vec, masked_digits);\n    }\n\n    uint64_t products[4];\n    _mm256_storeu_si256((__m256i*)products, product_vec);\n    uint64_t result = products[0] * products[1] * products[2] * products[3];\n\n    if (result == 1) {\n        return 0;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i product_vec = _mm256_set1_epi64x(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i two_vec = _mm256_set1_epi64x(2);\n\n    while (n > 0) {\n        uint64_t digits[4] = {0};\n        for (int i = 0; i < 4 && n > 0; i++) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digits);\n        __m256i mask = _mm256_cmpeq_epi64(_mm256_and_si256(digit_vec, one_vec), one_vec);\n        __m256i masked_digits = _mm256_and_si256(digit_vec, mask);\n        __m256i is_non_zero = _mm256_cmpgt_epi64(masked_digits, zero_vec);\n        masked_digits = _mm256_blendv_epi8(one_vec, masked_digits, is_non_zero);\n        product_vec = _mm256_mullo_epi64(product_vec, masked_digits);\n    }\n\n    uint64_t products[4];\n    _mm256_storeu_si256((__m256i*)products, product_vec);\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; i++) {\n        if (products[i] != 1) {\n            result *= products[i];\n        }\n    }\n\n    return (result == 1) ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i product_vec = _mm256_set1_epi64x(1);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i two_vec = _mm256_set1_epi64x(2);\n\n    while (n > 0) {\n        uint64_t digits[4] = {0};\n        for (int i = 0; i < 4 && n > 0; i++) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digits);\n        __m256i mask = _mm256_cmpeq_epi64(_mm256_and_si256(digit_vec, one_vec), one_vec);\n        __m256i masked_digits = _mm256_and_si256(digit_vec, mask);\n        __m256i non_zero = _mm256_cmpgt_epi64(masked_digits, zero_vec);\n        masked_digits = _mm256_blendv_epi8(one_vec, masked_digits, non_zero);\n        product_vec = _mm256_mullo_epi64(product_vec, masked_digits);\n    }\n\n    uint64_t products[4];\n    _mm256_storeu_si256((__m256i*)products, product_vec);\n\n    uint64_t result = products[0] * products[1] * products[2] * products[3];\n    if (result == 1) {\n        return 0;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    for (; i < aligned_size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        __m256 rounded = _mm256_round_ps(data, _MM_FROUND_CEIL);\n        __m256 squared = _mm256_mul_ps(rounded, rounded);\n        sum_vec = _mm256_add_ps(sum_vec, squared);\n    }\n\n    float sum_array[8];\n    _mm256_storeu_ps(sum_array, sum_vec);\n    uint64_t sum = 0;\n    for (size_t j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(sum_array[j]);\n    }\n\n    for (; i < vec_size; ++i) {\n        float rounded = ceil(lst[i]);\n        sum += static_cast<uint64_t>(rounded * rounded);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    for (; i < aligned_size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        // Round up using ceil\n        data = _mm256_round_ps(data, _MM_FROUND_CEIL);\n        // Square the elements\n        __m256 squared = _mm256_mul_ps(data, data);\n        // Accumulate the sum\n        sum_vec = _mm256_add_ps(sum_vec, squared);\n    }\n\n    // Horizontal sum of the vector\n    __m128 low = _mm256_extractf128_ps(sum_vec, 0);\n    __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n    low = _mm_add_ps(low, high);\n    __m128 shuf = _mm_movehdup_ps(low);\n    __m128 sums = _mm_add_ps(low, shuf);\n    shuf = _mm_movehl_ps(shuf, sums);\n    sums = _mm_add_ss(sums, shuf);\n    float sum = _mm_cvtss_f32(sums);\n\n    // Process remaining elements\n    for (; i < vec_size; ++i) {\n        float val = std::ceil(lst[i]);\n        sum += val * val;\n    }\n\n    return static_cast<uint64_t>(sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    for (; i < aligned_size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        // Round up using ceil\n        data = _mm256_round_ps(data, _MM_FROUND_CEIL);\n        // Square the elements\n        __m256 squared = _mm256_mul_ps(data, data);\n        // Accumulate the sum\n        sum_vec = _mm256_add_ps(sum_vec, squared);\n    }\n\n    // Horizontal sum of the vector\n    __m128 low = _mm256_extractf128_ps(sum_vec, 0);\n    __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n    low = _mm_add_ps(low, high);\n    __m128 shuf = _mm_movehdup_ps(low);\n    __m128 sums = _mm_add_ps(low, shuf);\n    shuf = _mm_movehl_ps(shuf, sums);\n    sums = _mm_add_ss(sums, shuf);\n    float sum = _mm_cvtss_f32(sums);\n\n    // Process remaining elements\n    for (; i < vec_size; ++i) {\n        float val = std::ceil(lst[i]);\n        sum += val * val;\n    }\n\n    return static_cast<uint64_t>(sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    const float* data = lst.data();\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        // Round up (ceil) each element\n        vec = _mm256_round_ps(vec, _MM_FROUND_CEIL);\n        // Square each element\n        vec = _mm256_mul_ps(vec, vec);\n        // Accumulate the squared values\n        sum_vec = _mm256_add_ps(sum_vec, vec);\n    }\n    \n    // Sum the elements of the sum_vec\n    __m128 low = _mm256_extractf128_ps(sum_vec, 0);\n    __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n    low = _mm_add_ps(low, high);\n    __m128 shuf = _mm_movehdup_ps(low);\n    __m128 sums = _mm_add_ps(low, shuf);\n    shuf = _mm_movehl_ps(shuf, sums);\n    sums = _mm_add_ss(sums, shuf);\n    float sum = _mm_cvtss_f32(sums);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = std::ceil(data[i]);\n        sum += val * val;\n    }\n    \n    return static_cast<uint64_t>(sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i = 0;\n    const size_t vec_size = lst.size();\n    const size_t aligned_size = vec_size - (vec_size % 8);\n\n    for (; i < aligned_size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        // Round up using ceil\n        data = _mm256_round_ps(data, _MM_FROUND_CEIL);\n        // Square the elements\n        __m256 squared = _mm256_mul_ps(data, data);\n        // Accumulate the squared values\n        sum_vec = _mm256_add_ps(sum_vec, squared);\n    }\n\n    // Horizontal sum of the vector\n    __m128 low = _mm256_extractf128_ps(sum_vec, 0);\n    __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n    low = _mm_add_ps(low, high);\n    __m128 shuf = _mm_movehdup_ps(low);\n    __m128 sums = _mm_add_ps(low, shuf);\n    shuf = _mm_movehl_ps(shuf, sums);\n    sums = _mm_add_ss(sums, shuf);\n    float sum = _mm_cvtss_f32(sums);\n\n    // Process remaining elements\n    for (; i < vec_size; ++i) {\n        float val = std::ceil(lst[i]);\n        sum += val * val;\n    }\n\n    return static_cast<uint64_t>(sum);\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    const size_t vec_size = 8; // AVX2 can process 8 ints at a time\n    const size_t n = arr.size();\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i prev = _mm256_loadu_si256((__m256i*)&arr[i-1]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (size_t j = 0; j < vec_size; ++j) {\n                if (arr[i-1 + j] > arr[i + j]) {\n                    result = i + j;\n                }\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i-1] > arr[i]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX2 can process 8 ints at a time\n    const size_t n = arr.size();\n\n    for (; i + vec_size < n; i += vec_size - 1) {\n        __m256i current = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i const*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (size_t j = 0; j < vec_size - 1; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    result = i + j + 1;\n                }\n            }\n        }\n    }\n\n    for (; i + 1 < n; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            result = i + 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load current and previous elements\n        __m256i curr = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        __m256i prev = _mm256_loadu_si256((__m256i const*)&arr[i-1]);\n\n        // Compare current <= previous\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        // Get mask of comparisons\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (1 << (j * 4))) {\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] <= arr[i-1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in blocks of 8 (AVX2 can handle 8 ints at once)\n    for (; i + 7 < size; i += 8) {\n        // Load current and previous elements\n        __m256i curr = _mm256_loadu_si256((__m256i const*)&arr[i]);\n        __m256i prev = _mm256_loadu_si256((__m256i const*)&arr[i-1]);\n\n        // Compare current <= previous\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        // Get the mask of the comparison result\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (1 << (j * 4))) { // Each int is 4 bytes\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] <= arr[i-1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load current and previous elements\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i prev = _mm256_loadu_si256((__m256i*)&arr[i-1]);\n\n        // Compare current <= previous\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        // Get mask of comparisons\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            int bit = __builtin_ctz(mask) / 4; // Each int is 4 bytes\n            result = i + bit;\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] <= arr[i-1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        \n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n        \n        __m256i neg_vals = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_vals = _mm256_and_si256(vec, pos_mask);\n        \n        neg_vals = _mm256_or_si256(neg_vals, _mm256_andnot_si256(neg_mask, _mm256_set1_epi32(std::numeric_limits<int>::min())));\n        pos_vals = _mm256_or_si256(pos_vals, _mm256_andnot_si256(pos_mask, _mm256_set1_epi32(std::numeric_limits<int>::max())));\n        \n        max_neg = _mm256_max_epi32(max_neg, neg_vals);\n        min_pos = _mm256_min_epi32(min_pos, pos_vals);\n    }\n\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int max_neg_result = std::numeric_limits<int>::min();\n    int min_pos_result = std::numeric_limits<int>::max();\n    \n    for (int j = 0; j < 8; ++j) {\n        if (max_neg_arr[j] > max_neg_result) {\n            max_neg_result = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] < min_pos_result) {\n            min_pos_result = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > max_neg_result) {\n            max_neg_result = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < min_pos_result) {\n            min_pos_result = lst[i];\n        }\n    }\n\n    if (max_neg_result == std::numeric_limits<int>::min()) {\n        max_neg_result = 0;\n    }\n    if (min_pos_result == std::numeric_limits<int>::max()) {\n        min_pos_result = 0;\n    }\n\n    return {max_neg_result, min_pos_result};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(0x80000000); // Initialize to smallest possible negative\n    __m256i min_pos = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to largest possible positive\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        \n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i neg_nums = _mm256_and_si256(vec, neg_mask);\n        __m256i current_max_neg = _mm256_max_epi32(max_neg, neg_nums);\n        max_neg = _mm256_blendv_epi8(max_neg, current_max_neg, neg_mask);\n\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n        __m256i pos_nums = _mm256_and_si256(vec, pos_mask);\n        __m256i current_min_pos = _mm256_min_epi32(min_pos, pos_nums);\n        min_pos = _mm256_blendv_epi8(min_pos, current_min_pos, pos_mask);\n    }\n\n    // Handle remaining elements\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int max_neg_scalar = 0x80000000;\n    int min_pos_scalar = 0x7FFFFFFF;\n\n    for (int j = 0; j < 8; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg_scalar) {\n            max_neg_scalar = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos_scalar) {\n            min_pos_scalar = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > max_neg_scalar) {\n            max_neg_scalar = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < min_pos_scalar) {\n            min_pos_scalar = lst[i];\n        }\n    }\n\n    if (max_neg_scalar == 0x80000000) max_neg_scalar = 0;\n    if (min_pos_scalar == 0x7FFFFFFF) min_pos_scalar = 0;\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        \n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n        \n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n        \n        __m256i neg_max_candidate = _mm256_max_epi32(neg_values, max_neg);\n        __m256i pos_min_candidate = _mm256_min_epi32(\n            _mm256_or_si256(pos_values, _mm256_set1_epi32(std::numeric_limits<int>::max())), \n            min_pos\n        );\n        \n        max_neg = _mm256_blendv_epi8(max_neg, neg_max_candidate, neg_mask);\n        min_pos = _mm256_blendv_epi8(min_pos, pos_min_candidate, pos_mask);\n    }\n\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int largest_neg = std::numeric_limits<int>::min();\n    int smallest_pos = std::numeric_limits<int>::max();\n\n    for (int j = 0; j < 8; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > largest_neg) {\n            largest_neg = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < smallest_pos) {\n            smallest_pos = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_neg) {\n            largest_neg = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_pos) {\n            smallest_pos = lst[i];\n        }\n    }\n\n    if (largest_neg == std::numeric_limits<int>::min()) {\n        largest_neg = 0;\n    }\n    if (smallest_pos == std::numeric_limits<int>::max()) {\n        smallest_pos = 0;\n    }\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(0x80000000); // Initialize to smallest possible int\n    __m256i min_pos = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to largest possible int\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        \n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i neg_nums = _mm256_and_si256(vec, neg_mask);\n        __m256i current_max_neg = _mm256_max_epi32(max_neg, neg_nums);\n        max_neg = _mm256_blendv_epi8(max_neg, current_max_neg, neg_mask);\n        \n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n        __m256i pos_nums = _mm256_and_si256(vec, pos_mask);\n        __m256i current_min_pos = _mm256_min_epi32(min_pos, pos_nums);\n        min_pos = _mm256_blendv_epi8(min_pos, current_min_pos, pos_mask);\n    }\n\n    // Extract results from SIMD registers\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    // Process remaining elements\n    int max_neg_scalar = 0x80000000;\n    int min_pos_scalar = 0x7FFFFFFF;\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0 && val > max_neg_scalar) {\n            max_neg_scalar = val;\n        } else if (val > 0 && val < min_pos_scalar) {\n            min_pos_scalar = val;\n        }\n    }\n\n    // Find the final max_neg and min_pos\n    for (int j = 0; j < 8; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg_scalar) {\n            max_neg_scalar = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos_scalar) {\n            min_pos_scalar = min_pos_arr[j];\n        }\n    }\n\n    // Handle cases where no negatives or positives were found\n    if (max_neg_scalar == 0x80000000) max_neg_scalar = 0;\n    if (min_pos_scalar == 0x7FFFFFFF) min_pos_scalar = 0;\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(0);\n    __m256i min_pos = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        \n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        __m256i neg_nums = _mm256_and_si256(vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, neg_nums);\n        \n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n        __m256i pos_nums = _mm256_and_si256(vec, pos_mask);\n        __m256i pos_nums_or_max = _mm256_or_si256(pos_nums, _mm256_andnot_si256(pos_mask, _mm256_set1_epi32(0x7FFFFFFF)));\n        min_pos = _mm256_min_epi32(min_pos, pos_nums_or_max);\n    }\n\n    // Extract results from SIMD registers\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int max_neg_val = *std::max_element(max_neg_arr, max_neg_arr + 8);\n    int min_pos_val = *std::min_element(min_pos_arr, min_pos_arr + 8);\n\n    // Handle remaining elements\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0) {\n            if (val > max_neg_val || max_neg_val == 0) {\n                max_neg_val = val;\n            }\n        } else if (val > 0) {\n            if (val < min_pos_val || min_pos_val == 0x7FFFFFFF) {\n                min_pos_val = val;\n            }\n        }\n    }\n\n    if (max_neg_val == 0) max_neg_val = 0;\n    if (min_pos_val == 0x7FFFFFFF) min_pos_val = 0;\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t current_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        current_fact *= i;\n        result *= current_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i;\n    const int vec_size = lst.size();\n    const int remainder = vec_size % 8;\n    const int aligned_size = vec_size - remainder;\n\n    for (i = 0; i < aligned_size; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod3_mask = _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, _mm256_set1_epi32(3)));\n        __m256i mod4_mask = _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, _mm256_set1_epi32(4)));\n        __m256i not_mod3_mask = _mm256_xor_si256(mod3_mask, _mm256_set1_epi32(-1));\n        __m256i mod4_not_mod3_mask = _mm256_and_si256(mod4_mask, not_mod3_mask);\n\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        __m256i cubed = _mm256_mullo_epi32(squared, data);\n\n        __m256i res = _mm256_blendv_epi8(\n            _mm256_blendv_epi8(data, squared, mod3_mask),\n            cubed,\n            mod4_not_mod3_mask\n        );\n\n        sum_vec = _mm256_add_epi32(sum_vec, res);\n    }\n\n    // Handle remaining elements\n    int sum = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < vec_size; ++i) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            val = val * val;\n        } else if (i % 4 == 0) {\n            val = val * val * val;\n        }\n        sum += val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n    const int vec_size = 8;\n    const int n = lst.size();\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n\n        // Create masks for multiples of 3 and 4\n        __m256i mod3_mask = _mm256_setzero_si256();\n        __m256i mod4_mask = _mm256_setzero_si256();\n        \n        // Calculate mod3 and mod4\n        for (int j = 0; j < vec_size; ++j) {\n            int idx = i + j;\n            if (idx % 3 == 0) mod3_mask = _mm256_insert_epi32(mod3_mask, -1, j);\n            if (idx % 4 == 0 && idx % 3 != 0) mod4_mask = _mm256_insert_epi32(mod4_mask, -1, j);\n        }\n\n        // Square elements where mod3_mask is set\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        __m256i res = _mm256_blendv_epi8(data, squared, mod3_mask);\n\n        // Cube elements where mod4_mask is set (data^3 = data * data * data)\n        __m256i cubed = _mm256_mullo_epi32(squared, data);\n        res = _mm256_blendv_epi8(res, cubed, mod4_mask);\n\n        // Accumulate the results\n        sum_vec = _mm256_add_epi32(sum_vec, res);\n    }\n\n    // Horizontal sum of the vector\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n              sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            val = val * val;\n        } else if (i % 4 == 0) {\n            val = val * val * val;\n        }\n        sum += val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i;\n    const int vec_size = 8;\n    const int n = lst.size();\n    \n    for (i = 0; i + vec_size <= n; i += vec_size) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        \n        // Check multiples of 3\n        __m256i mod3 = _mm256_rem_epi32(indices, _mm256_set1_epi32(3));\n        __m256i mask3 = _mm256_cmpeq_epi32(mod3, _mm256_setzero_si256());\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        __m256i res3 = _mm256_blendv_epi8(data, squared, mask3);\n        \n        // Check multiples of 4 but not 3\n        __m256i mod4 = _mm256_rem_epi32(indices, _mm256_set1_epi32(4));\n        __m256i mask4 = _mm256_andnot_si256(mask3, \n                          _mm256_cmpeq_epi32(mod4, _mm256_setzero_si256()));\n        __m256i cubed = _mm256_mullo_epi32(squared, data);\n        __m256i res = _mm256_blendv_epi8(res3, cubed, mask4);\n        \n        sum_vec = _mm256_add_epi32(sum_vec, res);\n    }\n    \n    // Horizontal sum\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n              sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            val = val * val;\n        } else if (i % 4 == 0) {\n            val = val * val * val;\n        }\n        sum += val;\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    const int vec_size = lst.size();\n    const int remainder = vec_size % 8;\n    const int aligned_size = vec_size - remainder;\n\n    for (int i = 0; i < aligned_size; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i mod4 = _mm256_set1_epi32(4);\n        \n        __m256i mod3_mask = _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, mod3));\n        __m256i mod4_mask = _mm256_cmpeq_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, mod4));\n        __m256i not_mod3_mask = _mm256_xor_si256(mod3_mask, _mm256_set1_epi32(-1));\n        __m256i mod4_not_mod3_mask = _mm256_and_si256(mod4_mask, not_mod3_mask);\n\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        __m256i square = _mm256_mullo_epi32(data, data);\n        __m256i cube = _mm256_mullo_epi32(square, data);\n        \n        __m256i res = _mm256_blendv_epi8(\n            data,\n            _mm256_blendv_epi8(data, cube, mod4_not_mod3_mask),\n            mod3_mask\n        );\n\n        sum_vec = _mm256_add_epi32(sum_vec, res);\n    }\n\n    int sum = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (int i = aligned_size; i < vec_size; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i;\n    const int vec_size = 8;\n    const int n = lst.size();\n\n    for (i = 0; i + vec_size <= n; i += vec_size) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n\n        // Check multiples of 3\n        __m256i mod3_mask = _mm256_cmpeq_epi32(\n            _mm256_setzero_si256(),\n            _mm256_and_si256(indices, _mm256_set1_epi32(3 - 1))\n        );\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        data = _mm256_blendv_epi8(data, squared, mod3_mask);\n\n        // Check multiples of 4 but not 3\n        __m256i mod4_mask = _mm256_cmpeq_epi32(\n            _mm256_setzero_si256(),\n            _mm256_and_si256(indices, _mm256_set1_epi32(4 - 1))\n        );\n        __m256i not_mod3_mask = _mm256_xor_si256(mod3_mask, _mm256_set1_epi32(-1));\n        __m256i mod4_not_mod3_mask = _mm256_and_si256(mod4_mask, not_mod3_mask);\n        __m256i cubed = _mm256_mullo_epi32(squared, data);\n        data = _mm256_blendv_epi8(data, cubed, mod4_not_mod3_mask);\n\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Sum the vector\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + \n              sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            val = val * val;\n        } else if (i % 4 == 0) {\n            val = val * val * val;\n        }\n        sum += val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mask = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i const*)&nums[i]);\n        \n        // Check if numbers are greater than 10\n        __m256i v_cmp_gt = _mm256_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values for digit checks\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m256i v_last_digit = _mm256_rem_epi32(v_abs, _mm256_set1_epi32(10));\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n        __m256i v_last_check = _mm256_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m256i v_first = v_abs;\n        __m256i v_div = _mm256_set1_epi32(10);\n        __m256i v_first_digit = v_abs;\n        while (1) {\n            __m256i v_next = _mm256_div_epi32(v_first, v_div);\n            __m256i v_cmp = _mm256_cmpgt_epi32(v_next, v_zero);\n            int mask = _mm256_movemask_epi8(v_cmp);\n            if (!mask) break;\n            v_first = v_next;\n            v_first_digit = v_next;\n        }\n        __m256i v_first_odd = _mm256_and_si256(v_first_digit, v_one);\n        __m256i v_first_check = _mm256_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine all conditions\n        __m256i v_cond = _mm256_and_si256(v_cmp_gt, v_last_check);\n        v_cond = _mm256_and_si256(v_cond, v_first_check);\n        \n        // Count the number of elements that meet all conditions\n        int mask = _mm256_movemask_epi8(v_cond);\n        count += _mm_popcnt_u32(mask) / 4; // Each int is 4 bytes\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mask = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i const*)&nums[i]);\n        \n        // Check if greater than 10\n        __m256i v_gt_ten = _mm256_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute value for digit checks\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m256i v_last_digit = _mm256_rem_epi32(v_abs, _mm256_set1_epi32(10));\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n        __m256i v_last_odd_mask = _mm256_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m256i v_first = v_abs;\n        __m256i v_div = _mm256_set1_epi32(10);\n        __m256i v_first_digit = v_first;\n        while (1) {\n            __m256i v_next = _mm256_div_epi32(v_first, v_div);\n            __m256i v_cmp = _mm256_cmpgt_epi32(v_next, v_zero);\n            int mask = _mm256_movemask_epi8(v_cmp);\n            if (!mask) break;\n            v_first = v_next;\n        }\n        __m256i v_first_odd = _mm256_and_si256(v_first, v_one);\n        __m256i v_first_odd_mask = _mm256_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m256i v_cond = _mm256_and_si256(v_gt_ten, _mm256_and_si256(v_last_odd_mask, v_first_odd_mask));\n        int mask = _mm256_movemask_epi8(v_cond);\n        count += _mm_popcnt_u32(mask) / 4; // Each int is 4 bytes\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mask = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        \n        // Check if greater than 10\n        __m256i v_cmp = _mm256_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m256i v_last_digit = _mm256_rem_epi32(v_abs, _mm256_set1_epi32(10));\n        \n        // Check if last digit is odd\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n        v_last_odd = _mm256_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m256i v_first = v_abs;\n        __m256i v_divisor = _mm256_set1_epi32(10);\n        while (1) {\n            __m256i v_next = _mm256_div_epi32(v_first, v_divisor);\n            __m256i v_cmp_zero = _mm256_cmpeq_epi32(v_next, v_zero);\n            if (_mm256_testz_si256(v_cmp_zero, v_mask) == 1) break;\n            v_first = v_next;\n        }\n        __m256i v_first_odd = _mm256_and_si256(v_first, v_one);\n        v_first_odd = _mm256_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m256i v_cond = _mm256_and_si256(v_cmp, _mm256_and_si256(v_last_odd, v_first_odd));\n        \n        // Count the number of set bits in the mask\n        int mask = _mm256_movemask_epi8(v_cond);\n        count += _mm_popcnt_u32(mask) / 4; // Each int is 4 bytes\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_result = _mm256_setzero_si256();\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i const*)&nums[i]);\n        \n        // Check if numbers are greater than 10\n        __m256i v_gt_ten = _mm256_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m256i v_last_digit = _mm256_rem_epi32(v_abs, _mm256_set1_epi32(10));\n        \n        // Check if last digit is odd\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n        v_last_odd = _mm256_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m256i v_first_digit = v_abs;\n        __m256i v_divisor = _mm256_set1_epi32(10);\n        __m256i v_has_digits = _mm256_cmpgt_epi32(v_first_digit, _mm256_set1_epi32(9));\n        \n        while (_mm256_movemask_epi8(v_has_digits)) {\n            __m256i v_next = _mm256_div_epi32(v_first_digit, v_divisor);\n            v_has_digits = _mm256_cmpgt_epi32(v_next, v_zero);\n            v_first_digit = _mm256_blendv_epi8(v_first_digit, v_next, v_has_digits);\n        }\n        \n        // Check if first digit is odd\n        __m256i v_first_odd = _mm256_and_si256(v_first_digit, v_one);\n        v_first_odd = _mm256_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m256i v_cond = _mm256_and_si256(v_gt_ten, _mm256_and_si256(v_first_odd, v_last_odd));\n        v_result = _mm256_add_epi32(v_result, _mm256_and_si256(v_cond, v_one));\n    }\n\n    // Sum the results from the vector\n    alignas(32) int res_arr[8];\n    _mm256_store_si256((__m256i*)res_arr, v_result);\n    for (int j = 0; j < 8; ++j) {\n        count += res_arr[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mask = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        \n        // Check if numbers > 10\n        __m256i v_gt_ten = _mm256_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values for digit checks\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m256i v_last_digit = _mm256_rem_epi32(v_abs, _mm256_set1_epi32(10));\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n        __m256i v_last_odd_mask = _mm256_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m256i v_first = v_abs;\n        __m256i v_div = _mm256_set1_epi32(10);\n        __m256i v_first_digit = v_first;\n        __m256i v_gt_ten_div = _mm256_cmpgt_epi32(v_first, _mm256_set1_epi32(9));\n        while (_mm256_movemask_epi8(v_gt_ten_div)) {\n            v_first = _mm256_div_epi32(v_first, v_div);\n            v_first_digit = _mm256_blendv_epi8(v_first_digit, v_first, v_gt_ten_div);\n            v_gt_ten_div = _mm256_cmpgt_epi32(v_first, _mm256_set1_epi32(9));\n        }\n        __m256i v_first_odd = _mm256_and_si256(v_first_digit, v_one);\n        __m256i v_first_odd_mask = _mm256_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m256i v_cond = _mm256_and_si256(v_gt_ten, _mm256_and_si256(v_last_odd_mask, v_first_odd_mask));\n        int mask = _mm256_movemask_epi8(v_cond);\n        count += _mm_popcnt_u32(mask) / 4; // Each int is 4 bytes\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit > 9) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t rem0 = 0, rem1 = 0, rem2 = 0;\n\n    __m256i v_i, v_sq, v_res, v_rem;\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    uint64_t i;\n    for (i = 1; i <= n - 3; i += 4) {\n        v_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        v_sq = _mm256_mullo_epi64(v_i, v_i);\n        v_res = _mm256_sub_epi64(v_sq, v_i);\n        v_res = _mm256_add_epi64(v_res, v_one);\n        v_rem = _mm256_rem_epu64(v_res, v_three);\n\n        uint64_t rems[4];\n        _mm256_storeu_si256((__m256i*)rems, v_rem);\n\n        for (int j = 0; j < 4; ++j) {\n            if (rems[j] == 0) rem0++;\n            else if (rems[j] == 1) rem1++;\n            else rem2++;\n        }\n    }\n\n    for (; i <= n; ++i) {\n        uint64_t val = i * i - i + 1;\n        uint64_t rem = val % 3;\n        if (rem == 0) rem0++;\n        else if (rem == 1) rem1++;\n        else rem2++;\n    }\n\n    count = rem0 * rem1 * rem2;\n    count += rem0 * (rem0 - 1) * (rem0 - 2) / 6;\n    count += rem1 * (rem1 - 1) * (rem1 - 2) / 6;\n    count += rem2 * (rem2 - 1) * (rem2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t rem0 = 0, rem1 = 0, rem2 = 0;\n\n    __m256i v_i, v_sq, v_res, v_rem;\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    uint64_t i;\n    for (i = 1; i <= n - 3; i += 4) {\n        v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n        v_sq = _mm256_mullo_epi64(v_i, v_i);\n        v_res = _mm256_sub_epi64(v_sq, v_i);\n        v_res = _mm256_add_epi64(v_res, v_one);\n        v_rem = _mm256_rem_epu64(v_res, v_three);\n\n        uint64_t rem[4];\n        _mm256_storeu_si256((__m256i*)rem, v_rem);\n\n        for (int j = 0; j < 4; ++j) {\n            if (rem[j] == 0) rem0++;\n            else if (rem[j] == 1) rem1++;\n            else rem2++;\n        }\n    }\n\n    for (; i <= n; ++i) {\n        uint64_t val = i * i - i + 1;\n        uint64_t r = val % 3;\n        if (r == 0) rem0++;\n        else if (r == 1) rem1++;\n        else rem2++;\n    }\n\n    count = rem0 * rem1 * rem2 + rem0 * (rem0 - 1) * (rem0 - 2) / 6 + \n            rem1 * (rem1 - 1) * (rem1 - 2) / 6 + rem2 * (rem2 - 1) * (rem2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t rem0 = 0, rem1 = 0, rem2 = 0;\n\n    __m256i v_i, v_sq, v_res, v_rem;\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    uint64_t i;\n    for (i = 1; i <= n - 3; i += 4) {\n        v_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        v_sq = _mm256_mullo_epi64(v_i, v_i);\n        v_res = _mm256_sub_epi64(v_sq, v_i);\n        v_res = _mm256_add_epi64(v_res, v_one);\n        v_rem = _mm256_rem_epu64(v_res, v_three);\n\n        uint64_t rems[4];\n        _mm256_storeu_si256((__m256i*)rems, v_rem);\n\n        for (int j = 0; j < 4; ++j) {\n            if (rems[j] == 0) rem0++;\n            else if (rems[j] == 1) rem1++;\n            else rem2++;\n        }\n    }\n\n    for (; i <= n; ++i) {\n        uint64_t val = i * i - i + 1;\n        uint64_t rem = val % 3;\n        if (rem == 0) rem0++;\n        else if (rem == 1) rem1++;\n        else rem2++;\n    }\n\n    count = rem0 * rem1 * rem2;\n    count += rem0 * (rem0 - 1) * (rem0 - 2) / 6;\n    count += rem1 * (rem1 - 1) * (rem1 - 2) / 6;\n    count += rem2 * (rem2 - 1) * (rem2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t rem0 = 0, rem1 = 0, rem2 = 0;\n\n    __m256i v_i, v_sq, v_res, v_rem;\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    uint64_t i;\n    for (i = 1; i <= n - 3; i += 4) {\n        v_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        v_sq = _mm256_mullo_epi64(v_i, v_i);\n        v_res = _mm256_sub_epi64(v_sq, v_i);\n        v_res = _mm256_add_epi64(v_res, v_one);\n        v_rem = _mm256_rem_epu64(v_res, v_three);\n\n        uint64_t rems[4];\n        _mm256_storeu_si256((__m256i*)rems, v_rem);\n\n        for (int j = 0; j < 4; ++j) {\n            if (rems[j] == 0) rem0++;\n            else if (rems[j] == 1) rem1++;\n            else rem2++;\n        }\n    }\n\n    for (; i <= n; ++i) {\n        uint64_t val = i * i - i + 1;\n        uint64_t rem = val % 3;\n        if (rem == 0) rem0++;\n        else if (rem == 1) rem1++;\n        else rem2++;\n    }\n\n    count = rem0 * rem1 * rem2;\n    count += rem0 * (rem0 - 1) * (rem0 - 2) / 6;\n    count += rem1 * (rem1 - 1) * (rem1 - 2) / 6;\n    count += rem2 * (rem2 - 1) * (rem2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t rem0 = 0, rem1 = 0, rem2 = 0;\n\n    __m256i v_i, v_sq, v_res, v_rem;\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    uint64_t i;\n    for (i = 1; i <= n - 3; i += 4) {\n        v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n        v_sq = _mm256_mullo_epi64(v_i, v_i);\n        v_res = _mm256_sub_epi64(v_sq, v_i);\n        v_res = _mm256_add_epi64(v_res, v_one);\n        v_rem = _mm256_rem_epu64(v_res, v_three);\n\n        uint64_t rem[4];\n        _mm256_storeu_si256((__m256i*)rem, v_rem);\n\n        for (int j = 0; j < 4; ++j) {\n            if (rem[j] == 0) rem0++;\n            else if (rem[j] == 1) rem1++;\n            else rem2++;\n        }\n    }\n\n    for (; i <= n; ++i) {\n        uint64_t val = i * i - i + 1;\n        uint64_t r = val % 3;\n        if (r == 0) rem0++;\n        else if (r == 1) rem1++;\n        else rem2++;\n    }\n\n    count = rem0 * rem1 * rem2;\n    count += rem0 * (rem0 - 1) * (rem0 - 2) / 6;\n    count += rem1 * (rem1 - 1) * (rem1 - 2) / 6;\n    count += rem2 * (rem2 - 1) * (rem2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i rem = _mm256_rem_epi32(n_vec, divisors);\n\n    int is_prime = 1;\n    for (int i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            is_prime = 0;\n            break;\n        }\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(rem, _mm256_setzero_si256()));\n    if (mask != 0) {\n        is_prime = 0;\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i remainders = _mm256_rem_epi32(n_vec, divisors);\n\n    int is_prime = 1;\n    int max_divisor = (int)sqrt(n) + 1;\n\n    for (int i = 0; i < 8; ++i) {\n        int divisor = _mm256_extract_epi32(divisors, i);\n        if (divisor >= max_divisor) {\n            break;\n        }\n        int remainder = _mm256_extract_epi32(remainders, i);\n        if (remainder == 0) {\n            is_prime = 0;\n            break;\n        }\n    }\n\n    if (is_prime) {\n        for (int d = 19; d <= max_divisor; d += 2) {\n            if (n % d == 0) {\n                is_prime = 0;\n                break;\n            }\n        }\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i remainders = _mm256_rem_epi32(n_vec, divisors);\n\n    int is_prime = 1;\n    int max_divisor = (int)sqrt(n) + 1;\n\n    for (int i = 0; i < 8; ++i) {\n        int divisor = _mm256_extract_epi32(divisors, i);\n        if (divisor >= max_divisor) {\n            break;\n        }\n        int remainder = _mm256_extract_epi32(remainders, i);\n        if (remainder == 0) {\n            is_prime = 0;\n            break;\n        }\n    }\n\n    if (is_prime) {\n        for (int i = 19; i <= max_divisor; i += 2) {\n            if (n % i == 0) {\n                is_prime = 0;\n                break;\n            }\n        }\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i remainders = _mm256_rem_epi32(n_vec, divisors);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i cmp = _mm256_cmpeq_epi32(remainders, zero);\n    int mask = _mm256_movemask_epi8(cmp);\n\n    if (mask != 0) {\n        return y;\n    }\n\n    for (int i = 19; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return y;\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n <= 3) {\n        return x;\n    }\n    if (n % 2 == 0 || n % 3 == 0) {\n        return y;\n    }\n\n    __m256i divisors = _mm256_setr_epi32(5, 7, 11, 13, 17, 19, 23, 29);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (int i = 5; i * i <= n; i += 30) {\n        __m256i current_div = _mm256_add_epi32(divisors, _mm256_set1_epi32(i));\n        __m256i remainder = _mm256_rem_epi32(n_vec, current_div);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, zeros);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    const float* data = lst.data();\n    size_t i = 0;\n    size_t size = lst.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        \n        // Check for positive numbers (vec > 0)\n        __m256 mask_pos = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        // Check for integers (vec == floor(vec))\n        __m256 floor_vec = _mm256_floor_ps(vec);\n        __m256 mask_int = _mm256_cmp_ps(vec, floor_vec, _CMP_EQ_OQ);\n        \n        // Check for odd numbers (int(vec) % 2 != 0)\n        __m256i int_vec = _mm256_cvttps_epi32(vec);\n        __m256i mod_mask = _mm256_set1_epi32(1);\n        __m256i mod_result = _mm256_and_si256(int_vec, mod_mask);\n        __m256 mask_odd = _mm256_castsi256_ps(_mm256_cmpgt_epi32(mod_result, _mm256_setzero_si256()));\n        \n        // Combine all masks\n        __m256 mask = _mm256_and_ps(mask_pos, _mm256_and_ps(mask_int, mask_odd));\n        \n        // Square the numbers and apply mask\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        squared = _mm256_and_ps(squared, mask);\n        \n        // Convert to 32-bit integers and accumulate\n        __m256i squared_int = _mm256_cvttps_epi32(squared);\n        sum = _mm256_add_epi32(sum, squared_int);\n    }\n\n    // Horizontal sum of the vector\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    uint64_t result = _mm_extract_epi32(sum128, 0);\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && val == floor(val) && (int(val) % 2 != 0)) {\n            result += val * val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    const float* data = lst.data();\n    size_t size = lst.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        \n        // Check if numbers are integers (fractional part is zero)\n        __m256 int_part = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        __m256 mask_int = _mm256_cmp_ps(vec, int_part, _CMP_EQ_OQ);\n        \n        // Check if numbers are positive\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask_pos = _mm256_cmp_ps(vec, zero, _CMP_GE_OQ);\n        \n        // Check if numbers are odd (integer & (x % 2) == 1)\n        __m256 two = _mm256_set1_ps(2.0f);\n        __m256 mod = _mm256_sub_ps(int_part, _mm256_mul_ps(_mm256_floor_ps(_mm256_div_ps(int_part, two)), two));\n        __m256 mask_odd = _mm256_cmp_ps(mod, _mm256_set1_ps(1.0f), _CMP_EQ_OQ);\n        \n        // Combine masks: positive, integer, and odd\n        __m256 mask = _mm256_and_ps(mask_int, _mm256_and_ps(mask_pos, mask_odd));\n        \n        // Square the numbers and apply mask\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        squared = _mm256_and_ps(squared, mask);\n        \n        // Convert to 64-bit integers and accumulate\n        __m256i squared_i32 = _mm256_cvtps_epi32(squared);\n        __m256i squared_i64_lo = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(squared_i32, 0));\n        __m256i squared_i64_hi = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(squared_i32, 1));\n        \n        sum = _mm256_add_epi64(sum, squared_i64_lo);\n        sum = _mm256_add_epi64(sum, squared_i64_hi);\n    }\n\n    // Horizontal sum of the 256-bit vector\n    __m128i sum128 = _mm_add_epi64(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    uint64_t result = _mm_extract_epi64(sum128, 0) + _mm_extract_epi64(sum128, 1);\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float num = lst[i];\n        if (num >= 0 && num == (int)num && (int)num % 2 != 0) {\n            result += (uint64_t)(num * num);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    const float* data = lst.data();\n    size_t i = 0;\n    size_t size = lst.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        \n        // Check for positive numbers (vec > 0)\n        __m256 mask_pos = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        // Check for integers (vec == floor(vec))\n        __m256 floor_vec = _mm256_floor_ps(vec);\n        __m256 mask_int = _mm256_cmp_ps(vec, floor_vec, _CMP_EQ_OQ);\n        \n        // Check for odd numbers (int(vec) % 2 != 0)\n        __m256i int_vec = _mm256_cvtps_epi32(floor_vec);\n        __m256i mod_mask = _mm256_set1_epi32(1);\n        __m256i mod_result = _mm256_and_si256(int_vec, mod_mask);\n        __m256 mask_odd = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mod_result, mod_mask));\n        \n        // Combine all masks\n        __m256 mask = _mm256_and_ps(mask_pos, _mm256_and_ps(mask_int, mask_odd));\n        \n        // Square the numbers and apply mask\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        squared = _mm256_and_ps(squared, mask);\n        \n        // Convert to integers and accumulate\n        __m256i squared_int = _mm256_cvtps_epi32(squared);\n        sum = _mm256_add_epi32(sum, squared_int);\n    }\n\n    // Horizontal sum of the vector\n    __m128i sum128 = _mm_add_epi32(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum128 = _mm_add_epi32(sum128, _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2)));\n    uint64_t result = _mm_extract_epi32(sum128, 0);\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && val == floor(val) && (int(val) % 2 != 0)) {\n            result += val * val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256i mask_odd = _mm256_set1_epi32(1);\n    __m256i mask_positive = _mm256_set1_epi32(0x7FFFFFFF);\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Check positive (>= 0)\n        __m256 cmp_pos = _mm256_cmp_ps(vec, zero, _CMP_GE_OQ);\n        __m256i pos_mask = _mm256_castps_si256(cmp_pos);\n        \n        // Check integer (vec == floor(vec))\n        __m256 floor_vec = _mm256_floor_ps(vec);\n        __m256 cmp_int = _mm256_cmp_ps(vec, floor_vec, _CMP_EQ_OQ);\n        __m256i int_mask = _mm256_castps_si256(cmp_int);\n        \n        // Combine masks (positive AND integer)\n        __m256i combined_mask = _mm256_and_si256(pos_mask, int_mask);\n        \n        // Convert to integers for odd check\n        __m256i vec_int = _mm256_cvtps_epi32(vec);\n        __m256i odd_check = _mm256_and_si256(vec_int, mask_odd);\n        __m256i is_odd = _mm256_cmpeq_epi32(odd_check, mask_odd);\n        \n        // Final mask (combined_mask AND is_odd)\n        __m256i final_mask = _mm256_and_si256(combined_mask, is_odd);\n        \n        // Square the values\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256i squared_int = _mm256_cvtps_epi32(squared);\n        \n        // Apply mask\n        __m256i masked = _mm256_and_si256(squared_int, final_mask);\n        \n        // Accumulate\n        sum = _mm256_add_epi32(sum, masked);\n    }\n\n    // Horizontal sum\n    uint32_t result_arr[8];\n    _mm256_storeu_si256((__m256i*)result_arr, sum);\n    uint64_t result = 0;\n    for (int j = 0; j < 8; ++j) {\n        result += result_arr[j];\n    }\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == floor(val) && ((int)val) % 2 == 1) {\n            result += val * val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256i mask_odd = _mm256_set1_epi32(1);\n    __m256 mask_positive = _mm256_set1_ps(0.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Check for positive numbers (vec > 0)\n        __m256 cmp_pos = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        \n        // Check for integers (vec == floor(vec))\n        __m256 floor_vec = _mm256_floor_ps(vec);\n        __m256 cmp_int = _mm256_cmp_ps(vec, floor_vec, _CMP_EQ_OQ);\n        \n        // Combine conditions (positive and integer)\n        __m256 mask = _mm256_and_ps(cmp_pos, cmp_int);\n        \n        // Convert to integers\n        __m256i int_vec = _mm256_cvtps_epi32(floor_vec);\n        \n        // Check for odd numbers (int_vec & 1)\n        __m256i odd = _mm256_and_si256(int_vec, mask_odd);\n        __m256i cmp_odd = _mm256_cmpeq_epi32(odd, mask_odd);\n        \n        // Apply mask to odd numbers\n        __m256i masked_odd = _mm256_and_si256(int_vec, cmp_odd);\n        \n        // Square the numbers\n        __m256i squared = _mm256_mullo_epi32(masked_odd, masked_odd);\n        \n        // Convert mask to epi32 for blending\n        __m256i mask_epi32 = _mm256_castps_si256(mask);\n        __m256i masked_result = _mm256_and_si256(squared, mask_epi32);\n        \n        // Accumulate the sum\n        sum = _mm256_add_epi64(sum, _mm256_cvtepu32_epi64(_mm256_extracti128_si256(masked_result, 0)));\n        sum = _mm256_add_epi64(sum, _mm256_cvtepu32_epi64(_mm256_extracti128_si256(masked_result, 1)));\n    }\n\n    // Horizontal sum of the 256-bit register\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    uint64_t total = result[0] + result[1] + result[2] + result[3];\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val > 0 && val == floor(val)) {\n            int int_val = (int)val;\n            if (int_val % 2 != 0) {\n                total += int_val * int_val;\n            }\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i const*)(game.data() + i));\n        __m256i guess_vec = _mm256_loadu_si256((__m256i const*)(guess.data() + i));\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i const*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i const*)&guess[i]);\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i const*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i const*)&guess[i]);\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i const*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i const*)&guess[i]);\n        \n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        \n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i const*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i const*)&guess[i]);\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}

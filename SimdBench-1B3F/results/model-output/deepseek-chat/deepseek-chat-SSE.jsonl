{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const*)(dst + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const*)(dst + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const*)(dst + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const*)(dst + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const*)(dst + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    __m128i seven_vec = _mm_set1_epi32(7);\n    \n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i tmp1_mul_tmp2 = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, tmp1_mul_tmp2);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    __m128i seven_vec = _mm_set1_epi32(7);\n    __m128i three_vec = _mm_set1_epi32(3);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i tmp_prod = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, tmp_prod);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    __m128i seven_vec = _mm_set1_epi32(7);\n    \n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i tmp1_mul_tmp2 = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, tmp1_mul_tmp2);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    __m128i seven_vec = _mm_set1_epi32(7);\n    \n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i tmp_prod = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, tmp_prod);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    __m128i seven_vec = _mm_set1_epi32(7);\n    \n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i tmp_prod = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, tmp_prod);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for (r = 0; r + 1 < rows; r += 2) {\n        __m128d v = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n        _mm_store_sd(&dst[r], v);\n        _mm_storeh_pd(&dst[r + 1], v);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for (r = 0; r + 1 < rows; r += 2) {\n        __m128d val = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n        _mm_store_pd(&dst[r], val);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for (r = 0; r + 1 < rows; r += 2) {\n        __m128d val = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n        _mm_store_sd(&dst[r], val);\n        _mm_storeh_pd(&dst[r + 1], val);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for (r = 0; r + 1 < rows; r += 2) {\n        __m128d val = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n        _mm_store_sd(&dst[r], val);\n        _mm_storeh_pd(&dst[r + 1], val);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 1 < rows; i += 2) {\n        __m128d v = _mm_set_pd(src[(i + 1) * cols + stride], src[i * cols + stride]);\n        _mm_store_pd(&dst[i], v);\n    }\n    \n    // Handle remaining elements if rows is odd\n    if (i < rows) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load indices\n        __m128i idx = _mm_loadu_si128((__m128i const*)(indices + i));\n        \n        // Gather operation\n        __m128 gathered = _mm_set_ps(\n            src[indices[i + 3]],\n            src[indices[i + 2]],\n            src[indices[i + 1]],\n            src[indices[i]]\n        );\n        _mm_storeu_ps(dst + i, gathered);\n\n        // Scatter operation\n        __m128 src_data = _mm_loadu_ps(src + i);\n        _mm_store_ps(dst + indices[i], src_data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        // Gather\n        dst[i] = src[indices[i]];\n        // Scatter\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load indices\n        __m128i idx = _mm_loadu_si128((__m128i const*)(indices + i));\n        \n        // Gather operation\n        __m128 gathered = _mm_set_ps(\n            src[indices[i + 3]],\n            src[indices[i + 2]],\n            src[indices[i + 1]],\n            src[indices[i]]\n        );\n        _mm_storeu_ps(dst + i, gathered);\n        \n        // Scatter operation\n        __m128 src_data = _mm_loadu_ps(src + i);\n        _mm_store_ps(dst + indices[i], src_data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        // Gather\n        dst[i] = src[indices[i]];\n        // Scatter\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(indices + i));\n        __m128 src_val = _mm_set_ps(src[i+3], src[i+2], src[i+1], src[i]);\n        \n        // Gather operation\n        float temp[4];\n        temp[0] = src[indices[i]];\n        temp[1] = src[indices[i+1]];\n        temp[2] = src[indices[i+2]];\n        temp[3] = src[indices[i+3]];\n        __m128 gathered = _mm_loadu_ps(temp);\n        _mm_storeu_ps(dst + i, gathered);\n        \n        // Scatter operation\n        dst[indices[i]] = src[i];\n        dst[indices[i+1]] = src[i+1];\n        dst[indices[i+2]] = src[i+2];\n        dst[indices[i+3]] = src[i+3];\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load indices\n        __m128i idx = _mm_loadu_si128((__m128i const*)(indices + i));\n        \n        // Gather operation\n        __m128 gathered;\n        gathered = _mm_set_ps(\n            src[indices[i + 3]],\n            src[indices[i + 2]],\n            src[indices[i + 1]],\n            src[indices[i]]\n        );\n        _mm_storeu_ps(dst + i, gathered);\n        \n        // Scatter operation\n        __m128 src_data = _mm_loadu_ps(src + i);\n        float tmp[4];\n        _mm_storeu_ps(tmp, src_data);\n        dst[indices[i]] = tmp[0];\n        dst[indices[i + 1]] = tmp[1];\n        dst[indices[i + 2]] = tmp[2];\n        dst[indices[i + 3]] = tmp[3];\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        // Gather\n        dst[i] = src[indices[i]];\n        // Scatter\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load indices\n        __m128i idx = _mm_loadu_si128((__m128i const*)(indices + i));\n        \n        // Gather operation\n        __m128 gathered;\n        gathered = _mm_set_ps(\n            src[indices[i + 3]],\n            src[indices[i + 2]],\n            src[indices[i + 1]],\n            src[indices[i]]\n        );\n        _mm_storeu_ps(dst + i, gathered);\n        \n        // Scatter operation\n        __m128 src_val = _mm_loadu_ps(src + i);\n        dst[indices[i]] = _mm_cvtss_f32(src_val);\n        dst[indices[i + 1]] = _mm_cvtss_f32(_mm_shuffle_ps(src_val, src_val, _MM_SHUFFLE(1, 1, 1, 1)));\n        dst[indices[i + 2]] = _mm_cvtss_f32(_mm_shuffle_ps(src_val, src_val, _MM_SHUFFLE(2, 2, 2, 2)));\n        dst[indices[i + 3]] = _mm_cvtss_f32(_mm_shuffle_ps(src_val, src_val, _MM_SHUFFLE(3, 3, 3, 3)));\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        // Gather\n        dst[i] = src[indices[i]];\n        // Scatter\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m128i left = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i right = _mm_loadu_si128((__m128i*)(arr + end - 7));\n        \n        // Reverse the 8 elements in each vector\n        right = _mm_shufflehi_epi16(_mm_shufflelo_epi16(right, 0x1B), 0x1B);\n        right = _mm_shuffle_epi32(right, 0x4E);\n        \n        left = _mm_shufflehi_epi16(_mm_shufflelo_epi16(left, 0x1B), 0x1B);\n        left = _mm_shuffle_epi32(left, 0x4E);\n        \n        _mm_storeu_si128((__m128i*)(arr + start), right);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), left);\n        \n        start += 8;\n        end -= 8;\n    }\n    \n    // Handle remaining elements (less than 8)\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m128i left = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i right = _mm_loadu_si128((__m128i*)(arr + end - 7));\n        \n        // Reverse the 8 elements in each vector\n        right = _mm_shufflelo_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shufflehi_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(2, 3, 0, 1));\n        \n        left = _mm_shufflelo_epi16(left, _MM_SHUFFLE(0, 1, 2, 3));\n        left = _mm_shufflehi_epi16(left, _MM_SHUFFLE(0, 1, 2, 3));\n        left = _mm_shuffle_epi32(left, _MM_SHUFFLE(2, 3, 0, 1));\n        \n        // Swap the vectors\n        _mm_storeu_si128((__m128i*)(arr + start), right);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), left);\n        \n        start += 8;\n        end -= 8;\n    }\n    \n    // Handle remaining elements (less than 8) with scalar code\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m128i left = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i right = _mm_loadu_si128((__m128i*)(arr + end - 7));\n        \n        // Reverse the elements in the right vector\n        right = _mm_shufflehi_epi16(_mm_shufflelo_epi16(right, 0x1B), 0x1B);\n        right = _mm_shuffle_epi32(right, 0x4E);\n        \n        // Reverse the elements in the left vector\n        left = _mm_shufflehi_epi16(_mm_shufflelo_epi16(left, 0x1B), 0x1B);\n        left = _mm_shuffle_epi32(left, 0x4E);\n        \n        _mm_storeu_si128((__m128i*)(arr + start), right);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), left);\n        \n        start += 8;\n        end -= 8;\n    }\n    \n    // Handle remaining elements (less than 8)\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        // Load 8 elements from start and end positions\n        __m128i start_vec = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i end_vec = _mm_loadu_si128((__m128i*)(arr + end - 7));\n\n        // Reverse the elements in the vectors\n        end_vec = _mm_shufflehi_epi16(_mm_shufflelo_epi16(end_vec, 0x1B), 0x1B);\n        end_vec = _mm_shuffle_epi32(end_vec, 0x4E);\n\n        // Swap the vectors\n        _mm_storeu_si128((__m128i*)(arr + start), end_vec);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), start_vec);\n\n        start += 8;\n        end -= 8;\n    }\n\n    // Handle remaining elements (less than 8) with scalar operations\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        __m128i left = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i right = _mm_loadu_si128((__m128i*)(arr + end - 7));\n        \n        // Reverse the 8 elements in each vector\n        right = _mm_shufflelo_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shufflehi_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(1, 0, 3, 2));\n        \n        left = _mm_shufflelo_epi16(left, _MM_SHUFFLE(0, 1, 2, 3));\n        left = _mm_shufflehi_epi16(left, _MM_SHUFFLE(0, 1, 2, 3));\n        left = _mm_shuffle_epi32(left, _MM_SHUFFLE(1, 0, 3, 2));\n        \n        // Store the reversed vectors\n        _mm_storeu_si128((__m128i*)(arr + start), right);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), left);\n        \n        start += 8;\n        end -= 8;\n    }\n    \n    // Handle remaining elements (less than 8)\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    const uint8_t *src = tensor + tensor_offset;\n    \n    size_t i = 0;\n    for (; i + 16 <= slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(src + i));\n        _mm_storeu_si128((__m128i*)(slice + i), data);\n    }\n    \n    for (; i < slice_size; i++) {\n        slice[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t stride = dim1 * dim2 * dim3;\n    size_t offset = slice_idx * dim1 * dim2;\n    \n    for (size_t i = 0; i < slice_size; i += 16) {\n        size_t remaining = slice_size - i;\n        if (remaining >= 16) {\n            __m128i data = _mm_loadu_si128((__m128i*)(tensor + offset + i));\n            _mm_storeu_si128((__m128i*)(slice + i), data);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                slice[i + j] = tensor[offset + i + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = slice_size / 16;\n    \n    for (; i < simd_size; i++) {\n        __m128i data = _mm_loadu_si128((__m128i*)(src + i * 16));\n        _mm_storeu_si128((__m128i*)(dst + i * 16), data);\n    }\n    \n    // Handle remaining elements\n    for (i = simd_size * 16; i < slice_size; i++) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * dim1 * dim2;\n    \n    for (size_t i = 0; i < slice_size; i += 16) {\n        size_t remaining = slice_size - i;\n        if (remaining >= 16) {\n            __m128i data = _mm_loadu_si128((__m128i*)(tensor + offset + i));\n            _mm_storeu_si128((__m128i*)(slice + i), data);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                slice[i + j] = tensor[offset + i + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t tensor_offset = slice_idx * slice_size;\n    \n    const uint8_t *src = tensor + tensor_offset;\n    uint8_t *dst = slice;\n    \n    size_t i = 0;\n    size_t simd_size = slice_size / 16;\n    \n    for (; i < simd_size * 16; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(src + i));\n        _mm_storeu_si128((__m128i*)(dst + i), data);\n    }\n    \n    for (; i < slice_size; i++) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n\n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i s0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i s1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i s2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i s3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i s4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i s5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i s6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i s7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(s0, s4);\n                    row1 = _mm_unpackhi_epi64(s0, s4);\n                    row2 = _mm_unpacklo_epi64(s1, s5);\n                    row3 = _mm_unpackhi_epi64(s1, s5);\n                    row4 = _mm_unpacklo_epi64(s2, s6);\n                    row5 = _mm_unpackhi_epi64(s2, s6);\n                    row6 = _mm_unpacklo_epi64(s3, s7);\n                    row7 = _mm_unpackhi_epi64(s3, s7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n            \n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t max_i = (i + block_size) < rows ? (i + block_size) : rows;\n            size_t max_j = (j + block_size) < cols ? (j + block_size) : cols;\n\n            for (size_t ii = i; ii < max_i; ii += 8) {\n                for (size_t jj = j; jj < max_j; jj += 8) {\n                    __m128i row0 = _mm_loadu_si128((__m128i*)(src + ii * cols + jj));\n                    __m128i row1 = _mm_loadu_si128((__m128i*)(src + (ii + 1) * cols + jj));\n                    __m128i row2 = _mm_loadu_si128((__m128i*)(src + (ii + 2) * cols + jj));\n                    __m128i row3 = _mm_loadu_si128((__m128i*)(src + (ii + 3) * cols + jj));\n                    __m128i row4 = _mm_loadu_si128((__m128i*)(src + (ii + 4) * cols + jj));\n                    __m128i row5 = _mm_loadu_si128((__m128i*)(src + (ii + 5) * cols + jj));\n                    __m128i row6 = _mm_loadu_si128((__m128i*)(src + (ii + 6) * cols + jj));\n                    __m128i row7 = _mm_loadu_si128((__m128i*)(src + (ii + 7) * cols + jj));\n\n                    __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                    __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                    __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                    __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                    __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                    __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                    __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                    __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                    __m128i tt0 = _mm_unpacklo_epi32(t0, t1);\n                    __m128i tt1 = _mm_unpackhi_epi32(t0, t1);\n                    __m128i tt2 = _mm_unpacklo_epi32(t2, t3);\n                    __m128i tt3 = _mm_unpackhi_epi32(t2, t3);\n                    __m128i tt4 = _mm_unpacklo_epi32(t4, t5);\n                    __m128i tt5 = _mm_unpackhi_epi32(t4, t5);\n                    __m128i tt6 = _mm_unpacklo_epi32(t6, t7);\n                    __m128i tt7 = _mm_unpackhi_epi32(t6, t7);\n\n                    row0 = _mm_unpacklo_epi64(tt0, tt4);\n                    row1 = _mm_unpackhi_epi64(tt0, tt4);\n                    row2 = _mm_unpacklo_epi64(tt1, tt5);\n                    row3 = _mm_unpackhi_epi64(tt1, tt5);\n                    row4 = _mm_unpacklo_epi64(tt2, tt6);\n                    row5 = _mm_unpackhi_epi64(tt2, tt6);\n                    row6 = _mm_unpacklo_epi64(tt3, tt7);\n                    row7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                    _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), row0);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 1) * rows + ii), row1);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 2) * rows + ii), row2);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 3) * rows + ii), row3);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 4) * rows + ii), row4);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 5) * rows + ii), row5);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 6) * rows + ii), row6);\n                    _mm_storeu_si128((__m128i*)(dst + (jj + 7) * rows + ii), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t remaining = dim - i;\n        if (remaining >= 4) {\n            __m128 elements = _mm_loadu_ps(&array[i * stride]);\n            sum = _mm_add_ps(sum, elements);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                sum = _mm_add_ss(sum, _mm_load_ss(&array[(i + j) * stride]));\n            }\n        }\n    }\n\n    float result = 0.0f;\n    __m128 temp = _mm_hadd_ps(sum, sum);\n    temp = _mm_hadd_ps(temp, temp);\n    _mm_store_ss(&result, temp);\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t stride = dim * dim + dim + 1;\n    size_t total_elements = dim * dim * dim;\n    \n    for (size_t i = 0; i < dim; i += 4) {\n        size_t remaining = dim - i;\n        if (remaining >= 4) {\n            __m128 elements = _mm_loadu_ps(&array[i * stride]);\n            sum = _mm_add_ps(sum, elements);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                sum = _mm_add_ss(sum, _mm_load_ss(&array[(i + j) * stride]));\n            }\n        }\n    }\n    \n    float result = 0.0f;\n    __m128 temp = _mm_hadd_ps(sum, sum);\n    temp = _mm_hadd_ps(temp, temp);\n    _mm_store_ss(&result, temp);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t idx = i * stride;\n        if (idx + 3 * stride < total_elements) {\n            __m128 val = _mm_loadu_ps(&array[idx]);\n            sum = _mm_add_ps(sum, val);\n        } else {\n            for (size_t j = 0; j < 4 && (i + j) < dim; ++j) {\n                sum = _mm_add_ss(sum, _mm_load_ss(&array[idx + j * stride]));\n            }\n        }\n    }\n\n    float result = 0.0f;\n    __m128 temp = _mm_hadd_ps(sum, sum);\n    temp = _mm_hadd_ps(temp, temp);\n    _mm_store_ss(&result, temp);\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t remaining = dim - i;\n        if (remaining >= 4) {\n            __m128 elements = _mm_loadu_ps(&array[i * stride]);\n            sum = _mm_add_ps(sum, elements);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                sum = _mm_add_ss(sum, _mm_load_ss(&array[(i + j) * stride]));\n            }\n        }\n    }\n\n    float result = 0.0f;\n    __m128 temp = _mm_hadd_ps(sum, sum);\n    temp = _mm_hadd_ps(temp, temp);\n    _mm_store_ss(&result, temp);\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t total_elements = dim * dim * dim;\n    size_t stride = dim * dim + dim + 1;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t remaining = dim - i;\n        if (remaining >= 4) {\n            __m128 elements = _mm_loadu_ps(&array[i * stride]);\n            sum_vec = _mm_add_ps(sum_vec, elements);\n        } else {\n            for (size_t j = 0; j < remaining; ++j) {\n                sum_vec = _mm_add_ss(sum_vec, _mm_load_ss(&array[(i + j) * stride]));\n            }\n        }\n    }\n\n    float sum_array[4];\n    _mm_storeu_ps(sum_array, sum_vec);\n    return sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scl = _mm_set1_pd(scale);\n    size_t total = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 1 < total; i += 2) {\n        __m128d val = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(val, thresh);\n        __m128d scaled = _mm_mul_pd(val, scl);\n        __m128d res = _mm_blendv_pd(val, scaled, mask);\n        _mm_storeu_pd(dst + i, res);\n    }\n\n    // Handle remaining elements if total is odd\n    if (total % 2 != 0) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scl = _mm_set1_pd(scale);\n    __m128d zero = _mm_setzero_pd();\n    __m128d mask, src_vec, scaled, result;\n\n    size_t total_elements = rows * cols;\n    size_t i;\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        src_vec = _mm_loadu_pd(src + i);\n        mask = _mm_cmpgt_pd(src_vec, thresh);\n        scaled = _mm_mul_pd(src_vec, scl);\n        result = _mm_blendv_pd(src_vec, scaled, mask);\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scl = _mm_set1_pd(scale);\n    __m128d zero = _mm_setzero_pd();\n    __m128d mask, val, scaled;\n\n    size_t total = rows * cols;\n    size_t i = 0;\n    for (; i + 1 < total; i += 2) {\n        val = _mm_loadu_pd(src + i);\n        mask = _mm_cmpgt_pd(val, thresh);\n        scaled = _mm_mul_pd(val, scl);\n        val = _mm_blendv_pd(val, scaled, mask);\n        _mm_storeu_pd(dst + i, val);\n    }\n\n    // Handle remaining elements\n    for (; i < total; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scl = _mm_set1_pd(scale);\n    \n    size_t total_elements = rows * cols;\n    size_t i;\n    \n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d val = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(val, thresh);\n        __m128d scaled = _mm_mul_pd(val, scl);\n        __m128d res = _mm_blendv_pd(val, scaled, mask);\n        _mm_storeu_pd(dst + i, res);\n    }\n    \n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scl = _mm_set1_pd(scale);\n    size_t total = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 1 < total; i += 2) {\n        __m128d val = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(val, thresh);\n        __m128d scaled = _mm_mul_pd(val, scl);\n        __m128d res = _mm_blendv_pd(val, scaled, mask);\n        _mm_storeu_pd(dst + i, res);\n    }\n\n    if (total % 2 != 0) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_data = _mm_loadu_pd(src_ptr + j);\n            _mm_storeu_pd(dst_ptr + j, row_data);\n        }\n        \n        for (; j < cols; ++j) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_data = _mm_loadu_pd(src_ptr + j);\n            _mm_storeu_pd(dst_ptr + j, row_data);\n        }\n        \n        for (; j < cols; ++j) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_simd = cols / 2;\n    size_t col_remain = cols % 2;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n\n        for (size_t j = 0; j < col_simd; ++j) {\n            __m128d data = _mm_loadu_pd(src_ptr + j * 2);\n            _mm_storeu_pd(dst_ptr + j * 2, data);\n        }\n\n        if (col_remain) {\n            dst_ptr[cols - 1] = src_ptr[cols - 1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_data = _mm_loadu_pd(src_ptr + j);\n            _mm_storeu_pd(dst_ptr + j, row_data);\n        }\n        \n        if (j < cols) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_data = _mm_loadu_pd(src_ptr + j);\n            _mm_storeu_pd(dst_ptr + j, row_data);\n        }\n        \n        // Handle remaining elements if cols is odd\n        if (j < cols) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i two_vec = _mm_set1_epi64x(2);\n    \n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i const*)(src2 + i));\n        \n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        sum_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        sum_vec = _mm_div_epi64(sum_vec, two_vec);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), sum_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i two_vec = _mm_set1_epi64x(2);\n    \n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i const*)(src2 + i));\n        \n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        sum_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        sum_vec = _mm_div_epi64(sum_vec, two_vec);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), sum_vec);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i two_vec = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i vec2 = _mm_loadu_si128((__m128i const*)(src2 + i));\n        \n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        __m128i sum_scalar = _mm_add_epi64(sum, scalar_vec);\n        __m128i result = _mm_div_epi64(sum_scalar, two_vec);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i two_vec = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i const*)(src2 + i));\n        \n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        sum_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        sum_vec = _mm_div_epi64(sum_vec, two_vec);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), sum_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i two_vec = _mm_set1_epi64x(2);\n    \n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i vec2 = _mm_loadu_si128((__m128i const*)(src2 + i));\n        \n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n        sum = _mm_div_epi64(sum, two_vec);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), sum);\n    }\n    \n    // Handle remaining elements if length is odd\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n        __m128i even = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even);\n    }\n\n    int result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, min_val);\n    int result = INT_MAX;\n\n    for (int j = 0; j < 4; j++) {\n        if (result_arr[j] < result) {\n            result = result_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n        __m128i even_elements = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even_elements);\n    }\n\n    int min_array[4];\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n    int result = (min_array[0] < min_array[2]) ? min_array[0] : min_array[2];\n\n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n        __m128i even = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even);\n    }\n\n    int result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, min_val);\n\n    int result = INT_MAX;\n    for (int j = 0; j < 4; j++) {\n        if (result_arr[j] < result) {\n            result = result_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n        __m128i even = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even);\n    }\n\n    int min_array[4];\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n    int result = (min_array[0] < min_array[2]) ? min_array[0] : min_array[2];\n\n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n        __m128i even = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even);\n    }\n\n    int result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, min_val);\n\n    int result = INT_MAX;\n    for (int j = 0; j < 4; j++) {\n        if (result_arr[j] < result) {\n            result = result_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m128i arith_shift = _mm_sra_epi32(src_vec, shift_vec);\n        \n        // Logical right shift (convert to uint, shift, then cast back)\n        __m128i logical_shift = _mm_srl_epi32(src_vec, shift_vec);\n        \n        // Add results\n        __m128i result = _mm_add_epi32(arith_shift, logical_shift);\n        \n        // Store result\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        unsigned int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + (int)logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i v_shift = _mm_set1_epi32(shift);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        \n        __m128i v_arith = _mm_sra_epi32(v_src, v_shift);\n        __m128i v_logical = _mm_srl_epi32(v_src, v_shift);\n        \n        __m128i v_result = _mm_add_epi32(v_arith, v_logical);\n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n    \n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        int logical = (unsigned int)src[i] >> shift;\n        dst[i] = arith + logical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i v_shift = _mm_set1_epi32(shift);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v_arith = _mm_sra_epi32(v_src, v_shift);\n        __m128i v_logical = _mm_srl_epi32(v_src, v_shift);\n        __m128i v_result = _mm_add_epi32(v_arith, v_logical);\n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n    \n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        int logical = (unsigned int)src[i] >> shift;\n        dst[i] = arith + logical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i v_shift = _mm_set1_epi32(shift);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        \n        // Arithmetic right shift\n        __m128i v_arith = _mm_sra_epi32(v_src, v_shift);\n        \n        // Logical right shift (convert to uint, shift, cast back)\n        __m128i v_logical = _mm_srl_epi32(v_src, v_shift);\n        \n        // Add results\n        __m128i v_result = _mm_add_epi32(v_arith, v_logical);\n        \n        // Store result\n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        unsigned int logical = (unsigned int)src[i] >> shift;\n        dst[i] = arith + (int)logical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    \n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i arith_shift = _mm_sra_epi32(src_vec, shift_vec);\n        __m128i logical_shift = _mm_srl_epi32(_mm_andnot_si128(_mm_set1_epi32(0), src_vec), shift_vec);\n        __m128i result = _mm_add_epi32(arith_shift, logical_shift);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        unsigned int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + (int)logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    int32_t sum_array[4] __attribute__((aligned(16)));\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    for (; i < total_elements; i++) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(matrix + i));\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n    \n    int32_t sum_array[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n    \n    int32_t sum_array[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(matrix + i));\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n    \n    int32_t sum_array[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n    \n    int32_t sum_array[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 16 elements at a time using SSE\n    const size_t simd_width = 16;\n    __m128i max_vals = _mm_set1_epi8(max_val);\n    __m128i max_indices = _mm_setzero_si128();\n    __m128i indices = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n    __m128i increment = _mm_set1_epi8(16);\n\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i mask = _mm_cmpgt_epi8(data, max_vals);\n        max_vals = _mm_max_epi8(max_vals, data);\n        max_indices = _mm_blendv_epi8(max_indices, indices, mask);\n        indices = _mm_add_epi8(indices, increment);\n    }\n\n    // Find the maximum value and index in the SIMD registers\n    int8_t max_arr[16];\n    int8_t idx_arr[16];\n    _mm_storeu_si128((__m128i*)max_arr, max_vals);\n    _mm_storeu_si128((__m128i*)idx_arr, max_indices);\n\n    for (size_t j = 0; j < 16; ++j) {\n        if (max_arr[j] > max_val) {\n            max_val = max_arr[j];\n            max_index = i - simd_width + idx_arr[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 16 elements at a time using SSE\n    for (; i + 16 <= length; i += 16) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i max_vec = _mm_set1_epi8(max_val);\n        __m128i cmp = _mm_cmpgt_epi8(vec, max_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            int pos = __builtin_ctz(mask);\n            max_val = src[i + pos];\n            max_index = i + pos;\n\n            // Check remaining elements in the current vector\n            for (int j = pos + 1; j < 16; ++j) {\n                if (src[i + j] > max_val) {\n                    max_val = src[i + j];\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 16 elements at a time using SSE\n    for (; i + 16 <= length; i += 16) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Compare with current max\n        __m128i max_vec = _mm_set1_epi8(max_val);\n        __m128i cmp = _mm_cmpgt_epi8(vec, max_vec);\n        \n        // Check if any element is greater than current max\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            // Find the first element greater than current max\n            int pos = __builtin_ctz(mask);\n            max_val = src[i + pos];\n            max_index = i + pos;\n            \n            // Check remaining elements in current vector\n            for (int j = pos + 1; j < 16; ++j) {\n                if (src[i + j] > max_val) {\n                    max_val = src[i + j];\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 16 elements at a time using SSE\n    for (; i + 16 <= length; i += 16) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i max_vec = _mm_set1_epi8(max_val);\n        __m128i cmp = _mm_cmpgt_epi8(vec, max_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            int pos = __builtin_ctz(mask);\n            max_val = src[i + pos];\n            max_index = i + pos;\n\n            // Check remaining elements in the current vector\n            for (int j = pos + 1; j < 16; ++j) {\n                if (src[i + j] > max_val) {\n                    max_val = src[i + j];\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    // Process 16 elements at a time using SSE\n    for (; i + 16 <= length; i += 16) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i max_vec = _mm_set1_epi8(max_val);\n        __m128i cmp = _mm_cmpgt_epi8(vec, max_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Find the first set bit in the mask\n            int pos = __builtin_ctz(mask);\n            max_val = src[i + pos];\n            max_index = i + pos;\n\n            // Check remaining elements in the current vector\n            for (int j = pos + 1; j < 16; ++j) {\n                if (src[i + j] > max_val) {\n                    max_val = src[i + j];\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set1_epi32(mask);\n    __m128i index_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment = _mm_set1_epi32(4);\n    \n    for (; i + 7 < length; i += 8) {\n        __m128i indices1 = _mm_add_epi32(index_vec, _mm_set1_epi32(i));\n        __m128i indices2 = _mm_add_epi32(indices1, _mm_set1_epi32(4));\n        \n        __m128i mask_result1 = _mm_and_si128(indices1, mask_vec);\n        __m128i mask_result2 = _mm_and_si128(indices2, mask_vec);\n        \n        __m128i cmp1 = _mm_cmpeq_epi32(mask_result1, _mm_setzero_si128());\n        __m128i cmp2 = _mm_cmpeq_epi32(mask_result2, _mm_setzero_si128());\n        \n        __m128i src1_data1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_data1 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i src1_data2 = _mm_loadu_si128((__m128i*)(src1 + i + 4));\n        __m128i src2_data2 = _mm_loadu_si128((__m128i*)(src2 + i + 4));\n        \n        __m128i blend1 = _mm_blendv_epi8(src1_data1, src2_data1, cmp1);\n        __m128i blend2 = _mm_blendv_epi8(src1_data2, src2_data2, cmp2);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), blend1);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), blend2);\n        \n        index_vec = _mm_add_epi32(index_vec, increment);\n    }\n    \n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i mask_vec = _mm_set1_epi32(mask);\n    __m128i index_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i indices1 = _mm_add_epi32(index_vec, _mm_set1_epi32(i));\n        __m128i indices2 = _mm_add_epi32(indices1, _mm_set1_epi32(4));\n        \n        __m128i mask_result1 = _mm_and_si128(indices1, mask_vec);\n        __m128i mask_result2 = _mm_and_si128(indices2, mask_vec);\n        \n        __m128i cmp1 = _mm_cmpeq_epi32(mask_result1, _mm_setzero_si128());\n        __m128i cmp2 = _mm_cmpeq_epi32(mask_result2, _mm_setzero_si128());\n        \n        __m128i src1_vec1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src1_vec2 = _mm_loadu_si128((__m128i*)(src1 + i + 4));\n        __m128i src2_vec1 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i src2_vec2 = _mm_loadu_si128((__m128i*)(src2 + i + 4));\n        \n        __m128i blend1 = _mm_blendv_epi8(src1_vec1, src2_vec1, cmp1);\n        __m128i blend2 = _mm_blendv_epi8(src1_vec2, src2_vec2, cmp2);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), blend1);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), blend2);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i mask_vec = _mm_set1_epi32(mask);\n    __m128i index_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment = _mm_set1_epi32(4);\n    \n    for (size_t i = 0; i < length; i += 8) {\n        __m128i and_result1 = _mm_and_si128(index_vec, mask_vec);\n        __m128i cmp1 = _mm_cmpeq_epi32(and_result1, _mm_setzero_si128());\n        __m128i src1_vec1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec1 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i res1 = _mm_blendv_epi8(src1_vec1, src2_vec1, cmp1);\n        _mm_storeu_si128((__m128i*)(dst + i), res1);\n        \n        index_vec = _mm_add_epi32(index_vec, increment);\n        \n        __m128i and_result2 = _mm_and_si128(index_vec, mask_vec);\n        __m128i cmp2 = _mm_cmpeq_epi32(and_result2, _mm_setzero_si128());\n        __m128i src1_vec2 = _mm_loadu_si128((__m128i*)(src1 + i + 4));\n        __m128i src2_vec2 = _mm_loadu_si128((__m128i*)(src2 + i + 4));\n        __m128i res2 = _mm_blendv_epi8(src1_vec2, src2_vec2, cmp2);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), res2);\n        \n        index_vec = _mm_add_epi32(index_vec, increment);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i mask_vec = _mm_set1_epi32(mask);\n    __m128i index_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i indices1 = _mm_add_epi32(index_vec, _mm_set1_epi32(i));\n        __m128i indices2 = _mm_add_epi32(indices1, _mm_set1_epi32(4));\n        __m128i mask_result1 = _mm_and_si128(indices1, mask_vec);\n        __m128i mask_result2 = _mm_and_si128(indices2, mask_vec);\n        __m128i cmp1 = _mm_cmpeq_epi32(mask_result1, _mm_setzero_si128());\n        __m128i cmp2 = _mm_cmpeq_epi32(mask_result2, _mm_setzero_si128());\n\n        __m128i src1_data1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src1_data2 = _mm_loadu_si128((__m128i*)(src1 + i + 4));\n        __m128i src2_data1 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i src2_data2 = _mm_loadu_si128((__m128i*)(src2 + i + 4));\n\n        __m128i blend1 = _mm_blendv_epi8(src1_data1, src2_data1, cmp1);\n        __m128i blend2 = _mm_blendv_epi8(src1_data2, src2_data2, cmp2);\n\n        _mm_storeu_si128((__m128i*)(dst + i), blend1);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), blend2);\n\n        index_vec = _mm_add_epi32(index_vec, increment);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set1_epi32(mask);\n    __m128i index_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (; i + 7 < length; i += 8) {\n        __m128i indices1 = _mm_add_epi32(index_vec, _mm_set1_epi32(i));\n        __m128i indices2 = _mm_add_epi32(indices1, _mm_set1_epi32(4));\n        \n        __m128i and_result1 = _mm_and_si128(indices1, mask_vec);\n        __m128i and_result2 = _mm_and_si128(indices2, mask_vec);\n        \n        __m128i cmp1 = _mm_cmpeq_epi32(and_result1, _mm_setzero_si128());\n        __m128i cmp2 = _mm_cmpeq_epi32(and_result2, _mm_setzero_si128());\n        \n        __m128i src1_data1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src1_data2 = _mm_loadu_si128((__m128i*)(src1 + i + 4));\n        __m128i src2_data1 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i src2_data2 = _mm_loadu_si128((__m128i*)(src2 + i + 4));\n        \n        __m128i blend1 = _mm_blendv_epi8(src1_data1, src2_data1, cmp1);\n        __m128i blend2 = _mm_blendv_epi8(src1_data2, src2_data2, cmp2);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), blend1);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), blend2);\n        \n        index_vec = _mm_add_epi32(index_vec, increment);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x01010101);\n\n        __m128i v1 = _mm_srli_epi32(v, 1);\n        v1 = _mm_and_si128(v1, mask1);\n        v = _mm_sub_epi32(v, v1);\n\n        __m128i v2 = _mm_srli_epi32(v, 2);\n        v2 = _mm_and_si128(v2, mask2);\n        v = _mm_and_si128(v, mask2);\n        v = _mm_add_epi32(v, v2);\n\n        __m128i v3 = _mm_srli_epi32(v, 4);\n        v3 = _mm_and_si128(v3, mask3);\n        v = _mm_and_si128(v, mask3);\n        v = _mm_add_epi32(v, v3);\n\n        v = _mm_mullo_epi32(v, mask4);\n        v = _mm_srli_epi32(v, 24);\n\n        _mm_storeu_si128((__m128i*)(dst + i), v);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v_pop = _mm_setzero_si128();\n        \n        for (int j = 0; j < 8; ++j) {\n            __m128i mask = _mm_set1_epi32(1 << j);\n            __m128i bits = _mm_and_si128(v, mask);\n            __m128i cmp = _mm_cmpeq_epi32(bits, mask);\n            v_pop = _mm_sub_epi32(v_pop, cmp);\n        }\n        \n        _mm_storeu_si128((__m128i*)(dst + i), v_pop);\n    }\n    \n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        dst[i] = x & 0x0000003F;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_pop = _mm_setzero_si128();\n        \n        for (int j = 0; j < 8; ++j) {\n            __m128i mask = _mm_set1_epi32(1 << j);\n            __m128i bits = _mm_and_si128(v, mask);\n            __m128i cmp = _mm_cmpeq_epi32(bits, mask);\n            v_pop = _mm_sub_epi32(v_pop, cmp);\n        }\n        \n        _mm_storeu_si128((__m128i*)&dst[i], v_pop);\n    }\n    \n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x & 0x0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F);\n        x = (x & 0x00FF00FF) + ((x >> 8) & 0x00FF00FF);\n        x = (x & 0x0000FFFF) + ((x >> 16) & 0x0000FFFF);\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x01010101);\n\n        __m128i v1 = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        __m128i v2 = _mm_add_epi32(_mm_and_si128(v1, mask2), _mm_and_si128(_mm_srli_epi32(v1, 2), mask2));\n        __m128i v3 = _mm_add_epi32(_mm_and_si128(v2, mask3), _mm_and_si128(_mm_srli_epi32(v2, 4), mask3));\n        __m128i v4 = _mm_mullo_epi32(v3, mask4);\n        __m128i popcnt = _mm_srli_epi32(v4, 24);\n\n        _mm_storeu_si128((__m128i*)(dst + i), popcnt);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n        __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n\n        __m128i v1 = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        __m128i v2 = _mm_add_epi32(_mm_and_si128(v1, mask2), _mm_and_si128(_mm_srli_epi32(v1, 2), mask2));\n        __m128i v3 = _mm_add_epi32(_mm_and_si128(v2, mask3), _mm_and_si128(_mm_srli_epi32(v2, 4), mask3));\n        __m128i v4 = _mm_add_epi32(_mm_and_si128(v3, mask4), _mm_and_si128(_mm_srli_epi32(v3, 8), mask4));\n        __m128i v5 = _mm_add_epi32(_mm_and_si128(v4, mask5), _mm_and_si128(_mm_srli_epi32(v4, 16), mask5));\n\n        _mm_storeu_si128((__m128i*)(dst + i), v5);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        dst[i] = x & 0x0000003F;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_set1_pd(0.0);\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a, lower_bound),\n                    _mm_cmple_pd(a, upper_bound)\n                );\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b, lower_bound),\n                    _mm_cmple_pd(b, upper_bound)\n                );\n\n                __m128d mask = _mm_and_pd(a_in_range, b_in_range);\n                __m128d product = _mm_mul_pd(a, b);\n                product = _mm_and_pd(product, mask);\n\n                sum = _mm_add_pd(sum, product);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n\n            if (n % 2 != 0) {\n                double a_val = A[i * n + (n - 1)];\n                double b_val = B[(n - 1) * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    C[i * p + j] += a_val * b_val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d low_bound = _mm_set1_pd(-100.0);\n    __m128d high_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n                \n                __m128d a_mask = _mm_and_pd(_mm_cmpge_pd(a, low_bound), _mm_cmple_pd(a, high_bound));\n                __m128d b_mask = _mm_and_pd(_mm_cmpge_pd(b, low_bound), _mm_cmple_pd(b, high_bound));\n                \n                __m128d a_masked = _mm_and_pd(a, a_mask);\n                __m128d b_masked = _mm_and_pd(b, b_mask);\n                \n                __m128d prod = _mm_mul_pd(a_masked, b_masked);\n                sum = _mm_add_pd(sum, prod);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d min_range = _mm_set1_pd(-100.0);\n    __m128d max_range = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a, min_range),\n                    _mm_cmple_pd(a, max_range)\n                );\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b, min_range),\n                    _mm_cmple_pd(b, max_range)\n                );\n\n                __m128d mask = _mm_and_pd(a_in_range, b_in_range);\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n\n                sum = _mm_add_pd(sum, prod);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n                \n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a, lower_bound),\n                    _mm_cmple_pd(a, upper_bound)\n                );\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b, lower_bound),\n                    _mm_cmple_pd(b, upper_bound)\n                );\n                \n                __m128d mask = _mm_and_pd(a_in_range, b_in_range);\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n                \n                sum = _mm_add_pd(sum, prod);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d min_range = _mm_set1_pd(-100.0);\n    __m128d max_range = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a, min_range),\n                    _mm_cmple_pd(a, max_range)\n                );\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b, min_range),\n                    _mm_cmple_pd(b, max_range)\n                );\n\n                __m128d valid = _mm_and_pd(a_in_range, b_in_range);\n                __m128d product = _mm_mul_pd(a, b);\n                product = _mm_and_pd(product, valid);\n\n                sum = _mm_add_pd(sum, product);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c_vec = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c_vec = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    \n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c_vec = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    \n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    \n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (float)((int)(sum + 0.5f * (sum >= 0 ? 1.0f : -1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (float)((int)(sum + 0.5f * (sum >= 0 ? 1.0f : -1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (float)((int)(sum + 0.5f * (sum >= 0 ? 1.0f : -1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_setzero_ps(), _mm_set_ss(sum), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (float)((int)(sum + 0.5f * (sum >= 0 ? 1.0f : -1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_setzero_ps();\n    __m128i mask_even = _mm_set_epi32(3, 2, 1, 0); // For checking even indices\n    \n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        \n        // Calculate absolute difference\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n        \n        // Check condition 1: abs_diff <= 50\n        __m128 cond1_mask = _mm_cmple_ps(abs_diff, threshold);\n        \n        // Check condition 2: index is even (i.e., multiple of 2)\n        __m128i indices = _mm_add_epi32(_mm_set1_epi32(i), mask_even);\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        \n        // Combine conditions\n        __m128 combined_mask = _mm_and_ps(cond1_mask, cond2_mask);\n        \n        // Calculate multiplication for elements that meet conditions\n        __m128 mul = _mm_mul_ps(v1, v2);\n        \n        // Round up toward positive infinity\n        __m128 rounded = _mm_ceil_ps(mul);\n        \n        // Select results based on conditions\n        __m128 res = _mm_or_ps(_mm_and_ps(combined_mask, rounded),\n                              _mm_andnot_ps(combined_mask, neg_one));\n        \n        _mm_storeu_ps(dst + i, res);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128i index_mask = _mm_set1_epi32(0x1);\n    __m128 abs_diff_threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i current_indices = _mm_add_epi32(indices, _mm_set1_epi32(i));\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(current_indices, index_mask), zero);\n        \n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n        __m128 diff_mask = _mm_cmple_ps(abs_diff, abs_diff_threshold);\n        \n        __m128 mask = _mm_and_ps(_mm_castsi128_ps(even_mask), diff_mask);\n        __m128 mul = _mm_mul_ps(v1, v2);\n        __m128 rounded = _mm_ceil_ps(mul);\n        \n        __m128 result = _mm_blendv_ps(minus_one, rounded, mask);\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128i index_mask = _mm_set1_epi32(1);\n    __m128 abs_diff_threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n    __m128i two = _mm_set1_epi32(2);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        \n        // Calculate absolute difference\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n        \n        // Check condition 1: abs_diff <= 50\n        __m128 cond1_mask = _mm_cmple_ps(abs_diff, abs_diff_threshold);\n        \n        // Check condition 2: index % 2 == 0\n        __m128i indices = _mm_setr_epi32(i, i+1, i+2, i+3);\n        __m128i mod_result = _mm_and_si128(indices, index_mask);\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(mod_result, _mm_setzero_si128()));\n        \n        // Combine conditions\n        __m128 mask = _mm_and_ps(cond1_mask, cond2_mask);\n        \n        // Calculate product and round up\n        __m128 product = _mm_mul_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(product, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n        \n        // Select result based on conditions\n        __m128 result = _mm_blendv_ps(minus_one, rounded, mask);\n        \n        _mm_storeu_ps(dst + i, result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128i index_mask = _mm_set1_epi32(1);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        \n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n        \n        __m128 cmp_diff = _mm_cmple_ps(abs_diff, threshold);\n        \n        __m128i indices = _mm_setr_epi32(i, i+1, i+2, i+3);\n        __m128i mod = _mm_and_si128(indices, index_mask);\n        __m128 cmp_idx = _mm_castsi128_ps(_mm_cmpeq_epi32(mod, _mm_setzero_si128()));\n        \n        __m128 mask = _mm_and_ps(cmp_diff, cmp_idx);\n        \n        __m128 mul = _mm_mul_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(mul, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);\n        \n        __m128 res = _mm_blendv_ps(neg_one, rounded, mask);\n        \n        _mm_storeu_ps(dst + i, res);\n    }\n\n    for (; i < length; ++i) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2) == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_setzero_ps();\n    __m128i index_mask = _mm_set1_epi32(0x1);\n\n    for (size_t i = 0; i < length; i += 4) {\n        // Load src1 and src2\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n\n        // Calculate absolute difference\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n\n        // Check condition 1: abs_diff <= 50\n        __m128 cond1_mask = _mm_cmple_ps(abs_diff, threshold);\n\n        // Check condition 2: index is multiple of 2\n        __m128i indices = _mm_setr_epi32(i, i+1, i+2, i+3);\n        __m128i mod_result = _mm_and_si128(indices, index_mask);\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(mod_result, _mm_setzero_si128()));\n\n        // Combine conditions\n        __m128 mask = _mm_and_ps(cond1_mask, cond2_mask);\n\n        // Calculate multiplication for elements that meet conditions\n        __m128 mul_result = _mm_mul_ps(v1, v2);\n        mul_result = _mm_ceil_ps(mul_result);\n\n        // Select results based on mask\n        __m128 result = _mm_blendv_ps(neg_one, mul_result, mask);\n\n        // Store result\n        _mm_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(mat + i);\n        __m128d res_vec = _mm_mul_pd(mat_vec, scalar_vec);\n        _mm_storeu_pd(dst + i, res_vec);\n    }\n\n    // Handle remaining element if total_elements is odd\n    if (total_elements % 2 != 0) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t size = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]);\n        __m128d res_vec = _mm_mul_pd(mat_vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], res_vec);\n    }\n\n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]);\n        __m128d res_vec = _mm_mul_pd(mat_vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], res_vec);\n    }\n\n    // Handle remaining element if total_elements is odd\n    if (total_elements % 2 != 0) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t size = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(mat + i);\n        __m128d res_vec = _mm_mul_pd(mat_vec, scalar_vec);\n        _mm_storeu_pd(dst + i, res_vec);\n    }\n\n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t size = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(mat + i);\n        __m128d res_vec = _mm_mul_pd(mat_vec, scalar_vec);\n        _mm_storeu_pd(dst + i, res_vec);\n    }\n\n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j;\n        \n        // Process 4 elements at a time\n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum_sq = _mm_add_ps(sum_sq, squared);\n        }\n        \n        // Sum the 4 partial sums\n        float sum = 0.0f;\n        float tmp[4];\n        _mm_storeu_ps(tmp, sum_sq);\n        sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n        \n        __m128 norm_vec = _mm_set1_ps(norm);\n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 normalized = _mm_div_ps(vec, norm_vec);\n            _mm_storeu_ps(row + j, normalized);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j = 0;\n        \n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum_sq = _mm_add_ps(sum_sq, squared);\n        }\n        \n        float sum = 0.0f;\n        __m128 shuf = _mm_movehl_ps(sum_sq, sum_sq);\n        sum_sq = _mm_add_ps(sum_sq, shuf);\n        shuf = _mm_shuffle_ps(sum_sq, sum_sq, 0x55);\n        sum_sq = _mm_add_ps(sum_sq, shuf);\n        _mm_store_ss(&sum, sum_sq);\n        \n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        __m128 norm_vec = _mm_set1_ps(norm);\n        j = 0;\n        \n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 normalized = _mm_div_ps(vec, norm_vec);\n            _mm_storeu_ps(row + j, normalized);\n        }\n        \n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j;\n        \n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum_sq = _mm_add_ps(sum_sq, squared);\n        }\n        \n        float sum = 0.0f;\n        __m128 temp = sum_sq;\n        temp = _mm_hadd_ps(temp, temp);\n        temp = _mm_hadd_ps(temp, temp);\n        _mm_store_ss(&sum, temp);\n        \n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        __m128 norm_vec = _mm_set1_ps(norm);\n        \n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 normalized = _mm_div_ps(vec, norm_vec);\n            _mm_storeu_ps(row + j, normalized);\n        }\n        \n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j;\n        \n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            sum_sq = _mm_add_ps(sum_sq, _mm_mul_ps(vec, vec));\n        }\n        \n        float sum = 0.0f;\n        float tmp[4];\n        _mm_storeu_ps(tmp, sum_sq);\n        sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n        \n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        float norm = sqrtf(sum);\n        __m128 norm_vec = _mm_set1_ps(norm);\n        \n        for (j = 0; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            vec = _mm_div_ps(vec, norm_vec);\n            _mm_storeu_ps(row + j, vec);\n        }\n        \n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum_sq = _mm_add_ps(sum_sq, squared);\n        }\n\n        // Horizontal sum of the sum_sq vector\n        __m128 shuf = _mm_movehdup_ps(sum_sq);\n        __m128 sums = _mm_add_ps(sum_sq, shuf);\n        shuf = _mm_movehl_ps(shuf, sums);\n        sums = _mm_add_ss(sums, shuf);\n        float norm_sq = _mm_cvtss_f32(sums);\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n\n        float norm = sqrtf(norm_sq);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n\n        __m128 inv_norm = _mm_set1_ps(1.0f / norm);\n        j = 0;\n\n        // Normalize 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 normalized = _mm_mul_ps(vec, inv_norm);\n            _mm_storeu_ps(row + j, normalized);\n        }\n\n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            size_t k = 0;\n            for (; k + 1 < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1];\n            if (k < n) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            __m128d rounded = _mm_round_pd(_mm_set_sd(res), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm_store_sd(&dst[i * p + j], rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            size_t k = 0;\n            for (; k + 1 < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1];\n            if (k < n) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = (double)(int)(res + 0.5);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            size_t k = 0;\n            for (; k + 1 < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double res = _mm_cvtsd_f64(_mm_hadd_pd(sum, sum));\n            if (k < n) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = _mm_cvtsd_f64(_mm_round_sd(_mm_setzero_pd(), _mm_set_sd(res), _MM_FROUND_TO_NEAREST_INT));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            size_t k = 0;\n            for (; k + 1 < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double res = temp[0] + temp[1];\n            if (k < n) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            __m128d rounded = _mm_cvtepi32_pd(_mm_cvtpd_epi32(_mm_set_sd(res)));\n            dst[i * p + j] = _mm_cvtsd_f64(rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            size_t k = 0;\n            for (; k + 1 < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double res = _mm_cvtsd_f64(sum) + _mm_cvtsd_f64(_mm_shuffle_pd(sum, sum, _MM_SHUFFLE2(0, 1)));\n            if (k < n) {\n                res += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = _mm_cvtsd_f64(_mm_round_pd(_mm_set_sd(res), _MM_FROUND_TO_NEAREST_INT));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    const __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 src_vec = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 scaled = _mm_mul_ps(src_vec, _mm_set1_ps(4.0f));\n            __m128 rounded = _mm_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m128 result = _mm_mul_ps(rounded, quarter);\n            _mm_storeu_ps(&dst[j * rows + i], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    const __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            __m128 tmp0 = _mm_unpacklo_ps(row0, row1);\n            __m128 tmp1 = _mm_unpackhi_ps(row0, row1);\n            __m128 tmp2 = _mm_unpacklo_ps(row2, row3);\n            __m128 tmp3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 col0 = _mm_movelh_ps(tmp0, tmp2);\n            __m128 col1 = _mm_movehl_ps(tmp2, tmp0);\n            __m128 col2 = _mm_movelh_ps(tmp1, tmp3);\n            __m128 col3 = _mm_movehl_ps(tmp3, tmp1);\n\n            // Round to nearest quarter\n            __m128 scaled0 = _mm_mul_ps(col0, _mm_set1_ps(4.0f));\n            __m128 scaled1 = _mm_mul_ps(col1, _mm_set1_ps(4.0f));\n            __m128 scaled2 = _mm_mul_ps(col2, _mm_set1_ps(4.0f));\n            __m128 scaled3 = _mm_mul_ps(col3, _mm_set1_ps(4.0f));\n\n            __m128 rounded0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(scaled0));\n            __m128 rounded1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(scaled1));\n            __m128 rounded2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(scaled2));\n            __m128 rounded3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(scaled3));\n\n            rounded0 = _mm_mul_ps(rounded0, quarter);\n            rounded1 = _mm_mul_ps(rounded1, quarter);\n            rounded2 = _mm_mul_ps(rounded2, quarter);\n            rounded3 = _mm_mul_ps(rounded3, quarter);\n\n            _mm_storeu_ps(&dst[j * rows + i], rounded0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], rounded1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], rounded2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], rounded3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    const __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 src_vec = _mm_loadu_ps(&src[i * cols + j]);\n            \n            // Round to nearest quarter\n            __m128 scaled = _mm_mul_ps(src_vec, _mm_set1_ps(4.0f));\n            __m128 rounded = _mm_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m128 result = _mm_mul_ps(rounded, quarter);\n            \n            // Transpose by storing to dst[j * rows + i]\n            _mm_store_ss(&dst[j * rows + i], result);\n            if (j + 1 < cols) _mm_store_ss(&dst[(j + 1) * rows + i], _mm_shuffle_ps(result, result, _MM_SHUFFLE(1, 1, 1, 1)));\n            if (j + 2 < cols) _mm_store_ss(&dst[(j + 2) * rows + i], _mm_shuffle_ps(result, result, _MM_SHUFFLE(2, 2, 2, 2)));\n            if (j + 3 < cols) _mm_store_ss(&dst[(j + 3) * rows + i], _mm_shuffle_ps(result, result, _MM_SHUFFLE(3, 3, 3, 3)));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    const __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            __m128 tmp0 = _mm_unpacklo_ps(row0, row1);\n            __m128 tmp1 = _mm_unpackhi_ps(row0, row1);\n            __m128 tmp2 = _mm_unpacklo_ps(row2, row3);\n            __m128 tmp3 = _mm_unpackhi_ps(row2, row3);\n\n            row0 = _mm_movelh_ps(tmp0, tmp2);\n            row1 = _mm_movehl_ps(tmp2, tmp0);\n            row2 = _mm_movelh_ps(tmp1, tmp3);\n            row3 = _mm_movehl_ps(tmp3, tmp1);\n\n            // Round to nearest quarter\n            row0 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row0, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row1 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row1, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row2 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row2, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row3 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row3, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n\n            _mm_storeu_ps(&dst[j * rows + i], row0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], row1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], row2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], row3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    const __m128 zero = _mm_setzero_ps();\n    \n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n            \n            __m128 tmp0 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 tmp1 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(3, 2, 3, 2));\n            __m128 tmp2 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 tmp3 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(3, 2, 3, 2));\n            \n            row0 = _mm_shuffle_ps(tmp0, tmp2, _MM_SHUFFLE(2, 0, 2, 0));\n            row1 = _mm_shuffle_ps(tmp0, tmp2, _MM_SHUFFLE(3, 1, 3, 1));\n            row2 = _mm_shuffle_ps(tmp1, tmp3, _MM_SHUFFLE(2, 0, 2, 0));\n            row3 = _mm_shuffle_ps(tmp1, tmp3, _MM_SHUFFLE(3, 1, 3, 1));\n            \n            // Round to nearest quarter\n            row0 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row0, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row1 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row1, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row2 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row2, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            row3 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row3, quarter), _MM_FROUND_TO_NEAREST_INT), quarter);\n            \n            _mm_storeu_ps(&dst[j * rows + i], row0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], row1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], row2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], row3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 vec2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 res = _mm_sub_ps(vec1, vec2);\n        \n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpge_ps(res, zero);\n        __m128 rounded = _mm_add_ps(res, _mm_set1_ps(0.5f));\n        __m128 floored = _mm_sub_ps(res, _mm_set1_ps(0.5f));\n        rounded = _mm_floor_ps(rounded);\n        floored = _mm_ceil_ps(floored);\n        res = _mm_or_ps(_mm_and_ps(mask, rounded), _mm_andnot_ps(mask, floored));\n        \n        _mm_storeu_ps(&dst[i], res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = val >= 0.0f ? floorf(val + 0.5f) : ceilf(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i);\n        __m128 res = _mm_sub_ps(vec1, vec2);\n        \n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpge_ps(res, zero);\n        __m128 rounded = _mm_add_ps(res, _mm_set1_ps(0.5f));\n        __m128 floored = _mm_sub_ps(res, _mm_set1_ps(0.5f));\n        __m128 final = _mm_blendv_ps(floored, rounded, mask);\n        final = _mm_cvtepi32_ps(_mm_cvtps_epi32(final));\n        \n        _mm_storeu_ps(dst + i, final);\n    }\n    \n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = val >= 0 ? (float)(int)(val + 0.5f) : (float)(int)(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i);\n        __m128 res = _mm_sub_ps(vec1, vec2);\n        res = _mm_cvtepi32_ps(_mm_cvtps_epi32(res));\n        _mm_storeu_ps(dst + i, res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val) : ceilf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 vec2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 res = _mm_sub_ps(vec1, vec2);\n        res = _mm_round_ps(res, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val + 0.5f) : ceilf(val - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 vec2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 res = _mm_sub_ps(vec1, vec2);\n        res = _mm_cvtepi32_ps(_mm_cvtps_epi32(res));\n        _mm_storeu_ps(&dst[i], res);\n    }\n    \n    for (; i < total_elements; i++) {\n        float val = tensor1[i] - tensor2[i];\n        dst[i] = (val >= 0) ? floorf(val) : ceilf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i;\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d res = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], res);\n    }\n    \n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[size - 1] = mat1[size - 1] * mat2[size - 1];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i;\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d res = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], res);\n    }\n    \n    for (; i < size; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i;\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d res = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], res);\n    }\n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i;\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d res = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], res);\n    }\n    \n    // Handle remaining elements if size is not a multiple of 2\n    for (; i < size; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t size = m * n;\n    size_t i;\n    for (i = 0; i + 1 < size; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d res = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], res);\n    }\n    \n    // Handle remaining elements if size is odd\n    if (size % 2 != 0) {\n        dst[size - 1] = mat1[size - 1] * mat2[size - 1];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 vec_val = _mm_loadu_ps(&vec[j]);\n            __m128 prod = _mm_mul_ps(mat_row, vec_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 vec_val = _mm_loadu_ps(&vec[j]);\n            __m128 prod = _mm_mul_ps(mat_row, vec_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_row = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_segment = _mm_loadu_ps(vec + j);\n            __m128 product = _mm_mul_ps(mat_row, vec_segment);\n            sum = _mm_add_ps(sum, product);\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float result = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 vec_val = _mm_loadu_ps(&vec[j]);\n            __m128 prod = _mm_mul_ps(mat_row, vec_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_row = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_val = _mm_loadu_ps(vec + j);\n            __m128 prod = _mm_mul_ps(mat_row, vec_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float res = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            res += mat[i * n + j] * vec[j];\n        }\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[2];\n    _mm_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[2];\n    _mm_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / (double)total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[2];\n    _mm_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1];\n\n    // Handle remaining elements if total_elements is odd\n    if (total_elements % 2 != 0) {\n        sum += mat[total_elements - 1];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[2];\n    _mm_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / (double)total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double temp_sum[2];\n    _mm_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / (double)total_elements;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            __m128i cmp;\n            if (direction) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmplt_epi32(current, next);\n            }\n            \n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            __m128i cmp;\n            if (direction) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmplt_epi32(current, next);\n            }\n            \n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            __m128i cmp;\n            if (direction) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmplt_epi32(current, next);\n            }\n            \n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            __m128i cmp;\n            if (direction) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmplt_epi32(current, next);\n            }\n            \n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        const int* row = matrix + i * cols;\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            __m128i cmp;\n            if (direction) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmplt_epi32(current, next);\n            }\n            \n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi16(1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v_quot = _mm_div_epi16(v_src, v_base);\n        __m128i v_rem = _mm_sub_epi16(v_src, _mm_mullo_epi16(v_quot, v_base));\n        __m128i v_mask = _mm_cmpgt_epi16(v_rem, v_zero);\n        v_mask = _mm_and_si128(v_mask, v_one);\n        v_quot = _mm_sub_epi16(v_quot, v_mask);\n        __m128i v_result = _mm_mullo_epi16(v_quot, v_base);\n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int16_t val = src[i];\n        int16_t quot = val / base;\n        int16_t rem = val % base;\n        if (rem > 0) {\n            quot--;\n        }\n        dst[i] = quot * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_one = _mm_set1_epi16(1);\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v_div = _mm_div_epi16(v_src, v_base);\n        __m128i v_mul = _mm_mullo_epi16(v_div, v_base);\n        _mm_storeu_si128((__m128i*)(dst + i), v_mul);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi16(1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i div = _mm_div_epi16(src_vec, base_vec);\n        __m128i mul = _mm_mullo_epi16(div, base_vec);\n        __m128i mask = _mm_cmplt_epi16(mul, src_vec);\n        __m128i adj = _mm_and_si128(mask, base_vec);\n        __m128i res = _mm_add_epi16(mul, adj);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        int16_t val = src[i];\n        int16_t div = val / base;\n        int16_t mul = div * base;\n        dst[i] = (mul < val) ? mul + base : mul;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_one = _mm_set1_epi16(1);\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v_div = _mm_div_epi16(v_src, v_base);\n        __m128i v_mul = _mm_mullo_epi16(v_div, v_base);\n        _mm_storeu_si128((__m128i*)(dst + i), v_mul);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi16(1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i div = _mm_div_epi16(src_vec, base_vec);\n        __m128i mul = _mm_mullo_epi16(div, base_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), mul);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(row + j);\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_vec, vec);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        \n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        \n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d mat_row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(mat_row, vec);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d mat_row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(mat_row, vec);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            for (; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n            if (match) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d mat_row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(mat_row, vec);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d mat = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d cmp = _mm_cmpeq_pd(vec, mat);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        if (match && j < cols) {\n            if (vector[j] != matrix[i * cols + j]) {\n                match = false;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(0xFF);\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(cmp, ones);\n        \n        equal_count += __builtin_popcount(_mm_movemask_epi8(mask));\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(cmp, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        equal_count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(cmp, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        equal_count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(cmp, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        equal_count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(cmp, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i vec_a = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_y = _mm_loadu_si128((__m128i*)&y[i]);\n        \n        __m128i mul_res = _mm_mul_epi32(vec_a, vec_x);\n        __m128i abs_y = _mm_sub_epi64(zero, _mm_cmpgt_epi64(zero, vec_y));\n        abs_y = _mm_or_si128(abs_y, vec_y);\n        \n        __m128i res = _mm_sub_epi64(mul_res, abs_y);\n        _mm_storeu_si128((__m128i*)&y[i], res);\n    }\n    \n    for (; i < length; ++i) {\n        int64_t temp = a * x[i];\n        y[i] = temp - (y[i] < 0 ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i a_vec = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i;\n    for (i = 0; i + 2 <= length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i const*)(x + i));\n        __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n        \n        __m128i ax = _mm_mul_epi32(a_vec, x_vec);\n        __m128i abs_y = _mm_abs_epi64(y_vec);\n        __m128i result = _mm_sub_epi64(ax, abs_y);\n        \n        _mm_storeu_si128((__m128i*)(y + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i a_vec = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i const*)(x + i));\n        __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n        \n        __m128i mul_res = _mm_mul_epi32(a_vec, x_vec);\n        __m128i abs_y = _mm_abs_epi64(y_vec);\n        __m128i res = _mm_sub_epi64(mul_res, abs_y);\n        \n        _mm_storeu_si128((__m128i*)(y + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i a_vec = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i y_vec = _mm_loadu_si128((__m128i*)&y[i]);\n        \n        __m128i ax = _mm_mul_epi32(a_vec, x_vec);\n        __m128i abs_y = _mm_abs_epi64(y_vec);\n        __m128i result = _mm_sub_epi64(ax, abs_y);\n        \n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n    \n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i vec_a = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i const*)(x + i));\n        __m128i vec_y = _mm_loadu_si128((__m128i const*)(y + i));\n        \n        __m128i mul_res = _mm_mul_epi32(vec_a, vec_x);\n        __m128i abs_y = _mm_sub_epi64(zero, _mm_cmpgt_epi64(zero, vec_y));\n        abs_y = _mm_xor_si128(abs_y, _mm_cmpgt_epi64(zero, vec_y));\n        abs_y = _mm_add_epi64(abs_y, vec_y);\n        \n        __m128i res = _mm_sub_epi64(mul_res, abs_y);\n        _mm_storeu_si128((__m128i*)(y + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        int64_t temp = a * x[i];\n        y[i] = temp - (y[i] < 0 ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_loadu_si128((__m128i*)vec);\n    __m128i max_val = min_val;\n\n    size_t i;\n    for (i = 8; i + 8 <= length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(vec + i));\n        min_val = _mm_min_epi16(min_val, chunk);\n        max_val = _mm_max_epi16(max_val, chunk);\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_val);\n    _mm_store_si128((__m128i*)max_arr, max_val);\n\n    int16_t min_result = min_arr[0];\n    int16_t max_result = max_arr[0];\n    for (size_t j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_result) min_result = min_arr[j];\n        if (max_arr[j] > max_result) max_result = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result = {min_result, max_result};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vec = _mm_loadu_si128((__m128i*)vec);\n    __m128i max_vec = min_vec;\n\n    size_t i;\n    for (i = 8; i + 8 <= length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, chunk);\n        max_vec = _mm_max_epi16(max_vec, chunk);\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_vec);\n    _mm_store_si128((__m128i*)max_arr, max_vec);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (size_t j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vec = _mm_loadu_si128((__m128i*)vec);\n    __m128i max_vec = min_vec;\n    \n    size_t i;\n    for (i = 8; i + 8 <= length; i += 8) {\n        __m128i current = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, current);\n        max_vec = _mm_max_epi16(max_vec, current);\n    }\n    \n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_vec);\n    _mm_store_si128((__m128i*)max_arr, max_vec);\n    \n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (size_t j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n    \n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n    \n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vec = _mm_loadu_si128((__m128i*)vec);\n    __m128i max_vec = min_vec;\n\n    size_t i;\n    for (i = 8; i + 8 <= length; i += 8) {\n        __m128i current = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, current);\n        max_vec = _mm_max_epi16(max_vec, current);\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_vec);\n    _mm_store_si128((__m128i*)max_arr, max_vec);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (size_t j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vec = _mm_loadu_si128((__m128i*)vec);\n    __m128i max_vec = min_vec;\n\n    size_t i;\n    for (i = 8; i + 8 <= length; i += 8) {\n        __m128i current = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, current);\n        max_vec = _mm_max_epi16(max_vec, current);\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_vec);\n    _mm_store_si128((__m128i*)max_arr, max_vec);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (size_t j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            int mask = _mm_movemask_epi8(cmp);\n            \n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j + 1] <= row[j]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        \n        for (; j + 4 < cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            int mask = _mm_movemask_epi8(cmp);\n            \n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t simd_elements = 4;\n    size_t num_simd_blocks = block_elements / simd_elements;\n    size_t remainder = block_elements % simd_elements;\n\n    const int* first_block = matrix;\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + i * cols + j;\n            for (size_t k = 0; k < num_simd_blocks; ++k) {\n                __m128i first = _mm_loadu_si128((__m128i*)(first_block + k * simd_elements));\n                __m128i current = _mm_loadu_si128((__m128i*)(current_block + k * simd_elements));\n                __m128i cmp = _mm_cmpeq_epi32(first, current);\n                if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                    return false;\n                }\n            }\n\n            for (size_t k = num_simd_blocks * simd_elements; k < block_elements; ++k) {\n                if (first_block[k] != current_block[k]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t simd_elements = 4;\n    size_t remaining_elements = block_elements % simd_elements;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0) {\n                continue;\n            }\n\n            for (size_t k = 0; k < block_elements; k += simd_elements) {\n                size_t first_block_idx = (k / block_size) * cols + (k % block_size);\n                size_t current_block_idx = ((i + (k / block_size)) * cols + (j + (k % block_size));\n\n                __m128i first_block = _mm_loadu_si128((__m128i*)&matrix[first_block_idx]);\n                __m128i current_block = _mm_loadu_si128((__m128i*)&matrix[current_block_idx]);\n                __m128i cmp_result = _mm_cmpeq_epi32(first_block, current_block);\n\n                if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                    return false;\n                }\n            }\n\n            for (size_t k = block_elements - remaining_elements; k < block_elements; ++k) {\n                size_t first_block_idx = (k / block_size) * cols + (k % block_size);\n                size_t current_block_idx = ((i + (k / block_size)) * cols + (j + (k % block_size)));\n\n                if (matrix[first_block_idx] != matrix[current_block_idx]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    const int* first_block = matrix;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* current_block = matrix + i * cols + j;\n            \n            for (size_t k = 0; k < block_elements; k += 4) {\n                if (k + 4 <= block_elements) {\n                    __m128i first = _mm_loadu_si128((__m128i*)(first_block + k));\n                    __m128i current = _mm_loadu_si128((__m128i*)(current_block + k));\n                    __m128i cmp = _mm_cmpeq_epi32(first, current);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                } else {\n                    for (size_t l = k; l < block_elements; ++l) {\n                        if (first_block[l] != current_block[l]) {\n                            return false;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    const int* first_block = matrix;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + i * cols + j;\n            size_t k = 0;\n\n            for (; k + 4 <= block_elements; k += 4) {\n                __m128i first = _mm_loadu_si128((__m128i*)(first_block + k));\n                __m128i current = _mm_loadu_si128((__m128i*)(current_block + k));\n                __m128i cmp = _mm_cmpeq_epi32(first, current);\n                if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                    return false;\n                }\n            }\n\n            for (; k < block_elements; ++k) {\n                if (first_block[k] != current_block[k]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n    for (size_t block_row = 0; block_row < num_blocks_rows; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_cols; ++block_col) {\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n            for (size_t i = 0; i < block_elements; i += 4) {\n                __m128i first = _mm_loadu_si128((__m128i*)(first_block + i));\n                __m128i current = _mm_loadu_si128((__m128i*)(current_block + i));\n                __m128i cmp = _mm_cmpeq_epi32(first, current);\n                if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return true;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d neg_tol = _mm_set1_pd(-tolerance);\n\n    for (size_t i = 0; i < block_size; i += 2) {\n        __m128d first_block = _mm_loadu_pd(vec + i);\n        \n        for (size_t block = 1; block < num_blocks; ++block) {\n            __m128d current = _mm_loadu_pd(vec + block * block_size + i);\n            __m128d diff = _mm_sub_pd(current, first_block);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            \n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return true;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d neg_tol = _mm_set1_pd(-tolerance);\n\n    for (size_t i = 0; i < block_size; i += 2) {\n        __m128d first_block = _mm_loadu_pd(vec + i);\n        \n        for (size_t block = 1; block < num_blocks; ++block) {\n            __m128d current = _mm_loadu_pd(vec + block * block_size + i);\n            __m128d diff = _mm_sub_pd(current, first_block);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size; i += 2) {\n            __m128d a = _mm_loadu_pd(first_block + i);\n            __m128d b = _mm_loadu_pd(current_block + i);\n            __m128d diff = _mm_sub_pd(a, b);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        current_block += block_size;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t block = 1; block < num_blocks; ++block) {\n        for (size_t i = 0; i < block_size; i += 2) {\n            __m128d a = _mm_loadu_pd(first_block + i);\n            __m128d b = _mm_loadu_pd(current_block + i);\n            __m128d diff = _mm_sub_pd(a, b);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        current_block += block_size;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    const double* first_block = vec;\n    const double* current_block = vec + block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d a = _mm_loadu_pd(first_block + j);\n            __m128d b = _mm_loadu_pd(current_block + j);\n            __m128d diff = _mm_sub_pd(a, b);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            sum = _mm_add_pd(sum, _mm_mul_pd(row_vec, row_vec));\n        }\n        double row_sum = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n        if (j < cols) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n        result[i] = (sqrt(row_sum) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d squared = _mm_mul_pd(row_vec, row_vec);\n            sum = _mm_add_pd(sum, squared);\n        }\n        double row_sum = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n        double norm = sqrt(row_sum);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d squared = _mm_mul_pd(row_vec, row_vec);\n            sum = _mm_add_pd(sum, squared);\n        }\n        double row_sum = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n        double norm = sqrt(row_sum);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d squared = _mm_mul_pd(row_vec, row_vec);\n            sum = _mm_add_pd(sum, squared);\n        }\n        double row_sum = _mm_cvtsd_f64(_mm_hadd_pd(sum, sum));\n        if (cols % 2 != 0) {\n            double last_val = matrix[i * cols + cols - 1];\n            row_sum += last_val * last_val;\n        }\n        result[i] = (sqrt(row_sum) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row = _mm_loadu_pd(matrix + i * cols + j);\n            sum = _mm_add_pd(sum, _mm_mul_pd(row, row));\n        }\n        double row_sum = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n        if (cols % 2 != 0) {\n            double last = matrix[i * cols + cols - 1];\n            row_sum += last * last;\n        }\n        result[i] = (sqrt(row_sum) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    \n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_maddubs_epi16(a, a);\n        __m128i b_sq = _mm_maddubs_epi16(b, b);\n        __m128i ab = _mm_maddubs_epi16(a, b);\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(_mm_cvtepi16_epi32(a_sq), _mm_cvtepi16_epi32(b_sq)));\n        sum_prod = _mm_add_epi32(sum_prod, _mm_cvtepi16_epi32(ab));\n    }\n\n    int32_t sum_sq_arr[4] = {0};\n    int32_t sum_prod_arr[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_sq_arr, sum_sq);\n    _mm_storeu_si128((__m128i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = sum_sq_arr[0] + sum_sq_arr[1] + sum_sq_arr[2] + sum_sq_arr[3];\n    int64_t total_sum_prod = sum_prod_arr[0] + sum_prod_arr[1] + sum_prod_arr[2] + sum_prod_arr[3];\n\n    for (; i < length; ++i) {\n        total_sum_sq += A[i] * A[i] + B[i] * B[i];\n        total_sum_prod += A[i] * B[i];\n    }\n\n    return total_sum_sq - total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_maddubs_epi16(a, a);\n        __m128i b_sq = _mm_maddubs_epi16(b, b);\n        __m128i prod = _mm_maddubs_epi16(a, b);\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_unpacklo_epi16(a_sq, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(a_sq, _mm_setzero_si128())\n        ));\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_unpacklo_epi16(b_sq, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(b_sq, _mm_setzero_si128())\n        ));\n        sum_prod = _mm_add_epi32(sum_prod, _mm_add_epi32(\n            _mm_unpacklo_epi16(prod, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(prod, _mm_setzero_si128())\n        ));\n    }\n\n    int32_t sum_sq_arr[4] = {0};\n    int32_t sum_prod_arr[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_sq_arr, sum_sq);\n    _mm_storeu_si128((__m128i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = sum_sq_arr[0] + sum_sq_arr[1] + sum_sq_arr[2] + sum_sq_arr[3];\n    int64_t total_sum_prod = sum_prod_arr[0] + sum_prod_arr[1] + sum_prod_arr[2] + sum_prod_arr[3];\n\n    for (; i < length; ++i) {\n        total_sum_sq += (A[i] * A[i]) + (B[i] * B[i]);\n        total_sum_prod += A[i] * B[i];\n    }\n\n    return total_sum_sq - 2 * total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_maddubs_epi16(a, a);\n        __m128i b_sq = _mm_maddubs_epi16(b, b);\n        __m128i ab = _mm_maddubs_epi16(a, b);\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_unpacklo_epi16(a_sq, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(a_sq, _mm_setzero_si128())\n        ));\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_unpacklo_epi16(b_sq, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(b_sq, _mm_setzero_si128())\n        ));\n        sum_prod = _mm_add_epi32(sum_prod, _mm_add_epi32(\n            _mm_unpacklo_epi16(ab, _mm_setzero_si128()),\n            _mm_unpackhi_epi16(ab, _mm_setzero_si128())\n        ));\n    }\n\n    int32_t sum_sq_arr[4] = {0};\n    int32_t sum_prod_arr[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_sq_arr, sum_sq);\n    _mm_storeu_si128((__m128i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = sum_sq_arr[0] + sum_sq_arr[1] + sum_sq_arr[2] + sum_sq_arr[3];\n    int64_t total_sum_prod = sum_prod_arr[0] + sum_prod_arr[1] + sum_prod_arr[2] + sum_prod_arr[3];\n\n    for (; i < length; ++i) {\n        total_sum_sq += (int64_t)A[i] * A[i] + (int64_t)B[i] * B[i];\n        total_sum_prod += (int64_t)A[i] * B[i];\n    }\n\n    return total_sum_sq - 2 * total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_maddubs_epi16(a, a);\n        __m128i b_sq = _mm_maddubs_epi16(b, b);\n        __m128i prod = _mm_maddubs_epi16(a, b);\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_madd_epi16(a_sq, _mm_set1_epi16(1)),\n            _mm_madd_epi16(b_sq, _mm_set1_epi16(1))\n        ));\n        sum_prod = _mm_add_epi32(sum_prod, _mm_madd_epi16(prod, _mm_set1_epi16(1)));\n    }\n\n    int32_t sum_sq_arr[4] = {0};\n    int32_t sum_prod_arr[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_sq_arr, sum_sq);\n    _mm_storeu_si128((__m128i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sum_sq = sum_sq_arr[0] + sum_sq_arr[1] + sum_sq_arr[2] + sum_sq_arr[3];\n    int64_t total_sum_prod = sum_prod_arr[0] + sum_prod_arr[1] + sum_prod_arr[2] + sum_prod_arr[3];\n\n    for (; i < length; ++i) {\n        total_sum_sq += A[i] * A[i] + B[i] * B[i];\n        total_sum_prod += A[i] * B[i];\n    }\n\n    return total_sum_sq - 2 * total_sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_maddubs_epi16(a, a);\n        __m128i b_sq = _mm_maddubs_epi16(b, b);\n        __m128i prod = _mm_maddubs_epi16(a, b);\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_cvtepi16_epi32(a_sq),\n            _mm_cvtepi16_epi32(_mm_srli_si128(a_sq, 8))\n        ));\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(\n            _mm_cvtepi16_epi32(b_sq),\n            _mm_cvtepi16_epi32(_mm_srli_si128(b_sq, 8))\n        );\n        sum_prod = _mm_add_epi32(sum_prod, _mm_add_epi32(\n            _mm_cvtepi16_epi32(prod),\n            _mm_cvtepi16_epi32(_mm_srli_si128(prod, 8))\n        ));\n    }\n\n    int32_t sum_sq_arr[4] = {0};\n    int32_t sum_prod_arr[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_sq_arr, sum_sq);\n    _mm_storeu_si128((__m128i*)sum_prod_arr, sum_prod);\n\n    int64_t total_sq = sum_sq_arr[0] + sum_sq_arr[1] + sum_sq_arr[2] + sum_sq_arr[3];\n    int64_t total_prod = sum_prod_arr[0] + sum_prod_arr[1] + sum_prod_arr[2] + sum_prod_arr[3];\n\n    for (; i < length; ++i) {\n        total_sq += A[i] * A[i] + B[i] * B[i];\n        total_prod += A[i] * B[i];\n    }\n\n    return total_sq - total_prod;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(~0ULL));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    // Handle remaining elements if length is odd\n    if (length & 1) {\n        C[length - 1] = A[length - 1] & ~B[length - 1];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    // Handle remaining elements if length is odd\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(0xFFFFFFFFFFFFFFFF));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    // Handle remaining elements if length is odd\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(~0ULL));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    // Handle remaining elements if length is odd\n    if (length & 1) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_pos = _mm_cmpgt_epi8(a, _mm_setzero_si128());\n        __m128i b_pos = _mm_cmpgt_epi8(b, _mm_setzero_si128());\n        __m128i a_neg = _mm_cmplt_epi8(a, _mm_setzero_si128());\n        __m128i b_neg = _mm_cmplt_epi8(b, _mm_setzero_si128());\n\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, _mm_set1_epi8(1)),\n            _mm_and_si128(both_neg, _mm_set1_epi8(-1))\n        );\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, _mm_setzero_si128());\n        __m128i b_pos = _mm_cmpgt_epi8(b, _mm_setzero_si128());\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        \n        __m128i a_neg = _mm_cmplt_epi8(a, _mm_setzero_si128());\n        __m128i b_neg = _mm_cmplt_epi8(b, _mm_setzero_si128());\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, _mm_set1_epi8(1)),\n            _mm_and_si128(both_neg, _mm_set1_epi8(-1))\n        );\n        \n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, _mm_setzero_si128());\n        __m128i b_pos = _mm_cmpgt_epi8(b, _mm_setzero_si128());\n        __m128i a_neg = _mm_cmplt_epi8(a, _mm_setzero_si128());\n        __m128i b_neg = _mm_cmplt_epi8(b, _mm_setzero_si128());\n        \n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, _mm_set1_epi8(1)),\n            _mm_and_si128(both_neg, _mm_set1_epi8(-1))\n        );\n        \n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, _mm_setzero_si128());\n        __m128i b_pos = _mm_cmpgt_epi8(b, _mm_setzero_si128());\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        \n        __m128i a_neg = _mm_cmplt_epi8(a, _mm_setzero_si128());\n        __m128i b_neg = _mm_cmplt_epi8(b, _mm_setzero_si128());\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, _mm_set1_epi8(1)),\n            _mm_and_si128(both_neg, _mm_set1_epi8(-1))\n        );\n        \n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, _mm_setzero_si128());\n        __m128i b_pos = _mm_cmpgt_epi8(b, _mm_setzero_si128());\n        __m128i a_neg = _mm_cmplt_epi8(a, _mm_setzero_si128());\n        __m128i b_neg = _mm_cmplt_epi8(b, _mm_setzero_si128());\n        \n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, _mm_set1_epi8(1)),\n            _mm_and_si128(both_neg, _mm_set1_epi8(-1))\n        );\n        \n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle midpoint element\n    if (length % 2 == 1) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n\n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Process remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n\n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Handle remaining elements before midpoint (XOR)\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint element\n    if (length % 2 == 1) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n    \n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    // Handle remaining elements after midpoint (AND)\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements before midpoint (XOR)\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        i = midpoint;\n    }\n\n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    // Process remaining elements after midpoint (AND)\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n    \n    for (i = 0; i + 4 <= total_elements; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n        \n        __m128i cond1_mask = _mm_set_epi32(\n            cond1[i+3] ? 0xFFFFFFFF : 0,\n            cond1[i+2] ? 0xFFFFFFFF : 0,\n            cond1[i+1] ? 0xFFFFFFFF : 0,\n            cond1[i]   ? 0xFFFFFFFF : 0\n        );\n        __m128i cond2_mask = _mm_set_epi32(\n            cond2[i+3] ? 0xFFFFFFFF : 0,\n            cond2[i+2] ? 0xFFFFFFFF : 0,\n            cond2[i+1] ? 0xFFFFFFFF : 0,\n            cond2[i]   ? 0xFFFFFFFF : 0\n        );\n        \n        __m128 case1 = _mm_and_ps(_mm_castsi128_ps(_mm_and_si128(cond1_mask, cond2_mask)), a);\n        __m128 case2 = _mm_and_ps(_mm_castsi128_ps(_mm_andnot_si128(cond2_mask, cond1_mask)), b);\n        __m128 case3 = _mm_and_ps(_mm_castsi128_ps(_mm_andnot_si128(cond1_mask, cond2_mask)), c);\n        __m128 case4 = _mm_and_ps(_mm_castsi128_ps(_mm_andnot_si128(_mm_or_si128(cond1_mask, cond2_mask), _mm_set1_epi32(0xFFFFFFFF))), d);\n        \n        __m128 result = _mm_or_ps(_mm_or_ps(case1, case2), _mm_or_ps(case3, case4));\n        _mm_storeu_ps(&out[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 mask1 = _mm_castsi128_ps(_mm_set_epi32(\n            cond1[i+3] ? 0xFFFFFFFF : 0,\n            cond1[i+2] ? 0xFFFFFFFF : 0,\n            cond1[i+1] ? 0xFFFFFFFF : 0,\n            cond1[i]   ? 0xFFFFFFFF : 0\n        ));\n        \n        __m128 mask2 = _mm_castsi128_ps(_mm_set_epi32(\n            cond2[i+3] ? 0xFFFFFFFF : 0,\n            cond2[i+2] ? 0xFFFFFFFF : 0,\n            cond2[i+1] ? 0xFFFFFFFF : 0,\n            cond2[i]   ? 0xFFFFFFFF : 0\n        ));\n        \n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n        \n        __m128 case1 = _mm_and_ps(_mm_and_ps(mask1, mask2), a);\n        __m128 case2 = _mm_and_ps(_mm_andnot_ps(mask2, mask1), b);\n        __m128 case3 = _mm_and_ps(_mm_andnot_ps(mask1, mask2), c);\n        __m128 case4 = _mm_and_ps(_mm_andnot_ps(mask1, _mm_andnot_ps(mask2, _mm_set1_ps(-1.0f))), d);\n        \n        __m128 result = _mm_or_ps(_mm_or_ps(case1, case2), _mm_or_ps(case3, case4));\n        _mm_storeu_ps(&out[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 mask1 = _mm_castsi128_ps(_mm_set_epi32(\n            cond1[i+3] ? 0xFFFFFFFF : 0,\n            cond1[i+2] ? 0xFFFFFFFF : 0,\n            cond1[i+1] ? 0xFFFFFFFF : 0,\n            cond1[i]   ? 0xFFFFFFFF : 0\n        ));\n        \n        __m128 mask2 = _mm_castsi128_ps(_mm_set_epi32(\n            cond2[i+3] ? 0xFFFFFFFF : 0,\n            cond2[i+2] ? 0xFFFFFFFF : 0,\n            cond2[i+1] ? 0xFFFFFFFF : 0,\n            cond2[i]   ? 0xFFFFFFFF : 0\n        ));\n        \n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n        \n        __m128 cond1_and_cond2 = _mm_and_ps(mask1, mask2);\n        __m128 cond1_and_not_cond2 = _mm_andnot_ps(mask2, mask1);\n        __m128 not_cond1_and_cond2 = _mm_andnot_ps(mask1, mask2);\n        __m128 not_cond1_and_not_cond2 = _mm_andnot_ps(_mm_or_ps(mask1, mask2), _mm_set1_ps(1.0f));\n        \n        __m128 result = _mm_add_ps(\n            _mm_add_ps(\n                _mm_and_ps(cond1_and_cond2, a),\n                _mm_and_ps(cond1_and_not_cond2, b)\n            ),\n            _mm_add_ps(\n                _mm_and_ps(not_cond1_and_cond2, c),\n                _mm_and_ps(not_cond1_and_not_cond2, d)\n            )\n        );\n        \n        _mm_storeu_ps(&out[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i c1 = _mm_loadu_si128((__m128i*)(cond1 + i));\n        __m128i c2 = _mm_loadu_si128((__m128i*)(cond2 + i));\n        \n        __m128i mask1 = _mm_and_si128(c1, c2);\n        __m128i mask2 = _mm_and_si128(c1, _mm_andnot_si128(c2, _mm_set1_epi32(-1)));\n        __m128i mask3 = _mm_and_si128(_mm_andnot_si128(c1, _mm_set1_epi32(-1)), c2);\n        __m128i mask4 = _mm_andnot_si128(_mm_or_si128(c1, c2), _mm_set1_epi32(-1));\n        \n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 c = _mm_loadu_ps(C + i);\n        __m128 d = _mm_loadu_ps(D + i);\n        \n        __m128 res = _mm_add_ps(\n            _mm_add_ps(\n                _mm_and_ps(_mm_castsi128_ps(mask1), a),\n                _mm_and_ps(_mm_castsi128_ps(mask2), b)\n            ),\n            _mm_add_ps(\n                _mm_and_ps(_mm_castsi128_ps(mask3), c),\n                _mm_and_ps(_mm_castsi128_ps(mask4), d)\n            )\n        );\n        \n        _mm_storeu_ps(out + i, res);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n        \n        __m128i cond1_mask = _mm_set_epi32(\n            cond1[i+3] ? 0xFFFFFFFF : 0,\n            cond1[i+2] ? 0xFFFFFFFF : 0,\n            cond1[i+1] ? 0xFFFFFFFF : 0,\n            cond1[i]   ? 0xFFFFFFFF : 0\n        );\n        __m128i cond2_mask = _mm_set_epi32(\n            cond2[i+3] ? 0xFFFFFFFF : 0,\n            cond2[i+2] ? 0xFFFFFFFF : 0,\n            cond2[i+1] ? 0xFFFFFFFF : 0,\n            cond2[i]   ? 0xFFFFFFFF : 0\n        );\n        \n        __m128 cond1_true = _mm_castsi128_ps(cond1_mask);\n        __m128 cond2_true = _mm_castsi128_ps(cond2_mask);\n        \n        __m128 a_or_b = _mm_or_ps(\n            _mm_and_ps(cond1_true, _mm_and_ps(cond2_true, a)),\n            _mm_and_ps(cond1_true, _mm_andnot_ps(cond2_true, b))\n        );\n        \n        __m128 c_or_d = _mm_or_ps(\n            _mm_andnot_ps(cond1_true, _mm_and_ps(cond2_true, c)),\n            _mm_andnot_ps(cond1_true, _mm_andnot_ps(cond2_true, d))\n        );\n        \n        __m128 result = _mm_or_ps(a_or_b, c_or_d);\n        _mm_storeu_ps(&out[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask4 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask8 = _mm_set1_epi32(0x00FF00FF);\n        __m128i mask16 = _mm_set1_epi32(0x0000FFFF);\n        \n        __m128i v1 = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        __m128i v2 = _mm_add_epi32(_mm_and_si128(v1, mask2), _mm_and_si128(_mm_srli_epi32(v1, 2), mask2));\n        __m128i v4 = _mm_add_epi32(_mm_and_si128(v2, mask4), _mm_and_si128(_mm_srli_epi32(v2, 4), mask4));\n        __m128i v8 = _mm_add_epi32(_mm_and_si128(v4, mask8), _mm_and_si128(_mm_srli_epi32(v4, 8), mask8));\n        __m128i v16 = _mm_add_epi32(_mm_and_si128(v8, mask16), _mm_and_si128(_mm_srli_epi32(v8, 16), mask16));\n        \n        __m128i result = _mm_packs_epi32(v16, v16);\n        result = _mm_packus_epi16(result, result);\n        \n        uint32_t res[4];\n        _mm_storeu_si128((__m128i*)res, result);\n        out[i] = res[0];\n        out[i+1] = res[1];\n        out[i+2] = res[2];\n        out[i+3] = res[3];\n    }\n    \n    for (; i < total_elements; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        out[i] = x & 0x3F;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        \n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n        __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n        \n        __m128i v1 = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        __m128i v2 = _mm_add_epi32(_mm_and_si128(v1, mask2), _mm_and_si128(_mm_srli_epi32(v1, 2), mask2));\n        __m128i v3 = _mm_add_epi32(_mm_and_si128(v2, mask3), _mm_and_si128(_mm_srli_epi32(v2, 4), mask3));\n        __m128i v4 = _mm_add_epi32(_mm_and_si128(v3, mask4), _mm_and_si128(_mm_srli_epi32(v3, 8), mask4));\n        __m128i v5 = _mm_add_epi32(_mm_and_si128(v4, mask5), _mm_and_si128(_mm_srli_epi32(v4, 16), mask5));\n        \n        uint32_t counts[4];\n        _mm_storeu_si128((__m128i*)counts, v5);\n        \n        for (size_t j = 0; j < 4; j++) {\n            out[i + j] = (uint8_t)counts[j];\n        }\n    }\n    \n    for (; i < total_elements; i++) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        out[i] = (uint8_t)(x & 0x3F);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i v_pop = _mm_setzero_si128();\n        \n        for (int j = 0; j < 32; ++j) {\n            __m128i mask = _mm_set1_epi32(1 << j);\n            __m128i bits = _mm_and_si128(v, mask);\n            __m128i cmp = _mm_cmpeq_epi32(bits, mask);\n            v_pop = _mm_sub_epi32(v_pop, cmp);\n        }\n\n        __m128i v_pop8 = _mm_packs_epi32(v_pop, v_pop);\n        v_pop8 = _mm_packus_epi16(v_pop8, v_pop8);\n        *((uint32_t*)(out + i)) = _mm_cvtsi128_si32(v_pop8);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        uint8_t count = 0;\n        while (val) {\n            count += val & 1;\n            val >>= 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n        __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n\n        __m128i v1 = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        __m128i v2 = _mm_add_epi32(_mm_and_si128(v1, mask2), _mm_and_si128(_mm_srli_epi32(v1, 2), mask2));\n        __m128i v3 = _mm_add_epi32(_mm_and_si128(v2, mask3), _mm_and_si128(_mm_srli_epi32(v2, 4), mask3));\n        __m128i v4 = _mm_add_epi32(_mm_and_si128(v3, mask4), _mm_and_si128(_mm_srli_epi32(v3, 8), mask4));\n        __m128i v5 = _mm_add_epi32(_mm_and_si128(v4, mask5), _mm_and_si128(_mm_srli_epi32(v4, 16), mask5));\n\n        __m128i res = _mm_packs_epi32(v5, v5);\n        res = _mm_packus_epi16(res, res);\n        _mm_store_ss((float*)&out[i], _mm_castsi128_ps(res));\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val * 0x01010101) >> 24;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n        __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n\n        v = _mm_sub_epi32(v, _mm_and_si128(_mm_srli_epi32(v, 1), mask1);\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask3), _mm_and_si128(_mm_srli_epi32(v, 4), mask3));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 8), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask5), _mm_and_si128(_mm_srli_epi32(v, 16), mask5));\n\n        __m128i result = _mm_packs_epi32(v, v);\n        result = _mm_packus_epi16(result, result);\n        _mm_store_ss((float*)&out[i], _mm_castsi128_ps(result));\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val + (val >> 8)) & 0x00FF00FF;\n        val = (val + (val >> 16)) & 0x0000FFFF;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i;\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (i = 0; i + 3 < size; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i remainder = _mm_abs_epi32(a);\n        remainder = _mm_sub_epi32(remainder, _mm_mullo_epi32(_mm_div_epi32(remainder, three), three));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        __m128i abs_val = _mm_and_si128(a, mask);\n        __m128i res = _mm_blendv_epi8(a, abs_val, cmp);\n        _mm_storeu_si128((__m128i*)(B + i), res);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 3 < size; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i remainder = _mm_abs_epi32(a);\n        remainder = _mm_sub_epi32(remainder, _mm_mullo_epi32(_mm_div_epi32(remainder, three), three));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        __m128i abs_val = _mm_and_si128(a, mask);\n        __m128i result = _mm_blendv_epi8(a, abs_val, cmp);\n        _mm_storeu_si128((__m128i*)(B + i), result);\n    }\n\n    for (; i < size; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i remainder = _mm_abs_epi32(a);\n        remainder = _mm_sub_epi32(remainder, _mm_mullo_epi32(_mm_div_epi32(remainder, three), three));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        __m128i abs_a = _mm_and_si128(a, mask);\n        __m128i result = _mm_blendv_epi8(a, abs_a, cmp);\n        _mm_storeu_si128((__m128i*)&B[i], result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i remainder = _mm_abs_epi32(a);\n        remainder = _mm_sub_epi32(remainder, _mm_mullo_epi32(_mm_div_epi32(remainder, three), three);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        __m128i abs_a = _mm_and_si128(a, mask);\n        __m128i result = _mm_blendv_epi8(a, abs_a, cmp);\n        _mm_storeu_si128((__m128i*)(B + i), result);\n    }\n    \n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = abs(val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i remainder = _mm_abs_epi32(a);\n        remainder = _mm_sub_epi32(remainder, _mm_mullo_epi32(_mm_div_epi32(remainder, three), three));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        __m128i abs_a = _mm_and_si128(a, mask);\n        __m128i result = _mm_blendv_epi8(a, abs_a, cmp);\n        _mm_storeu_si128((__m128i*)(B + i), result);\n    }\n\n    for (; i < total_elements; ++i) {\n        int val = A[i];\n        B[i] = (val % 3 == 0) ? abs(val) : val;\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xF);\n    __m128i rotate_val = _mm_set1_epi32(rotate_bits);\n    __m128i neg_one = _mm_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(src_vec, mask_low4);\n        __m128i cond = _mm_cmpeq_epi32(low4, all_ones);\n        \n        __m128i rotated = _mm_or_si128(\n            _mm_srl_epi32(src_vec, rotate_val),\n            _mm_sll_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_val))\n        );\n        \n        __m128i negated = _mm_xor_si128(src_vec, neg_one);\n        \n        __m128i res = _mm_blendv_epi8(negated, rotated, cond);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF);\n    __m128i mask_all1 = _mm_set1_epi32(0xF);\n    __m128i rot_bits = _mm_set1_epi32(rotate_bits);\n    __m128i inv_rot_bits = _mm_set1_epi32(32 - rotate_bits);\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(src_vec, mask_low4);\n        \n        __m128i cond = _mm_cmpeq_epi32(low4, mask_all1);\n        \n        __m128i rotated = _mm_or_si128(\n            _mm_srl_epi32(src_vec, rot_bits),\n            _mm_sll_epi32(src_vec, inv_rot_bits)\n        );\n        \n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n        \n        __m128i res = _mm_blendv_epi8(negated, rotated, cond);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    \n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xF);\n    __m128i rotate_val = _mm_set1_epi32(rotate_bits);\n    __m128i neg_one = _mm_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(src_vec, mask_low4);\n        __m128i cond = _mm_cmpeq_epi32(low4, all_ones);\n        \n        __m128i rotated = _mm_or_si128(\n            _mm_srl_epi32(src_vec, rotate_val),\n            _mm_sll_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_val))\n        );\n        \n        __m128i negated = _mm_xor_si128(src_vec, neg_one);\n        \n        __m128i res = _mm_blendv_epi8(negated, rotated, cond);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xF);\n    __m128i rotate_val = _mm_set1_epi32(rotate_bits);\n    __m128i inv_rotate_val = _mm_set1_epi32(32 - rotate_bits);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(src_vec, mask_low4);\n        __m128i cond = _mm_cmpeq_epi32(low4, all_ones);\n        \n        __m128i rotated = _mm_or_si128(\n            _mm_srl_epi32(src_vec, rotate_val),\n            _mm_sll_epi32(src_vec, inv_rotate_val)\n        );\n        \n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n        \n        __m128i result = _mm_blendv_epi8(negated, rotated, cond);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xF);\n    __m128i rotate_val = _mm_set1_epi32(rotate_bits);\n    __m128i inv_rotate_val = _mm_set1_epi32(32 - rotate_bits);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(src_vec, mask_low4);\n        __m128i cond = _mm_cmpeq_epi32(low4, all_ones);\n\n        __m128i rotated = _mm_or_si128(\n            _mm_srl_epi32(src_vec, rotate_val),\n            _mm_sll_epi32(src_vec, inv_rotate_val)\n        );\n\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n        __m128i res = _mm_blendv_epi8(negated, rotated, cond);\n\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(fabsf(epsilon));\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_max_ps(diff, _mm_sub_ps(zero, diff));\n\n        __m128 mask_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 mask_lt = _mm_cmplt_ps(diff, _mm_sub_ps(zero, eps));\n        __m128 mask_eq = _mm_cmple_ps(abs_diff, eps);\n\n        __m128 res = _mm_blendv_ps(\n            _mm_blendv_ps(neg_one, one, mask_gt),\n            zero,\n            mask_eq\n        );\n\n        __m128i res_i32 = _mm_cvtps_epi32(res);\n        __m128i res_i8 = _mm_packs_epi32(res_i32, res_i32);\n        res_i8 = _mm_packs_epi16(res_i8, res_i8);\n\n        *(int32_t*)&out[i] = _mm_cvtsi128_si32(res_i8);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        float abs_diff = fabsf(diff);\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 abs_eps = _mm_andnot_ps(_mm_set1_ps(-0.0f), eps);\n    __m128 neg_abs_eps = _mm_sub_ps(_mm_setzero_ps(), abs_eps);\n    \n    __m128i ones = _mm_set1_epi8(1);\n    __m128i neg_ones = _mm_set1_epi8(-1);\n    __m128i zeros = _mm_set1_epi8(0);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 mask_gt = _mm_cmpgt_ps(diff, abs_eps);\n        __m128 mask_lt = _mm_cmplt_ps(diff, neg_abs_eps);\n        \n        __m128i res_gt = _mm_and_si128(_mm_castps_si128(mask_gt), ones);\n        __m128i res_lt = _mm_and_si128(_mm_castps_si128(mask_lt), neg_ones);\n        __m128i res = _mm_or_si128(res_gt, res_lt);\n\n        _mm_storeu_epi8(&out[i], res);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(fabsf(epsilon));\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_max_ps(diff, _mm_sub_ps(zero, diff));\n\n        __m128 mask_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 mask_lt = _mm_cmplt_ps(diff, _mm_sub_ps(zero, eps));\n        __m128 mask_eq = _mm_cmple_ps(abs_diff, eps);\n\n        __m128 res = _mm_add_ps(\n            _mm_and_ps(mask_gt, one),\n            _mm_and_ps(mask_lt, neg_one)\n        );\n        res = _mm_add_ps(res, _mm_and_ps(mask_eq, zero));\n\n        __m128i res_i32 = _mm_cvtps_epi32(res);\n        __m128i res_i16 = _mm_packs_epi32(res_i32, res_i32);\n        __m128i res_i8 = _mm_packs_epi16(res_i16, res_i16);\n\n        _mm_store_ss((float*)&out[i], _mm_castsi128_ps(res_i8));\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        float abs_diff = fabsf(diff);\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(fabsf(epsilon));\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(zero, diff), diff);\n        \n        __m128 mask_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 mask_lt = _mm_cmplt_ps(diff, _mm_sub_ps(zero, eps));\n        \n        __m128 res = _mm_and_ps(mask_gt, one);\n        res = _mm_or_ps(res, _mm_and_ps(mask_lt, neg_one));\n        \n        __m128i res_i32 = _mm_cvtps_epi32(res);\n        __m128i res_i16 = _mm_packs_epi32(res_i32, res_i32);\n        __m128i res_i8 = _mm_packs_epi16(res_i16, res_i16);\n        \n        *(int32_t*)(out + i) = _mm_cvtsi128_si32(res_i8);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(fabsf(epsilon));\n    __m128 neg_eps = _mm_set1_ps(-fabsf(epsilon));\n    \n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n        \n        __m128 gt_mask = _mm_cmpgt_ps(diff, eps);\n        __m128 lt_mask = _mm_cmplt_ps(diff, neg_eps);\n        \n        __m128 gt_result = _mm_and_ps(gt_mask, _mm_set1_ps(1.0f));\n        __m128 lt_result = _mm_and_ps(lt_mask, _mm_set1_ps(-1.0f));\n        __m128 result = _mm_or_ps(gt_result, lt_result);\n        \n        __m128i int_result = _mm_cvtps_epi32(result);\n        __m128i byte_result = _mm_packs_epi32(int_result, int_result);\n        byte_result = _mm_packs_epi16(byte_result, byte_result);\n        \n        int32_t res = _mm_cvtsi128_si32(byte_result);\n        out[i] = (res >> 0) & 0xFF;\n        out[i+1] = (res >> 8) & 0xFF;\n        out[i+2] = (res >> 16) & 0xFF;\n        out[i+3] = (res >> 24) & 0xFF;\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps((float)INT32_MAX);\n    const __m128 float_min = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 clamped = _mm_min_ps(_mm_max_ps(a, float_min), float_max);\n        __m128i converted = _mm_cvtps_epi32(clamped);\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps((float)INT32_MAX);\n    const __m128 float_min = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 clamped = _mm_min_ps(_mm_max_ps(a, float_min), float_max);\n        __m128i converted = _mm_cvtps_epi32(clamped);\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 zero = _mm_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128i result = _mm_cvtps_epi32(a);\n        \n        __m128 mask_gt = _mm_cmpgt_ps(a, _mm_cvtepi32_ps(int_max));\n        __m128 mask_lt = _mm_cmplt_ps(a, _mm_cvtepi32_ps(int_min));\n        \n        result = _mm_or_si128(_mm_and_si128(_mm_castps_si128(mask_gt), int_max),\n                             _mm_andnot_si128(_mm_castps_si128(mask_gt), result));\n        result = _mm_or_si128(_mm_and_si128(_mm_castps_si128(mask_lt), int_min),\n                             _mm_andnot_si128(_mm_castps_si128(mask_lt), result));\n        \n        _mm_storeu_si128((__m128i*)&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps((float)INT32_MAX);\n    const __m128 float_min = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 clamped = _mm_max_ps(_mm_min_ps(a, float_max), float_min);\n        __m128i converted = _mm_cvtps_epi32(clamped);\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps((float)INT32_MAX);\n    const __m128 float_min = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 val = _mm_loadu_ps(&A[i]);\n        __m128 clamped = _mm_min_ps(_mm_max_ps(val, float_min), float_max);\n        __m128i converted = _mm_cvtps_epi32(clamped);\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    size_t i;\n    \n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i scaled_vec = _mm_mullo_epi32(int_vec, scale_vec);\n        __m128 float_vec = _mm_cvtepi32_ps(scaled_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    size_t i;\n    \n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i scaled_vec = _mm_mullo_epi32(int_vec, scale_vec);\n        __m128 float_vec = _mm_cvtepi32_ps(scaled_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    size_t i;\n    \n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i scaled_vec = _mm_mullo_epi32(int_vec, scale_vec);\n        __m128 float_vec = _mm_cvtepi32_ps(scaled_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    size_t i;\n    \n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i scaled_int = _mm_mullo_epi32(int_vec, scale_vec);\n        __m128 float_vec = _mm_cvtepi32_ps(scaled_int);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i scaled_vec = _mm_mullo_epi32(int_vec, scale_vec);\n        __m128 float_vec = _mm_cvtepi32_ps(scaled_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_flt = _mm_set1_pd(FLT_MAX);\n    __m128d min_flt = _mm_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 2 <= size; i += 2) {\n        __m128d d = _mm_loadu_pd(&A[i]);\n        d = _mm_min_pd(d, max_flt);\n        d = _mm_max_pd(d, min_flt);\n        __m128 f = _mm_cvtpd_ps(d);\n        _mm_storel_pi((__m64*)&B[i], f);\n        _mm_storeh_pi((__m64*)&B[i+1], f);\n    }\n    \n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_d = _mm_set1_pd(-FLT_MAX);\n    \n    for (size_t i = 0; i < size; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm_min_pd(a, max_d);\n        a = _mm_max_pd(a, min_d);\n        \n        // Convert to float\n        __m128 b = _mm_cvtpd_ps(a);\n        \n        // Store results\n        if (i + 1 < size) {\n            _mm_storeu_ps(&B[i], b);\n        } else {\n            // Handle last element if size is odd\n            float temp[2];\n            _mm_storeu_ps(temp, b);\n            B[i] = temp[0];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_d = _mm_set1_pd(-FLT_MAX);\n    \n    for (size_t i = 0; i < size; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm_min_pd(a, max_d);\n        a = _mm_max_pd(a, min_d);\n        \n        // Convert double to float\n        __m128 a_float = _mm_cvtpd_ps(a);\n        \n        // Store results\n        if (i + 1 < size) {\n            _mm_storeu_ps(&B[i], a_float);\n        } else {\n            // Handle last element if size is odd\n            _mm_store_ss(&B[i], a_float);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d flt_max = _mm_set1_pd(FLT_MAX);\n    const __m128d neg_flt_max = _mm_set1_pd(-FLT_MAX);\n    \n    size_t i;\n    for (i = 0; i + 3 < size; i += 4) {\n        __m128d d0 = _mm_loadu_pd(A + i);\n        __m128d d1 = _mm_loadu_pd(A + i + 2);\n        \n        // Clamp values to FLT_MAX range\n        d0 = _mm_min_pd(d0, flt_max);\n        d0 = _mm_max_pd(d0, neg_flt_max);\n        \n        d1 = _mm_min_pd(d1, flt_max);\n        d1 = _mm_max_pd(d1, neg_flt_max);\n        \n        // Convert to float\n        __m128 f0 = _mm_cvtpd_ps(d0);\n        __m128 f1 = _mm_cvtpd_ps(d1);\n        \n        // Pack and store\n        __m128 f = _mm_shuffle_ps(f0, f1, _MM_SHUFFLE(1, 0, 1, 0));\n        _mm_storeu_ps(B + i, f);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_d = _mm_set1_pd(-FLT_MAX);\n    \n    for (size_t i = 0; i < size; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n        \n        // Clamp values to FLT_MAX and -FLT_MAX\n        a = _mm_min_pd(a, max_d);\n        a = _mm_max_pd(a, min_d);\n        \n        // Convert to float\n        __m128 b = _mm_cvtpd_ps(a);\n        \n        // Store results\n        if (i + 1 < size) {\n            _mm_storeu_ps(&B[i], b);\n        } else {\n            // Handle last element if size is odd\n            _mm_store_ss(&B[i], b);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_cvtepi16_epi32(a);\n        __m128i hi = _mm_cvtepi16_epi32(_mm_srli_si128(a, 8));\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_cvtepi16_epi32(a);\n        __m128i hi = _mm_cvtepi16_epi32(_mm_srli_si128(a, 8));\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_cvtepi16_epi32(a);\n        __m128i hi = _mm_cvtepi16_epi32(_mm_srli_si128(a, 8));\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_cvtepi16_epi32(a);\n        __m128i hi = _mm_cvtepi16_epi32(_mm_srli_si128(a, 8));\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_cvtepi16_epi32(a);\n        __m128i hi = _mm_cvtepi16_epi32(_mm_srli_si128(a, 8));\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_max = _mm_set1_ps(max_val);\n    __m128 v_range = _mm_sub_ps(v_max, v_min);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128i c = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 mask = _mm_castsi128_ps(_mm_cmpgt_epi32(c, _mm_setzero_si128()));\n\n        __m128 normalized = _mm_div_ps(_mm_sub_ps(a, v_min), v_range);\n        normalized = _mm_max_ps(normalized, v_zero);\n        normalized = _mm_min_ps(normalized, v_one);\n\n        __m128 result = _mm_blendv_ps(a, normalized, mask);\n        _mm_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_max = _mm_set1_ps(max_val);\n    __m128 v_range = _mm_sub_ps(v_max, v_min);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128i c = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 mask = _mm_castsi128_ps(_mm_cmpgt_epi32(c, _mm_setzero_si128()));\n        \n        __m128 normalized = _mm_div_ps(_mm_sub_ps(a, v_min), v_range);\n        normalized = _mm_max_ps(normalized, v_zero);\n        normalized = _mm_min_ps(normalized, v_one);\n        \n        __m128 result = _mm_blendv_ps(a, normalized, mask);\n        _mm_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_max = _mm_set1_ps(max_val);\n    __m128 v_range = _mm_sub_ps(v_max, v_min);\n    __m128 v_zero = _mm_set1_ps(0.0f);\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128i v_zero_int = _mm_set1_epi32(0);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 v_a = _mm_loadu_ps(&A[i]);\n        __m128i v_control = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128i v_mask = _mm_cmpgt_epi32(v_control, v_zero_int);\n        __m128 v_control_mask = _mm_castsi128_ps(_mm_castps_si128(_mm_cvtepi32_ps(v_mask)));\n\n        __m128 v_normalized = _mm_div_ps(_mm_sub_ps(v_a, v_min), v_range);\n        v_normalized = _mm_max_ps(v_normalized, v_zero);\n        v_normalized = _mm_min_ps(v_normalized, v_one);\n\n        __m128 v_result = _mm_blendv_ps(v_a, v_normalized, v_control_mask);\n        _mm_storeu_ps(&B[i], v_result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_max = _mm_set1_ps(max_val);\n    __m128 v_range = _mm_sub_ps(v_max, v_min);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128i v_zero_i = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 v_a = _mm_loadu_ps(&A[i]);\n        __m128i v_control = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128i v_mask = _mm_cmpgt_epi32(v_control, v_zero_i);\n        __m128 v_mask_ps = _mm_castsi128_ps(v_mask);\n\n        __m128 v_normalized = _mm_sub_ps(v_a, v_min);\n        v_normalized = _mm_div_ps(v_normalized, v_range);\n        v_normalized = _mm_max_ps(v_normalized, v_zero);\n        v_normalized = _mm_min_ps(v_normalized, v_one);\n\n        __m128 v_result = _mm_blendv_ps(v_a, v_normalized, v_mask_ps);\n        _mm_storeu_ps(&B[i], v_result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_max = _mm_set1_ps(max_val);\n    __m128 v_range = _mm_sub_ps(v_max, v_min);\n    __m128 v_zero = _mm_set1_ps(0.0f);\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128i v_zero_int = _mm_set1_epi32(0);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 v_a = _mm_loadu_ps(&A[i]);\n        __m128i v_control = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128i v_mask = _mm_cmpgt_epi32(v_control, v_zero_int);\n        __m128 v_mask_ps = _mm_castsi128_ps(v_mask);\n\n        __m128 v_normalized = _mm_div_ps(_mm_sub_ps(v_a, v_min), v_range);\n        v_normalized = _mm_max_ps(v_normalized, v_zero);\n        v_normalized = _mm_min_ps(v_normalized, v_one);\n\n        __m128 v_result = _mm_blendv_ps(v_a, v_normalized, v_mask_ps);\n        _mm_storeu_ps(&B[i], v_result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i shifted = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i hi16_vec = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(hi16_vec);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n    \n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        B[i] = sqrtf((float)val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i shifted = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        B[i] = sqrtf((float)val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i shifted = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n    \n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i hi = _mm_srli_epi32(a, 16);\n        __m128 hi_float = _mm_cvtepi32_ps(hi);\n        __m128 sqrt_val = _mm_sqrt_ps(hi_float);\n        _mm_storeu_ps(B + i, sqrt_val);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t hi = A[i] >> 16;\n        B[i] = sqrtf((float)hi);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        union {\n            uint32_t u;\n            float f;\n        } pun = { .u = A[i] };\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union {\n            uint32_t u;\n            float f;\n        } pun;\n        pun.u = A[i];\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union {\n            uint32_t u;\n            float f;\n        } pun;\n        pun.u = A[i];\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        union {\n            uint32_t u;\n            float f;\n        } pun;\n        pun.u = A[i];\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        union {\n            uint32_t u;\n            float f;\n        } pun;\n        pun.u = A[i];\n        B[i] = pun.f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i a_prev = _mm_loadu_si128((__m128i*)(A + i - 1));\n        __m128i a_curr = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i diff_int = _mm_sub_epi32(a_curr, a_prev);\n\n        __m128i cond_mask = _mm_set_epi32(\n            cond[i+3] ? -1 : 0,\n            cond[i+2] ? -1 : 0,\n            cond[i+1] ? -1 : 0,\n            cond[i] ? -1 : 0\n        );\n\n        __m128i masked_diff = _mm_and_si128(diff_int, cond_mask);\n        __m128 diff_float = _mm_cvtepi32_ps(masked_diff);\n\n        _mm_storeu_ps(diff + i, diff_float);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i curr = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i prev = _mm_loadu_si128((__m128i*)(A + i - 1));\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)(cond + i));\n        cond_mask = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1));\n\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n        __m128 zeros = _mm_setzero_ps();\n        diff_float = _mm_blendv_ps(zeros, diff_float, _mm_castsi128_ps(cond_mask));\n\n        _mm_storeu_ps(diff + i, diff_float);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)(cond + i));\n        cond_mask = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1));\n\n        __m128i a_curr = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i a_prev = _mm_loadu_si128((__m128i*)(A + i - 1));\n        __m128i diff_int = _mm_sub_epi32(a_curr, a_prev);\n\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n        __m128 zeros = _mm_setzero_ps();\n        __m128 result = _mm_blendv_ps(zeros, diff_float, _mm_castsi128_ps(cond_mask));\n\n        _mm_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    \n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    __m128i zero = _mm_setzero_si128();\n    __m128 zero_float = _mm_setzero_ps();\n\n    for (; i + 3 < size; i += 4) {\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)(cond + i));\n        cond_vec = _mm_cmpeq_epi32(cond_vec, _mm_set1_epi32(1));\n        \n        __m128i a_curr = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i a_prev = _mm_loadu_si128((__m128i*)(A + i - 1));\n        __m128i diff_int = _mm_sub_epi32(a_curr, a_prev);\n        \n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n        diff_float = _mm_and_ps(diff_float, _mm_castsi128_ps(cond_vec));\n        \n        _mm_storeu_ps(diff + i, diff_float);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    \n    diff[0] = 0.0f;\n    if (size == 1) return;\n    \n    size_t i = 1;\n    __m128i zero = _mm_setzero_si128();\n    __m128 zero_float = _mm_setzero_ps();\n    \n    for (; i + 3 < size; i += 4) {\n        // Load conditions\n        __m128i cond_mask = _mm_set_epi32(\n            cond[i+3] ? -1 : 0,\n            cond[i+2] ? -1 : 0,\n            cond[i+1] ? -1 : 0,\n            cond[i] ? -1 : 0\n        );\n        \n        // Load current and previous elements\n        __m128i curr = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i prev = _mm_loadu_si128((__m128i*)(A + i - 1));\n        \n        // Compute differences\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n        \n        // Convert to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n        \n        // Apply condition mask\n        __m128 res = _mm_and_ps(diff_float, _mm_castsi128_ps(cond_mask));\n        \n        // Store result\n        _mm_storeu_ps(diff + i, res);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((const __m128i*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, ones);\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n    \n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((const __m128i*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, ones);\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((const __m128i*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, ones);\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((__m128i const*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, ones);\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n    \n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((__m128i const*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, ones);\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)0xFFFFFFFF << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadl_epi64((const __m128i*)(index + i));\n        idx = _mm_slli_epi64(idx, 3); // Multiply by 8 (sizeof(double))\n        \n        double* addr0 = (double*)((char*)vec + _mm_extract_epi16(idx, 0));\n        double* addr1 = (double*)((char*)vec + _mm_extract_epi16(idx, 4));\n        \n        __m128d val0 = _mm_load_sd(addr0);\n        __m128d val1 = _mm_load_sd(addr1);\n        __m128d val = _mm_unpacklo_pd(val0, val1);\n        \n        sum = _mm_add_pd(sum, val);\n    }\n\n    double result = _mm_cvtsd_f64(_mm_hadd_pd(sum, sum));\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadl_epi64((const __m128i*)(index + i));\n        idx = _mm_slli_epi64(idx, 3); // Multiply by 8 (sizeof(double))\n        double* addr0 = (double*)((char*)vec + _mm_extract_epi16(idx, 0));\n        double* addr1 = (double*)((char*)vec + _mm_extract_epi16(idx, 4));\n        __m128d vals = _mm_set_pd(*addr1, *addr0);\n        sum = _mm_add_pd(sum, vals);\n    }\n\n    double result = _mm_cvtsd_f64(_mm_hadd_pd(sum, sum));\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadl_epi64((const __m128i*)(index + i));\n        idx = _mm_unpacklo_epi16(idx, _mm_setzero_si128());\n        __m128i idx32 = _mm_unpacklo_epi16(idx, _mm_setzero_si128());\n        __m128d v0 = _mm_load_sd(vec + _mm_extract_epi32(idx32, 0));\n        __m128d v1 = _mm_load_sd(vec + _mm_extract_epi32(idx32, 1));\n        __m128d combined = _mm_unpacklo_pd(v0, v1);\n        sum = _mm_add_pd(sum, combined);\n    }\n\n    double result = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadl_epi64((const __m128i*)(index + i));\n        idx = _mm_unpacklo_epi16(idx, _mm_setzero_si128());\n        __m128i idx32 = _mm_unpacklo_epi16(idx, _mm_setzero_si128());\n        \n        __m128d val0 = _mm_load_sd(vec + _mm_extract_epi32(idx32, 0));\n        __m128d val1 = _mm_load_sd(vec + _mm_extract_epi32(idx32, 1));\n        \n        __m128d vals = _mm_unpacklo_pd(val0, val1);\n        sum = _mm_add_pd(sum, vals);\n    }\n\n    double result = _mm_cvtsd_f64(_mm_hadd_pd(sum, sum));\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadl_epi64((const __m128i*)(index + i));\n        idx = _mm_slli_epi64(idx, 3);\n        double* addr0 = (double*)((char*)vec + _mm_extract_epi16(idx, 0));\n        double* addr1 = (double*)((char*)vec + _mm_extract_epi16(idx, 4));\n        __m128d val = _mm_set_pd(*addr1, *addr0);\n        sum = _mm_add_pd(sum, val);\n    }\n\n    double result = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    __m128d input_val = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_val = _mm_set1_pd(kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d zero = _mm_setzero_pd();\n            sum = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * output_size + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    \n                    __m128d input_vec = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_vec = _mm_loadu_pd(&kernel[kernel_idx]);\n                    __m128d prod = _mm_mul_pd(input_vec, kernel_vec);\n                    sum = _mm_add_pd(sum, prod);\n                }\n            }\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            double total = result[0] + result[1];\n            \n            // Handle odd kernel_size\n            if (kernel_size % 2 != 0) {\n                for (size_t ki = 0; ki < kernel_size; ++ki) {\n                    size_t kj = kernel_size - 1;\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    total += input[input_idx] * kernel[kernel_idx];\n                }\n            }\n            \n            // ReLU activation\n            output[i * output_size + j] = total > 0.0 ? total : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    __m128d input_val = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_val = _mm_set1_pd(kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d zero = _mm_setzero_pd();\n            sum = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * output_size + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            \n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    \n                    __m128d input_vec = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_vec = _mm_loadu_pd(&kernel[kernel_idx]);\n                    \n                    __m128d prod = _mm_mul_pd(input_vec, kernel_vec);\n                    sum = _mm_add_pd(sum, prod);\n                }\n            }\n            \n            double result[2];\n            _mm_storeu_pd(result, sum);\n            double total = result[0] + result[1];\n            \n            // Handle odd kernel_size\n            if (kernel_size % 2 != 0) {\n                for (size_t ki = 0; ki < kernel_size; ++ki) {\n                    size_t kj = kernel_size - 1;\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    total += input[input_idx] * kernel[kernel_idx];\n                }\n            }\n            \n            // ReLU activation\n            output[i * output_size + j] = total > 0.0 ? total : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    \n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m128d sum = _mm_setzero_pd();\n            \n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    \n                    __m128d input_vec = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_vec = _mm_loadu_pd(&kernel[kernel_idx]);\n                    \n                    __m128d prod = _mm_mul_pd(input_vec, kernel_vec);\n                    sum = _mm_add_pd(sum, prod);\n                }\n            }\n            \n            double result[2];\n            _mm_storeu_pd(result, sum);\n            double total = result[0] + result[1];\n            \n            // Handle odd kernel_size\n            if (kernel_size % 2 != 0) {\n                for (size_t ki = 0; ki < kernel_size; ki++) {\n                    size_t input_idx = (i + ki) * input_size + (j + kernel_size - 1);\n                    size_t kernel_idx = ki * kernel_size + (kernel_size - 1);\n                    total += input[input_idx] * kernel[kernel_idx];\n                }\n            }\n            \n            // ReLU activation\n            output[i * output_size + j] = total > 0.0 ? total : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t i, j;\n    size_t end = numbers.size() - 1;\n    size_t simd_end = end - (end % 2);\n\n    for (i = 0; i < simd_end; i += 2) {\n        __m128d current = _mm_loadu_pd(&numbers[i]);\n        for (j = i + 1; j < numbers.size(); ++j) {\n            __m128d next = _mm_set1_pd(numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    for (; i < end; ++i) {\n        for (j = i + 1; j < numbers.size(); ++j) {\n            if (fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t i, j;\n    size_t end = numbers.size() - 1;\n\n    for (i = 0; i < end; ++i) {\n        __m128d num_i = _mm_set1_pd(numbers[i]);\n        size_t simd_end = ((end - i) / 2) * 2 + i;\n\n        for (j = i + 1; j <= simd_end; j += 2) {\n            __m128d nums_j = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(nums_j, num_i);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n\n        // Handle remaining elements\n        for (; j <= end; ++j) {\n            double diff = fabs(numbers[j] - numbers[i]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < numbers.size() - 1; ++i) {\n        __m128d current = _mm_set1_pd(numbers[i]);\n        size_t j = i + 1;\n        \n        for (; j + 1 < numbers.size(); j += 2) {\n            __m128d next = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(zero, diff), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n\n        for (; j < numbers.size(); ++j) {\n            double diff = fabs(numbers[i] - numbers[j]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t i, j;\n    size_t end = numbers.size() - 1;\n    size_t simd_end = end & ~1;\n\n    for (i = 0; i < simd_end; i += 2) {\n        __m128d current = _mm_loadu_pd(&numbers[i]);\n        for (j = i + 1; j < numbers.size(); ++j) {\n            __m128d next = _mm_set1_pd(numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    for (; i < end; ++i) {\n        for (j = i + 1; j < numbers.size(); ++j) {\n            double diff = fabs(numbers[i] - numbers[j]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t i, j;\n    size_t end = numbers.size() - 1;\n    size_t simd_end = end & ~1;\n\n    for (i = 0; i < simd_end; i += 2) {\n        __m128d current = _mm_loadu_pd(&numbers[i]);\n        for (j = i + 1; j < numbers.size(); ++j) {\n            __m128d next = _mm_set1_pd(numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements if any\n    for (; i < end; ++i) {\n        for (j = i + 1; j < numbers.size(); ++j) {\n            if (fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i = 0;\n    int size = operations.size();\n    bool result = false;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            result = true;\n            break;\n        }\n    }\n\n    if (!result) {\n        int remaining_balance = 0;\n        for (int j = 0; j < 4; ++j) {\n            if (i + j < size) {\n                remaining_balance += operations[i + j];\n                if (remaining_balance < 0) {\n                    result = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i = 0;\n    int size = operations.size();\n    bool result = false;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            result = true;\n            break;\n        }\n    }\n\n    if (!result) {\n        int remaining_balance = 0;\n        for (int j = 0; j < 4; ++j) {\n            if (i + j < size) {\n                remaining_balance += operations[i + j];\n                if (remaining_balance < 0) {\n                    result = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i;\n    int size = operations.size();\n    \n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, op);\n        \n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    int remaining = size % 4;\n    if (remaining != 0) {\n        int temp[4] = {0, 0, 0, 0};\n        for (int j = 0; j < remaining; ++j) {\n            temp[j] = operations[i + j];\n        }\n        __m128i op = _mm_loadu_si128((__m128i*)temp);\n        balance = _mm_add_epi32(balance, op);\n        \n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i = 0;\n    int size = operations.size();\n    bool result = false;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            result = true;\n            break;\n        }\n    }\n\n    if (!result) {\n        int remaining_balance = 0;\n        for (int j = 0; j < 4; ++j) {\n            if (i + j < size) {\n                remaining_balance += operations[i + j];\n                if (remaining_balance < 0) {\n                    result = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    if (operations.empty()) {\n        return false;\n    }\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i;\n    int size = operations.size();\n    \n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, op);\n        \n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    int remaining = size % 4;\n    if (remaining != 0) {\n        int temp[4] = {0, 0, 0, 0};\n        for (int j = 0; j < remaining; ++j) {\n            temp[j] = operations[i + j];\n        }\n        __m128i op = _mm_loadu_si128((__m128i*)temp);\n        balance = _mm_add_epi32(balance, op);\n        \n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    double* data = numbers.data();\n    size_t i;\n\n    for (i = 0; i + 1 < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        sum = _mm_add_pd(sum, v);\n    }\n\n    double sum_scalar = _mm_cvtsd_f64(sum) + _mm_cvtsd_f64(_mm_unpackhi_pd(sum, sum));\n    for (; i < n; ++i) {\n        sum_scalar += data[i];\n    }\n\n    double mean = sum_scalar / n;\n\n    __m128d v_mean = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    for (i = 0; i + 1 < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        __m128d diff = _mm_sub_pd(v, v_mean);\n        __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_scalar = _mm_cvtsd_f64(mad_sum) + _mm_cvtsd_f64(_mm_unpackhi_pd(mad_sum, mad_sum));\n    for (; i < n; ++i) {\n        mad_scalar += fabs(data[i] - mean);\n    }\n\n    return mad_scalar / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    double* data = numbers.data();\n\n    // Calculate mean\n    for (size_t i = 0; i < n / 2 * 2; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        sum = _mm_add_pd(sum, v);\n    }\n    double mean = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n\n    // Handle remaining element if n is odd\n    if (n % 2 != 0) {\n        mean += data[n - 1];\n    }\n    mean /= n;\n\n    __m128d v_mean = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    // Calculate sum of absolute differences\n    for (size_t i = 0; i < n / 2 * 2; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        __m128d diff = _mm_sub_pd(v, v_mean);\n        __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n    double mad = _mm_cvtsd_f64(_mm_add_pd(mad_sum, _mm_unpackhi_pd(mad_sum, mad_sum)));\n\n    // Handle remaining element if n is odd\n    if (n % 2 != 0) {\n        mad += fabs(data[n - 1] - mean);\n    }\n    mad /= n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    double* data = numbers.data();\n\n    // Calculate mean\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        sum = _mm_add_pd(sum, v);\n    }\n    double sum_scalar = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n    double mean = sum_scalar / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    // Calculate sum of absolute differences\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        __m128d diff = _mm_sub_pd(v, mean_vec);\n        __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n    double mad_sum_scalar = _mm_cvtsd_f64(_mm_add_pd(mad_sum, _mm_unpackhi_pd(mad_sum, mad_sum)));\n\n    // Handle odd-sized vectors\n    if (n % 2 != 0) {\n        double last_diff = fabs(data[n-1] - mean);\n        mad_sum_scalar += last_diff;\n    }\n\n    return mad_sum_scalar / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    double* data = numbers.data();\n\n    // Calculate mean\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        sum = _mm_add_pd(sum, v);\n    }\n    double sum_scalar = _mm_cvtsd_f64(sum) + _mm_cvtsd_f64(_mm_unpackhi_pd(sum, sum));\n    double mean = sum_scalar / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    // Calculate sum of absolute differences\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        __m128d diff = _mm_sub_pd(v, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n    double mad_sum_scalar = _mm_cvtsd_f64(mad_sum) + _mm_cvtsd_f64(_mm_unpackhi_pd(mad_sum, mad_sum));\n\n    // Handle odd-sized vectors\n    if (n % 2 != 0) {\n        double last_diff = fabs(data[n-1] - mean);\n        mad_sum_scalar += last_diff;\n    }\n\n    return mad_sum_scalar / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    double* data = numbers.data();\n\n    // Calculate mean\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        sum = _mm_add_pd(sum, v);\n    }\n    double sum_scalar = _mm_cvtsd_f64(_mm_add_pd(sum, _mm_unpackhi_pd(sum, sum)));\n    double mean = sum_scalar / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    // Calculate sum of absolute differences\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d v = _mm_loadu_pd(data + i);\n        __m128d diff = _mm_sub_pd(v, mean_vec);\n        __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n    double mad_sum_scalar = _mm_cvtsd_f64(_mm_add_pd(mad_sum, _mm_unpackhi_pd(mad_sum, mad_sum)));\n\n    // Handle odd-sized vectors\n    if (n % 2 != 0) {\n        double last_diff = fabs(data[n-1] - mean);\n        mad_sum_scalar += last_diff;\n    }\n\n    return mad_sum_scalar / n;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 4 <= original_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i shuffled1 = _mm_unpacklo_epi32(data, delimiter_vec);\n        __m128i shuffled2 = _mm_unpackhi_epi32(data, delimiter_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], shuffled1);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], shuffled2);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != original_size - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 4 <= original_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i shuffled1 = _mm_unpacklo_epi32(data, delimiter_vec);\n        __m128i shuffled2 = _mm_unpackhi_epi32(data, delimiter_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], shuffled1);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], shuffled2);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i * 2 + 1 < new_size) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n    \n    // Process elements in chunks of 4\n    for (; i + 4 <= original_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Interleave data and delimiter\n        __m128i low = _mm_unpacklo_epi32(data, delimiter_vec);\n        __m128i high = _mm_unpackhi_epi32(data, delimiter_vec);\n        \n        // Store the interleaved results\n        _mm_storeu_si128((__m128i*)&result[i * 2], low);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], high);\n    }\n\n    // Handle remaining elements\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != original_size - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 4 <= original_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i interleaved_lo = _mm_unpacklo_epi32(data, delimiter_vec);\n        __m128i interleaved_hi = _mm_unpackhi_epi32(data, delimiter_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], interleaved_lo);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], interleaved_hi);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != original_size - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 4 <= original_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i shuffled1 = _mm_unpacklo_epi32(data, delimiter_vec);\n        __m128i shuffled2 = _mm_unpackhi_epi32(data, delimiter_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], shuffled1);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], shuffled2);\n    }\n\n    for (; i < original_size; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != original_size - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n    \n    size_t i = 0;\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Sum calculation\n        __m128i sum_low = _mm_srai_epi32(_mm_unpacklo_epi16(data, data), 16);\n        __m128i sum_high = _mm_srai_epi32(_mm_unpackhi_epi16(data, data), 16);\n        sum_vec = _mm_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m128i prod_low = _mm_unpacklo_epi16(data, _mm_set1_epi16(1));\n        __m128i prod_high = _mm_unpackhi_epi16(data, _mm_set1_epi16(1));\n        prod_vec = _mm_mullo_epi16(prod_vec, prod_low);\n        prod_vec = _mm_mullo_epi16(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum\n    int32_t sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    // Horizontal product\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t product = 1;\n    for (int j = 0; j < 8; ++j) {\n        product *= prod_array[j];\n    }\n    \n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n    \n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n    size_t i = 0;\n    size_t size = numbers.size();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Sum calculation\n        __m128i sum_low = _mm_cvtepi16_epi32(data);\n        __m128i sum_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        sum_vec = _mm_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m128i prod_low = _mm_cvtepi16_epi32(data);\n        __m128i prod_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum\n    int32_t sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    // Horizontal product\n    int32_t prod_array[4];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t product = 1;\n    for (int j = 0; j < 4; ++j) {\n        product *= prod_array[j];\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n    \n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n    \n    size_t i = 0;\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Sum calculation\n        __m128i sum_low = _mm_cvtepi16_epi32(data);\n        __m128i sum_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        sum_vec = _mm_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m128i prod_low = _mm_cvtepi16_epi32(data);\n        __m128i prod_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum and product\n    int32_t sum_arr[4];\n    int32_t prod_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n    _mm_storeu_si128((__m128i*)prod_arr, prod_vec);\n    \n    int64_t sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    int64_t product = prod_arr[0] * prod_arr[1] * prod_arr[2] * prod_arr[3];\n    \n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n    \n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n    size_t i = 0;\n    size_t size = numbers.size();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Sum calculation\n        __m128i sum_low = _mm_cvtepi16_epi32(data);\n        __m128i sum_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        sum_vec = _mm_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m128i prod_low = _mm_cvtepi16_epi32(data);\n        __m128i prod_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum of sum_vec\n    __m128i sum_tmp = _mm_add_epi32(sum_vec, _mm_srli_si128(sum_vec, 8));\n    sum_tmp = _mm_add_epi32(sum_tmp, _mm_srli_si128(sum_tmp, 4));\n    int32_t sum = _mm_cvtsi128_si32(sum_tmp);\n    \n    // Horizontal product of prod_vec\n    __m128i prod_tmp = _mm_mul_epu32(prod_vec, _mm_srli_si128(prod_vec, 8));\n    int64_t prod = _mm_cvtsi128_si64(prod_tmp) * _mm_extract_epi64(prod_tmp, 1);\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        prod *= numbers[i];\n    }\n    \n    return {sum, prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n    size_t i = 0;\n    size_t size = numbers.size();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Sum calculation\n        __m128i sum_low = _mm_cvtepi16_epi32(data);\n        __m128i sum_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        sum_vec = _mm_add_epi32(sum_vec, sum_low);\n        sum_vec = _mm_add_epi32(sum_vec, sum_high);\n        \n        // Product calculation\n        __m128i prod_low = _mm_cvtepi16_epi32(data);\n        __m128i prod_high = _mm_cvtepi16_epi32(_mm_srli_si128(data, 8));\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_low);\n        prod_vec = _mm_mullo_epi32(prod_vec, prod_high);\n    }\n    \n    // Horizontal sum\n    int32_t sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n    int64_t sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    \n    // Horizontal product\n    int32_t prod_arr[4];\n    _mm_storeu_si128((__m128i*)prod_arr, prod_vec);\n    int64_t product = 1;\n    for (int j = 0; j < 4; ++j) {\n        product *= prod_arr[j];\n    }\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n    \n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) return {};\n    \n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n    \n    for (size_t i = 1; i < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i prev_max = _mm_loadu_si128((__m128i*)&result[i-1]);\n        \n        __m128i shifted_prev = _mm_slli_si128(prev_max, 4);\n        shifted_prev = _mm_insert_epi32(shifted_prev, result[i-1], 0);\n        \n        __m128i max1 = _mm_max_epi32(shifted_prev, current);\n        \n        __m128i max2 = _mm_max_epi32(prev_max, max1);\n        \n        _mm_storeu_si128((__m128i*)&result[i], max2);\n        \n        for (size_t j = i; j < std::min(i + 4, size); ++j) {\n            if (j > 0) {\n                result[j] = std::max(result[j-1], numbers[j]);\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    // Process the remaining elements in chunks of 4 (SSE can handle 4 ints at once)\n    for (size_t i = 1; i < size; i += 4) {\n        size_t remaining = std::min(size - i, static_cast<size_t>(4));\n        \n        // Load current numbers\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        // Load previous max values\n        __m128i prev_max;\n        if (i == 1) {\n            prev_max = _mm_set1_epi32(result[i-1]);\n        } else {\n            prev_max = _mm_loadu_si128((__m128i*)&result[i-1]);\n        }\n        \n        // Compute current max\n        __m128i max_val = _mm_max_epi32(current, prev_max);\n        \n        // Store results\n        _mm_storeu_si128((__m128i*)&result[i], max_val);\n        \n        // Handle the remaining elements if not a multiple of 4\n        if (remaining < 4) {\n            for (size_t j = i; j < i + remaining; ++j) {\n                result[j] = std::max(numbers[j], result[j-1]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < size; i += 4) {\n        __m128i current_max = _mm_loadu_si128((__m128i*)&result[i-1]);\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        __m128i shifted_max = _mm_slli_si128(current_max, 4);\n        shifted_max = _mm_insert_epi32(shifted_max, result[i-1], 0);\n        \n        __m128i new_max = _mm_max_epi32(shifted_max, current_vals);\n        \n        int remaining = std::min(4, (int)(size - i));\n        for (int j = 0; j < remaining; ++j) {\n            result[i + j] = _mm_extract_epi32(new_max, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) return numbers;\n    \n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n    \n    for (size_t i = 1; i < size; i += 4) {\n        __m128i current_max = _mm_loadu_si128((__m128i*)&result[i-1]);\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        __m128i max_vals = _mm_max_epi32(current_max, current_vals);\n        \n        _mm_storeu_si128((__m128i*)&result[i], max_vals);\n        \n        // Handle remaining elements if not aligned to 4\n        for (size_t j = i; j < std::min(i + 4, size); ++j) {\n            result[j] = std::max(result[j-1], numbers[j]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    size_t size = numbers.size();\n    std::vector<int> result(size);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < size; i++) {\n        result[i] = std::max(result[i-1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n    \n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&b[i]);\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n    \n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n    \n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&b[i]);\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n    \n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n    \n    // Pad the shorter string with '0's to match the length\n    if (a.size() < len) {\n        a.insert(a.begin(), len - a.size(), '0');\n    }\n    if (b.size() < len) {\n        b.insert(b.begin(), len - b.size(), '0');\n    }\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&b[i]);\n        \n        __m128i xor_result = _mm_xor_si128(a_vec, b_vec);\n        \n        _mm_storeu_si128((__m128i*)&result[i], xor_result);\n    }\n\n    // Handle remaining elements\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::max(a.size(), b.size());\n    a.resize(len, '0');\n    b.resize(len, '0');\n    std::string result(len, '0');\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(a.data() + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(b.data() + i));\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(result.data() + i), res);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n    \n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&b[i]);\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n    \n    for (; i < len; ++i) {\n        result[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 4 strings at a time using SIMD\n    for (; i + 3 < size; i += 4) {\n        __m128i len_vec = _mm_set_epi32(\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        alignas(16) int lengths[4];\n        _mm_store_si128((__m128i*)lengths, len_vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (lengths[j] > max_len) {\n                max_len = lengths[j];\n                max_idx = i + j;\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        size_t len = strings[i].length();\n        if (len > max_len) {\n            max_len = len;\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    const size_t size = strings.size();\n    const size_t simd_width = 16;\n\n    for (size_t i = 0; i < size; i += simd_width) {\n        __m128i max_lens = _mm_setzero_si128();\n        __m128i indices = _mm_setzero_si128();\n        size_t remaining = std::min(simd_width, size - i);\n\n        for (size_t j = 0; j < remaining; ++j) {\n            size_t len = strings[i + j].length();\n            max_lens = _mm_insert_epi32(max_lens, len, j);\n            indices = _mm_insert_epi32(indices, i + j, j);\n        }\n\n        for (size_t j = remaining; j < simd_width; ++j) {\n            max_lens = _mm_insert_epi32(max_lens, 0, j);\n            indices = _mm_insert_epi32(indices, 0, j);\n        }\n\n        alignas(16) int max_lens_arr[4];\n        alignas(16) int indices_arr[4];\n        _mm_store_si128((__m128i*)max_lens_arr, max_lens);\n        _mm_store_si128((__m128i*)indices_arr, indices);\n\n        for (size_t j = 0; j < 4; ++j) {\n            if (max_lens_arr[j] > max_len) {\n                max_len = max_lens_arr[j];\n                max_idx = indices_arr[j];\n            }\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 4 strings at a time using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i lengths = _mm_set_epi32(\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        // Find max among the 4 lengths\n        __m128i max1 = _mm_shuffle_epi32(lengths, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i max2 = _mm_max_epi32(lengths, max1);\n        __m128i max3 = _mm_shuffle_epi32(max2, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i max4 = _mm_max_epi32(max2, max3);\n        int current_max = _mm_extract_epi32(max4, 0);\n\n        if (current_max > max_len) {\n            max_len = current_max;\n            // Find the index of the first string with max_len in this batch\n            for (size_t j = 0; j < 4; ++j) {\n                if (strings[i+j].length() == max_len) {\n                    max_idx = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    const size_t size = strings.size();\n    const size_t simd_size = size - (size % 4);\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i len_vec = _mm_set_epi32(\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        alignas(16) int lengths[4];\n        _mm_store_si128((__m128i*)lengths, len_vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (lengths[j] > max_len) {\n                max_len = lengths[j];\n                max_idx = i + j;\n            }\n        }\n    }\n\n    for (size_t i = simd_size; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n    size_t i = 0;\n    const size_t size = strings.size();\n\n    // Process 4 strings at a time using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i len_vec = _mm_set_epi32(\n            strings[i+3].length(),\n            strings[i+2].length(),\n            strings[i+1].length(),\n            strings[i].length()\n        );\n\n        alignas(16) int lengths[4];\n        _mm_store_si128((__m128i*)lengths, len_vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (lengths[j] > max_len) {\n                max_len = lengths[j];\n                max_idx = i + j;\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < size; ++i) {\n        if (strings[i].length() > max_len) {\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va, vb, vzero, vtemp;\n    uint64_t temp;\n    \n    va = _mm_set_epi64x(0, a);\n    vb = _mm_set_epi64x(0, b);\n    vzero = _mm_setzero_si128();\n    \n    while (1) {\n        if (b == 0) {\n            _mm_store_si128((__m128i*)&temp, va);\n            return temp;\n        }\n        \n        vtemp = vb;\n        vb = _mm_sub_epi64(va, _mm_mul_epu32(vb, _mm_set1_epi64x(a / b)));\n        va = vtemp;\n        \n        temp = _mm_cvtsi128_si64(vb);\n        b = temp;\n        a = _mm_cvtsi128_si64(va);\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va, vb, vzero, vcmp, vswap, vtemp;\n    uint64_t result[2];\n    \n    vzero = _mm_setzero_si128();\n    va = _mm_set_epi64x(0, a);\n    vb = _mm_set_epi64x(0, b);\n    \n    while (1) {\n        vcmp = _mm_cmpeq_epi64(vb, vzero);\n        if (_mm_movemask_epi8(vcmp) != 0) {\n            _mm_storel_epi64((__m128i*)result, va);\n            return result[0];\n        }\n        \n        vtemp = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = vtemp;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(0, a);\n    __m128i vb = _mm_set_epi64x(0, b);\n    \n    while (b != 0) {\n        __m128i vtemp = vb;\n        vb = _mm_sub_epi64(va, _mm_mul_epu32(vb, _mm_set1_epi64x(a / b)));\n        va = vtemp;\n        \n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    \n    return a;\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(0, a);\n    __m128i vb = _mm_set_epi64x(0, b);\n    \n    while (b != 0) {\n        __m128i vtemp = vb;\n        vb = _mm_sub_epi64(va, _mm_mul_epu32(vb, _mm_set_epi64x(0, a / b)));\n        va = vtemp;\n        a = b;\n        b = _mm_cvtsi128_si64(vb);\n    }\n    \n    return a;\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(0, a);\n    __m128i vb = _mm_set_epi64x(0, b);\n    \n    while (1) {\n        __m128i vcmp = _mm_cmpeq_epi64(vb, _mm_setzero_si128());\n        if (!_mm_testz_si128(vcmp, vcmp)) {\n            uint64_t result;\n            _mm_storel_epi64((__m128i*)&result, va);\n            return result;\n        }\n        \n        __m128i vrem = _mm_sub_epi64(va, _mm_mul_epu32(vb, _mm_div_epu32(va, vb)));\n        __m128i vswap = _mm_blendv_epi8(vb, va, vrem);\n        vb = _mm_blendv_epi8(vrem, vb, vrem);\n        va = vswap;\n    }\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size() - 1; i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i + 1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        \n        float diffs[4];\n        _mm_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 4; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i < numbers.size() - 1; ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i + 1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        \n        float diffs[4];\n        _mm_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 4; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i + 1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff);\n\n        float diffs[4];\n        _mm_storeu_ps(diffs, abs_diff);\n\n        for (int j = 0; j < 4; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i + 1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff);\n\n        float diffs[4];\n        _mm_storeu_ps(diffs, abs_diff);\n\n        for (int j = 0; j < 4; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    float num1 = 0.0f, num2 = 0.0f;\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i + 1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        \n        float diffs[4];\n        _mm_storeu_ps(diffs, diff);\n\n        for (int j = 0; j < 4; ++j) {\n            if (diffs[j] < min_diff) {\n                min_diff = diffs[j];\n                num1 = numbers[i + j];\n                num2 = numbers[i + j + 1];\n            }\n        }\n    }\n\n    for (; i + 1 < numbers.size(); ++i) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            num1 = numbers[i];\n            num2 = numbers[i + 1];\n        }\n    }\n\n    return {num1, num2};\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        __m128 sub = _mm_sub_ps(data, min_vec);\n        __m128 div = _mm_div_ps(sub, range_vec);\n        _mm_storeu_ps(&numbers[i], div);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    size_t i = 0;\n\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        __m128 sub = _mm_sub_ps(data, min_vec);\n        __m128 res = _mm_div_ps(sub, range_vec);\n        _mm_storeu_ps(&numbers[i], res);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        __m128 sub = _mm_sub_ps(data, min_vec);\n        __m128 div = _mm_div_ps(sub, range_vec);\n        _mm_storeu_ps(&numbers[i], div);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    for (float num : numbers) {\n        min_val = std::min(min_val, num);\n        max_val = std::max(max_val, num);\n    }\n\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        __m128 normalized = _mm_sub_ps(data, min_vec);\n        normalized = _mm_div_ps(normalized, range_vec);\n        _mm_storeu_ps(&numbers[i], normalized);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return numbers;\n\n    // Find min and max using scalar operations\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::fill(numbers.begin(), numbers.end(), 0.0f);\n        return numbers;\n    }\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        __m128 sub = _mm_sub_ps(data, min_vec);\n        __m128 div = _mm_div_ps(sub, range_vec);\n        _mm_storeu_ps(&numbers[i], div);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] - min_val) / range;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) return 1;\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 2 separately\n    if (n % 2 == 0) {\n        max_divisor = n / 2;\n        return max_divisor;\n    }\n\n    // Check odd divisors from 3 to sqrt(n)\n    uint64_t start = 3;\n    uint64_t end = sqrt_n;\n    if (end % 2 == 0) end--;\n\n    // Process 4 divisors at a time using SIMD\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i ones = _mm_set1_epi64x(1);\n    __m128i zeros = _mm_set1_epi64x(0);\n\n    for (uint64_t i = start; i <= end; i += 8) {\n        // Create vector of 4 consecutive odd divisors\n        __m128i divisors = _mm_set_epi64x(i+6, i+4);\n        __m128i divisors2 = _mm_set_epi64x(i+2, i);\n\n        // Compute n % divisors\n        __m128i rem1 = _mm_sub_epi64(n_vec, _mm_mul_epu32(_mm_div_epu32(n_vec, divisors), divisors));\n        __m128i rem2 = _mm_sub_epi64(n_vec, _mm_mul_epu32(_mm_div_epu32(n_vec, divisors2), divisors2));\n\n        // Check if remainder is zero\n        __m128i cmp1 = _mm_cmpeq_epi64(rem1, zeros);\n        __m128i cmp2 = _mm_cmpeq_epi64(rem2, zeros);\n\n        // Get masks\n        int mask1 = _mm_movemask_epi8(cmp1);\n        int mask2 = _mm_movemask_epi8(cmp2);\n\n        // Check each divisor\n        uint64_t divisors_arr[4] = {i, i+2, i+4, i+6};\n        int masks[4] = {mask2 & 0xFFFF, (mask2 >> 16) & 0xFFFF, mask1 & 0xFFFF, (mask1 >> 16) & 0xFFFF};\n\n        for (int j = 0; j < 4; j++) {\n            if (masks[j] && divisors_arr[j] <= end) {\n                uint64_t candidate = n / divisors_arr[j];\n                if (candidate > max_divisor) {\n                    max_divisor = candidate;\n                }\n            }\n        }\n    }\n\n    // If no divisors found, n is prime\n    if (max_divisor == 1 && n > 1) {\n        return 1;\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 2 elements at a time using SSE2\n    for (uint64_t i = 2; i <= sqrt_n; i += 2) {\n        __m128i divisors = _mm_set_epi64x(i + 1, i);\n        __m128i remainders = _mm_set1_epi64x(n);\n        remainders = _mm_sub_epi64(remainders, _mm_mul_epu32(divisors, _mm_set1_epi64x(n / i)));\n\n        uint64_t rem_arr[2];\n        _mm_storeu_si128((__m128i*)rem_arr, remainders);\n\n        for (int j = 0; j < 2; j++) {\n            if (rem_arr[j] == 0) {\n                uint64_t candidate = (j == 0) ? i : (i + 1);\n                if (candidate > max_divisor && candidate < n) {\n                    max_divisor = candidate;\n                }\n                uint64_t pair = n / candidate;\n                if (pair > max_divisor && pair < n) {\n                    max_divisor = pair;\n                }\n            }\n        }\n    }\n\n    // Handle odd numbers\n    if (sqrt_n % 2 == 0 && (sqrt_n + 1) <= n / 2) {\n        uint64_t candidate = sqrt_n + 1;\n        if (n % candidate == 0 && candidate > max_divisor) {\n            max_divisor = candidate;\n        }\n    }\n\n    return max_divisor == 1 ? 1 : max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (uint64_t i = 2; i <= sqrt_n; i += 2) {\n        __m128i v_i = _mm_set_epi64x(i + 1, i);\n        __m128i v_remainder = _mm_sub_epi64(v_n, _mm_mul_epu32(_mm_div_epu32(v_n, v_i), v_i));\n\n        uint64_t remainders[2];\n        _mm_storeu_si128((__m128i*)remainders, v_remainder);\n\n        if (remainders[0] == 0 && i > max_divisor) {\n            max_divisor = i;\n        }\n        if (remainders[1] == 0 && (i + 1) > max_divisor) {\n            max_divisor = i + 1;\n        }\n    }\n\n    if (max_divisor == 1 && (n % 2 == 0)) {\n        max_divisor = n / 2;\n    }\n\n    if (max_divisor == 1 && n > 1) {\n        max_divisor = 1;\n        for (uint64_t i = sqrt_n; i >= 1; --i) {\n            if (n % i == 0) {\n                max_divisor = i;\n                break;\n            }\n        }\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_inc = _mm_set1_epi64x(2);\n    __m128i v_indices = _mm_set_epi64x(3, 1);\n\n    for (uint64_t i = 2; i <= sqrt_n; i += 2) {\n        __m128i v_rem;\n        __m128i v_divisors = _mm_add_epi64(v_indices, v_inc);\n        \n        v_rem = _mm_sub_epi64(v_n, _mm_mul_epu32(_mm_div_epu32(v_n, v_divisors), v_divisors));\n\n        uint64_t rem_arr[2];\n        _mm_storeu_si128((__m128i*)rem_arr, v_rem);\n\n        for (int j = 0; j < 2; ++j) {\n            uint64_t rem = rem_arr[j];\n            uint64_t divisor = i + j;\n            if (rem == 0 && divisor > max_divisor) {\n                max_divisor = divisor;\n            }\n            uint64_t pair = n / divisor;\n            if (rem == 0 && pair != divisor && pair > max_divisor) {\n                max_divisor = pair;\n            }\n        }\n\n        v_indices = _mm_add_epi64(v_indices, _mm_set1_epi64x(2));\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 2) {\n        return 1;\n    }\n\n    uint64_t max_divisor = 1;\n    uint64_t sqrt_n = sqrt(n);\n\n    // Process 2 elements at a time using SSE2\n    for (uint64_t i = 2; i <= sqrt_n; i += 2) {\n        __m128i divisors = _mm_set_epi64x(i + 1, i);\n        __m128i remainders = _mm_set1_epi64x(n);\n        remainders = _mm_sub_epi64(remainders, _mm_mul_epu32(divisors, _mm_set1_epi64x(n / i)));\n\n        uint64_t rem[2];\n        _mm_storeu_si128((__m128i*)rem, remainders);\n\n        if (rem[0] == 0 && (n / i) > max_divisor) {\n            max_divisor = n / i;\n        }\n        if (rem[1] == 0 && (n / (i + 1)) > max_divisor) {\n            max_divisor = n / (i + 1);\n        }\n    }\n\n    // Check remaining odd divisor if sqrt_n was even\n    if (sqrt_n % 2 == 0 && sqrt_n + 1 <= n / 2) {\n        uint64_t i = sqrt_n + 1;\n        if (n % i == 0) {\n            max_divisor = n / i;\n        }\n    }\n\n    return max_divisor == 1 ? 1 : (max_divisor == n ? 1 : max_divisor);\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    __m128i vn = _mm_set1_epi32(n);\n    __m128i vdiv = _mm_set_epi32(7, 5, 3, 2);\n    __m128i vrem = _mm_set1_epi32(0);\n    __m128i vzero = _mm_set1_epi32(0);\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    vn = _mm_set1_epi32(n);\n    int i = 3;\n    while (i * i <= n) {\n        vdiv = _mm_set_epi32(i+6, i+4, i+2, i);\n        vrem = _mm_rem_epi32(vn, vdiv);\n        __m128i vcmp = _mm_cmpeq_epi32(vrem, vzero);\n\n        int mask = _mm_movemask_epi8(vcmp);\n        if (mask) {\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j*4))) {\n                    int divisor = i + j*2;\n                    while (n % divisor == 0) {\n                        factors.push_back(divisor);\n                        n /= divisor;\n                    }\n                }\n            }\n            vn = _mm_set1_epi32(n);\n        }\n        i += 8;\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_divisor = _mm_set_epi32(3, 2, 3, 2);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_zero = _mm_setzero_si128();\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    int divisor = 3;\n    while (n > 1) {\n        __m128i v_remainder = _mm_sub_epi32(v_n, _mm_mullo_epi32(_mm_div_epi32(v_n, v_divisor), v_divisor));\n        __m128i v_mask = _mm_cmpeq_epi32(v_remainder, v_zero);\n        int mask = _mm_movemask_epi8(v_mask);\n\n        if (mask & 0x000F) {\n            factors.push_back(divisor);\n            n /= divisor;\n            v_n = _mm_set1_epi32(n);\n            continue;\n        }\n        if (mask & 0x00F0) {\n            factors.push_back(divisor + 2);\n            n /= (divisor + 2);\n            v_n = _mm_set1_epi32(n);\n            continue;\n        }\n\n        divisor += 6;\n        v_divisor = _mm_add_epi32(v_divisor, _mm_set1_epi32(6));\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    // Handle 2s separately\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Check for odd divisors from 3 to sqrt(n)\n    int max_divisor = sqrt(n);\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_inc = _mm_set1_epi32(8);\n    __m128i v_div = _mm_setr_epi32(3, 5, 7, 9);\n\n    for (int i = 3; i <= max_divisor; i += 8) {\n        __m128i v_rem = _mm_rem_epu32(v_n, v_div);\n        __m128i v_mask = _mm_cmpeq_epi32(v_rem, _mm_setzero_si128());\n\n        int mask = _mm_movemask_epi8(v_mask);\n        if (mask) {\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) {\n                    int divisor = _mm_extract_epi32(v_div, j);\n                    while (n % divisor == 0) {\n                        factors.push_back(divisor);\n                        n /= divisor;\n                    }\n                    max_divisor = sqrt(n);\n                    v_n = _mm_set1_epi32(n);\n                }\n            }\n        }\n        v_div = _mm_add_epi32(v_div, v_inc);\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    // Handle even numbers using SIMD\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n        v_n = _mm_set1_epi32(n);\n    }\n\n    // Check for odd divisors up to sqrt(n)\n    int max_divisor = sqrt(n);\n    __m128i v_div = _mm_setr_epi32(3, 5, 7, 9);\n    __m128i v_inc = _mm_set1_epi32(8);\n\n    for (int div = 3; div <= max_divisor; div += 8) {\n        __m128i v_rem = _mm_setzero_si128();\n        __m128i v_current_div = v_div;\n\n        // Check 4 divisors in parallel\n        for (int i = 0; i < 2; ++i) {\n            __m128i v_tmp = _mm_set1_epi32(n);\n            v_rem = _mm_rem_epu32(v_tmp, v_current_div);\n            __m128i mask = _mm_cmpeq_epi32(v_rem, _mm_setzero_si128());\n\n            int m[4];\n            _mm_storeu_si128((__m128i*)m, mask);\n            int d[4];\n            _mm_storeu_si128((__m128i*)d, v_current_div);\n\n            for (int j = 0; j < 4; ++j) {\n                if (m[j] == -1 && d[j] <= max_divisor) {\n                    while (n % d[j] == 0) {\n                        factors.push_back(d[j]);\n                        n /= d[j];\n                        max_divisor = sqrt(n);\n                    }\n                }\n            }\n            v_current_div = _mm_add_epi32(v_current_div, _mm_set1_epi32(4));\n        }\n        v_div = _mm_add_epi32(v_div, v_inc);\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    __m128i vn = _mm_set1_epi32(n);\n    __m128i vdivisor = _mm_set_epi32(7, 5, 3, 2);\n    __m128i vinc = _mm_set1_epi32(8);\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    vn = _mm_set1_epi32(n);\n    while (n > 1) {\n        __m128i vremainder = _mm_sub_epi32(vn, _mm_mullo_epi32(_mm_div_epi32(vn, vdivisor), vdivisor);\n        __m128i mask = _mm_cmpeq_epi32(vremainder, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask)) {\n            for (int i = 0; i < 4; ++i) {\n                int divisor = _mm_extract_epi32(vdivisor, i);\n                if (n % divisor == 0) {\n                    do {\n                        factors.push_back(divisor);\n                        n /= divisor;\n                    } while (n % divisor == 0);\n                    vn = _mm_set1_epi32(n);\n                    break;\n                }\n            }\n        } else {\n            vdivisor = _mm_add_epi32(vdivisor, vinc);\n            if (_mm_extract_epi32(vdivisor, 0) * _mm_extract_epi32(vdivisor, 0) > n) {\n                factors.push_back(n);\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    const int simd_width = 4;\n    int i = 0;\n    for (; i + simd_width <= l.size(); i += simd_width) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        \n        float tmp[simd_width];\n        _mm_storeu_ps(tmp, vec);\n        \n        for (int j = 0; j < simd_width; ++j) {\n            if (mask[j]) {\n                result.push_back(tmp[j]);\n            }\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    const int simd_width = 4;\n    int i = 0;\n    for (; i + simd_width <= l.size(); i += simd_width) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        \n        float tmp[simd_width];\n        _mm_storeu_ps(tmp, vec);\n        \n        for (int j = 0; j < simd_width; ++j) {\n            if (mask[j]) {\n                result.push_back(tmp[j]);\n            }\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    size_t i = 0;\n    const size_t aligned_size = l.size() - (l.size() % 4);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        \n        float tmp[4];\n        _mm_storeu_ps(tmp, vec);\n        \n        if (_mm_movemask_ps(mask) & 0x1) result.push_back(tmp[0]);\n        if (_mm_movemask_ps(mask) & 0x2) result.push_back(tmp[1]);\n        if (_mm_movemask_ps(mask) & 0x4) result.push_back(tmp[2]);\n        if (_mm_movemask_ps(mask) & 0x8) result.push_back(tmp[3]);\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    size_t i = 0;\n    const size_t aligned_size = l.size() - (l.size() % 4);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        \n        float tmp[4];\n        _mm_storeu_ps(tmp, vec);\n        \n        if (_mm_movemask_ps(mask) & 0x1) result.push_back(tmp[0]);\n        if (_mm_movemask_ps(mask) & 0x2) result.push_back(tmp[1]);\n        if (_mm_movemask_ps(mask) & 0x4) result.push_back(tmp[2]);\n        if (_mm_movemask_ps(mask) & 0x8) result.push_back(tmp[3]);\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n    \n    size_t i = 0;\n    const size_t aligned_size = l.size() - (l.size() % 4);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        \n        float tmp[4];\n        _mm_storeu_ps(tmp, vec);\n        \n        for (int j = 0; j < 4; ++j) {\n            if (mask[j]) {\n                result.push_back(tmp[j]);\n            }\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m128i divisors = _mm_set_epi64x(5, 7);\n    __m128i increments = _mm_set_epi64x(6, 6);\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        __m128i rem = _mm_sub_epi64(n_vec, _mm_mul_epu32(_mm_div_epi64(n_vec, divisors), divisors));\n        __m128i cmp = _mm_cmpeq_epi64(rem, zero);\n        if (!_mm_test_all_zeros(cmp, cmp)) {\n            return false;\n        }\n        divisors = _mm_add_epi64(divisors, increments);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m128i divisors = _mm_set_epi64x(5, 7);\n    __m128i increments = _mm_set_epi64x(6, 6);\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        __m128i rem = _mm_sub_epi64(n_vec, _mm_mul_epu32(_mm_div_epu32(n_vec, divisors), divisors));\n        __m128i cmp = _mm_cmpeq_epi32(rem, zero);\n        if (!_mm_test_all_zeros(cmp, cmp)) {\n            return false;\n        }\n        divisors = _mm_add_epi64(divisors, increments);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisors up to sqrt(n) using SIMD\n    int64_t max_divisor = sqrt(n) + 1;\n    __m128i divisors = _mm_set_epi64x(5, 7);\n    __m128i increments = _mm_set1_epi64x(6);\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        __m128i remainders = _mm_sub_epi64(n_vec, _mm_mul_epu32(_mm_div_epi32(n_vec, divisors), divisors));\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            return false;\n        }\n        divisors = _mm_add_epi64(divisors, increments);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i divisors = _mm_set_epi32(0, 0, 5, 7);\n    __m128i increments = _mm_set_epi32(0, 0, 6, 6);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i * i <= n; i += 6) {\n        __m128i current = _mm_add_epi32(divisors, _mm_set1_epi32(i - 5));\n        __m128i remainders = _mm_rem_epu32(n_vec, current);\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i divisors = _mm_set_epi32(5, 7, 11, 13);\n    __m128i increments = _mm_set1_epi32(6);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i * i <= n; i += 6) {\n        __m128i current = _mm_add_epi32(divisors, _mm_set1_epi32(i - 5));\n        __m128i remainder = _mm_rem_epu32(n_vec, current);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        if (!_mm_test_all_zeros(cmp, cmp)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 16 bytes (SSE2)\n    const size_t chunk_size = 16;\n    size_t remaining = len;\n    i = 0;\n\n    while (remaining >= chunk_size) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        // Create masks for uppercase and lowercase letters\n        __m128i upper_mask = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1));\n        __m128i lower_mask = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1));\n        __m128i upper_bound = _mm_cmpgt_epi8(_mm_set1_epi8('Z' + 1), chunk);\n        __m128i lower_bound = _mm_cmpgt_epi8(_mm_set1_epi8('z' + 1), chunk);\n        \n        __m128i is_upper = _mm_and_si128(upper_mask, upper_bound);\n        __m128i is_lower = _mm_and_si128(lower_mask, lower_bound);\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n        \n        // Toggle case: uppercase to lowercase and vice versa\n        __m128i toggle = _mm_set1_epi8(0x20);\n        __m128i toggled = _mm_xor_si128(chunk, _mm_and_si128(is_letter, toggle));\n        \n        _mm_storeu_si128((__m128i*)&result[i], toggled);\n        \n        i += chunk_size;\n        remaining -= chunk_size;\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            result[i] = isupper(s[i]) ? tolower(s[i]) : toupper(s[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    const int n = result.size();\n    const int simd_width = 16;\n    const int num_simd_iterations = n / simd_width;\n    const int remainder = n % simd_width;\n\n    __m128i upper_mask = _mm_set1_epi8(0x20);\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n\n    for (int i = 0; i < num_simd_iterations; ++i) {\n        int offset = i * simd_width;\n        __m128i chunk = _mm_loadu_si128((__m128i*)&result[offset]);\n\n        __m128i is_lower = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n            _mm_cmpgt_epi8(lower_z, chunk)\n        );\n        __m128i is_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n            _mm_cmpgt_epi8(upper_z, chunk)\n        );\n        __m128i is_letter = _mm_or_si128(is_lower, is_upper);\n\n        if (!_mm_testz_si128(is_letter, is_letter)) {\n            has_letters = true;\n            __m128i toggled = _mm_xor_si128(chunk, upper_mask);\n            __m128i masked_toggle = _mm_blendv_epi8(chunk, toggled, is_letter);\n            _mm_storeu_si128((__m128i*)&result[offset], masked_toggle);\n        }\n    }\n\n    for (int i = num_simd_iterations * simd_width; i < n; ++i) {\n        char c = result[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            result[i] ^= 0x20;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    __m128i zero = _mm_setzero_si128();\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n                                        _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_z, _mm_set1_epi8(1))));\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n                                        _mm_cmplt_epi8(chunk, _mm_add_epi8(lower_z, _mm_set1_epi8(1))));\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n        if (!_mm_testz_si128(is_letter, is_letter)) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    // Check remaining characters\n    for (; i < len && !has_letters; ++i) {\n        char c = s[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (has_letters) {\n        // Toggle case for letters\n        __m128i case_mask = _mm_set1_epi8(0x20);\n        i = 0;\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n            __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n                                            _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_z, _mm_set1_epi8(1))));\n            __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n                                            _mm_cmplt_epi8(chunk, _mm_add_epi8(lower_z, _mm_set1_epi8(1))));\n            __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n            __m128i toggled = _mm_xor_si128(chunk, _mm_and_si128(is_letter, case_mask));\n            _mm_storeu_si128((__m128i*)&result[i], toggled);\n        }\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = s[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 0x20;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 0x20;\n            }\n        }\n    } else {\n        // Reverse the string\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    for (; i < len; i++) {\n        if (isalpha(s[i])) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process 16 bytes at a time using SSE\n    const size_t simd_size = 16;\n    size_t simd_iters = len / simd_size;\n    i = 0;\n\n    for (; i < simd_iters * simd_size; i += simd_size) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        // Mask for lowercase letters\n        __m128i lower_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)),\n            _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1))\n        );\n        \n        // Mask for uppercase letters\n        __m128i upper_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)),\n            _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1))\n        );\n        \n        // Combine masks to get all letters\n        __m128i letter_mask = _mm_or_si128(lower_mask, upper_mask);\n        \n        // Flip case: lowercase -> subtract 32, uppercase -> add 32\n        __m128i flip_lower = _mm_sub_epi8(chunk, _mm_set1_epi8(32));\n        __m128i flip_upper = _mm_add_epi8(chunk, _mm_set1_epi8(32));\n        \n        // Select flipped cases based on masks\n        __m128i flipped = _mm_or_si128(\n            _mm_and_si128(lower_mask, flip_lower),\n            _mm_and_si128(upper_mask, flip_upper)\n        );\n        \n        // Combine with original (non-letters remain unchanged)\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(letter_mask, flipped),\n            _mm_andnot_si128(letter_mask, chunk)\n        );\n        \n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n\n    // Process remaining bytes\n    for (; i < len; i++) {\n        char c = s[i];\n        if (isupper(c)) {\n            result[i] = tolower(c);\n        } else if (islower(c)) {\n            result[i] = toupper(c);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    bool has_letters = false;\n    size_t len = s.size();\n    size_t i = 0;\n\n    // Check if there are any letters in the string\n    __m128i zero = _mm_setzero_si128();\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        // Check for uppercase letters\n        __m128i upper_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_z, _mm_set1_epi8(1)))\n        );\n        // Check for lowercase letters\n        __m128i lower_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chunk, _mm_add_epi8(lower_z, _mm_set1_epi8(1)))\n        );\n        __m128i letter_mask = _mm_or_si128(upper_mask, lower_mask);\n        \n        if (!_mm_testz_si128(letter_mask, letter_mask)) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    // Check remaining characters\n    for (; i < len && !has_letters; ++i) {\n        char c = s[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (has_letters) {\n        // Toggle case of letters\n        i = 0;\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n            \n            // Check for uppercase letters\n            __m128i upper_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_z, _mm_set1_epi8(1)))\n            );\n            // Check for lowercase letters\n            __m128i lower_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(lower_z, _mm_set1_epi8(1)))\n            );\n            __m128i letter_mask = _mm_or_si128(upper_mask, lower_mask);\n            \n            // Toggle case\n            __m128i toggled = _mm_xor_si128(chunk, _mm_and_si128(letter_mask, case_mask));\n            _mm_storeu_si128((__m128i*)&result[i], toggled);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = result[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 0x20;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 0x20;\n            }\n        }\n    } else {\n        // Reverse the string\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> temp;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        if (i < l.size()) {\n            temp.push_back(l[i]);\n        }\n    }\n    std::sort(temp.begin(), temp.end());\n    \n    for (size_t i = 0, j = 0; i < l.size(); i += 3, j++) {\n        if (i < l.size()) {\n            l[i] = temp[j];\n        }\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return l;\n    }\n\n    std::vector<int> indices;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        indices.push_back(i);\n    }\n\n    std::vector<int> values;\n    for (size_t i = 0; i < indices.size(); ++i) {\n        if (indices[i] < l.size()) {\n            values.push_back(l[indices[i]]);\n        }\n    }\n\n    std::sort(values.begin(), values.end());\n\n    for (size_t i = 0; i < indices.size(); ++i) {\n        if (indices[i] < l.size()) {\n            l[indices[i]] = values[i];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return l;\n    }\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < l.size(); i += 3) {\n        divisible_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Place the sorted elements back into the original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 3, j++) {\n        l[i] = divisible_by_three[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) return l;\n\n    // Extract elements divisible by 3\n    std::vector<int> divisible_by_3;\n    for (size_t i = 0; i < n; i += 3) {\n        divisible_by_3.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_3.begin(), divisible_by_3.end());\n\n    // Place the sorted elements back into the original vector\n    for (size_t i = 0, j = 0; i < n; i += 3, ++j) {\n        l[i] = divisible_by_3[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) return l;\n\n    std::vector<int> elements_to_sort;\n    for (size_t i = 0; i < n; i += 3) {\n        elements_to_sort.push_back(l[i]);\n    }\n\n    std::sort(elements_to_sort.begin(), elements_to_sort.end());\n\n    size_t idx = 0;\n    for (size_t i = 0; i < n; i += 3) {\n        l[i] = elements_to_sort[idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max_val = _mm_loadu_ps(&l[0]);\n    size_t i;\n    for (i = 4; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, current);\n    }\n    \n    float max_array[4];\n    _mm_storeu_ps(max_array, max_val);\n    float max_result = max_array[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_array[j] > max_result) {\n            max_result = max_array[j];\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > max_result) {\n            max_result = l[i];\n        }\n    }\n    \n    return max_result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max = _mm_loadu_ps(&l[0]);\n    size_t i;\n    for (i = 4; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max = _mm_max_ps(max, current);\n    }\n    \n    float max_arr[4];\n    _mm_storeu_ps(max_arr, max);\n    float result = max_arr[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_arr[j] > result) {\n            result = max_arr[j];\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max_val = _mm_loadu_ps(&l[0]);\n    size_t i;\n    for (i = 4; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, current);\n    }\n    \n    float max_array[4];\n    _mm_storeu_ps(max_array, max_val);\n    float max_result = max_array[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_array[j] > max_result) {\n            max_result = max_array[j];\n        }\n    }\n    \n    for (; i < l.size(); ++i) {\n        if (l[i] > max_result) {\n            max_result = l[i];\n        }\n    }\n    \n    return max_result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return 0.0f;\n\n    __m128 max_val = _mm_load_ss(&l[0]);\n    int i;\n    for (i = 0; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, current);\n    }\n\n    float max_array[4];\n    _mm_storeu_ps(max_array, max_val);\n    float max_result = max_array[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_array[j] > max_result) {\n            max_result = max_array[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_result) {\n            max_result = l[i];\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max = _mm_loadu_ps(&l[0]);\n    size_t i;\n    for (i = 4; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max = _mm_max_ps(max, current);\n    }\n\n    float max_arr[4];\n    _mm_storeu_ps(max_arr, max);\n    float result = max_arr[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_arr[j] > result) {\n            result = max_arr[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i sevens = _mm_set1_epi32(7);\n    __m128i zeros = _mm_setzero_si128();\n    __m128i elevens = _mm_set1_epi32(11);\n    __m128i thirteens = _mm_set1_epi32(13);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(nums, _mm_set1_epi32(n));\n        \n        // Check divisible by 11 or 13\n        __m128i rem11 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, elevens), elevens));\n        __m128i rem13 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, thirteens), thirteens));\n        __m128i div_mask = _mm_or_si128(_mm_cmpeq_epi32(rem11, zeros), _mm_cmpeq_epi32(rem13, zeros));\n        \n        // Count 7s in each number\n        __m128i temp = nums;\n        __m128i digit_counts = _mm_setzero_si128();\n        for (int j = 0; j < 10; j++) {\n            __m128i digit = _mm_sub_epi32(temp, _mm_mullo_epi32(_mm_div_epi32(temp, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n            digit_counts = _mm_add_epi32(digit_counts, _mm_and_si128(_mm_cmpeq_epi32(digit, sevens), _mm_set1_epi32(1)));\n            temp = _mm_div_epi32(temp, _mm_set1_epi32(10));\n        }\n        \n        // Combine masks and accumulate counts\n        __m128i res = _mm_and_si128(_mm_and_si128(mask, div_mask), digit_counts);\n        int counts[4];\n        _mm_storeu_si128((__m128i*)counts, res);\n        count += counts[0] + counts[1] + counts[2] + counts[3];\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i v_sevens = _mm_set1_epi32(7);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_eleven = _mm_set1_epi32(11);\n    __m128i v_thirteen = _mm_set1_epi32(13);\n    __m128i v_ten = _mm_set1_epi32(10);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v_nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_mask = _mm_cmplt_epi32(v_nums, _mm_set1_epi32(n));\n        \n        // Check divisible by 11 or 13\n        __m128i v_rem11 = _mm_sub_epi32(v_nums, _mm_mullo_epi32(_mm_div_epi32(v_nums, v_eleven), v_eleven));\n        __m128i v_rem13 = _mm_sub_epi32(v_nums, _mm_mullo_epi32(_mm_div_epi32(v_nums, v_thirteen), v_thirteen));\n        __m128i v_div11 = _mm_cmpeq_epi32(v_rem11, v_zero);\n        __m128i v_div13 = _mm_cmpeq_epi32(v_rem13, v_zero);\n        __m128i v_div = _mm_or_si128(v_div11, v_div13);\n        \n        // Count 7s in each number\n        __m128i v_count7 = _mm_setzero_si128();\n        __m128i v_current = v_nums;\n        \n        while (1) {\n            __m128i v_digit = _mm_sub_epi32(v_current, _mm_mullo_epi32(_mm_div_epi32(v_current, v_ten), v_ten));\n            __m128i v_is7 = _mm_cmpeq_epi32(v_digit, v_sevens);\n            v_count7 = _mm_sub_epi32(v_count7, v_is7); // Add 1 if equal\n            \n            v_current = _mm_div_epi32(v_current, v_ten);\n            __m128i v_done = _mm_cmpeq_epi32(v_current, v_zero);\n            if (_mm_movemask_epi8(v_done) == 0xFFFF) break;\n        }\n        \n        // Combine results\n        v_count7 = _mm_and_si128(v_count7, v_div);\n        v_count7 = _mm_and_si128(v_count7, v_mask);\n        \n        // Sum the counts\n        alignas(16) int counts[4];\n        _mm_store_si128((__m128i*)counts, v_count7);\n        count += counts[0] + counts[1] + counts[2] + counts[3];\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i sevens = _mm_set1_epi32(7);\n    __m128i zeros = _mm_setzero_si128();\n    __m128i elevens = _mm_set1_epi32(11);\n    __m128i thirteens = _mm_set1_epi32(13);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(nums, _mm_set1_epi32(n));\n        \n        // Check divisible by 11 or 13\n        __m128i rem11 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, elevens), elevens));\n        __m128i rem13 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, thirteens), thirteens));\n        __m128i div_mask = _mm_or_si128(_mm_cmpeq_epi32(rem11, zeros), _mm_cmpeq_epi32(rem13, zeros));\n        div_mask = _mm_and_si128(div_mask, mask);\n\n        // Count 7s in each number\n        __m128i temp = nums;\n        __m128i digit_counts = _mm_setzero_si128();\n        for (int j = 0; j < 10; j++) {\n            __m128i digits = _mm_sub_epi32(temp, _mm_mullo_epi32(_mm_div_epi32(temp, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n            digit_counts = _mm_add_epi32(digit_counts, _mm_and_si128(_mm_cmpeq_epi32(digits, sevens), _mm_set1_epi32(1)));\n            temp = _mm_div_epi32(temp, _mm_set1_epi32(10));\n        }\n\n        // Multiply by div_mask and accumulate\n        digit_counts = _mm_and_si128(digit_counts, div_mask);\n        int counts[4];\n        _mm_storeu_si128((__m128i*)counts, digit_counts);\n        count += counts[0] + counts[1] + counts[2] + counts[3];\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i vcount = _mm_setzero_si128();\n    __m128i v7 = _mm_set1_epi32('7');\n    __m128i v11 = _mm_set1_epi32(11);\n    __m128i v13 = _mm_set1_epi32(13);\n    __m128i vzero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i vi = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(vi, _mm_set1_epi32(n));\n        \n        // Check divisible by 11 or 13\n        __m128i rem11 = _mm_sub_epi32(vi, _mm_mullo_epi32(_mm_div_epi32(vi, v11), v11));\n        __m128i rem13 = _mm_sub_epi32(vi, _mm_mullo_epi32(_mm_div_epi32(vi, v13), v13));\n        __m128i div11 = _mm_cmpeq_epi32(rem11, vzero);\n        __m128i div13 = _mm_cmpeq_epi32(rem13, vzero);\n        __m128i div = _mm_or_si128(div11, div13);\n        \n        // Convert number to string and count '7's\n        for (int j = 0; j < 4; j++) {\n            int num = _mm_extract_epi32(vi, j);\n            if (num >= n) continue;\n            \n            if (_mm_extract_epi32(div, j)) {\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (char *p = str; *p; p++) {\n                    if (*p == '7') count++;\n                }\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i sevens = _mm_set1_epi32(7);\n    __m128i zeros = _mm_setzero_si128();\n    __m128i elevens = _mm_set1_epi32(11);\n    __m128i thirteens = _mm_set1_epi32(13);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(nums, _mm_set1_epi32(n));\n        \n        // Check divisible by 11 or 13\n        __m128i rem11 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, elevens), elevens));\n        __m128i rem13 = _mm_sub_epi32(nums, _mm_mullo_epi32(_mm_div_epi32(nums, thirteens), thirteens));\n        __m128i div11 = _mm_cmpeq_epi32(rem11, zeros);\n        __m128i div13 = _mm_cmpeq_epi32(rem13, zeros);\n        __m128i div = _mm_or_si128(div11, div13);\n        \n        // Count 7s in each number\n        __m128i num = nums;\n        __m128i sum = zeros;\n        for (int j = 0; j < 4; j++) {\n            __m128i digit = _mm_sub_epi32(num, _mm_mullo_epi32(_mm_div_epi32(num, _mm_set1_epi32(10)), _mm_set1_epi32(10));\n            sum = _mm_add_epi32(sum, _mm_cmpeq_epi32(digit, sevens));\n            num = _mm_div_epi32(num, _mm_set1_epi32(10));\n        }\n        \n        // Combine conditions\n        __m128i res = _mm_and_si128(_mm_and_si128(div, mask), sum);\n        int mask_bits = _mm_movemask_epi8(_mm_cmpgt_epi32(res, zeros));\n        \n        // Extract and accumulate counts\n        alignas(16) int counts[4];\n        _mm_store_si128((__m128i*)counts, res);\n        for (int j = 0; j < 4; j++) {\n            if (mask_bits & (1 << (j*4))) {\n                count += counts[j];\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = evens[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the evens\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = evens[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) {\n        return l;\n    }\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into the original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = evens[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) {\n        return l;\n    }\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = evens[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    if (l.size() < 2) return l;\n\n    // Extract even indices\n    std::vector<float> evens;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Merge back into original vector\n    for (size_t i = 0, j = 0; i < l.size(); i += 2, ++j) {\n        l[i] = evens[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i sum_ij = _mm_add_epi32(vi, vj);\n            \n            int k = j + 1;\n            for (; k + 3 < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(sum_ij, vk);\n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            int k = j + 1;\n            for (; k + 3 < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(vi, vj);\n                sum = _mm_add_epi32(sum, vk);\n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i sum_ij = _mm_add_epi32(vi, vj);\n            int remaining = n - (j + 1);\n            int k = j + 1;\n\n            for (; k + 3 < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(sum_ij, vk);\n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i sum_ij = _mm_add_epi32(vi, vj);\n            int remaining = n - j - 1;\n            int k = j + 1;\n\n            for (; k + 3 < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(sum_ij, vk);\n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i sum_ij = _mm_add_epi32(vi, vj);\n            int remaining = n - j - 1;\n            int k = j + 1;\n\n            for (; k + 3 < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(sum_ij, vk);\n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t aligned_size = l.size() - (l.size() % 4);\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)&l[i], vec);\n    }\n\n    for (; i < l.size(); ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t vec_size = l.size();\n    const size_t aligned_size = vec_size - (vec_size % 4);\n\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)&l[i], vec);\n    }\n\n    for (; i < vec_size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    size_t size = l.size();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i + 3 < size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)&l[i], vec);\n    }\n\n    for (; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    size_t size = l.size();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)&l[i], vec);\n    }\n\n    for (; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t i = 0;\n    const size_t size = l.size();\n    const size_t aligned_size = size - (size % 4);\n    \n    __m128i one = _mm_set1_epi32(1);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)&l[i], vec);\n    }\n    \n    for (; i < size; ++i) {\n        l[i] += 1;\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m128i a = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 3 < l.size(); j += 4) {\n            __m128i b = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(a, b);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m128i a = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 3 < l.size(); j += 4) {\n            __m128i b = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(a, b);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m128i a = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 3 < l.size(); j += 4) {\n            __m128i b = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(a, b);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m128i a = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 3 < l.size(); j += 4) {\n            __m128i b = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(a, b);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        __m128i current = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n        for (; j + 3 < l.size(); j += 4) {\n            __m128i elements = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(current, elements);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        for (; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        result.push_back('0' + remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 16 <= j) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + j - 15));\n        \n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            return false;\n        }\n        \n        i += 16;\n        j -= 16;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 16 <= j) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + j - 15));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            return false;\n        }\n        \n        i += 16;\n        j -= 16;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 16 <= j) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + j - 15));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            return false;\n        }\n        \n        i += 16;\n        j -= 16;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 16 <= j) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + j - 15));\n        \n        __m128i reversed_right = _mm_shuffle_epi8(right, \n            _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            return false;\n        }\n        \n        i += 16;\n        j -= 16;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n    \n    const char* str = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n    \n    while (i + 16 <= j) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + j - 15));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            return false;\n        }\n        \n        i += 16;\n        j -= 16;\n    }\n    \n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    \n    const __m128i a_vec = _mm_set1_epi8('a');\n    const __m128i five_vec = _mm_set1_epi8(5);\n    const __m128i twenty_six_vec = _mm_set1_epi8(26);\n    \n    size_t i = 0;\n    for (; i + 15 < s.length(); i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i shifted = _mm_sub_epi8(chars, five_vec);\n        __m128i mod = _mm_sub_epi8(shifted, a_vec);\n        __m128i mask = _mm_cmpgt_epi8(a_vec, shifted);\n        __m128i add = _mm_and_si128(mask, twenty_six_vec);\n        mod = _mm_add_epi8(mod, add);\n        __m128i result = _mm_add_epi8(mod, a_vec);\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n    \n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    \n    const __m128i a_vec = _mm_set1_epi8('a');\n    const __m128i five_vec = _mm_set1_epi8(5);\n    const __m128i twenty_six_vec = _mm_set1_epi8(26);\n    \n    size_t i = 0;\n    for (; i + 16 <= s.length(); i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i shifted = _mm_sub_epi8(chars, five_vec);\n        __m128i mod = _mm_sub_epi8(shifted, a_vec);\n        __m128i mod_lt_zero = _mm_cmplt_epi8(mod, _mm_setzero_si128());\n        mod = _mm_add_epi8(mod, _mm_and_si128(mod_lt_zero, twenty_six_vec));\n        __m128i result = _mm_add_epi8(mod, a_vec);\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n    \n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    \n    const __m128i a = _mm_set1_epi8('a');\n    const __m128i five = _mm_set1_epi8(5);\n    const __m128i twenty_six = _mm_set1_epi8(26);\n    \n    size_t i = 0;\n    for (; i + 15 < s.length(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i shifted = _mm_sub_epi8(chunk, five);\n        __m128i diff = _mm_sub_epi8(shifted, a);\n        __m128i mod = _mm_add_epi8(_mm_and_si128(diff, _mm_set1_epi8(0x1F)), twenty_six);\n        __m128i result = _mm_add_epi8(mod, a);\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n    \n    for (; i < s.length(); i++) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    \n    const __m128i a_vec = _mm_set1_epi8('a');\n    const __m128i five_vec = _mm_set1_epi8(5);\n    const __m128i twenty_six_vec = _mm_set1_epi8(26);\n    \n    size_t i = 0;\n    for (; i + 16 <= s.length(); i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        // Subtract 5 and handle underflow by adding 26 if needed\n        __m128i shifted = _mm_sub_epi8(chars, five_vec);\n        __m128i underflow = _mm_cmplt_epi8(shifted, a_vec);\n        __m128i correction = _mm_and_si128(underflow, twenty_six_vec);\n        shifted = _mm_add_epi8(shifted, correction);\n        \n        _mm_storeu_si128((__m128i*)&out[i], shifted);\n    }\n    \n    // Handle remaining characters\n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    \n    const __m128i a_vec = _mm_set1_epi8('a');\n    const __m128i five_vec = _mm_set1_epi8(5);\n    const __m128i twenty_six_vec = _mm_set1_epi8(26);\n    \n    size_t i = 0;\n    for (; i + 15 < s.length(); i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        __m128i shifted = _mm_sub_epi8(chars, five_vec);\n        __m128i mod = _mm_sub_epi8(shifted, a_vec);\n        __m128i mod_neg = _mm_add_epi8(mod, twenty_six_vec);\n        __m128i mask = _mm_cmpgt_epi8(mod, _mm_setzero_si128());\n        mod = _mm_or_si128(_mm_and_si128(mask, mod), _mm_andnot_si128(mask, mod_neg));\n        __m128i result = _mm_add_epi8(mod, a_vec);\n        \n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n    \n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t size = l.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t size = l.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t size = l.size();\n    size_t simd_size = size - (size % 4);\n\n    for (; i < simd_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t size = l.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t size = l.size();\n    size_t simd_size = size - (size % 4);\n\n    for (; i < simd_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n\n    for (int i = 0; i < len; i += 16) {\n        int remaining = len - i;\n        int chunk_size = remaining < 16 ? remaining : 16;\n\n        __m128i chunk;\n        if (chunk_size == 16) {\n            chunk = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        } else {\n            char buffer[16] = {0};\n            std::memcpy(buffer, brackets.data() + i, chunk_size);\n            chunk = _mm_loadu_si128((__m128i*)buffer);\n        }\n\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n\n        int mask_open = _mm_movemask_epi8(cmp_open);\n        int mask_close = _mm_movemask_epi8(cmp_close);\n\n        for (int j = 0; j < chunk_size; ++j) {\n            if (mask_open & (1 << j)) {\n                balance++;\n            } else if (mask_close & (1 << j)) {\n                balance--;\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n\n    for (int i = 0; i < len; i += 16) {\n        int remaining = len - i;\n        int chunk_size = remaining < 16 ? remaining : 16;\n\n        __m128i chunk;\n        if (chunk_size == 16) {\n            chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        } else {\n            char temp[16] = {0};\n            for (int j = 0; j < chunk_size; j++) {\n                temp[j] = brackets[i + j];\n            }\n            chunk = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n\n        int mask_open = _mm_movemask_epi8(cmp_open);\n        int mask_close = _mm_movemask_epi8(cmp_close);\n\n        balance += __builtin_popcount(mask_open);\n        balance -= __builtin_popcount(mask_close);\n\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    int balance = 0;\n\n    for (int i = 0; i < len; i += 16) {\n        int remaining = len - i;\n        int chunk_size = remaining < 16 ? remaining : 16;\n\n        __m128i chunk;\n        if (chunk_size == 16) {\n            chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        } else {\n            char temp[16] = {0};\n            for (int j = 0; j < chunk_size; j++) {\n                temp[j] = brackets[i + j];\n            }\n            chunk = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n\n        int mask_open = _mm_movemask_epi8(cmp_open);\n        int mask_close = _mm_movemask_epi8(cmp_close);\n\n        for (int j = 0; j < chunk_size; j++) {\n            if (mask_open & (1 << j)) {\n                balance++;\n            } else if (mask_close & (1 << j)) {\n                balance--;\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) {\n        return false;\n    }\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    int balance = 0;\n\n    for (int i = 0; i < len; i += 16) {\n        int remaining = len - i;\n        int chunk_size = remaining < 16 ? remaining : 16;\n\n        __m128i chunk;\n        if (chunk_size == 16) {\n            chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        } else {\n            char buffer[16] = {0};\n            for (int j = 0; j < chunk_size; ++j) {\n                buffer[j] = brackets[i + j];\n            }\n            chunk = _mm_loadu_si128((__m128i*)buffer);\n        }\n\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n\n        int mask_open = _mm_movemask_epi8(cmp_open);\n        int mask_close = _mm_movemask_epi8(cmp_close);\n\n        for (int j = 0; j < chunk_size; ++j) {\n            if (mask_open & (1 << j)) {\n                balance++;\n            } else if (mask_close & (1 << j)) {\n                balance--;\n                if (balance < 0) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.size();\n    if (len % 2 != 0) {\n        return false;\n    }\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n    __m128i min = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n        __m128i diff = _mm_sub_epi8(_mm_and_si128(cmp_open, _mm_set1_epi8(1)),\n                                   _mm_and_si128(cmp_close, _mm_set1_epi8(1)));\n        sum = _mm_add_epi8(sum, diff);\n        min = _mm_min_epi8(min, sum);\n    }\n\n    int balance = 0;\n    int min_balance = 0;\n    for (int j = 0; j < 16; ++j) {\n        balance += ((char*)&sum)[j];\n        if (balance < min_balance) {\n            min_balance = balance;\n        }\n    }\n\n    for (; i < len; ++i) {\n        balance += (brackets[i] == '<') ? 1 : -1;\n        if (balance < min_balance) {\n            min_balance = balance;\n        }\n    }\n\n    return balance == 0 && min_balance >= 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    __m128i open_mask = _mm_set1_epi8('(');\n    __m128i close_mask = _mm_set1_epi8(')');\n    __m128i balance_inc = _mm_set1_epi8(1);\n    __m128i balance_dec = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_cmp = _mm_cmpeq_epi8(chunk, open_mask);\n        __m128i close_cmp = _mm_cmpeq_epi8(chunk, close_mask);\n\n        __m128i open_count = _mm_and_si128(open_cmp, balance_inc);\n        __m128i close_count = _mm_and_si128(close_cmp, balance_dec);\n\n        __m128i delta = _mm_add_epi8(open_count, close_count);\n        int mask = _mm_movemask_epi8(delta);\n\n        for (int j = 0; j < 16; ++j) {\n            if (mask & (1 << j)) {\n                balance += (delta.m128i_i8[j] > 0) ? 1 : -1;\n                if (balance < 0) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    __m128i open_mask = _mm_set1_epi8('(');\n    __m128i close_mask = _mm_set1_epi8(')');\n    __m128i increment = _mm_set1_epi8(1);\n    __m128i decrement = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_cmp = _mm_cmpeq_epi8(chunk, open_mask);\n        __m128i close_cmp = _mm_cmpeq_epi8(chunk, close_mask);\n        \n        __m128i delta = _mm_add_epi8(\n            _mm_and_si128(open_cmp, increment),\n            _mm_and_si128(close_cmp, decrement)\n        );\n\n        alignas(16) int8_t deltas[16];\n        _mm_store_si128((__m128i*)deltas, delta);\n\n        for (int j = 0; j < 16; ++j) {\n            balance += deltas[j];\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    __m128i open_mask = _mm_set1_epi8('(');\n    __m128i close_mask = _mm_set1_epi8(')');\n    __m128i increment = _mm_set1_epi8(1);\n    __m128i decrement = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_cmp = _mm_cmpeq_epi8(chunk, open_mask);\n        __m128i close_cmp = _mm_cmpeq_epi8(chunk, close_mask);\n\n        __m128i open_count = _mm_and_si128(open_cmp, increment);\n        __m128i close_count = _mm_and_si128(close_cmp, decrement);\n\n        __m128i delta = _mm_add_epi8(open_count, close_count);\n        alignas(16) int8_t deltas[16];\n        _mm_store_si128((__m128i*)deltas, delta);\n\n        for (int j = 0; j < 16; ++j) {\n            balance += deltas[j];\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    __m128i open_mask = _mm_set1_epi8('(');\n    __m128i close_mask = _mm_set1_epi8(')');\n    __m128i increment = _mm_set1_epi8(1);\n    __m128i decrement = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_cmp = _mm_cmpeq_epi8(chunk, open_mask);\n        __m128i close_cmp = _mm_cmpeq_epi8(chunk, close_mask);\n        __m128i delta = _mm_add_epi8(_mm_and_si128(open_cmp, increment), \n                                    _mm_and_si128(close_cmp, decrement));\n\n        alignas(16) int8_t deltas[16];\n        _mm_store_si128((__m128i*)deltas, delta);\n\n        for (int j = 0; j < 16; ++j) {\n            balance += deltas[j];\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n\n        int mask_open = _mm_movemask_epi8(cmp_open);\n        int mask_close = _mm_movemask_epi8(cmp_close);\n\n        balance += __builtin_popcount(mask_open);\n        balance -= __builtin_popcount(mask_close);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return {};\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    __m128 multiplier = _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f);\n\n    for (; i + 4 <= result.size(); i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 res = _mm_mul_ps(coeffs, multiplier);\n        _mm_storeu_ps(&result[i], res);\n    }\n\n    for (; i < result.size(); ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.empty()) {\n        return {};\n    }\n    \n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    const size_t aligned_size = (result.size() / 4) * 4;\n    \n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128i current_indices = _mm_add_epi32(_mm_set1_epi32(i), indices);\n        __m128 current_coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 current_deriv = _mm_mul_ps(current_coeffs, _mm_cvtepi32_ps(current_indices));\n        _mm_storeu_ps(&result[i], current_deriv);\n    }\n    \n    for (; i < result.size(); ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    __m128 multiplier = _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f);\n\n    for (; i + 4 <= result.size(); i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 res = _mm_mul_ps(coeffs, multiplier);\n        _mm_storeu_ps(&result[i], res);\n    }\n\n    for (; i < result.size(); ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n\n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    __m128 multiplier = _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f);\n\n    for (; i + 4 <= result.size(); i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 res = _mm_mul_ps(coeffs, multiplier);\n        _mm_storeu_ps(&result[i], res);\n    }\n\n    for (; i < result.size(); ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.empty()) return {};\n    \n    std::vector<float> result(xs.size() - 1);\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t n = result.size();\n    \n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n    __m128 one = _mm_set1_ps(1.0f);\n    \n    for (; i + simd_size <= n; i += simd_size) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128i current_indices = _mm_add_epi32(indices, _mm_set1_epi32(i));\n        __m128 factors = _mm_cvtepi32_ps(current_indices);\n        __m128 derivative = _mm_mul_ps(coeffs, factors);\n        _mm_storeu_ps(&result[i], derivative);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n\n    __m128i vowels = _mm_set1_epi8(0);\n    vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i lower = _mm_or_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('a')),\n                                    _mm_cmpeq_epi8(chunk, _mm_set1_epi8('e')));\n        lower = _mm_or_si128(lower, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('i')));\n        lower = _mm_or_si128(lower, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('o')));\n        lower = _mm_or_si128(lower, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('u')));\n\n        __m128i upper = _mm_or_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('A')),\n                                    _mm_cmpeq_epi8(chunk, _mm_set1_epi8('E')));\n        upper = _mm_or_si128(upper, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('I')));\n        upper = _mm_or_si128(upper, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('O')));\n        upper = _mm_or_si128(upper, _mm_cmpeq_epi8(chunk, _mm_set1_epi8('U')));\n\n        __m128i mask = _mm_or_si128(lower, upper);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    if (len > 0 && (str[len-1] == 'y' || str[len-1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n    \n    __m128i vowels = _mm_set1_epi8(0);\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('a'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('e'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('i'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('o'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('u'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('A'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('E'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('I'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('O'), vowels));\n    vowels = _mm_or_si128(vowels, _mm_cmpeq_epi8(_mm_set1_epi8('U'), vowels));\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i cmp = _mm_cmpeq_epi8(_mm_and_si128(chunk, _mm_set1_epi8(0xDF)), vowels);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n    }\n\n    for (; i < len - 1; ++i) {\n        char c = str[i] | 0x20;\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            ++count;\n        }\n    }\n\n    if (len > 0) {\n        char last = str[len - 1] | 0x20;\n        if (last == 'a' || last == 'e' || last == 'i' || last == 'o' || last == 'u' || last == 'y') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    // Convert to lowercase for case-insensitive comparison\n    std::transform(s.begin(), s.end(), s.begin(), ::tolower);\n\n    int len = s.size();\n    int count = 0;\n\n    // Check if last character is 'y'\n    bool last_is_y = (s.back() == 'y');\n    if (last_is_y) {\n        count++;\n        len--; // Exclude 'y' from SIMD processing\n    }\n\n    // Process 16 characters at a time\n    const int simd_width = 16;\n    int i = 0;\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i e_vowel = _mm_set1_epi8('e');\n    __m128i i_vowel = _mm_set1_epi8('i');\n    __m128i o_vowel = _mm_set1_epi8('o');\n    __m128i u_vowel = _mm_set1_epi8('u');\n\n    for (; i + simd_width <= len; i += simd_width) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        __m128i a_match = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i e_match = _mm_cmpeq_epi8(chunk, e_vowel);\n        __m128i i_match = _mm_cmpeq_epi8(chunk, i_vowel);\n        __m128i o_match = _mm_cmpeq_epi8(chunk, o_vowel);\n        __m128i u_match = _mm_cmpeq_epi8(chunk, u_vowel);\n        \n        __m128i any_match = _mm_or_si128(\n            _mm_or_si128(a_match, e_match),\n            _mm_or_si128(\n                _mm_or_si128(i_match, o_match),\n                u_match\n            )\n        );\n        \n        int mask = _mm_movemask_epi8(any_match);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    int count = 0;\n    size_t len = s.size();\n    const char* str = s.c_str();\n\n    // Convert to lowercase for case-insensitive comparison\n    std::string lowerStr = s;\n    std::transform(lowerStr.begin(), lowerStr.end(), lowerStr.begin(), ::tolower);\n    const char* lower = lowerStr.c_str();\n\n    // Check for 'y' at the end\n    if (lower[len - 1] == 'y') {\n        count++;\n    }\n\n    // Process the string in chunks of 16 bytes\n    const size_t chunkSize = 16;\n    size_t remaining = len;\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i mask = _mm_setr_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 1, 0, 0, 1, 0, 0\n    ); // Mask for a, e, i, o, u\n\n    while (remaining >= chunkSize) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)lower);\n        __m128i eqA = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('a'));\n        __m128i eqE = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('e'));\n        __m128i eqI = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('i'));\n        __m128i eqO = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('o'));\n        __m128i eqU = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('u'));\n\n        __m128i or1 = _mm_or_si128(eqA, eqE);\n        __m128i or2 = _mm_or_si128(eqI, eqO);\n        __m128i or3 = _mm_or_si128(or1, or2);\n        __m128i or4 = _mm_or_si128(or3, eqU);\n\n        int mask = _mm_movemask_epi8(or4);\n        count += __builtin_popcount(mask);\n\n        lower += chunkSize;\n        remaining -= chunkSize;\n    }\n\n    // Process remaining characters\n    for (size_t i = len - remaining; i < len; ++i) {\n        char c = lower[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    // Convert string to lowercase for case-insensitive comparison\n    std::transform(s.begin(), s.end(), s.begin(), ::tolower);\n\n    int len = s.size();\n    int count = 0;\n    int i = 0;\n\n    // Check for 'y' at the end\n    if (s.back() == 'y') {\n        count++;\n        len--; // Exclude 'y' from SIMD processing\n    }\n\n    // Load vowels into SIMD registers\n    __m128i vowels = _mm_set1_epi8(0);\n    vowels = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 'u', 'o', 'i', 'e', 'a', 0, 0, 0);\n\n    // Process 16 characters at a time\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i cmp = _mm_cmpeq_epi8(chunk, vowels);\n        int mask = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_Z = _mm_set1_epi8('Z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_A, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_Z, _mm_set1_epi8(1)))\n        );\n        __m128i upper_chars = _mm_and_si128(chunk, mask);\n        __m128i sum_lo = _mm_unpacklo_epi8(upper_chars, zero);\n        __m128i sum_hi = _mm_unpackhi_epi8(upper_chars, zero);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_add_epi32(sum_lo, sum_hi));\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum_shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuf);\n    sum_shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuf);\n    sum += _mm_cvtsi128_si32(sum_vec);\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i upper_sum = zero;\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_Z = _mm_set1_epi8('Z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_A, _mm_set1_epi8(1))),\n                                        _mm_cmpgt_epi8(_mm_add_epi8(upper_Z, _mm_set1_epi8(1)), chunk));\n        __m128i masked = _mm_and_si128(chunk, is_upper);\n        upper_sum = _mm_add_epi32(upper_sum, _mm_sad_epu8(masked, zero));\n    }\n\n    // Sum the partial results from SIMD\n    uint64_t upper_sum_arr[2];\n    _mm_storeu_si128((__m128i*)upper_sum_arr, upper_sum);\n    sum += upper_sum_arr[0] + upper_sum_arr[1];\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_Z = _mm_set1_epi8('Z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i mask1 = _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_A, _mm_set1_epi8(1)));\n        __m128i mask2 = _mm_cmpgt_epi8(_mm_add_epi8(upper_Z, _mm_set1_epi8(1)), chunk);\n        __m128i mask = _mm_and_si128(mask1, mask2);\n        __m128i upper_chars = _mm_and_si128(chunk, mask);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_sad_epu8(upper_chars, zero));\n    }\n\n    sum += _mm_extract_epi16(sum_vec, 0) + _mm_extract_epi16(sum_vec, 4);\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_Z = _mm_set1_epi8('Z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_A, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_Z, _mm_set1_epi8(1)))\n        );\n        __m128i upper_chars = _mm_and_si128(chunk, mask);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_sad_epu8(upper_chars, zero));\n    }\n\n    sum += _mm_extract_epi16(sum_vec, 0) + _mm_extract_epi16(sum_vec, 4);\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_Z = _mm_set1_epi8('Z');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        __m128i is_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_A, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_Z, _mm_set1_epi8(1)))\n        );\n        \n        __m128i masked = _mm_and_si128(chunk, is_upper);\n        \n        __m128i sum_lo = _mm_unpacklo_epi8(masked, zero);\n        __m128i sum_hi = _mm_unpackhi_epi8(masked, zero);\n        \n        sum_vec = _mm_add_epi32(sum_vec, _mm_add_epi32(sum_lo, sum_hi));\n    }\n\n    // Horizontal sum of sum_vec\n    __m128i sum_shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuf);\n    sum_shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuf);\n    sum += _mm_cvtsi128_si32(sum_vec);\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t simd_width = 4;\n    const size_t n = arr.size();\n    const size_t simd_end = n - (n % simd_width);\n\n    __m128i min_vals = _mm_set1_epi32(INT_MAX);\n    __m128i min_indices = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n\n    for (; i < simd_end; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(data, _mm_set1_epi32(1));\n        mask = _mm_cmpeq_epi32(mask, _mm_setzero_si128());\n        \n        __m128i current_min = _mm_or_si128(data, _mm_andnot_si128(mask, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp = _mm_cmplt_epi32(current_min, min_vals);\n        \n        min_vals = _mm_min_epi32(min_vals, current_min);\n        min_indices = _mm_blendv_epi8(min_indices, indices, cmp);\n        \n        indices = _mm_add_epi32(indices, _mm_set1_epi32(simd_width));\n    }\n\n    int temp_min[4], temp_idx[4];\n    _mm_storeu_si128((__m128i*)temp_min, min_vals);\n    _mm_storeu_si128((__m128i*)temp_idx, min_indices);\n\n    for (int j = 0; j < 4; ++j) {\n        if (temp_min[j] < min_even) {\n            min_even = temp_min[j];\n            min_index = temp_idx[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t simd_width = 4;\n    const size_t n = arr.size();\n    const size_t end_simd = n - (n % simd_width);\n\n    __m128i min_vals = _mm_set1_epi32(INT_MAX);\n    __m128i min_indices = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n\n    for (; i < end_simd; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(data, _mm_set1_epi32(1));\n        mask = _mm_cmpeq_epi32(mask, _mm_setzero_si128());\n        \n        __m128i current_min = _mm_or_si128(data, _mm_andnot_si128(mask, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp = _mm_cmplt_epi32(current_min, min_vals);\n        \n        min_vals = _mm_min_epi32(min_vals, current_min);\n        min_indices = _mm_blendv_epi8(min_indices, indices, cmp);\n        \n        indices = _mm_add_epi32(indices, _mm_set1_epi32(simd_width));\n    }\n\n    int simd_min[4], simd_idx[4];\n    _mm_storeu_si128((__m128i*)simd_min, min_vals);\n    _mm_storeu_si128((__m128i*)simd_idx, min_indices);\n\n    for (int j = 0; j < 4; ++j) {\n        if (simd_min[j] < min_even) {\n            min_even = simd_min[j];\n            min_index = simd_idx[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t simd_width = 4;\n    const size_t n = arr.size();\n    const size_t end_simd = n - (n % simd_width);\n\n    __m128i min_vals = _mm_set1_epi32(INT_MAX);\n    __m128i min_idxs = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (; i < end_simd; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(data, _mm_set1_epi32(1));\n        mask = _mm_cmpeq_epi32(mask, _mm_setzero_si128());\n        \n        __m128i current_min = _mm_or_si128(data, _mm_andnot_si128(mask, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp = _mm_cmplt_epi32(current_min, min_vals);\n        \n        min_vals = _mm_min_epi32(min_vals, current_min);\n        min_idxs = _mm_blendv_epi8(min_idxs, indices, cmp);\n        \n        indices = _mm_add_epi32(indices, increment);\n    }\n\n    int min_vals_arr[4], min_idxs_arr[4];\n    _mm_storeu_si128((__m128i*)min_vals_arr, min_vals);\n    _mm_storeu_si128((__m128i*)min_idxs_arr, min_idxs);\n\n    for (int j = 0; j < 4; ++j) {\n        if (min_vals_arr[j] < min_even) {\n            min_even = min_vals_arr[j];\n            min_index = min_idxs_arr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t n = arr.size();\n    const size_t end_simd = n - (n % simd_size);\n\n    __m128i min_vals = _mm_set1_epi32(INT_MAX);\n    __m128i min_indices = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (; i < end_simd; i += simd_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(data, _mm_set1_epi32(1));\n        mask = _mm_cmpeq_epi32(mask, _mm_setzero_si128());\n        \n        __m128i current_min = _mm_min_epi32(data, min_vals);\n        __m128i new_min_vals = _mm_blendv_epi8(min_vals, current_min, mask);\n        __m128i new_min_indices = _mm_blendv_epi8(min_indices, indices, mask);\n        \n        min_vals = new_min_vals;\n        min_indices = new_min_indices;\n        indices = _mm_add_epi32(indices, increment);\n    }\n\n    int simd_min[4];\n    int simd_idx[4];\n    _mm_storeu_si128((__m128i*)simd_min, min_vals);\n    _mm_storeu_si128((__m128i*)simd_idx, min_indices);\n\n    for (int j = 0; j < 4; ++j) {\n        if (simd_min[j] < min_even && (simd_min[j] % 2) == 0) {\n            min_even = simd_min[j];\n            min_index = simd_idx[j];\n        } else if (simd_min[j] == min_even && simd_idx[j] < min_index && (simd_min[j] % 2) == 0) {\n            min_index = simd_idx[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            } else if (arr[i] == min_even && i < min_index) {\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const size_t simd_width = 4;\n    const size_t n = arr.size();\n    const size_t end_simd = n - (n % simd_width);\n\n    __m128i min_vals = _mm_set1_epi32(INT_MAX);\n    __m128i min_idxs = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (; i < end_simd; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(data, _mm_set1_epi32(1));\n        mask = _mm_cmpeq_epi32(mask, _mm_setzero_si128());\n        \n        __m128i current_min = _mm_blendv_epi8(_mm_set1_epi32(INT_MAX), data, mask);\n        __m128i current_idx = _mm_blendv_epi8(_mm_set1_epi32(-1), indices, mask);\n        \n        __m128i cmp = _mm_cmplt_epi32(current_min, min_vals);\n        min_vals = _mm_min_epi32(min_vals, current_min);\n        min_idxs = _mm_blendv_epi8(min_idxs, current_idx, cmp);\n        \n        indices = _mm_add_epi32(indices, increment);\n    }\n\n    int simd_min[4], simd_idx[4];\n    _mm_storeu_si128((__m128i*)simd_min, min_vals);\n    _mm_storeu_si128((__m128i*)simd_idx, min_idxs);\n\n    for (int j = 0; j < 4; ++j) {\n        if (simd_min[j] < min_even && (simd_min[j] % 2) == 0) {\n            min_even = simd_min[j];\n            min_index = simd_idx[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    for (const auto& pair : freq) {\n        if (pair.first == pair.second && pair.first > max_val) {\n            max_val = pair.first;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process the map in chunks of 4 for SIMD\n    auto it = freq.begin();\n    int size = freq.size();\n    int remainder = size % 4;\n    int simd_iters = size - remainder;\n\n    __m128i max_val_vec = _mm_set1_epi32(-1);\n    __m128i max_key_vec = _mm_set1_epi32(-1);\n\n    for (int i = 0; i < simd_iters; i += 4) {\n        // Load keys and frequencies\n        int keys[4], freqs[4];\n        for (int j = 0; j < 4; ++j, ++it) {\n            keys[j] = it->first;\n            freqs[j] = it->second;\n        }\n\n        __m128i key_vec = _mm_loadu_si128((__m128i*)keys);\n        __m128i freq_vec = _mm_loadu_si128((__m128i*)freqs);\n\n        // Compare freq >= key\n        __m128i cmp = _mm_cmpge_epi32(freq_vec, key_vec);\n        __m128i mask = _mm_and_si128(cmp, _mm_cmpgt_epi32(key_vec, _mm_setzero_si128()));\n\n        // Blend potential candidates\n        __m128i candidate_key = _mm_and_si128(key_vec, mask);\n        __m128i candidate_val = _mm_and_si128(freq_vec, mask);\n\n        // Update max_key_vec and max_val_vec\n        __m128i gt_mask = _mm_cmpgt_epi32(candidate_key, max_key_vec);\n        max_key_vec = _mm_or_si128(\n            _mm_and_si128(gt_mask, candidate_key),\n            _mm_andnot_si128(gt_mask, max_key_vec)\n        );\n        max_val_vec = _mm_or_si128(\n            _mm_and_si128(gt_mask, candidate_val),\n            _mm_andnot_si128(gt_mask, max_val_vec)\n        );\n    }\n\n    // Extract max from SIMD registers\n    int simd_keys[4], simd_vals[4];\n    _mm_storeu_si128((__m128i*)simd_keys, max_key_vec);\n    _mm_storeu_si128((__m128i*)simd_vals, max_val_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        if (simd_keys[i] > max_key) {\n            max_key = simd_keys[i];\n            max_val = simd_vals[i];\n        }\n    }\n\n    // Process remaining elements\n    for (int i = 0; i < remainder; ++i, ++it) {\n        int key = it->first;\n        int val = it->second;\n        if (val >= key && key > 0 && key > max_key) {\n            max_key = key;\n            max_val = val;\n        }\n    }\n\n    return max_key;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    for (const auto& pair : freq) {\n        if (pair.first == pair.second && pair.first > max_val) {\n            max_val = pair.first;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    int max_key = -1;\n\n    // Process keys in chunks of 4 using SSE\n    auto it = freq.begin();\n    int size = freq.size();\n    int remainder = size % 4;\n    int chunks = size / 4;\n\n    for (int i = 0; i < chunks; ++i) {\n        __m128i keys = _mm_set_epi32(\n            (it + 0)->first,\n            (it + 1)->first,\n            (it + 2)->first,\n            (it + 3)->first\n        );\n        __m128i freqs = _mm_set_epi32(\n            (it + 0)->second,\n            (it + 1)->second,\n            (it + 2)->second,\n            (it + 3)->second\n        );\n\n        __m128i cmp = _mm_cmpge_epi32(freqs, keys);\n        int mask = _mm_movemask_epi8(cmp);\n\n        for (int j = 0; j < 4; ++j) {\n            if (mask & (1 << (j * 4))) {\n                int current_key = (it + j)->first;\n                if (current_key > max_key) {\n                    max_key = current_key;\n                    max_val = current_key;\n                }\n            }\n        }\n        it += 4;\n    }\n\n    // Process remaining elements\n    for (int i = 0; i < remainder; ++i) {\n        int key = it->first;\n        int count = it->second;\n        if (count >= key && key > max_key) {\n            max_key = key;\n            max_val = key;\n        }\n        ++it;\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq;\n    for (int num : lst) {\n        freq[num]++;\n    }\n\n    int max_val = -1;\n    for (const auto& pair : freq) {\n        if (pair.first == pair.second && pair.first > max_val) {\n            max_val = pair.first;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    // Check if the sum is <= w\n    int sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    int i;\n    for (i = 0; i + 4 <= q.size(); i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, v);\n    }\n    // Sum the remaining elements\n    for (; i < q.size(); ++i) {\n        sum += q[i];\n    }\n    // Horizontal sum of the SIMD vector\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n    if (sum > w) {\n        return false;\n    }\n\n    // Check if the vector is a palindrome\n    int left = 0;\n    int right = q.size() - 1;\n    while (left < right) {\n        if (q[left] != q[right]) {\n            return false;\n        }\n        left++;\n        right--;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    // Check if the sum is <= w\n    int sum = std::accumulate(q.begin(), q.end(), 0);\n    if (sum > w) return false;\n\n    // Check if the vector is a palindrome using SIMD\n    size_t n = q.size();\n    for (size_t i = 0; i < n / 2; i += 4) {\n        // Load 4 elements from the beginning and end\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - 4]);\n\n        // Reverse the right vector\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n\n        // Compare the elements\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n\n        // If any comparison fails, return false\n        if (mask != 0xFFFF) return false;\n    }\n\n    // Handle remaining elements (if any)\n    for (size_t i = (n / 2) & ~3; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    // Check if the vector is a palindrome\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t end = n / 2;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 3 < end; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < end; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum of elements using SSE\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t j = 0;\n    for (; j + 3 < n; j += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[j]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    // Sum the elements of sum_vec\n    int sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Add remaining elements\n    for (; j < n; ++j) {\n        sum += q[j];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n    \n    // Check if the vector is a palindrome\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t j = n - 1;\n    \n    // Process elements in pairs using SIMD\n    for (; i + 3 < j - 2; i += 4, j -= 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[j - 3]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n    \n    // Handle remaining elements\n    for (; i < j; ++i, --j) {\n        if (q[i] != q[j]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n    \n    if (!is_palindrome) return false;\n    \n    // Calculate sum of elements using SIMD\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t k;\n    for (k = 0; k + 3 < n; k += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[k]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n    \n    // Sum the remaining elements\n    int sum = 0;\n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n    sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    \n    for (; k < n; ++k) {\n        sum += q[k];\n    }\n    \n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    // Check if palindromic using SIMD\n    size_t n = q.size();\n    bool is_palindrome = true;\n    size_t i = 0;\n    size_t end = n / 2;\n\n    for (; i + 4 <= end; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    for (; i < end; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Calculate sum using SIMD\n    int sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t j = 0;\n    for (; j + 4 <= n; j += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[j]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    alignas(16) int sum_arr[4];\n    _mm_store_si128((__m128i*)sum_arr, sum_vec);\n    sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    for (; j < n; ++j) {\n        sum += q[j];\n    }\n\n    return is_palindrome && (sum <= w);\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        __m128i left = _mm_set1_epi32(arr[i]);\n        __m128i right = _mm_set1_epi32(arr[j]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        __m128i left = _mm_set1_epi32(arr[i]);\n        __m128i right = _mm_set1_epi32(arr[j]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        // Process 4 elements at a time using SSE\n        if (j - i >= 3) {\n            __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n            __m128i right = _mm_loadu_si128((__m128i*)&arr[j-3]);\n            right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3)); // Reverse the right elements\n\n            __m128i cmp = _mm_cmpeq_epi32(left, right);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFF) {\n                // Compare each element individually\n                for (int k = 0; k < 4; ++k) {\n                    if (arr[i + k] != arr[j - 3 + (3 - k)]) {\n                        changes++;\n                    }\n                }\n            }\n            i += 4;\n            j -= 4;\n        } else {\n            if (arr[i] != arr[j]) {\n                changes++;\n            }\n            i++;\n            j--;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        __m128i left = _mm_set1_epi32(arr[i]);\n        __m128i right = _mm_set1_epi32(arr[j]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        __m128i left = _mm_set1_epi32(arr[i]);\n        __m128i right = _mm_set1_epi32(arr[j]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0xFFFF) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0;\n    size_t sum2 = 0;\n    \n    // Calculate sum for lst1\n    for (const auto& s : lst1) {\n        sum1 += s.length();\n    }\n    \n    // Calculate sum for lst2\n    for (const auto& s : lst2) {\n        sum2 += s.length();\n    }\n    \n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0;\n    size_t sum2 = 0;\n    \n    // Process lst1\n    for (const auto& s : lst1) {\n        sum1 += s.size();\n    }\n    \n    // Process lst2\n    for (const auto& s : lst2) {\n        sum2 += s.size();\n    }\n    \n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0;\n    size_t sum2 = 0;\n    \n    // Process lst1 in chunks of 4 strings using SSE\n    size_t i = 0;\n    for (; i + 4 <= lst1.size(); i += 4) {\n        __m128i len_vec = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            len_vec = _mm_add_epi32(len_vec, _mm_set1_epi32(lst1[i + j].size()));\n        }\n        alignas(16) int len_arr[4];\n        _mm_store_si128((__m128i*)len_arr, len_vec);\n        sum1 += len_arr[0] + len_arr[1] + len_arr[2] + len_arr[3];\n    }\n    // Process remaining strings in lst1\n    for (; i < lst1.size(); ++i) {\n        sum1 += lst1[i].size();\n    }\n    \n    // Process lst2 in chunks of 4 strings using SSE\n    i = 0;\n    for (; i + 4 <= lst2.size(); i += 4) {\n        __m128i len_vec = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            len_vec = _mm_add_epi32(len_vec, _mm_set1_epi32(lst2[i + j].size()));\n        }\n        alignas(16) int len_arr[4];\n        _mm_store_si128((__m128i*)len_arr, len_vec);\n        sum2 += len_arr[0] + len_arr[1] + len_arr[2] + len_arr[3];\n    }\n    // Process remaining strings in lst2\n    for (; i < lst2.size(); ++i) {\n        sum2 += lst2[i].size();\n    }\n    \n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0;\n    size_t sum2 = 0;\n    \n    // Process lst1 in chunks of 4 strings using SSE\n    size_t i = 0;\n    for (; i + 4 <= lst1.size(); i += 4) {\n        __m128i len_vec = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            len_vec = _mm_add_epi32(len_vec, _mm_set1_epi32(lst1[i + j].size()));\n        }\n        alignas(16) int len_arr[4];\n        _mm_store_si128((__m128i*)len_arr, len_vec);\n        sum1 += len_arr[0] + len_arr[1] + len_arr[2] + len_arr[3];\n    }\n    // Process remaining strings in lst1\n    for (; i < lst1.size(); ++i) {\n        sum1 += lst1[i].size();\n    }\n    \n    // Process lst2 in chunks of 4 strings using SSE\n    i = 0;\n    for (; i + 4 <= lst2.size(); i += 4) {\n        __m128i len_vec = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            len_vec = _mm_add_epi32(len_vec, _mm_set1_epi32(lst2[i + j].size()));\n        }\n        alignas(16) int len_arr[4];\n        _mm_store_si128((__m128i*)len_arr, len_vec);\n        sum2 += len_arr[0] + len_arr[1] + len_arr[2] + len_arr[3];\n    }\n    // Process remaining strings in lst2\n    for (; i < lst2.size(); ++i) {\n        sum2 += lst2[i].size();\n    }\n    \n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0;\n    size_t sum2 = 0;\n    \n    // Calculate sum for lst1\n    for (const auto& s : lst1) {\n        sum1 += s.length();\n    }\n    \n    // Calculate sum for lst2\n    for (const auto& s : lst2) {\n        sum2 += s.length();\n    }\n    \n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 0) return false;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vres = _mm_set1_epi64x(1);\n    __m128i vone = _mm_set1_epi64x(1);\n\n    while (1) {\n        __m128i vcmp = _mm_cmpeq_epi64(vres, vx);\n        if (_mm_movemask_epi8(vcmp)) {\n            return true;\n        }\n        __m128i vmul = _mm_mul_epu32(vres, vn);\n        if (_mm_movemask_epi8(_mm_cmpgt_epi64(vmul, vx))) {\n            return false;\n        }\n        vres = vmul;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    \n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vres = _mm_set1_epi64x(1);\n    __m128i vone = _mm_set1_epi64x(1);\n    \n    while (1) {\n        vres = _mm_mul_epu32(vres, vn);\n        __m128i cmp = _mm_cmpeq_epi32(vres, vx);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) return true;\n        \n        __m128i cmp_gt = _mm_cmpgt_epi32(vres, vx);\n        mask = _mm_movemask_epi8(cmp_gt);\n        if (mask != 0) return false;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    \n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vresult = _mm_set1_epi64x(1);\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_setzero_si128();\n    \n    while (1) {\n        __m128i vcmp = _mm_cmpeq_epi64(vresult, vx);\n        if (_mm_movemask_epi8(vcmp)) {\n            return true;\n        }\n        \n        __m128i vmul = _mm_mul_epu32(vresult, vn);\n        if (_mm_movemask_epi8(_mm_cmplt_epi64(vresult, vmul)) {\n            vresult = vmul;\n        } else {\n            break;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    \n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vres = _mm_set1_epi64x(1);\n    __m128i vone = _mm_set1_epi64x(1);\n    \n    while (1) {\n        __m128i vcmp = _mm_cmpeq_epi64(vres, vx);\n        if (_mm_movemask_epi8(vcmp)) return true;\n        \n        __m128i vmul = _mm_mul_epu32(vres, vn);\n        if (_mm_movemask_epi8(_mm_cmpgt_epi64(vres, vmul))) return false;\n        \n        vres = vmul;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n    \n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vres = _mm_set1_epi64x(1);\n    __m128i vone = _mm_set1_epi64x(1);\n    \n    while (1) {\n        __m128i vcmp = _mm_cmpeq_epi64(vres, vx);\n        if (_mm_movemask_epi8(vcmp)) return true;\n        \n        __m128i vmul = _mm_mul_epu32(vres, vn);\n        if (_mm_movemask_epi8(_mm_cmplt_epi64(vres, vmul)) == 0) break;\n        \n        vres = vmul;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n    \n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(num.data() + i));\n        \n        // Convert ASCII to hex values\n        __m128i hex_digits = _mm_sub_epi8(chunk, _mm_set1_epi8('0'));\n        __m128i mask_gt9 = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('9'));\n        __m128i adjust = _mm_and_si128(mask_gt9, _mm_set1_epi8(7));\n        hex_digits = _mm_sub_epi8(hex_digits, adjust);\n        \n        // Check if digits are in valid range (0-15)\n        __m128i valid_mask = _mm_cmplt_epi8(hex_digits, _mm_set1_epi8(16));\n        \n        // Lookup prime status\n        __m128i prime_status = _mm_shuffle_epi8(prime_mask, hex_digits);\n        prime_status = _mm_and_si128(prime_status, valid_mask);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(prime_status));\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val = (c <= '9') ? (c - '0') : (c - 'A' + 10);\n        if (val < 16 && (prime_mask.m128i_u8[val] & 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set1_epi8(0x12); // Binary: 00010010 (bits represent primes 2,3,5,7,B,D)\n    const __m128i ascii_mask = _mm_set1_epi8(0x0F); // Mask to get lower nibble\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i nine = _mm_set1_epi8('9');\n    const __m128i A = _mm_set1_epi8('A');\n    const __m128i F = _mm_set1_epi8('F');\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&num[i]);\n        \n        // Convert ASCII to nibble values\n        __m128i is_digit = _mm_cmplt_epi8(chunk, _mm_set1_epi8(':'));\n        __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(chunk, _mm_set1_epi8('G')), \n                                        _mm_cmpgt_epi8(chunk, _mm_set1_epi8('@')));\n        \n        __m128i digit_part = _mm_and_si128(_mm_sub_epi8(chunk, _mm_set1_epi8('0')), \n                                          _mm_and_si128(is_digit, ascii_mask));\n        __m128i alpha_part = _mm_and_si128(_mm_sub_epi8(chunk, _mm_set1_epi8('A') + 10), \n                                          _mm_and_si128(is_upper, ascii_mask));\n        __m128i nibbles = _mm_or_si128(digit_part, alpha_part);\n        \n        // Check if nibble is prime (2,3,5,7,11,13)\n        __m128i shifted = _mm_sll_epi16(_mm_set1_epi16(1), nibbles);\n        __m128i prime_check = _mm_and_si128(shifted, prime_mask);\n        __m128i cmp = _mm_cmpgt_epi8(prime_check, zero);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val;\n        if (c >= '0' && c <= '9') {\n            val = c - '0';\n        } else if (c >= 'A' && c <= 'F') {\n            val = c - 'A' + 10;\n        } else {\n            continue;\n        }\n        \n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set1_epi8(0x12); // Binary: 00010010 (bits 1,4,5 represent primes 2,3,5,7,B,D)\n    const __m128i ascii_mask = _mm_set1_epi8(0x0F); // Mask to get lower nibble\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i nine = _mm_set1_epi8('9');\n    const __m128i A = _mm_set1_epi8('A');\n    const __m128i F = _mm_set1_epi8('F');\n\n    size_t len = num.size();\n    size_t i = 0;\n    int count = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&num[i]);\n        \n        // Convert ASCII to values 0-15\n        __m128i digits = _mm_and_si128(chunk, ascii_mask);\n        __m128i is_letter = _mm_cmpgt_epi8(chunk, nine);\n        __m128i letter_digits = _mm_sub_epi8(_mm_sub_epi8(chunk, A), _mm_set1_epi8(10 - 0xA));\n        digits = _mm_blendv_epi8(digits, letter_digits, is_letter);\n\n        // Check if digit is prime (2,3,5,7,11,13)\n        __m128i shifted = _mm_sll_epi16(_mm_set1_epi16(1), digits);\n        __m128i prime_check = _mm_and_si128(shifted, prime_mask);\n        __m128i is_prime = _mm_cmpgt_epi8(prime_check, zero);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(is_prime));\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int digit = (c <= '9') ? (c - '0') : (c - 'A' + 10);\n        if (digit == 2 || digit == 3 || digit == 5 || digit == 7 || digit == 11 || digit == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set1_epi8(0x12); // Binary: 00010010 (bits represent primes 2,3,5,7,B,D)\n    const __m128i ascii_mask = _mm_set1_epi8(0x0F); // Mask to get lower nibble\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i nine = _mm_set1_epi8('9');\n    const __m128i A = _mm_set1_epi8('A');\n    const __m128i F = _mm_set1_epi8('F');\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&num[i]);\n        \n        // Convert ASCII to nibble values\n        __m128i gt9 = _mm_cmpgt_epi8(chunk, nine);\n        __m128i leF = _mm_cmple_epi8(chunk, F);\n        __m128i is_hex_letter = _mm_and_si128(gt9, leF);\n        __m128i letter_val = _mm_sub_epi8(chunk, _mm_set1_epi8('A' - 10));\n        __m128i digit_val = _mm_sub_epi8(chunk, _mm_set1_epi8('0'));\n        __m128i val = _mm_blendv_epi8(digit_val, letter_val, is_hex_letter);\n        \n        // Create mask for primes (2,3,5,7,11,13)\n        __m128i shifted = _mm_sll_epi16(_mm_set1_epi16(1), val);\n        __m128i masked = _mm_and_si128(shifted, prime_mask);\n        __m128i cmp = _mm_cmpeq_epi8(masked, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        count += 16 - _mm_popcnt_u32(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val;\n        if (c >= 'A' && c <= 'F') {\n            val = c - 'A' + 10;\n        } else {\n            val = c - '0';\n        }\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set1_epi8(0x12); // Binary: 00010010 (bits represent primes 2,3,5,7,B,D)\n    const __m128i ascii_mask = _mm_set1_epi8(0x0F); // Mask to get lower nibble\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i nine = _mm_set1_epi8('9');\n    const __m128i A = _mm_set1_epi8('A');\n    const __m128i F = _mm_set1_epi8('F');\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&num[i]);\n        \n        // Convert ASCII to nibble values\n        __m128i gt9 = _mm_cmpgt_epi8(chunk, nine);\n        __m128i leF = _mm_cmplt_epi8(chunk, _mm_set1_epi8('G'));\n        __m128i is_letter = _mm_and_si128(gt9, leF);\n        \n        __m128i letter_val = _mm_sub_epi8(chunk, A);\n        __m128i digit_val = _mm_sub_epi8(chunk, _mm_set1_epi8('0'));\n        __m128i val = _mm_blendv_epi8(digit_val, letter_val, is_letter);\n        val = _mm_add_epi8(val, _mm_set1_epi8(10)); // Letters become 10-15\n        \n        // Check if value is prime (2,3,5,7,11,13)\n        __m128i shifted = _mm_sll_epi16(_mm_set1_epi16(1), val);\n        __m128i prime_check = _mm_and_si128(shifted, prime_mask);\n        __m128i is_prime = _mm_cmpgt_epi8(prime_check, zero);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(is_prime));\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val;\n        if (c >= 'A' && c <= 'F') {\n            val = 10 + (c - 'A');\n        } else {\n            val = c - '0';\n        }\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i shifted1 = _mm_bsrli_si128(chunk, 1);\n        __m128i shifted2 = _mm_bsrli_si128(chunk, 2);\n        \n        __m128i cmp1 = _mm_cmpeq_epi8(chunk, shifted1);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, shifted2);\n        __m128i or_result = _mm_or_si128(cmp1, cmp2);\n        \n        if (!_mm_testz_si128(or_result, or_result)) {\n            return false;\n        }\n    }\n\n    for (; i + 2 < len; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    __m128i prev1, prev2, current, cmp1, cmp2;\n\n    for (; i + 16 <= len; i += 16) {\n        current = _mm_loadu_si128((__m128i*)&s[i]);\n        prev1 = _mm_loadu_si128((__m128i*)&s[i+1]);\n        prev2 = _mm_loadu_si128((__m128i*)&s[i+2]);\n\n        cmp1 = _mm_cmpeq_epi8(current, prev1);\n        cmp2 = _mm_cmpeq_epi8(current, prev2);\n        __m128i res1 = _mm_or_si128(cmp1, cmp2);\n\n        cmp1 = _mm_cmpeq_epi8(prev1, prev2);\n        __m128i res2 = _mm_or_si128(res1, cmp1);\n\n        if (!_mm_testz_si128(res2, res2)) {\n            return false;\n        }\n    }\n\n    for (; i + 3 <= len; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&s[i]);\n        __m128i shifted1 = _mm_bsrli_si128(chunk, 1);\n        __m128i shifted2 = _mm_bsrli_si128(chunk, 2);\n        \n        __m128i cmp1 = _mm_cmpeq_epi8(chunk, shifted1);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, shifted2);\n        __m128i cmp3 = _mm_cmpeq_epi8(shifted1, shifted2);\n        \n        __m128i or_result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n        int mask = _mm_movemask_epi8(or_result);\n        \n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i + 2 < len; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    __m128i prev1, prev2, current;\n    char last1 = s[0];\n    char last2 = s[1];\n\n    for (i = 2; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        prev1 = _mm_loadu_si128((__m128i*)&s[i-1]);\n        prev2 = _mm_loadu_si128((__m128i*)&s[i-2]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(chunk, prev1);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, prev2);\n        __m128i cmp3 = _mm_cmpeq_epi8(prev1, prev2);\n\n        __m128i res = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n        if (!_mm_testz_si128(res, res)) {\n            return false;\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] == s[i-1] || s[i] == s[i-2] || s[i-1] == s[i-2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i shifted1 = _mm_alignr_epi8(chunk, chunk, 1);\n        __m128i shifted2 = _mm_alignr_epi8(chunk, chunk, 2);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(chunk, shifted1);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, shifted2);\n        __m128i cmp3 = _mm_cmpeq_epi8(shifted1, shifted2);\n\n        __m128i or_result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n        int mask = _mm_movemask_epi8(or_result);\n\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (; i + 2 < len; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i len_vec = _mm_set1_epi32(len);\n    __m128i zeros = _mm_setzero_si128();\n\n    for (int i = 3; i * i <= len; i += 8) {\n        __m128i current_div = _mm_add_epi32(divisors, _mm_set1_epi32(i - 3));\n        __m128i remainder = _mm_rem_epu32(len_vec, current_div);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zeros);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i len_vec = _mm_set1_epi32(len);\n    __m128i zeros = _mm_setzero_si128();\n\n    int max_divisor = sqrt(len) + 1;\n    for (int i = 3; i <= max_divisor; i += 8) {\n        __m128i current_divisors = _mm_add_epi32(divisors, _mm_set1_epi32(i - 3));\n        __m128i remainders = _mm_rem_epu32(len_vec, current_divisors);\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zeros);\n        if (_mm_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m128i divisors = _mm_set_epi32(0, 0, 0, 3);\n    __m128i increment = _mm_set1_epi32(4);\n    __m128i len_vec = _mm_set1_epi32(len);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 3; i * i <= len; i += 4) {\n        __m128i current_divisors = _mm_add_epi32(divisors, _mm_set_epi32(6, 4, 2, 0));\n        __m128i remainders = _mm_rem_epu32(len_vec, current_divisors);\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n        if (!_mm_test_all_zeros(cmp, cmp)) {\n            return false;\n        }\n        divisors = _mm_add_epi32(divisors, increment);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i len_vec = _mm_set1_epi32(len);\n    __m128i zeros = _mm_setzero_si128();\n\n    int max_divisor = sqrt(len) + 1;\n    int i = 3;\n    while (i <= max_divisor) {\n        __m128i current_divisors = _mm_add_epi32(divisors, _mm_set1_epi32(i - 3));\n        __m128i remainders = _mm_rem_epu32(len_vec, current_divisors);\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zeros);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i len_vec = _mm_set1_epi32(len);\n    __m128i zeros = _mm_setzero_si128();\n\n    int sqrt_len = sqrt(len) + 1;\n    for (int i = 3; i <= sqrt_len; i += 8) {\n        __m128i current = _mm_add_epi32(divisors, _mm_set1_epi32(i - 3));\n        __m128i remainders = _mm_rem_epu32(len_vec, current);\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zeros);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    while (N > 0) {\n        uint64_t chunk = N % 10000;\n        N /= 10000;\n        \n        uint64_t d0 = chunk % 10;\n        uint64_t d1 = (chunk / 10) % 10;\n        uint64_t d2 = (chunk / 100) % 10;\n        uint64_t d3 = (chunk / 1000) % 10;\n        \n        __m128i digits = _mm_set_epi32(0, d3, d2, d1);\n        sum_vec = _mm_add_epi32(sum_vec, digits);\n        sum += d0;\n    }\n    \n    alignas(16) uint32_t temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n    \n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary;\n    while (sum > 0) {\n        binary.push_back('0' + (sum & 1));\n        sum >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    while (N > 0) {\n        uint64_t chunk = N % 10000;\n        N /= 10000;\n        \n        uint64_t d0 = chunk % 10;\n        uint64_t d1 = (chunk / 10) % 10;\n        uint64_t d2 = (chunk / 100) % 10;\n        uint64_t d3 = (chunk / 1000) % 10;\n        \n        __m128i digits = _mm_set_epi32(0, 0, d3 * 0x01010101, (d2 << 24) | (d1 << 16) | (d0 << 8));\n        __m128i mask = _mm_set1_epi8(0x0F);\n        __m128i unpacked = _mm_and_si128(digits, mask);\n        \n        sum_vec = _mm_add_epi32(sum_vec, _mm_sad_epu8(unpacked, _mm_setzero_si128()));\n    }\n    \n    sum = _mm_extract_epi32(sum_vec, 0) + _mm_extract_epi32(sum_vec, 2);\n    \n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary;\n    while (sum > 0) {\n        binary.push_back((sum % 2) + '0');\n        sum /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    while (N > 0) {\n        uint64_t chunk = N % 10000;\n        N /= 10000;\n        \n        uint64_t d0 = chunk % 10;\n        uint64_t d1 = (chunk / 10) % 10;\n        uint64_t d2 = (chunk / 100) % 10;\n        uint64_t d3 = chunk / 1000;\n        \n        __m128i digits = _mm_set_epi32(0, 0, d3 * 256 + d2, d1 * 256 + d0);\n        __m128i mask = _mm_set1_epi8(0x0F);\n        __m128i unpacked = _mm_and_si128(digits, mask);\n        __m128i sum_part = _mm_sad_epu8(unpacked, _mm_setzero_si128());\n        \n        sum_vec = _mm_add_epi64(sum_vec, sum_part);\n    }\n    \n    sum = _mm_extract_epi64(sum_vec, 0) + _mm_extract_epi64(sum_vec, 1);\n    \n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary;\n    while (sum > 0) {\n        binary.push_back('0' + (sum & 1));\n        sum >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    while (N > 0) {\n        uint64_t chunk = N % 10000;\n        N /= 10000;\n        \n        __m128i digits = _mm_set_epi16(0, 0, 0, 0, (chunk / 1000) % 10, (chunk / 100) % 10, (chunk / 10) % 10, chunk % 10);\n        sum_vec = _mm_add_epi16(sum_vec, digits);\n    }\n    \n    alignas(16) uint16_t temp[8];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    \n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n    \n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary;\n    while (sum > 0) {\n        binary.push_back('0' + (sum % 2));\n        sum /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    while (N > 0) {\n        uint64_t chunk = N % 10000;\n        N /= 10000;\n        \n        __m128i digits = _mm_set_epi16(0, 0, 0, 0, (chunk / 1000) % 10, (chunk / 100) % 10, (chunk / 10) % 10, chunk % 10);\n        sum_vec = _mm_add_epi16(sum_vec, digits);\n    }\n    \n    uint16_t temp[8];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    \n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n    \n    if (sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary;\n    while (sum > 0) {\n        binary += (sum % 2) + '0';\n        sum /= 2;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(1);\n    \n    for (int i = 1; i < size; i += 4) {\n        if (i + 3 >= size) {\n            for (int j = i; j < size; j += 2) {\n                if (lst[j] % 2 == 0) {\n                    sum += lst[j];\n                }\n            }\n            break;\n        }\n        \n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i elements = _mm_i32gather_epi32(lst.data(), indices, 4);\n        \n        __m128i even_mask = _mm_and_si128(elements, mask);\n        even_mask = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n        \n        __m128i odd_indices = _mm_and_si128(indices, mask);\n        odd_indices = _mm_cmpeq_epi32(odd_indices, _mm_set1_epi32(1));\n        \n        __m128i combined_mask = _mm_and_si128(even_mask, odd_indices);\n        __m128i masked_elements = _mm_and_si128(elements, combined_mask);\n        \n        sum_vec = _mm_add_epi32(sum_vec, masked_elements);\n    }\n    \n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    int size = lst.size();\n    int i;\n\n    for (i = 1; i < size - 3; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i mask = _mm_set1_epi32(1);\n        __m128i odd_indices = _mm_and_si128(indices, mask);\n        __m128i is_odd = _mm_cmpeq_epi32(odd_indices, mask);\n\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i even_mask = _mm_set1_epi32(1);\n        __m128i even_elements = _mm_and_si128(data, even_mask);\n        __m128i is_even = _mm_cmpeq_epi32(even_elements, _mm_setzero_si128());\n\n        __m128i combined_mask = _mm_and_si128(is_odd, is_even);\n        __m128i masked_data = _mm_and_si128(data, combined_mask);\n        sum = _mm_add_epi32(sum, masked_data);\n    }\n\n    int result = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; i += 2) {\n        if (lst[i] % 2 == 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i < size; i += 4) {\n        if (i + 3 >= size) {\n            for (int j = i; j < size; j += 2) {\n                if (lst[j] % 2 == 0) {\n                    sum += lst[j];\n                }\n            }\n            break;\n        }\n\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i elements = _mm_i32gather_epi32(lst.data(), indices, 4);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(elements, mask), zero);\n        __m128i odd_indices = _mm_and_si128(_mm_set_epi32(1, 0, 1, 0), mask);\n        __m128i combined_mask = _mm_and_si128(even_mask, odd_indices);\n        __m128i masked_elements = _mm_and_si128(elements, combined_mask);\n        sum_vec = _mm_add_epi32(sum_vec, masked_elements);\n    }\n\n    sum += _mm_extract_epi32(sum_vec, 0);\n    sum += _mm_extract_epi32(sum_vec, 1);\n    sum += _mm_extract_epi32(sum_vec, 2);\n    sum += _mm_extract_epi32(sum_vec, 3);\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set_epi32(0, 1, 0, 1);\n    __m128i mask_even = _mm_set1_epi32(0xFFFFFFFE);\n\n    for (int i = 0; i < size - 3; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i odd_indices = _mm_and_si128(vec, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi32(odd_indices, mask_odd);\n        __m128i even_val = _mm_and_si128(vec, mask_even);\n        __m128i masked = _mm_and_si128(is_odd, even_val);\n        sum_vec = _mm_add_epi32(sum_vec, masked);\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (int i = (size / 4) * 4; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set_epi32(0, -1, 0, -1);\n    __m128i mask_even = _mm_set1_epi32(1);\n\n    for (int i = 0; i < size - 3; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i odd_indices = _mm_and_si128(vec, mask_odd);\n        __m128i is_even = _mm_and_si128(vec, mask_even);\n        is_even = _mm_cmpeq_epi32(is_even, _mm_setzero_si128());\n        __m128i res = _mm_and_si128(odd_indices, is_even);\n        sum_vec = _mm_add_epi32(sum_vec, res);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    sum = temp[1] + temp[3];\n\n    for (int i = (size / 4) * 4; i < size; i++) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.length(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 16;\n    const size_t len = s.length();\n    \n    __m128i shift = _mm_set1_epi8(4); // Shift each character by 4\n    \n    for (; i + block_size <= len; i += block_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i encrypted = _mm_add_epi8(data, shift);\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n    \n    // Handle remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.length(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 16;\n    const size_t len = s.length();\n    \n    __m128i shift = _mm_set1_epi8(4); // Shift each character by 4\n    \n    for (; i + block_size <= len; i += block_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i encrypted = _mm_add_epi8(data, shift);\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n    \n    // Handle remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.length(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 16;\n    const size_t len = s.length();\n    \n    __m128i shift = _mm_set1_epi8(4); // Shift each byte by 4\n    \n    for (; i + block_size <= len; i += block_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i encrypted = _mm_add_epi8(data, shift);\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n    \n    // Handle remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.length(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 16;\n    const size_t len = s.length();\n    \n    __m128i shift = _mm_set1_epi8(4); // Shift each byte by 4\n    \n    for (; i + block_size <= len; i += block_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i encrypted = _mm_add_epi8(data, shift);\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n    \n    // Handle remaining bytes\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.length(), '\\0');\n    size_t i = 0;\n    const size_t block_size = 16;\n    size_t aligned_size = s.length() - s.length() % block_size;\n\n    __m128i shift = _mm_set1_epi8(4); // Shift each byte by 4 (2*2)\n\n    for (; i < aligned_size; i += block_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i encrypted = _mm_add_epi8(data, shift);\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n\n    // Handle remaining characters\n    for (; i < s.length(); ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const int simd_width = 16;\n    size_t i = 0;\n    size_t len = message.size();\n    \n    __m128i vowel_mask = _mm_set1_epi8(0x20);\n    __m128i a_mask = _mm_set1_epi8('a');\n    __m128i e_mask = _mm_set1_epi8('e');\n    __m128i i_mask = _mm_set1_epi8('i');\n    __m128i o_mask = _mm_set1_epi8('o');\n    __m128i u_mask = _mm_set1_epi8('u');\n    __m128i A_mask = _mm_set1_epi8('A');\n    __m128i E_mask = _mm_set1_epi8('E');\n    __m128i I_mask = _mm_set1_epi8('I');\n    __m128i O_mask = _mm_set1_epi8('O');\n    __m128i U_mask = _mm_set1_epi8('U');\n    __m128i two = _mm_set1_epi8(2);\n    __m128i case_mask = _mm_set1_epi8(0x20);\n    \n    for (; i + simd_width <= len; i += simd_width) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&message[i]);\n        \n        // Check for lowercase vowels\n        __m128i lower_a = _mm_cmpeq_epi8(chunk, a_mask);\n        __m128i lower_e = _mm_cmpeq_epi8(chunk, e_mask);\n        __m128i lower_i = _mm_cmpeq_epi8(chunk, i_mask);\n        __m128i lower_o = _mm_cmpeq_epi8(chunk, o_mask);\n        __m128i lower_u = _mm_cmpeq_epi8(chunk, u_mask);\n        __m128i lower_vowels = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(\n            lower_a, lower_e), lower_i), lower_o), lower_u);\n        \n        // Check for uppercase vowels\n        __m128i upper_a = _mm_cmpeq_epi8(chunk, A_mask);\n        __m128i upper_e = _mm_cmpeq_epi8(chunk, E_mask);\n        __m128i upper_i = _mm_cmpeq_epi8(chunk, I_mask);\n        __m128i upper_o = _mm_cmpeq_epi8(chunk, O_mask);\n        __m128i upper_u = _mm_cmpeq_epi8(chunk, U_mask);\n        __m128i upper_vowels = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(\n            upper_a, upper_e), upper_i), upper_o), upper_u);\n        \n        // Combine all vowels\n        __m128i vowels = _mm_or_si128(lower_vowels, upper_vowels);\n        \n        // Add 2 to vowels\n        __m128i shifted_vowels = _mm_add_epi8(chunk, two);\n        \n        // Replace vowels with shifted ones\n        __m128i encoded = _mm_blendv_epi8(chunk, shifted_vowels, vowels);\n        \n        // Toggle case\n        __m128i toggled = _mm_xor_si128(encoded, case_mask);\n        \n        _mm_storeu_si128((__m128i*)&result[i], toggled);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        bool is_vowel = (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                         c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U');\n        if (is_vowel) {\n            c += 2;\n        }\n        c ^= 0x20; // Toggle case\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const __m128i vowel_mask = _mm_set1_epi8(0x20);\n    const __m128i vowels_lower = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i vowels_upper = _mm_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i two_ahead = _mm_set1_epi8(2);\n    const __m128i case_mask = _mm_set1_epi8(0x20);\n    const __m128i a_lower = _mm_set1_epi8('a');\n    const __m128i z_lower = _mm_set1_epi8('z');\n    const __m128i a_upper = _mm_set1_epi8('A');\n    const __m128i z_upper = _mm_set1_epi8('Z');\n    \n    size_t i = 0;\n    for (; i + 16 <= message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(message.data() + i));\n        \n        // Check for lowercase letters\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)), \n                                        _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1)));\n        // Check for uppercase letters\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)), \n                                        _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1)));\n        \n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(0x20));\n        __m128i case_swapped = _mm_or_si128(_mm_and_si128(is_lower, swapped_case), \n                                           _mm_and_si128(is_upper, swapped_case));\n        __m128i unchanged = _mm_andnot_si128(_mm_or_si128(is_lower, is_upper), chunk);\n        __m128i processed = _mm_or_si128(case_swapped, unchanged);\n        \n        // Process vowels\n        __m128i lower_vowel = _mm_cmpeq_epi8(_mm_shuffle_epi8(vowels_lower, chunk), chunk);\n        __m128i upper_vowel = _mm_cmpeq_epi8(_mm_shuffle_epi8(vowels_upper, chunk), chunk);\n        __m128i is_vowel = _mm_or_si128(lower_vowel, upper_vowel);\n        \n        __m128i vowel_plus_two = _mm_add_epi8(processed, two_ahead);\n        __m128i vowel_processed = _mm_and_si128(is_vowel, vowel_plus_two);\n        __m128i non_vowel = _mm_andnot_si128(is_vowel, processed);\n        __m128i final = _mm_or_si128(vowel_processed, non_vowel);\n        \n        _mm_storeu_si128((__m128i*)(result.data() + i), final);\n    }\n    \n    // Process remaining characters\n    for (; i < message.size(); ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20; // Swap to uppercase\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20; // Swap to lowercase\n        }\n        \n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U' ||\n            c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            c += 2;\n        }\n        \n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const __m128i mask_lower = _mm_set1_epi8(0x20);\n    const __m128i mask_upper = _mm_set1_epi8(0xDF);\n    const __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0);\n    const __m128i two_ahead = _mm_set1_epi8(2);\n    \n    size_t i = 0;\n    for (; i + 16 <= message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(message.data() + i));\n        \n        // Swap case\n        __m128i swapped = _mm_xor_si128(chunk, mask_lower);\n        \n        // Identify vowels\n        __m128i vowel_mask = _mm_setzero_si128();\n        for (int j = 0; j < 10; j++) {\n            __m128i vowel = _mm_set1_epi8(_mm_extract_epi8(vowels, j));\n            __m128i cmp = _mm_cmpeq_epi8(swapped, vowel);\n            vowel_mask = _mm_or_si128(vowel_mask, cmp);\n        }\n        \n        // Replace vowels\n        __m128i replaced = _mm_add_epi8(swapped, _mm_and_si128(vowel_mask, two_ahead));\n        \n        // Store result\n        _mm_storeu_si128((__m128i*)(result.data() + i), replaced);\n    }\n    \n    // Process remaining characters\n    for (; i < message.size(); i++) {\n        char c = message[i];\n        // Swap case\n        c ^= 0x20;\n        \n        // Check if vowel\n        bool is_vowel = (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                         c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U');\n        \n        // Replace if vowel\n        if (is_vowel) {\n            c += 2;\n        }\n        \n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const int simd_width = 16;\n    size_t i = 0;\n    size_t len = message.size();\n    \n    __m128i vowel_mask = _mm_set1_epi8(0x20);\n    __m128i a_mask = _mm_set1_epi8('a');\n    __m128i e_mask = _mm_set1_epi8('e');\n    __m128i i_mask = _mm_set1_epi8('i');\n    __m128i o_mask = _mm_set1_epi8('o');\n    __m128i u_mask = _mm_set1_epi8('u');\n    __m128i two = _mm_set1_epi8(2);\n    __m128i case_mask = _mm_set1_epi8(0x20);\n    \n    for (; i + simd_width <= len; i += simd_width) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&message[i]);\n        \n        // Check for vowels (case-insensitive)\n        __m128i lower_chunk = _mm_or_si128(chunk, vowel_mask);\n        __m128i is_a = _mm_cmpeq_epi8(lower_chunk, a_mask);\n        __m128i is_e = _mm_cmpeq_epi8(lower_chunk, e_mask);\n        __m128i is_i = _mm_cmpeq_epi8(lower_chunk, i_mask);\n        __m128i is_o = _mm_cmpeq_epi8(lower_chunk, o_mask);\n        __m128i is_u = _mm_cmpeq_epi8(lower_chunk, u_mask);\n        __m128i is_vowel = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(is_a, is_e), is_i), _mm_or_si128(is_o, is_u));\n        \n        // Replace vowels with +2\n        __m128i vowel_plus2 = _mm_add_epi8(chunk, two);\n        chunk = _mm_blendv_epi8(chunk, vowel_plus2, is_vowel);\n        \n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, case_mask);\n        chunk = _mm_blendv_epi8(chunk, swapped_case, _mm_cmpgt_epi8(chunk, _mm_set1_epi8(0x40)));\n        \n        _mm_storeu_si128((__m128i*)&result[i], chunk);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        bool is_vowel = false;\n        switch (c | 0x20) {\n            case 'a': case 'e': case 'i': case 'o': case 'u':\n                is_vowel = true;\n                c += 2;\n                break;\n        }\n        c ^= 0x20;\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    result.resize(message.size());\n    \n    const __m128i vowel_mask = _mm_set1_epi8(0x20);\n    const __m128i vowels_lower = _mm_set_epi8(0, 0, 0, 0, 0, 'u', 'o', 'i', 'e', 'a', 0, 0, 0, 0, 0, 0);\n    const __m128i vowels_upper = _mm_set_epi8(0, 0, 0, 0, 0, 'U', 'O', 'I', 'E', 'A', 0, 0, 0, 0, 0, 0);\n    const __m128i two_ahead = _mm_set1_epi8(2);\n    const __m128i a_lower = _mm_set1_epi8('a');\n    const __m128i z_lower = _mm_set1_epi8('z');\n    const __m128i a_upper = _mm_set1_epi8('A');\n    const __m128i z_upper = _mm_set1_epi8('Z');\n    const __m128i case_mask = _mm_set1_epi8(0x20);\n    \n    size_t i = 0;\n    for (; i + 16 <= message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(message.data() + i));\n        \n        // Check for lowercase letters\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)),\n                 _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1)));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)),\n                 _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1)));\n        \n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, case_mask);\n        __m128i final_chunk = _mm_or_si128(\n            _mm_and_si128(is_lower, swapped_case),\n            _mm_andnot_si128(is_lower, chunk)\n        );\n        final_chunk = _mm_or_si128(\n            _mm_and_si128(is_upper, swapped_case),\n            _mm_andnot_si128(is_upper, final_chunk)\n        );\n        \n        // Check for vowels (lowercase)\n        __m128i vowel_check_lower = _mm_or_si128(\n            _mm_cmpeq_epi8(chunk, _mm_set1_epi8('a')),\n            _mm_or_si128(\n                _mm_cmpeq_epi8(chunk, _mm_set1_epi8('e')),\n                _mm_or_si128(\n                    _mm_cmpeq_epi8(chunk, _mm_set1_epi8('i')),\n                    _mm_or_si128(\n                        _mm_cmpeq_epi8(chunk, _mm_set1_epi8('o')),\n                        _mm_cmpeq_epi8(chunk, _mm_set1_epi8('u'))\n                    )\n                )\n            )\n        );\n        \n        // Check for vowels (uppercase)\n        __m128i vowel_check_upper = _mm_or_si128(\n            _mm_cmpeq_epi8(chunk, _mm_set1_epi8('A')),\n            _mm_or_si128(\n                _mm_cmpeq_epi8(chunk, _mm_set1_epi8('E')),\n                _mm_or_si128(\n                    _mm_cmpeq_epi8(chunk, _mm_set1_epi8('I')),\n                    _mm_or_si128(\n                        _mm_cmpeq_epi8(chunk, _mm_set1_epi8('O')),\n                        _mm_cmpeq_epi8(chunk, _mm_set1_epi8('U'))\n                    )\n                )\n            )\n        );\n        \n        __m128i vowel_check = _mm_or_si128(vowel_check_lower, vowel_check_upper);\n        \n        // Add 2 to vowels\n        __m128i vowel_plus_two = _mm_add_epi8(chunk, two_ahead);\n        \n        // Handle wrap-around for lowercase vowels\n        __m128i wrap_lower = _mm_cmpgt_epi8(vowel_plus_two, z_lower);\n        __m128i wrapped_lower = _mm_sub_epi8(vowel_plus_two, _mm_set1_epi8(26));\n        vowel_plus_two = _mm_or_si128(\n            _mm_and_si128(wrap_lower, wrapped_lower),\n            _mm_andnot_si128(wrap_lower, vowel_plus_two)\n        );\n        \n        // Handle wrap-around for uppercase vowels\n        __m128i wrap_upper = _mm_cmpgt_epi8(vowel_plus_two, z_upper);\n        __m128i wrapped_upper = _mm_sub_epi8(vowel_plus_two, _mm_set1_epi8(26));\n        vowel_plus_two = _mm_or_si128(\n            _mm_and_si128(wrap_upper, wrapped_upper),\n            _mm_andnot_si128(wrap_upper, vowel_plus_two)\n        );\n        \n        // Apply vowel transformation\n        final_chunk = _mm_or_si128(\n            _mm_and_si128(vowel_check, vowel_plus_two),\n            _mm_andnot_si128(vowel_check, final_chunk)\n        );\n        \n        _mm_storeu_si128((__m128i*)(result.data() + i), final_chunk);\n    }\n    \n    // Handle remaining characters\n    for (; i < message.size(); ++i) {\n        char c = message[i];\n        bool is_lower = (c >= 'a' && c <= 'z');\n        bool is_upper = (c >= 'A' && c <= 'Z');\n        \n        if (is_lower || is_upper) {\n            c ^= 0x20; // Swap case\n        }\n        \n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n            if ((is_lower && c > 'z') || (is_upper && c > 'Z')) {\n                c -= 26;\n            }\n        }\n        \n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((const __m128i*)(key.data() + i));\n            \n            // Check if all are lowercase\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_sub_epi8(lower_a, _mm_set1_epi8(1))),\n                _mm_cmpgt_epi8(lower_z, chars)\n            );\n            int mask_lower = _mm_movemask_epi8(is_lower);\n            if (mask_lower != 0xFFFF) {\n                all_lower = false;\n            }\n\n            // Check if all are uppercase\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_sub_epi8(upper_a, _mm_set1_epi8(1))),\n                _mm_cmpgt_epi8(upper_z, chars)\n            );\n            int mask_upper = _mm_movemask_epi8(is_upper);\n            if (mask_upper != 0xFFFF) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (all_lower && !(c >= 'a' && c <= 'z')) {\n                all_lower = false;\n            }\n            if (all_upper && !(c >= 'A' && c <= 'Z')) {\n                all_upper = false;\n            }\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    int all_lower = 1;\n    int all_upper = 1;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n        size_t i = 0;\n\n        __m128i lower_mask = _mm_set1_epi8(0x20);\n        __m128i upper_bound = _mm_set1_epi8('Z');\n        __m128i lower_bound = _mm_set1_epi8('A');\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((const __m128i*)&key[i]);\n            __m128i upper_chars = _mm_and_si128(chars, _mm_set1_epi8(0x5F));\n\n            // Check if all are letters\n            __m128i is_letter = _mm_and_si128(\n                _mm_cmpgt_epi8(upper_chars, _mm_set1_epi8('A' - 1)),\n                _mm_cmplt_epi8(upper_chars, _mm_set1_epi8('Z' + 1))\n            );\n            int mask = _mm_movemask_epi8(is_letter);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n\n            // Check case\n            __m128i is_lower = _mm_cmpeq_epi8(_mm_and_si128(chars, lower_mask), lower_mask);\n            __m128i is_upper = _mm_cmpeq_epi8(_mm_and_si128(chars, lower_mask), _mm_setzero_si128());\n\n            int lower_mask_result = _mm_movemask_epi8(is_lower);\n            int upper_mask_result = _mm_movemask_epi8(is_upper);\n\n            all_lower &= (lower_mask_result == 0xFFFF);\n            all_upper &= (upper_mask_result == 0xFFFF);\n        }\n\n        // Handle remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            all_lower &= islower(c);\n            all_upper &= isupper(c);\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n        size_t i = 0;\n\n        __m128i lower_mask = _mm_set1_epi8(0x20);\n        __m128i upper_bound = _mm_set1_epi8('Z');\n        __m128i lower_bound = _mm_set1_epi8('A');\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((const __m128i*)(key.data() + i));\n            __m128i upper_chars = _mm_and_si128(chars, _mm_set1_epi8(0x5F));\n\n            // Check if characters are letters\n            __m128i is_letter = _mm_and_si128(\n                _mm_cmpgt_epi8(upper_chars, _mm_set1_epi8('A' - 1)),\n                _mm_cmplt_epi8(upper_chars, _mm_set1_epi8('Z' + 1))\n            );\n\n            // Check case\n            __m128i is_lower = _mm_cmpeq_epi8(_mm_and_si128(chars, lower_mask), lower_mask);\n            __m128i is_upper = _mm_cmpeq_epi8(_mm_and_si128(chars, lower_mask), _mm_setzero_si128());\n\n            // Combine results\n            __m128i valid_lower = _mm_and_si128(is_letter, is_lower);\n            __m128i valid_upper = _mm_and_si128(is_letter, is_upper);\n\n            if (!_mm_test_all_ones(_mm_or_si128(valid_lower, valid_upper))) {\n                return false;\n            }\n\n            all_lower &= _mm_test_all_ones(valid_lower);\n            all_upper &= _mm_test_all_ones(valid_upper);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            bool is_lower = islower(c);\n            bool is_upper = isupper(c);\n            if (!is_lower && !is_upper) {\n                return false;\n            }\n            all_lower &= is_lower;\n            all_upper &= is_upper;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_bound = _mm_set1_epi8('Z');\n    __m128i lower_bound = _mm_set1_epi8('A');\n    __m128i zero = _mm_setzero_si128();\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n        size_t i = 0;\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((const __m128i*)(key.data() + i));\n            \n            // Check if all are letters\n            __m128i is_letter = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_set1_epi8('A' - 1)),\n                _mm_cmplt_epi8(chars, _mm_set1_epi8('z' + 1))\n            );\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(is_letter, zero)) != 0) {\n                return false;\n            }\n\n            // Check case\n            __m128i upper_chars = _mm_and_si128(chars, _mm_set1_epi8(~0x20));\n            __m128i is_upper = _mm_cmpeq_epi8(upper_chars, chars);\n            __m128i is_lower = _mm_cmpeq_epi8(_mm_or_si128(chars, lower_mask), chars);\n\n            int upper_mask = _mm_movemask_epi8(is_upper);\n            int lower_mask_result = _mm_movemask_epi8(is_lower);\n\n            if (upper_mask != 0xFFFF) {\n                all_upper = false;\n            }\n            if (lower_mask_result != 0xFFFF) {\n                all_lower = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) {\n                return false;\n            }\n            if (isupper(c)) {\n                all_lower = false;\n            } else {\n                all_upper = false;\n            }\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n\n    bool has_lower = false;\n    bool has_upper = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n        size_t i = 0;\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((const __m128i*)(key.data() + i));\n            \n            __m128i upper_or = _mm_or_si128(chars, lower_mask);\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_set1_epi8('A' - 1)),\n                _mm_cmplt_epi8(chars, _mm_set1_epi8('Z' + 1))\n            );\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_set1_epi8('a' - 1)),\n                _mm_cmplt_epi8(chars, _mm_set1_epi8('z' + 1))\n            );\n            __m128i is_alpha = _mm_or_si128(is_upper, is_lower);\n\n            int mask_upper = _mm_movemask_epi8(is_upper);\n            int mask_lower = _mm_movemask_epi8(is_lower);\n            int mask_alpha = _mm_movemask_epi8(is_alpha);\n\n            if (mask_alpha != 0xFFFF) {\n                return false;\n            }\n\n            if (mask_upper) has_upper = true;\n            if (mask_lower) has_lower = true;\n            if (has_upper && has_lower) return false;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!isalpha(c)) return false;\n            if (islower(c)) has_lower = true;\n            if (isupper(c)) has_upper = true;\n            if (has_upper && has_lower) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // SIMD check for divisibility\n        __m128i vi = _mm_set1_epi32(i);\n        int j = 0;\n        for (; j + 4 <= primes.size() && primes[j] <= sqrt_i; j += 4) {\n            __m128i vprimes = _mm_loadu_si128((__m128i*)&primes[j]);\n            __m128i remainder = _mm_rem_epu32(vi, vprimes);\n            __m128i cmp = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n            if (_mm_movemask_epi8(cmp) != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        // Handle remaining primes\n        for (; j < primes.size() && primes[j] <= sqrt_i && is_prime; ++j) {\n            if (i % primes[j] == 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // Check divisibility by existing primes using SIMD\n        for (size_t j = 0; j < primes.size(); j += 4) {\n            __m128i prime_vec = _mm_loadu_si128((__m128i*)&primes[j]);\n            __m128i i_vec = _mm_set1_epi32(i);\n            __m128i rem = _mm_rem_epu32(i_vec, prime_vec);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(rem, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n\n            if (primes[j] > sqrt_i) break;\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        __m128i vi = _mm_set1_epi32(i);\n        int j = 0;\n        for (; j < primes.size() && primes[j] <= sqrt_i; j += 4) {\n            __m128i vprimes = _mm_loadu_si128((__m128i*)&primes[j]);\n            __m128i remainder = _mm_rem_epu32(vi, vprimes);\n            __m128i cmp = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        if (is_prime) {\n            for (; j < primes.size() && primes[j] <= sqrt_i; ++j) {\n                if (i % primes[j] == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        // SIMD check for divisors\n        __m128i divisors = _mm_set_epi32(3, 5, 7, 11);\n        __m128i current = _mm_set1_epi32(i);\n        __m128i remainders = _mm_rem_epu32(current, divisors);\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            is_prime = false;\n        } else {\n            // Scalar check for remaining divisors\n            for (int j = 13; j <= sqrt_i; j += 2) {\n                if (i % j == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    primes.push_back(2);\n    if (n == 2) return primes;\n\n    for (int i = 3; i < n; i += 2) {\n        bool is_prime = true;\n        int sqrt_i = sqrt(i) + 1;\n\n        __m128i vi = _mm_set1_epi32(i);\n        int j = 0;\n        for (; j < primes.size() && primes[j] <= sqrt_i; j += 4) {\n            __m128i vprimes = _mm_loadu_si128((__m128i*)&primes[j]);\n            __m128i remainder = _mm_rem_epu32(vi, vprimes);\n            __m128i cmp = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                is_prime = false;\n                break;\n            }\n        }\n\n        if (is_prime) {\n            for (; j < primes.size() && primes[j] <= sqrt_i; ++j) {\n                if (i % primes[j] == 0) {\n                    is_prime = false;\n                    break;\n                }\n            }\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i vowels = _mm_set1_epi8('A' | ('E' << 8) | ('I' << 16) | ('O' << 24) | ('U' << 32));\n    __m128i mask = _mm_set1_epi8(0xDF); // Mask to convert to uppercase\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&s[i]);\n        __m128i upper_chunk = _mm_and_si128(chunk, mask);\n\n        __m128i cmpA = _mm_cmpeq_epi8(upper_chunk, _mm_set1_epi8('A'));\n        __m128i cmpE = _mm_cmpeq_epi8(upper_chunk, _mm_set1_epi8('E'));\n        __m128i cmpI = _mm_cmpeq_epi8(upper_chunk, _mm_set1_epi8('I'));\n        __m128i cmpO = _mm_cmpeq_epi8(upper_chunk, _mm_set1_epi8('O'));\n        __m128i cmpU = _mm_cmpeq_epi8(upper_chunk, _mm_set1_epi8('U'));\n\n        __m128i vowel_mask = _mm_or_si128(_mm_or_si128(cmpA, cmpE), \n                                         _mm_or_si128(_mm_or_si128(cmpI, cmpO), cmpU));\n\n        __m128i even_mask = _mm_set_epi8(0, -1, 0, -1, 0, -1, 0, -1, \n                                        0, -1, 0, -1, 0, -1, 0, -1);\n        __m128i result = _mm_and_si128(vowel_mask, even_mask);\n\n        count += _mm_popcnt_u32(_mm_movemask_epi8(result));\n    }\n\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i] & 0xDF;\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i vowels = _mm_set1_epi8('A' | ('E' << 8) | ('I' << 16) | ('O' << 24) | ('U' << 32));\n    __m128i mask_even = _mm_set1_epi16(0x00FF);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&s[i]);\n        __m128i even_chars = _mm_and_si128(chunk, mask_even);\n        \n        __m128i cmpA = _mm_cmpeq_epi8(even_chars, _mm_set1_epi8('A'));\n        __m128i cmpE = _mm_cmpeq_epi8(even_chars, _mm_set1_epi8('E'));\n        __m128i cmpI = _mm_cmpeq_epi8(even_chars, _mm_set1_epi8('I'));\n        __m128i cmpO = _mm_cmpeq_epi8(even_chars, _mm_set1_epi8('O'));\n        __m128i cmpU = _mm_cmpeq_epi8(even_chars, _mm_set1_epi8('U'));\n        \n        __m128i result = _mm_or_si128(\n            _mm_or_si128(cmpA, cmpE),\n            _mm_or_si128(_mm_or_si128(cmpI, cmpO), cmpU)\n        );\n        \n        int mask = _mm_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Mask for even indices (0, 2, 4, ...)\n    const __m128i even_mask = _mm_set_epi8(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n\n    // Uppercase vowels: A, E, I, O, U\n    const __m128i vowels = _mm_set1_epi8('A');\n    const __m128i vowel_e = _mm_set1_epi8('E');\n    const __m128i vowel_i = _mm_set1_epi8('I');\n    const __m128i vowel_o = _mm_set1_epi8('O');\n    const __m128i vowel_u = _mm_set1_epi8('U');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        \n        // Check even indices\n        __m128i indices = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n        __m128i even = _mm_cmpeq_epi8(_mm_and_si128(indices, _mm_set1_epi8(1)), _mm_setzero_si128());\n        \n        // Check uppercase vowels\n        __m128i is_a = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i is_e = _mm_cmpeq_epi8(chunk, vowel_e);\n        __m128i is_i = _mm_cmpeq_epi8(chunk, vowel_i);\n        __m128i is_o = _mm_cmpeq_epi8(chunk, vowel_o);\n        __m128i is_u = _mm_cmpeq_epi8(chunk, vowel_u);\n        \n        __m128i vowel = _mm_or_si128(_mm_or_si128(_mm_or_si128(is_a, is_e), _mm_or_si128(is_i, is_o)), is_u);\n        \n        // Combine conditions (even index AND vowel)\n        __m128i result = _mm_and_si128(even, vowel);\n        \n        // Count matches\n        count += _mm_popcnt_u32(_mm_movemask_epi8(result));\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i vowel_mask = _mm_set1_epi8(0x20);\n    __m128i upper_A = _mm_set1_epi8('A');\n    __m128i upper_E = _mm_set1_epi8('E');\n    __m128i upper_I = _mm_set1_epi8('I');\n    __m128i upper_O = _mm_set1_epi8('O');\n    __m128i upper_U = _mm_set1_epi8('U');\n    __m128i even_mask = _mm_set_epi8(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)&s[i]);\n        __m128i upper_chunk = _mm_and_si128(chunk, vowel_mask);\n        \n        __m128i is_A = _mm_cmpeq_epi8(upper_chunk, upper_A);\n        __m128i is_E = _mm_cmpeq_epi8(upper_chunk, upper_E);\n        __m128i is_I = _mm_cmpeq_epi8(upper_chunk, upper_I);\n        __m128i is_O = _mm_cmpeq_epi8(upper_chunk, upper_O);\n        __m128i is_U = _mm_cmpeq_epi8(upper_chunk, upper_U);\n        \n        __m128i vowel = _mm_or_si128(_mm_or_si128(_mm_or_si128(is_A, is_E), _mm_or_si128(is_I, is_O)), is_U);\n        __m128i even_pos = _mm_and_si128(vowel, even_mask);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(even_pos));\n    }\n\n    for (; i < len; i += 2) {\n        if (i >= len) break;\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i vowels = _mm_set1_epi8(0);\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('A'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i even_mask = _mm_set_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m128i even_chars = _mm_and_si128(chunk, even_mask);\n        \n        __m128i cmp = _mm_cmpeq_epi8(_mm_and_si128(even_chars, _mm_set1_epi8(0xDF)), vowels);\n        int mask = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; i += 2) {\n        if (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int remainder = n % 2;\n    __m128i increment = _mm_set1_epi32(2);\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n    __m128i base = _mm_set1_epi32(n);\n    __m128i mask = _mm_set1_epi32(remainder);\n    __m128i one = _mm_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i current = _mm_add_epi32(base, _mm_mullo_epi32(indices, increment));\n        __m128i adjusted = _mm_sub_epi32(current, _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(current, one), mask), one));\n        \n        _mm_storeu_si128((__m128i*)&result[i], adjusted);\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for (int i = (n / 4) * 4; i < n; ++i) {\n        result[i] = n + 2 * i;\n        if ((result[i] % 2) != remainder) {\n            result[i] -= 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) {\n        return result;\n    }\n\n    int remainder = n % 2;\n    __m128i increment = _mm_set1_epi32(2);\n    __m128i current = _mm_setr_epi32(n, n + (remainder ? 1 : 2), n + (remainder ? 3 : 4), n + (remainder ? 5 : 6));\n    __m128i mask = _mm_set1_epi32(remainder ? 1 : 0);\n    __m128i adjust = _mm_set1_epi32(remainder ? 0 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128((__m128i*)&result[i], current);\n        current = _mm_add_epi32(current, increment);\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for (int i = (n / 4) * 4; i < n; ++i) {\n        result[i] = n + (i * 2) + (remainder ? 0 : 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int* data = result.data();\n\n    __m128i increment = _mm_set1_epi32(2);\n    __m128i current = _mm_setr_epi32(n, n + (n % 2 == 0 ? 2 : 1), \n                                    n + (n % 2 == 0 ? 4 : 2), \n                                    n + (n % 2 == 0 ? 6 : 3));\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i parity = _mm_and_si128(_mm_set1_epi32(n), mask);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128((__m128i*)(data + i), current);\n        current = _mm_add_epi32(current, increment);\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for (int i = (n / 4) * 4; i < n; ++i) {\n        data[i] = n + (n % 2 == 0 ? 2 * i : 2 * i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int size = n;\n    int i = 0;\n\n    // Process elements in blocks of 4 for SIMD\n    __m128i increment = _mm_set1_epi32(2);\n    __m128i current = _mm_set_epi32(n + 6, n + 4, n + 2, n);\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i parity = _mm_and_si128(_mm_set1_epi32(n), mask);\n\n    for (; i + 4 <= size; i += 4) {\n        _mm_storeu_si128((__m128i*)&result[i], current);\n        current = _mm_add_epi32(current, increment);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        result[i] = n + 2 * i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    if (n <= 0) {\n        return result;\n    }\n\n    result.resize(n);\n    int* data = result.data();\n\n    __m128i increment = _mm_set1_epi32(2);\n    __m128i current = _mm_setr_epi32(n, n + (n % 2 == 0 ? 2 : 1), \n             n + 2 * (n % 2 == 0 ? 2 : 1), \n             n + 3 * (n % 2 == 0 ? 2 : 1));\n    __m128i mask = _mm_set1_epi32(n % 2);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128((__m128i*)(data + i), current);\n        current = _mm_add_epi32(current, increment);\n        current = _mm_add_epi32(current, _mm_and_si128(mask, increment));\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for (int i = (n / 4) * 4; i < n; ++i) {\n        data[i] = n + i * 2 - (n % 2 == 0 ? 0 : 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n    __m128i v_sum = _mm_add_epi64(v_n, v_m);\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_avg = _mm_srli_epi64(_mm_add_epi64(v_sum, v_one), 1);\n\n    int64_t avg_arr[2];\n    _mm_storeu_si128((__m128i*)avg_arr, v_avg);\n    int64_t avg = avg_arr[0];\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back((avg & 1) ? '1' : '0');\n        avg >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n    __m128i v_sum = _mm_add_epi64(v_n, v_m);\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_avg = _mm_srli_epi64(_mm_add_epi64(v_sum, v_one), 1);\n\n    int64_t avg_arr[2];\n    _mm_storeu_si128((__m128i*)avg_arr, v_avg);\n    int64_t avg = avg_arr[0];\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back((avg & 1) ? '1' : '0');\n        avg >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n    __m128i v_sum = _mm_add_epi64(v_n, v_m);\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_avg = _mm_srli_epi64(_mm_add_epi64(v_sum, v_one), 1);\n\n    int64_t avg_arr[2];\n    _mm_storeu_si128((__m128i*)avg_arr, v_avg);\n    int64_t avg = avg_arr[0];\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back((avg & 1) ? '1' : '0');\n        avg >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n    __m128i vec_m = _mm_set1_epi64x(m);\n    __m128i sum = _mm_add_epi64(vec_n, vec_m);\n    __m128i one = _mm_set1_epi64x(1);\n    sum = _mm_add_epi64(sum, one);\n    __m128i avg = _mm_srli_epi64(sum, 1);\n    \n    int64_t result[2];\n    _mm_storeu_si128((__m128i*)result, avg);\n    int64_t rounded_avg = result[0];\n\n    if (rounded_avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (rounded_avg > 0) {\n        binary.push_back((rounded_avg & 1) ? '1' : '0');\n        rounded_avg >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n    __m128i v_sum = _mm_add_epi64(v_n, v_m);\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_avg = _mm_srli_epi64(_mm_add_epi64(v_sum, v_one), 1);\n\n    int64_t avg_arr[2];\n    _mm_storeu_si128((__m128i*)avg_arr, v_avg);\n    int64_t avg = avg_arr[0];\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg > 0) {\n        binary.push_back((avg & 1) ? '1' : '0');\n        avg >>= 1;\n    }\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_mask = _mm_set1_epi32(10000);\n    \n    for (int i = 0; i < n; i += 4) {\n        int end = std::min(i + 4, n);\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i is_even = _mm_and_si128(indices, _mm_set1_epi32(1));\n        is_even = _mm_cmpeq_epi32(is_even, _mm_setzero_si128());\n        \n        // Calculate factorial for even indices\n        __m128i fact = _mm_set1_epi32(1);\n        __m128i counter = _mm_set1_epi32(1);\n        __m128i fact_mask = _mm_and_si128(is_even, _mm_set1_epi32(0xFFFFFFFF));\n        \n        for (int j = 1; j <= i+3; ++j) {\n            __m128i j_vec = _mm_set1_epi32(j);\n            __m128i cmp = _mm_cmpgt_epi32(j_vec, indices);\n            __m128i update_mask = _mm_andnot_si128(cmp, fact_mask);\n            \n            fact = _mm_mullo_epi32(fact, _mm_blendv_epi8(_mm_set1_epi32(1), j_vec, update_mask));\n            fact = _mm_and_si128(fact, mod_mask);\n        }\n        \n        // Calculate sum for odd indices\n        __m128i sum = _mm_set1_epi32(0);\n        __m128i sum_mask = _mm_andnot_si128(is_even, _mm_set1_epi32(0xFFFFFFFF));\n        \n        for (int j = 1; j <= i+3; ++j) {\n            __m128i j_vec = _mm_set1_epi32(j);\n            __m128i cmp = _mm_cmpgt_epi32(j_vec, indices);\n            __m128i update_mask = _mm_andnot_si128(cmp, sum_mask);\n            \n            sum = _mm_add_epi32(sum, _mm_blendv_epi8(_mm_set1_epi32(0), j_vec, update_mask));\n        }\n        \n        // Combine results\n        __m128i res = _mm_or_si128(_mm_and_si128(fact, is_even), _mm_and_si128(sum, _mm_andnot_si128(is_even, _mm_set1_epi32(0xFFFFFFFF))));\n        \n        // Store results\n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i*)temp, res);\n        \n        for (int j = 0; j < end - i; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m128i mod_mask = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i idx = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        __m128i mask = _mm_and_si128(idx, one);\n        __m128i is_even = _mm_cmpeq_epi32(mask, zero);\n\n        // Compute factorial for even indices\n        __m128i fact = _mm_set1_epi32(1);\n        __m128i current = _mm_set1_epi32(1);\n        __m128i end = _mm_add_epi32(idx, one);\n        \n        while (1) {\n            __m128i not_finished = _mm_cmplt_epi32(current, end);\n            if (_mm_test_all_zeros(not_finished, not_finished)) break;\n            \n            fact = _mm_mullo_epi32(fact, current);\n            fact = _mm_and_si128(fact, _mm_or_si128(not_finished, is_even));\n            current = _mm_add_epi32(current, one);\n        }\n        fact = _mm_and_si128(fact, is_even);\n\n        // Compute sum for odd indices\n        __m128i sum = _mm_mullo_epi32(idx, _mm_add_epi32(idx, one));\n        sum = _mm_srli_epi32(sum, 1);\n        sum = _mm_andnot_si128(is_even, sum);\n\n        // Combine results\n        __m128i res = _mm_or_si128(fact, sum);\n        res = _mm_and_si128(res, mod_mask);\n\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if ((i+1) % 2 == 0) {\n            // Factorial for even indices\n            int fact = 1;\n            for (int j = 1; j <= i+1; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            // Sum for odd indices\n            result[i] = ((i+1) * (i+2) / 2) % 10000;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_mask = _mm_set1_epi32(10000);\n    \n    for (int i = 0; i < n; i += 4) {\n        int end = std::min(i + 4, n);\n        __m128i indices = _mm_set_epi32(i+4 > n ? 0 : i+3, \n                                      i+3 > n ? 0 : i+2, \n                                      i+2 > n ? 0 : i+1, \n                                      i+1 > n ? 0 : i);\n        \n        __m128i ones = _mm_set1_epi32(1);\n        __m128i is_even = _mm_and_si128(indices, ones);\n        is_even = _mm_cmpeq_epi32(is_even, _mm_setzero_si128());\n        \n        // Compute factorial for even indices\n        __m128i fact = _mm_set1_epi32(1);\n        __m128i counter = _mm_set1_epi32(1);\n        __m128i fact_mask = _mm_cmplt_epi32(counter, _mm_add_epi32(indices, ones));\n        \n        while (_mm_movemask_epi8(fact_mask)) {\n            fact = _mm_mullo_epi32(fact, _mm_and_si128(fact_mask, counter));\n            counter = _mm_add_epi32(counter, ones);\n            fact_mask = _mm_cmplt_epi32(counter, _mm_add_epi32(indices, ones));\n        }\n        fact = _mm_and_si128(fact, mod_mask);\n        \n        // Compute sum for odd indices\n        __m128i sum = _mm_mullo_epi32(indices, _mm_add_epi32(indices, ones));\n        sum = _mm_srli_epi32(sum, 1);\n        sum = _mm_and_si128(sum, mod_mask);\n        \n        // Select between factorial and sum based on even/odd\n        __m128i res = _mm_or_si128(_mm_and_si128(is_even, fact), \n                                  _mm_andnot_si128(is_even, sum));\n        \n        // Store results\n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i*)temp, res);\n        for (int j = 0; j < end - i; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m128i mod_mask = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i current_fact = _mm_set1_epi32(1);\n    __m128i current_sum = _mm_set1_epi32(0);\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        __m128i mask = _mm_and_si128(indices, one);\n        __m128i is_even = _mm_cmpeq_epi32(mask, zero);\n\n        // Calculate factorial for even indices\n        __m128i fact_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n        fact_vec = _mm_add_epi32(fact_vec, one); // Adjust to 1-based index\n        current_fact = _mm_mullo_epi32(current_fact, fact_vec);\n        current_fact = _mm_and_si128(current_fact, _mm_sub_epi32(mod_mask, one)); // Mod 10000\n\n        // Calculate sum for odd indices\n        __m128i sum_vec = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        __m128i sum_result = _mm_mullo_epi32(sum_vec, _mm_add_epi32(sum_vec, one));\n        sum_result = _mm_srli_epi32(sum_result, 1); // sum = n*(n+1)/2\n        sum_result = _mm_and_si128(sum_result, _mm_sub_epi32(mod_mask, one)); // Mod 10000\n\n        // Combine results\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(is_even, current_fact),\n            _mm_andnot_si128(is_even, sum_result)\n        );\n\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        int idx = i + 1;\n        if (idx % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= idx; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            result[i] = (idx * (idx + 1) / 2) % 10000;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    __m128i mod_mask = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i current_fact = _mm_set1_epi32(1);\n    __m128i current_sum = _mm_set1_epi32(0);\n\n    int i;\n    for (i = 0; i <= n - 4; i += 4) {\n        __m128i indices = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        __m128i mask = _mm_and_si128(indices, one);\n        __m128i is_even = _mm_cmpeq_epi32(mask, zero);\n\n        // Calculate factorial for even indices\n        __m128i fact_incr = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        current_fact = _mm_mullo_epi32(current_fact, fact_incr);\n        current_fact = _mm_and_si128(current_fact, _mm_cmpgt_epi32(is_even, zero));\n        __m128i fact_res = _mm_and_si128(current_fact, is_even);\n        fact_res = _mm_and_si128(fact_res, mod_mask);\n\n        // Calculate sum for odd indices\n        __m128i sum_incr = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        current_sum = _mm_add_epi32(current_sum, sum_incr);\n        __m128i sum_res = _mm_andnot_si128(is_even, current_sum);\n\n        // Combine results\n        __m128i res = _mm_or_si128(fact_res, sum_res);\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if ((i+1) % 2 == 0) {\n            // Factorial for even indices\n            int fact = 1;\n            for (int j = 1; j <= i+1; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i] = fact;\n        } else {\n            // Sum for odd indices\n            result[i] = (i+1)*(i+2)/2;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_odd_counts = _mm_setzero_si128();\n    \n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i max_val = _mm_set1_epi32(n);\n        __m128i mask = _mm_cmplt_epi32(nums, max_val);\n        nums = _mm_min_epi32(nums, max_val);\n        \n        __m128i original = nums;\n        __m128i reversed = _mm_setzero_si128();\n        __m128i temp = nums;\n        \n        for (int j = 0; j < 4; ++j) {\n            __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(0xF));\n            reversed = _mm_or_si128(_mm_slli_epi32(reversed, 4), digit);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n        \n        __m128i is_palindrome = _mm_cmpeq_epi32(original, reversed);\n        __m128i is_odd = _mm_and_si128(_mm_and_si128(original, _mm_set1_epi32(1)), is_palindrome);\n        __m128i is_even = _mm_andnot_si128(_mm_and_si128(original, _mm_set1_epi32(1)), is_palindrome);\n        \n        __m128i odd_count = _mm_and_si128(is_odd, _mm_set1_epi32(1));\n        __m128i even_count = _mm_and_si128(is_even, _mm_set1_epi32(1));\n        \n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_unpacklo_epi32(even_count, odd_count));\n    }\n    \n    int even = _mm_extract_epi32(even_odd_counts, 0);\n    int odd = _mm_extract_epi32(even_odd_counts, 1);\n    \n    return {even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_odd_counts = _mm_setzero_si128();\n    \n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        \n        // Reverse each number\n        __m128i reversed = _mm_setzero_si128();\n        __m128i temp = nums;\n        for (int j = 0; j < 4; ++j) {\n            int num = _mm_extract_epi32(temp, j);\n            int rev = 0;\n            while (num > 0) {\n                rev = rev * 10 + num % 10;\n                num /= 10;\n            }\n            reversed = _mm_insert_epi32(reversed, rev, j);\n        }\n        \n        // Check palindrome (num == reversed)\n        __m128i is_palindrome = _mm_cmpeq_epi32(nums, reversed);\n        \n        // Check even/odd\n        __m128i parity = _mm_and_si128(nums, _mm_set1_epi32(1));\n        __m128i is_odd = _mm_cmpeq_epi32(parity, _mm_set1_epi32(1));\n        \n        // Mask palindrome results\n        __m128i pal_odd = _mm_and_si128(is_palindrome, is_odd);\n        __m128i pal_even = _mm_and_si128(is_palindrome, _mm_xor_si128(is_odd, _mm_set1_epi32(-1)));\n        \n        // Count bits (1s represent palindromes)\n        __m128i odd_count = _mm_and_si128(pal_odd, _mm_set1_epi32(1));\n        __m128i even_count = _mm_and_si128(pal_even, _mm_set1_epi32(1));\n        \n        // Horizontal sum\n        odd_count = _mm_hadd_epi32(odd_count, _mm_setzero_si128());\n        odd_count = _mm_hadd_epi32(odd_count, _mm_setzero_si128());\n        even_count = _mm_hadd_epi32(even_count, _mm_setzero_si128());\n        even_count = _mm_hadd_epi32(even_count, _mm_setzero_si128());\n        \n        // Accumulate counts\n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_unpacklo_epi32(even_count, odd_count));\n    }\n    \n    // Extract final counts\n    int even = _mm_extract_epi32(even_odd_counts, 0);\n    int odd = _mm_extract_epi32(even_odd_counts, 1);\n    \n    // Handle remaining elements (n % 4 != 0)\n    for (int i = (n / 4) * 4 + 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        while (num > 0) {\n            rev = rev * 10 + num % 10;\n            num /= 10;\n        }\n        if (i == rev) {\n            if (i % 2 == 0) even++;\n            else odd++;\n        }\n    }\n    \n    return {even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_odd_counts = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    \n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(\n            std::min(i + 3, n),\n            std::min(i + 2, n),\n            std::min(i + 1, n),\n            i\n        );\n        \n        __m128i reversed = _mm_setzero_si128();\n        __m128i temp = nums;\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(temp, _mm_setzero_si128()))) {\n            __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(0xF));\n            reversed = _mm_add_epi32(_mm_mullo_epi32(reversed, _mm_set1_epi32(10)), digit);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n        \n        __m128i is_palindrome = _mm_cmpeq_epi32(nums, reversed);\n        __m128i is_odd = _mm_and_si128(_mm_and_si128(nums, one), is_palindrome);\n        __m128i is_even = _mm_andnot_si128(_mm_and_si128(nums, one), is_palindrome);\n        \n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_and_si128(is_even, one));\n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_srli_epi32(_mm_and_si128(is_odd, one), 1));\n    }\n    \n    int even_count = _mm_extract_epi32(even_odd_counts, 0) + _mm_extract_epi32(even_odd_counts, 2);\n    int odd_count = _mm_extract_epi32(even_odd_counts, 1) + _mm_extract_epi32(even_odd_counts, 3);\n    \n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_odd_counts = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        nums = _mm_min_epi32(nums, _mm_set1_epi32(n));\n\n        __m128i original = nums;\n        __m128i reversed = zero;\n        __m128i temp = nums;\n\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(temp, zero))) {\n            __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(0xF));\n            reversed = _mm_add_epi32(_mm_mullo_epi32(reversed, _mm_set1_epi32(10)), digit);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n\n        __m128i is_palindrome = _mm_cmpeq_epi32(original, reversed);\n        __m128i is_even = _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(nums, one), zero), is_palindrome);\n        __m128i is_odd = _mm_and_si128(_mm_cmpneq_epi32(_mm_and_si128(nums, one), zero), is_palindrome);\n\n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_and_si128(is_even, one));\n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_slli_epi32(_mm_and_si128(is_odd, one), 32));\n    }\n\n    int even_count = _mm_extract_epi32(even_odd_counts, 0);\n    int odd_count = _mm_extract_epi32(even_odd_counts, 2);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_odd_counts = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i masks = _mm_cmpgt_epi32(_mm_set1_epi32(n), nums);\n        nums = _mm_and_si128(nums, masks);\n\n        // Reverse the digits for each number\n        __m128i reversed = _mm_setzero_si128();\n        __m128i temp = nums;\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(temp, zero))) {\n            __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(0xF));\n            reversed = _mm_add_epi32(_mm_mullo_epi32(reversed, _mm_set1_epi32(10)), digit);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n\n        // Check palindrome (nums == reversed)\n        __m128i is_palindrome = _mm_cmpeq_epi32(nums, reversed);\n        is_palindrome = _mm_and_si128(is_palindrome, masks);\n\n        // Check even/odd\n        __m128i parity = _mm_and_si128(nums, one);\n        __m128i is_even = _mm_cmpeq_epi32(parity, zero);\n        __m128i is_odd = _mm_cmpeq_epi32(parity, one);\n\n        // Count even palindromes\n        __m128i even_pals = _mm_and_si128(is_palindrome, is_even);\n        even_pals = _mm_and_si128(even_pals, one);\n        __m128i odd_pals = _mm_and_si128(is_palindrome, is_odd);\n        odd_pals = _mm_and_si128(odd_pals, one);\n\n        even_odd_counts = _mm_add_epi32(even_odd_counts, _mm_hadd_epi32(even_pals, odd_pals));\n    }\n\n    alignas(16) int counts[4];\n    _mm_store_si128((__m128i*)counts, even_odd_counts);\n\n    std::vector<int> result(2);\n    result[0] = counts[0] + counts[1]; // even count\n    result[1] = counts[2] + counts[3]; // odd count\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const size_t size = n.size();\n    const size_t simd_size = size - (size % 4);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_mask = _mm_set1_epi32(0xFFFF);\n\n    for (; i < simd_size; i += 4) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sign_mask = _mm_cmplt_epi32(nums, zero);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n\n        __m128i sum = _mm_setzero_si128();\n        __m128i temp = abs_nums;\n        \n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(temp, zero))) {\n            __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(0xF));\n            sum = _mm_add_epi32(sum, digit);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n\n        __m128i sign_digit = _mm_srli_epi32(abs_nums, 28);\n        sign_digit = _mm_or_si128(sign_digit, _mm_set1_epi32(0xFFFFFFF0));\n        sign_digit = _mm_and_si128(sign_digit, sign_mask);\n        sum = _mm_add_epi32(sum, sign_digit);\n\n        __m128i cmp = _mm_cmpgt_epi32(sum, zero);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n        \n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        \n        if (num < 0) {\n            int sign_digit = -(abs(num) / (int)pow(10, (int)log10(abs(num))));\n            sum += sign_digit;\n        }\n        \n        if (sum > 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t n_size = n.size();\n    const size_t simd_end = n_size - (n_size % simd_size);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_mask = _mm_set1_epi32(0xFFFF);\n\n    for (; i < simd_end; i += simd_size) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n        __m128i sign_mask = _mm_cmplt_epi32(nums, zero);\n        \n        __m128i sum = _mm_setzero_si128();\n        __m128i t = abs_nums;\n        \n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(t, zero))) {\n            __m128i digit = _mm_and_si128(t, _mm_set1_epi32(0xF));\n            sum = _mm_add_epi32(sum, digit);\n            t = _mm_srli_epi32(t, 4);\n        }\n        \n        __m128i signed_sum = _mm_sub_epi32(sum, _mm_slli_epi32(_mm_and_si128(sign_mask, _mm_set1_epi32(1)), 1);\n        signed_sum = _mm_add_epi32(signed_sum, _mm_and_si128(sign_mask, _mm_set1_epi32(1)));\n        \n        __m128i cmp = _mm_cmpgt_epi32(signed_sum, zero);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < n_size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int abs_num = abs(num);\n        \n        while (abs_num > 0) {\n            sum += abs_num % 10;\n            abs_num /= 10;\n        }\n        \n        if (num < 0) {\n            sum -= 2 * (num % 10);\n        }\n        \n        if (sum > 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t len = n.size();\n    const size_t simd_end = len - (len % simd_size);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_mask = _mm_set1_epi32(0xFFFF);\n\n    for (; i < simd_end; i += simd_size) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sign_mask = _mm_cmplt_epi32(nums, zero);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n\n        __m128i sum = _mm_setzero_si128();\n        __m128i t = abs_nums;\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(t, zero))) {\n            __m128i digit = _mm_and_si128(t, _mm_set1_epi32(0xF));\n            sum = _mm_add_epi32(sum, digit);\n            t = _mm_srli_epi32(t, 4);\n        }\n\n        __m128i sign_digit = _mm_and_si128(sign_mask, _mm_set1_epi32(1));\n        sum = _mm_sub_epi32(sum, sign_digit);\n\n        __m128i cmp = _mm_cmpgt_epi32(sum, zero);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int sign = num < 0 ? -1 : 1;\n        num = abs(num);\n        while (num > 0) {\n            sum += num % 10;\n            num /= 10;\n        }\n        if (sign < 0) {\n            sum -= 1;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t len = n.size();\n    const size_t simd_end = len - (len % simd_size);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_mask = _mm_set1_epi32(0xFFFF);\n\n    for (; i < simd_end; i += simd_size) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sign_mask = _mm_cmplt_epi32(nums, zero);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n\n        __m128i sum = _mm_setzero_si128();\n        __m128i t = abs_nums;\n        \n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(t, zero))) {\n            __m128i digit = _mm_and_si128(t, _mm_set1_epi32(0xF));\n            sum = _mm_add_epi32(sum, digit);\n            t = _mm_srli_epi32(t, 4);\n        }\n\n        __m128i signed_sum = _mm_sub_epi32(sum, _mm_slli_epi32(_mm_and_si128(sign_mask, _mm_set1_epi32(1)), 1));\n        __m128i cmp = _mm_cmpgt_epi32(signed_sum, zero);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int t = abs(num);\n        while (t > 0) {\n            sum += t % 10;\n            t /= 10;\n        }\n        if (num < 0) {\n            sum -= 2 * (num % 10);\n        }\n        if (sum > 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const size_t simd_size = 4; // SSE can process 4 ints at a time\n    const size_t simd_end = n.size() - (n.size() % simd_size);\n\n    for (; i < simd_end; i += simd_size) {\n        __m128i v = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sum = _mm_setzero_si128();\n        __m128i mask = _mm_cmpeq_epi32(v, _mm_setzero_si128());\n        mask = _mm_xor_si128(mask, _mm_set1_epi32(0xFFFFFFFF));\n\n        __m128i abs_v = _mm_abs_epi32(v);\n        __m128i digits = abs_v;\n\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(digits, _mm_setzero_si128()))) {\n            __m128i mod = _mm_sub_epi32(digits, _mm_mullo_epi32(_mm_div_epi32(digits, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n            sum = _mm_add_epi32(sum, mod);\n            digits = _mm_div_epi32(digits, _mm_set1_epi32(10));\n        }\n\n        // Handle negative numbers (add first digit's sign)\n        __m128i sign_mask = _mm_cmplt_epi32(v, _mm_setzero_si128());\n        __m128i first_digit = _mm_div_epi32(abs_v, _mm_set1_epi32(10));\n        first_digit = _mm_div_epi32(first_digit, _mm_set1_epi32(10));\n        first_digit = _mm_sub_epi32(_mm_setzero_si128(), first_digit);\n        first_digit = _mm_and_si128(first_digit, sign_mask);\n        sum = _mm_add_epi32(sum, first_digit);\n\n        __m128i cmp = _mm_cmpgt_epi32(sum, _mm_setzero_si128());\n        int mask_result = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += __builtin_popcount(mask_result);\n    }\n\n    // Process remaining elements\n    for (; i < n.size(); ++i) {\n        int num = n[i];\n        int sum = 0;\n        int abs_num = abs(num);\n        while (abs_num > 0) {\n            sum += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (num < 0) {\n            int first_digit = 0;\n            int tmp = abs(num);\n            while (tmp >= 10) {\n                tmp /= 10;\n            }\n            first_digit = -tmp;\n            sum += first_digit;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n    \n    int n = arr.size();\n    int pivot = 0;\n    \n    // Find the pivot point where order breaks\n    for (int i = 0; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            pivot = i + 1;\n            break;\n        }\n    }\n    \n    if (pivot == 0) return true; // Already sorted\n    \n    // Check if remaining elements are sorted\n    bool sorted = true;\n    int i = pivot;\n    \n    // Process remaining elements in SIMD\n    __m128i prev = _mm_set1_epi32(arr[pivot]);\n    for (; i + 4 <= n; i += 4) {\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            sorted = false;\n            break;\n        }\n        prev = _mm_set1_epi32(arr[i + 3]);\n    }\n    \n    // Handle remaining elements\n    for (; i < n && sorted; i++) {\n        if (arr[i - 1] > arr[i]) {\n            sorted = false;\n            break;\n        }\n    }\n    \n    // Check if first part is sorted and last element <= first element\n    if (sorted) {\n        __m128i first = _mm_set1_epi32(arr[0]);\n        __m128i last = _mm_set1_epi32(arr.back());\n        __m128i cmp = _mm_cmpgt_epi32(last, first);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            sorted = false;\n        }\n    }\n    \n    return sorted;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n    \n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n    \n    for (int i = 0; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            count++;\n            pos = i;\n        }\n    }\n    \n    if (count == 0) return true;\n    if (count > 1) return false;\n    \n    if (arr[n - 1] <= arr[0]) {\n        return true;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n\n    for (int i = 0; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            count++;\n            pos = i;\n        }\n    }\n\n    if (count == 0) {\n        return true;\n    }\n\n    if (count > 1) {\n        return false;\n    }\n\n    if (arr[n - 1] <= arr[0]) {\n        return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n    \n    int n = arr.size();\n    int pivot = 0;\n    \n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i-1]) {\n            pivot = i;\n            break;\n        }\n    }\n    \n    if (pivot == 0) return true;\n    \n    __m128i prev, curr;\n    bool sorted = true;\n    \n    for (int i = pivot + 1; i < n; i += 4) {\n        int remaining = n - i;\n        if (remaining >= 4) {\n            prev = _mm_loadu_si128((__m128i*)&arr[i-1]);\n            curr = _mm_loadu_si128((__m128i*)&arr[i]);\n            __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n        } else {\n            for (int j = i; j < n; ++j) {\n                if (arr[j] < arr[j-1]) {\n                    sorted = false;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n    \n    if (!sorted) return false;\n    \n    if (arr.back() > arr[0]) return false;\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n    \n    int n = arr.size();\n    int count = 0;\n    int pos = -1;\n    \n    for (int i = 0; i < n - 1; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n        __m128i cmp = _mm_cmpgt_epi32(current, next);\n        int mask = _mm_movemask_epi8(cmp);\n        \n        if (mask != 0) {\n            for (int j = 0; j < 4 && (i + j) < n - 1; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    count++;\n                    pos = i + j;\n                }\n            }\n        }\n    }\n    \n    if (count == 0) return true;\n    if (count != 1) return false;\n    \n    return (arr[n - 1] <= arr[0]);\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int count_odd1 = 0;\n    int count_even2 = 0;\n    size_t i = 0;\n    const size_t vec_size = lst1.size();\n    const size_t simd_size = vec_size - (vec_size % 4);\n\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i < simd_size; i += 4) {\n        __m128i v1 = _mm_loadu_si128((__m128i const*)&lst1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i const*)&lst2[i]);\n\n        __m128i res1 = _mm_and_si128(v1, mask);\n        __m128i res2 = _mm_and_si128(v2, mask);\n\n        __m128i cmp1 = _mm_cmpeq_epi32(res1, zero);\n        __m128i cmp2 = _mm_cmpeq_epi32(res2, zero);\n\n        int mask1 = _mm_movemask_epi8(cmp1);\n        int mask2 = _mm_movemask_epi8(cmp2);\n\n        count_odd1 += 4 - __builtin_popcount(mask1) / 4;\n        count_even2 += __builtin_popcount(mask2) / 4;\n    }\n\n    for (; i < vec_size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            count_odd1++;\n        }\n        if (lst2[i] % 2 == 0) {\n            count_even2++;\n        }\n    }\n\n    return (count_odd1 <= count_even2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int count_odd1 = 0;\n    int count_even2 = 0;\n    size_t i = 0;\n    const size_t size = lst1.size();\n    \n    __m128i mask = _mm_set1_epi32(1);\n    \n    for (; i + 3 < size; i += 4) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const*)&lst1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i const*)&lst2[i]);\n        \n        __m128i res1 = _mm_and_si128(vec1, mask);\n        __m128i res2 = _mm_and_si128(vec2, mask);\n        \n        __m128i cmp1 = _mm_cmpeq_epi32(res1, mask);\n        __m128i cmp2 = _mm_cmpeq_epi32(res2, _mm_setzero_si128());\n        \n        int mask1 = _mm_movemask_ps(_mm_castsi128_ps(cmp1));\n        int mask2 = _mm_movemask_ps(_mm_castsi128_ps(cmp2));\n        \n        count_odd1 += __builtin_popcount(mask1);\n        count_even2 += __builtin_popcount(mask2);\n    }\n    \n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            count_odd1++;\n        }\n        if (lst2[i] % 2 == 0) {\n            count_even2++;\n        }\n    }\n    \n    return (count_odd1 <= count_even2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int count_odd1 = 0;\n    int count_even2 = 0;\n    size_t i = 0;\n    const size_t vec_size = lst1.size();\n    \n    __m128i mask = _mm_set1_epi32(1);\n    \n    for (; i + 3 < vec_size; i += 4) {\n        __m128i v1 = _mm_loadu_si128((__m128i const*)&lst1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i const*)&lst2[i]);\n        \n        __m128i res1 = _mm_and_si128(v1, mask);\n        __m128i res2 = _mm_and_si128(v2, mask);\n        \n        int odd1_mask = _mm_movemask_epi8(_mm_cmpeq_epi32(res1, mask));\n        int even2_mask = _mm_movemask_epi8(_mm_cmpeq_epi32(res2, _mm_setzero_si128()));\n        \n        count_odd1 += __builtin_popcount(odd1_mask) / 4;\n        count_even2 += __builtin_popcount(even2_mask) / 4;\n    }\n    \n    for (; i < vec_size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            count_odd1++;\n        }\n        if (lst2[i] % 2 == 0) {\n            count_even2++;\n        }\n    }\n    \n    return (count_odd1 <= count_even2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    int odd_in_lst1 = 0;\n    int even_in_lst2 = 0;\n    \n    __m128i mask = _mm_set1_epi32(1);\n    \n    for (int i = 0; i < size; i += 4) {\n        int remaining = size - i;\n        if (remaining >= 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n            \n            __m128i res1 = _mm_and_si128(v1, mask);\n            __m128i res2 = _mm_and_si128(v2, mask);\n            \n            __m128i cmp1 = _mm_cmpeq_epi32(res1, mask);\n            __m128i cmp2 = _mm_cmpeq_epi32(res2, _mm_setzero_si128());\n            \n            odd_in_lst1 += _mm_popcnt_u32(_mm_movemask_epi8(cmp1)) / 4;\n            even_in_lst2 += _mm_popcnt_u32(_mm_movemask_epi8(cmp2)) / 4;\n        } else {\n            for (int j = i; j < size; ++j) {\n                if (lst1[j] % 2 != 0) {\n                    odd_in_lst1++;\n                }\n                if (lst2[j] % 2 == 0) {\n                    even_in_lst2++;\n                }\n            }\n        }\n    }\n    \n    return (odd_in_lst1 <= even_in_lst2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int count_odd1 = 0;\n    int count_even2 = 0;\n    size_t i = 0;\n    const size_t size = lst1.size();\n    \n    // Process elements in chunks of 4 using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i v1 = _mm_loadu_si128((__m128i const*)&lst1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i const*)&lst2[i]);\n        \n        // Mask for odd numbers in lst1 (lst1[i] & 1)\n        __m128i mask1 = _mm_and_si128(v1, _mm_set1_epi32(1));\n        // Count odd numbers in lst1\n        count_odd1 += _mm_popcnt_u32(_mm_movemask_ps(_mm_castsi128_ps(mask1)));\n        \n        // Mask for even numbers in lst2 (~lst2[i] & 1)\n        __m128i mask2 = _mm_andnot_si128(v2, _mm_set1_epi32(1));\n        // Count even numbers in lst2\n        count_even2 += _mm_popcnt_u32(_mm_movemask_ps(_mm_castsi128_ps(mask2)));\n    }\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0) {\n            count_odd1++;\n        }\n        if (lst2[i] % 2 == 0) {\n            count_even2++;\n        }\n    }\n    \n    return (count_odd1 <= count_even2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        int count = 0;\n        size_t i = 0;\n        const size_t len = s.length();\n        \n        // Process 16 characters at a time using SSE\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n            __m128i ascii_zero = _mm_set1_epi8('0');\n            __m128i digits = _mm_sub_epi8(chunk, ascii_zero);\n            __m128i mask = _mm_set1_epi8(1);\n            __m128i odd = _mm_and_si128(digits, mask);\n            __m128i cmp = _mm_cmpeq_epi8(odd, mask);\n            int mask_bits = _mm_movemask_epi8(cmp);\n            count += __builtin_popcount(mask_bits);\n        }\n        \n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n        \n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t count = 0;\n        size_t len = s.length();\n        size_t i = 0;\n        \n        // Process 16 characters at a time using SSE2\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n            __m128i ascii0 = _mm_set1_epi8('0');\n            __m128i digits = _mm_sub_epi8(chunk, ascii0);\n            __m128i mask = _mm_set1_epi8(1);\n            __m128i odd = _mm_and_si128(digits, mask);\n            __m128i cmp = _mm_cmpeq_epi8(odd, mask);\n            int mask_bits = _mm_movemask_epi8(cmp);\n            count += __builtin_popcount(mask_bits);\n        }\n        \n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n        \n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        int count = 0;\n        size_t i = 0;\n        const size_t len = s.length();\n        \n        // Process 16 characters at a time using SSE\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n            __m128i ascii0 = _mm_set1_epi8('0');\n            __m128i digits = _mm_sub_epi8(chunk, ascii0);\n            __m128i mask = _mm_set1_epi8(1);\n            __m128i odd = _mm_and_si128(digits, mask);\n            __m128i cmp = _mm_cmpeq_epi8(odd, mask);\n            int bitmask = _mm_movemask_epi8(cmp);\n            count += __builtin_popcount(bitmask);\n        }\n        \n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n        \n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t count = 0;\n        size_t i = 0;\n        const size_t len = s.length();\n        const size_t simd_width = 16;\n        const size_t simd_iterations = len / simd_width;\n        \n        for (; i < simd_iterations * simd_width; i += simd_width) {\n            __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n            __m128i digits = _mm_sub_epi8(chars, _mm_set1_epi8('0'));\n            __m128i odd_mask = _mm_and_si128(digits, _mm_set1_epi8(1));\n            __m128i cmp = _mm_cmpeq_epi8(odd_mask, _mm_set1_epi8(1));\n            int mask = _mm_movemask_epi8(cmp);\n            count += __builtin_popcount(mask);\n        }\n        \n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n        \n        std::string res = \"the number of odd elements \";\n        res += std::to_string(count);\n        res += \"n the str\";\n        res += std::to_string(count);\n        res += \"ng \";\n        res += std::to_string(count);\n        res += \" of the \";\n        res += std::to_string(count);\n        res += \"nput.\";\n        result.push_back(res);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& s : lst) {\n        size_t count = 0;\n        size_t len = s.length();\n        size_t i = 0;\n\n        // Process 16 characters at a time using SSE2\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n            __m128i ascii_zero = _mm_set1_epi8('0');\n            __m128i digits = _mm_sub_epi8(chunk, ascii_zero);\n            __m128i mask = _mm_set1_epi8(1);\n            __m128i odd = _mm_and_si128(digits, mask);\n            __m128i cmp = _mm_cmpeq_epi8(odd, mask);\n            int mask_bits = _mm_movemask_epi8(cmp);\n            count += __builtin_popcount(mask_bits);\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        // Create the output string\n        std::string out = \"the number of odd elements Xn the strXng X of the Xnput.\";\n        std::replace(out.begin(), out.end(), 'X', static_cast<char>(count + '0'));\n        result.push_back(out);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 2 (since SSE2 can handle 2 int64_t at once)\n    __m128i min_vec = _mm_set1_epi64x(min_sum);\n    __m128i current_vec = _mm_set1_epi64x(current_sum);\n    \n    for (; i + 1 < n; i += 2) {\n        __m128i nums_vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Calculate current_sum + nums[i] and nums[i] for both elements\n        __m128i sum_candidate = _mm_add_epi64(current_vec, nums_vec);\n        __m128i new_current = _mm_min_epi64(sum_candidate, nums_vec);\n        \n        // Update min_sum\n        min_vec = _mm_min_epi64(min_vec, new_current);\n        \n        // Broadcast the last element of new_current to both lanes for next iteration\n        current_vec = _mm_shuffle_epi32(new_current, _MM_SHUFFLE(3, 3, 1, 1));\n    }\n\n    // Extract the results from the SIMD registers\n    alignas(16) int64_t min_arr[2];\n    _mm_store_si128((__m128i*)min_arr, min_vec);\n    min_sum = std::min(min_sum, std::min(min_arr[0], min_arr[1]));\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    \n    // Process elements in chunks of 2 (since SSE2 can handle 2 int64_t at a time)\n    const size_t simd_width = 2;\n    size_t n = nums.size();\n    \n    for (; i + simd_width <= n; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i current_vec = _mm_set1_epi64x(current_sum);\n        \n        // Calculate prefix sums\n        __m128i prefix = _mm_add_epi64(current_vec, vec);\n        \n        // Horizontal add to get individual sums\n        int64_t sum1 = _mm_extract_epi64(prefix, 0);\n        int64_t sum2 = _mm_extract_epi64(prefix, 1);\n        \n        // Update current_sum and min_sum\n        current_sum = std::min(sum1, sum2);\n        if (sum1 < sum2) {\n            current_sum = sum1;\n            min_sum = std::min(min_sum, sum1);\n        } else {\n            current_sum = sum2;\n            min_sum = std::min(min_sum, sum2);\n        }\n        \n        // Also consider starting new subarray from current element\n        current_sum = std::min(current_sum, nums[i+1]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    size_t n = nums.size();\n\n    // Process elements in chunks of 2 (since SSE2 can handle 2 int64_t at once)\n    for (; i + 1 < n; i += 2) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i current_vec = _mm_set1_epi64x(current_sum);\n        \n        // Calculate prefix sums\n        __m128i prefix = _mm_add_epi64(current_vec, vec);\n        \n        // Horizontal add to get individual sums\n        int64_t sum1 = _mm_extract_epi64(prefix, 0);\n        int64_t sum2 = _mm_extract_epi64(prefix, 1);\n        \n        // Update current_sum and min_sum\n        current_sum = std::min(sum1, sum2);\n        min_sum = std::min(min_sum, current_sum);\n        \n        // Carry over the minimum prefix sum\n        current_sum = std::min(current_sum, sum2);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    \n    // Process elements in chunks of 2 (since SSE2 can handle 2 int64_t at once)\n    const size_t simd_width = 2;\n    size_t n = nums.size();\n    \n    for (; i + simd_width <= n; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i current_vec = _mm_set1_epi64x(current_sum);\n        \n        // Calculate prefix sums\n        __m128i prefix = _mm_add_epi64(current_vec, vec);\n        \n        // Horizontal add to get individual sums\n        int64_t sum1 = _mm_extract_epi64(prefix, 0);\n        int64_t sum2 = _mm_extract_epi64(prefix, 1);\n        \n        // Update current_sum and min_sum\n        current_sum = std::min(sum1, sum2);\n        min_sum = std::min(min_sum, current_sum);\n        \n        // Carry over the minimum of the two sums\n        current_sum = std::min(sum2, sum1 + nums[i+1]);\n    }\n    \n    // Process remaining elements\n    for (; i < n; ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    \n    int64_t min_sum = nums[0];\n    int64_t current_sum = nums[0];\n    size_t i = 1;\n    \n    // Process elements 2 at a time using SSE\n    __m128i min_vec = _mm_set1_epi64x(min_sum);\n    __m128i current_vec = _mm_set1_epi64x(current_sum);\n    \n    for (; i + 1 < nums.size(); i += 2) {\n        __m128i nums_vec = _mm_set_epi64x(nums[i+1], nums[i]);\n        \n        // Update current_sum: min(nums[i], current_sum + nums[i])\n        current_vec = _mm_add_epi64(current_vec, nums_vec);\n        current_vec = _mm_min_epi64(current_vec, nums_vec);\n        \n        // Update min_sum\n        min_vec = _mm_min_epi64(min_vec, current_vec);\n    }\n    \n    // Extract values from SIMD registers\n    alignas(16) int64_t tmp_min[2];\n    alignas(16) int64_t tmp_current[2];\n    _mm_store_si128((__m128i*)tmp_min, min_vec);\n    _mm_store_si128((__m128i*)tmp_current, current_vec);\n    \n    min_sum = std::min(tmp_min[0], tmp_min[1]);\n    current_sum = tmp_current[1]; // Last element in the vector\n    \n    // Process remaining elements\n    for (; i < nums.size(); ++i) {\n        current_sum = std::min(nums[i], current_sum + nums[i]);\n        min_sum = std::min(min_sum, current_sum);\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int j = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (; j + 15 < cols; j += 16) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&row[j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&row[j + 8]);\n            sum = _mm_add_epi32(sum, _mm_add_epi32(v1, v2));\n        }\n\n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i*)temp, sum);\n        int row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            row_sum += row[j];\n        }\n\n        total += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty() || grid[0].empty() || capacity <= 0) {\n        return 0;\n    }\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int j = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (; j + 15 < cols; j += 16) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&row[j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&row[j + 8]);\n            sum = _mm_add_epi32(sum, _mm_add_epi32(v1, v2));\n        }\n\n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i*)temp, sum);\n        int row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            row_sum += row[j];\n        }\n\n        total += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (capacity == 0) return 0;\n    \n    int total = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n    \n    for (const auto &row : grid) {\n        int sum = 0;\n        int i = 0;\n        \n        // Process 16 elements at a time using SSE\n        __m128i v_sum = _mm_setzero_si128();\n        for (; i + 15 < cols; i += 16) {\n            __m128i v_data = _mm_loadu_si128((__m128i*)&row[i]);\n            v_sum = _mm_add_epi32(v_sum, _mm_sad_epu8(v_data, _mm_setzero_si128()));\n        }\n        \n        // Sum the partial results from SIMD\n        alignas(16) uint32_t sum_arr[4];\n        _mm_store_si128((__m128i*)sum_arr, v_sum);\n        sum += sum_arr[0] + sum_arr[2];\n        \n        // Process remaining elements\n        for (; i < cols; ++i) {\n            sum += row[i];\n        }\n        \n        total += (sum + capacity - 1) / capacity;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (capacity <= 0) return 0;\n    \n    int total = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n    \n    for (const auto &row : grid) {\n        int sum = 0;\n        int i = 0;\n        \n        // Process 16 elements at a time using SSE\n        __m128i sum_vec = _mm_setzero_si128();\n        for (; i + 15 < cols; i += 16) {\n            __m128i v = _mm_loadu_si128((__m128i*)&row[i]);\n            sum_vec = _mm_add_epi32(sum_vec, v);\n        }\n        \n        // Horizontal sum of the vector\n        __m128i shuffle = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n        sum_vec = _mm_add_epi32(sum_vec, shuffle);\n        shuffle = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n        sum_vec = _mm_add_epi32(sum_vec, shuffle);\n        sum += _mm_cvtsi128_si32(sum_vec);\n        \n        // Process remaining elements\n        for (; i < cols; ++i) {\n            sum += row[i];\n        }\n        \n        total += (sum + capacity - 1) / capacity;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (capacity <= 0) return 0;\n    \n    int total = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n    if (cols == 0) return 0;\n    \n    for (const auto& row : grid) {\n        int col = 0;\n        __m128i sum = _mm_setzero_si128();\n        \n        // Process 16 elements at a time (SSE2)\n        for (; col + 15 < cols; col += 16) {\n            __m128i v = _mm_loadu_si128((__m128i*)&row[col]);\n            sum = _mm_add_epi32(sum, _mm_sad_epu8(v, _mm_setzero_si128()));\n        }\n        \n        // Sum the partial results\n        uint64_t sum_u64[2];\n        _mm_storeu_si128((__m128i*)sum_u64, sum);\n        int row_sum = sum_u64[0] + sum_u64[1];\n        \n        // Process remaining elements\n        for (; col < cols; ++col) {\n            row_sum += row[col];\n        }\n        \n        total += (row_sum + capacity - 1) / capacity;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_pos = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        \n        // Check odd elements (data & 1)\n        __m128i odd_mask = _mm_cmpeq_epi16(_mm_and_si128(data, mask_odd), mask_odd);\n        \n        // Check even positions (mask_even_pos)\n        __m128i even_pos_mask = _mm_and_si128(odd_mask, mask_even_pos);\n        \n        // Mask the data\n        __m128i masked = _mm_and_si128(data, even_pos_mask);\n        \n        // Sum the masked values\n        sum = _mm_add_epi64(sum, _mm_sad_epu8(masked, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    int64_t result = 0;\n    int64_t sum_arr[2];\n    _mm_storeu_si128((__m128i*)sum_arr, sum);\n    result = sum_arr[0] + sum_arr[1];\n\n    for (; i < size; i += 2) {\n        if (lst[i] & 1) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_pos = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i odd_mask = _mm_and_si128(data, mask_odd);\n        odd_mask = _mm_cmpeq_epi16(odd_mask, mask_odd);\n        \n        __m128i even_pos_mask = _mm_and_si128(mask_even_pos, odd_mask);\n        even_pos_mask = _mm_cmpeq_epi16(even_pos_mask, mask_even_pos);\n        \n        __m128i masked_data = _mm_and_si128(data, even_pos_mask);\n        sum_vec = _mm_add_epi64(sum_vec, _mm_sad_epu8(masked_data, _mm_setzero_si128()));\n    }\n    \n    sum += _mm_extract_epi64(sum_vec, 0) + _mm_extract_epi64(sum_vec, 1);\n    \n    for (; i < size; i += 2) {\n        if (lst[i] & 1) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    // Process 8 elements at a time (128-bit SSE registers can hold 8 int16_t)\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_pos = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        \n        // Check odd elements: (data & 1) != 0\n        __m128i odd_mask = _mm_cmpeq_epi16(_mm_and_si128(data, mask_odd), mask_odd);\n        \n        // Check even positions: (i % 2 == 0)\n        __m128i even_pos_mask = _mm_cmpeq_epi16(mask_even_pos, _mm_setzero_si128());\n        \n        // Combine masks: odd elements AND even positions\n        __m128i combined_mask = _mm_and_si128(odd_mask, even_pos_mask);\n        \n        // Extract elements that match both conditions\n        __m128i selected = _mm_and_si128(data, combined_mask);\n        \n        // Sum the selected elements horizontally\n        __m128i sum_low = _mm_unpacklo_epi16(selected, _mm_setzero_si128());\n        __m128i sum_high = _mm_unpackhi_epi16(selected, _mm_setzero_si128());\n        sum_vec = _mm_add_epi32(sum_vec, _mm_add_epi32(sum_low, sum_high));\n    }\n    \n    // Sum the partial results from SIMD\n    int32_t partial_sum[4];\n    _mm_storeu_si128((__m128i*)partial_sum, sum_vec);\n    sum += partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n    \n    // Process remaining elements\n    for (; i < size; i++) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    const size_t size = lst.size();\n    const size_t aligned_size = size & ~7;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask_pos = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n        __m128i mask_odd = _mm_set1_epi16(1);\n        \n        __m128i even_pos = _mm_and_si128(data, mask_pos);\n        __m128i is_even_pos = _mm_cmpeq_epi16(even_pos, _mm_setzero_si128());\n        \n        __m128i odd_val = _mm_and_si128(data, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_val, mask_odd);\n        \n        __m128i mask = _mm_and_si128(is_even_pos, is_odd);\n        __m128i selected = _mm_and_si128(data, mask);\n        \n        sum = _mm_add_epi64(sum, _mm_sad_epu8(selected, _mm_setzero_si128()));\n    }\n\n    int64_t result = _mm_extract_epi64(sum, 0) + _mm_extract_epi64(sum, 1);\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_pos = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i odd = _mm_and_si128(data, mask_odd);\n        odd = _mm_cmpeq_epi16(odd, mask_odd);\n        __m128i even_pos = _mm_and_si128(mask_even_pos, odd);\n        __m128i masked = _mm_and_si128(data, even_pos);\n        \n        __m128i sum_low = _mm_cvtepi16_epi32(masked);\n        __m128i sum_high = _mm_cvtepi16_epi32(_mm_srli_si128(masked, 8));\n        sum = _mm_add_epi32(sum, sum_low);\n        sum = _mm_add_epi32(sum, sum_high);\n    }\n\n    // Handle remaining elements\n    int64_t total = 0;\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    total += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; i += 2) {\n        if (lst[i] & 1) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF); // For absolute value\n    __m128i lower_bound = _mm_set1_epi32(10);\n    __m128i upper_bound = _mm_set1_epi32(99);\n\n    for (; i + 4 <= k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_and_si128(vec, mask);\n        __m128i cmp_ge = _mm_cmpgt_epi32(abs_vec, lower_bound);\n        __m128i cmp_le = _mm_cmplt_epi32(abs_vec, upper_bound);\n        __m128i in_range = _mm_and_si128(cmp_ge, cmp_le);\n        __m128i masked = _mm_and_si128(vec, in_range);\n        sum_vec = _mm_add_epi32(sum_vec, masked);\n    }\n\n    // Horizontal sum\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        int abs_val = val < 0 ? -val : val;\n        if (abs_val >= 10 && abs_val <= 99) {\n            sum += val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF); // For absolute value\n    \n    for (; i + 4 <= k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Check if elements have at most two digits (<= 99 or >= -99)\n        __m128i abs_vec = _mm_and_si128(vec, mask);\n        __m128i cmp = _mm_cmplt_epi32(abs_vec, _mm_set1_epi32(100));\n        __m128i masked = _mm_and_si128(vec, cmp);\n        \n        // Horizontal addition\n        sum_vec = _mm_add_epi32(sum_vec, masked);\n    }\n    \n    // Sum the remaining elements\n    for (; i < k; i++) {\n        int val = arr[i];\n        if (val < 100 && val > -100) {\n            sum += val;\n        }\n    }\n    \n    // Extract sum from SIMD register\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    int i = 0;\n    int result = 0;\n    \n    for (; i + 4 <= k; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Check if elements have at most two digits (<= 99 and >= -99)\n        __m128i mask1 = _mm_cmplt_epi32(v, _mm_set1_epi32(100));\n        __m128i mask2 = _mm_cmpgt_epi32(v, _mm_set1_epi32(-100));\n        __m128i mask = _mm_and_si128(mask1, mask2);\n        \n        // Apply mask\n        __m128i masked = _mm_and_si128(v, mask);\n        \n        // Horizontal addition\n        __m128i sum_lo = _mm_unpacklo_epi32(masked, _mm_setzero_si128());\n        __m128i sum_hi = _mm_unpackhi_epi32(masked, _mm_setzero_si128());\n        sum = _mm_add_epi32(sum, sum_lo);\n        sum = _mm_add_epi32(sum, sum_hi);\n    }\n    \n    // Sum the SIMD lanes\n    int simd_sum[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)simd_sum, sum);\n    result = simd_sum[0] + simd_sum[1] + simd_sum[2] + simd_sum[3];\n    \n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        if (val > -100 && val < 100) {\n            result += val;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_99 = _mm_set1_epi32(99);\n    __m128i mask_neg_100 = _mm_set1_epi32(-100);\n    int i = 0;\n\n    for (; i + 4 <= k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Check if elements are <= 99 (abs(x) <= 99)\n        __m128i cmp_upper = _mm_cmpgt_epi32(vec, mask_99);\n        __m128i cmp_lower = _mm_cmplt_epi32(vec, mask_neg_100);\n        __m128i cmp = _mm_or_si128(cmp_upper, cmp_lower);\n        \n        // Mask elements that are outside [-99, 99]\n        __m128i masked = _mm_andnot_si128(cmp, vec);\n        \n        // Accumulate the sum\n        sum = _mm_add_epi32(sum, masked);\n    }\n\n    // Horizontal sum of the SIMD register\n    int result = 0;\n    int temp[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)temp, sum);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        if (val >= -99 && val <= 99) {\n            result += val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n    __m128i ten = _mm_set1_epi32(10);\n    __m128i hundred = _mm_set1_epi32(100);\n    \n    int i;\n    for (i = 0; i + 4 <= k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Check if elements are less than 100 (two digits)\n        __m128i cmp_lt_100 = _mm_cmplt_epi32(vec, hundred);\n        // Check if elements are greater than or equal to -99 (two digits negative)\n        __m128i cmp_gt_n99 = _mm_cmpgt_epi32(vec, _mm_set1_epi32(-100));\n        // Check if elements are positive (for two-digit positive numbers)\n        __m128i is_pos = _mm_cmpgt_epi32(vec, _mm_set1_epi32(9));\n        // Check if elements are negative (for two-digit negative numbers)\n        __m128i is_neg = _mm_cmplt_epi32(vec, _mm_set1_epi32(-9));\n        \n        // Combine conditions: (0 < x < 100) or (-100 < x < 0)\n        __m128i cond1 = _mm_and_si128(cmp_lt_100, is_pos);\n        __m128i cond2 = _mm_and_si128(cmp_gt_n99, is_neg);\n        __m128i cond = _mm_or_si128(cond1, cond2);\n        \n        // Also include single-digit numbers (0-9 and -9-0)\n        __m128i single_digit_pos = _mm_and_si128(_mm_cmpgt_epi32(vec, _mm_set1_epi32(-1)), _mm_cmplt_epi32(vec, ten));\n        __m128i single_digit_neg = _mm_and_si128(_mm_cmplt_epi32(vec, _mm_set1_epi32(1)), _mm_cmpgt_epi32(vec, _mm_set1_epi32(-10)));\n        __m128i single_digit = _mm_or_si128(single_digit_pos, single_digit_neg);\n        \n        // Final condition: two-digit or single-digit\n        __m128i final_cond = _mm_or_si128(cond, single_digit);\n        \n        // Mask out elements that don't meet the condition\n        __m128i masked = _mm_and_si128(vec, _mm_and_si128(final_cond, mask));\n        \n        // Add to sum\n        sum = _mm_add_epi32(sum, masked);\n    }\n    \n    // Sum the 4 elements of the sum vector\n    int result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, sum);\n    int result = result_arr[0] + result_arr[1] + result_arr[2] + result_arr[3];\n    \n    // Handle remaining elements\n    for (; i < k; ++i) {\n        int val = arr[i];\n        if ((val >= 0 && val < 100) || (val < 0 && val > -100)) {\n            if ((val >= 10 || val <= -10) || (val >= 0 && val < 10) || (val < 0 && val > -10)) {\n                result += val;\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t size = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i current;\n\n    for (i = 1; i + 3 < size; i += 4) {\n        current = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i shifted_prev = _mm_alignr_epi8(current, prev, 12);\n        __m128i cmp = _mm_cmpgt_epi32(shifted_prev, current);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n        prev = current;\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i - 1] > lst[i]) {\n            return false;\n        }\n    }\n\n    for (i = 1; i < size; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i + 1 < size && lst[i] == lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t size = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i current;\n\n    for (i = 1; i + 3 < size; i += 4) {\n        current = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i shifted_prev = _mm_slli_si128(prev, 12);\n        shifted_prev = _mm_shuffle_epi32(shifted_prev, _MM_SHUFFLE(0, 3, 2, 1));\n        __m128i combined = _mm_or_si128(shifted_prev, _mm_srli_si128(current, 4));\n        __m128i cmp_lt = _mm_cmplt_epi32(combined, current);\n        __m128i cmp_eq = _mm_cmpeq_epi32(combined, current);\n        __m128i cmp_res = _mm_or_si128(cmp_lt, cmp_eq);\n        int mask = _mm_movemask_epi8(cmp_res);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n        prev = current;\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] < lst[i-1]) {\n            return false;\n        }\n        if (i > 1 && lst[i] == lst[i-1] && lst[i-1] == lst[i-2]) {\n            return false;\n        }\n    }\n\n    for (i = 2; i < size; ++i) {\n        if (lst[i] == lst[i-1] && lst[i-1] == lst[i-2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t size = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i const*)&lst[i+1]);\n        \n        // Check for duplicates in current chunk\n        __m128i shifted = _mm_srli_si128(current, 4);\n        __m128i cmp = _mm_cmpeq_epi32(current, shifted);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n\n        // Check if current <= next\n        __m128i cmp_lt = _mm_cmpgt_epi32(current, next);\n        if (_mm_movemask_epi8(cmp_lt) != 0) {\n            return false;\n        }\n\n        // Check if last element of current is <= first element of next\n        if (i + 4 < size) {\n            int last_current = lst[i + 3];\n            int first_next = lst[i + 4];\n            if (last_current > first_next) {\n                return false;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (; i + 1 < size; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        if (lst[i] == lst[i + 1] && i > 0 && lst[i - 1] == lst[i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t size = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i const*)&lst[i + 1]);\n        \n        __m128i cmp_leq = _mm_cmplt_epi32(prev, current);\n        __m128i cmp_eq = _mm_cmpeq_epi32(prev, current);\n        __m128i cmp_dup = _mm_cmpeq_epi32(current, next);\n        \n        int mask_leq = _mm_movemask_epi8(cmp_leq);\n        int mask_eq = _mm_movemask_epi8(cmp_eq);\n        int mask_dup = _mm_movemask_epi8(cmp_dup);\n        \n        if ((mask_leq != 0xFFFF) && (mask_eq != 0xFFFF)) {\n            return false;\n        }\n        \n        if (mask_dup == 0xFFFF && i + 4 < size) {\n            return false;\n        }\n        \n        prev = current;\n    }\n\n    for (; i + 1 < size; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        if (i + 2 < size && lst[i] == lst[i + 1] && lst[i + 1] == lst[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t size = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i current;\n\n    for (i = 1; i + 4 <= size; i += 4) {\n        current = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n\n        // Check for duplicates (more than 1)\n        __m128i next = _mm_loadu_si128((__m128i const*)&lst[i-1]);\n        __m128i eq = _mm_cmpeq_epi32(next, current);\n        if (_mm_movemask_epi8(eq) != 0) {\n            // Check if there are more than 1 duplicates\n            __m128i shifted = _mm_srli_si128(next, 4);\n            __m128i eq_shifted = _mm_cmpeq_epi32(shifted, current);\n            if (_mm_movemask_epi8(eq_shifted) != 0) {\n                return false;\n            }\n        }\n        prev = _mm_set1_epi32(lst[i+3]);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (lst[i-1] > lst[i]) {\n            return false;\n        }\n        if (i > 1 && lst[i-2] == lst[i-1] && lst[i-1] == lst[i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_mag = _mm_setzero_si128();\n    __m128i prod_sign = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    size_t i = 0;\n    for (; i + 4 <= arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_and_si128(vec, mask);\n        sum_mag = _mm_add_epi32(sum_mag, abs_vec);\n\n        __m128i sign = _mm_srai_epi32(vec, 31);\n        sign = _mm_or_si128(sign, _mm_cmpeq_epi32(vec, zero));\n        prod_sign = _mm_mullo_epi32(prod_sign, sign);\n    }\n\n    int sum_mag_arr[4];\n    int prod_sign_arr[4];\n    _mm_storeu_si128((__m128i*)sum_mag_arr, sum_mag);\n    _mm_storeu_si128((__m128i*)prod_sign_arr, prod_sign);\n\n    int total_sum = sum_mag_arr[0] + sum_mag_arr[1] + sum_mag_arr[2] + sum_mag_arr[3];\n    int total_prod = prod_sign_arr[0] * prod_sign_arr[1] * prod_sign_arr[2] * prod_sign_arr[3];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += abs(val);\n        if (val > 0) {\n            total_prod *= 1;\n        } else if (val < 0) {\n            total_prod *= -1;\n        } else {\n            total_prod = 0;\n        }\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_mag = _mm_setzero_si128();\n    __m128i prod_sign = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i minus_one = _mm_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Compute magnitudes\n        __m128i abs_mask = _mm_srai_epi32(vec, 31);\n        __m128i mag = _mm_add_epi32(_mm_xor_si128(vec, abs_mask), _mm_and_si128(abs_mask, one));\n        sum_mag = _mm_add_epi32(sum_mag, mag);\n        \n        // Compute signs\n        __m128i sign = _mm_or_si128(_mm_cmpgt_epi32(vec, zero), _mm_and_si128(_mm_cmpeq_epi32(vec, zero), zero));\n        sign = _mm_or_si128(sign, _mm_and_si128(_mm_cmplt_epi32(vec, zero), minus_one));\n        prod_sign = _mm_mullo_epi32(prod_sign, sign);\n    }\n\n    // Horizontal sum of sum_mag\n    __m128i sum_shuf = _mm_shuffle_epi32(sum_mag, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_mag = _mm_add_epi32(sum_mag, sum_shuf);\n    sum_shuf = _mm_shuffle_epi32(sum_mag, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_mag = _mm_add_epi32(sum_mag, sum_shuf);\n    int total_sum = _mm_cvtsi128_si32(sum_mag);\n\n    // Horizontal product of prod_sign\n    __m128i prod_shuf = _mm_shuffle_epi32(prod_sign, _MM_SHUFFLE(2, 3, 0, 1));\n    prod_sign = _mm_mullo_epi32(prod_sign, prod_shuf);\n    prod_shuf = _mm_shuffle_epi32(prod_sign, _MM_SHUFFLE(1, 0, 3, 2));\n    prod_sign = _mm_mullo_epi32(prod_sign, prod_shuf);\n    int total_prod = _mm_cvtsi128_si32(prod_sign);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += abs(val);\n        if (val > 0) {\n            total_prod *= 1;\n        } else if (val < 0) {\n            total_prod *= -1;\n        } else {\n            total_prod = 0;\n        }\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_mag = _mm_setzero_si128();\n    __m128i prod_sign = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_abs = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask_sign = _mm_set1_epi32(0x80000000);\n\n    size_t i = 0;\n    for (; i + 4 <= arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        __m128i abs_vec = _mm_and_si128(vec, mask_abs);\n        sum_mag = _mm_add_epi32(sum_mag, abs_vec);\n\n        __m128i sign = _mm_and_si128(vec, mask_sign);\n        __m128i is_zero = _mm_cmpeq_epi32(vec, zero);\n        __m128i sign_extracted = _mm_or_si128(\n            _mm_srli_epi32(sign, 31),\n            _mm_set1_epi32(1)\n        );\n        sign_extracted = _mm_sub_epi32(_mm_setzero_si128(), sign_extracted);\n        sign_extracted = _mm_andnot_si128(is_zero, sign_extracted);\n        prod_sign = _mm_mullo_epi32(prod_sign, sign_extracted);\n    }\n\n    int sum_mag_arr[4];\n    int prod_sign_arr[4];\n    _mm_storeu_si128((__m128i*)sum_mag_arr, sum_mag);\n    _mm_storeu_si128((__m128i*)prod_sign_arr, prod_sign);\n\n    int total_sum = sum_mag_arr[0] + sum_mag_arr[1] + sum_mag_arr[2] + sum_mag_arr[3];\n    int total_prod = prod_sign_arr[0] * prod_sign_arr[1] * prod_sign_arr[2] * prod_sign_arr[3];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += abs(val);\n        if (val == 0) {\n            total_prod = 0;\n        } else if (val < 0) {\n            total_prod *= -1;\n        }\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_mag = _mm_setzero_si128();\n    __m128i prod_sign = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_abs = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask_sign = _mm_set1_epi32(0x80000000);\n\n    size_t i = 0;\n    for (; i + 4 <= arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Compute magnitudes\n        __m128i mag = _mm_and_si128(vec, mask_abs);\n        sum_mag = _mm_add_epi32(sum_mag, mag);\n        \n        // Compute signs\n        __m128i sign = _mm_and_si128(vec, mask_sign);\n        __m128i is_neg = _mm_cmplt_epi32(vec, zero);\n        __m128i is_zero = _mm_cmpeq_epi32(vec, zero);\n        __m128i sign_val = _mm_or_si128(_mm_srli_epi32(sign, 31), _mm_set1_epi32(1));\n        sign_val = _mm_sub_epi32(_mm_setzero_si128(), sign_val);\n        sign_val = _mm_add_epi32(sign_val, _mm_set1_epi32(1));\n        sign_val = _mm_blendv_epi8(sign_val, zero, is_zero);\n        prod_sign = _mm_mullo_epi32(prod_sign, sign_val);\n    }\n\n    // Horizontal sum of sum_mag\n    __m128i sum_shuf = _mm_shuffle_epi32(sum_mag, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_mag = _mm_add_epi32(sum_mag, sum_shuf);\n    sum_shuf = _mm_shuffle_epi32(sum_mag, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_mag = _mm_add_epi32(sum_mag, sum_shuf);\n    int total_sum = _mm_cvtsi128_si32(sum_mag);\n\n    // Horizontal product of prod_sign\n    __m128i prod_shuf = _mm_shuffle_epi32(prod_sign, _MM_SHUFFLE(2, 3, 0, 1));\n    prod_sign = _mm_mullo_epi32(prod_sign, prod_shuf);\n    prod_shuf = _mm_shuffle_epi32(prod_sign, _MM_SHUFFLE(1, 0, 3, 2));\n    prod_sign = _mm_mullo_epi32(prod_sign, prod_shuf);\n    int total_prod = _mm_cvtsi128_si32(prod_sign);\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += abs(val);\n        if (val == 0) {\n            total_prod = 0;\n        } else if (val < 0) {\n            total_prod *= -1;\n        }\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_mag = _mm_setzero_si128();\n    __m128i prod_sign = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i minus_one = _mm_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        \n        // Compute magnitudes\n        __m128i abs_mask = _mm_cmpgt_epi32(zero, vec);\n        __m128i mag = _mm_add_epi32(_mm_and_si128(abs_mask, vec), _mm_andnot_si128(abs_mask, vec));\n        sum_mag = _mm_add_epi32(sum_mag, mag);\n        \n        // Compute signs\n        __m128i sign = _mm_or_si128(\n            _mm_cmpeq_epi32(vec, zero),\n            _mm_or_si128(\n                _mm_and_si128(_mm_cmpgt_epi32(vec, zero), one),\n                _mm_and_si128(_mm_cmplt_epi32(vec, zero), minus_one)\n            )\n        );\n        prod_sign = _mm_mullo_epi32(prod_sign, sign);\n    }\n\n    // Horizontal sum of sum_mag\n    int sum_mag_arr[4];\n    _mm_storeu_si128((__m128i*)sum_mag_arr, sum_mag);\n    int total_sum = sum_mag_arr[0] + sum_mag_arr[1] + sum_mag_arr[2] + sum_mag_arr[3];\n\n    // Horizontal product of prod_sign\n    int prod_sign_arr[4];\n    _mm_storeu_si128((__m128i*)prod_sign_arr, prod_sign);\n    int total_prod = prod_sign_arr[0] * prod_sign_arr[1] * prod_sign_arr[2] * prod_sign_arr[3];\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += abs(val);\n        if (val > 0) {\n            total_prod *= 1;\n        } else if (val < 0) {\n            total_prod *= -1;\n        } else {\n            total_prod = 0;\n        }\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible paths of length k and select the lex smallest\n    // This is a brute-force approach and not optimized with SIMD\n    // SIMD optimization would require a more complex approach\n    std::vector<int> min_path;\n    int min_lex = INT_MAX;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<std::vector<int>> visited(N, std::vector<int>(N, 0));\n            std::vector<int> current_path;\n            std::vector<std::pair<int, int>> stack;\n            stack.push_back({i, j});\n            visited[i][j] = 1;\n            current_path.push_back(grid[i][j]);\n\n            while (!stack.empty()) {\n                if (current_path.size() == k) {\n                    bool is_smaller = false;\n                    for (int idx = 0; idx < k; ++idx) {\n                        if (current_path[idx] < min_path[idx]) {\n                            is_smaller = true;\n                            break;\n                        } else if (current_path[idx] > min_path[idx]) {\n                            break;\n                        }\n                    }\n                    if (is_smaller || min_path.empty()) {\n                        min_path = current_path;\n                    }\n                    break;\n                }\n\n                auto [x, y] = stack.back();\n                bool found = false;\n                int min_neighbor = INT_MAX;\n                int next_x = -1, next_y = -1;\n\n                // Check neighbors in SIMD-friendly way (though this part is still sequential)\n                if (x > 0 && !visited[x-1][y] && grid[x-1][y] < min_neighbor) {\n                    min_neighbor = grid[x-1][y];\n                    next_x = x-1;\n                    next_y = y;\n                }\n                if (x < N-1 && !visited[x+1][y] && grid[x+1][y] < min_neighbor) {\n                    min_neighbor = grid[x+1][y];\n                    next_x = x+1;\n                    next_y = y;\n                }\n                if (y > 0 && !visited[x][y-1] && grid[x][y-1] < min_neighbor) {\n                    min_neighbor = grid[x][y-1];\n                    next_x = x;\n                    next_y = y-1;\n                }\n                if (y < N-1 && !visited[x][y+1] && grid[x][y+1] < min_neighbor) {\n                    min_neighbor = grid[x][y+1];\n                    next_x = x;\n                    next_y = y+1;\n                }\n\n                if (next_x != -1) {\n                    visited[next_x][next_y] = 1;\n                    current_path.push_back(grid[next_x][next_y]);\n                    stack.push_back({next_x, next_y});\n                } else {\n                    stack.pop_back();\n                    if (!current_path.empty()) {\n                        current_path.pop_back();\n                    }\n                    if (!stack.empty()) {\n                        visited[stack.back().first][stack.back().second] = 0;\n                    }\n                }\n            }\n        }\n    }\n\n    return min_path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible paths of length k and select the lex smallest\n    // This is a brute-force approach since SIMD may not directly help with path finding\n    // but can be used to parallelize certain operations like comparisons\n    \n    std::vector<int> min_path(k, INT_MAX);\n    \n    // Directions: up, down, left, right\n    const int dx[] = {-1, 1, 0, 0};\n    const int dy[] = {0, 0, -1, 1};\n    \n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<std::vector<int>> paths;\n            std::vector<int> current_path = {grid[i][j]};\n            std::vector<std::pair<int, int>> positions = {{i, j}};\n            \n            for (int step = 1; step < k; ++step) {\n                std::vector<std::vector<int>> new_paths;\n                std::vector<std::pair<int, int>> new_positions;\n                \n                for (size_t p = 0; p < paths.size(); ++p) {\n                    int x = positions[p].first;\n                    int y = positions[p].second;\n                    \n                    for (int d = 0; d < 4; ++d) {\n                        int nx = x + dx[d];\n                        int ny = y + dy[d];\n                        \n                        if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                            std::vector<int> new_path = paths[p];\n                            new_path.push_back(grid[nx][ny]);\n                            new_paths.push_back(new_path);\n                            new_positions.emplace_back(nx, ny);\n                        }\n                    }\n                }\n                \n                if (step == 1) {\n                    for (int d = 0; d < 4; ++d) {\n                        int nx = i + dx[d];\n                        int ny = j + dy[d];\n                        \n                        if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                            std::vector<int> new_path = current_path;\n                            new_path.push_back(grid[nx][ny]);\n                            new_paths.push_back(new_path);\n                            new_positions.emplace_back(nx, ny);\n                        }\n                    }\n                }\n                \n                paths = std::move(new_paths);\n                positions = std::move(new_positions);\n            }\n            \n            // Compare paths using SIMD for vectorized comparisons\n            for (const auto &path : paths) {\n                bool is_smaller = false;\n                // Compare 4 elements at a time using SIMD\n                for (int idx = 0; idx < k; idx += 4) {\n                    __m128i current_min = _mm_loadu_si128((__m128i*)&min_path[idx]);\n                    __m128i current_path_vec = _mm_loadu_si128((__m128i*)&path[idx]);\n                    \n                    __m128i cmp = _mm_cmplt_epi32(current_path_vec, current_min);\n                    int mask = _mm_movemask_epi8(cmp);\n                    \n                    if (mask != 0) {\n                        is_smaller = true;\n                        break;\n                    }\n                    \n                    __m128i cmp_eq = _mm_cmpeq_epi32(current_path_vec, current_min);\n                    mask = _mm_movemask_epi8(cmp_eq);\n                    \n                    if (mask != 0xFFFF) {\n                        break;\n                    }\n                }\n                \n                if (is_smaller) {\n                    min_path = path;\n                }\n            }\n        }\n    }\n    \n    return min_path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (const auto &row : grid) {\n            for (int val : row) {\n                min_val = std::min(min_val, val);\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible paths of length k and select the lex smallest\n    std::vector<int> min_path;\n    std::vector<int> current_path;\n    int min_lex = INT_MAX;\n\n    // Directions: up, down, left, right\n    const int dirs[4][2] = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};\n\n    // SIMD variables for parallel comparison\n    __m128i current_min = _mm_set1_epi32(INT_MAX);\n    __m128i path_vals;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<std::vector<bool>> visited(N, std::vector<bool>(N, false));\n            current_path.clear();\n            current_path.push_back(grid[i][j]);\n            visited[i][j] = true;\n\n            // Recursive DFS with SIMD for path comparison\n            std::function<void(int, int, int)> dfs = [&](int x, int y, int step) {\n                if (step == k) {\n                    // Compare paths lexicographically using SIMD\n                    int cmp_result = 0;\n                    for (int idx = 0; idx < k; idx += 4) {\n                        int vals[4] = {0};\n                        for (int v = 0; v < 4 && idx + v < k; ++v) {\n                            vals[v] = current_path[idx + v];\n                        }\n                        path_vals = _mm_loadu_si128((__m128i*)vals);\n                        __m128i cmp = _mm_cmplt_epi32(path_vals, current_min);\n                        if (_mm_movemask_epi8(cmp)) {\n                            cmp_result = -1;\n                            break;\n                        }\n                    }\n                    if (cmp_result < 0 || (cmp_result == 0 && current_path < min_path)) {\n                        min_path = current_path;\n                        for (int idx = 0; idx < k; idx += 4) {\n                            int vals[4] = {0};\n                            for (int v = 0; v < 4 && idx + v < k; ++v) {\n                                vals[v] = current_path[idx + v];\n                            }\n                            current_min = _mm_loadu_si128((__m128i*)vals);\n                        }\n                    }\n                    return;\n                }\n\n                for (int d = 0; d < 4; ++d) {\n                    int nx = x + dirs[d][0];\n                    int ny = y + dirs[d][1];\n                    if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                        if (!visited[nx][ny]) {\n                            visited[nx][ny] = true;\n                            current_path.push_back(grid[nx][ny]);\n                            dfs(nx, ny, step + 1);\n                            current_path.pop_back();\n                            visited[nx][ny] = false;\n                        }\n                    }\n                }\n            };\n\n            dfs(i, j, 1);\n        }\n    }\n\n    return min_path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (grid[i][j] < min_val) {\n                    min_val = grid[i][j];\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible starting positions\n    std::vector<std::pair<int, int>> positions;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            positions.emplace_back(i, j);\n        }\n    }\n\n    // SIMD variables for tracking min path\n    __m128i min_path = _mm_set1_epi32(INT_MAX);\n    std::vector<int> result(k, INT_MAX);\n\n    for (const auto& start : positions) {\n        int x = start.first;\n        int y = start.second;\n        std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k, INT_MAX)));\n        dp[x][y][0] = grid[x][y];\n\n        for (int step = 1; step < k; ++step) {\n            for (int i = 0; i < N; ++i) {\n                for (int j = 0; j < N; j += 4) {\n                    __m128i current_min = _mm_set1_epi32(INT_MAX);\n                    int remaining = std::min(4, N - j);\n                    for (int l = 0; l < remaining; ++l) {\n                        int ni = i;\n                        int nj = j + l;\n                        if (dp[ni][nj][step - 1] == INT_MAX) continue;\n\n                        // Check neighbors\n                        for (int di = -1; di <= 1; ++di) {\n                            for (int dj = -1; dj <= 1; ++dj) {\n                                if ((di == 0 && dj == 0) || (di != 0 && dj != 0)) continue;\n                                int new_i = ni + di;\n                                int new_j = nj + dj;\n                                if (new_i >= 0 && new_i < N && new_j >= 0 && new_j < N) {\n                                    int new_val = dp[ni][nj][step - 1] * 100 + grid[new_i][new_j];\n                                    if (new_val < dp[new_i][new_j][step]) {\n                                        dp[new_i][new_j][step] = new_val;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        // Find the minimal path of length k starting at (x,y)\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (dp[i][j][k - 1] < result[0] * 100 + result[1]) {\n                    // Reconstruct path\n                    std::vector<int> path;\n                    int ci = i, cj = j, cstep = k - 1;\n                    path.push_back(grid[ci][cj]);\n                    while (cstep > 0) {\n                        for (int di = -1; di <= 1; ++di) {\n                            for (int dj = -1; dj <= 1; ++dj) {\n                                if ((di == 0 && dj == 0) || (di != 0 && dj != 0)) continue;\n                                int new_i = ci + di;\n                                int new_j = cj + dj;\n                                if (new_i >= 0 && new_i < N && new_j >= 0 && new_j < N) {\n                                    if (dp[new_i][new_j][cstep - 1] * 100 + grid[ci][cj] == dp[ci][cj][cstep]) {\n                                        ci = new_i;\n                                        cj = new_j;\n                                        cstep--;\n                                        path.push_back(grid[ci][cj]);\n                                        di = 2; // break outer loop\n                                        break;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    std::reverse(path.begin(), path.end());\n                    if (path < result) {\n                        result = path;\n                    }\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = INT_MAX;\n        for (const auto &row : grid) {\n            for (int val : row) {\n                if (val < min_val) {\n                    min_val = val;\n                }\n            }\n        }\n        return {min_val};\n    }\n\n    // Find all possible paths of length k and select the lex smallest\n    // This is a brute-force approach optimized with SIMD for comparison\n    std::vector<int> min_path;\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    int min_start = -1;\n    int min_i = -1, min_j = -1;\n\n    // SIMD comparison for finding the starting position with minimal value\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; j += 4) {\n            int remaining = std::min(4, N - j);\n            int vals[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n            for (int l = 0; l < remaining; ++l) {\n                vals[l] = grid[i][j + l];\n            }\n            __m128i current = _mm_loadu_si128((__m128i*)vals);\n            __m128i cmp = _mm_cmplt_epi32(current, min_vec);\n            if (_mm_movemask_epi8(cmp)) {\n                for (int l = 0; l < remaining; ++l) {\n                    if (vals[l] < _mm_extract_epi32(min_vec, l)) {\n                        min_vec = _mm_insert_epi32(min_vec, vals[l], l);\n                        min_i = i;\n                        min_j = j + l;\n                    }\n                }\n            }\n        }\n    }\n\n    // Now perform BFS to find the lex smallest path of length k starting at (min_i, min_j)\n    // This part is not SIMD optimized as it's more complex for path finding\n    std::vector<std::vector<int>> paths;\n    std::vector<std::pair<int, int>> current = {{min_i, min_j}};\n    std::vector<int> current_path = {grid[min_i][min_j]};\n\n    while (current_path.size() < k) {\n        std::vector<std::pair<int, int>> next;\n        std::vector<int> next_min_path;\n        int min_next_val = INT_MAX;\n        for (auto &pos : current) {\n            int i = pos.first;\n            int j = pos.second;\n            // Check all 4 neighbors\n            int di[] = {-1, 1, 0, 0};\n            int dj[] = {0, 0, -1, 1};\n            for (int d = 0; d < 4; ++d) {\n                int ni = i + di[d];\n                int nj = j + dj[d];\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    int val = grid[ni][nj];\n                    if (val < min_next_val) {\n                        min_next_val = val;\n                        next.clear();\n                        next.emplace_back(ni, nj);\n                        next_min_path = current_path;\n                        next_min_path.push_back(val);\n                    } else if (val == min_next_val) {\n                        next.emplace_back(ni, nj);\n                    }\n                }\n            }\n        }\n        current = next;\n        current_path.push_back(min_next_val);\n    }\n\n    return current_path;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i has_odd = _mm_set1_epi32(0);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i vdigit = _mm_set1_epi32(digit);\n        __m128i mask = _mm_and_si128(vdigit, one);\n        __m128i is_odd = _mm_cmpeq_epi32(mask, one);\n\n        has_odd = _mm_or_si128(has_odd, is_odd);\n\n        __m128i masked_digit = _mm_or_si128(vdigit, _mm_andnot_si128(is_odd, one));\n        product = _mm_mullo_epi32(product, masked_digit);\n    }\n\n    int32_t result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, product);\n\n    int32_t has_odd_arr[4];\n    _mm_storeu_si128((__m128i*)has_odd_arr, has_odd);\n\n    uint64_t result = result_arr[0];\n    int any_odd = has_odd_arr[0] | has_odd_arr[1] | has_odd_arr[2] | has_odd_arr[3];\n\n    return any_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i has_odd = _mm_setzero_si128();\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i vdigit = _mm_set1_epi32(digit);\n        __m128i mask = _mm_and_si128(vdigit, one);\n        __m128i is_odd = _mm_cmpeq_epi32(mask, one);\n\n        has_odd = _mm_or_si128(has_odd, is_odd);\n\n        __m128i product_low = _mm_and_si128(product, _mm_set1_epi32(0xFFFFFFFF));\n        __m128i digit_low = _mm_and_si128(vdigit, _mm_set1_epi32(0xFFFFFFFF));\n        __m128i new_product_low = _mm_mullo_epi32(product_low, _mm_add_epi32(digit_low, _mm_andnot_si128(is_odd, one)));\n\n        product = _mm_blendv_epi8(product, new_product_low, is_odd);\n    }\n\n    uint32_t result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, product);\n\n    uint32_t res = result_arr[0];\n    int has_odd_result = _mm_movemask_epi8(_mm_cmpgt_epi32(has_odd, zero)) != 0;\n\n    return has_odd_result ? res : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i has_odd = _mm_set1_epi32(0);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n        \n        __m128i vdigit = _mm_set1_epi32(digit);\n        __m128i mask = _mm_and_si128(vdigit, one);\n        __m128i is_odd = _mm_cmpeq_epi32(mask, one);\n        \n        has_odd = _mm_or_si128(has_odd, is_odd);\n        \n        __m128i product_mask = _mm_blendv_epi8(one, vdigit, is_odd);\n        product = _mm_mullo_epi32(product, product_mask);\n    }\n\n    int has_odd_result = _mm_movemask_epi8(_mm_cmpeq_epi32(has_odd, zero)) != 0xFFFF;\n    if (!has_odd_result) return 0;\n\n    uint32_t result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, product);\n    \n    uint64_t result = 1;\n    for (int i = 0; i < 4; i++) {\n        result *= result_arr[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i has_odd = _mm_set1_epi32(0);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i vdigit = _mm_set1_epi32(digit);\n        __m128i mask = _mm_and_si128(vdigit, one);\n        __m128i is_odd = _mm_cmpeq_epi32(mask, one);\n\n        has_odd = _mm_or_si128(has_odd, is_odd);\n\n        __m128i product_part = _mm_blendv_epi8(one, vdigit, is_odd);\n        product = _mm_mullo_epi32(product, product_part);\n    }\n\n    int32_t result_arr[4];\n    _mm_storeu_si128((__m128i*)result_arr, product);\n\n    uint64_t result = 1;\n    for (int i = 0; i < 4; ++i) {\n        result *= result_arr[i];\n    }\n\n    int32_t has_odd_arr[4];\n    _mm_storeu_si128((__m128i*)has_odd_arr, has_odd);\n\n    int any_odd = 0;\n    for (int i = 0; i < 4; ++i) {\n        if (has_odd_arr[i]) {\n            any_odd = 1;\n            break;\n        }\n    }\n\n    return any_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i even_mask = _mm_set1_epi32(1);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i vdigit = _mm_set1_epi32(digit);\n        __m128i is_odd = _mm_and_si128(vdigit, one);\n        __m128i cmp = _mm_cmpeq_epi32(is_odd, one);\n\n        __m128i masked = _mm_and_si128(vdigit, cmp);\n        __m128i masked_nonzero = _mm_cmpeq_epi32(masked, zero);\n        masked_nonzero = _mm_xor_si128(masked_nonzero, _mm_set1_epi32(-1));\n        masked = _mm_and_si128(masked, masked_nonzero);\n\n        __m128i default_one = _mm_or_si128(masked, _mm_andnot_si128(masked_nonzero, one));\n        product = _mm_mullo_epi32(product, default_one);\n    }\n\n    uint32_t result[4];\n    _mm_storeu_si128((__m128i*)result, product);\n\n    uint64_t final_product = 1;\n    for (int i = 0; i < 4; ++i) {\n        final_product *= result[i];\n    }\n\n    return (final_product == 1) ? 0 : final_product;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_ceil_ps(vec);\n        vec = _mm_mul_ps(vec, vec);\n        sum = _mm_add_ps(sum, vec);\n    }\n    \n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, sum);\n    uint64_t result = temp[0] + temp[1] + temp[2] + temp[3];\n    \n    for (; i < size; ++i) {\n        float val = ceilf(lst[i]);\n        result += val * val;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_ceil_ps(vec);\n        vec = _mm_mul_ps(vec, vec);\n        sum = _mm_add_ps(sum, vec);\n    }\n\n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, sum);\n    float result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < lst.size(); ++i) {\n        float val = std::ceil(lst[i]);\n        result += val * val;\n    }\n\n    return static_cast<uint64_t>(result);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_ceil_ps(vec);\n        vec = _mm_mul_ps(vec, vec);\n        sum = _mm_add_ps(sum, vec);\n    }\n    \n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, sum);\n    uint64_t result = temp[0] + temp[1] + temp[2] + temp[3];\n    \n    for (; i < size; ++i) {\n        float val = ceilf(lst[i]);\n        result += val * val;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_ceil_ps(vec);\n        vec = _mm_mul_ps(vec, vec);\n        sum = _mm_add_ps(sum, vec);\n    }\n\n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, sum);\n    uint64_t result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < lst.size(); ++i) {\n        float val = ceilf(lst[i]);\n        result += val * val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_ceil_ps(vec);\n        vec = _mm_mul_ps(vec, vec);\n        sum = _mm_add_ps(sum, vec);\n    }\n    \n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, sum);\n    float result = temp[0] + temp[1] + temp[2] + temp[3];\n    \n    for (; i < size; ++i) {\n        float val = std::ceil(lst[i]);\n        result += val * val;\n    }\n    \n    return static_cast<uint64_t>(result);\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in blocks of 4 using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&arr[i-1]);\n\n        // Compare current elements with previous elements (current <= previous)\n        __m128i cmp = _mm_cmplt_epi32(current, previous);\n\n        // Get the comparison mask\n        int mask = _mm_movemask_epi8(cmp);\n\n        // Check each of the 4 elements\n        if (mask & 0x0000000F) { // First element\n            result = i;\n        }\n        if (mask & 0x000000F0) { // Second element\n            result = i + 1;\n        }\n        if (mask & 0x00000F00) { // Third element\n            result = i + 2;\n        }\n        if (mask & 0x0000F000) { // Fourth element\n            result = i + 3;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] < arr[i-1]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in blocks of 4 using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&arr[i-1]);\n        __m128i cmp = _mm_cmpgt_epi32(previous, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Check each element in the current SIMD block\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j - 1] > arr[i + j]) {\n                    result = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i - 1] > arr[i]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    // Process elements in blocks of 4 using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&arr[i-1]);\n\n        // Compare current <= previous (not greater than or equal)\n        __m128i cmp = _mm_cmplt_epi32(current, previous);\n\n        // Get mask of comparison results\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            // Check each element in the SIMD register\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] < arr[i + j - 1]) {\n                    result = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&arr[i-1]);\n        __m128i cmp = _mm_cmpgt_epi32(previous, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) {\n                    result = i + j;\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i-1] > arr[i]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int result = -1;\n    size_t i = 1;\n    size_t size = arr.size();\n\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&arr[i-1]);\n        __m128i cmp = _mm_cmpgt_epi32(previous, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) {\n                    if (i + j < size && arr[i + j - 1] > arr[i + j]) {\n                        result = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i - 1] > arr[i]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000); // Initialize to smallest possible negative\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF); // Initialize to largest possible positive\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&lst[i]);\n        \n        // Mask for negative numbers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i neg_nums = _mm_and_si128(vec, neg_mask);\n        __m128i neg_candidates = _mm_or_si128(neg_nums, _mm_andnot_si128(neg_mask, max_neg));\n        max_neg = _mm_max_epi32(max_neg, neg_candidates);\n        \n        // Mask for positive numbers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n        __m128i pos_nums = _mm_and_si128(vec, pos_mask);\n        __m128i pos_candidates = _mm_or_si128(pos_nums, _mm_andnot_si128(pos_mask, min_pos));\n        min_pos = _mm_min_epi32(min_pos, pos_candidates);\n    }\n\n    // Handle remaining elements\n    int max_neg_arr[4], min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    int max_negative = 0x80000000;\n    int min_positive = 0x7FFFFFFF;\n    for (int j = 0; j < 4; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_negative) {\n            max_negative = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_positive) {\n            min_positive = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0 && val > max_negative) {\n            max_negative = val;\n        } else if (val > 0 && val < min_positive) {\n            min_positive = val;\n        }\n    }\n\n    if (max_negative == 0x80000000) max_negative = 0;\n    if (min_positive == 0x7FFFFFFF) min_positive = 0;\n\n    return {max_negative, min_positive};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int max_neg = 0;\n    int min_pos = 0;\n    \n    if (lst.empty()) {\n        return {max_neg, min_pos};\n    }\n\n    __m128i max_neg_vec = _mm_set1_epi32(INT_MIN);\n    __m128i min_pos_vec = _mm_set1_epi32(INT_MAX);\n    __m128i zero_vec = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        \n        __m128i neg_mask = _mm_cmplt_epi32(data, zero_vec);\n        __m128i pos_mask = _mm_cmpgt_epi32(data, zero_vec);\n        \n        __m128i neg_values = _mm_and_si128(data, neg_mask);\n        __m128i pos_values = _mm_and_si128(data, pos_mask);\n        \n        max_neg_vec = _mm_max_epi32(max_neg_vec, neg_values);\n        min_pos_vec = _mm_min_epi32(min_pos_vec, _mm_or_si128(pos_values, _mm_andnot_si128(pos_mask, _mm_set1_epi32(INT_MAX))));\n    }\n\n    // Extract results from SIMD registers\n    int max_neg_arr[4];\n    int min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg_vec);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos_vec);\n    \n    for (int j = 0; j < 4; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg) {\n            max_neg = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos) {\n            min_pos = min_pos_arr[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0 && val > max_neg) {\n            max_neg = val;\n        } else if (val > 0 && val < min_pos) {\n            min_pos = val;\n        }\n    }\n\n    return {max_neg != INT_MIN ? max_neg : 0, min_pos != INT_MAX ? min_pos : 0};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000); // Initialize to smallest possible negative\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF); // Initialize to largest possible positive\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&lst[i]);\n        \n        // Mask for negative numbers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i neg_nums = _mm_and_si128(vec, neg_mask);\n        __m128i neg_candidates = _mm_or_si128(neg_nums, _mm_andnot_si128(neg_mask, max_neg));\n        max_neg = _mm_max_epi32(max_neg, neg_candidates);\n        \n        // Mask for positive numbers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n        __m128i pos_nums = _mm_and_si128(vec, pos_mask);\n        __m128i pos_candidates = _mm_or_si128(pos_nums, _mm_andnot_si128(pos_mask, min_pos));\n        min_pos = _mm_min_epi32(min_pos, pos_candidates);\n    }\n\n    // Extract results from SIMD registers\n    int max_neg_arr[4], min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    // Handle remaining elements\n    int max_neg_scalar = 0x80000000;\n    int min_pos_scalar = 0x7FFFFFFF;\n    \n    for (int j = 0; j < 4; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg_scalar) {\n            max_neg_scalar = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos_scalar) {\n            min_pos_scalar = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0 && val > max_neg_scalar) {\n            max_neg_scalar = val;\n        } else if (val > 0 && val < min_pos_scalar) {\n            min_pos_scalar = val;\n        }\n    }\n\n    int a = (max_neg_scalar == 0x80000000) ? 0 : max_neg_scalar;\n    int b = (min_pos_scalar == 0x7FFFFFFF) ? 0 : min_pos_scalar;\n    \n    return {a, b};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int max_neg = 0;\n    int min_pos = 0;\n    __m128i v_max_neg = _mm_set1_epi32(INT_MIN);\n    __m128i v_min_pos = _mm_set1_epi32(INT_MAX);\n    __m128i v_zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        __m128i mask_neg = _mm_cmplt_epi32(v, v_zero);\n        __m128i neg = _mm_and_si128(v, mask_neg);\n        v_max_neg = _mm_max_epi32(v_max_neg, neg);\n        \n        __m128i mask_pos = _mm_cmpgt_epi32(v, v_zero);\n        __m128i pos = _mm_or_si128(v, _mm_andnot_si128(mask_pos, _mm_set1_epi32(INT_MAX)));\n        v_min_pos = _mm_min_epi32(v_min_pos, pos);\n    }\n\n    int max_neg_arr[4], min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, v_max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, v_min_pos);\n\n    for (int j = 0; j < 4; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg) {\n            max_neg = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos) {\n            min_pos = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        int val = lst[i];\n        if (val < 0 && val > max_neg) {\n            max_neg = val;\n        } else if (val > 0 && val < min_pos) {\n            min_pos = val;\n        }\n    }\n\n    if (max_neg == INT_MIN) max_neg = 0;\n    if (min_pos == INT_MAX) min_pos = 0;\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000); // Initialize to smallest possible negative\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF); // Initialize to largest possible positive\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n        \n        __m128i neg_values = _mm_and_si128(vec, neg_mask);\n        __m128i pos_values = _mm_and_si128(vec, pos_mask);\n        \n        __m128i neg_max_candidate = _mm_or_si128(neg_values, _mm_andnot_si128(neg_mask, max_neg));\n        __m128i pos_min_candidate = _mm_or_si128(pos_values, _mm_andnot_si128(pos_mask, min_pos));\n        \n        max_neg = _mm_max_epi32(max_neg, neg_max_candidate);\n        min_pos = _mm_min_epi32(min_pos, pos_min_candidate);\n    }\n\n    int max_neg_arr[4], min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    int max_neg_result = 0x80000000;\n    int min_pos_result = 0x7FFFFFFF;\n    \n    for (int j = 0; j < 4; ++j) {\n        if (max_neg_arr[j] < 0 && max_neg_arr[j] > max_neg_result) {\n            max_neg_result = max_neg_arr[j];\n        }\n        if (min_pos_arr[j] > 0 && min_pos_arr[j] < min_pos_result) {\n            min_pos_result = min_pos_arr[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > max_neg_result) {\n            max_neg_result = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < min_pos_result) {\n            min_pos_result = lst[i];\n        }\n    }\n\n    if (max_neg_result == 0x80000000) max_neg_result = 0;\n    if (min_pos_result == 0x7FFFFFFF) min_pos_result = 0;\n\n    return {max_neg_result, min_pos_result};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t partial_fact = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        partial_fact *= i;\n        result *= partial_fact;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    const int size = lst.size();\n    const int aligned_size = size & ~3;\n\n    for (int i = 0; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_set_epi32(\n            (i+3) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i+2) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i+1) % 3 == 0 ? 0xFFFFFFFF : 0,\n            i % 3 == 0 ? 0xFFFFFFFF : 0\n        );\n        __m128i mask4 = _mm_set_epi32(\n            ((i+3) % 4 == 0 && (i+3) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i+2) % 4 == 0 && (i+2) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i+1) % 4 == 0 && (i+1) % 3 != 0) ? 0xFFFFFFFF : 0,\n            (i % 4 == 0 && i % 3 != 0) ? 0xFFFFFFFF : 0\n        );\n\n        __m128i square = _mm_mullo_epi32(vec, vec);\n        __m128i cube = _mm_mullo_epi32(square, vec);\n\n        __m128i res = _mm_blendv_epi8(\n            _mm_blendv_epi8(vec, cube, mask4),\n            square, mask3\n        );\n\n        sum = _mm_add_epi32(sum, res);\n    }\n\n    int result = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    const int size = lst.size();\n    const int aligned_size = size - (size % 4);\n\n    for (int i = 0; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_set_epi32(\n            (i+3) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i+2) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i+1) % 3 == 0 ? 0xFFFFFFFF : 0,\n            i % 3 == 0 ? 0xFFFFFFFF : 0\n        );\n        __m128i mask4 = _mm_set_epi32(\n            ((i+3) % 4 == 0 && (i+3) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i+2) % 4 == 0 && (i+2) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i+1) % 4 == 0 && (i+1) % 3 != 0) ? 0xFFFFFFFF : 0,\n            (i % 4 == 0 && i % 3 != 0) ? 0xFFFFFFFF : 0\n        );\n\n        __m128i square = _mm_mullo_epi32(vec, vec);\n        __m128i cube = _mm_mullo_epi32(square, vec);\n        \n        __m128i res = _mm_blendv_epi8(\n            _mm_blendv_epi8(vec, cube, mask4),\n            square, mask3\n        );\n        \n        sum = _mm_add_epi32(sum, res);\n    }\n\n    int result = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    for (int i = 0; i < 4; ++i) {\n        result += temp[i];\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    const int size = lst.size();\n    int i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i mask3 = _mm_set_epi32(\n            (i + 3) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i + 2) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i + 1) % 3 == 0 ? 0xFFFFFFFF : 0,\n            i % 3 == 0 ? 0xFFFFFFFF : 0\n        );\n        __m128i mask4 = _mm_set_epi32(\n            ((i + 3) % 4 == 0 && (i + 3) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i + 2) % 4 == 0 && (i + 2) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i + 1) % 4 == 0 && (i + 1) % 3 != 0) ? 0xFFFFFFFF : 0,\n            (i % 4 == 0 && i % 3 != 0) ? 0xFFFFFFFF : 0\n        );\n\n        __m128i square = _mm_mullo_epi32(vec, vec);\n        __m128i cube = _mm_mullo_epi32(square, vec);\n\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(mask3, square),\n            _mm_andnot_si128(mask3, vec)\n        );\n        res = _mm_or_si128(\n            _mm_and_si128(mask4, cube),\n            _mm_andnot_si128(mask4, res)\n        );\n\n        sum = _mm_add_epi32(sum, res);\n    }\n\n    // Handle remaining elements\n    int remaining_sum = 0;\n    for (; i < size; ++i) {\n        if (i % 3 == 0) {\n            remaining_sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            remaining_sum += lst[i] * lst[i] * lst[i];\n        } else {\n            remaining_sum += lst[i];\n        }\n    }\n\n    // Sum the SIMD vector\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum);\n    int simd_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return simd_sum + remaining_sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    const int size = lst.size();\n    int i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_set_epi32(\n            (i+3) % 3 == 0 ? 1 : 0,\n            (i+2) % 3 == 0 ? 1 : 0,\n            (i+1) % 3 == 0 ? 1 : 0,\n            (i) % 3 == 0 ? 1 : 0\n        );\n        __m128i mask4 = _mm_set_epi32(\n            ((i+3) % 4 == 0 && (i+3) % 3 != 0) ? 1 : 0,\n            ((i+2) % 4 == 0 && (i+2) % 3 != 0) ? 1 : 0,\n            ((i+1) % 4 == 0 && (i+1) % 3 != 0) ? 1 : 0,\n            ((i) % 4 == 0 && (i) % 3 != 0) ? 1 : 0\n        );\n\n        __m128i square = _mm_mullo_epi32(vec, vec);\n        __m128i cube = _mm_mullo_epi32(square, vec);\n\n        __m128i res = _mm_blendv_epi8(\n            _mm_blendv_epi8(vec, cube, mask4),\n            square, mask3\n        );\n\n        sum = _mm_add_epi32(sum, res);\n    }\n\n    int result = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    const int size = lst.size();\n    const int aligned_size = size - (size % 4);\n\n    for (int i = 0; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_set_epi32(\n            (i + 3) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i + 2) % 3 == 0 ? 0xFFFFFFFF : 0,\n            (i + 1) % 3 == 0 ? 0xFFFFFFFF : 0,\n            i % 3 == 0 ? 0xFFFFFFFF : 0\n        );\n        __m128i mask4 = _mm_set_epi32(\n            ((i + 3) % 4 == 0 && (i + 3) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i + 2) % 4 == 0 && (i + 2) % 3 != 0) ? 0xFFFFFFFF : 0,\n            ((i + 1) % 4 == 0 && (i + 1) % 3 != 0) ? 0xFFFFFFFF : 0,\n            (i % 4 == 0 && i % 3 != 0) ? 0xFFFFFFFF : 0\n        );\n\n        __m128i square = _mm_mullo_epi32(vec, vec);\n        __m128i cube = _mm_mullo_epi32(square, vec);\n\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(mask3, square),\n            _mm_andnot_si128(mask3, \n                _mm_or_si128(\n                    _mm_and_si128(mask4, cube),\n                    _mm_andnot_si128(mask4, vec)\n                )\n            )\n        );\n\n        sum = _mm_add_epi32(sum, res);\n    }\n\n    int result = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = nums.size();\n    \n    __m128i v_ten = _mm_set1_epi32(10);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_result = _mm_setzero_si128();\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Check if greater than 10\n        __m128i v_gt_ten = _mm_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values for digit checks\n        __m128i v_abs = _mm_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m128i v_last_digit = _mm_sub_epi32(v_abs, _mm_mullo_epi32(_mm_div_epi32(v_abs, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n        \n        // Check if last digit is odd\n        __m128i v_last_odd = _mm_and_si128(v_last_digit, v_one);\n        v_last_odd = _mm_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m128i v_first = v_abs;\n        __m128i v_divisor = _mm_set1_epi32(10);\n        while (true) {\n            __m128i v_next = _mm_div_epi32(v_first, v_divisor);\n            __m128i v_cmp = _mm_cmpeq_epi32(v_next, v_zero);\n            if (_mm_movemask_epi8(v_cmp) == 0xFFFF) break;\n            v_first = v_next;\n        }\n        __m128i v_first_digit = _mm_sub_epi32(v_first, _mm_mullo_epi32(_mm_div_epi32(v_first, v_divisor), v_divisor));\n        \n        // Check if first digit is odd\n        __m128i v_first_odd = _mm_and_si128(v_first_digit, v_one);\n        v_first_odd = _mm_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine all conditions\n        __m128i v_cond = _mm_and_si128(v_gt_ten, _mm_and_si128(v_last_odd, v_first_odd));\n        \n        // Accumulate results\n        v_result = _mm_add_epi32(v_result, _mm_and_si128(v_cond, v_one));\n    }\n    \n    // Sum the results from SIMD registers\n    alignas(16) int result_arr[4];\n    _mm_store_si128((__m128i*)result_arr, v_result);\n    count += result_arr[0] + result_arr[1] + result_arr[2] + result_arr[3];\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = nums.size();\n    \n    __m128i v_ten = _mm_set1_epi32(10);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_mask = _mm_set1_epi32(1);\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Check if greater than 10\n        __m128i v_gt_ten = _mm_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute value for digit checks\n        __m128i v_abs = _mm_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m128i v_last_digit = _mm_sub_epi32(v_abs, _mm_mullo_epi32(_mm_div_epi32(v_abs, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n        __m128i v_last_odd = _mm_and_si128(v_last_digit, v_one);\n        __m128i v_last_odd_mask = _mm_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m128i v_first = v_abs;\n        __m128i v_divisor = _mm_set1_epi32(10);\n        while (1) {\n            __m128i v_next = _mm_div_epi32(v_first, v_divisor);\n            __m128i v_cmp = _mm_cmpeq_epi32(v_next, v_zero);\n            if (_mm_movemask_epi8(v_cmp) == 0xFFFF) break;\n            v_first = v_next;\n        }\n        __m128i v_first_odd = _mm_and_si128(v_first, v_one);\n        __m128i v_first_odd_mask = _mm_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine all conditions\n        __m128i v_combined = _mm_and_si128(v_gt_ten, _mm_and_si128(v_last_odd_mask, v_first_odd_mask));\n        \n        // Count the number of elements that meet all conditions\n        count += _mm_popcnt_u32(_mm_movemask_ps(_mm_castsi128_ps(v_combined)));\n    }\n    \n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = nums.size();\n    __m128i v_ten = _mm_set1_epi32(10);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_mask = _mm_set1_epi32(1);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Check if greater than 10\n        __m128i v_cmp = _mm_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute values\n        __m128i v_abs = _mm_abs_epi32(v_nums);\n        \n        // Extract last digit (num % 10)\n        __m128i v_last_digit = _mm_sub_epi32(v_abs, _mm_mullo_epi32(_mm_div_epi32(v_abs, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n        \n        // Check if last digit is odd\n        __m128i v_last_odd = _mm_and_si128(v_last_digit, v_one);\n        v_last_odd = _mm_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Extract first digit\n        __m128i v_first = v_abs;\n        __m128i v_div = _mm_set1_epi32(10);\n        while (true) {\n            __m128i v_next = _mm_div_epi32(v_first, v_div);\n            __m128i v_cmp_zero = _mm_cmplt_epi32(v_next, v_div);\n            if (_mm_movemask_epi8(v_cmp_zero) == 0xFFFF) break;\n            v_first = v_next;\n        }\n        __m128i v_first_digit = _mm_sub_epi32(v_first, _mm_mullo_epi32(_mm_div_epi32(v_first, _mm_set1_epi32(10)), _mm_set1_epi32(10));\n        \n        // Check if first digit is odd\n        __m128i v_first_odd = _mm_and_si128(v_first_digit, v_one);\n        v_first_odd = _mm_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m128i v_cond = _mm_and_si128(v_cmp, _mm_and_si128(v_first_odd, v_last_odd));\n        \n        // Count number of set bits\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(v_cond));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    __m128i v_ten = _mm_set1_epi32(10);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_mask = _mm_set1_epi32(1);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Check if greater than 10\n        __m128i v_gt_ten = _mm_cmpgt_epi32(v_nums, v_ten);\n        \n        // Get absolute value for digit checks\n        __m128i v_abs = _mm_abs_epi32(v_nums);\n        \n        // Get last digit (num % 10)\n        __m128i v_last_digit = _mm_sub_epi32(v_abs, _mm_mullo_epi32(_mm_div_epi32(v_abs, _mm_set1_epi32(10)), _mm_set1_epi32(10)));\n        __m128i v_last_odd = _mm_and_si128(v_last_digit, v_one);\n        __m128i v_last_odd_mask = _mm_cmpeq_epi32(v_last_odd, v_one);\n        \n        // Get first digit\n        __m128i v_tmp = v_abs;\n        __m128i v_first_digit = v_tmp;\n        while (1) {\n            __m128i v_div = _mm_div_epi32(v_tmp, _mm_set1_epi32(10));\n            __m128i v_cmp = _mm_cmpeq_epi32(v_div, v_zero);\n            if (_mm_movemask_epi8(v_cmp) == 0xFFFF) break;\n            v_first_digit = v_div;\n            v_tmp = v_div;\n        }\n        __m128i v_first_odd = _mm_and_si128(v_first_digit, v_one);\n        __m128i v_first_odd_mask = _mm_cmpeq_epi32(v_first_odd, v_one);\n        \n        // Combine conditions\n        __m128i v_cond = _mm_and_si128(v_gt_ten, _mm_and_si128(v_last_odd_mask, v_first_odd_mask));\n        count += _mm_popcnt_u32(_mm_movemask_ps(_mm_castsi128_ps(v_cond)));\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = nums.size();\n    __m128i v_ten = _mm_set1_epi32(10);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_result = _mm_setzero_si128();\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i v_gt_ten = _mm_cmpgt_epi32(v_nums, v_ten);\n\n        // Extract first and last digits\n        __m128i v_abs = _mm_abs_epi32(v_nums);\n        __m128i v_last_digit = _mm_and_si128(v_abs, _mm_set1_epi32(0x1));\n\n        // Compute first digit\n        __m128i v_first_digit = v_abs;\n        __m128i v_mask = _mm_set1_epi32(0xFFFFFFFF);\n        __m128i v_div = _mm_set1_epi32(10);\n        while (1) {\n            __m128i v_cmp = _mm_cmpgt_epi32(v_first_digit, _mm_set1_epi32(9));\n            if (_mm_test_all_zeros(v_cmp, v_mask)) break;\n            v_first_digit = _mm_div_epi32(v_first_digit, v_div);\n        }\n        v_first_digit = _mm_and_si128(v_first_digit, _mm_set1_epi32(0x1));\n\n        // Check if both first and last digits are odd\n        __m128i v_both_odd = _mm_and_si128(v_first_digit, v_last_digit);\n        v_both_odd = _mm_and_si128(v_both_odd, v_gt_ten);\n\n        // Accumulate results\n        v_result = _mm_add_epi32(v_result, _mm_and_si128(v_both_odd, v_one));\n    }\n\n    // Sum the results from SIMD registers\n    alignas(16) int result_arr[4];\n    _mm_store_si128((__m128i*)result_arr, v_result);\n    count += result_arr[0] + result_arr[1] + result_arr[2] + result_arr[3];\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = abs_num;\n                while (first_digit > 9) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t mod0 = 0, mod1 = 0, mod2 = 0;\n\n    __m128i v_i, v_sq, v_res, v_mod;\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n; i += 4) {\n        v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        v_sq = _mm_mullo_epi32(v_i, v_i);\n        v_res = _mm_sub_epi32(v_sq, v_i);\n        v_res = _mm_add_epi32(v_res, v_one);\n        v_mod = _mm_rem_epu32(v_res, v_three);\n\n        alignas(16) uint32_t mods[4];\n        _mm_store_si128((__m128i*)mods, v_mod);\n\n        for (int j = 0; j < 4 && (i + j) <= n; ++j) {\n            switch (mods[j]) {\n                case 0: mod0++; break;\n                case 1: mod1++; break;\n                case 2: mod2++; break;\n            }\n        }\n    }\n\n    count += mod0 * mod1 * mod2;\n    count += mod0 * (mod0 - 1) * (mod0 - 2) / 6;\n    count += mod1 * (mod1 - 1) * (mod1 - 2) / 6;\n    count += mod2 * (mod2 - 1) * (mod2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t mod0 = 0, mod1 = 0, mod2 = 0;\n\n    __m128i v_i, v_sq, v_res, v_mod;\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n; i += 4) {\n        v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        v_sq = _mm_mullo_epi32(v_i, v_i);\n        v_res = _mm_sub_epi32(v_sq, v_i);\n        v_res = _mm_add_epi32(v_res, v_one);\n        v_mod = _mm_rem_epu32(v_res, v_three);\n\n        uint32_t mods[4];\n        _mm_storeu_si128((__m128i*)mods, v_mod);\n\n        for (int j = 0; j < 4 && (i + j) <= n; ++j) {\n            switch (mods[j]) {\n                case 0: mod0++; break;\n                case 1: mod1++; break;\n                case 2: mod2++; break;\n            }\n        }\n    }\n\n    count += mod0 * mod1 * mod2;\n    count += mod0 * (mod0 - 1) * (mod0 - 2) / 6;\n    count += mod1 * (mod1 - 1) * (mod1 - 2) / 6;\n    count += mod2 * (mod2 - 1) * (mod2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t mod0 = 0, mod1 = 0, mod2 = 0;\n\n    __m128i v_i, v_sq, v_res, v_mod;\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n; i += 4) {\n        v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        v_sq = _mm_mullo_epi32(v_i, v_i);\n        v_res = _mm_sub_epi32(v_sq, v_i);\n        v_res = _mm_add_epi32(v_res, v_one);\n        v_mod = _mm_rem_epu32(v_res, v_three);\n\n        alignas(16) uint32_t mods[4];\n        _mm_store_si128((__m128i*)mods, v_mod);\n\n        for (int j = 0; j < 4 && (i + j) <= n; ++j) {\n            switch (mods[j]) {\n                case 0: mod0++; break;\n                case 1: mod1++; break;\n                case 2: mod2++; break;\n            }\n        }\n    }\n\n    count += mod0 * mod1 * mod2;\n    count += mod0 * (mod0 - 1) * (mod0 - 2) / 6;\n    count += mod1 * (mod1 - 1) * (mod1 - 2) / 6;\n    count += mod2 * (mod2 - 1) * (mod2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t mod0 = 0, mod1 = 0, mod2 = 0;\n\n    __m128i v_i, v_sq, v_res, v_mod;\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n; i += 4) {\n        v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        v_sq = _mm_mullo_epi32(v_i, v_i);\n        v_res = _mm_sub_epi32(v_sq, v_i);\n        v_res = _mm_add_epi32(v_res, v_one);\n        v_mod = _mm_rem_epu32(v_res, v_three);\n\n        alignas(16) uint32_t mods[4];\n        _mm_store_si128((__m128i*)mods, v_mod);\n\n        for (int j = 0; j < 4 && (i + j) <= n; ++j) {\n            switch (mods[j]) {\n                case 0: mod0++; break;\n                case 1: mod1++; break;\n                case 2: mod2++; break;\n            }\n        }\n    }\n\n    count += mod0 * mod1 * mod2;\n    count += mod0 * (mod0 - 1) * (mod0 - 2) / 6;\n    count += mod1 * (mod1 - 1) * (mod1 - 2) / 6;\n    count += mod2 * (mod2 - 1) * (mod2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t count = 0;\n    uint64_t mod0 = 0, mod1 = 0, mod2 = 0;\n\n    __m128i v_i, v_sq, v_res, v_mod;\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    for (uint64_t i = 1; i <= n; i += 4) {\n        v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        v_sq = _mm_mullo_epi32(v_i, v_i);\n        v_res = _mm_sub_epi32(v_sq, v_i);\n        v_res = _mm_add_epi32(v_res, v_one);\n        v_mod = _mm_rem_epu32(v_res, v_three);\n\n        alignas(16) uint32_t mods[4];\n        _mm_store_si128((__m128i*)mods, v_mod);\n\n        for (int j = 0; j < 4 && (i + j) <= n; ++j) {\n            switch (mods[j]) {\n                case 0: mod0++; break;\n                case 1: mod1++; break;\n                case 2: mod2++; break;\n            }\n        }\n    }\n\n    count += mod0 * mod1 * mod2;\n    count += mod0 * (mod0 - 1) * (mod0 - 2) / 6;\n    count += mod1 * (mod1 - 1) * (mod1 - 2) / 6;\n    count += mod2 * (mod2 - 1) * (mod2 - 2) / 6;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n <= 3) {\n        return x;\n    }\n    if (n % 2 == 0 || n % 3 == 0) {\n        return y;\n    }\n\n    __m128i divisors = _mm_set_epi32(5, 7, 11, 13);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i remainders = _mm_rem_epu32(n_vec, divisors);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n    int mask = _mm_movemask_epi8(cmp);\n\n    if (mask != 0) {\n        return y;\n    }\n\n    for (int i = 17; i * i <= n; i += 30) {\n        __m128i inc = _mm_set_epi32(i + 24, i + 12, i + 6, i);\n        __m128i rem = _mm_rem_epu32(n_vec, inc);\n        cmp = _mm_cmpeq_epi32(rem, zero);\n        mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n <= 3) {\n        return x;\n    }\n    if (n % 2 == 0 || n % 3 == 0) {\n        return y;\n    }\n\n    __m128i divisors = _mm_set_epi32(5, 7, 11, 13);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i zeros = _mm_setzero_si128();\n\n    for (int i = 5; i * i <= n; i += 30) {\n        __m128i i_vec = _mm_set_epi32(i + 24, i + 18, i + 12, i + 6);\n        __m128i rem = _mm_rem_epi32(n_vec, i_vec);\n        __m128i cmp = _mm_cmpeq_epi32(rem, zeros);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n\n        i_vec = _mm_set_epi32(i + 28, i + 22, i + 16, i + 10);\n        rem = _mm_rem_epi32(n_vec, i_vec);\n        cmp = _mm_cmpeq_epi32(rem, zeros);\n        mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i remainders = _mm_rem_epu32(n_vec, divisors);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n\n    int mask = _mm_movemask_epi8(cmp);\n    if (mask != 0) {\n        return y;\n    }\n\n    for (int i = 11; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n\n    __m128i divisors = _mm_set_epi32(3, 5, 7, 9);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i zeros = _mm_setzero_si128();\n    __m128i rem = _mm_rem_epi32(n_vec, divisors);\n    __m128i cmp = _mm_cmpeq_epi32(rem, zeros);\n    int mask = _mm_movemask_epi8(cmp);\n\n    if (mask != 0) {\n        return y;\n    }\n\n    for (int i = 11; i * i <= n; i += 30) {\n        __m128i inc = _mm_set_epi32(i, i+6, i+12, i+18);\n        rem = _mm_rem_epi32(n_vec, inc);\n        cmp = _mm_cmpeq_epi32(rem, zeros);\n        mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n\n        inc = _mm_set_epi32(i+24, 0, 0, 0);\n        rem = _mm_rem_epi32(n_vec, inc);\n        cmp = _mm_cmpeq_epi32(rem, zeros);\n        mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) {\n        return y;\n    }\n    if (n <= 3) {\n        return x;\n    }\n    if (n % 2 == 0 || n % 3 == 0) {\n        return y;\n    }\n\n    __m128i divisors = _mm_set_epi32(5, 7, 11, 13);\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i remainders = _mm_rem_epi32(n_vec, divisors);\n\n    int is_prime = 1;\n    for (int i = 5; i * i <= n; i += 6) {\n        __m128i i_vec = _mm_set1_epi32(i);\n        __m128i i_plus_2 = _mm_set1_epi32(i + 2);\n        \n        __m128i rem1 = _mm_rem_epi32(n_vec, i_vec);\n        __m128i rem2 = _mm_rem_epi32(n_vec, i_plus_2);\n        \n        int mask1 = _mm_movemask_epi8(_mm_cmpeq_epi32(rem1, _mm_setzero_si128()));\n        int mask2 = _mm_movemask_epi8(_mm_cmpeq_epi32(rem2, _mm_setzero_si128()));\n        \n        if (mask1 || mask2) {\n            is_prime = 0;\n            break;\n        }\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    const __m128 zero = _mm_setzero_ps();\n    const __m128i one = _mm_set1_epi32(1);\n    const __m128i mask_sign = _mm_set1_epi32(0x80000000);\n    const __m128i mask_float = _mm_set1_epi32(0x4B000000);\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i ivec = _mm_cvttps_epi32(vec);\n        __m128 converted = _mm_cvtepi32_ps(ivec);\n        __m128 mask_eq = _mm_cmpeq_ps(vec, converted);\n        __m128 mask_neg = _mm_cmplt_ps(vec, zero);\n        __m128 mask_valid = _mm_andnot_ps(mask_neg, mask_eq);\n\n        __m128i ivec_odd = _mm_and_si128(ivec, one);\n        __m128 mask_odd = _mm_castsi128_ps(_mm_cmpeq_epi32(ivec_odd, one));\n        __m128 mask_final = _mm_and_ps(mask_valid, mask_odd);\n\n        __m128 squared = _mm_mul_ps(vec, vec);\n        __m128 masked_result = _mm_and_ps(squared, mask_final);\n\n        __m128i res_i = _mm_cvttps_epi32(masked_result);\n        sum = _mm_add_epi64(sum, _mm_add_epi64(_mm_cvtepi32_epi64(res_i), _mm_cvtepi32_epi64(_mm_shuffle_epi32(res_i, _MM_SHUFFLE(2, 3, 0, 1)))));\n    }\n\n    uint64_t result = _mm_extract_epi64(sum, 0) + _mm_extract_epi64(sum, 1);\n\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val && (int)val % 2 != 0) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    const __m128 zero = _mm_setzero_ps();\n    const __m128i one = _mm_set1_epi32(1);\n    const __m128i mask_sign = _mm_set1_epi32(0x80000000);\n    const __m128i mask_float = _mm_set1_epi32(0x4B000000);\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        \n        // Check for negative numbers (sign bit set)\n        __m128i vec_int = _mm_castps_si128(vec);\n        __m128i is_negative = _mm_and_si128(vec_int, mask_sign);\n        __m128i not_negative = _mm_cmpeq_epi32(is_negative, _mm_setzero_si128());\n        \n        // Check if numbers are integers (fractional part is zero)\n        __m128i truncated = _mm_cvttps_epi32(vec);\n        __m128 truncated_float = _mm_cvtepi32_ps(truncated);\n        __m128 is_equal = _mm_cmpeq_ps(vec, truncated_float);\n        __m128i is_integer = _mm_castps_si128(is_equal);\n        \n        // Check if numbers are odd (bit 0 set)\n        __m128i truncated_odd = _mm_and_si128(truncated, one);\n        __m128i is_odd = _mm_cmpeq_epi32(truncated_odd, one);\n        \n        // Combine conditions: not negative, integer, and odd\n        __m128i mask = _mm_and_si128(not_negative, _mm_and_si128(is_integer, is_odd));\n        \n        // Square the numbers and mask\n        __m128 squared = _mm_mul_ps(vec, vec);\n        __m128i squared_int = _mm_cvttps_epi32(squared);\n        squared_int = _mm_and_si128(squared_int, mask);\n        \n        // Accumulate the sum\n        sum = _mm_add_epi64(sum, _mm_srli_epi64(_mm_mul_epu32(squared_int, _mm_set1_epi32(1)), 0);\n    }\n\n    // Sum the SIMD lanes\n    uint64_t result = _mm_extract_epi64(sum, 0) + _mm_extract_epi64(sum, 1);\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        float num = lst[i];\n        if (num >= 0 && num == (int)num && (int)num % 2 != 0) {\n            result += (int)num * (int)num;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i mask_positive = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask_integer = _mm_set1_epi32(0xFFFFFFFF);\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i int_vec = _mm_cvtps_epi32(vec);\n        __m128 reconverted = _mm_cvtepi32_ps(int_vec);\n        __m128 is_integer = _mm_cmpeq_ps(vec, reconverted);\n        __m128 is_positive = _mm_cmpge_ps(vec, _mm_setzero_ps());\n        __m128 mask = _mm_and_ps(is_integer, is_positive);\n        __m128i int_mask = _mm_castps_si128(mask);\n\n        __m128i and_result = _mm_and_si128(int_vec, int_mask);\n        __m128i and_one = _mm_and_si128(and_result, one);\n        __m128i is_odd = _mm_cmpeq_epi32(and_one, one);\n        __m128i odd_mask = _mm_and_si128(is_odd, int_mask);\n\n        __m128i squared = _mm_mullo_epi32(and_result, and_result);\n        __m128i masked_squared = _mm_and_si128(squared, odd_mask);\n\n        sum = _mm_add_epi32(sum, masked_squared);\n    }\n\n    uint32_t temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    uint64_t result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val) {\n            int int_val = (int)val;\n            if (int_val % 2 != 0) {\n                result += int_val * int_val;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i mask_neg = _mm_set1_epi32(0x80000000);\n    __m128i mask_int = _mm_set1_epi32(0x3F800000);\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i ivec = _mm_cvtps_epi32(vec);\n        __m128 converted = _mm_cvtepi32_ps(ivec);\n        __m128 is_int = _mm_cmpeq_ps(vec, converted);\n\n        __m128i is_neg = _mm_castps_si128(_mm_cmplt_ps(vec, _mm_setzero_ps()));\n        __m128i is_odd = _mm_and_si128(ivec, one);\n        is_odd = _mm_cmpeq_epi32(is_odd, one);\n\n        __m128i valid = _mm_and_si128(_mm_castps_si128(is_int), \n                _mm_andnot_si128(is_neg, is_odd));\n        __m128i squared = _mm_mullo_epi32(ivec, ivec);\n        squared = _mm_and_si128(squared, valid);\n\n        sum = _mm_add_epi32(sum, squared);\n    }\n\n    uint64_t result = 0;\n    alignas(16) int32_t temp[4];\n    _mm_store_si128((__m128i*)temp, sum);\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j];\n    }\n\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val && (int)val % 2 == 1) {\n            result += (int)val * (int)val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i mask_positive = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask_float = _mm_set1_epi32(0x3F800000);\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i ivec = _mm_cvtps_epi32(vec);\n        __m128i is_float = _mm_castps_si128(_mm_cmpeq_ps(_mm_cvtepi32_ps(ivec), vec));\n        __m128i is_positive = _mm_cmpgt_epi32(_mm_and_si128(ivec, mask_positive), zero);\n        __m128i is_odd = _mm_and_si128(ivec, one);\n        __m128i mask = _mm_and_si128(_mm_and_si128(is_float, is_positive), is_odd);\n        \n        __m128i squared = _mm_mullo_epi32(ivec, ivec);\n        __m128i masked_result = _mm_and_si128(squared, mask);\n        \n        sum = _mm_add_epi32(sum, masked_result);\n    }\n\n    uint64_t result = 0;\n    alignas(16) int32_t temp[4];\n    _mm_store_si128((__m128i*)temp, sum);\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j];\n    }\n\n    for (; i < lst.size(); ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val && (int)val % 2 == 1) {\n            result += (int)val * (int)val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < size; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    size_t i = 0;\n    for (; i + 4 <= game.size(); i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n    for (; i < game.size(); ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n    return result;\n}\n```"}

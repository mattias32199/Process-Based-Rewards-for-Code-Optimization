{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        const bool* mask_ptr = mask + i;\n        const int64_t* src_ptr = src + i;\n        int64_t* dst_ptr = dst + i;\n        \n        uint32_t m = (uint32_t)mask_ptr[0] \n                   | ((uint32_t)mask_ptr[1] << 8) \n                   | ((uint32_t)mask_ptr[2] << 16) \n                   | ((uint32_t)mask_ptr[3] << 24);\n        \n        __m128i mask8 = _mm_cvtsi32_si128(m);\n        __m128i mask_compare = _mm_cmpgt_epi8(mask8, _mm_setzero_si128());\n        __m256i mask_vector = _mm256_cvtepi8_epi64(mask_compare);\n        mask_vector = _mm256_and_si256(mask_vector, _mm256_set1_epi64x(0x8000000000000000));\n        \n        __m256i data = _mm256_loadu_si256((const __m256i*)src_ptr);\n        _mm256_maskstore_epi64(dst_ptr, mask_vector, data);\n    }\n    \n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    const __m128i zero = _mm_setzero_si128();\n    for (; i + 4 <= length; i += 4) {\n        __m128i mask_bytes = _mm_loadu_si32((const void*)(mask + i));\n        __m128i mask_nonzero = _mm_cmpneq_epi8(mask_bytes, zero);\n        __m128i mask_32 = _mm_cvtepi8_epi32(mask_nonzero);\n        __m256i mask_vec = _mm256_cvtepi32_epi64(mask_32);\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        _mm256_maskstore_epi64(dst + i, mask_vec, src_vec);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i mask_32 = _mm_loadu_si32((const void*)(mask + i));\n        __m256i mask_vec = _mm256_cvtepi8_epi64(mask_32);\n        __m256i mask_zero = _mm256_cmpeq_epi64(mask_vec, _mm256_setzero_si256());\n        __m256i mask_cmp = _mm256_xor_si256(mask_zero, _mm256_set1_epi64x(-1));\n        \n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((const __m256i*)(dst + i));\n        __m256i res = _mm256_blendv_epi8(dst_vec, src_vec, mask_cmp);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n    \n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stdint.h>\n#include <cstring>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        uint32_t m;\n        memcpy(&m, mask + i, sizeof(uint32_t));\n\n        int64_t mask0 = -((m & 0x000000FF) != 0);\n        int64_t mask1 = -((m & 0x0000FF00) != 0);\n        int64_t mask2 = -((m & 0x00FF0000) != 0);\n        int64_t mask3 = -((m & 0xFF000000) != 0);\n\n        __m256i mask_vec = _mm256_set_epi64x(mask3, mask2, mask1, mask0);\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        _mm256_maskstore_epi64(dst + i, mask_vec, src_vec);\n    }\n\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i all_ones = _mm_set1_epi8(-1);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i mask8 = _mm_cvtsi32_si128(*(const int*)(mask + i));\n        __m128i is_zero = _mm_cmpeq_epi8(mask8, _mm_setzero_si128());\n        __m128i mask8_nonzero = _mm_xor_si128(is_zero, all_ones);\n        __m128i mask32 = _mm_cvtepi8_epi32(mask8_nonzero);\n        __m256i mask_store = _mm256_cvtepi32_epi64(mask32);\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        _mm256_maskstore_epi64(dst + i, mask_store, src_vec);\n    }\n\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i scale_vec = _mm256_set1_epi32(scale);\n        __m256i A = _mm256_mullo_epi32(data, scale_vec);\n        __m256i A_squared = _mm256_mullo_epi32(A, A);\n        __m256i sum = _mm256_add_epi32(A, A_squared);\n        \n        __m128i low_sum = _mm256_castsi256_si128(sum);\n        __m128i high_sum = _mm256_extractf128_si256(sum, 1);\n        \n        __m256d dlow = _mm256_cvtepi32_pd(low_sum);\n        __m256d dhigh = _mm256_cvtepi32_pd(high_sum);\n        \n        __m256d div7 = _mm256_set1_pd(7.0);\n        dlow = _mm256_div_pd(dlow, div7);\n        dhigh = _mm256_div_pd(dhigh, div7);\n        \n        __m128i res_low = _mm256_cvtpd_epi32(dlow);\n        __m128i res_high = _mm256_cvtpd_epi32(dhigh);\n        \n        __m256i result = _mm256_set_m128i(res_high, res_low);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        int A = src[i] * scale;\n        int tmp1 = A >> 3;\n        int tmp2 = A << 3;\n        int sum_val = A + tmp1 * tmp2;\n        dst[i] = sum_val / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((const __m256i *)(src + i));\n        __m256i product = _mm256_mullo_epi32(v_src, v_scale);\n        __m256i tmp1 = _mm256_srai_epi32(product, 3);\n        __m256i tmp2 = _mm256_slli_epi32(product, 3);\n        __m256i tmp_prod = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i numerator = _mm256_add_epi32(product, tmp_prod);\n\n        int n0 = _mm256_extract_epi32(numerator, 0) / 7;\n        int n1 = _mm256_extract_epi32(numerator, 1) / 7;\n        int n2 = _mm256_extract_epi32(numerator, 2) / 7;\n        int n3 = _mm256_extract_epi32(numerator, 3) / 7;\n        int n4 = _mm256_extract_epi32(numerator, 4) / 7;\n        int n5 = _mm256_extract_epi32(numerator, 5) / 7;\n        int n6 = _mm256_extract_epi32(numerator, 6) / 7;\n        int n7 = _mm256_extract_epi32(numerator, 7) / 7;\n\n        __m256i result = _mm256_set_epi32(n7, n6, n5, n4, n3, n2, n1, n0);\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        int element = src[i];\n        int64_t product = (int64_t)element * scale;\n        int64_t tmp1 = product >> 3;\n        int64_t tmp2 = product << 3;\n        int64_t numerator = product + tmp1 * tmp2;\n        dst[i] = (int)(numerator / 7);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i mul_scale = _mm256_mullo_epi32(data, scale_vec);\n        __m256i tmp1 = _mm256_srai_epi32(mul_scale, 3);\n        __m256i tmp2 = _mm256_slli_epi32(mul_scale, 3);\n        __m256i product = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(mul_scale, product);\n\n        __m128i sum_low = _mm256_castsi256_si128(sum);\n        __m128i sum_high = _mm256_extracti128_si256(sum, 1);\n\n        __m256d dbl_low = _mm256_cvtepi32_pd(sum_low);\n        __m256d dbl_high = _mm256_cvtepi32_pd(sum_high);\n\n        __m256d div7 = _mm256_set1_pd(7.0);\n        __m256d res_low_d = _mm256_div_pd(dbl_low, div7);\n        __m256d res_high_d = _mm256_div_pd(dbl_high, div7);\n\n        __m128i res_low = _mm256_cvtpd_epi32(res_low_d);\n        __m128i res_high = _mm256_cvtpd_epi32(res_high_d);\n\n        _mm_storeu_si128((__m128i*)(dst + i), res_low);\n        _mm_storeu_si128((__m128i*)(dst + i + 4), res_high);\n    }\n\n    for (; i < length; i++) {\n        int32_t x = src[i];\n        int32_t mul_scale = x * scale;\n        int32_t tmp1 = mul_scale >> 3;\n        int32_t tmp2 = mul_scale << 3;\n        int32_t product = tmp1 * tmp2;\n        int32_t sum = mul_scale + product;\n        dst[i] = sum / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    \n    for (; i + 7 < length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i base = _mm256_mullo_epi32(v_src, v_scale);\n        __m256i tmp1 = _mm256_srai_epi32(base, 3);\n        __m256i tmp2 = _mm256_slli_epi32(base, 3);\n        __m256i product = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(base, product);\n        \n        int sum_arr[8];\n        _mm256_storeu_si256((__m256i*)sum_arr, sum);\n        for (int j = 0; j < 8; j++) {\n            sum_arr[j] /= 7;\n        }\n        __m256i result = _mm256_loadu_si256((__m256i*)sum_arr);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        int base = src[i] * scale;\n        int tmp1 = base >> 3;\n        int tmp2 = base << 3;\n        int product = tmp1 * tmp2;\n        int sum = base + product;\n        dst[i] = sum / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        __m256i scale_vec = _mm256_set1_epi32(scale);\n        __m256 div_vec = _mm256_set1_ps(7.0f);\n        for (; i <= length - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n            __m256i v = _mm256_mullo_epi32(data, scale_vec);\n            __m256i tmp1 = _mm256_srai_epi32(v, 3);\n            __m256i tmp2 = _mm256_slli_epi32(v, 3);\n            __m256i product = _mm256_mullo_epi32(tmp1, tmp2);\n            __m256i num = _mm256_add_epi32(v, product);\n            __m256 num_f = _mm256_cvtepi32_ps(num);\n            __m256 res_f = _mm256_div_ps(num_f, div_vec);\n            __m256i res = _mm256_cvtps_epi32(res_f);\n            _mm256_storeu_si256((__m256i*)(dst + i), res);\n        }\n    }\n    for (; i < length; i++) {\n        int v = src[i] * scale;\n        int tmp1 = v >> 3;\n        int tmp2 = v << 3;\n        dst[i] = (v + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r + 3 < rows; r += 4) {\n        __m256i vidx = _mm256_set_epi64x((r+3)*cols, (r+2)*cols, (r+1)*cols, r*cols);\n        __m256d vdata = _mm256_i64gather_pd(src + stride, vidx, 8);\n        _mm256_storeu_pd(dst + r, vdata);\n    }\n    for (; r < rows; r++) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r + 3 < rows; r += 4) {\n        __m256i vindex = _mm256_set_epi64x(\n            (r+3) * cols + stride,\n            (r+2) * cols + stride,\n            (r+1) * cols + stride,\n            r * cols + stride\n        );\n        __m256d values = _mm256_i64gather_pd(src, vindex, 8);\n        _mm256_storeu_pd(dst + r, values);\n    }\n    for (; r < rows; r++) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r + 3 < rows; r += 4) {\n        long long r0 = r;\n        long long r1 = r + 1;\n        long long r2 = r + 2;\n        long long r3 = r + 3;\n        long long offset0 = (r0 * cols + stride) * sizeof(double);\n        long long offset1 = (r1 * cols + stride) * sizeof(double);\n        long long offset2 = (r2 * cols + stride) * sizeof(double);\n        long long offset3 = (r3 * cols + stride) * sizeof(double);\n        __m256i vindex = _mm256_set_epi64x(offset3, offset2, offset1, offset0);\n        __m256d data = _mm256_i64gather_pd(src, vindex, 1);\n        _mm256_storeu_pd(dst + r, data);\n    }\n    for (; r < rows; r++) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        size_t index0 = i * cols + stride;\n        size_t index1 = (i + 1) * cols + stride;\n        size_t index2 = (i + 2) * cols + stride;\n        size_t index3 = (i + 3) * cols + stride;\n\n        __m256i vidx = _mm256_setr_epi64x(index0, index1, index2, index3);\n        __m256d data = _mm256_i64gather_pd(src, vidx, 8);\n        _mm256_storeu_pd(dst + i, data);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        __m256i indices = _mm256_setr_epi64x(\n            (long long)(i * cols + stride),\n            (long long)((i+1) * cols + stride),\n            (long long)((i+2) * cols + stride),\n            (long long)((i+3) * cols + stride)\n        );\n        __m256d data = _mm256_i64gather_pd(src, indices, 8);\n        _mm256_storeu_pd(dst + i, data);\n    }\n    \n    for (; i < rows; i++) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vindex = _mm256_loadu_si256((__m256i*)(indices + i));\n        __m256 gathered = _mm256_i32gather_ps(src, vindex, 4);\n        _mm256_storeu_ps(dst + i, gathered);\n        __m256 src_data = _mm256_loadu_ps(src + i);\n        _mm256_i32scatter_ps(dst, vindex, src_data, 4);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    // Gather operation: dst[i] = src[indices[i]]\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 values = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, values);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: dst[indices[i]] = src[i]\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 data_vec = _mm256_loadu_ps(src + i);\n        int idx_arr[8];\n        float data_arr[8];\n        _mm256_storeu_si256((__m256i*)idx_arr, idx_vec);\n        _mm256_storeu_ps(data_arr, data_vec);\n        for (int j = 0; j < 8; j++) {\n            dst[idx_arr[j]] = data_arr[j];\n        }\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    // Gather operation: dst[i] = src[indices[i]]\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i vindex = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 gathered = _mm256_i32gather_ps(src, vindex, 4);\n        _mm256_storeu_ps(dst + i, gathered);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: dst[indices[i]] = src[i]\n    for (i = 0; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 values = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, values);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n    for (i = 0; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    if (length == 0) return;\n\n    // Gather operation: dst[i] = src[indices[i]]\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 values = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, values);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: dst[indices[i]] = src[i]\n    i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 vals = _mm256_loadu_ps(src + i);\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        _mm256_i32scatter_ps(dst, idx, vals, 4);\n    }\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n    uint64_t left = start;\n    uint64_t right = end;\n\n    const __m256i shuffle_mask = _mm256_setr_epi8(\n        14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1,\n        14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1\n    );\n\n    while (right - left >= 31) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i*)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i*)(arr + right - 15));\n\n        __m256i left_swapped = _mm256_permute2x128_si256(left_vec, left_vec, 0x01);\n        __m256i right_swapped = _mm256_permute2x128_si256(right_vec, right_vec, 0x01);\n        __m256i left_reversed = _mm256_shuffle_epi8(left_swapped, shuffle_mask);\n        __m256i right_reversed = _mm256_shuffle_epi8(right_swapped, shuffle_mask);\n\n        _mm256_storeu_si256((__m256i*)(arr + left), right_reversed);\n        _mm256_storeu_si256((__m256i*)(arr + right - 15), left_reversed);\n\n        left += 16;\n        right -= 16;\n    }\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n    uint64_t left = start;\n    uint64_t right = end;\n    static const __m256i reverse_mask = _mm256_set_epi8(\n        1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14,\n        1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14\n    );\n\n    while (right - left + 1 >= 32) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i const*)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i const*)(arr + right - 15));\n\n        __m256i swapped_left = _mm256_permute2x128_si256(left_vec, left_vec, 0x01);\n        __m256i rev_left = _mm256_shuffle_epi8(swapped_left, reverse_mask);\n        __m256i swapped_right = _mm256_permute2x128_si256(right_vec, right_vec, 0x01);\n        __m256i rev_right = _mm256_shuffle_epi8(swapped_right, reverse_mask);\n\n        _mm256_storeu_si256((__m256i*)(arr + left), rev_right);\n        _mm256_storeu_si256((__m256i*)(arr + right - 15), rev_left);\n\n        left += 16;\n        right -= 16;\n    }\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) {\n        return;\n    }\n    uint64_t left = start;\n    uint64_t right = end;\n    const __m256i rev_index = _mm256_set_epi16(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15);\n\n    while (left + 15 < right - 15) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i*)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i*)(arr + right - 15));\n        __m256i left_rev = _mm256_permutexvar_epi16(rev_index, left_vec);\n        __m256i right_rev = _mm256_permutexvar_epi16(rev_index, right_vec);\n        _mm256_storeu_si256((__m256i*)(arr + left), right_rev);\n        _mm256_storeu_si256((__m256i*)(arr + right - 15), left_rev);\n        left += 16;\n        right -= 16;\n    }\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t left = start;\n    uint64_t right = end;\n    const __m256i reversal_mask = _mm256_setr_epi16(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0);\n\n    while (right - left + 1 >= 32) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i const*)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i const*)(arr + right - 15));\n        __m256i rev_left = _mm256_permutevar16x16_epi16(left_vec, reversal_mask);\n        __m256i rev_right = _mm256_permutevar16x16_epi16(right_vec, reversal_mask);\n        _mm256_storeu_si256((__m256i*)(arr + left), rev_right);\n        _mm256_storeu_si256((__m256i*)(arr + right - 15), rev_left);\n        left += 16;\n        right -= 16;\n    }\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) {\n        return;\n    }\n    uint64_t l_ptr = start;\n    uint64_t r_ptr = end;\n    __m256i rev_mask = _mm256_setr_epi16(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0);\n\n    while (r_ptr - l_ptr >= 31) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i*)(arr + l_ptr));\n        __m256i right_vec = _mm256_loadu_si256((__m256i*)(arr + r_ptr - 15));\n        __m256i left_rev = _mm256_permutevar16x16_epi16(left_vec, rev_mask);\n        __m256i right_rev = _mm256_permutevar16x16_epi16(right_vec, rev_mask);\n        _mm256_storeu_si256((__m256i*)(arr + l_ptr), right_rev);\n        _mm256_storeu_si256((__m256i*)(arr + r_ptr - 15), left_rev);\n        l_ptr += 16;\n        r_ptr -= 16;\n    }\n\n    while (l_ptr < r_ptr) {\n        int16_t temp = arr[l_ptr];\n        arr[l_ptr] = arr[r_ptr];\n        arr[r_ptr] = temp;\n        l_ptr++;\n        r_ptr--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base_ptr = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *slice_ptr = slice + i * dim2;\n        size_t safe_j = 0;\n        if (dim2 * dim3 > slice_idx + 3) {\n            safe_j = (dim2 * dim3 - slice_idx - 3) / dim3;\n        }\n        size_t j = 0;\n        for (; j + 7 < dim2 && j + 7 < safe_j; j += 8) {\n            __m256i indices = _mm256_setr_epi32(0, dim3, 2*dim3, 3*dim3, 4*dim3, 5*dim3, 6*dim3, 7*dim3);\n            __m256i gathered = _mm256_i32gather_epi32((const int*)(base_ptr + j * dim3), indices, 1);\n            __m128i lane0 = _mm256_castsi256_si128(gathered);\n            __m128i lane1 = _mm256_extracti128_si256(gathered, 1);\n            lane0 = _mm_srli_epi32(lane0, 24);\n            lane1 = _mm_srli_epi32(lane1, 24);\n            __m128i packed16 = _mm_packus_epi32(lane0, lane1);\n            __m128i packed8 = _mm_packus_epi16(packed16, packed16);\n            _mm_storel_epi64((__m128i*)(slice_ptr + j), packed8);\n        }\n        for (; j < dim2; j++) {\n            slice_ptr[j] = base_ptr[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base_ptr = tensor + i * (dim2 * dim3) + slice_idx;\n        uint8_t *dest_ptr = slice + i * dim2;\n        size_t j = 0;\n\n        if (dim2 >= 32) {\n            __m256i index_vec = _mm256_setr_epi32(0, dim3, 2*dim3, 3*dim3, 4*dim3, 5*dim3, 6*dim3, 7*dim3);\n            for (; j <= dim2 - 32; j += 32) {\n                for (int k = 0; k < 4; k++) {\n                    const uint8_t *group_base = base_ptr + (j + 8*k) * dim3;\n                    __m256i gathered = _mm256_i32gather_epi32((const int*)group_base, index_vec, 1);\n                    gathered = _mm256_and_si256(gathered, _mm256_set1_epi32(0xFF));\n                    \n                    __m128i low_lane = _mm256_castsi256_si128(gathered);\n                    __m128i high_lane = _mm256_extracti128_si256(gathered, 1);\n                    __m128i packed16 = _mm_packus_epi32(low_lane, high_lane);\n                    __m128i packed8 = _mm_packus_epi16(packed16, packed16);\n                    _mm_storel_epi64((__m128i*)(dest_ptr + j + 8*k), packed8);\n                }\n            }\n        }\n\n        for (; j < dim2; j++) {\n            dest_ptr[j] = base_ptr[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base_ptr = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *out_ptr = slice + i * dim2;\n        size_t j = 0;\n\n        for (; j + 7 < dim2; j += 8) {\n            __m256i index_vec = _mm256_setr_epi32(\n                (int)(j * dim3),\n                (int)((j+1) * dim3),\n                (int)((j+2) * dim3),\n                (int)((j+3) * dim3),\n                (int)((j+4) * dim3),\n                (int)((j+5) * dim3),\n                (int)((j+6) * dim3),\n                (int)((j+7) * dim3)\n            );\n            __m256i gathered = _mm256_i32gather_epi32((const int*)base_ptr, index_vec, 1);\n            gathered = _mm256_and_si256(gathered, _mm256_set1_epi32(0xFF));\n            __m128i lo = _mm256_castsi256_si128(gathered);\n            __m128i hi = _mm256_extracti128_si256(gathered, 1);\n            __m128i packed16 = _mm_packus_epi32(lo, hi);\n            __m128i packed8 = _mm_packus_epi16(packed16, _mm_setzero_si128());\n            _mm_storel_epi64((__m128i*)(out_ptr + j), packed8);\n        }\n\n        for (; j < dim2; j++) {\n            out_ptr[j] = base_ptr[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *tensor_i = tensor + i * dim2 * dim3;\n        uint8_t *slice_i = slice + i * dim2;\n\n        if (slice_idx + 3 < dim3) {\n            size_t j = 0;\n            for (; j + 7 < dim2; j += 8) {\n                const uint8_t *base_ptr = tensor_i + j * dim3 + slice_idx;\n                __m256i offset = _mm256_set_epi32(7*dim3, 6*dim3, 5*dim3, 4*dim3, 3*dim3, 2*dim3, 1*dim3, 0*dim3);\n                __m256i gathered = _mm256_i32gather_epi32((const int*)base_ptr, offset, 1);\n                \n                uint32_t temp[8];\n                _mm256_storeu_si256((__m256i*)temp, gathered);\n                for (int k = 0; k < 8; k++) {\n                    slice_i[j + k] = (uint8_t)temp[k];\n                }\n            }\n            for (; j < dim2; j++) {\n                slice_i[j] = tensor_i[j * dim3 + slice_idx];\n            }\n        } else {\n            for (size_t j = 0; j < dim2; j++) {\n                slice_i[j] = tensor_i[j * dim3 + slice_idx];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    if (slice_idx > dim3 - 4) {\n        for (size_t i = 0; i < dim1; i++) {\n            const uint8_t *base_i = tensor + i * dim2 * dim3 + slice_idx;\n            for (size_t j = 0; j < dim2; j++) {\n                slice[i * dim2 + j] = base_i[j * dim3];\n            }\n        }\n    } else {\n        for (size_t i = 0; i < dim1; i++) {\n            const uint8_t *base_i = tensor + i * dim2 * dim3 + slice_idx;\n            size_t j = 0;\n            for (; j + 8 <= dim2; j += 8) {\n                const uint8_t *base_j = base_i + j * dim3;\n                __m256i vindex = _mm256_set_epi32(7*dim3, 6*dim3, 5*dim3, 4*dim3, 3*dim3, 2*dim3, 1*dim3, 0);\n                __m256i gathered = _mm256_i32gather_epi32((const int*)base_j, vindex, 1);\n                gathered = _mm256_and_si256(gathered, _mm256_set1_epi32(0xFF));\n                __m256i packed16 = _mm256_packus_epi32(gathered, _mm256_setzero_si256());\n                __m128i packed16_lo = _mm256_castsi256_si128(packed16);\n                __m128i packed8 = _mm_packus_epi16(packed16_lo, _mm_setzero_si128());\n                _mm_storel_epi64((__m128i*)(slice + i * dim2 + j), packed8);\n            }\n            for (; j < dim2; j++) {\n                slice[i * dim2 + j] = base_i[j * dim3];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t block_i = 0; block_i < rows; block_i += block_size) {\n        size_t i_end = block_i + block_size;\n        if (i_end > rows) i_end = rows;\n        for (size_t block_j = 0; block_j < cols; block_j += block_size) {\n            size_t j_end = block_j + block_size;\n            if (j_end > cols) j_end = cols;\n            for (size_t j = block_j; j < j_end; j += 16) {\n                size_t chunk_width = j_end - j;\n                if (chunk_width > 16) chunk_width = 16;\n                for (size_t i = block_i; i < i_end; i += 16) {\n                    size_t chunk_height = i_end - i;\n                    if (chunk_height > 16) chunk_height = 16;\n                    if (chunk_width == 16 && chunk_height == 16) {\n                        __m256i r0 = _mm256_loadu_si256((const __m256i*)(src + (i+0)*cols + j));\n                        __m256i r1 = _mm256_loadu_si256((const __m256i*)(src + (i+1)*cols + j));\n                        __m256i r2 = _mm256_loadu_si256((const __m256i*)(src + (i+2)*cols + j));\n                        __m256i r3 = _mm256_loadu_si256((const __m256i*)(src + (i+3)*cols + j));\n                        __m256i r4 = _mm256_loadu_si256((const __m256i*)(src + (i+4)*cols + j));\n                        __m256i r5 = _mm256_loadu_si256((const __m256i*)(src + (i+5)*cols + j));\n                        __m256i r6 = _mm256_loadu_si256((const __m256i*)(src + (i+6)*cols + j));\n                        __m256i r7 = _mm256_loadu_si256((const __m256i*)(src + (i+7)*cols + j));\n                        __m256i r8 = _mm256_loadu_si256((const __m256i*)(src + (i+8)*cols + j));\n                        __m256i r9 = _mm256_loadu_si256((const __m256i*)(src + (i+9)*cols + j));\n                        __m256i r10 = _mm256_loadu_si256((const __m256i*)(src + (i+10)*cols + j));\n                        __m256i r11 = _mm256_loadu_si256((const __m256i*)(src + (i+11)*cols + j));\n                        __m256i r12 = _mm256_loadu_si256((const __m256i*)(src + (i+12)*cols + j));\n                        __m256i r13 = _mm256_loadu_si256((const __m256i*)(src + (i+13)*cols + j));\n                        __m256i r14 = _mm256_loadu_si256((const __m256i*)(src + (i+14)*cols + j));\n                        __m256i r15 = _mm256_loadu_si256((const __m256i*)(src + (i+15)*cols + j));\n\n                        __m256i t0 = _mm256_unpacklo_epi16(r0, r1);\n                        __m256i t1 = _mm256_unpackhi_epi16(r0, r1);\n                        __m256i t2 = _mm256_unpacklo_epi16(r2, r3);\n                        __m256i t3 = _mm256_unpackhi_epi16(r2, r3);\n                        __m256i t4 = _mm256_unpacklo_epi16(r4, r5);\n                        __m256i t5 = _mm256_unpackhi_epi16(r4, r5);\n                        __m256i t6 = _mm256_unpacklo_epi16(r6, r7);\n                        __m256i t7 = _mm256_unpackhi_epi16(r6, r7);\n                        __m256i t8 = _mm256_unpacklo_epi16(r8, r9);\n                        __m256i t9 = _mm256_unpackhi_epi16(r8, r9);\n                        __m256i t10 = _mm256_unpacklo_epi16(r10, r11);\n                        __m256i t11 = _mm256_unpackhi_epi16(r10, r11);\n                        __m256i t12 = _mm256_unpacklo_epi16(r12, r13);\n                        __m256i t13 = _mm256_unpackhi_epi16(r12, r13);\n                        __m256i t14 = _mm256_unpacklo_epi16(r14, r15);\n                        __m256i t15 = _mm256_unpackhi_epi16(r14, r15);\n\n                        __m256i u0 = _mm256_unpacklo_epi32(t0, t2);\n                        __m256i u1 = _mm256_unpackhi_epi32(t0, t2);\n                        __m256i u2 = _mm256_unpacklo_epi32(t1, t3);\n                        __m256i u3 = _mm256_unpackhi_epi32(t1, t3);\n                        __m256i u4 = _mm256_unpacklo_epi32(t4, t6);\n                        __m256i u5 = _mm256_unpackhi_epi32(t4, t6);\n                        __m256i u6 = _mm256_unpacklo_epi32(t5, t7);\n                        __m256i u7 = _mm256_unpackhi_epi32(t5, t7);\n                        __m256i u8 = _mm256_unpacklo_epi32(t8, t10);\n                        __m256i u9 = _mm256_unpackhi_epi32(t8, t10);\n                        __m256i u10 = _mm256_unpacklo_epi32(t9, t11);\n                        __m256i u11 = _mm256_unpackhi_epi32(t9, t11);\n                        __m256i u12 = _mm256_unpacklo_epi32(t12, t14);\n                        __m256i u13 = _mm256_unpackhi_epi32(t12, t14);\n                        __m256i u14 = _mm256_unpacklo_epi32(t13, t15);\n                        __m256i u15 = _mm256_unpackhi_epi32(t13, t15);\n\n                        __m256i v0 = _mm256_unpacklo_epi64(u0, u4);\n                        __m256i v1 = _mm256_unpackhi_epi64(u0, u4);\n                        __m256i v2 = _mm256_unpacklo_epi64(u1, u5);\n                        __m256i v3 = _mm256_unpackhi_epi64(u1, u5);\n                        __m256i v4 = _mm256_unpacklo_epi64(u2, u6);\n                        __m256i v5 = _mm256_unpackhi_epi64(u2, u6);\n                        __m256i v6 = _mm256_unpacklo_epi64(u3, u7);\n                        __m256i v7 = _mm256_unpackhi_epi64(u3, u7);\n                        __m256i v8 = _mm256_unpacklo_epi64(u8, u12);\n                        __m256i v9 = _mm256_unpackhi_epi64(u8, u12);\n                        __m256i v10 = _mm256_unpacklo_epi64(u9, u13);\n                        __m256i v11 = _mm256_unpackhi_epi64(u9, u13);\n                        __m256i v12 = _mm256_unpacklo_epi64(u10, u14);\n                        __m256i v13 = _mm256_unpackhi_epi64(u10, u14);\n                        __m256i v14 = _mm256_unpacklo_epi64(u11, u15);\n                        __m256i v15 = _mm256_unpackhi_epi64(u11, u15);\n\n                        r0 = _mm256_permute2x128_si256(v0, v8, 0x20);\n                        r1 = _mm256_permute2x128_si256(v1, v9, 0x20);\n                        r2 = _mm256_permute2x128_si256(v2, v10, 0x20);\n                        r3 = _mm256_permute2x128_si256(v3, v11, 0x20);\n                        r4 = _mm256_permute2x128_si256(v4, v12, 0x20);\n                        r5 = _mm256_permute2x128_si256(v5, v13, 0x20);\n                        r6 = _mm256_permute2x128_si256(v6, v14, 0x20);\n                        r7 = _mm256_permute2x128_si256(v7, v15, 0x20);\n                        r8 = _mm256_permute2x128_si256(v0, v8, 0x31);\n                        r9 = _mm256_permute2x128_si256(v1, v9, 0x31);\n                        r10 = _mm256_permute2x128_si256(v2, v10, 0x31);\n                        r11 = _mm256_permute2x128_si256(v3, v11, 0x31);\n                        r12 = _mm256_permute2x128_si256(v4, v12, 0x31);\n                        r13 = _mm256_permute2x128_si256(v5, v13, 0x31);\n                        r14 = _mm256_permute2x128_si256(v6, v14, 0x31);\n                        r15 = _mm256_permute2x128_si256(v7, v15, 0x31);\n\n                        _mm256_storeu_si256((__m256i*)(dst + (j+0)*rows + i), r0);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+1)*rows + i), r1);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+2)*rows + i), r2);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+3)*rows + i), r3);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+4)*rows + i), r4);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+5)*rows + i), r5);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+6)*rows + i), r6);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+7)*rows + i), r7);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+8)*rows + i), r8);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+9)*rows + i), r9);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+10)*rows + i), r10);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+11)*rows + i), r11);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+12)*rows + i), r12);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+13)*rows + i), r13);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+14)*rows + i), r14);\n                        _mm256_storeu_si256((__m256i*)(dst + (j+15)*rows + i), r15);\n                    } else {\n                        for (size_t r = 0; r < chunk_height; r++) {\n                            for (size_t c = 0; c < chunk_width; c++) {\n                                dst[(j + c) * rows + (i + r)] = src[(i + r) * cols + (j + c)];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        size_t i_end = i + block_size;\n        if (i_end > rows) i_end = rows;\n        \n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t j_end = j + block_size;\n            if (j_end > cols) j_end = cols;\n            \n            for (size_t ii = i; ii < i_end; ii += 8) {\n                size_t ii_next = ii + 8;\n                if (ii_next > i_end) ii_next = i_end;\n                size_t row_chunk = ii_next - ii;\n                \n                for (size_t jj = j; jj < j_end; jj += 16) {\n                    size_t jj_next = jj + 16;\n                    if (jj_next > j_end) jj_next = j_end;\n                    size_t col_chunk = jj_next - jj;\n                    \n                    if (row_chunk == 8 && col_chunk == 16) {\n                        __m256i row0 = _mm256_loadu_si256((const __m256i*)(src + (ii+0)*cols + jj));\n                        __m256i row1 = _mm256_loadu_si256((const __m256i*)(src + (ii+1)*cols + jj));\n                        __m256i row2 = _mm256_loadu_si256((const __m256i*)(src + (ii+2)*cols + jj));\n                        __m256i row3 = _mm256_loadu_si256((const __m256i*)(src + (ii+3)*cols + jj));\n                        __m256i row4 = _mm256_loadu_si256((const __m256i*)(src + (ii+4)*cols + jj));\n                        __m256i row5 = _mm256_loadu_si256((const __m256i*)(src + (ii+5)*cols + jj));\n                        __m256i row6 = _mm256_loadu_si256((const __m256i*)(src + (ii+6)*cols + jj));\n                        __m256i row7 = _mm256_loadu_si256((const __m256i*)(src + (ii+7)*cols + jj));\n                        \n                        __m128i row0_lo = _mm256_extracti128_si256(row0, 0);\n                        __m128i row0_hi = _mm256_extracti128_si256(row0, 1);\n                        __m128i row1_lo = _mm256_extracti128_si256(row1, 0);\n                        __m128i row1_hi = _mm256_extracti128_si256(row1, 1);\n                        __m128i row2_lo = _mm256_extracti128_si256(row2, 0);\n                        __m128i row2_hi = _mm256_extracti128_si256(row2, 1);\n                        __m128i row3_lo = _mm256_extracti128_si256(row3, 0);\n                        __m128i row3_hi = _mm256_extracti128_si256(row3, 1);\n                        __m128i row4_lo = _mm256_extracti128_si256(row4, 0);\n                        __m128i row4_hi = _mm256_extracti128_si256(row4, 1);\n                        __m128i row5_lo = _mm256_extracti128_si256(row5, 0);\n                        __m128i row5_hi = _mm256_extracti128_si256(row5, 1);\n                        __m128i row6_lo = _mm256_extracti128_si256(row6, 0);\n                        __m128i row6_hi = _mm256_extracti128_si256(row6, 1);\n                        __m128i row7_lo = _mm256_extracti128_si256(row7, 0);\n                        __m128i row7_hi = _mm256_extracti128_si256(row7, 1);\n                        \n                        __m128i low[8] = {row0_lo, row1_lo, row2_lo, row3_lo, row4_lo, row5_lo, row6_lo, row7_lo};\n                        __m128i high[8] = {row0_hi, row1_hi, row2_hi, row3_hi, row4_hi, row5_hi, row6_hi, row7_hi};\n                        \n                        __m128i t0 = _mm_unpacklo_epi16(low[0], low[1]);\n                        __m128i t1 = _mm_unpacklo_epi16(low[2], low[3]);\n                        __m128i t2 = _mm_unpackhi_epi16(low[0], low[1]);\n                        __m128i t3 = _mm_unpackhi_epi16(low[2], low[3]);\n                        __m128i t4 = _mm_unpacklo_epi16(low[4], low[5]);\n                        __m128i t5 = _mm_unpacklo_epi16(low[6], low[7]);\n                        __m128i t6 = _mm_unpackhi_epi16(low[4], low[5]);\n                        __m128i t7 = _mm_unpackhi_epi16(low[6], low[7]);\n                        \n                        __m128i u0 = _mm_unpacklo_epi32(t0, t1);\n                        __m128i u1 = _mm_unpackhi_epi32(t0, t1);\n                        __m128i u2 = _mm_unpacklo_epi32(t2, t3);\n                        __m128i u3 = _mm_unpackhi_epi32(t2, t3);\n                        __m128i u4 = _mm_unpacklo_epi32(t4, t5);\n                        __m128i u5 = _mm_unpackhi_epi32(t4, t5);\n                        __m128i u6 = _mm_unpacklo_epi32(t6, t7);\n                        __m128i u7 = _mm_unpackhi_epi32(t6, t7);\n                        \n                        __m128i v0 = _mm_unpacklo_epi64(u0, u4);\n                        __m128i v1 = _mm_unpackhi_epi64(u0, u4);\n                        __m128i v2 = _mm_unpacklo_epi64(u1, u5);\n                        __m128i v3 = _mm_unpackhi_epi64(u1, u5);\n                        __m128i v4 = _mm_unpacklo_epi64(u2, u6);\n                        __m128i v5 = _mm_unpackhi_epi64(u2, u6);\n                        __m128i v6 = _mm_unpacklo_epi64(u3, u7);\n                        __m128i v7 = _mm_unpackhi_epi64(u3, u7);\n                        \n                        _mm_storeu_si128((__m128i*)(dst + (jj+0)*rows + ii), v0);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+1)*rows + ii), v1);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+2)*rows + ii), v2);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+3)*rows + ii), v3);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+4)*rows + ii), v4);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+5)*rows + ii), v5);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+6)*rows + ii), v6);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+7)*rows + ii), v7);\n                        \n                        t0 = _mm_unpacklo_epi16(high[0], high[1]);\n                        t1 = _mm_unpacklo_epi16(high[2], high[3]);\n                        t2 = _mm_unpackhi_epi16(high[0], high[1]);\n                        t3 = _mm_unpackhi_epi16(high[2], high[3]);\n                        t4 = _mm_unpacklo_epi16(high[4], high[5]);\n                        t5 = _mm_unpacklo_epi16(high[6], high[7]);\n                        t6 = _mm_unpackhi_epi16(high[4], high[5]);\n                        t7 = _mm_unpackhi_epi16(high[6], high[7]);\n                        \n                        u0 = _mm_unpacklo_epi32(t0, t1);\n                        u1 = _mm_unpackhi_epi32(t0, t1);\n                        u2 = _mm_unpacklo_epi32(t2, t3);\n                        u3 = _mm_unpackhi_epi32(t2, t3);\n                        u4 = _mm_unpacklo_epi32(t4, t5);\n                        u5 = _mm_unpackhi_epi32(t4, t5);\n                        u6 = _mm_unpacklo_epi32(t6, t7);\n                        u7 = _mm_unpackhi_epi32(t6, t7);\n                        \n                        v0 = _mm_unpacklo_epi64(u0, u4);\n                        v1 = _mm_unpackhi_epi64(u0, u4);\n                        v2 = _mm_unpacklo_epi64(u1, u5);\n                        v3 = _mm_unpackhi_epi64(u1, u5);\n                        v4 = _mm_unpacklo_epi64(u2, u6);\n                        v5 = _mm_unpackhi_epi64(u2, u6);\n                        v6 = _mm_unpacklo_epi64(u3, u7);\n                        v7 = _mm_unpackhi_epi64(u3, u7);\n                        \n                        _mm_storeu_si128((__m128i*)(dst + (jj+8)*rows + ii), v0);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+9)*rows + ii), v1);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+10)*rows + ii), v2);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+11)*rows + ii), v3);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+12)*rows + ii), v4);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+13)*rows + ii), v5);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+14)*rows + ii), v6);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+15)*rows + ii), v7);\n                    } else {\n                        for (size_t x = ii; x < ii_next; x++) {\n                            for (size_t y = jj; y < jj_next; y++) {\n                                dst[y * rows + x] = src[x * cols + y];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        size_t i_end = i + block_size;\n        if (i_end > rows) i_end = rows;\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t j_end = j + block_size;\n            if (j_end > cols) j_end = cols;\n            \n            for (size_t ii = i; ii < i_end; ii += 16) {\n                size_t ii_end = ii + 16;\n                if (ii_end > i_end) ii_end = i_end;\n                for (size_t jj = j; jj < j_end; jj += 16) {\n                    size_t jj_end = jj + 16;\n                    if (jj_end > j_end) jj_end = j_end;\n                    \n                    size_t micro_rows = ii_end - ii;\n                    size_t micro_cols = jj_end - jj;\n                    \n                    if (micro_rows == 16 && micro_cols == 16) {\n                        __m256i row0 = _mm256_loadu_si256((__m256i*)(src + ii*cols + jj));\n                        __m256i row1 = _mm256_loadu_si256((__m256i*)(src + (ii+1)*cols + jj));\n                        __m256i row2 = _mm256_loadu_si256((__m256i*)(src + (ii+2)*cols + jj));\n                        __m256i row3 = _mm256_loadu_si256((__m256i*)(src + (ii+3)*cols + jj));\n                        __m256i row4 = _mm256_loadu_si256((__m256i*)(src + (ii+4)*cols + jj));\n                        __m256i row5 = _mm256_loadu_si256((__m256i*)(src + (ii+5)*cols + jj));\n                        __m256i row6 = _mm256_loadu_si256((__m256i*)(src + (ii+6)*cols + jj));\n                        __m256i row7 = _mm256_loadu_si256((__m256i*)(src + (ii+7)*cols + jj));\n                        __m256i row8 = _mm256_loadu_si256((__m256i*)(src + (ii+8)*cols + jj));\n                        __m256i row9 = _mm256_loadu_si256((__m256i*)(src + (ii+9)*cols + jj));\n                        __m256i row10 = _mm256_loadu_si256((__m256i*)(src + (ii+10)*cols + jj));\n                        __m256i row11 = _mm256_loadu_si256((__m256i*)(src + (ii+11)*cols + jj));\n                        __m256i row12 = _mm256_loadu_si256((__m256i*)(src + (ii+12)*cols + jj));\n                        __m256i row13 = _mm256_loadu_si256((__m256i*)(src + (ii+13)*cols + jj));\n                        __m256i row14 = _mm256_loadu_si256((__m256i*)(src + (ii+14)*cols + jj));\n                        __m256i row15 = _mm256_loadu_si256((__m256i*)(src + (ii+15)*cols + jj));\n\n                        __m256i a0 = _mm256_unpacklo_epi16(row0, row1);\n                        __m256i a1 = _mm256_unpackhi_epi16(row0, row1);\n                        __m256i a2 = _mm256_unpacklo_epi16(row2, row3);\n                        __m256i a3 = _mm256_unpackhi_epi16(row2, row3);\n                        __m256i a4 = _mm256_unpacklo_epi16(row4, row5);\n                        __m256i a5 = _mm256_unpackhi_epi16(row4, row5);\n                        __m256i a6 = _mm256_unpacklo_epi16(row6, row7);\n                        __m256i a7 = _mm256_unpackhi_epi16(row6, row7);\n                        __m256i a8 = _mm256_unpacklo_epi16(row8, row9);\n                        __m256i a9 = _mm256_unpackhi_epi16(row8, row9);\n                        __m256i a10 = _mm256_unpacklo_epi16(row10, row11);\n                        __m256i a11 = _mm256_unpackhi_epi16(row10, row11);\n                        __m256i a12 = _mm256_unpacklo_epi16(row12, row13);\n                        __m256i a13 = _mm256_unpackhi_epi16(row12, row13);\n                        __m256i a14 = _mm256_unpacklo_epi16(row14, row15);\n                        __m256i a15 = _mm256_unpackhi_epi16(row14, row15);\n\n                        __m256i b0 = _mm256_unpacklo_epi32(a0, a2);\n                        __m256i b1 = _mm256_unpackhi_epi32(a0, a2);\n                        __m256i b2 = _mm256_unpacklo_epi32(a1, a3);\n                        __m256i b3 = _mm256_unpackhi_ep\u678132(a1, a3);\n                        __m256i b4 = _mm256_unpacklo_epi32(a4, a6);\n                        __m256i b5 = _mm256_unpackhi_epi32(a4, a6);\n                        __m256i b6 = _mm256_unpacklo_epi32(a5, a7);\n                        __m256i b7 = _mm256_unpackhi_epi32(a5, a7);\n                        __m256i b8 = _mm256_unpacklo_epi32(a8, a10);\n                        __m256i b9 = _mm256_unpackhi_epi32(a8, a10);\n                        __m256i b10 = _mm256_unpacklo_epi32(a9, a11);\n                        __m256i b11 = _mm256_unpackhi_epi32(a9, a11);\n                        __m256i b12 = _\u6781256_unpacklo_epi32(a12, a14);\n                        __m256i b13 = _mm256_unpackhi_epi32(a12, a14);\n                        __m256i b14 = _mm256_unpacklo_epi32(a13, a15);\n                        __m256i b15 = _mm256_unpackhi_epi32(a13, a15);\n\n                        __m256i c0 = _mm256_unpacklo_epi64(b0, b4);\n                        __m256i c1 = _mm256_unpackhi_epi64(b0, b4);\n                        __m256i c2 = _mm256_unpacklo_epi64(b1, b5);\n                        __m256i c3 = _mm256_unpackhi_epi64(b1, b5);\n                        __m256i c4 = _mm256_unpacklo_epi64(b2, b6);\n                        __m256i c5 = _mm256_unpackhi_epi64(b2, b6);\n                        __m256i c6 = _mm256_unpacklo_epi64(b3, b7);\n                        __m256i c7 = _mm256_unpackhi_epi64(b3, b7);\n                        __m256i c8 = _mm256_unpacklo_epi64(b8, b12);\n                        __m256i c9 = _mm256_unpackhi_epi64(b8, b12);\n                        __m256i c10 = _mm256_unpacklo_epi64(b9, b13);\n                        __m256i c11 = _mm256_unpackhi_epi64(b9, b13);\n                        __m256i c12 = _mm256_unpacklo_epi64(b10, b14);\n                        __m256i c13 = _mm256_unpackhi_epi64(b10, b14);\n                        __m256i c14 = _mm256_unpacklo_epi64(b11, b15);\n                        __m256i c15 = _mm256_unpackhi_epi64(b11, b15);\n\n                        __m256i d0 = _mm256_permute2x128_si256(c0, c8, 0x20);\n                        __m256i d1 = _mm256_permute2x128_si256(c1, c9, 0x20);\n                        __m256i d2 = _mm256_permute2x128_si256(c2, c10, 0x20);\n                        __m256i d3 = _mm256_permute2x128_si256(c3, c11, 0x20);\n                        __m256i d4 = _mm256_permute2x128_si256(c4, c12, 0x20);\n                        __m256i d5 = _mm256_permute2x128_si256(c5, c13, 0x20);\n                        __m256i d6 = _mm256_permute2x128_si256(c6, c14, 0x20);\n                        __m256i d7 = _mm256_permute2x128_si256(c7, c15, 0x20);\n                        __m256i d8 = _mm256_permute2x128_si256(c0, c8, 0x31);\n                        __m256i d9 = _mm256_permute2x128_si256(c1, c9, 0x31);\n                        __m256i d10 = _mm256_permute2x128_si256(c2, c10, 0x31);\n                        __m256i d11 = _mm256_permute2x128_si256(c3, c11, 0x31);\n                        __m256i d12 = _mm256_permute2x128_si256(c4, c12, 0x31);\n                        __m256i d13 = _mm256_permute2x128_si256(c5, c13, 0x31);\n                        __m256i d14 = _mm256_permute2x128_si256(c6, c14, 0x31);\n                        __m256i d15 = _mm256_permute2x128_si256(c7, c15, 0x31);\n\n                        _mm256_storeu_si256((__m256i*)(dst + (jj)*rows + ii), d0);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+1)*rows + ii), d1);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+2)*rows + ii), d2);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+3)*rows + ii), d3);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+4)*rows + ii), d4);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+5)*rows + ii), d5);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+6)*rows + ii), d6);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+7)*rows + ii), d7);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+8)*rows + ii), d8);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+9)*rows + ii), d9);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+10)*rows + ii), d10);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+11)*rows + ii), d11);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+12)*rows + ii), d12);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+13)*rows + ii), d13);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+14)*rows + ii), d14);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+15)*rows + ii), d15);\n                    } else {\n                        for (size_t iii = ii; iii < ii_end; iii++) {\n                            for (size_t jjj = jj; jjj < jj_end; jjj++) {\n                                dst[jjj * rows + iii] = src[iii * cols + jjj];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        size_t i_end = (i + block_size < rows) ? i + block_size : rows;\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t j_end = (j + block_size < cols) ? j + block_size : cols;\n            for (size_t ii = i; ii < i_end; ii += 16) {\n                size_t i_block_end = (ii + 16 < i_end) ? ii + 16 : i_end;\n                size_t block_height = i_block_end - ii;\n                for (size_t jj = j; jj < j_end; jj += 16) {\n                    size_t j_block_end = (jj + 16 < j_end) ? jj + 16 : j_end;\n                    size_t block_width = j_block_end - jj;\n                    if (block_height == 16 && block_width == 16) {\n                        __m256i r0 = _mm256_loadu_si256((const __m256i*)(src + ii * cols + jj));\n                        __m256i r1 = _mm256_loadu_si256((const __m256i*)(src + (ii+1) * cols + jj));\n                        __m256i r2 = _mm256_loadu_si256((const __m256i*)(src + (ii+2) * cols + jj));\n                        __m256i r3 = _mm256_loadu_si256((const __m256i*)(src + (ii+3) * cols + jj));\n                        __m256i r4 = _mm256_loadu_si256((const __m256i*)(src + (ii+4) * cols + jj));\n                        __m256i r5 = _mm256_loadu_si256((const __m256i*)(src + (ii+5) * cols + jj));\n                        __m256i r6 = _mm256_loadu_si256((const __m256i*)(src + (ii+6) * cols + jj));\n                        __m256i r7 = _mm256_loadu_si256((const __m256i*)(src + (ii+7) * cols + jj));\n                        __m256i r8 = _mm256_loadu_si256((const __m256i*)(src + (ii+8) * cols + jj));\n                        __m256i r9 = _mm256_loadu_si256((const __m256i*)(src + (ii+9) * cols + jj));\n                        __m256i r10 = _mm256_loadu_si256((const __m256i*)(src + (ii+10) * cols + jj));\n                        __m256i r11 = _mm256_loadu_si256((const __m256i*)(src + (ii+11) * cols + jj));\n                        __m256i r12 = _mm256_loadu_si256((const __m256i*)(src + (ii+12) * cols + jj));\n                        __m256i r13 = _mm256_loadu_si256((const __m256i*)(src + (ii+13) * cols + jj));\n                        __m256i r14 = _mm256_loadu_si256((const __m256i*)(src + (ii+14) * cols + jj));\n                        __m256i r15 = _mm256_loadu_si256((const __m256i*)(src + (ii+15) * cols + jj));\n\n                        __m256i t0 = _mm256_unpacklo_epi16(r0, r1);\n                        __m256i t1 = _mm256_unpackhi_epi16(r0, r1);\n                        __m256i t2 = _mm256_unpacklo_epi16(r2, r3);\n                        __m256i t3 = _mm256_unpackhi_epi16(r2, r3);\n                        __m256i t4 = _mm256_unpacklo_epi16(r4, r5);\n                        __m256i t5 = _mm256_unpackhi_epi16(r4, r5);\n                        __m256i t6 = _mm256_unpacklo_epi16(r6, r7);\n                        __m256i t7 = _mm256_unpackhi_epi16(r6, r7);\n                        __m256i t8 = _mm256_unpacklo_epi16(r8, r9);\n                        __m256i t9 = _mm256_unpackhi_epi16(r8, r9);\n                        __m256i t10 = _mm256_unpacklo_epi16(r10, r11);\n                        __m256i t11 = _mm256_unpackhi_epi16(r10, r11);\n                        __m256i t12 = _mm256_unpacklo_epi16(r12, r13);\n                        __m256i t13 = _mm256_unpackhi_epi16(r12, r13);\n                        __m256i t14 = _mm256_unpacklo_epi16(r14, r15);\n                        __m256i t15 = _mm256_unpackhi_epi16(r14, r15);\n\n                        r0 = _mm256_unpacklo_epi32(t0, t2);\n                        r1 = _mm256_unpackhi_epi32(t0, t2);\n                        r2 = _mm256_unpacklo_epi32(t1, t3);\n                        r3 = _mm256_unpackhi_epi32(t1, t3);\n                        r4 = _mm256_unpacklo_epi32(t4, t6);\n                        r5 = _mm256_unpackhi_epi32(t4, t6);\n                        r6 = _mm256_unpacklo_epi32(t5, t7);\n                        r7 = _mm256_unpackhi_epi32(t5, t7);\n                        r8 = _mm256_unpacklo_epi32(t8, t10);\n                        r9 = _mm256_unpackhi_epi32(t8, t10);\n                        r10 = _mm256_unpacklo_epi32(t9, t11);\n                        r11 = _mm256_unpackhi_epi32(t9, t11);\n                        r12 = _mm256_unpacklo_epi32(t12, t14);\n                        r13 = _mm256_unpackhi_epi32(t12, t14);\n                        r14 = _mm256_unpacklo_epi32(t13, t15);\n                        r15 = _mm256_unpackhi_epi32(t13, t15);\n\n                        t0 = _mm256_unpacklo_epi64(r0, r4);\n                        t1 = _mm256_unpackhi_epi64(r0, r4);\n                        t2 = _mm256_unpacklo_epi64(r1, r5);\n                        t3 = _mm256_unpackhi_epi64(r1, r5);\n                        t4 = _mm256_unpacklo_epi64(r2, r6);\n                        t5 = _mm256_unpackhi_epi64(r2, r6);\n                        t6 = _mm256_unpacklo_epi64(r3, r7);\n                        t7 = _mm256_unpackhi_epi64(r3, r7);\n                        t8 = _mm256_unpacklo_epi64(r8, r12);\n                        t9 = _mm256_unpackhi_epi64(r8, r12);\n                        t10 = _mm256_unpacklo_epi64(r9, r13);\n                        t11 = _mm256_unpackhi_epi64(r9, r13);\n                        t12 = _mm256_unpacklo_epi64(r10, r14);\n                        t13 = _mm256_unpackhi_epi64(r10, r14);\n                        t14 = _mm256_unpacklo_epi64(r11, r15);\n                        t15 = _mm256_unpackhi_epi64(r11, r15);\n\n                        _mm256_storeu_si256((__m256i*)(dst + (jj) * rows + ii), t0);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+1) * rows + ii), t1);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+2) * rows + ii), t2);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+3) * rows + ii), t3);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+4) * rows + ii), t4);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+5) * rows + ii), t5);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+6) * rows + ii), t6);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+7) * rows + ii), t7);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+8) * rows + ii), t8);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+9) * rows + ii), t9);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+10) * rows + ii), t10);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+11) * rows + ii), t11);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+12) * rows + ii), t12);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+13) * rows + ii), t13);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+14) * rows + ii), t14);\n                        _mm256_storeu_si256((__m256i*)(dst + (jj+15) * rows + ii), t15);\n                    } else {\n                        for (size_t y = ii; y < ii + block_height; y++) {\n                            for (size_t x = jj; x < jj + block_width; x++) {\n                                dst[x * rows + y] = src[y * cols + x];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i_block = 0; i_block < rows; i_block += block_size) {\n        size_t i_start = i_block;\n        size_t i_end = (i_block + block_size < rows) ? i_block + block_size : rows;\n        for (size_t j_block = 0; j_block < cols; j_block += block_size) {\n            size_t j_start = j_block;\n            size_t j_end = (j_block + block_size < cols) ? j_block + block_size : cols;\n            \n            for (size_t i_in_block = 0; i_in_block < i_end - i_start; i_in_block += 8) {\n                size_t i_step = (i_end - i_start - i_in_block < 8) ? i_end - i_start - i_in_block : 8;\n                for (size_t j_in_block = 0; j_in_block < j_end - j_start; j_in_block += 16) {\n                    size_t j_step = (j_end - j_start - j_in_block < 16) ? j_end - j_start - j_in_block : 16;\n                    \n                    if (i_step == 8 && j_step == 16) {\n                        const int16_t *src_ptr = src + (i_start + i_in_block) * cols + j_start + j_in_block;\n                        int16_t *dst_ptr = dst + (j_start + j_in_block) * rows + i_start + i_in_block;\n                        \n                        __m256i row0 = _mm256_loadu_si256((const __m256i*)(src_ptr + 0 * cols));\n                        __m256i row1 = _mm256_loadu_si256((const __m256i*)(src_ptr + 1 * cols));\n                        __m256i row2 = _mm256_loadu_si256((const __m256i*)(src_ptr + 2 * cols));\n                        __m256i row3 = _mm256_loadu_si256((const __m256i*)(src_ptr + 3 * cols));\n                        __m256i row4 = _mm256_loadu_si256((const __m256i*)(src_ptr + 4 * cols));\n                        __m256i row5 = _mm256_loadu_si256((const __m256i*)(src_ptr + 5 * cols));\n                        __m256i row6 = _mm256_loadu_si256((const __m256i*)(src_ptr + 6 * cols));\n                        __m256i row7 = _mm256_loadu_si256((const __m256i*)(src_ptr + 7 * cols));\n                        \n                        __m128i row0_low = _mm256_extracti128_si256(row0, 0);\n                        __m128i row0_high = _mm256_extracti128_si256(row0, 1);\n                        __m128i row1_low = _mm256_extracti128_si256(row1, 0);\n                        __m128i row1_high = _mm256_extracti128_si256(row1, 1);\n                        __m128i row2_low = _mm256_extracti128_si256(row2, 0);\n                        __m128i row2_high = _mm256_extracti128_si256(row2, 1);\n                        __m128i row3_low = _mm256_extracti128_si256(row3, 0);\n                        __m128i row3_high = _mm256_extracti128_si256(row3, 1);\n                        __m128i row4_low = _mm256_extracti128_si256(row4, 0);\n                        __m128i row4_high = _mm256_extracti128_si256(row4, 1);\n                        __m128i row5_low = _mm256_extracti128_si256(row5, 0);\n                        __m128i row5_high = _mm256_extracti128_si256(row5, 1);\n                        __m128i row6_low = _mm256_extracti128_si256(row6, 0);\n                        __m128i row6_high = _mm256_extracti128_si256(row6, 1);\n                        __m128i row7_low = _mm256_extracti128_si256(row7, 0);\n                        __m128i row7_high = _mm256_extracti128_si256(row7, 1);\n                        \n                        __m128i t0 = _mm_unpacklo_epi16(row0_low, row1_low);\n                        __m128i t1 = _mm_unpackhi_epi16(row0_low, row1_low);\n                        __m128i t2 = _mm_unpacklo_epi16(row2_low, row3_low);\n                        __m128i t3 = _mm_unpackhi_epi16(row2_low, row3_low);\n                        __m128i t4 = _mm_unpacklo_epi16(row4_low, row5_low);\n                        __m128i t5 = _mm_unpackhi_epi16(row4_low, row5_low);\n                        __m128i t6 = _mm_unpacklo_epi16(row6_low, row7_low);\n                        __m128i t7 = _mm_unpackhi_epi16(row6_low, row7_low);\n                        \n                        __m128i u0 = _mm_unpacklo_epi32(t0, t2);\n                        __m128i u1 = _mm_unpackhi_epi32(t0, t2);\n                        __m128i u2 = _mm_unpacklo_epi32(t1, t3);\n                        __m128i u3 = _mm_unpackhi_epi32(t1, t3);\n                        __m128i u4 = _mm_unpacklo_epi32(t4, t6);\n                        __m128i u5 = _mm_unpackhi_epi32(t4, t6);\n                        __m128i u6 = _mm_unpacklo_epi32(t5, t7);\n                        __m128i u7 = _mm_unpackhi_epi32(t5, t7);\n                        \n                        __m128i v0 = _mm_unpacklo_epi64(u0, u4);\n                        __m128i v1 = _mm_unpackhi_epi64(u0, u4);\n                        __m128i v2 = _mm_unpacklo_epi64(u1, u5);\n                        __m128i v3 = _mm_unpackhi_epi64(u1, u5);\n                        __m128i v4 = _mm_unpacklo_epi64(u2, u6);\n                        __m128i v5 = _mm_unpackhi_epi64(u2, u6);\n                        __m128i v6 = _mm_unpacklo_epi64(u3, u7);\n                        __m128i v7 = _mm_unpackhi_epi64(u3, u7);\n                        \n                        __m128i w0 = _mm_unpacklo_epi16(row0_high, row1_high);\n                        __m128i w1 = _mm_unpackhi_epi16(row0_high, row1_high);\n                        __m128i w2 = _mm_unpacklo_epi16(row2_high, row3_high);\n                        __m128i w3 = _mm_unpackhi_epi16(row2_high, row3_high);\n                        __m128i w4 = _mm_unpacklo_epi16(row4_high, row5_high);\n                        __m128i w5 = _mm_unpackhi_epi16(row4_high, row5_high);\n                        __m128i w6 = _mm_unpacklo_epi16(row6_high, row7_high);\n                        __m128i w7 = _mm_unpackhi_epi16(row6_high, row7_high);\n                        \n                        __m128i x0 = _mm_unpacklo_epi32(w0, w2);\n                        __m128i x1 = _mm_unpackhi_epi32(w0, w2);\n                        __m128i x2 = _mm_unpacklo_epi32(w1, w3);\n                        __m128i x3 = _mm_unpackhi_epi32(w1, w3);\n                        __m128i x4 = _mm_unpacklo_epi32(w4, w6);\n                        __m128i x5 = _mm_unpackhi_epi32(w4, w6);\n                        __m128i x6 = _mm_unpacklo_epi32(w5, w7);\n                        __m128i x7 = _mm_unpackhi_epi32(w5, w7);\n                        \n                        __m128i y0 = _mm_unpacklo_epi64(x0, x4);\n                        __m128i y1 = _mm_unpackhi_epi64(x0, x4);\n                        __m128i y2 = _mm_unpacklo_epi64(x1, x5);\n                        __m128i y3 = _mm_unpackhi_epi64(x1, x5);\n                        __m128i y4 = _mm_unpacklo_epi64(x2, x6);\n                        __m128i y5 = _mm_unpackhi_epi64(x2, x6);\n                        __m128i y6 = _mm_unpacklo_epi64(x3, x7);\n                        __m128i y7 = _mm_unpackhi_epi64(x3, x7);\n                        \n                        _mm_storeu_si128((__m128i*)(dst_ptr + 0 * rows), v0);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 1 * rows), v1);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 2 * rows), v2);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 3 * rows), v3);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 4 * rows), v4);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 5 * rows), v5);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 6 * rows), v6);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 7 * rows), v7);\n                        \n                        _mm_storeu_si128((__m128i*)(dst_ptr + 8 * rows), y0);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 9 * rows), y1);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 10 * rows), y2);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 11 * rows), y3);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 12 * rows), y4);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 13 * rows), y5);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 14 * rows), y6);\n                        _mm_storeu_si128((__m128i*)(dst_ptr + 15 * rows), y7);\n                    } else {\n                        for (size_t i = 0; i < i_step; i++) {\n                            for (size_t j = 0; j < j_step; j++) {\n                                dst[(j_start + j_in_block + j) * rows + (i_start + i_in_block + i)] = \n                                    src[(i_start + i_in_block + i) * cols + (j_start + j_in_block + j)];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    size_t step = dim * dim + dim + 1;\n    __m256 accum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 8 <= dim; i += 8) {\n        __m256 v = _mm256_set_ps(\n            array[(i+7) * step],\n            array[(i+6) * step],\n            array[(i+5) * step],\n            array[(i+4) * step],\n            array[(i+3) * step],\n            array[(i+2) * step],\n            array[(i+1) * step],\n            array[i * step]\n        );\n        accum = _mm256_add_ps(accum, v);\n    }\n\n    __m128 vlow = _mm256_castps256_ps128(accum);\n    __m128 vhigh = _mm256_extractf128_ps(accum, 1);\n    vlow = _mm_add_ps(vlow, vhigh);\n    __m128 hadd = _mm_hadd_ps(vlow, vlow);\n    hadd = _mm_hadd_ps(hadd, hadd);\n    float sum = _mm_cvtss_f32(hadd);\n\n    for (; i < dim; i++) {\n        sum += array[i * step];\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    \n    size_t total_step = dim * dim + dim + 1;\n    __m256 acc = _mm256_setzero_ps();\n    size_t i = 0;\n    \n    if (dim >= 8) {\n        for (; i <= dim - 8; i += 8) {\n            if ((i + 7) * total_step <= UINT_MAX) {\n                size_t base_index = i * total_step;\n                __m256i indices = _mm256_setr_epi32(\n                    base_index,\n                    base_index + total_step,\n                    base_index + 2 * total_step,\n                    base_index + 3 * total_step,\n                    base_index + 4 * total_step,\n                    base_index + 5 * total_step,\n                    base_index + 6 * total_step,\n                    base_index + 7 * total_step\n                );\n                __m256 v = _mm256_i32gather_ps(array, indices, 4);\n                acc = _mm256_add_ps(acc, v);\n            } else {\n                break;\n            }\n        }\n    }\n    \n    __m128 high = _mm256_extractf128_ps(acc, 1);\n    __m128 low = _mm256_castps256_ps128(acc);\n    __m128 sum128 = _mm_add_ps(high, low);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    float sum = _mm_cvtss_f32(sum128);\n    \n    for (; i < dim; i++) {\n        size_t index = i * total_step;\n        sum += array[index];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    size_t stride = dim * dim + dim + 1;\n    __m256 acc = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 7 < dim; i += 8) {\n        __m256 values = _mm256_set_ps(\n            array[(i+7)*stride],\n            array[(i+6)*stride],\n            array[(i+5)*stride],\n            array[(i+4)*stride],\n            array[(i+3)*stride],\n            array[(i+2)*stride],\n            array[(i+1)*stride],\n            array[i*stride]\n        );\n        acc = _mm256_add_ps(acc, values);\n    }\n\n    __m128 hi = _mm256_extractf128_ps(acc, 1);\n    __m128 lo = _mm256_castps256_ps128(acc);\n    __m128 sum128 = _mm_add_ps(hi, lo);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    sum128 = _mm_hadd_ps(sum128, sum128);\n    float sum = _mm_cvtss_f32(sum128);\n\n    for (; i < dim; i++) {\n        sum += array[i * stride];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    size_t stride = dim * dim + dim + 1;\n    __m256 accum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 8 <= dim; i += 8) {\n        __m256i base_index = _mm256_set1_epi32(i * stride);\n        __m256i j_vec = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n        __m256i step_vec = _mm256_set1_epi32(stride);\n        __m256i j_times_stride = _mm256_mullo_epi32(j_vec, step_vec);\n        __m256i indices = _mm256_add_epi32(base_index, j_times_stride);\n        __m256 v = _mm256_i32gather_ps(array, indices, 4);\n        accum = _mm256_add_ps(accum, v);\n    }\n\n    __m256 hadd1 = _mm256_hadd_ps(accum, accum);\n    __m256 hadd2 = _mm256_hadd_ps(hadd1, hadd1);\n    __m128 low = _mm256_castps256_ps128(hadd2);\n    __m128 high = _mm256_extractf128_ps(hadd2, 1);\n    float sum = _mm_cvtss_f32(low) + _mm_cvtss_f32(high);\n\n    for (; i < dim; i++) {\n        sum += array[i * stride];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t step = dim * dim + dim + 1;\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 8 <= dim; i += 8) {\n        __m256 v = _mm256_setr_ps(\n            array[i * step],\n            array[(i+1) * step],\n            array[(i+2) * step],\n            array[(i+3) * step],\n            array[(i+4) * step],\n            array[(i+5) * step],\n            array[(i+6) * step],\n            array[(i+7) * step]\n        );\n        sum_vec = _mm256_add_ps(sum_vec, v);\n    }\n\n    __m256 permuted = _mm256_permute2f128_ps(sum_vec, sum_vec, 0x01);\n    __m256 sum_inter = _mm256_add_ps(sum_vec, permuted);\n    __m256 hadd1 = _mm256_hadd_ps(sum_inter, sum_inter);\n    __m256 hadd2 = _mm256_hadd_ps(hadd1, hadd1);\n    float sum = _mm256_cvtss_f32(hadd2);\n\n    for (; i < dim; i++) {\n        sum += array[i * step];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    \n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh_vec, _CMP_GT_OS);\n        __m256d scaled = _mm256_mul_pd(data, scale_vec);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n    \n    for (; i < total; i++) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    \n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (; i + 3 < total; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d scaled_data = _mm256_mul_pd(data, scale_vec);\n        __m256d mask = _mm256_cmp_pd(data, thresh_vec, _CMP_GT_OS);\n        __m256d result = _mm256_blendv_pd(data, scaled_data, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (; i < total; ++i) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n    size_t vec_end = total - (total % 4);\n\n    for (; i < vec_end; i += 4) {\n        __m256d v_src = _mm256_loadu_pd(src + i);\n        __m256d cmp_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OS);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, cmp_mask);\n        _mm256_storeu_pd(dst + i, v_result);\n    }\n\n    for (; i < total; ++i) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    \n    // Broadcast threshold and scale to AVX vectors\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    \n    // Process 4 elements per iteration\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d mask = _mm256_cmp_pd(data, thresh_vec, _CMP_GT_OS);\n        __m256d scaled = _mm256_mul_pd(data, scale_vec);\n        __m256d result = _mm256_blendv_pd(data, scaled, mask);\n        _mm256_storeu_pd(dst + i, result);\n    }\n    \n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold)\n            dst[i] = src[i] * scale;\n        else\n            dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d ones = _mm256_set1_pd(1.0);\n\n    for (; i + 3 < total; i += 4) {\n        __m256d data = _mm256_loadu_pd(src + i);\n        __m256d cmp = _mm256_cmp_pd(data, thresh_vec, _CMP_GT_OS);\n        __m256d factors = _mm256_blendv_pd(ones, scale_vec, cmp);\n        __m256d result = _mm256_mul_pd(data, factors);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (; i < total; ++i) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double *src_row = src + indices[i] * cols;\n        double *dst_row = dst + i * cols;\n        size_t j = 0;\n        \n        // Process 4 elements per iteration using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d data = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, data);\n        }\n        \n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        const double *src_row = src + indices[i] * cols;\n        double *dst_row = dst + i * cols;\n        size_t j = 0;\n        while (j < cols) {\n            if (cols - j >= 4) {\n                __m256d data = _mm256_loadu_pd(src_row + j);\n                _mm256_storeu_pd(dst_row + j, data);\n                j += 4;\n            } else {\n                for (size_t k = j; k < cols; k++) {\n                    dst_row[k] = src_row[k];\n                }\n                j = cols;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_chunk = _mm256_loadu_pd(src_ptr + j);\n            _mm256_storeu_pd(dst_ptr + j, row_chunk);\n        }\n        for (; j < cols; ++j) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        const size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d data = _mm256_loadu_pd(src_ptr + j);\n            _mm256_storeu_pd(dst_ptr + j, data);\n        }\n        \n        for (; j < cols; ++j) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        size_t idx = indices[i];\n        const double *src_row = src + idx * cols;\n        double *dst_row = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d data = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, data);\n        }\n        \n        if (j + 1 < cols) {\n            __m128d data = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, data);\n            j += 2;\n        }\n        \n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    const __m256i sign_bit_mask = _mm256_set1_epi64x(0x8000000000000000);\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i total = _mm256_add_epi64(sum, v_scalar);\n        \n        __m256i sign_mask = _mm256_srli_epi64(total, 63);\n        __m256i adjusted = _mm256_add_epi64(total, sign_mask);\n        __m256i shifted = _mm256_srli_epi64(adjusted, 1);\n        __m256i sign_bit = _mm256_and_si256(total, sign_bit_mask);\n        __m256i result = _mm256_or_si256(shifted, sign_bit);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        int64_t temp = src1[i] + src2[i] + scalar;\n        dst[i] = temp / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i const *)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i const *)(src2 + i));\n        __m256i sum = _mm256_add_epi64(vsrc1, vsrc2);\n        sum = _mm256_add_epi64(sum, vscalar);\n        \n        __m256i carry = _mm256_srli_epi64(sum, 63);\n        __m256i adjusted = _mm256_add_epi64(sum, carry);\n        \n        __m256i sign = _mm256_cmpgt_epi64(_mm256_setzero_si256(), adjusted);\n        __m256i sign_bit = _mm256_and_si256(sign, _mm256_set1_epi64x(0x8000000000000000));\n        __m256i shifted = _mm256_srli_epi64(adjusted, 1);\n        __m256i result = _mm256_or_si256(shifted, sign_bit);\n        \n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m256i v_scalar = _mm256_set1_epi64x(scalar);\n        __m256i hi_bit = _mm256_set1_epi64x(0x8000000000000000);\n        __m256i one = _mm256_set1_epi64x(1);\n        __m256i zero = _mm256_setzero_si256();\n\n        for (; i + 3 < length; i += 4) {\n            __m256i a = _mm256_loadu_si256((__m256i const*)(src1 + i));\n            __m256i b = _mm256_loadu_si256((__m256i const*)(src2 + i));\n            __m256i sum = _mm256_add_epi64(a, b);\n            __m256i v = _mm256_add_epi64(sum, v_scalar);\n\n            __m256i sign = _mm256_and_si256(v, hi_bit);\n            __m256i shifted = _mm256_srli_epi64(v, 1);\n            __m256i arith_shifted = _mm256_or_si256(shifted, sign);\n\n            __m256i sign_mask = _mm256_cmpgt_epi64(zero, v);\n            __m256i v_and_one = _mm256_and_si256(v, one);\n            __m256i adjustment = _mm256_and_si256(v_and_one, sign_mask);\n            __m256i res = _mm256_add_epi64(arith_shifted, adjustment);\n\n            _mm256_storeu_si256((__m256i*)(dst + i), res);\n        }\n    }\n    for (; i < length; i++) {\n        int64_t x = src1[i] + src2[i] + scalar;\n        dst[i] = x / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    for (; i + 3 < length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n        \n        int64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, v_sum);\n        dst[i]   = temp[0] / 2;\n        dst[i+1] = temp[1] / 2;\n        dst[i+2] = temp[2] / 2;\n        dst[i+3] = temp[3] / 2;\n    }\n    for (; i < length; i++) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i vsum = _mm256_add_epi64(vsrc1, vsrc2);\n        vsum = _mm256_add_epi64(vsum, vscalar);\n        \n        __m256i zero = _mm256_setzero_si256();\n        __m256i sign_mask = _mm256_cmpgt_epi64(zero, vsum);\n        __m256i one = _mm256_set1_epi64x(1);\n        __m256i to_add = _mm256_and_si256(sign_mask, one);\n        __m256i adjusted = _mm256_add_epi64(vsum, to_add);\n        \n        __m256i shifted = _mm256_srli_epi64(adjusted, 1);\n        __m256i sign_bit = _mm256_and_si256(adjusted, _mm256_set1_epi64x(0x8000000000000000));\n        __m256i result = _mm256_or_si256(shifted, sign_bit);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        int64_t temp = src1[i] + src2[i] + scalar;\n        dst[i] = (temp < 0) ? (temp + 1) / 2 : temp / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <climits>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    const __m256i permute_mask = _mm256_set_epi32(0, 0, 0, 0, 6, 4, 2, 0);\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i permuted = _mm256_permutevar8x32_epi32(vec, permute_mask);\n        __m128i evens = _mm256_castsi256_si128(permuted);\n        min_vec = _mm_min_epi32(min_vec, evens);\n    }\n\n    int min_remaining = INT_MAX;\n    for (size_t j = i; j < length; j++) {\n        if (j % 2 == 0) {\n            if (src[j] < min_remaining) {\n                min_remaining = src[j];\n            }\n        }\n    }\n\n    __m128i t1 = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i t2 = _mm_min_epi32(min_vec, t1);\n    __m128i t3 = _mm_movehl_epi64(t2, t2);\n    __m128i t4 = _mm_min_epi32(t2, t3);\n    int min_simd = _mm_extract_epi32(t4, 0);\n\n    int overall_min = (min_simd < min_remaining) ? min_simd : min_remaining;\n    return overall_min;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <climits>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n    int min_val = INT_MAX;\n    size_t i = 0;\n    if (length >= 16) {\n        __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n        for (; i + 16 <= length; i += 16) {\n            __m256i v1 = _mm256_loadu_si256((__m256i const*)(src + i));\n            __m256i v2 = _mm256_loadu_si256((__m256i const*)(src + i + 8));\n            __m256i maxv = _mm256_set1_epi32(INT_MAX);\n            __m256i candidate1 = _mm256_blend_epi32(v1, maxv, 0xAA);\n            __m256i candidate2 = _mm256_blend_epi32(v2, maxv, 0xAA);\n            __m256i min_candidate = _mm256_min_epi32(candidate1, candidate2);\n            min_vec = _mm256_min_epi32(min_vec, min_candidate);\n        }\n        __m128i low128 = _mm256_castsi256_si128(min_vec);\n        __m128i high128 = _mm256_extracti128_si256(min_vec, 1);\n        __m128i min128 = _mm_min_epi32(low128, high128);\n        __m128i min_shuf = _mm_shuffle_epi32(min128, _MM_SHUFFLE(2, 3, 0, 1));\n        min128 = _mm_min_epi32(min128, min_shuf);\n        min_shuf = _mm_shuffle_epi32(min128, _MM_SHUFFLE(1, 0, 3, 2));\n        min128 = _mm_min_epi32(min128, min_shuf);\n        min_val = _mm_extract_epi32(min128, 0);\n    }\n    if (i % 2 != 0) {\n        i++;\n    }\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n    __m256i max_val = _mm256_set1_epi32(INT_MAX);\n    __m256i min_vec = max_val;\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_chunk = _mm256_blend_epi32(chunk, max_val, 0xAA);\n        min_vec = _mm256_min_epi32(min_vec, even_chunk);\n    }\n    __m128i low128 = _mm256_castsi256_si128(min_vec);\n    __m128i high128 = _mm256_extracti128_si256(min_vec, 1);\n    __m128i min128 = _mm_min_epi32(low128, high128);\n    __m128i min128_shifted = _mm_shuffle_epi32(min128, _MM_SHUFFLE(2,3,0,1));\n    __m128i min_temp = _mm_min_epi32(min128, min128_shifted);\n    __m128i min_temp_shifted = _mm_shuffle_epi32(min_temp, _MM_SHUFFLE(1,0,3,2));\n    __m128i min_val128 = _mm_min_epi32(min_temp, min_temp_shifted);\n    int min_val = _mm_extract_epi32(min_val128, 0);\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n    \n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n    \n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i candidate = _mm256_blend_epi32(data, _mm256_set1_epi32(INT_MAX), 0xAA);\n        min_vec = _mm256_min_epi32(min_vec, candidate);\n    }\n    \n    __m128i low128 = _mm256_castsi256_si128(min_vec);\n    __m128i high128 = _mm256_extracti128_si256(min_vec, 1);\n    __m128i min128 = _mm_min_epi32(low128, high128);\n    __m128i shuf = _mm_shuffle_epi32(min128, _MM_SHUFFLE(2,3,0,1));\n    __m128i mins = _mm_min_epi32(min128, shuf);\n    shuf = _mm_shuffle_epi32(mins, _MM_SHUFFLE(1,0,3,2));\n    mins = _mm_min_epi32(mins, shuf);\n    int result = _mm_extract_epi32(mins, 0);\n    \n    for (; i < length; i += 2) {\n        if (src[i] < result) {\n            result = src[i];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n    \n    static const __m256i byte_mask = _mm256_setr_epi8(\n        0, 0, 0, 0, -1, -1, -1, -1,\n        0, 0, 0, 0, -1, -1, -1, -1,\n        0, 0, 0, 0, -1, -1, -1, -1,\n        0, 0, 0, 0, -1, -1, -1, -1\n    );\n    \n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i max_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i candidate = _mm256_blendv_epi8(data, max_vec, byte_mask);\n        min_vec = _mm256_min_epi32(min_vec, candidate);\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(min_vec);\n    __m128i high128 = _mm256_extractf128_si256(min_vec, 1);\n    low128 = _mm_min_epi32(low128, high128);\n    \n    __m128i shuf1 = _mm_shuffle_epi32(low128, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i min1 = _mm_min_epi32(low128, shuf1);\n    \n    __m128i shuf2 = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1, 0, 3, 2));\n    min1 = _mm_min_epi32(min1, shuf2);\n    \n    int min_val = _mm_extract_epi32(min1, 0);\n    \n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n    \n    return min_val;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i arith = _mm256_srav_epi32(data, shift_vec);\n        __m256i logic = _mm256_srlv_epi32(data, shift_vec);\n        __m256i result = _mm256_add_epi32(arith, logic);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; ++i) {\n        int arith = src[i] >> shift;\n        int logic = (unsigned int)src[i] >> shift;\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    if (length == 0) return;\n\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32((int)shift);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i v_arith = _mm256_srav_epi32(v_src, v_shift);\n        __m256i v_logical = _mm256_srlv_epi32(v_src, v_shift);\n        __m256i v_sum = _mm256_add_epi32(v_arith, v_logical);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_sum);\n    }\n\n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        int logical = (int)((unsigned int)(src[i]) >> shift);\n        dst[i] = arith + logical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i arith = _mm256_srav_epi32(data, shift_vec);\n        __m256i logic = _mm256_srlv_epi32(data, shift_vec);\n        __m256i result = _mm256_add_epi32(arith, logic);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; ++i) {\n        int arith = src[i] >> shift;\n        int logic = (unsigned int)src[i] >> shift;\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    if (length == 0) return;\n    \n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    size_t i = 0;\n    \n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i arith_shift = _mm256_srav_epi32(data, shift_vec);\n        __m256i logical_shift = _mm256_srlv_epi32(data, shift_vec);\n        __m256i sum = _mm256_add_epi32(arith_shift, logical_shift);\n        _mm256_storeu_si256((__m256i*)(dst + i), sum);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        __m256i v_shift = _mm256_set1_epi32(shift);\n        for (; i <= length - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256((__m256i const*)(src + i));\n            __m256i arith = _mm256_srav_epi32(v, v_shift);\n            __m256i logical = _mm256_srlv_epi32(v, v_shift);\n            __m256i result = _mm256_add_epi32(arith, logical);\n            _mm256_storeu_si256((__m256i*)(dst + i), result);\n        }\n    }\n    for (; i < length; i++) {\n        int val = src[i];\n        int a = val >> shift;\n        int b = (int)((unsigned int)val >> shift);\n        dst[i] = a + b;\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t total_sum = 0;\n\n    if (total_elements == 0) {\n        return 0;\n    }\n\n    __m256i acc = _mm256_setzero_si256();\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        __m128i low128 = _mm256_castsi256_si128(vec);\n        __m128i high128 = _mm256_extractf128_si256(vec, 1);\n        __m256i low64 = _mm256_cvtepi32_epi64(low128);\n        __m256i high64 = _mm256_cvtepi32_epi64(high128);\n        acc = _mm256_add_epi64(acc, low64);\n        acc = _mm256_add_epi64(acc, high64);\n    }\n\n    __m128i low_acc = _mm256_castsi256_si128(acc);\n    __m128i high_acc = _mm256_extractf128_si256(acc, 1);\n    __m128i sum128 = _mm_add_epi64(low_acc, high_acc);\n    uint64_t tmp[2];\n    _mm_storeu_si128((__m128i*)tmp, sum128);\n    total_sum = (int64_t)(tmp[0] + tmp[1]);\n\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t total_sum = 0;\n    __m256i acc = _mm256_setzero_si256();\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        __m128i low128 = _mm256_castsi256_si128(v);\n        __m128i high128 = _mm256_extracti128_si256(v, 1);\n        __m256i low64 = _mm256_cvtepi32_epi64(low128);\n        __m256i high64 = _mm256_cvtepi32_epi64(high128);\n        acc = _mm256_add_epi64(acc, low64);\n        acc = _mm256_add_epi64(acc, high64);\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, acc);\n    total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t total_sum = 0;\n\n    if (total_elements >= 8) {\n        __m256i acc_low = _mm256_setzero_si256();\n        __m256i acc_high = _mm256_setzero_si256();\n\n        for (; i <= total_elements - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n            __m128i v_low = _mm256_castsi256_si128(v);\n            __m128i v_high = _mm256_extractf128_si256(v, 1);\n            __m256i v_low_64 = _mm256_cvtepi32_epi64(v_low);\n            __m256i v_high_64 = _mm256_cvtepi32_epi64(v_high);\n            acc_low = _mm256_add_epi64(acc_low, v_low_64);\n            acc_high = _mm256_add_epi64(acc_high, v_high_64);\n        }\n\n        __m256i acc = _mm256_add_epi64(acc_low, acc_high);\n        __m128i low_acc = _mm256_castsi256_si128(acc);\n        __m128i high_acc = _mm256_extractf128_si256(acc, 1);\n        __m128i sum128 = _mm_add_epi64(low_acc, high_acc);\n        uint64_t tmp[2];\n        _mm_storeu_si128((__m128i*)tmp, sum128);\n        total_sum = tmp[0] + tmp[1];\n    }\n\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    __m256i acc = _mm256_setzero_si256();\n    \n    for (; i + 7 < total; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        __m128i low128 = _mm256_castsi256_si128(vec);\n        __m128i high128 = _mm256_extracti128_si256(vec, 1);\n        __m256i low64 = _mm256_cvtepi32_epi64(low128);\n        __m256i high64 = _mm256_cvtepi32_epi64(high128);\n        __m256i sum8 = _mm256_add_epi64(low64, high64);\n        acc = _mm256_add_epi64(acc, sum8);\n    }\n    \n    __m128i low_acc = _mm256_castsi256_si128(acc);\n    __m128i high_acc = _mm256_extracti128_si256(acc, 1);\n    __m128i sum128 = _mm_add_epi64(low_acc, high_acc);\n    __m128i high64 = _mm_unpackhi_epi64(sum128, sum128);\n    __m128i result = _mm_add_epi64(sum128, high64);\n    int64_t total_sum = _mm_cvtsi128_si64(result);\n    \n    for (; i < total; i++) {\n        total_sum += matrix[i];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t total_sum = 0;\n\n    if (total_elements >= 8) {\n        __m256i acc_low = _mm256_setzero_si256();\n        __m256i acc_high = _mm256_setzero_si256();\n        \n        for (; i <= total_elements - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256((__m256i const*)(matrix + i));\n            __m128i v_low = _mm256_extracti128_si256(v, 0);\n            __m128i v_high = _mm256_extracti128_si256(v, 1);\n            __m256i low64 = _mm256_cvtepi32_epi64(v_low);\n            __m256i high64 = _mm256_cvtepi32_epi64(v_high);\n            acc_low = _mm256_add_epi64(acc_low, low64);\n            acc_high = _mm256_add_epi64(acc_high, high64);\n        }\n        \n        acc_low = _mm256_add_epi64(acc_low, acc_high);\n        __m128i low_acc = _mm256_extracti128_si256(acc_low, 0);\n        __m128i high_acc = _mm256_extracti128_si256(acc_low, 1);\n        __m128i sum128 = _mm_add_epi64(low_acc, high_acc);\n        int64_t partial_sum[2];\n        _mm_storeu_si128((__m128i*)partial_sum, sum128);\n        total_sum = partial_sum[0] + partial_sum[1];\n    }\n\n    for (; i < total_elements; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n    int8_t global_max_val = -128;\n    size_t global_max_idx = 0;\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m128i v_low = _mm256_castsi256_si128(v);\n        __m128i v_high = _mm256_extracti128_si256(v, 1);\n        __m128i max128 = _mm_max_epi8(v_low, v_high);\n        __m128i max64 = _mm_max_epi8(max128, _mm_srli_si128(max128, 8));\n        __m128i max32 = _mm_max_epi8(max64, _mm_srli_si128(max64, 4));\n        __m128i max16 = _mm_max_epi8(max32, _mm_srli_si128(max32, 2));\n        __m128i max8 = _mm_max_epi8(max16, _mm_srli_si128(max16, 1));\n        int8_t max_val_in_chunk = _mm_extract_epi8(max8, 0);\n\n        __m256i max_bc = _mm256_set1_epi8(max_val_in_chunk);\n        __m256i mask = _mm256_cmpeq_epi8(v, max_bc);\n        unsigned mask32 = (unsigned)_mm256_movemask_epi8(mask);\n        int pos_in_chunk = __builtin_ctz(mask32);\n        size_t candidate_idx = i + pos_in_chunk;\n\n        if (max_val_in_chunk > global_max_val) {\n            global_max_val = max_val_in_chunk;\n            global_max_idx = candidate_idx;\n        } else if (max_val_in_chunk == global_max_val && candidate_idx < global_max_idx) {\n            global_max_idx = candidate_idx;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > global_max_val) {\n            global_max_val = src[i];\n            global_max_idx = i;\n        } else if (src[i] == global_max_val && i < global_max_idx) {\n            global_max_idx = i;\n        }\n    }\n\n    return global_max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <limits.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t global_index = 0;\n    int8_t global_max = INT8_MIN;\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i current_max_vec = _mm256_set1_epi8(global_max);\n        __m256i cmp = _mm256_cmpgt_epi8(chunk, current_max_vec);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            __m128i low = _mm256_castsi256_si128(chunk);\n            __m128i high = _mm256_extracti128_si256(chunk, 1);\n            __m128i max128 = _mm_max_epi8(low, high);\n            __m128i shifted = _mm_srli_si128(max128, 8);\n            __m128i max8 = _mm_max_epi8(max128, shifted);\n            shifted = _mm_srli_si128(max8, 4);\n            __m128i max4 = _mm_max_epi8(max8, shifted);\n            shifted = _mm_srli_si128(max4, 2);\n            __m128i max2 = _mm_max_epi8(max4, shifted);\n            shifted = _mm_srli_si128(max2, 1);\n            __m128i max1 = _mm_max_epi8(max2, shifted);\n            int8_t chunk_max = (int8_t)_mm_extract_epi8(max1, 0);\n            global_max = chunk_max;\n            __m256i max_val_vec = _mm256_set1_epi8(global_max);\n            __m256i cmp_eq = _mm256_cmpeq_epi8(chunk, max_val_vec);\n            unsigned mask = (unsigned)_mm256_movemask_epi8(cmp_eq);\n            unsigned pos = (unsigned)__builtin_ctz(mask);\n            global_index = i + pos;\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_index = i;\n        }\n    }\n\n    return global_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    int8_t global_max = -128;\n    size_t global_index = 0;\n    __m256i v_global_max = _mm256_set1_epi8(global_max);\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(chunk, v_global_max);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            int pos = __builtin_ctz(mask);\n            global_max = src[i + pos];\n            global_index = i + pos;\n            for (size_t j = i + pos + 1; j < i + 32; j++) {\n                if (src[j] > global_max) {\n                    global_max = src[j];\n                    global_index = j;\n                }\n            }\n            v_global_max = _mm256_set1_epi8(global_max);\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_index = i;\n        }\n    }\n\n    return global_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <limits.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    int8_t global_max = INT8_MIN;\n    size_t global_index = 0;\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i global_broadcast = _mm256_set1_epi8(global_max);\n        __m256i cmp = _mm256_cmpgt_epi8(chunk, global_broadcast);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            __m128i low = _mm256_castsi256_si128(chunk);\n            __m128i high = _mm256_extracti128_si256(chunk, 1);\n            __m128i max128 = _mm_max_epi8(low, high);\n            __m128i v1 = _mm_max_epi8(max128, _mm_srli_si128(max128, 8));\n            __m128i v2 = _mm_max_epi8(v1, _mm_srli_si128(v1, 4));\n            __m128i v3 = _mm_max_epi8(v2, _mm_srli_si128(v2, 2));\n            __m128i v4 = _mm_max_epi8(v3, _mm_srli_si128(v3, 1));\n            int8_t max_in_chunk = (int8_t)_mm_extract_epi8(v4, 0);\n\n            global_max = max_in_chunk;\n            __m256i max_broadcast = _mm256_set1_epi8(global_max);\n            __m256i mask = _mm256_cmpeq_epi8(chunk, max_broadcast);\n            int bitmask = _mm256_movemask_epi8(mask);\n            if (bitmask != 0) {\n                int pos = __builtin_ffs(bitmask) - 1;\n                global_index = i + pos;\n            }\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_index = i;\n        }\n    }\n\n    return global_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t full_chunks = length / 32;\n    size_t tail_start = full_chunks * 32;\n    int8_t global_max_val = -128;\n    size_t global_max_index = 0;\n\n    for (size_t i = 0; i < full_chunks; i++) {\n        size_t base = i * 32;\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + base));\n        __m128i v_low = _mm256_castsi256_si128(chunk);\n        __m128i v_high = _mm256_extracti128_si256(chunk, 1);\n        __m128i v_max128 = _mm_max_epi8(v_low, v_high);\n\n        __m128i v_shifted = _mm_bsrli_si128(v_max128, 8);\n        v_max128 = _mm_max_epi8(v_max128, v_shifted);\n        v_shifted = _mm_bsrli_si128(v_max128, 4);\n        v_max128 = _mm_max_epi8(v_max128, v_shifted);\n        v_shifted = _mm_bsrli_si128(v_max128, 2);\n        v_max128 = _mm_max_epi8(v_max128, v_shifted);\n        v_shifted = _mm_bsrli_si128(v_max128, 1);\n        v_max128 = _mm_max_epi8(v_max128, v_shifted);\n        int8_t max_val = _mm_extract_epi8(v_max128, 0);\n\n        __m256i max_val_vec = _mm256_set1_epi8(max_val);\n        __m256i mask_vec = _mm256_cmpeq_epi8(chunk, max_val_vec);\n        unsigned int mask32 = (unsigned int)_mm256_movemask_epi8(mask_vec);\n        unsigned int index_in_chunk;\n        if (mask32) {\n            index_in_chunk = _tzcnt_u32(mask32);\n        } else {\n            index_in_chunk = 0;\n        }\n\n        if (max_val > global_max_val) {\n            global_max_val = max_val;\n            global_max_index = base + index_in_chunk;\n        }\n    }\n\n    for (size_t i = tail_start; i < length; i++) {\n        if (src[i] > global_max_val) {\n            global_max_val = src[i];\n            global_max_index = i;\n        }\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vector_size = 16;\n\n    if (length >= vector_size) {\n        for (; i <= length - vector_size; i += vector_size) {\n            __m256i indices_low = _mm256_setr_epi32(\n                (uint32_t)(i), (uint32_t)(i+1), (uint32_t)(i+2), (uint32_t)(i+3),\n                (uint32_t)(i+4), (uint32_t)(i+5), (uint32_t)(i+6), (uint32_t)(i+7)\n            );\n            __m256i indices_high = _mm256_setr_epi32(\n                (uint32_t)(i+8), (uint32_t)(i+9), (uint32_t)(i+10), (uint32_t)(i+11),\n                (uint32_t)(i+12), (uint32_t)(i+13), (uint32_t)(i+14), (uint32_t)(i+15)\n            );\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n            __m256i and_low = _mm256_and_si256(indices_low, mask_vec);\n            __m256i and_high = _mm256_and_si256(indices_high, mask_vec);\n\n            __m256i cmp_low = _mm256_cmpeq_epi32(and_low, _mm256_setzero_si256());\n            __m256i cmp_high = _mm256_cmpeq_epi32(and_high, _mm256_setzero_si256());\n\n            cmp_low = _mm256_xor_si256(cmp_low, _mm256_set1_epi32(-1));\n            cmp_high = _mm256_xor_si256(cmp_high, _mm256_set1_epi32(-1));\n\n            __m256i mask_16 = _mm256_packs_epi32(cmp_low, cmp_high);\n            __m256i permuted = _mm256_permute4x64_epi64(mask_16, 0xD8);\n\n            __m256i v_src1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n            __m256i v_src2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n\n            __m256i blended = _mm256_or_si256(\n                _mm256_and_si256(permuted, v_src1),\n                _mm256_andnot_si256(permuted, v_src2)\n            );\n            _mm256_storeu_si256((__m256i*)(dst + i), blended);\n        }\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        __m256i mask_vec = _mm256_set1_epi32(mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i all_ones = _mm256_set1_epi32(-1);\n        \n        for (; i <= length - 16; i += 16) {\n            __m256i src1_vec = _mm256_loadu_si256((__m256i const*)(src1 + i));\n            __m256i src2_vec = _mm256_loadu_si256((__m256i const*)(src2 + i));\n            \n            __m256i base_vec = _mm256_set1_epi32((uint32_t)i);\n            __m256i offsets1 = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n            __m256i indices1 = _mm256_add_epi32(base_vec, offsets1);\n            __m256i offsets2 = _mm256_setr_epi32(8,9,10,11,12,13,14,15);\n            __m256i indices2 = _mm256_add_epi32(base_vec, offsets2);\n            \n            __m256i and1 = _mm256_and_si256(indices1, mask_vec);\n            __m256i and2 = _mm256_and_si256(indices2, mask_vec);\n            \n            __m256i cmp1 = _mm256_cmpeq_epi32(and1, zero);\n            __m256i cmp2 = _mm256_cmpeq_epi32(and2, zero);\n            cmp1 = _mm256_xor_si256(cmp1, all_ones);\n            cmp2 = _mm256_xor_si256(cmp2, all_ones);\n            \n            __m256i mask16 = _mm256_packs_epi32(cmp1, cmp2);\n            __m256i res = _mm256_blendv_epi8(src2_vec, src1_vec, mask16);\n            _mm256_storeu_si256((__m256i*)(dst + i), res);\n        }\n    }\n    \n    for (; i < length; i++) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        __m256i step = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n        __m256i mask_vec = _mm256_set1_epi32(mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i all_ones = _mm256_set1_epi32(-1);\n\n        for (; i <= length - 16; i += 16) {\n            __m256i src1_data = _mm256_loadu_si256((const __m256i*)(src1 + i));\n            __m256i src2_data = _mm256_loadu_si256((const __m256i*)(src2 + i));\n\n            __m256i base1 = _mm256_set1_epi32((uint32_t)i);\n            __m256i idx1 = _mm256_add_epi32(base1, step);\n            __m256i and1 = _mm256_and_si256(idx1, mask_vec);\n            __m256i cmp1 = _mm256_cmpeq_epi32(and1, zero);\n            cmp1 = _mm256_xor_si256(cmp1, all_ones);\n\n            __m256i base2 = _mm256_set1_epi32((uint32_t)(i+8));\n            __m256i idx2 = _mm256_add_epi32(base2, step);\n            __m256i and2 = _mm256_and_si256(idx2, mask_vec);\n            __m256i cmp2 = _mm256_cmpeq_epi32(and2, zero);\n            cmp2 = _mm256_xor_si256(cmp2, all_ones);\n\n            __m128i cmp1_low = _mm256_castsi256_si128(cmp1);\n            __m128i cmp1_high = _mm256_extracti128_si256(cmp1, 1);\n            __m128i cond_low = _mm_packs_epi32(cmp1_low, cmp1_high);\n\n            __m128i cmp2_low = _mm256_castsi256_si128(cmp2);\n            __m128i cmp2_high = _mm256_extracti128_si256(cmp2, 1);\n            __m128i cond_high = _mm_packs_epi32(cmp2_low, cmp2_high);\n\n            __m128i src1_low = _mm256_castsi256_si128(src1_data);\n            __m128i src1_high = _mm256_extracti128_si256(src1_data, 1);\n            __m128i src2_low = _mm256_castsi256_si128(src2_data);\n            __m128i src2_high = _mm256_extracti128_si256(src2_data, 1);\n\n            __m128i res_low = _mm_or_si128(\n                _mm_and_si128(cond_low, src1_low),\n                _mm_andnot_si128(cond_low, src2_low)\n            );\n            __m128i res_high = _mm_or_si128(\n                _mm_and_si128(cond_high, src1_high),\n                _mm_andnot_si128(cond_high, src2_high)\n            );\n\n            __m256i res = _mm256_castsi128_si256(res_low);\n            res = _mm256_inserti128_si256(res, res_high, 1);\n            _mm256_storeu_si256((__m256i*)(dst + i), res);\n        }\n    }\n\n    for (; i < length; i++) {\n        if (((uint32_t)i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i vmask = _mm256_set1_epi32(mask);\n    \n    for (; i + 15 < length; i += 16) {\n        __m256i vbase1 = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i vbase2 = _mm256_setr_epi32(i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        \n        __m256i vand1 = _mm256_and_si256(vbase1, vmask);\n        __m256i vand2 = _mm256_and_si256(vbase2, vmask);\n        \n        __m256i vcmp1 = _mm256_cmpgt_epi32(vand1, _mm256_setzero_si256());\n        __m256i vcmp2 = _mm256_cmpgt_epi32(vand2, _mm256_setzero_si256());\n        \n        __m256i vcond = _mm256_packs_epi32(vcmp1, vcmp2);\n        \n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        \n        __m256i vres = _mm256_blendv_epi8(vsrc2, vsrc1, vcond);\n        _mm256_storeu_si256((__m256i*)(dst + i), vres);\n    }\n    \n    for (; i < length; i++) {\n        uint32_t idx = (uint32_t)i;\n        dst[i] = (idx & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    if (length == 0) return;\n\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    __m256i step = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i all_ones = _mm256_set1_epi32(-1);\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i base_low = _mm256_set1_epi32(static_cast<uint32_t>(i));\n        __m256i indices_low = _mm256_add_epi32(base_low, step);\n        __m256i base_high = _mm256_set1_epi32(static_cast<uint32_t>(i + 8));\n        __m256i indices_high = _mm256_add_epi32(base_high, step);\n\n        __m256i and_low = _mm256_and_si256(indices_low, mask_vec);\n        __m256i and_high = _mm256_and_si256(indices_high, mask_vec);\n\n        __m256i cmp_low = _mm256_cmpeq_epi32(and_low, zero);\n        __m256i cmp_high = _mm256_cmpeq_epi32(and_high, zero);\n\n        cmp_low = _mm256_xor_si256(cmp_low, all_ones);\n        cmp_high = _mm256_xor_si256(cmp_high, all_ones);\n\n        __m256i packed_cmp = _mm256_packs_epi32(cmp_low, cmp_high);\n        __m256i permuted_cmp = _mm256_permute4x64_epi64(packed_cmp, 0x27);\n\n        __m256i src1_data = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i src2_data = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i res = _mm256_blendv_epi8(src2_data, src1_data, permuted_cmp);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; i++) {\n        if ((static_cast<uint32_t>(i) & mask) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i mask55 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask33 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask0F = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i multiplier = _mm256_set1_epi32(0x01010101);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i x = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        __m256i cnt = _mm256_and_si256(x, mask55);\n        __m256i tmp = _mm256_srli_epi32(x, 1);\n        tmp = _mm256_and_si256(tmp, mask55);\n        cnt = _mm256_add_epi32(cnt, tmp);\n        \n        tmp = _mm256_srli_epi32(cnt, 2);\n        tmp = _mm256_and_si256(tmp, mask33);\n        cnt = _mm256_and_si256(cnt, mask33);\n        cnt = _mm256_add_epi32(cnt, tmp);\n        \n        tmp = _mm256_srli_epi32(cnt, 4);\n        tmp = _mm256_and_si256(tmp, mask0F);\n        cnt = _mm256_and_si256(cnt, mask0F);\n        cnt = _mm256_add_epi32(cnt, tmp);\n        \n        cnt = _mm256_mullo_epi32(cnt, multiplier);\n        cnt = _mm256_srli_epi32(cnt, 24);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), cnt);\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x & 0x0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F);\n        dst[i] = (x * 0x01010101) >> 24;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i mask55 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask33 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask0F = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i maskFF = _mm256_set1_epi32(0x000000FF);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n\n        __m256i step1 = _mm256_add_epi32(\n            _mm256_and_si256(data, mask55),\n            _mm256_and_si256(_mm256_srli_epi32(data, 1), mask55)\n        );\n\n        __m256i step2 = _mm256_add_epi32(\n            _mm256_and_si256(step1, mask33),\n            _mm256_and_si256(_mm256_srli_epi32(step1, 2), mask33)\n        );\n\n        __m256i step3 = _mm256_add_epi32(\n            _mm256_and_si256(step2, mask0F),\n            _mm256_and_si256(_mm256_srli_epi32(step2, 4), mask0F)\n        );\n\n        __m256i step4 = _mm256_add_epi32(step3, _mm256_srli_epi32(step3, 8));\n        step4 = _mm256_add_epi32(step4, _mm256_srli_epi32(step4, 16));\n        __m256i result = _mm256_and_si256(step4, maskFF);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x & 0x0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F);\n        x = (x + (x >> 8));\n        x = (x + (x >> 16)) & 0xFF;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    \n    const __m128i popcnt_lut = _mm_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, \n        1, 2, 2, 3, 2, 3, 3, 4\n    );\n    __m256i lut = _mm256_broadcastsi128_si256(popcnt_lut);\n    __m256i low_mask = _mm256_set1_epi8(0x0F);\n    __m256i ones_byte = _mm256_set1_epi8(1);\n    __m256i ones_word = _mm256_set1_epi16(1);\n    \n    for (; i + 8 <= length; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i low_nibbles = _mm256_and_si256(v, low_mask);\n        __m256i high_nibbles = _mm256_and_si256(_mm256_srli_epi16(v, 4), low_mask);\n        \n        __m256i pop_low = _mm256_shuffle_epi8(lut, low_nibbles);\n        __m256i pop_high = _mm256_shuffle_epi8(lut, high_nibbles);\n        __m256i pop_per_byte = _mm256_add_epi8(pop_low, pop_high);\n        \n        __m256i horizontal_sum = _mm256_maddubs_epi16(pop_per_byte, ones_byte);\n        __m256i sum32 = _mm256_madd_epi16(horizontal_sum, ones_word);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), sum32);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i m1 = _mm256_set1_epi32(0x55555555);\n    const __m256i m2 = _mm256_set1_epi32(0x33333333);\n    const __m256i m4 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mul = _mm256_set1_epi32(0x01010101);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i t1 = _mm256_sub_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 1), m1));\n        __m256i t2 = _mm256_add_epi32(\n            _mm256_and_si256(t1, m2),\n            _mm256_and_si256(_mm256_srli_epi32(t1, 2), m2)\n        );\n        __m256i t3 = _mm256_and_si256(\n            _mm256_add_epi32(t2, _mm256_srli_epi32(t2, 4)),\n            m4\n        );\n        __m256i t4 = _mm256_mullo_epi32(t3, mul);\n        __m256i res = _mm256_srli_epi32(t4, 24);\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        dst[i] = (x * 0x01010101) >> 24;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_step = 8;\n\n    if (length >= vec_step) {\n        const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n        const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n        const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n        const __m256i mult = _mm256_set1_epi32(0x01010101);\n\n        for (; i + vec_step <= length; i += vec_step) {\n            __m256i v = _mm256_loadu_si256((const __m256i*)(src + i));\n            __m256i t1 = _mm256_srli_epi32(v, 1);\n            t1 = _mm256_and_si256(t1, mask1);\n            v = _mm256_sub_epi32(v, t1);\n\n            __m256i t2 = _mm256_and_si256(v, mask2);\n            __m256i t3 = _mm256_srli_epi32(v, 2);\n            t3 = _mm256_and_si256(t3, mask2);\n            v = _mm256_add_epi32(t2, t3);\n\n            __m256i t4 = _mm256_srli_epi32(v, 4);\n            v = _mm256_add_epi32(v, t4);\n            v = _mm256_and_si256(v, mask3);\n\n            v = _mm256_mullo_epi32(v, mult);\n            v = _mm256_srli_epi32(v, 24);\n\n            _mm256_storeu_si256((__m256i*)(dst + i), v);\n        }\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 16;\n    const size_t vectors = length / vector_elements;\n\n    for (; i < vectors * vector_elements; i += vector_elements) {\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i vsum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), vsum);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 16;\n    \n    for (; i + simd_elements <= length; i += simd_elements) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i result = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 16;\n    const size_t simd_bound = length - (length % simd_elements);\n\n    for (; i < simd_bound; i += simd_elements) {\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i result = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i result = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_elements = 16;\n    const size_t vec_step = vec_elements;\n    size_t vec_iters = length / vec_step;\n\n    for (; i < vec_iters * vec_step; i += vec_step) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i result = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t total = m * p;\n    size_t idx = 0;\n    for (; idx + 4 <= total; idx += 4) {\n        _mm256_storeu_pd(C + idx, _mm256_setzero_pd());\n    }\n    for (; idx < total; idx++) {\n        C[idx] = 0.0;\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t k = 0; k < n; k++) {\n            double a_val = A[i * n + k];\n            if (a_val < -100.0 || a_val > 100.0) {\n                continue;\n            }\n            __m256d a_vec = _mm256_set1_pd(a_val);\n            size_t j = 0;\n            for (; j + 4 <= p; j += 4) {\n                __m256d b_vec = _mm256_loadu_pd(B + k * p + j);\n                __m256d low = _mm256_set1_pd(-100.0);\n                __m256d high = _mm256_set1_pd(100.0);\n                __m256d mask_low = _mm256_cmp_pd(b_vec, low, _CMP_GE_OQ);\n                __m256d mask_high = _mm256_cmp_pd(b_vec, high, _CMP_LE_OQ);\n                __m256d mask = _mm256_and_pd(mask_low, mask_high);\n                __m256d product = _mm256_mul_pd(a_vec, b_vec);\n                product = _mm256_and_pd(mask, product);\n                __m256d c_vec = _mm256_loadu_pd(C + i * p + j);\n                c_vec = _mm256_add_pd(c_vec, product);\n                _mm256_storeu_pd(C + i * p + j, c_vec);\n            }\n            for (; j < p; j++) {\n                double b_val = B[k * p + j];\n                if (b_val >= -100.0 && b_val <= 100.0) {\n                    C[i * p + j] += a_val * b_val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t total = m * p;\n    size_t i_total = 0;\n    __m256d zero = _mm256_setzero_pd();\n    for (; i_total + 3 < total; i_total += 4) {\n        _mm256_storeu_pd(C + i_total, zero);\n    }\n    for (; i_total < total; i_total++) {\n        C[i_total] = 0.0;\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < n; j++) {\n            double a_val = A[i * n + j];\n            if (a_val >= -100.0 && a_val <= 100.0) {\n                size_t k = 0;\n                for (; k <= p - 4; k += 4) {\n                    __m256d b_vec = _mm256_loadu_pd(B + j * p + k);\n                    __m256d low_bound = _mm256_set1_pd(-100.0);\n                    __m256d high_bound = _mm256_set1_pd(100.0);\n                    __m256d mask_low = _mm256_cmp_pd(b_vec, low_bound, _CMP_GE_OQ);\n                    __m256d mask_high = _mm256_cmp_pd(b_vec, high_bound, _CMP_LE_OQ);\n                    __m256d mask = _mm256_and_pd(mask_low, mask_high);\n                    __m256d a_vec = _mm256_set1_pd(a_val);\n                    __m256d product = _mm256_mul_pd(a_vec, b_vec);\n                    __m256d masked_product = _mm256_and_pd(mask, product);\n                    __m256d c_vec = _mm256_loadu_pd(C + i * p + k);\n                    c_vec = _mm256_add_pd(c_vec, masked_product);\n                    _mm256_storeu_pd(C + i * p + k, c_vec);\n                }\n                for (; k < p; k++) {\n                    double b_val = B[j * p + k];\n                    if (b_val >= -100.0 && b_val <= 100.0) {\n                        C[i * p + k] += a_val * b_val;\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double low_bound_val = -100.0;\n    const double high_bound_val = 100.0;\n    const __m256d low_bound = _mm256_set1_pd(low_bound_val);\n    const __m256d high_bound = _mm256_set1_pd(high_bound_val);\n\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j + 3 < p; j += 4) {\n            __m256d c0 = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                double a_val = A[i * n + k];\n                bool condA = (a_val >= low_bound_val && a_val <= high_bound_val);\n                __m256d a_vec = _mm256_set1_pd(a_val);\n                __m256d a_mask = _mm256_set1_pd(condA ? 1.0 : 0.0);\n                a_vec = _mm256_mul_pd(a_vec, a_mask);\n\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d cmp_low = _mm256_cmp_pd(b_vec, low_bound, _CMP_GE_OQ);\n                __m256d cmp_high = _mm256_cmp_pd(b_vec, high_bound, _CMP_LE_OQ);\n                __m256d in_rangeB = _mm256_and_pd(cmp_low, cmp_high);\n                __m256d b_masked = _mm256_blendv_pd(_mm256_setzero_pd(), b_vec, in_rangeB);\n\n                __m256d product = _mm256_mul_pd(a_vec, b_masked);\n                c0 = _mm256_add_pd(c0, product);\n            }\n            _mm256_storeu_pd(&C[i * p + j], c0);\n        }\n\n        for (; j < p; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= low_bound_val && a_val <= high_bound_val && \n                    b_val >= low_bound_val && b_val <= high_bound_val) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j <= p - 4; j += 4) {\n            __m256d c_acc = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                double a_val = A[i * n + k];\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d a_vec = _mm256_set1_pd(a_val);\n                __m256d low = _mm256_set1_pd(-100.0);\n                __m256d high = _mm256_set1_pd(100.0);\n                \n                __m256d mask_a = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vec, low, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vec, high, _CMP_LE_OQ)\n                );\n                __m256d mask_b = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vec, low, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vec, high, _CMP_LE_OQ)\n                );\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                \n                __m256d product = _mm256_mul_pd(a_vec, b_vec);\n                product = _mm256_and_pd(product, mask);\n                c_acc = _mm256_add_pd(c_acc, product);\n            }\n            _mm256_storeu_pd(&C[i * p + j], c_acc);\n        }\n        \n        for (; j < p; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            C[i * p + j] = 0.0;\n        }\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t jb = 0; jb < p; jb += 4) {\n            size_t rem = p - jb;\n            __m256d c_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                double a_val = A[i * n + k];\n                if (a_val >= -100.0 && a_val <= 100.0) {\n                    if (rem >= 4) {\n                        __m256d b_vec = _mm256_loadu_pd(B + k * p + jb);\n                        __m256d low = _mm256_set1_pd(-100.0);\n                        __m256d high = _mm256_set1_pd(100.0);\n                        __m256d mask_low = _mm256_cmp_pd(b_vec, low, _CMP_GE_OQ);\n                        __m256d mask_high = _mm256_cmp_pd(b_vec, high, _CMP_LE_OQ);\n                        __m256d mask = _mm256_and_pd(mask_low, mask_high);\n                        b_vec = _mm256_and_pd(b_vec, mask);\n                        __m256d a_vec = _mm256_set1_pd(a_val);\n                        c_vec = _mm256_add_pd(c_vec, _mm256_mul_pd(a_vec, b_vec));\n                    } else {\n                        __m256i loadmask = _mm256_setr_epi64x(\n                            (rem > 0) ? -1 : 0,\n                            (rem > 1) ? -1 : 0,\n                            (rem > 2) ? -1 : 0,\n                            0\n                        );\n                        __m256d b_vec = _mm256_maskload_pd(B + k * p + jb, loadmask);\n                        __m256d low = _mm256_set1_pd(-100.0);\n                        __m256d high = _mm256_set1_pd(100.0);\n                        __m256d mask_low = _mm256_cmp_pd(b_vec, low, _CMP_GE_OQ);\n                        __m256d mask_high = _mm256_cmp_pd(b_vec, high, _CMP_LE_OQ);\n                        __m256d mask = _mm256_and_pd(mask_low, mask_high);\n                        b_vec = _mm256_and_pd(b_vec, mask);\n                        __m256d a_vec = _mm256_set1_pd(a_val);\n                        c_vec = _mm256_add_pd(c_vec, _mm256_mul_pd(a_vec, b_vec));\n                    }\n                }\n            }\n            if (rem >= 4) {\n                _mm256_storeu_pd(C + i * p + jb, c_vec);\n            } else {\n                __m256i storemask = _mm256_setr_epi64x(\n                    (rem > 0) ? -1 : 0,\n                    (rem > 1) ? -1 : 0,\n                    (rem > 2) ? -1 : 0,\n                    0\n                );\n                _mm256_maskstore_pd(C + i * p + jb, storemask, c_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i sum = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), sum);\n    }\n\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n    \n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const size_t vec_size = 4;\n\n    for (; i + vec_size <= total_elements; i += vec_size) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < total; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 8;\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; i++) {\n        __m128 vs1 = _mm_load_ss(src1 + i);\n        __m128 vs2 = _mm_load_ss(src2 + i);\n        __m128 vsum = _mm_add_ss(vs1, vs2);\n        __m128 vround = _mm_round_ss(vsum, vsum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_store_ss(dst + i, vround);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_floats = 8;\n    size_t aligned_length = length & ~(vec_floats - 1);\n\n    for (; i < aligned_length; i += vec_floats) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_NINT);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; i++) {\n        float sum = src1[i] + src2[i];\n        dst[i] = nearbyintf(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_floats = 8;\n    size_t vec_end = length - (length % vec_floats);\n    \n    for (; i < vec_end; i += vec_floats) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; i++) {\n        float result = src1[i] + src2[i];\n        dst[i] = (result >= 0.0f) ? (int)(result + 0.5f) : (int)(result - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_step = 8;\n\n    for (; i + vec_step <= length; i += vec_step) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(v1, v2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (sum >= 0) ? (int)(sum + 0.5f) : (int)(sum - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_floats = 8;\n    size_t vec_iters = length / vec_floats;\n\n    __m256 vec1, vec2, res_vec;\n    for (; i < vec_iters * vec_floats; i += vec_floats) {\n        vec1 = _mm256_loadu_ps(src1 + i);\n        vec2 = _mm256_loadu_ps(src2 + i);\n        res_vec = _mm256_add_ps(vec1, vec2);\n        res_vec = _mm256_round_ps(res_vec, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, res_vec);\n    }\n\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = _mm_cvtss_f32(_mm_round_ss(_mm_set_ss(0.0f), _mm_set_ss(sum), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 condition2_mask = _mm256_setr_ps(-1.0f, 0.0f, -1.0f, 0.0f, -1.0f, 0.0f, -1.0f, 0.0f);\n    const __m256 diff_threshold = _mm256_set1_ps(50.0f);\n    const __m256 sign_mask = _mm256_set1_ps(-0.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_andnot_ps(sign_mask, diff);\n        __m256 condition1 = _mm256_cmp_ps(abs_diff, diff_threshold, _CMP_LE_OQ);\n        __m256 condition = _mm256_and_ps(condition1, condition2_mask);\n\n        __m256 product = _mm256_mul_ps(v1, v2);\n        __m256 ceil_val = _mm256_ceil_ps(product);\n        __m256 result = _mm256_blendv_ps(neg_one, ceil_val, condition);\n\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    for (; i < length; i++) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    // Precompute masks\n    const __m256 abs_mask = _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF));\n    const __m256i cond_b_mask_i = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n    const __m256 cond_b_mask = _mm256_castsi256_ps(cond_b_mask_i);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 fifty = _mm256_set1_ps(50.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_and_ps(diff, abs_mask);\n        \n        __m256 cond_a = _mm256_cmp_ps(abs_diff, fifty, _CMP_LE_OQ);\n        __m256 mask = _mm256_and_ps(cond_a, cond_b_mask);\n        \n        __m256 prod = _mm256_mul_ps(v1, v2);\n        __m256 ceil_val = _mm256_ceil_ps(prod);\n        \n        __m256 res = _mm256_blendv_ps(neg_one, ceil_val, mask);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256i lane_parity_vec = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n    const __m256 abs_mask = _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF));\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256i one = _mm256_set1_epi32(1);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_and_ps(diff, abs_mask);\n        __m256 cmp = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n        \n        uint32_t base_parity = i & 1;\n        __m256i base_parity_vec = _mm256_set1_epi32(base_parity);\n        __m256i global_parity = _mm256_xor_si256(base_parity_vec, lane_parity_vec);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(global_parity, one), _mm256_setzero_si256());\n        \n        __m256i mask_int = _mm256_castps_si256(cmp);\n        __m256i combined_mask = _mm256_and_si256(even_mask, mask_int);\n        __m256 mask_float = _mm256_castsi256_ps(combined_mask);\n        \n        __m256 product = _mm256_mul_ps(v1, v2);\n        __m256 rounded = _mm256_ceil_ps(product);\n        __m256 result = _mm256_blendv_ps(neg_one, rounded, mask_float);\n        \n        _mm256_storeu_ps(dst + i, result);\n    }\n    \n    for (; i < length; ++i) {\n        if (i % 2 == 0 && fabsf(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    static const __m256i even_mask_int = _mm256_setr_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n    static const __m256 even_mask = _mm256_castsi256_ps(even_mask_int);\n    const __m256 abs_mask = _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF));\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        \n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_and_ps(diff, abs_mask);\n        __m256 cmp_mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n        __m256 cond_mask = _mm256_and_ps(cmp_mask, even_mask);\n        \n        __m256 product = _mm256_mul_ps(a, b);\n        __m256 rounded = _mm256_ceil_ps(product);\n        \n        __m256 result = _mm256_blendv_ps(neg_one, rounded, cond_mask);\n        _mm256_storeu_ps(dst + i, result);\n    }\n    \n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    // Precompute constant vectors\n    __m256 abs_mask = _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF));\n    __m256 fifty = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n    \n    // Create condition2 mask (even indices)\n    __m256i lane_indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i lane_parity = _mm256_and_si256(lane_indices, ones);\n    __m256i condition2_mask_int = _mm256_cmpeq_epi32(lane_parity, _mm256_setzero_si256());\n    __m256 condition2_mask = _mm256_castsi256_ps(condition2_mask_int);\n\n    // Vectorized loop\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n        \n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_and_ps(diff, abs_mask);\n        \n        // Condition1: |v1 - v2| <= 50\n        __m256 condition1_mask = _mm256_cmp_ps(abs_diff, fifty, _CMP_LE_OS);\n        \n        // Combine conditions\n        __m256 condition_mask = _mm256_and_ps(condition1_mask, condition2_mask);\n        \n        // Compute product and ceil\n        __m256 product = _mm256_mul_ps(v1, v2);\n        __m256 ceil_product = _mm256_ceil_ps(product);\n        \n        // Blend based on condition\n        __m256 result = _mm256_blendv_ps(negative_one, ceil_product, condition_mask);\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && fabsf(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d data_vec = _mm256_loadu_pd(mat + i);\n        __m256d result_vec = _mm256_mul_pd(data_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        __m256d data = _mm256_loadu_pd(mat + i);\n        __m256d result = _mm256_mul_pd(data, scalar_vec);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (; i < total; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d data = _mm256_loadu_pd(mat + i);\n        __m256d result = _mm256_mul_pd(data, scalar_vec);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    \n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(mat + i);\n        __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, result);\n    }\n    \n    for (; i < total; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        __m256d res = _mm256_mul_pd(vec, scalar_vec);\n        _mm256_storeu_pd(dst + i, res);\n    }\n    \n    for (; i < total; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m256 sum_sq = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 7 < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 square = _mm256_mul_ps(vec, vec);\n            sum_sq = _mm256_add_ps(sum_sq, square);\n        }\n\n        __m128 low_lane = _mm256_castps256_ps128(sum_sq);\n        __m128 high_lane = _mm256_extractf128_ps(sum_sq, 1);\n        __m128 sum128 = _mm_add_ps(low_lane, high_lane);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sq_sum = _mm_cvtss_f32(sum128);\n\n        for (; j < n; j++) {\n            float val = row[j];\n            sq_sum += val * val;\n        }\n\n        float norm = sqrtf(sq_sum);\n        if (norm == 0.0f) continue;\n\n        float inv_norm = 1.0f / norm;\n        __m256 inv_norm_vec = _mm256_set1_ps(inv_norm);\n        j = 0;\n\n        for (; j + 7 < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm_vec);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        for (; j < n; j++) {\n            row[j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    if (n == 0) {\n        return;\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 7 < n; j += 8) {\n            __m256 data = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(data, data);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n\n        __m256 vsum1 = _mm256_hadd_ps(sum_vec, sum_vec);\n        __m256 vsum2 = _mm256_hadd_ps(vsum1, vsum1);\n        __m128 lo = _mm256_castps256_ps128(vsum2);\n        __m128 hi = _mm256_extractf128_ps(vsum2, 1);\n        __m128 total = _mm_add_ss(lo, hi);\n        float sum = _mm_cvtss_f32(total);\n\n        for (; j < n; j++) {\n            float val = row[j];\n            sum += val * val;\n        }\n\n        float norm;\n        if (sum == 0.0f) {\n            norm = 1.0f;\n        } else {\n            norm = 1.0f / sqrtf(sum);\n        }\n\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 data = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_mul_ps(data, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n\n        for (; j < n; j++) {\n            row[j] = row[j] * norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Vectorized sum of squares\n        for (; j + 7 < n; j += 8) {\n            __m256 data = _mm256_loadu_ps(row + j);\n            __m256 square = _mm256_mul_ps(data, data);\n            sum_vec = _mm256_add_ps(sum_vec, square);\n        }\n\n        // Horizontal sum of vector\n        __m128 low = _mm256_extractf128_ps(sum_vec, 0);\n        __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n        __m128 sum128 = _mm_add_ps(low, high);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float total = _mm_cvtss_f32(sum128);\n\n        // Scalar sum of remaining elements\n        for (; j < n; ++j) {\n            total += row[j] * row[j];\n        }\n\n        // Skip division if norm is zero\n        if (total == 0.0f) continue;\n        float norm = sqrtf(total);\n        float inv_norm = 1.0f / norm;\n        __m256 inv_norm_vec = _mm256_set1_ps(inv_norm);\n\n        // Vectorized normalization\n        j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 data = _mm256_loadu_ps(row + j);\n            data = _mm256_mul_ps(data, inv_norm_vec);\n            _mm256_storeu_ps(row + j, data);\n        }\n\n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 7 < n; j += 8) {\n            __m256 v = _mm256_loadu_ps(row + j);\n            __m256 vsq = _mm256_mul_ps(v, v);\n            sum_vec = _mm256_add_ps(sum_vec, vsq);\n        }\n\n        __m128 low = _mm256_castps256_ps128(sum_vec);\n        __m128 high = _mm256_extractf128_ps(sum_vec, 1);\n        __m128 sum4 = _mm_add_ps(low, high);\n        sum4 = _mm_hadd_ps(sum4, sum4);\n        sum4 = _mm_hadd_ps(sum4, sum4);\n        float sum_sq = _mm_cvtss_f32(sum4);\n\n        for (; j < n; j++) {\n            float val = row[j];\n            sum_sq += val * val;\n        }\n\n        float norm = std::sqrt(sum_sq);\n        if (norm == 0.0f) {\n            norm = 1.0f;\n        }\n        float inv_norm = 1.0f / norm;\n        __m256 norm_vec = _mm256_set1_ps(inv_norm);\n        j = 0;\n\n        for (; j + 7 < n; j += 8) {\n            __m256 v = _mm256_loadu_ps(row + j);\n            v = _mm256_mul_ps(v, norm_vec);\n            _mm256_storeu_ps(row + j, v);\n        }\n\n        for (; j < n; j++) {\n            row[j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t vec_floats = 8; // AVX processes 8 floats per vector\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_sq = _mm256_setzero_ps();\n        size_t j = 0;\n        \n        // Vectorized sum of squares\n        for (; j + vec_floats <= n; j += vec_floats) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_sq = _mm256_fmadd_ps(vec, vec, sum_sq);\n        }\n        \n        // Horizontal sum of squared vector\n        __m128 low = _mm256_extractf128_ps(sum_sq, 0);\n        __m128 high = _mm256_extractf128_ps(sum_sq, 1);\n        low = _mm_add_ps(low, high);\n        __m128 shuf = _mm_movehdup_ps(low);\n        __m128 sums = _mm_add_ps(low, shuf);\n        shuf = _mm_movehl_ps(shuf, sums);\n        float total_sq = _mm_cvtss_f32(_mm_add_ss(sums, shuf));\n        \n        // Scalar tail for sum of squares\n        for (; j < n; ++j) {\n            total_sq += row[j] * row[j];\n        }\n        \n        // Compute norm with zero handling\n        float norm = (total_sq == 0.0f) ? 1.0f : std::sqrt(total_sq);\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        \n        // Vectorized normalization\n        for (; j + vec_floats <= n; j += vec_floats) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(row + j, vec);\n        }\n        \n        // Scalar tail for normalization\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        for (; j + 3 < p; j += 4) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; ++k) {\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d a = _mm256_broadcast_sd(&mat1[i * n + k]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            __m256d rounded = _mm256_round_pd(sum, _MM_FROUND_TO_NEAREST_INT);\n            _mm256_storeu_pd(&dst[i * p + j], rounded);\n        }\n        for (; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n            __m128d t = _mm_set_sd(sum);\n            t = _mm_round_sd(t, t, _MM_FROUND_TO_NEAREST_INT);\n            _mm_store_sd(&dst[i * p + j], t);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j + 3 < p; j += 4) {\n            __m256d accum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m256d a = _mm256_broadcast_sd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                accum = _mm256_fmadd_pd(a, b, accum);\n            }\n            __m256d rounded = _mm256_round_pd(accum, _MM_FROUND_TO_NEAREST_INT);\n            _mm256_storeu_pd(&dst[i * p + j], rounded);\n        }\n        for (; j < p; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n            __m128d v = _mm_set_sd(sum);\n            v = _mm_round_sd(_mm_setzero_pd(), v, _MM_FROUND_TO_NEAREST_INT);\n            _mm_store_sd(&dst[i * p + j], v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j + 3 < p; j += 4) {\n            __m256d acc = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                double a_val = mat1[i * n + k];\n                __m256d a_vec = _mm256_set1_pd(a_val);\n                __m256d b_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d prod = _mm256_mul_pd(a_vec, b_vec);\n                acc = _mm256_add_pd(acc, prod);\n            }\n            __m256d rounded = _mm256_round_pd(acc, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], rounded);\n        }\n        for (; j < p; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n            __m128d t_in = _mm_set_sd(sum);\n            __m128d t_rounded = _mm_round_sd(t_in, t_in, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm_store_sd(&dst[i * p + j], t_rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        double* dst_row = dst + i * p;\n        const double* mat1_row = mat1 + i * n;\n\n        size_t j = 0;\n        for (; j + 3 < p; j += 4) {\n            _mm256_storeu_pd(dst_row + j, _mm256_setzero_pd());\n        }\n        for (; j < p; j++) {\n            dst_row[j] = 0.0;\n        }\n\n        for (size_t k = 0; k < n; k++) {\n            double a = mat1_row[k];\n            __m256d a_vec = _mm256_set1_pd(a);\n            j = 0;\n            for (; j + 3 < p; j += 4) {\n                __m256d b = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d c = _mm256_loadu_pd(dst_row + j);\n                c = _mm256_fmadd_pd(a_vec, b, c);\n                _mm256_storeu_pd(dst_row + j, c);\n            }\n            for (; j < p; j++) {\n                dst_row[j] += a * mat2[k * p + j];\n            }\n        }\n\n        j = 0;\n        for (; j + 3 < p; j += 4) {\n            __m256d v = _mm256_loadu_pd(dst_row + j);\n            v = _mm256_round_pd(v, _MM_FROUND_TO_NEAREST_INT);\n            _mm256_storeu_pd(dst_row + j, v);\n        }\n        for (; j < p; j++) {\n            __m128d w = _mm_load_sd(dst_row + j);\n            w = _mm_round_sd(_mm_setzero_pd(), w, _MM_FROUND_TO_NEAREST_INT);\n            _mm_store_sd(dst_row + j, w);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t jj = 0;\n        for (; jj + 3 < p; jj += 4) {\n            __m256d acc = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m256d a = _mm256_broadcast_sd(mat1 + i * n + k);\n                __m256d b = _mm256_loadu_pd(mat2 + k * p + jj);\n                acc = _mm256_add_pd(acc, _mm256_mul_pd(a, b));\n            }\n            __m256d rounded = _mm256_round_pd(acc, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(dst + i * p + jj, rounded);\n        }\n\n        for (; jj < p; jj++) {\n            double acc = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                acc += mat1[i * n + k] * mat2[k * p + jj];\n            }\n            __m128d a_val = _mm_set_sd(acc);\n            __m128d rounded = _mm_round_sd(a_val, a_val, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm_store_sd(dst + i * p + jj, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t rows_aligned = rows & ~(size_t)7;\n    size_t cols_aligned = cols & ~(size_t)7;\n    \n    for (size_t i = 0; i < rows_aligned; i += 8) {\n        for (size_t j = 0; j < cols_aligned; j += 8) {\n            __m256 r0 = _mm256_loadu_ps(src + (i+0)*cols + j);\n            __m256 r1 = _mm256_loadu_ps(src + (i+1)*cols + j);\n            __m256 r2 = _mm256_loadu_ps(src + (i+2)*cols + j);\n            __m256 r3 = _mm256_loadu_ps(src + (i+3)*cols + j);\n            __m256 r4 = _mm256_loadu_ps(src + (i+4)*cols + j);\n            __m256 r5 = _mm256_loadu_ps(src + (i+5)*cols + j);\n            __m256 r6 = _mm256_loadu_ps(src + (i+6)*cols + j);\n            __m256 r7 = _mm256_loadu_ps(src + (i+7)*cols + j);\n\n            __m256 scale = _mm256_set1_ps(4.0f);\n            r0 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r0, scale), scale);\n            r1 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r1, scale), scale);\n            r2 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r2, scale), scale);\n            r3 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r3, scale), scale);\n            r4 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r4, scale), scale);\n            r5 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r5, scale), scale);\n            r6 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r6, scale), scale);\n            r7 = _mm256_div_ps(_mm256_round_ps(_mm256_mul_ps(r7, scale), scale);\n\n            __m256 t0 = _mm256_unpacklo_ps(r0, r1);\n            __m256 t1 = _mm256_unpackhi_ps(r0, r1);\n            __m256 t2 = _mm256_unpacklo_ps(r2, r3);\n            __m256 t3 = _mm256_unpackhi_ps(r2, r3);\n            __m256 t4 = _mm256_unpacklo_ps(r4, r5);\n            __m256 t5 = _mm256_unpackhi_ps(r4, r5);\n            __m256 t6 = _mm256_unpacklo_ps(r6, r7);\n            __m256 t7 = _mm256_unpackhi_ps(r6, r7);\n\n            __m256 tt0 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt1 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt2 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt3 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt4 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt5 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt6 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt7 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(3,2,3,2));\n\n            r0 = _mm256_permute2f128_ps(tt0, tt4, 0x20);\n            r1 = _mm256_permute2f128_ps(tt1, tt5, 0x20);\n            r2 = _mm256_permute2f128_ps(tt2, tt6, 0x20);\n            r3 = _mm256_permute2f128_ps(tt3, tt7, 0x20);\n            r4 = _mm256_permute2f128_ps(tt0, tt4, 0x31);\n            r5 = _mm256_permute2f128_ps(tt1, tt5, 0x31);\n            r6 = _mm256_permute2f128_ps(tt2, tt6, 0x31);\n            r7 = _mm256_permute2f128_ps(tt3, tt7, 0x31);\n\n            _mm256_storeu_ps(dst + (j+0)*rows + i, r0);\n            _mm256_storeu_ps(dst + (j+1)*rows + i, r1);\n            _mm256_storeu_ps(dst + (j+2)*rows + i, r2);\n            _mm256_storeu_ps(dst + (j+3)*rows + i, r3);\n            _mm256_storeu_ps(dst + (j+4)*rows + i, r4);\n            _mm256_storeu_ps(dst + (j+5)*rows + i, r5);\n            _mm256_storeu_ps(dst + (j+6)*rows + i, r6);\n            _mm256_storeu_ps(dst + (j+7)*rows + i, r7);\n        }\n    }\n\n    for (size_t i = rows_aligned; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 v = _mm_set_ss(src[i * cols + j] * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            float rounded_val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = rounded_val;\n        }\n    }\n\n    for (size_t j = cols_aligned; j < cols; j++) {\n        for (size_t i = 0; i < rows_aligned; i++) {\n            __m128 v = _mm_set_ss(src[i * cols + j] * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            float rounded_val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = rounded_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t rows_aligned = rows & ~(size_t)7;\n    size_t cols_aligned = cols & ~(size_t)7;\n    \n    __m256 scale = _mm256_set1_ps(4.0f);\n    __m256 inv_scale = _mm256_set1_ps(0.25f);\n\n    for (size_t i0 = 0; i0 < rows_aligned; i0 += 8) {\n        for (size_t j0 = 0; j0 < cols_aligned; j0 += 8) {\n            __m256 r0 = _mm256_loadu_ps(src + (i0+0)*cols + j0);\n            __m256 r1 = _mm256_loadu_ps(src + (i0+1)*cols + j0);\n            __m256 r2 = _mm256_loadu_ps(src + (i0+2)*cols + j0);\n            __m256 r3 = _mm256_loadu_ps(src + (i0+3)*cols + j0);\n            __m256 r4 = _mm256_loadu_ps(src + (i0+4)*cols + j0);\n            __m256 r5 = _mm256_loadu_ps(src + (i0+5)*cols + j0);\n            __m256 r6 = _mm256_loadu_ps(src + (i0+6)*cols + j0);\n            __m256 r7 = _mm256_loadu_ps(src + (i0+7)*cols + j0);\n\n            __m256 t0 = _mm256_unpacklo_ps(r0, r1);\n            __m256 t1 = _mm256_unpackhi_ps(r0, r1);\n            __m256 t2 = _mm256_unpacklo_ps(r2, r3);\n            __m256 t3 = _mm256_unpackhi_ps(r2, r3);\n            __m256 t4 = _mm256_unpacklo_ps(r4, r5);\n            __m256 t5 = _mm256_unpackhi_ps(r4, r5);\n            __m256 t6 = _mm256_unpacklo_ps(r6, r7);\n            __m256 t7 = _mm256_unpackhi_ps(r6, r7);\n\n            __m256 tt0 = _mm256_shuffle_ps(t0, t2, 0x44);\n            __m256 tt1 = _mm256_shuffle_ps(t0, t2, 0xEE);\n            __m256 tt2 = _mm256_shuffle_ps(t1, t3, 0x44);\n            __m256 tt3 = _mm256_shuffle_ps(t1, t3, 0xEE);\n            __m256 tt4 = _mm256_shuffle_ps(t4, t6, 0x44);\n            __m256 tt5 = _mm256_shuffle_ps(t4, t6, 0xEE);\n            __m256 tt6 = _mm256_shuffle_ps(t5, t7, 0x44);\n            __m256 tt7 = _mm256_shuffle_ps(t5, t7, 0xEE);\n\n            r0 = _mm256_permute2f128_ps(tt0, tt4, 0x20);\n            r1 = _mm256_permute2f128_ps(tt1, tt5, 0x20);\n            r2 = _mm256_permute2f128_ps(tt2, tt6, 0x20);\n            r3 = _mm256_permute2f128_ps(tt3, tt7, 0x20);\n            r4 = _mm256_permute2f128_ps(tt0, tt4, 0x31);\n            r5 = _mm256_permute2f128_ps(tt1, tt5, 0x31);\n            r6 = _mm256_permute2f128_ps(tt2, tt6, 0x31);\n            r7 = _mm256_permute2f128_ps(tt3, tt7, 0x31);\n\n            r0 = _mm256_mul_ps(r0, scale);\n            r0 = _mm256_round_ps(r0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r0 = _mm256_mul_ps(r0, inv_scale);\n            r1 = _mm256_mul_ps(r1, scale);\n            r1 = _mm256_round_ps(r1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r1 = _mm256_mul_ps(r1, inv_scale);\n            r2 = _mm256_mul_ps(r2, scale);\n            r2 = _mm256_round_ps(r2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r2 = _mm256_mul_ps(r2, inv_scale);\n            r3 = _mm256_mul_ps(r3, scale);\n            r3 = _mm256_round_ps(r3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r3 = _mm256_mul_ps(r3, inv_scale);\n            r4 = _mm256_mul_ps(r4, scale);\n            r4 = _mm256_round_ps(r4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r4 = _mm256_mul_ps(r4, inv_scale);\n            r5 = _mm256_mul_ps(r5, scale);\n            r5 = _mm256_round_ps(r5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r5 = _mm256_mul_ps(r5, inv_scale);\n            r6 = _mm256_mul_ps(r6, scale);\n            r6 = _mm256_round_ps(r6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r6 = _mm256_mul_ps(r6, inv_scale);\n            r7 = _mm256_mul_ps(r7, scale);\n            r7 = _mm256_round_ps(r7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r7 = _mm256_mul_ps(r7, inv_scale);\n\n            _mm256_storeu_ps(dst + (j0+0)*rows + i0, r0);\n            _mm256_storeu_ps(dst + (j0+1)*rows + i0, r1);\n            _mm256_storeu_ps(dst + (j0+2)*rows + i0, r2);\n            _mm256_storeu_ps(dst + (j0+3)*rows + i0, r3);\n            _mm256_storeu_ps(dst + (j0+4)*rows + i0, r4);\n            _mm256_storeu_ps(dst + (j0+5)*rows + i0, r5);\n            _mm256_storeu_ps(dst + (j0+6)*rows + i0, r6);\n            _mm256_storeu_ps(dst + (j0+7)*rows + i0, r7);\n        }\n    }\n\n    for (size_t i = 0; i < rows_aligned; i++) {\n        for (size_t j = cols_aligned; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v = _mm_set_ss(val * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = val;\n        }\n    }\n\n    for (size_t i = rows_aligned; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v = _mm_set_ss(val * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t rows_full = rows & ~7;\n    size_t cols_full = cols & ~7;\n    \n    const __m256 four = _mm256_set1_ps(4.0f);\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    \n    for (size_t i0 = 0; i0 < rows_full; i0 += 8) {\n        for (size_t j0 = 0; j0 < cols_full; j0 += 8) {\n            __m256 r0 = _mm256_loadu_ps(src + (i0 + 0) * cols + j0);\n            __m256 r1 = _mm256_loadu_ps(src + (i0 + 1) * cols + j0);\n            __m256 r2 = _mm256_loadu_ps(src + (i0 + 2) * cols + j0);\n            __m256 r3 = _mm256_loadu_ps(src + (i0 + 3) * cols + j0);\n            __m256 r4 = _mm256_loadu_ps(src + (i0 + 4) * cols + j0);\n            __m256 r5 = _mm256_loadu_ps(src + (i0 + 5) * cols + j0);\n            __m256 r6 = _mm256_loadu_ps(src + (i0 + 6) * cols + j0);\n            __m256 r7 = _mm256_loadu_ps(src + (i0 + 7) * cols + j0);\n            \n            r0 = _mm256_mul_ps(r0, four);\n            r0 = _mm256_round_ps(r0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r0 = _mm256_mul_ps(r0, quarter);\n            \n            r1 = _mm256_mul_ps(r1, four);\n            r1 = _mm256_round_ps(r1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r1 = _mm256_mul_ps(r1, quarter);\n            \n            r2 = _mm256_mul_ps(r2, four);\n            r2 = _mm256_round_ps(r2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r2 = _mm256_mul_ps(r2, quarter);\n            \n            r3 = _mm256_mul_ps(r3, four);\n            r3 = _mm256_round_ps(r3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r3 = _mm256_mul_ps(r3, quarter);\n            \n            r4 = _mm256_mul_ps(r4, four);\n            r4 = _mm256_round_ps(r4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r4 = _mm256_mul_ps(r4, quarter);\n            \n            r5 = _mm256_mul_ps(r5, four);\n            r5 = _mm256_round_ps(r5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r5 = _mm256_mul_ps(r5, quarter);\n            \n            r6 = _mm256_mul_ps(r6, four);\n            r6 = _mm256_round_ps(r6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r6 = _mm256_mul_ps(r6, quarter);\n            \n            r7 = _mm256_mul_ps(r7, four);\n            r7 = _mm256_round_ps(r7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            r7 = _mm256_mul_ps(r7, quarter);\n            \n            __m256 t0 = _mm256_unpacklo_ps(r0, r1);\n            __m256 t1 = _mm256_unpackhi_ps(r0, r1);\n            __m256 t2 = _mm256_unpacklo_ps(r2, r3);\n            __m256 t3 = _mm256_unpackhi_ps(r2, r3);\n            __m256 t4 = _mm256_unpacklo_ps(r4, r5);\n            __m256 t5 = _mm256_unpackhi_ps(r4, r5);\n            __m256 t6 = _mm256_unpacklo_ps(r6, r7);\n            __m256 t7 = _mm256_unpackhi_ps(r6, r7);\n            \n            __m256 tt0 = _mm256_shuffle_ps(t0, t2, 0x44);\n            __m256 tt1 = _mm256_shuffle_ps(t0, t2, 0xEE);\n            __m256 tt2 = _mm256_shuffle_ps(t1, t3, 0x44);\n            __m256 tt3 = _mm256_shuffle_ps(t1, t3, 0xEE);\n            __m256 tt4 = _mm256_shuffle_ps(t4, t6, 0x44);\n            __m256 tt5 = _mm256_shuffle_ps(t4, t6, 0xEE);\n            __m256 tt6 = _mm256_shuffle_ps(t5, t7, 0x44);\n            __m256 tt7 = _mm256_shuffle_ps(t5, t7, 0xEE);\n            \n            r0 = _mm256_permute2f128_ps(tt0, tt4, 0x20);\n            r1 = _mm256_permute2f128_ps(tt1, tt5, 0x20);\n            r2 = _mm256_permute2f128_ps(tt2, tt6, 0x20);\n            r3 = _mm256_permute2f128_ps(tt3, tt7, 0x20);\n            r4 = _mm256_permute2f128_ps(tt0, tt4, 0x31);\n            r5 = _mm256_permute2f128_ps(tt1, tt5, 0x31);\n            r6 = _mm256_permute2f128_ps(tt2, tt6, 0x31);\n            r7 = _mm256_permute2f128_ps(tt3, tt7, 0x31);\n            \n            _mm256_storeu_ps(dst + (j0 + 0) * rows + i0, r0);\n            _mm256_storeu_ps(dst + (j0 + 1) * rows + i0, r1);\n            _mm256_storeu_ps(dst + (j0 + 2) * rows + i0, r2);\n            _mm256_storeu_ps(dst + (j0 + 3) * rows + i0, r3);\n            _mm256_storeu_ps(dst + (j0 + 4) * rows + i0, r4);\n            _mm256_storeu_ps(dst + (j0 + 5) * rows + i0, r5);\n            _mm256_storeu_ps(dst + (j0 + 6) * rows + i0, r6);\n            _mm256_storeu_ps(dst + (j0 + 7) * rows + i0, r7);\n        }\n    }\n    \n    for (size_t i = 0; i < rows_full; i++) {\n        for (size_t j = cols_full; j < cols; j++) {\n            float val = src[i * cols + j];\n            float t = val * 4.0f;\n            if (t >= 0) {\n                val = (int)(t + 0.5f) * 0.25f;\n            } else {\n                val = (int)(t - 0.5f) * 0.25f;\n            }\n            dst[j * rows + i] = val;\n        }\n    }\n    \n    for (size_t i = rows_full; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float val = src[i * cols + j];\n            float t = val * 4.0f;\n            if (t >= 0) {\n                val = (int)(t + 0.5f) * 0.25f;\n            } else {\n                val = (int)(t - 0.5f) * 0.25f;\n            }\n            dst[j * rows + i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 inv_factor = _mm256_set1_ps(0.25f);\n    size_t i_block_end = rows & ~((size_t)7);\n    size_t j_block_end = cols & ~((size_t)7);\n\n    for (size_t i_block = 0; i_block < i_block_end; i_block += 8) {\n        for (size_t j_block = 0; j_block < j_block_end; j_block += 8) {\n            __m256 rows_reg[8];\n            for (int i = 0; i < 8; i++) {\n                const float* src_ptr = src + (i_block + i) * cols + j_block;\n                rows_reg[i] = _mm256_loadu_ps(src_ptr);\n            }\n\n            for (int i = 0; i < 8; i++) {\n                rows_reg[i] = _mm256_mul_ps(rows_reg[i], factor);\n                rows_reg[i] = _mm256_round_ps(rows_reg[i], _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                rows_reg[i] = _mm256_mul_ps(rows_reg[i], inv_factor);\n            }\n\n            __m256 t0 = _mm256_unpacklo_ps(rows_reg[0], rows_reg[1]);\n            __m256 t1 = _mm256_unpackhi_ps(rows_reg[0], rows_reg[1]);\n            __m256 t2 = _mm256_unpacklo_ps(rows_reg[2], rows_reg[3]);\n            __m256 t3 = _mm256_unpackhi_ps(rows_reg[2], rows_reg[3]);\n            __m256 t4 = _mm256_unpacklo_ps(rows_reg[4], rows_reg[5]);\n            __m256 t5 = _mm256_unpackhi_ps(rows_reg[4], rows_reg[5]);\n            __m256 t6 = _mm256_unpacklo_ps(rows_reg[6], rows_reg[7]);\n            __m256 t7 = _mm256_unpackhi_ps(rows_reg[6], rows_reg[7]);\n\n            __m256 tt0 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt1 = _mm256_shuffle_ps(t0, t2, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt2 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt3 = _mm256_shuffle_ps(t1, t3, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt4 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt5 = _mm256_shuffle_ps(t4, t6, _MM_SHUFFLE(3,2,3,2));\n            __m256 tt6 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(1,0,1,0));\n            __m256 tt7 = _mm256_shuffle_ps(t5, t7, _MM_SHUFFLE(3,2,3,2));\n\n            rows_reg[0] = _mm256_permute2f128_ps(tt0, tt4, 0x20);\n            rows_reg[1] = _mm256_permute2f128_ps(tt1, tt5, 0x20);\n            rows_reg[2] = _mm256_permute2f128_ps(tt2, tt6, 0x20);\n            rows_reg[3] = _mm256_permute2f128_ps(tt3, tt7, 0x20);\n            rows_reg[4] = _mm256_permute2f128_ps(tt0, tt4, 0x31);\n            rows_reg[5] = _mm256_permute2f128_ps(tt1, tt5, 0x31);\n            rows_reg[6] = _mm256_permute2f128_ps(tt2, tt6, 0x31);\n            rows_reg[7] = _mm256_permute2f128_ps(tt3, tt7, 0x31);\n\n            for (int j = 0; j < 8; j++) {\n                float* dst_ptr = dst + (j_block + j) * rows + i_block;\n                _mm256_storeu_ps(dst_ptr, rows_reg[j]);\n            }\n        }\n    }\n\n    for (size_t i = 0; i < i_block_end; i++) {\n        for (size_t j = j_block_end; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v = _mm_set_ss(val * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = val;\n        }\n    }\n\n    for (size_t i = i_block_end; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v = _mm_set_ss(val * 4.0f);\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            val = _mm_cvtss_f32(v) * 0.25f;\n            dst[j * rows + i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i_end = rows - (rows % 8);\n    size_t j_end = cols - (cols % 8);\n    __m256 four = _mm256_set1_ps(4.0f);\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    \n    for (size_t i = 0; i < i_end; i += 8) {\n        for (size_t j = 0; j < j_end; j += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + (i+0)*cols + j);\n            __m256 row1 = _mm256_loadu_ps(src + (i+1)*cols + j);\n            __m256 row2 = _mm256_loadu_ps(src + (i+2)*cols + j);\n            __m256 row3 = _mm256_loadu_ps(src + (i+3)*cols + j);\n            __m256 row4 = _mm256_loadu_ps(src + (i+4)*cols + j);\n            __m256 row5 = _mm256_loadu_ps(src + (i+5)*cols + j);\n            __m256 row6 = _mm256_loadu_ps(src + (i+6)*cols + j);\n            __m256 row7 = _mm256_loadu_ps(src + (i+7)*cols + j);\n            \n            row0 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row0, four), quarter);\n            row1 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row1, four), quarter);\n            row2 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row2, four), quarter);\n            row3 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row3, four), quarter);\n            row4 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row4, four), quarter);\n            row5 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row5, four), quarter);\n            row6 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row6, four), quarter);\n            row7 = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(row7, four), quarter);\n            \n            __m256 tmp0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 tmp1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 tmp2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 tmp3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 tmp4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 tmp5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 tmp6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 tmp7 = _mm256_unpackhi_ps(row6, row7);\n            \n            __m256 t0 = _mm256_shuffle_ps(tmp0, tmp2, _MM_SHUFFLE(1,0,1,0));\n            __m256 t1 = _mm256_shuffle_ps(tmp0, tmp2, _MM_SHUFFLE(3,2,3,2));\n            __m256 t2 = _mm256_shuffle_ps(tmp1, tmp3, _MM_SHUFFLE(1,0,1,0));\n            __m256 t3 = _mm256_shuffle_ps(tmp1, tmp3, _MM_SHUFFLE(3,2,3,2));\n            __m256 t4 = _mm256_shuffle_ps(tmp4, tmp6, _MM_SHUFFLE(1,0,1,0));\n            __m256 t5 = _mm256_shuffle_ps(tmp4, tmp6, _MM_SHUFFLE(3,2,3,2));\n            __m256 t6 = _mm256_shuffle_ps(tmp5, tmp7, _MM_SHUFFLE(1,0,1,0));\n            __m256 t7 = _mm256_shuffle_ps(tmp5, tmp7, _MM_SHUFFLE(3,2,3,2));\n            \n            row0 = _mm256_permute2f128_ps(t0, t4, 0x20);\n            row1 = _mm256_permute2f128_ps(t1, t5, 0x20);\n            row2 = _mm256_permute2f128_ps(t2, t6, 0x20);\n            row3 = _mm256_permute2f128_ps(t3, t7, 0x20);\n            row4 = _mm256_permute2f128_ps(t0, t4, 0x31);\n            row5 = _mm256_permute2f128_ps(t1, t5, 0x31);\n            row6 = _mm256_permute2f128_ps(t2, t6, 0x31);\n            row7 = _mm256_permute2f128_ps(t3, t7, 0x31);\n            \n            _mm256_storeu_ps(dst + (j+0)*rows + i, row0);\n            _mm256_storeu_ps(dst + (j+1)*rows + i, row1);\n            _mm256_storeu_ps(dst + (j+2)*rows + i, row2);\n            _mm256_storeu_ps(dst + (j+3)*rows + i, row3);\n            _mm256_storeu_ps(dst + (j+4)*rows + i, row4);\n            _mm256_storeu_ps(dst + (j+5)*rows + i, row5);\n            _mm256_storeu_ps(dst + (j+6)*rows + i, row6);\n            _mm256_storeu_ps(dst + (j+7)*rows + i, row7);\n        }\n    }\n    \n    for (size_t i = 0; i < i_end; i++) {\n        for (size_t j = j_end; j < cols; j++) {\n            float value = src[i*cols + j];\n            __m128 v = _mm_set_ss(value);\n            v = _mm_mul_ss(v, _mm_set_ss(4.0f));\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v = _mm_mul_ss(v, _mm_set_ss(0.25f));\n            dst[j*rows + i] = _mm_cvtss_f32(v);\n        }\n    }\n    \n    for (size_t i = i_end; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float value = src[i*cols + j];\n            __m128 v = _mm_set_ss(value);\n            v = _mm_mul_ss(v, _mm_set_ss(4.0f));\n            v = _mm_round_ss(v, v, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v = _mm_mul_ss(v, _mm_set_ss(0.25f));\n            dst[j*rows + i] = _mm_cvtss_f32(v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 diff = _mm256_sub_ps(vec1, vec2);\n        \n        __m256 abs_diff = _mm256_andnot_ps(_mm256_castsi256_ps(sign_mask), diff);\n        __m256 sign = _mm256_and_ps(_mm256_castsi256_ps(sign_mask), diff);\n        __m256 truncated = _mm256_round_ps(abs_diff, _MM_FROUND_TRUNC);\n        __m256 result = _mm256_or_ps(truncated, sign);\n        \n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total_elements; ++i) {\n        float diff = tensor1[i] - tensor2[i];\n        dst[i] = (diff >= 0.0f) ? (float)(int)(diff) : (float)(int)(-(-diff));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256 a = _mm256_loadu_ps(tensor1 + i);\n        __m256 b = _mm256_loadu_ps(tensor2 + i);\n        __m256 sub = _mm256_sub_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    size_t remaining = total - i;\n    if (remaining > 0) {\n        float temp_t1[8] = {0};\n        float temp_t2[8] = {0};\n        float temp_dst[8] = {0};\n\n        for (size_t j = 0; j < remaining; j++) {\n            temp_t1[j] = tensor1[i + j];\n            temp_t2[j] = tensor2[i + j];\n        }\n\n        __m256 a = _mm256_loadu_ps(temp_t1);\n        __m256 b = _mm256_loadu_ps(temp_t2);\n        __m256 sub = _mm256_sub_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(temp_dst, rounded);\n\n        for (size_t j = 0; j < remaining; j++) {\n            dst[i + j] = temp_dst[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256 t1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 t2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 diff = _mm256_sub_ps(t1, t2);\n        __m256 res = _mm256_trunc_ps(diff);\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for (; i < total; i++) {\n        float diff = tensor1[i] - tensor2[i];\n        __m128 v = _mm_set_ss(diff);\n        v = _mm_round_ss(v, v, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        dst[i] = _mm_cvtss_f32(v);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total; i += 8) {\n        __m256 a = _mm256_loadu_ps(tensor1 + i);\n        __m256 b = _mm256_loadu_ps(tensor2 + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 trunc = _mm256_trunc_ps(diff);\n        _mm256_storeu_ps(dst + i, trunc);\n    }\n\n    for (; i < total; i++) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 diff = _mm_sub_ss(a, b);\n        __m128 trunc = _mm_round_ss(diff, diff, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm_store_ss(dst + i, trunc);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const size_t avx_float_count = 8;\n\n    for (; i + avx_float_count <= total; i += avx_float_count) {\n        __m256 a = _mm256_loadu_ps(tensor1 + i);\n        __m256 b = _mm256_loadu_ps(tensor2 + i);\n        __m256 sub = _mm256_sub_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < total; i++) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 sub = _mm_sub_ss(a, b);\n        __m128 rounded = _mm_round_ss(sub, sub, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm_store_ss(dst + i, rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Process 4 doubles per iteration (AVX vector width for doubles)\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements (less than 4)\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    \n    // Process 4 elements per iteration using AVX (256-bit registers)\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d res = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, res);\n    }\n    \n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stddef.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    const size_t vec_doubles = 4;\n\n    for (; i + vec_doubles - 1 < total_elements; i += vec_doubles) {\n        __m256d a = _mm256_loadu_pd(mat1 + i);\n        __m256d b = _mm256_loadu_pd(mat2 + i);\n        __m256d prod = _mm256_mul_pd(a, b);\n        _mm256_storeu_pd(dst + i, prod);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    const size_t vec_step = 4;\n    size_t vec_limit = total - (total % vec_step);\n    \n    for (; i < vec_limit; i += vec_step) {\n        __m256d a = _mm256_loadu_pd(mat1 + i);\n        __m256d b = _mm256_loadu_pd(mat2 + i);\n        __m256d c = _mm256_mul_pd(a, b);\n        _mm256_storeu_pd(dst + i, c);\n    }\n    \n    for (; i < total; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 acc = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            acc = _mm256_add_ps(acc, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n        __m128 low = _mm256_castps256_ps128(acc);\n        __m128 high = _mm256_extractf128_ps(acc, 1);\n        low = _mm_add_ps(low, high);\n        low = _mm_hadd_ps(low, low);\n        low = _mm_hadd_ps(low, low);\n        float sum;\n        _mm_store_ss(&sum, low);\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 accum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_val = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_val = _mm256_loadu_ps(vec + j);\n            __m256 prod = _mm256_mul_ps(mat_val, vec_val);\n            accum = _mm256_add_ps(accum, prod);\n        }\n        __m128 low = _mm256_castps256_ps128(accum);\n        __m128 high = _mm256_extractf128_ps(accum, 1);\n        low = _mm_add_ps(low, high);\n        low = _mm_hadd_ps(low, low);\n        low = _mm_hadd_ps(low, low);\n        float sum = _mm_cvtss_f32(low);\n\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 accumulator = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            __m256 product = _mm256_mul_ps(mat_vec, vec_vec);\n            accumulator = _mm256_add_ps(accumulator, product);\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, accumulator);\n        float sum = temp[0] + temp[1] + temp[2] + temp[3] + \n                   temp[4] + temp[5] + temp[6] + temp[7];\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 acc = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            __m256 a = _mm256_loadu_ps(mat + i * n + j);\n            __m256 b = _mm256_loadu_ps(vec + j);\n            __m256 prod = _mm256_mul_ps(a, b);\n            acc = _mm256_add_ps(acc, prod);\n        }\n\n        __m128 low = _mm256_castps256_ps128(acc);\n        __m128 high = _mm256_extractf128_ps(acc, 1);\n        __m128 sum128 = _mm_add_ps(low, high);\n        __m128 hadd = _mm_hadd_ps(sum128, sum128);\n        hadd = _mm_hadd_ps(hadd, hadd);\n        float sum = _mm_cvtss_f32(hadd);\n\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 accumulator = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            accumulator = _mm256_add_ps(accumulator, _mm256_mul_ps(mat_vec, vec_vec));\n        }\n\n        __m128 low = _mm256_castps256_ps128(accumulator);\n        __m128 high = _mm256_extractf128_ps(accumulator, 1);\n        __m128 sum128 = _mm_add_ps(low, high);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        sum128 = _mm_hadd_ps(sum128, sum128);\n        float sum = _mm_cvtss_f32(sum128);\n\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    if (total == 0) return 0.0;\n    \n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    size_t vec_end = total - (total % 4);\n\n    for (; i < vec_end; i += 4) {\n        __m256d data = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n\n    __m128d low = _mm256_extractf128_pd(sum_vec, 0);\n    __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n    low = _mm_add_pd(low, high);\n    __m128d sum_sse = _mm_hadd_pd(low, low);\n    double sum = _mm_cvtsd_f64(sum_sse);\n\n    for (; i < total; i++) {\n        sum += mat[i];\n    }\n\n    return sum / (double)total;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d acc = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        acc = _mm256_add_pd(acc, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, acc);\n    double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    if (total == 0) {\n        return 0.0;\n    }\n    \n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d v = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, v);\n    }\n    \n    __m128d vlow = _mm256_castpd256_pd128(sum_vec);\n    __m128d vhigh = _mm256_extractf128_pd(sum_vec, 1);\n    vlow = _mm_add_pd(vlow, vhigh);\n    __m128d vswapped = _mm_permute_pd(vlow, 1);\n    __m128d sum128 = _mm_add_sd(vlow, vswapped);\n    double sum = _mm_cvtsd_f64(sum128);\n    \n    for (; i < total; i++) {\n        sum += mat[i];\n    }\n    \n    return sum / total;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n    __m256d sum_vec = _mm256_setzero_pd();\n    \n    for (; i + 3 < total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n    \n    double sum = 0.0;\n    if (i < total_elements) {\n        __m256d partial_sum = _mm256_setzero_pd();\n        double buffer[4] = {0};\n        size_t remaining = total_elements - i;\n        for (size_t j = 0; j < remaining; ++j) {\n            buffer[j] = mat[i + j];\n        }\n        __m256d tail = _mm256_loadu_pd(buffer);\n        partial_sum = _mm256_add_pd(partial_sum, tail);\n        sum_vec = _mm256_add_pd(sum_vec, partial_sum);\n    }\n    \n    __m128d low128 = _mm256_castpd256_pd128(sum_vec);\n    __m128d high128 = _mm256_extractf128_pd(sum_vec, 1);\n    __m128d sum128 = _mm_add_pd(low128, high128);\n    __m128d high64 = _mm_unpackhi_pd(sum128, sum128);\n    sum = _mm_cvtsd_f64(_mm_add_sd(sum128, high64));\n    \n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d v = _mm256_loadu_pd(mat + i);\n        sum_vec = _mm256_add_pd(sum_vec, v);\n    }\n    \n    __m128d vlow = _mm256_castpd256_pd128(sum_vec);\n    __m128d vhigh = _mm256_extractf128_pd(sum_vec, 1);\n    vlow = _mm_add_pd(vlow, vhigh);\n    __m128d high64 = _mm_unpackhi_pd(vlow, vlow);\n    double total_sum = _mm_cvtsd_f64(_mm_add_sd(vlow, high64));\n    \n    for (; i < total; ++i) {\n        total_sum += mat[i];\n    }\n    \n    return total_sum / total;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const bool dir = directions[i];\n        const int* row_ptr = matrix + i * cols;\n\n        if (cols <= 1) {\n            continue;\n        }\n\n        bool valid = true;\n        size_t j = 0;\n\n        if (dir) {\n            for (; j + 8 < cols; j += 8) {\n                __m256i v1 = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n                __m256i v2 = _mm256_loadu_si256((__m256i*)(row_ptr + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(v1, v2);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) {\n                    valid = false;\n                    break;\n                }\n            }\n            if (valid) {\n                for (; j < cols - 1; j++) {\n                    if (row_ptr[j] > row_ptr[j + 1]) {\n                        valid = false;\n                        break;\n                    }\n                }\n            }\n        } else {\n            for (; j + 8 < cols; j += 8) {\n                __m256i v1 = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n                __m256i v2 = _mm256_loadu_si256((__m256i*)(row_ptr + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(v2, v1);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) {\n                    valid = false;\n                    break;\n                }\n            }\n            if (valid) {\n                for (; j < cols - 1; j++) {\n                    if (row_ptr[j] < row_ptr[j + 1]) {\n                        valid = false;\n                        break;\n                    }\n                }\n            }\n        }\n\n        if (!valid) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        bool dir = directions[i];\n\n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n\n        while (j + 8 < cols) {\n            __m256i v_prev = _mm256_loadu_si256((const __m256i*)(row + j));\n            __m256i v_next = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n\n            if (dir) {\n                __m256i cmp = _mm256_cmpgt_epi32(v_prev, v_next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(v_next, v_prev);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n            j += 8;\n        }\n\n        for (; j < cols - 1; j++) {\n            if (dir) {\n                if (row[j] > row[j+1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j+1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (rows == 0 || cols <= 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        bool dir = directions[i];\n        bool sorted = true;\n\n        if (dir) { // Ascending\n            size_t j = 0;\n            if (cols >= 9) {\n                for (; j <= cols - 9; j += 8) {\n                    __m256i v1 = _mm256_loadu_si256((const __m256i*)(row + j));\n                    __m256i v2 = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n                    __m256i cmp = _mm256_cmpgt_epi32(v1, v2);\n                    if (!_mm256_testz_si256(cmp, cmp)) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n            if (sorted) {\n                for (; j < cols - 1; j++) {\n                    if (row[j] > row[j + 1]) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n        } else { // Descending\n            size_t j = 0;\n            if (cols >= 9) {\n                for (; j <= cols - 9; j += 8) {\n                    __m256i v1 = _mm256_loadu_si256((const __m256i*)(row + j));\n                    __m256i v2 = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n                    __m256i cmp = _mm256_cmpgt_epi32(v2, v1);\n                    if (!_mm256_testz_si256(cmp, cmp)) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n            if (sorted) {\n                for (; j < cols - 1; j++) {\n                    if (row[j] < row[j + 1]) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n        }\n\n        if (!sorted) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (rows == 0 || cols <= 1) {\n        return true;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row_ptr = matrix + i * cols;\n        bool dir = directions[i];\n        size_t num_pairs = cols - 1;\n        size_t j = 0;\n\n        if (num_pairs >= 8) {\n            for (; j <= num_pairs - 8; j += 8) {\n                __m256i v1 = _mm256_loadu_si256((const __m256i*)(row_ptr + j));\n                __m256i v2 = _mm256_loadu_si256((const __m256i*)(row_ptr + j + 1));\n\n                __m256i cmp;\n                if (dir) {\n                    cmp = _mm256_cmpgt_epi32(v1, v2);\n                } else {\n                    cmp = _mm256_cmpgt_epi32(v2, v1);\n                }\n\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < num_pairs; j++) {\n            if (dir) {\n                if (row_ptr[j] > row_ptr[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row_ptr[j] < row_ptr[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        bool asc = directions[i];\n        \n        if (cols <= 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 8 <= cols - 1; j += 8) {\n            __m256i v1 = _mm256_loadu_si256((const __m256i*)(row + j));\n            __m256i v2 = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (asc) {\n                cmp = _mm256_cmpgt_epi32(v1, v2);\n            } else {\n                cmp = _mm256_cmpgt_epi32(v2, v1);\n            }\n            \n            if (!_mm256_testz_si256(cmp, cmp)) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; j++) {\n            if (asc) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base);\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(src + i));\n        int16_t v_arr[16];\n        _mm256_storeu_si256((__m256i*)v_arr, v);\n        int16_t q_arr[16];\n        for (int j = 0; j < 16; j++) {\n            q_arr[j] = v_arr[j] / base;\n        }\n        __m256i q = _mm256_loadu_si256((__m256i*)q_arr);\n        __m256i multiple1 = _mm256_mullo_epi16(q, base_vec);\n        __m256i mask = _mm256_cmpgt_epi16(multiple1, v);\n        __m256i adjust = _mm256_and_si256(mask, base_vec);\n        __m256i result = _mm256_sub_epi16(multiple1, adjust);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        int16_t val = src[i];\n        int16_t q = val / base;\n        int16_t multiple1 = q * base;\n        if (multiple1 > val) {\n            dst[i] = multiple1 - base;\n        } else {\n            dst[i] = multiple1;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256 v_base_f = _mm256_set1_ps((float)base);\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        __m128i v_src_low = _mm256_extracti128_si256(v_src, 0);\n        __m128i v_src_high = _mm256_extracti128_si256(v_src, 1);\n\n        __m256i v_src_low_i32 = _mm256_cvtepi16_epi32(v_src_low);\n        __m256i v_src_high_i32 = _mm256_cvtepi16_epi32(v_src_high);\n\n        __m256 v_src_low_f = _mm256_cvtepi32_ps(v_src_low_i32);\n        __m256 v_src_high_f = _mm256_cvtepi32_ps(v_src_high_i32);\n\n        v_src_low_f = _mm256_div_ps(v_src_low_f, v_base_f);\n        v_src_high_f = _mm256_div_ps(v_src_high_f, v_base_f);\n\n        v_src_low_f = _mm256_floor_ps(v_src_low_f);\n        v_src_high_f = _mm256_floor_ps(v_src_high_f);\n\n        v_src_low_f = _mm256_mul_ps(v_src_low_f, v_base_f);\n        v_src_high_f = _mm256_mul_ps(v_src_high_f, v_base_f);\n\n        __m256i v_result_low_i32 = _mm256_cvtps_epi32(v_src_low_f);\n        __m256i v_result_high_i32 = _mm256_cvtps_epi32(v_src_high_f);\n\n        __m128i low_low = _mm256_castsi256_si128(v_result_low_i32);\n        __m128i low_high = _mm256_extracti128_si256(v_result_low_i32, 1);\n        __m128i low_packed = _mm_packs_epi32(low_low, low_high);\n\n        __m128i high_low = _mm256_castsi256_si128(v_result_high_i32);\n        __m128i high_high = _mm256_extracti128_si256(v_result_high_i32, 1);\n        __m128i high_packed = _mm_packs_epi32(high_low, high_high);\n\n        __m256i v_result = _mm256_set_m128i(high_packed, low_packed);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        float f = (float)src[i];\n        f = floorf(f / (float)base) * (float)base;\n        dst[i] = (int16_t)f;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    uint8_t base_minus_one = base - 1;\n    size_t i = 0;\n\n    if (length >= 16) {\n        __m256i base_minus_one_32 = _mm256_set1_epi32(base_minus_one);\n        __m256 base_f = _mm256_set1_ps((float)base);\n        __m256i base_32 = _mm256_set1_epi32((int32_t)base);\n        __m256i zero = _mm256_setzero_si256();\n\n        for (; i + 16 <= length; i += 16) {\n            __m256i v = _mm256_loadu_si256((__m256i const*)(src + i));\n            __m128i v_low16 = _mm256_extracti128_si256(v, 0);\n            __m128i v_high16 = _mm256_extracti128_si256(v, 1);\n            __m256i v_low32 = _mm256_cvtepi16_epi32(v_low16);\n            __m256i v_high32 = _mm256_cvtepi16_epi32(v_high16);\n\n            __m256i sign_low = _mm256_cmpgt_epi32(zero, v_low32);\n            __m256i sign_high = _mm256_cmpgt_epi32(zero, v_high32);\n            __m256i adjust_low = _mm256_and_si256(sign_low, base_minus_one_32);\n            __m256i adjust_high = _mm256_and_si256(sign_high, base_minus_one_32);\n            __m256i adjusted_low = _mm256_sub_epi32(v_low32, adjust_low);\n            __m256i adjusted_high = _mm256_sub_epi32(v_high32, adjust_high);\n\n            __m256 adjusted_low_f = _mm256_cvtepi32_ps(adjusted_low);\n            __m256 adjusted_high_f = _mm256_cvtepi32_ps(adjusted_high);\n            __m256 quotient_low_f = _mm256_div_ps(adjusted_low_f, base_f);\n            __m256 quotient_high_f = _mm256_div_ps(adjusted_high_f, base_f);\n            __m256i quotient_low = _mm256_cvttps_epi32(quotient_low_f);\n            __m256i quotient_high = _mm256_cvttps_epi32(quotient_high_f);\n\n            __m256i result_low_32 = _mm256_mullo_epi32(quotient_low, base_32);\n            __m256i result_high_32 = _mm256_mullo_epi32(quotient_high, base_32);\n\n            __m256i packed = _mm256_packs_epi32(result_low_32, result_high_32);\n            __m256i permuted = _mm256_permute4x64_epi64(packed, 0x27);\n            _mm256_storeu_si256((__m256i*)(dst + i), permuted);\n        }\n    }\n\n    for (; i < length; i++) {\n        int16_t x = src[i];\n        int16_t adjusted = x - (x < 0 ? base_minus_one : 0);\n        dst[i] = (adjusted / (int)base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    if (base == 1) {\n        for (size_t i = 0; i < length; i++) {\n            dst[i] = src[i];\n        }\n        return;\n    }\n\n    size_t i = 0;\n    float base_f = (float)base;\n    __m256 base_vec_f = _mm256_set1_ps(base_f);\n    __m256i base_vec_i32 = _mm256_set1_epi32(base);\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m256i src_i32 = _mm256_cvtepi16_epi32(src_vec);\n        __m256 src_f = _mm256_cvtepi32_ps(src_i32);\n        __m256 div_f = _mm256_div_ps(src_f, base_vec_f);\n        __m256 floor_f = _mm256_floor_ps(div_f);\n        __m256i floor_i32 = _mm256_cvtps_epi32(floor_f);\n        __m256i result_i32 = _mm256_mullo_epi32(floor_i32, base_vec_i32);\n        __m128i lo = _mm256_castsi256_si128(result_i32);\n        __m128i hi = _mm256_extracti128_si256(result_i32, 1);\n        __m128i packed = _mm_packs_epi32(lo, hi);\n        _mm_storeu_si128((__m128i*)(dst + i), packed);\n    }\n\n    for (; i < length; i++) {\n        int32_t n = src[i];\n        int32_t d = base;\n        int32_t q;\n        if (n >= 0) {\n            q = n / d;\n        } else {\n            q = (n - d + 1) / d;\n        }\n        dst[i] = (int16_t)(q * d);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <cmath>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    if (base == 1) {\n        for (size_t i = 0; i < length; i++) {\n            dst[i] = src[i];\n        }\n        return;\n    }\n\n    float base_f = (float)base;\n    size_t i = 0;\n\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i v_src = _mm256_loadu_si256((const __m256i*)(src + i));\n            \n            __m128i v_src_low = _mm256_extracti128_si256(v_src, 0);\n            __m128i v_src_high = _mm256_extracti128_si256(v_src, 1);\n            \n            __m256i v_src_low_i32 = _mm256_cvtepi16_epi32(v_src_low);\n            __m256i v_src_high_i32 = _mm256_cvtepi16_epi32(v_src_high);\n            \n            __m256 v_src_low_f = _mm256_cvtepi32_ps(v_src_low_i32);\n            __m256 v_src_high_f = _mm256_cvtepi32_ps(v_src_high_i32);\n            \n            __m256 v_base = _mm256_set1_ps(base_f);\n            \n            __m256 v_q_low = _mm256_div_ps(v_src_low_f, v_base);\n            __m256 v_q_high = _mm256_div_ps(v_src_high_f, v_base);\n            v_q_low = _mm256_floor_ps(v_q_low);\n            v_q_high = _mm256_floor_ps(v_q_high);\n            \n            __m256 v_dst_low_f = _mm256_mul_ps(v_q_low, v_base);\n            __m256 v_dst_high_f = _mm256_mul_ps(v_q_high, v_base);\n            \n            __m256i v_dst_low_i32 = _mm256_cvtps_epi32(v_dst_low_f);\n            __m256i v_dst_high_i32 = _mm256_cvtps_epi32(v_dst_high_f);\n            \n            __m128i v_dst_low_i16 = _mm_packs_epi32(\n                _mm256_castsi256_si128(v_dst_low_i32),\n                _mm256_extractf128_si256(v_dst_low_i32, 1)\n            );\n            __m128i v_dst_high_i16 = _mm_packs_epi32(\n                _mm256_castsi256_si128(v_dst_high_i32),\n                _mm256_extractf128_si256(v_dst_high_i32, 1)\n            );\n            \n            __m256i v_dst = _mm256_castsi128_si256(v_dst_low_i16);\n            v_dst = _mm256_inserti128_si256(v_dst, v_dst_high_i16, 1);\n            \n            _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n        }\n    }\n\n    for (; i < length; i++) {\n        float f = (float)src[i];\n        float q = floorf(f / base_f);\n        dst[i] = (int16_t)(q * base_f);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n\n        // Process full AVX2 chunks (4 doubles per chunk)\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(row + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        // Process remaining elements (0-3 doubles)\n        if (match) {\n            for (; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n\n        // Process full SIMD chunks (4 doubles per iteration)\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(row_start + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        // Process remaining elements (0-3 doubles)\n        if (match) {\n            for (; j < cols; ++j) {\n                if (row_start[j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        size_t col = 0;\n        bool match = true;\n\n        // Process 4 elements at a time using AVX\n        for (; col + 3 < cols; col += 4) {\n            __m256d row_chunk = _mm256_loadu_pd(row_ptr + col);\n            __m256d vec_chunk = _mm256_loadu_pd(vector + col);\n            __m256d cmp_result = _mm256_cmp_pd(row_chunk, vec_chunk, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_result);\n            if (mask != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        // Check remaining elements (0-3) if we still have a potential match\n        if (match) {\n            for (; col < cols; ++col) {\n                if (row_ptr[col] != vector[col]) {\n                    match = false;\n                    break;\n                }\n            }\n            if (match) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const double* row_ptr = matrix + i * cols;\n        size_t j = 0;\n        bool row_matches = true;\n        \n        // Process 4 elements per iteration using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_chunk = _mm256_loadu_pd(row_ptr + j);\n            __m256d vec_chunk = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_chunk, vec_chunk, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) {\n                row_matches = false;\n                break;\n            }\n        }\n        \n        // Handle remaining elements\n        if (row_matches) {\n            for (; j < cols; j++) {\n                if (row_ptr[j] != vector[j]) {\n                    row_matches = false;\n                    break;\n                }\n            }\n        }\n        \n        if (row_matches) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n\n        // Process 4 elements per iteration using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(row_ptr + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                match = false;\n                break;\n            }\n        }\n\n        // Check remaining elements\n        if (match) {\n            for (; j < cols; ++j) {\n                if (row_ptr[j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 32) {\n        for (; i <= total_elements - 32; i += 32) {\n            __m256i vecA = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i vecB = _mm256_loadu_si256((const __m256i*)(B + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n            unsigned mask = (unsigned)_mm256_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    for (; i + 31 < total; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a_vec, b_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < total; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    for (; i + 31 < total; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a, b);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t count = 0;\n\n    if (total_elements >= 32) {\n        __m256i vA, vB, vCmp;\n        for (; i <= total_elements - 32; i += 32) {\n            vA = _mm256_loadu_si256((const __m256i*)(A + i));\n            vB = _mm256_loadu_si256((const __m256i*)(B + i));\n            vCmp = _mm256_cmpeq_epi8(vA, vB);\n            unsigned mask = _mm256_movemask_epi8(vCmp);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t count = 0;\n\n    const size_t stride = 32;\n    size_t vec_iters = total_elements / stride;\n    \n    for (; i < vec_iters * stride; i += stride) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(a_vec, b_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    int32_t a_lo32 = (int32_t)(a & 0xFFFFFFFF);\n    int32_t a_hi32 = (int32_t)((a >> 32) & 0xFFFFFFFF);\n    __m256i a_lo_vec = _mm256_set1_epi64x(a_lo32);\n    __m256i a_hi_vec = _mm256_set1_epi64x(a_hi32);\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n\n        __m256i x_hi = _mm256_srli_epi64(x_vec, 32);\n        __m256i low_part = _mm256_mul_epi32(a_lo_vec, x_vec);\n        __m256i mid1 = _mm256_mul_epi32(a_lo_vec, x_hi);\n        __m256i mid2 = _mm256_mul_epi32(a_hi_vec, x_vec);\n        __m256i mid = _mm256_add_epi64(mid1, mid2);\n        __m256i mid_shifted = _mm256_slli_epi64(mid, 32);\n        __m256i product = _mm256_add_epi64(low_part, mid_shifted);\n\n        __m256i sign_mask = _mm256_srli_epi64(y_vec, 63);\n        sign_mask = _mm256_sub_epi64(zero, sign_mask);\n        __m256i abs_y = _mm256_sub_epi64(_mm256_xor_si256(y_vec, sign_mask), sign_mask);\n        __m256i result = _mm256_sub_epi64(abs_y, product);\n\n        _mm256_storeu_si256((__m256i*)(y + i), result);\n    }\n\n    for (; i < length; i++) {\n        int64_t temp = a * x[i];\n        y[i] = (y[i] < 0 ? -y[i] : y[i]) - temp;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    const __m256i mask_low32 = _mm256_set1_epi64x(0xFFFFFFFF);\n    size_t i = 0;\n    size_t end = length & ~(size_t)3;\n\n    for (; i < end; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n\n        __m256i a_lo = _mm256_and_si256(a_vec, mask_low32);\n        __m256i a_hi = _mm256_srli_epi64(a_vec, 32);\n        __m256i x_lo = _mm256_and_si256(x_vec, mask_low32);\n        __m256i x_hi = _mm256_srli_epi64(x_vec, 32);\n\n        __m256i t0 = _mm256_mul_epu32(a_lo, x_lo);\n        __m256i t1 = _mm256_mul_epu32(a_lo, x_hi);\n        __m256i t2 = _mm256_mul_epu32(a_hi, x_lo);\n\n        t1 = _mm256_slli_epi64(t1, 32);\n        t2 = _mm256_slli_epi64(t2, 32);\n\n        __m256i product = _mm256_add_epi64(t0, t1);\n        product = _mm256_add_epi64(product, t2);\n\n        __m256i diff = _mm256_sub_epi64(product, y_vec);\n\n        __m256i sign = _mm256_srli_epi64(diff, 63);\n        sign = _mm256_sub_epi64(_mm256_setzero_si256(), sign);\n        __m256i abs_diff = _mm256_xor_si256(diff, sign);\n        abs_diff = _mm256_sub_epi64(abs_diff, sign);\n\n        _mm256_storeu_si256((__m256i*)(y + i), abs_diff);\n    }\n\n    for (; i < length; i++) {\n        int64_t temp = a * x[i] - y[i];\n        y[i] = (temp < 0) ? -temp : temp;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    if (length >= 4) {\n        for (; i <= length - 4; i += 4) {\n            __m256i x_vec = _mm256_loadu_si256((const __m256i*)(x + i));\n            __m256i y_vec = _mm256_loadu_si256((const __m256i*)(y + i));\n            \n            int64_t x0 = _mm256_extract_epi64(x_vec, 0);\n            int64_t x1 = _mm256_extract_epi64(x_vec, 1);\n            int64_t x2 = _mm256_extract_epi64(x_vec, 2);\n            int64_t x3 = _mm256_extract_epi64(x_vec, 3);\n            \n            __m256i mul_vec = _mm256_set_epi64x(a * x3, a * x2, a * x1, a * x0);\n            __m256i sub = _mm256_sub_epi64(mul_vec, y_vec);\n            \n            __m256i zero = _mm256_setzero_si256();\n            __m256i mask = _mm256_cmpgt_epi64(zero, sub);\n            __m256i abs = _mm256_sub_epi64(_mm256_xor_si256(sub, mask), mask);\n            \n            _mm256_storeu_si256((__m256i*)(y + i), abs);\n        }\n    }\n    \n    for (; i < length; ++i) {\n        int64_t temp = a * x[i] - y[i];\n        if (temp < 0) {\n            y[i] = -temp;\n        } else {\n            y[i] = temp;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    const size_t vec_width = 4;\n    for (; i + vec_width <= length; i += vec_width) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n\n        int64_t x0 = _mm256_extract_epi64(x_vec, 0);\n        int64_t x1 = _mm256_extract_epi64(x_vec, 1);\n        int64_t x2 = _mm256_extract_epi64(x_vec, 2);\n        int64_t x3 = _mm256_extract_epi64(x_vec, 3);\n\n        int64_t p0 = a * x0;\n        int64_t p1 = a * x1;\n        int64_t p2 = a * x2;\n        int64_t p3 = a * x3;\n\n        __m256i p_vec = _mm256_set_epi64x(p3, p2, p1, p0);\n        __m256i diff = _mm256_sub_epi64(p_vec, y_vec);\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi64(zero, diff);\n        __m256i abs_diff = _mm256_sub_epi64(_mm256_xor_si256(diff, mask), mask);\n        \n        _mm256_storeu_si256((__m256i*)(y + i), abs_diff);\n    }\n\n    for (; i < length; ++i) {\n        int64_t temp = a * x[i] - y[i];\n        y[i] = (temp < 0) ? -temp : temp;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64x(a);\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i y_vec = _mm256_loadu_si256((__m256i const*)(y + i));\n\n        __m256i a_low = _mm256_and_si256(a_vec, _mm256_set1_epi64x(0xFFFFFFFF));\n        __m256i x_low = _mm256_and_si256(x_vec, _mm256_set1_epi64x(0xFFFFFFFF));\n        __m256i a_high = _mm256_srli_epi64(a_vec, 32);\n        __m256i x_high = _mm256_srli_epi64(x_vec, 32);\n\n        __m256i prod_low = _mm256_mul_epu32(a_low, x_low);\n        __m256i prod_a = _mm256_mul_epu32(a_low, x_high);\n        __m256i prod_b = _mm256_mul_epu32(a_high, x_low);\n        __m256i prod_mid = _mm256_add_epi64(prod_a, prod_b);\n        prod_mid = _mm256_slli_epi64(prod_mid, 32);\n        __m256i product = _mm256_add_epi64(prod_low, prod_mid);\n\n        __m256i diff = _mm256_sub_epi64(product, y_vec);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i sign = _mm256_srli_epi64(diff, 63);\n        sign = _mm256_sub_epi64(zero, sign);\n        __m256i abs_val = _mm256_sub_epi64(_mm256_xor_si256(diff, sign), sign);\n\n        _mm256_storeu_si256((__m256i*)(y + i), abs_val);\n    }\n\n    for (; i < length; i++) {\n        int64_t temp = a * x[i] - y[i];\n        y[i] = (temp >= 0) ? temp : -temp;\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length < 16) {\n        int16_t min_val = vec[0];\n        int16_t max_val = vec[0];\n        for (size_t i = 1; i < length; i++) {\n            if (vec[i] < min_val) min_val = vec[i];\n            if (vec[i] > max_val) max_val = vec[i];\n        }\n        return (MinMaxPair){min_val, max_val};\n    }\n\n    __m256i v_min = _mm256_loadu_si256((const __m256i*)vec);\n    __m256i v_max = v_min;\n    size_t i = 16;\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(vec + i));\n        v_min = _mm256_min_epi16(v_min, v);\n        v_max = _mm256_max_epi16(v_max, v);\n    }\n\n    int16_t min_arr[16];\n    int16_t max_arr[16];\n    _mm256_storeu_si256((__m256i*)min_arr, v_min);\n    _mm256_storeu_si256((__m256i*)max_arr, v_max);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 16; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; i++) {\n        int16_t x = vec[i];\n        if (x < min_val) min_val = x;\n        if (x > max_val) max_val = x;\n    }\n\n    return (MinMaxPair){min_val, max_val};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(32767);\n    __m256i max_vec = _mm256_set1_epi16(-32768);\n    \n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n    \n    int16_t min_arr[16] __attribute__((aligned(32)));\n    int16_t max_arr[16] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)min_arr, min_vec);\n    _mm256_store_si256((__m256i*)max_arr, max_vec);\n    \n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 16; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n    \n    for (; i < length; i++) {\n        if (vec[i] < min_val) min_val = vec[i];\n        else if (vec[i] > max_val) max_val = vec[i];\n    }\n    \n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(0x7FFF);\n    __m256i max_vec = _mm256_set1_epi16(0x8000);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    __m128i min_low = _mm256_castsi256_si128(min_vec);\n    __m128i min_high = _mm256_extracti128_si256(min_vec, 1);\n    __m128i min128 = _mm_min_epi16(min_low, min_high);\n    int16_t min_vals[8];\n    _mm_storeu_si128((__m128i*)min_vals, min128);\n\n    __m128i max_low = _mm256_castsi256_si128(max_vec);\n    __m128i max_high = _mm256_extracti128_si256(max_vec, 1);\n    __m128i max128 = _mm_max_epi16(max_low, max_high);\n    int16_t max_vals[8];\n    _mm_storeu_si128((__m128i*)max_vals, max128);\n\n    int16_t min_val = min_vals[0];\n    int16_t max_val = max_vals[0];\n    for (int j = 1; j < 8; j++) {\n        if (min_vals[j] < min_val) min_val = min_vals[j];\n        if (max_vals[j] > max_val) max_val = max_vals[j];\n    }\n\n    for (; i < length; i++) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_reg = _mm256_set1_epi16(32767);\n    __m256i max_reg = _mm256_set1_epi16(-32768);\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(vec + i));\n        min_reg = _mm256_min_epi16(min_reg, data);\n        max_reg = _mm256_max_epi16(max_reg, data);\n    }\n\n    int16_t min_arr[16];\n    int16_t max_arr[16];\n    _mm256_storeu_si256((__m256i*)min_arr, min_reg);\n    _mm256_storeu_si256((__m256i*)max_arr, max_reg);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 16; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; i++) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n\n    if (length >= 16) {\n        __m256i min_vec = _mm256_loadu_si256((__m256i const*)vec);\n        __m256i max_vec = min_vec;\n        size_t i = 16;\n\n        for (; i <= length - 16; i += 16) {\n            __m256i chunk = _mm256_loadu_si256((__m256i const*)(vec + i));\n            min_vec = _mm256_min_epi16(min_vec, chunk);\n            max_vec = _mm256_max_epi16(max_vec, chunk);\n        }\n\n        int16_t min_buffer[16];\n        int16_t max_buffer[16];\n        _mm256_storeu_si256((__m256i*)min_buffer, min_vec);\n        _mm256_storeu_si256((__m256i*)max_buffer, max_vec);\n\n        int16_t min_val = min_buffer[0];\n        int16_t max_val = max_buffer[0];\n        for (int j = 1; j < 16; j++) {\n            if (min_buffer[j] < min_val) min_val = min_buffer[j];\n            if (max_buffer[j] > max_val) max_val = max_buffer[j];\n        }\n\n        for (; i < length; i++) {\n            if (vec[i] < min_val) min_val = vec[i];\n            if (vec[i] > max_val) max_val = vec[i];\n        }\n\n        result.min_num = min_val;\n        result.max_num = max_val;\n    } else {\n        int16_t min_val = vec[0];\n        int16_t max_val = vec[0];\n        for (size_t i = 1; i < length; i++) {\n            if (vec[i] < min_val) min_val = vec[i];\n            if (vec[i] > max_val) max_val = vec[i];\n        }\n        result.min_num = min_val;\n        result.max_num = max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    \n    for (size_t i = 0; i < rows; i++) {\n        const int* row_ptr = matrix + i * cols;\n        if (cols == 1) {\n            continue;\n        }\n        \n        size_t j = 0;\n        while (j <= cols - 9) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row_ptr + j));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row_ptr + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != -1) {\n                return false;\n            }\n            j += 8;\n        }\n        \n        while (j < cols - 1) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n            j++;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    \n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        \n        if (cols == 1) {\n            continue;\n        }\n        \n        size_t j = 0;\n        if (cols > 8) {\n            for (; j < cols - 8; j += 8) {\n                __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row_ptr + j));\n                __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row_ptr + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n        \n        for (; j < cols - 1; ++j) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n\n        if (cols == 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        while (j < cols - 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                return false;\n            }\n            j += 8;\n        }\n\n        for (; j < cols - 1; j++) {\n            if (row[j] >= row[j+1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row_ptr = matrix + i * cols;\n        \n        if (cols == 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 8 <= cols - 1; j += 8) {\n            __m256i v1 = _mm256_loadu_si256((const __m256i*)(row_ptr + j));\n            __m256i v2 = _mm256_loadu_si256((const __m256i*)(row_ptr + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(v2, v1);\n            __m256i all_ones = _mm256_set1_epi32(-1);\n            if (!_mm256_testc_si256(cmp, all_ones)) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; j++) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    \n    for (size_t i = 0; i < rows; i++) {\n        const int* row_ptr = matrix + i * cols;\n        if (cols == 1) {\n            continue;\n        }\n        \n        size_t j = 0;\n        if (cols >= 9) {\n            for (; j <= cols - 9; j += 8) {\n                __m256i v0 = _mm256_loadu_si256((const __m256i*)(row_ptr + j));\n                __m256i v1 = _mm256_loadu_si256((const __m256i*)(row_ptr + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(v0, v1);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n        \n        for (; j < cols - 1; j++) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    \n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n    \n    for (size_t i_block = 0; i_block < num_blocks_rows; i_block++) {\n        for (size_t j_block = 0; j_block < num_blocks_cols; j_block++) {\n            if (i_block == 0 && j_block == 0) {\n                continue;\n            }\n            \n            for (size_t i = 0; i < block_size; i++) {\n                size_t j = 0;\n                for (; j + 7 < block_size; j += 8) {\n                    __m256i ref_vec = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j));\n                    __m256i cur_vec = _mm256_loadu_si256((__m256i const*)(matrix + (i_block * block_size + i) * cols + j_block * block_size + j));\n                    __m256i cmp = _mm256_cmpeq_epi32(ref_vec, cur_vec);\n                    unsigned int mask = _mm256_movemask_epi8(cmp);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n                \n                for (; j < block_size; j++) {\n                    if (matrix[i * cols + j] != matrix[(i_block * block_size + i) * cols + j_block * block_size + j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0) \n        return false;\n    if (rows % block_size != 0 || cols % block_size != 0) \n        return false;\n    \n    size_t blocks_ver = rows / block_size;\n    size_t blocks_hor = cols / block_size;\n    \n    for (size_t i = 0; i < blocks_ver; i++) {\n        for (size_t j = 0; j < blocks_hor; j++) {\n            if (i == 0 && j == 0) \n                continue;\n            \n            const int* current_block = matrix + i * block_size * cols + j * block_size;\n            \n            for (size_t r = 0; r < block_size; r++) {\n                const int* ref_row = matrix + r * cols;\n                const int* cur_row = current_block + r * cols;\n                \n                size_t k = 0;\n                for (; k + 7 < block_size; k += 8) {\n                    __m256i v_ref = _mm256_loadu_si256((const __m256i*)(ref_row + k));\n                    __m256i v_cur = _mm256_loadu_si256((const __m256i*)(cur_row + k));\n                    __m256i v_cmp = _mm256_cmpeq_epi32(v_ref, v_cur);\n                    if (_mm256_movemask_epi8(v_cmp) != 0xFFFFFFFF) \n                        return false;\n                }\n                \n                for (; k < block_size; k++) {\n                    if (ref_row[k] != cur_row[k]) \n                        return false;\n                }\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    for (size_t i = 0; i < num_blocks_rows; i++) {\n        for (size_t j = 0; j < num_blocks_cols; j++) {\n            if (i == 0 && j == 0) continue;\n\n            for (size_t r = 0; r < block_size; r++) {\n                const int* ref_row = matrix + r * cols;\n                const int* cur_row = matrix + (i * block_size + r) * cols + j * block_size;\n                \n                size_t k = 0;\n                for (; k + 7 < block_size; k += 8) {\n                    __m256i ref_vec = _mm256_loadu_si256((const __m256i*)(ref_row + k));\n                    __m256i cur_vec = _mm256_loadu_si256((const __m256i*)(cur_row + k));\n                    __m256i cmp = _mm256_cmpeq_epi32(ref_vec, cur_vec);\n                    unsigned mask = _mm256_movemask_epi8(cmp);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; k < block_size; k++) {\n                    if (ref_row[k] != cur_row[k]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0) \n        return false;\n    if (rows % block_size != 0 || cols % block_size != 0)\n        return false;\n    \n    size_t row_blocks = rows / block_size;\n    size_t col_blocks = cols / block_size;\n\n    for (size_t block_i = 0; block_i < row_blocks; block_i++) {\n        for (size_t block_j = 0; block_j < col_blocks; block_j++) {\n            if (block_i == 0 && block_j == 0) \n                continue;\n\n            const int* block_start = matrix + (block_i * block_size) * cols + (block_j * block_size);\n            \n            for (size_t r = 0; r < block_size; r++) {\n                const int* current_row = block_start + r * cols;\n                const int* ref_row = matrix + r * cols;\n                \n                size_t c = 0;\n                for (; c + 7 < block_size; c += 8) {\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_row + c));\n                    __m256i ref_vec = _mm256_loadu_si256((const __m256i*)(ref_row + c));\n                    __m256i cmp = _mm256_cmpeq_epi32(current_vec, ref_vec);\n                    int mask = _mm256_movemask_epi8(cmp);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n                \n                for (; c < block_size; c++) {\n                    if (current_row[c] != ref_row[c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0) \n        return false;\n    if (rows % block_size != 0 || cols % block_size != 0)\n        return false;\n    if (rows == 0 || cols == 0)\n        return true;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0)\n                continue;\n\n            for (size_t r = 0; r < block_size; r++) {\n                const int* ref_row = matrix + r * cols;\n                const int* tgt_row = matrix + (i + r) * cols + j;\n                size_t c = 0;\n\n                for (; c + 7 < block_size; c += 8) {\n                    __m256i ref_vec = _mm256_loadu_si256((const __m256i*)(ref_row + c));\n                    __m256i tgt_vec = _mm256_loadu_si256((const __m256i*)(tgt_row + c));\n                    __m256i cmp = _mm256_cmpeq_epi32(ref_vec, tgt_vec);\n                    unsigned mask = (unsigned)_mm256_movemask_epi8(cmp);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; c < block_size; c++) {\n                    if (ref_row[c] != tgt_row[c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0) {\n        return false;\n    }\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 1) {\n        return false;\n    }\n    size_t full_vectors = block_size / 4;\n    size_t remainder = block_size % 4;\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    \n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        const double* ref_ptr = vec;\n        const double* cur_ptr = vec + block_idx * block_size;\n        \n        for (size_t i = 0; i < full_vectors; ++i) {\n            __m256d ref_vec = _mm256_loadu_pd(ref_ptr);\n            __m256d cur_vec = _mm256_loadu_pd(cur_ptr);\n            __m256d diff = _mm256_sub_pd(ref_vec, cur_vec);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d mask = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_GT_OS);\n            if (_mm256_movemask_pd(mask) != 0) {\n                return false;\n            }\n            ref_ptr += 4;\n            cur_ptr += 4;\n        }\n        \n        for (size_t r = 0; r < remainder; ++r) {\n            double diff = std::fabs(*ref_ptr - *cur_ptr);\n            if (diff > tolerance) {\n                return false;\n            }\n            ++ref_ptr;\n            ++cur_ptr;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < block_size) {\n        return false;\n    }\n    \n    size_t n_blocks = length / block_size;\n    if (n_blocks <= 1) {\n        return true;\n    }\n    \n    for (size_t b = 1; b < n_blocks; b++) {\n        const double* block_ptr = vec + b * block_size;\n        size_t offset = 0;\n        for (; offset + 4 <= block_size; offset += 4) {\n            __m256d ref = _mm256_loadu_pd(vec + offset);\n            __m256d val = _mm256_loadu_pd(block_ptr + offset);\n            __m256d diff = _mm256_sub_pd(ref, val);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n            __m256d tol_vec = _mm256_set1_pd(tolerance);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_GT_OS);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        \n        for (; offset < block_size; offset++) {\n            double ref_val = vec[offset];\n            double val_val = block_ptr[offset];\n            double diff = ref_val - val_val;\n            if (diff < 0) {\n                diff = -diff;\n            }\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size || tolerance < 0.0 || tolerance != tolerance) {\n        return false;\n    }\n    size_t n_blocks = length / block_size;\n    if (n_blocks <= 1) {\n        return true;\n    }\n    for (size_t i = 1; i < n_blocks; i++) {\n        const double* base_block = vec;\n        const double* curr_block = vec + i * block_size;\n        size_t j = 0;\n        __m256d tol_vec = _mm256_set1_pd(tolerance);\n        __m256d sign_mask = _mm256_set1_pd(-0.0);\n        for (; j + 4 <= block_size; j += 4) {\n            __m256d base = _mm256_loadu_pd(base_block + j);\n            __m256d curr = _mm256_loadu_pd(curr_block + j);\n            __m256d diff = _mm256_sub_pd(base, curr);\n            __m256d abs_diff = _mm256_andnot_pd(sign_mask, diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_GT_OS);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        for (; j < block_size; j++) {\n            double diff = base_block[j] - curr_block[j];\n            double abs_diff = (diff < 0.0) ? -diff : diff;\n            if (abs_diff > tolerance) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < block_size || tolerance < 0.0 || tolerance != tolerance) {\n        return false;\n    }\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 1) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; i++) {\n        size_t j = 0;\n        for (; j <= block_size - 4; j += 4) {\n            __m256d v0 = _mm256_loadu_pd(vec + j);\n            __m256d vi = _mm256_loadu_pd(vec + i * block_size + j);\n            __m256d diff = _mm256_sub_pd(v0, vi);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_GT_OS);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j < block_size; j++) {\n            double a = vec[j];\n            double b = vec[i * block_size + j];\n            double diff = a - b;\n            if (diff < 0) {\n                diff = -diff;\n            }\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || tolerance < 0.0 || tolerance != tolerance) {\n        return false;\n    }\n    \n    size_t num_blocks = length / block_size;\n    if (num_blocks <= 1) {\n        return true;\n    }\n\n    for (size_t block_idx = 1; block_idx < num_blocks; block_idx++) {\n        size_t offset = block_idx * block_size;\n        size_t j = 0;\n\n        for (; j + 3 < block_size; j += 4) {\n            __m256d a = _mm256_loadu_pd(vec + j);\n            __m256d b = _mm256_loadu_pd(vec + offset + j);\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n            __m256d tol_vec = _mm256_set1_pd(tolerance);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j < block_size; j++) {\n            double a_val = vec[j];\n            double b_val = vec[offset + j];\n            double diff = a_val - b_val;\n            double abs_diff = diff < 0.0 ? -diff : diff;\n            if (abs_diff > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        __m256d acc = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            acc = _mm256_add_pd(acc, squared);\n        }\n        double tail_sum = 0.0;\n        for (; j < cols; j++) {\n            double val = matrix[i * cols + j];\n            tail_sum += val * val;\n        }\n        double vector_sum[4];\n        _mm256_storeu_pd(vector_sum, acc);\n        double total_squared = tail_sum + vector_sum[0] + vector_sum[1] + vector_sum[2] + vector_sum[3];\n        __m128d x = _mm_set_sd(total_squared);\n        __m128d root = _mm_sqrt_sd(x, x);\n        double norm = _mm_cvtsd_f64(root);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        const double* row_ptr = matrix + i * cols;\n        size_t j = 0;\n        \n        // Process 4 elements per iteration using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row_ptr + j);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n        \n        // Horizontal sum of accumulated values\n        __m128d low = _mm256_castpd256_pd128(sum_vec);\n        __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n        low = _mm_add_pd(low, high);\n        __m128d hsum = _mm_hadd_pd(low, low);\n        double total = _mm_cvtsd_f64(hsum);\n        \n        // Process remaining elements\n        for (; j < cols; ++j) {\n            double val = row_ptr[j];\n            total += val * val;\n        }\n        \n        // Compare squared norm with squared threshold\n        double thr = thresholds[i];\n        if (thr < 0.0) {\n            result[i] = true;\n        } else {\n            double thr_sq = thr * thr;\n            result[i] = (total > thr_sq);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        __m256d v_sq_sum = _mm256_setzero_pd();\n        size_t j = 0;\n\n        for (; j + 3 < cols; j += 4) {\n            __m256d v = _mm256_loadu_pd(row_ptr + j);\n            __m256d v_sq = _mm256_mul_pd(v, v);\n            v_sq_sum = _mm256_add_pd(v_sq_sum, v_sq);\n        }\n\n        if (j < cols) {\n            size_t remaining = cols - j;\n            __m256i mask = _mm256_setr_epi64x(\n                (remaining > 0) ? -1LL : 0,\n                (remaining > 1) ? -1LL : 0,\n                (remaining > 2) ? -1LL : 0,\n                (remaining > 3) ? -1LL : 0\n            );\n            __m256d v_rem = _mm256_maskload_pd(row_ptr + j, mask);\n            __m256d v_rem_sq = _mm256_mul_pd(v_rem, v_rem);\n            v_sq_sum = _mm256_add_pd(v_sq_sum, v_rem_sq);\n        }\n\n        __m128d vlow = _mm256_castpd256_pd128(v_sq_sum);\n        __m128d vhigh = _mm256_extractf128_pd(v_sq_sum, 1);\n        vlow = _mm_add_pd(vlow, vhigh);\n        __m128d vhigh2 = _mm_unpackhi_pd(vlow, vlow);\n        __m128d vsum = _mm_add_sd(vlow, vhigh2);\n        __m128d vsqrt = _mm_sqrt_sd(vsum, vsum);\n        double norm = _mm_cvtsd_f64(vsqrt);\n\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = matrix + i * cols;\n        __m256d sum_sq = _mm256_setzero_pd();\n        size_t j = 0;\n\n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row_start + j);\n            __m256d sq = _mm256_mul_pd(vec, vec);\n            sum_sq = _mm256_add_pd(sum_sq, sq);\n        }\n\n        double final_sum = 0.0;\n        __m128d sum_low = _mm256_castpd256_pd128(sum_sq);\n        __m128d sum_high = _mm256_extractf128_pd(sum_sq, 1);\n        sum_low = _mm_add_pd(sum_low, sum_high);\n        __m128d high64 = _mm_unpackhi_pd(sum_low, sum_low);\n        final_sum = _mm_cvtsd_f64(_mm_add_sd(sum_low, high64));\n\n        for (; j < cols; ++j) {\n            double val = row_start[j];\n            final_sum += val * val;\n        }\n\n        double threshold_sq = thresholds[i] * thresholds[i];\n        result[i] = (final_sum > threshold_sq);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        const double* row_ptr = matrix + i * cols;\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row_ptr + j);\n            __m256d sq = _mm256_mul_pd(vec, vec);\n            sum_vec = _mm256_add_pd(sum_vec, sq);\n        }\n\n        double total = 0.0;\n        double sum_arr[4];\n        _mm256_storeu_pd(sum_arr, sum_vec);\n        total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n        for (; j < cols; j++) {\n            double val = row_ptr[j];\n            total += val * val;\n        }\n\n        double norm = sqrt(total);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t numAVX = length / 8;\n\n    for (; i < numAVX * 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 8;\n    const size_t vectorized_length = length - (length % vector_elements);\n\n    for (; i < vectorized_length; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 8;\n    size_t vectors = length / vector_elements;\n\n    for (; i < vectors * vector_elements; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 8;\n    const size_t vectors = length / vector_elements;\n\n    for (; i < vectors * vector_elements; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 8;\n    const size_t vectors = length / vector_elements;\n\n    for (; i < vectors * vector_elements; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_or_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstatic inline int32_t horizontal_sum_epi32(__m256i a) {\n    __m128i low = _mm256_castsi256_si128(a);\n    __m128i high = _mm256_extracti128_si256(a, 1);\n    __m128i sum = _mm_add_epi32(low, high);\n    __m128i shifted = _mm_srli_si128(sum, 8);\n    __m128i sum1 = _mm_add_epi32(sum, shifted);\n    shifted = _mm_srli_si128(sum1, 4);\n    __m128i sum2 = _mm_add_epi32(sum1, shifted);\n    return _mm_cvtsi128_si32(sum2);\n}\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    int64_t total_sqA = 0;\n    int64_t total_sqB = 0;\n    int64_t total_prod = 0;\n    size_t i = 0;\n    const size_t stride = 32;\n\n    for (; i + stride <= length; i += stride) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m128i va_low128 = _mm256_castsi256_si128(va);\n        __m128i va_high128 = _mm256_extractf128_si256(va, 1);\n        __m128i vb_low128 = _mm256_castsi256_si128(vb);\n        __m128i vb_high128 = _mm256_extractf128_si256(vb, 1);\n\n        __m256i va_low = _mm256_cvtepi8_epi16(va_low128);\n        __m256i va_high = _mm256_cvtepi8_epi16(va_high128);\n        __m256i vb_low = _mm256_cvtepi8_epi16(vb_low128);\n        __m256i vb_high = _mm256_cvtepi8_epi16(vb_high128);\n\n        __m256i sqA_low = _mm256_mullo_epi16(va_low, va_low);\n        __m256i sqA_high = _mm256_mullo_epi16(va_high, va_high);\n        __m256i sqA_low_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqA_low, 0));\n        __m256i sqA_low_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqA_low, 1));\n        __m256i sqA_high_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqA_high, 0));\n        __m256i sqA_high_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqA_high, 1));\n        __m256i sum_sqA = _mm256_setzero_si256();\n        sum_sqA = _mm256_add_epi32(sum_sqA, sqA_low_lo32);\n        sum_sqA = _mm256_add_epi32(sum_sqA, sqA_low_hi32);\n        sum_sqA = _mm256_add_epi32(sum_sqA, sqA_high_lo32);\n        sum_sqA = _mm256_add_epi32(sum_sqA, sqA_high_hi32);\n        total_sqA += horizontal_sum_epi32(sum_sqA);\n\n        __m256i sqB_low = _mm256_mullo_epi16(vb_low, vb_low);\n        __m256i sqB_high = _mm256_mullo_epi16(vb_high, vb_high);\n        __m256i sqB_low_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqB_low, 0));\n        __m256i sqB_low_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqB_low, 1));\n        __m256i sqB_high_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqB_high, 0));\n        __m256i sqB_high_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(sqB_high, 1));\n        __m256i sum_sqB = _mm256_setzero_si256();\n        sum_sqB = _mm256_add_epi32(sum_sqB, sqB_low_lo32);\n        sum_sqB = _mm256_add_epi32(sum_sqB, sqB_low_hi32);\n        sum_sqB = _mm256_add_epi32(sum_sqB, sqB_high_lo32);\n        sum_sqB = _mm256_add_epi32(sum_sqB, sqB_high_hi32);\n        total_sqB += horizontal_sum_epi32(sum_sqB);\n\n        __m256i prod_low = _mm256_mullo_epi16(va_low, vb_low);\n        __m256i prod_high = _mm256_mullo_epi16(va_high, vb_high);\n        __m256i prod_low_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_low, 0));\n        __m256i prod_low_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_low, 1));\n        __m256i prod_high_lo32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_high, 0));\n        __m256i prod_high_hi32 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(prod_high, 1));\n        __m256i sum_prod = _mm256_setzero_si256();\n        sum_prod = _mm256_add_epi32(sum_prod, prod_low_lo32);\n        sum_prod = _mm256_add_epi32(sum_prod, prod_low_hi32);\n        sum_prod = _mm256_add_epi32(sum_prod, prod_high_lo32);\n        sum_prod = _mm256_add_epi32(sum_prod, prod_high_hi32);\n        total_prod += horizontal_sum_epi32(sum_prod);\n    }\n\n    for (; i < length; i++) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        total_sqA += (int32_t)a * a;\n        total_sqB += (int32_t)b * b;\n        total_prod += (int32_t)a * b;\n    }\n\n    int64_t result = total_sqA + total_sqB - 2 * total_prod;\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t total = 0;\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m256i a_low = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vecA));\n        __m256i a_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 1));\n        __m256i b_low = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vecB));\n        __m256i b_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 1));\n\n        __m256i diff_low = _mm256_sub_epi16(a_low, b_low);\n        __m256i diff_high = _mm256_sub_epi16(a_high, b_high);\n\n        __m256i dll = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(diff_low));\n        __m256i dlh = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(diff_low, 1));\n        __m256i dhl = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(diff_high));\n        __m256i dhh = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(diff_high, 1));\n\n        __m256i sq0 = _mm256_mullo_epi32(dll, dll);\n        __m256i sq1 = _mm256_mullo_epi32(dlh, dlh);\n        __m256i sq2 = _mm256_mullo_epi32(dhl, dhl);\n        __m256i sq3 = _mm256_mullo_epi32(dhh, dhh);\n\n        __m256i sum32 = _mm256_add_epi32(sq0, sq1);\n        sum32 = _mm256_add_epi32(sum32, sq2);\n        sum32 = _mm256_add_epi32(sum32, sq3);\n\n        __m128i low = _mm256_castsi256_si128(sum32);\n        __m128i high = _mm256_extracti128_si256(sum32, 1);\n        low = _mm_add_epi32(low, high);\n        __m128i hadd = _mm_hadd_epi32(low, low);\n        hadd = _mm_hadd_epi32(hadd, hadd);\n        total += (int64_t)_mm_extract_epi32(hadd, 0);\n    }\n\n    for (; i < length; ++i) {\n        int16_t diff = (int16_t)A[i] - (int16_t)B[i];\n        total += (int64_t)diff * (int64_t)diff;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t sum_sq = 0;\n    int64_t sum_prod = 0;\n    size_t i = 0;\n\n    if (length >= 32) {\n        __m256i sq_acc32 = _mm256_setzero_si256();\n        __m256i prod_acc32 = _mm256_setzero_si256();\n\n        for (; i <= length - 32; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n            __m256i a_lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(a));\n            __m256i a_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a, 1));\n            __m256i b_lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(b));\n            __m256i b_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b, 1));\n\n            __m256i sq_lo = _mm256_madd_epi16(a_lo, a_lo);\n            __m256i sq_hi = _mm256_madd_epi16(a_hi, a_hi);\n            __m256i prod_lo = _mm256_madd_epi16(a_lo, b_lo);\n            __m256i prod_hi = _mm256_madd_epi16(a_hi, b_hi);\n\n            sq_acc32 = _mm256_add_epi32(sq_acc32, sq_lo);\n            sq_acc32 = _mm256_add_epi32(sq_acc32, sq_hi);\n            prod_acc32 = _mm256_add_epi32(prod_acc32, prod_lo);\n            prod_acc32 = _mm256_add_epi32(prod_acc32, prod_hi);\n        }\n\n        int32_t sq_temp[8] __attribute__((aligned(32)));\n        int32_t prod_temp[8] __attribute__((aligned(32)));\n        _mm256_store_si256((__m256i*)sq_temp, sq_acc32);\n        _mm256_store_si256((__m256i*)prod_temp, prod_acc32);\n\n        for (int j = 0; j < 8; j++) {\n            sum_sq += (int64_t)sq_temp[j];\n            sum_prod += (int64_t)prod_temp[j];\n        }\n    }\n\n    for (; i < length; i++) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        sum_sq += (int64_t)a_val * a_val;\n        sum_prod += (int64_t)a_val * b_val;\n    }\n\n    return sum_sq - sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sumA2 = _mm256_setzero_si256();\n    __m256i sumB2 = _mm256_setzero_si256();\n    __m256i sumAB = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m128i va_low128 = _mm256_castsi256_si128(va);\n        __m128i va_high128 = _mm256_extractf128_si256(va, 1);\n        __m256i va_low = _mm256_cvtepi8_epi16(va_low128);\n        __m256i va_high = _mm256_cvtepi8_epi16(va_high128);\n\n        __m128i vb_low128 = _mm256_castsi256_si128(vb);\n        __m128i vb_high128 = _mm256_extractf128_si256(vb, 1);\n        __m256i vb_low = _mm256_cvtepi8_epi16(vb_low128);\n        __m256i vb_high = _mm256_cvtepi8_epi16(vb_high128);\n\n        __m256i a_sq_low = _mm256_madd_epi16(va_low, va_low);\n        __m256i b_sq_low = _mm256_madd_epi16(vb_low, vb_low);\n        __m256i ab_low = _mm256_madd_epi16(va_low, vb_low);\n\n        __m256i a_sq_high = _mm256_madd_epi16(va_high, va_high);\n        __m256i b_sq_high = _mm256_madd_epi16(vb_high, vb_high);\n        __m256i ab_high = _mm256_madd_epi16(va_high, vb_high);\n\n        __m128i a_sq_low_low128 = _mm256_castsi256_si128(a_sq_low);\n        __m128i a_sq_low_high128 = _mm256_extractf128_si256(a_sq_low, 1);\n        __m256i a_sq_low_low64 = _mm256_cvtepi32_epi64(a_sq_low_low128);\n        __m256i a_sq_low_high64 = _mm256_cvtepi32_epi64(a_sq_low_high128);\n        sumA2 = _mm256_add_epi64(sumA2, a_sq_low_low64);\n        sumA2 = _mm256_add_epi64(sumA2, a_sq_low_high64);\n\n        __m128i a_sq_high_low128 = _mm256_castsi256_si128(a_sq_high);\n        __m128i a_sq_high_high128 = _mm256_extractf128_si256(a_sq_high, 1);\n        __m256i a_sq_high_low64 = _mm256_cvtepi32_epi64(a_sq_high_low128);\n        __m256i a_sq_high_high64 = _mm256_cvtepi32_epi64(a_sq_high_high128);\n        sumA2 = _mm256_add_epi64(sumA2, a_sq_high_low64);\n        sumA2 = _mm256_add_epi64(sumA2, a_sq_high_high64);\n\n        __m128i b_sq_low_low128 = _mm256_castsi256_si128(b_sq_low);\n        __m128i b_sq_low_high128 = _mm256_extractf128_si256(b_sq_low, 1);\n        __m256i b_sq_low_low64 = _mm256_cvtepi32_epi64(b_sq_low_low128);\n        __m256i b_sq_low_high64 = _mm256_cvtepi32_epi64(b_sq_low_high128);\n        sumB2 = _mm256_add_epi64(sumB2, b_sq_low_low64);\n        sumB2 = _mm256_add_epi64(sumB2, b_sq_low_high64);\n\n        __m128i b_sq_high_low128 = _mm256_castsi256_si128(b_sq_high);\n        __m128i b_sq_high_high128 = _mm256_extractf128_si256(b_sq_high, 1);\n        __m256i b_sq_high_low64 = _mm256_cvtepi32_epi64(b_sq_high_low128);\n        __m256i b_sq_high_high64 = _mm256_cvtepi32_epi64(b_sq_high_high128);\n        sumB2 = _mm256_add_epi64(sumB2, b_sq_high_low64);\n        sumB2 = _mm256_add_epi64(sumB2, b_sq_high_high64);\n\n        __m128i ab_low_low128 = _mm256_castsi256_si128(ab_low);\n        __m128i ab_low_high128 = _mm256_extractf128_si256(ab_low, 1);\n        __m256i ab_low_low64 = _mm256_cvtepi32_epi64(ab_low_low128);\n        __m256i ab_low_high64 = _mm256_cvtepi32_epi64(ab_low_high128);\n        sumAB = _mm256_add_epi64(sumAB, ab_low_low64);\n        sumAB = _mm256_add_epi64(sumAB, ab_low_high64);\n\n        __m128i ab_high_low128 = _mm256_castsi256_si128(ab_high);\n        __m128i ab_high_high128 = _mm256_extractf128_si256(ab_high, 1);\n        __m256i ab_high_low64 = _mm256_cvtepi32_epi64(ab_high_low128);\n        __m256i ab_high_high64 = _mm256_cvtepi32_epi64(ab_high_high128);\n        sumAB = _mm256_add_epi64(sumAB, ab_high_low64);\n        sumAB = _mm256_add_epi64(sumAB, ab_high_high64);\n    }\n\n    uint64_t a2_arr[4], b2_arr[4], ab_arr[4];\n    _mm256_storeu_si256((__m256i*)a2_arr, sumA2);\n    _mm256_storeu_si256((__m256i*)b2_arr, sumB2);\n    _mm256_storeu_si256((__m256i*)ab_arr, sumAB);\n\n    int64_t totalA2 = a2_arr[0] + a2_arr[1] + a2_arr[2] + a2_arr[3];\n    int64_t totalB2 = b2_arr[0] + b2_arr[1] + b2_arr[2] + b2_arr[3];\n    int64_t totalAB = ab_arr[0] + ab_arr[1] + ab_arr[2] + ab_arr[3];\n\n    for (; i < length; ++i) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        totalA2 += (int64_t)a * a;\n        totalB2 += (int64_t)b * b;\n        totalAB += (int64_t)a * b;\n    }\n\n    return totalA2 + totalB2 - 2 * totalAB;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t total_sq = 0;\n    int64_t total_prod = 0;\n    size_t i = 0;\n    const __m256i ones = _mm256_set1_epi16(1);\n    uint32_t temp[8];\n\n    for (; i + 31 < length; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vb = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m128i va_low128 = _mm256_extracti128_si256(va, 0);\n        __m128i va_high128 = _mm256_extracti128_si256(va, 1);\n        __m256i va_low = _mm256_cvtepi8_epi16(va_low128);\n        __m256i va_high = _mm256_cvtepi8_epi16(va_high128);\n\n        __m128i vb_low128 = _mm256_extracti128_si256(vb, 0);\n        __m128i vb_high128 = _mm256_extracti128_si256(vb, 1);\n        __m256i vb_low = _mm256_cvtepi8_epi16(vb_low128);\n        __m256i vb_high = _mm256_cvtepi8_epi16(vb_high128);\n\n        __m256i sq_low = _mm256_mullo_epi16(va_low, va_low);\n        __m256i sq_high = _mm256_mullo_epi16(va_high, va_high);\n        __m256i sq_low_32 = _mm256_madd_epi16(sq_low, ones);\n        __m256i sq_high_32 = _mm256_madd_epi16(sq_high, ones);\n        __m256i sq_32 = _mm256_add_epi32(sq_low_32, sq_high_32);\n\n        __m256i prod_low = _mm256_mullo_epi16(va_low, vb_low);\n        __m256i prod_high = _mm256_mullo_epi16(va_high, vb_high);\n        __m256i prod_low_32 = _mm256_madd_epi16(prod_low, ones);\n        __m256i prod_high_32 = _mm256_madd_epi16(prod_high, ones);\n        __m256i prod_32 = _mm256_add_epi32(prod_low_32, prod_high_32);\n\n        _mm256_storeu_si256((__m256i*)temp, sq_32);\n        for (int j = 0; j < 8; j++) {\n            total_sq += (int64_t)(int32_t)temp[j];\n        }\n\n        _mm256_storeu_si256((__m256i*)temp, prod_32);\n        for (int j = 0; j < 8; j++) {\n            total_prod += (int64_t)(int32_t)temp[j];\n        }\n    }\n\n    for (; i < length; i++) {\n        int8_t a = A[i];\n        total_sq += (int64_t)a * a;\n        total_prod += (int64_t)a * B[i];\n    }\n\n    return total_sq - total_prod;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_elements = 4;\n    for (; i + vector_elements - 1 < length; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_andnot_si256(b_vec, a_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    const __m256i ones = _mm256_set1_epi64x(-1LL);\n    size_t i = 0;\n    const size_t vector_elements = 4;\n    const size_t vectorized_end = length - (length % vector_elements);\n\n    for (; i < vectorized_end; i += vector_elements) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b_vec = _mm256_xor_si256(b_vec, ones);\n        __m256i result = _mm256_and_si256(a_vec, not_b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_size = 4;\n    for (; i + vector_size - 1 < length; i += vector_size) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_andnot_si256(b_vec, a_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t step = 4;\n    const size_t avx_limit = length - (length % step);\n\n    for (; i < avx_limit; i += step) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_andnot_si256(b_vec, a_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vector_size = 4; // 256-bit vector holds 4 uint64_t\n\n    for (; i + vector_size - 1 < length; i += vector_size) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_andnot_si256(b_vec, a_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i minus_one = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        \n        __m256i a_neg = _mm256_cmpgt_epi8(zero, a);\n        __m256i b_neg = _mm256_cmpgt_epi8(zero, b);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i res = _mm256_and_si256(both_pos, one);\n        res = _mm256_or_si256(res, _mm256_and_si256(both_neg, minus_one));\n        \n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one_vec = _mm256_set1_epi8(1);\n    const __m256i minus_one_vec = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        \n        __m256i a_neg = _mm256_cmplt_epi8(a, zero);\n        __m256i b_neg = _mm256_cmplt_epi8(b, zero);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i pos_part = _mm256_and_si256(both_pos, one_vec);\n        __m256i neg_part = _mm256_and_si256(both_neg, minus_one_vec);\n        __m256i res = _mm256_or_si256(pos_part, neg_part);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i minus_one = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a_vec, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b_vec, zero);\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        \n        __m256i a_neg = _mm256_cmpgt_epi8(zero, a_vec);\n        __m256i b_neg = _mm256_cmpgt_epi8(zero, b_vec);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i pos_part = _mm256_and_si256(both_pos, one);\n        __m256i neg_part = _mm256_and_si256(both_neg, minus_one);\n        \n        __m256i res = _mm256_or_si256(pos_part, neg_part);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one_vec = _mm256_set1_epi8(1);\n    const __m256i neg_one_vec = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n\n        __m256i a_neg = _mm256_cmpgt_epi8(zero, a);\n        __m256i b_neg = _mm256_cmpgt_epi8(zero, b);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i pos_part = _mm256_and_si256(both_pos, one_vec);\n        __m256i neg_part = _mm256_and_si256(both_neg, neg_one_vec);\n        __m256i res = _mm256_or_si256(pos_part, neg_part);\n\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i minus_one = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        \n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i a_neg = _mm256_cmpgt_epi8(zero, a);\n        __m256i b_neg = _mm256_cmpgt_epi8(zero, b);\n        \n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n        \n        __m256i res_pos = _mm256_and_si256(both_pos, one);\n        __m256i res_neg = _mm256_and_si256(both_neg, minus_one);\n        __m256i result = _mm256_or_si256(res_pos, res_neg);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid_index = length / 2;\n    \n    // Process XOR part (indices [0, mid_index-1])\n    size_t i = 0;\n    for (; i + 16 <= mid_index; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < mid_index; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Handle midpoint if length is odd\n    if (length % 2 == 1) {\n        C[mid_index] = A[mid_index];\n    }\n    \n    // Process AND part\n    size_t j = (length % 2 == 1) ? mid_index + 1 : mid_index;\n    for (; j + 16 <= length; j += 16) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + j));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + j));\n        __m256i res = _mm256_and_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + j), res);\n    }\n    for (; j < length; j++) {\n        C[j] = A[j] & B[j];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) return;\n    size_t mid = length / 2;\n    \n    // Process elements before mid (XOR operation)\n    size_t i = 0;\n    for (; i + 16 <= mid; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n    for (; i < mid; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle midpoint element\n    C[mid] = A[mid];\n    \n    // Process elements after mid (AND operation)\n    i = mid + 1;\n    for (; i + 16 <= length; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_and_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before midpoint (XOR)\n    for (; i + 16 <= mid; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle midpoint\n    C[mid] = A[mid];\n    i = mid + 1;\n\n    // Process elements after midpoint (AND)\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) return;\n    \n    size_t mid = length / 2;\n    int is_odd = length & 1;\n    size_t part3_start = is_odd ? mid + 1 : mid;\n\n    size_t i = 0;\n    size_t end_vec1 = mid & ~0x0F;\n    for (; i < end_vec1; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < mid; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    if (is_odd) {\n        C[mid] = A[mid];\n    }\n\n    i = part3_start;\n    size_t part3_len = length - part3_start;\n    size_t end_vec3 = part3_start + (part3_len & ~0x0F);\n    for (; i < end_vec3; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) return;\n    \n    size_t mid = length / 2;\n    size_t and_start = length - mid;\n\n    // Process XOR region [0, mid)\n    size_t i = 0;\n    for (; i + 15 < mid; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i res = _mm256_xor_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Process AND region [and_start, length)\n    i = and_start;\n    for (; i + 15 < length; i += 16) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i res = _mm256_and_si256(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), res);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n\n    // Handle midpoint element for odd lengths\n    if (length % 2 == 1) {\n        C[mid] = A[mid];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 7 < total_elements; i += 8) {\n        __m256 A_vec = _mm256_loadu_ps(A + i);\n        __m256 B_vec = _mm256_loadu_ps(B + i);\n        __m256 C_vec = _mm256_loadu_ps(C + i);\n        __m256 D_vec = _mm256_loadu_ps(D + i);\n        \n        __m128i cond1_8 = _mm_loadl_epi64((const __m128i*)(cond1 + i));\n        __m128i cond2_8 = _mm_loadl_epi64((const __m128i*)(cond2 + i));\n        \n        __m256i cond1_int = _mm256_cvtepi8_epi32(cond1_8);\n        __m256i cond2_int = _mm256_cvtepi8_epi32(cond2_8);\n        \n        __m256i mask1_int = _mm256_cmpgt_epi32(cond1_int, _mm256_setzero_si256());\n        __m256i mask2_int = _mm256_cmpgt_epi32(cond2_int, _mm256_setzero_si256());\n        \n        __m256 mask1_float = _mm256_castsi256_ps(mask1_int);\n        __m256 mask2_float = _mm256_castsi256_ps(mask2_int);\n        \n        __m256 temp1 = _mm256_blendv_ps(B_vec, A_vec, mask2_float);\n        __m256 temp2 = _mm256_blendv_ps(D_vec, C_vec, mask2_float);\n        __m256 out_vec = _mm256_blendv_ps(temp2, temp1, mask1_float);\n        \n        _mm256_storeu_ps(out + i, out_vec);\n    }\n    \n    for (; i < total_elements; i++) {\n        bool c1 = cond1[i];\n        bool c2 = cond2[i];\n        if (c1 && c2) {\n            out[i] = A[i];\n        } else if (c1 && !c2) {\n            out[i] = B[i];\n        } else if (!c1 && c2) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 c = _mm256_loadu_ps(C + i);\n        __m256 d = _mm256_loadu_ps(D + i);\n\n        __m128i cond1_bytes = _mm_loadu_si64(cond1 + i);\n        __m128i cond2_bytes = _mm_loadu_si64(cond2 + i);\n\n        __m256i cond1_int = _mm256_cvtepu8_epi32(cond1_bytes);\n        cond1_int = _mm256_slli_epi32(cond1_int, 31);\n        __m256 cond1_mask = _mm256_castsi256_ps(cond1_int);\n\n        __m256i cond2_int = _mm256_cvtepu8_epi32(cond2_bytes);\n        cond2_int = _mm256_slli_epi32(cond2_int, 31);\n        __m256 cond2_mask = _mm256_castsi256_ps(cond2_int);\n\n        __m256 temp1 = _mm256_blendv_ps(b, a, cond2_mask);\n        __m256 temp2 = _mm256_blendv_ps(d, c, cond2_mask);\n        __m256 res = _mm256_blendv_ps(temp2, temp1, cond1_mask);\n\n        _mm256_storeu_ps(out + i, res);\n    }\n\n    for (; i < total; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    for (; i + 7 < total; i += 8) {\n        __m128i cond1_8 = _mm_loadl_epi64((const __m128i*)(cond1 + i));\n        __m128i cond2_8 = _mm_loadl_epi64((const __m128i*)(cond2 + i));\n        \n        __m256i cond1_mask = _mm256_cmpgt_epi32(\n            _mm256_cvtepu8_epi32(cond1_8),\n            _mm256_setzero_si256()\n        );\n        __m256i cond2_mask = _mm256_cmpgt_epi32(\n            _mm256_cvtepu8_epi32(cond2_8),\n            _mm256_setzero_si256()\n        );\n        \n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 c_vec = _mm256_loadu_ps(C + i);\n        __m256 d_vec = _mm256_loadu_ps(D + i);\n        \n        __m256 true_branch = _mm256_blendv_ps(b_vec, a_vec, _mm256_castsi256_ps(cond2_mask));\n        __m256 false_branch = _mm256_blendv_ps(d_vec, c_vec, _mm256_castsi256_ps(cond2_mask));\n        __m256 res = _mm256_blendv_ps(false_branch, true_branch, _mm256_castsi256_ps(cond1_mask));\n        \n        _mm256_storeu_ps(out + i, res);\n    }\n    \n    for (; i < total; i++) {\n        if (cond1[i]) {\n            out[i] = cond2[i] ? A[i] : B[i];\n        } else {\n            out[i] = cond2[i] ? C[i] : D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 c_vec = _mm256_loadu_ps(C + i);\n        __m256 d_vec = _mm256_loadu_ps(D + i);\n\n        __m128i cond1_byte = _mm_loadl_epi64((const __m128i*)(cond1 + i));\n        __m128i cond2_byte = _mm_loadl_epi64((const __m128i*)(cond2 + i));\n\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask1_byte = _mm_cmpgt_epi8(cond1_byte, zero);\n        __m128i mask2_byte = _mm_cmpgt_epi8(cond2_byte, zero);\n\n        __m128i mask1_low = _mm_cvtepi8_epi32(mask1_byte);\n        __m128i mask1_high = _mm_cvtepi8_epi32(_mm_srli_si128(mask1_byte, 4));\n        __m256i mask1_32bit = _mm256_set_m128i(mask1_high, mask1_low);\n        __m256 mask1_float = _mm256_castsi256_ps(mask1_32bit);\n\n        __m128i mask2_low = _mm_cvtepi8_epi32(mask2_byte);\n        __m128i mask2_high = _mm_cvtepi8_epi32(_mm_srli_si128(mask2_byte, 4));\n        __m256i mask2_32bit = _mm256_set_m128i(mask2_high, mask2_low);\n        __m256 mask2_float = _mm256_castsi256_ps(mask2_32bit);\n\n        __m256 temp1 = _mm256_blendv_ps(b_vec, a_vec, mask2_float);\n        __m256 temp2 = _mm256_blendv_ps(d_vec, c_vec, mask2_float);\n        __m256 res = _mm256_blendv_ps(temp2, temp1, mask1_float);\n\n        _mm256_storeu_ps(out + i, res);\n    }\n\n    for (; i < total; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m128i c1_8 = _mm_loadl_epi64((const __m128i*)(cond1 + i));\n        __m128i c2_8 = _mm_loadl_epi64((const __m128i*)(cond2 + i));\n\n        __m256i c1_32 = _mm256_cvtepu8_epi32(c1_8);\n        __m256i c2_32 = _mm256_cvtepu8_epi32(c2_8);\n\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i c1_mask = _mm256_cmpeq_epi32(c1_32, ones);\n        __m256i c2_mask = _mm256_cmpeq_epi32(c2_32, ones);\n\n        __m256 mask1 = _mm256_castsi256_ps(c1_mask);\n        __m256 mask2 = _mm256_castsi256_ps(c2_mask);\n\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 c = _mm256_loadu_ps(C + i);\n        __m256 d = _mm256_loadu_ps(D + i);\n\n        __m256 step1 = _mm256_blendv_ps(b, a, mask2);\n        __m256 step2 = _mm256_blendv_ps(d, c, mask2);\n        __m256 res = _mm256_blendv_ps(step2, step1, mask1);\n\n        _mm256_storeu_ps(out + i, res);\n    }\n\n    for (; i < total; i++) {\n        if (cond1[i]) {\n            if (cond2[i]) {\n                out[i] = A[i];\n            } else {\n                out[i] = B[i];\n            }\n        } else {\n            if (cond2[i]) {\n                out[i] = C[i];\n            } else {\n                out[i] = D[i];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t n = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    const __m128i lookup_128 = _mm_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n    const __m256i lookup_table = _mm256_broadcastsi128_si256(lookup_128);\n    const __m256i mask = _mm256_set1_epi8(0x0F);\n    const __m256i ones8 = _mm256_set1_epi8(1);\n    const __m256i ones16 = _mm256_set1_epi16(1);\n    \n    for (; i + 7 < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i low_nibbles = _mm256_and_si256(v, mask);\n        __m256i high_nibbles = _mm256_and_si256(_mm256_srli_epi16(v, 4), mask);\n        __m256i pop_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        __m256i pop_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        __m256i byte_count = _mm256_add_epi8(pop_low, pop_high);\n        __m256i sum16 = _mm256_maddubs_epi16(byte_count, ones8);\n        __m256i sum32 = _mm256_madd_epi16(sum16, ones16);\n        __m128i low_sum32 = _mm256_castsi256_si128(sum32);\n        __m128i high_sum32 = _mm256_extracti128_si256(sum32, 1);\n        __m128i packed16 = _mm_packus_epi32(low_sum32, high_sum32);\n        __m128i packed8 = _mm_packus_epi16(packed16, _mm_setzero_si128());\n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n    \n    for (; i < n; i++) {\n        out[i] = (uint8_t)__builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i m1 = _mm256_set1_epi32(0x55555555);\n        __m256i t1 = _mm256_srli_epi32(v, 1);\n        t1 = _mm256_and_si256(t1, m1);\n        v = _mm256_sub_epi32(v, t1);\n        \n        __m256i m2 = _mm256_set1_epi32(0x33333333);\n        __m256i t2 = _mm256_srli_epi32(v, 2);\n        t2 = _mm256_and_si256(t2, m2);\n        v = _mm256_and_si256(v, m2);\n        v = _mm256_add_epi32(v, t2);\n        \n        __m256i m3 = _mm256_set1_epi32(0x0F0F0F0F);\n        __m256i t3 = _mm256_srli_epi32(v, 4);\n        v = _mm256_add_epi32(v, t3);\n        v = _mm256_and_si256(v, m3);\n        \n        __m256i m4 = _mm256_set1_epi32(0x01010101);\n        v = _mm256_mullo_epi32(v, m4);\n        v = _mm256_srli_epi32(v, 24);\n        \n        __m128i low = _mm256_castsi256_si128(v);\n        __m128i high = _mm256_extracti128_si256(v, 1);\n        __m128i packed16 = _mm_packus_epi32(low, high);\n        __m128i packed8 = _mm_packus_epi16(packed16, _mm_setzero_si128());\n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        out[i] = (uint8_t)((x * 0x01010101) >> 24);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    const __m256i m1 = _mm256_set1_epi32(0x55555555);\n    const __m256i m2 = _mm256_set1_epi32(0x33333333);\n    const __m256i m3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i m4 = _mm256_set1_epi32(0x01010101);\n    const __m256i shuffle_mask = _mm256_set_epi8(\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        28, 24, 20, 16, 12, 8, 4, 0\n    );\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i t = _mm256_srli_epi32(v, 1);\n        t = _mm256_and_si256(t, m1);\n        v = _mm256_sub_epi32(v, t);\n        \n        t = _mm256_srli_epi32(v, 2);\n        t = _mm256_and_si256(t, m2);\n        v = _mm256_and_si256(v, m2);\n        v = _mm256_add_epi32(v, t);\n        \n        t = _mm256_srli_epi32(v, 4);\n        v = _mm256_add_epi32(v, t);\n        v = _mm256_and_si256(v, m3);\n        \n        v = _mm256_mullo_epi32(v, m4);\n        v = _mm256_srli_epi32(v, 24);\n        \n        __m256i res = _mm256_shuffle_epi8(v, shuffle_mask);\n        _mm_storel_epi64((__m128i*)(out + i), _mm256_castsi256_si128(res));\n    }\n\n    for (; i < total_elements; ++i) {\n        out[i] = (uint8_t)__builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    static const __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n    __m256i low_mask = _mm256_set1_epi8(0x0F);\n\n    for (; i + 7 < total; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i lo_nibble = _mm256_and_si256(v, low_mask);\n        __m256i hi_nibble = _mm256_and_si256(_mm256_srli_epi32(v, 4), low_mask);\n        __m256i cnt_lo = _mm256_shuffle_epi8(lookup, lo_nibble);\n        __m256i cnt_hi = _mm256_shuffle_epi8(lookup, hi_nibble);\n        __m256i cnt_per_byte = _mm256_add_epi8(cnt_lo, cnt_hi);\n        __m256i ones8 = _mm256_set1_epi8(1);\n        __m256i cnt_per_byte_16 = _mm256_maddubs_epi16(cnt_per_byte, ones8);\n        __m256i ones16 = _mm256_set1_epi16(1);\n        __m256i cnt_per_32 = _mm256_madd_epi16(cnt_per_byte_16, ones16);\n        __m128i lo = _mm256_castsi256_si128(cnt_per_32);\n        __m128i hi = _mm256_extracti128_si256(cnt_per_32, 1);\n        __m128i packed16 = _mm_packus_epi32(lo, hi);\n        __m128i packed8 = _mm_packus_epi16(packed16, _mm_setzero_si128());\n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n\n    for (; i < total; i++) {\n        out[i] = (uint8_t)_mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    __m256i mask55 = _mm256_set1_epi32(0x55555555);\n    __m256i mask33 = _mm256_set1_epi32(0x33333333);\n    __m256i mask0F = _mm256_set1_epi32(0x0F0F0F0F);\n    __m256i mask01 = _mm256_set1_epi32(0x01010101);\n    const __m128i pack_mask = _mm_set_epi8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 12, 8, 4, 0);\n\n    for (; i + 7 < total; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i t1 = _mm256_srli_epi32(vec, 1);\n        t1 = _mm256_and_si256(t1, mask55);\n        vec = _mm256_sub_epi32(vec, t1);\n        __m256i t2 = _mm256_and_si256(vec, mask33);\n        __m256i t3 = _mm256_srli_epi32(vec, 2);\n        t3 = _mm256_and_si256(t3, mask33);\n        vec = _mm256_add_epi32(t2, t3);\n        __m256i t4 = _mm256_add_epi32(vec, _mm256_srli_epi32(vec, 4));\n        vec = _mm256_and_si256(t4, mask0F);\n        vec = _mm256_mullo_epi32(vec, mask01);\n        vec = _mm256_srli_epi32(vec, 24);\n        __m128i low = _mm256_castsi256_si128(vec);\n        __m128i high = _mm256_extracti128_si256(vec, 1);\n        __m128i low4 = _mm_shuffle_epi8(low, pack_mask);\n        __m128i high4 = _mm_shuffle_epi8(high, pack_mask);\n        __m128i high_shift = _mm_slli_si128(high4, 4);\n        __m128i res = _mm_or_si128(low4, high_shift);\n        _mm_storel_epi64((__m128i*)(out + i), res);\n    }\n\n    for (; i < total; ++i) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        out[i] = (uint8_t)((x * 0x01010101) >> 24);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdlib.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    for (; i + 7 < total; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i abs_v = _mm256_abs_epi32(v);\n        int vals[8];\n        _mm256_storeu_si256((__m256i*)vals, v);\n        int mask_vals[8];\n        for (int j = 0; j < 8; j++) {\n            mask_vals[j] = (vals[j] % 3 == 0) ? -1 : 0;\n        }\n        __m256i mask = _mm256_loadu_si256((const __m256i*)mask_vals);\n        __m256i res = _mm256_blendv_epi8(v, abs_v, mask);\n        _mm256_storeu_si256((__m256i*)(B + i), res);\n    }\n    for (; i < total; i++) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        const int* a_row = A + r * cols;\n        int* b_row = B + r * cols;\n        size_t c = 0;\n        for (; c + 8 <= cols; c += 8) {\n            __m256i a_vec = _mm256_loadu_si256((const __m256i*)(a_row + c));\n            __m256i sign_mask = _mm256_srai_epi32(a_vec, 31);\n            __m256i a_abs = _mm256_xor_si256(a_vec, sign_mask);\n            a_abs = _mm256_sub_epi32(a_abs, sign_mask);\n            \n            __m256i v = a_abs;\n            __m256i mask16 = _mm256_set1_epi32(0xFFFF);\n            __m256i high16 = _mm256_srli_epi32(v, 16);\n            __m256i low16 = _mm256_and_si256(v, mask16);\n            v = _mm256_add_epi32(high16, low16);\n            \n            __m256i mask8 = _mm256_set1_epi32(0xFF);\n            __m256i high8 = _mm256_srli_epi32(v, 8);\n            __m256i low8 = _mm256_and_si256(v, mask8);\n            v = _mm256_add_epi32(high8, low8);\n            \n            __m256i mask4 = _mm256_set1_epi32(0xF);\n            __m256i high4 = _mm256_srli_epi32(v, 4);\n            __m256i low4 = _mm256_and_si256(v, mask4);\n            v = _mm256_add_epi32(high4, low4);\n            \n            __m256i mask3 = _mm256_set1_epi32(3);\n            for (int j = 0; j < 3; j++) {\n                __m256i high2 = _mm256_srli_epi32(v, 2);\n                __m256i low2 = _mm256_and_si256(v, mask3);\n                v = _mm256_add_epi32(high2, low2);\n            }\n            \n            __m256i cmp_ge3 = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(2));\n            v = _mm256_sub_epi32(v, _mm256_and_si256(cmp_ge3, mask3));\n            \n            __m256i zero = _mm256_setzero_si256();\n            __m256i mask_div3 = _mm256_cmpeq_epi32(v, zero);\n            __m256i result = _mm256_blendv_epi8(a_vec, a_abs, mask_div3);\n            _mm256_storeu_si256((__m256i*)(b_row + c), result);\n        }\n        \n        for (; c < cols; c++) {\n            int val = a_row[c];\n            if (val % 3 == 0) {\n                b_row[c] = (val < 0) ? -val : val;\n            } else {\n                b_row[c] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        \n        int values[8];\n        _mm256_storeu_si256((__m256i*)values, vec);\n        int mask_bits = 0;\n        for (int j = 0; j < 8; j++) {\n            if (values[j] % 3 == 0) {\n                mask_bits |= (1 << j);\n            }\n        }\n\n        __m256i mask = _mm256_set_epi32(\n            (mask_bits & (1 << 7)) ? -1 : 0,\n            (mask_bits & (1 << 6)) ? -1 : 0,\n            (mask_bits & (1 << 5)) ? -1 : 0,\n            (mask_bits & (1 << 4)) ? -1 : 0,\n            (mask_bits & (1 << 3)) ? -1 : 0,\n            (mask_bits & (1 << 2)) ? -1 : 0,\n            (mask_bits & (1 << 1)) ? -1 : 0,\n            (mask_bits & (1 << 0)) ? -1 : 0\n        );\n\n        __m256i result = _mm256_blendv_epi8(vec, abs_vec, mask);\n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n\n    for (; i < total_elements; i++) {\n        int a = A[i];\n        if (a % 3 == 0) {\n            B[i] = (a < 0) ? -a : a;\n        } else {\n            B[i] = a;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i v_abs = _mm256_abs_epi32(v);\n        __m256 v_absf = _mm256_cvtepi32_ps(v_abs);\n        __m256 div = _mm256_div_ps(v_absf, _mm256_set1_ps(3.0f));\n        __m256i quotient = _mm256_cvttps_epi32(div);\n        __m256i t1 = _mm256_mullo_epi32(quotient, _mm256_set1_epi32(3));\n        __m256i remainder = _mm256_sub_epi32(v_abs, t1);\n        __m256i mask = _mm256_cmpeq_epi32(remainder, _mm256_setzero_si256());\n        __m256i res = _mm256_blendv_epi8(v, v_abs, mask);\n        _mm256_storeu_si256((__m256i*)(B + i), res);\n    }\n\n    for (; i < total; i++) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = (val < 0) ? -val : val;\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    for (; i + 7 < total; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i abs_a = _mm256_abs_epi32(a);\n        int tmp[8];\n        _mm256_storeu_si256((__m256i*)tmp, a);\n        \n        __m256i mask = _mm256_set_epi32(\n            (tmp[7] % 3 == 0) ? -1 : 0,\n            (tmp[6] % 3 == 0) ? -1 : 0,\n            (tmp[5] % 3 == 0) ? -1 : 0,\n            (tmp[4] % 3 == 0) ? -1 : 0,\n            (tmp[3] % 3 == 0) ? -1 : 0,\n            (tmp[2] % 3 == 0) ? -1 : 0,\n            (tmp[1] % 3 == 0) ? -1 : 0,\n            (tmp[0] % 3 == 0) ? -1 : 0\n        );\n        \n        __m256i res = _mm256_blendv_epi8(a, abs_a, mask);\n        _mm256_storeu_si256((__m256i*)(B + i), res);\n    }\n\n    for (; i < total; i++) {\n        if (A[i] % 3 == 0) {\n            int val = A[i];\n            B[i] = (val < 0) ? -val : val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    if (length >= 8) {\n        const __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n        const __m256i low4_mask = _mm256_set1_epi32(0xF);\n        const __m256i compare_val = _mm256_set1_epi32(0xF);\n        const __m256i shift_right = _mm256_set1_epi32(rotate_bits);\n        const __m256i shift_left = _mm256_set1_epi32(32 - rotate_bits);\n        \n        for (; i <= length - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n            __m256i low4 = _mm256_and_si256(vec, low4_mask);\n            __m256i mask = _mm256_cmpeq_epi32(low4, compare_val);\n            __m256i negated = _mm256_xor_si256(vec, all_ones);\n            __m256i right_part = _mm256_srlv_epi32(vec, shift_right);\n            __m256i left_part = _mm256_sllv_epi32(vec, shift_left);\n            __m256i rotated = _mm256_or_si256(right_part, left_part);\n            __m256i res = _mm256_blendv_epi8(negated, rotated, mask);\n            _mm256_storeu_si256((__m256i*)(dst + i), res);\n        }\n    }\n    for (; i < length; i++) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    uint32_t n = rotate_bits & 0x1F;\n    uint32_t left_shift = (32 - n) & 0x1F;\n    __m256i low4_mask = _mm256_set1_epi32(0xF);\n    __m256i right_shift_vec = _mm256_set1_epi32(n);\n    __m256i left_shift_vec = _mm256_set1_epi32(left_shift);\n    __m256i all_ones = _mm256_set1_epi32(-1);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i x = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4_val = _mm256_and_si256(x, low4_mask);\n        __m256i condition = _mm256_cmpeq_epi32(low4_val, low4_mask);\n        \n        __m256i right_part = _mm256_srlv_epi32(x, right_shift_vec);\n        __m256i left_part = _mm256_sllv_epi32(x, left_shift_vec);\n        __m256i rotated = _mm256_or_si256(right_part, left_part);\n        \n        __m256i not_x = _mm256_xor_si256(x, all_ones);\n        __m256i result = _mm256_blendv_epi8(not_x, rotated, condition);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        if ((x & 0xF) == 0xF) {\n            dst[i] = (x >> n) | (x << left_shift);\n        } else {\n            dst[i] = ~x;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    uint8_t actual_rotate = rotate_bits & 0x1F;\n    uint8_t inv_rotate = (32 - actual_rotate) & 0x1F;\n    size_t i = 0;\n\n    if (length >= 8) {\n        __m256i v_rot_mask = _mm256_set1_epi32(0xF);\n        __m256i v_ones = _mm256_set1_epi32(-1);\n        \n        for (; i <= length - 8; i += 8) {\n            __m256i v_src = _mm256_loadu_si256((__m256i const*)(src + i));\n            __m256i v_low4 = _mm256_and_si256(v_src, v_rot_mask);\n            __m256i v_cond = _mm256_cmpeq_epi32(v_low4, v_rot_mask);\n            \n            __m256i v_rot_right = _mm256_srli_epi32(v_src, actual_rotate);\n            __m256i v_rot_left = _mm256_slli_epi32(v_src, inv_rotate);\n            __m256i v_rot = _mm256_or_si256(v_rot_right, v_rot_left);\n            \n            __m256i v_not = _mm256_xor_si256(v_src, v_ones);\n            __m256i v_result = _mm256_blendv_epi8(v_not, v_rot, v_cond);\n            \n            _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n        }\n    }\n\n    for (; i < length; i++) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> actual_rotate) | (val << inv_rotate);\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    uint32_t r = rotate_bits & 0x1F;\n    size_t i = 0;\n    const size_t vec_len = length / 8 * 8;\n\n    for (; i < vec_len; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i low4 = _mm256_and_si256(vec, _mm256_set1_epi32(0xF));\n        __m256i mask = _mm256_cmpeq_epi32(low4, _mm256_set1_epi32(0xF));\n        \n        __m256i shift_right = _mm256_set1_epi32(r);\n        __m256i shift_left = _mm256_set1_epi32(32 - r);\n        __m256i right_part = _mm256_srlv_epi32(vec, shift_right);\n        __m256i left_part = _mm256_sllv_epi32(vec, shift_left);\n        __m256i rotated = _mm256_or_si256(right_part, left_part);\n        \n        __m256i not_x = _mm256_xor_si256(vec, _mm256_set1_epi32(-1));\n        __m256i res = _mm256_blendv_epi8(not_x, rotated, mask);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        if ((x & 0xF) == 0xF) {\n            dst[i] = (x >> r) | (x << (32 - r));\n        } else {\n            dst[i] = ~x;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i v_rot = _mm256_set1_epi32(rotate_bits);\n    __m256i v_rot_left = _mm256_set1_epi32(32 - rotate_bits);\n    __m256i v_low4_mask = _mm256_set1_epi32(0xF);\n    __m256i v_ones = _mm256_set1_epi32(-1);\n    \n    for (; i + 7 < length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i low4 = _mm256_and_si256(v_src, v_low4_mask);\n        __m256i mask = _mm256_cmpeq_epi32(low4, v_low4_mask);\n        \n        __m256i right_shift = _mm256_srlv_epi32(v_src, v_rot);\n        __m256i left_shift = _mm256_sllv_epi32(v_src, v_rot_left);\n        __m256i rotated = _mm256_or_si256(right_shift, left_shift);\n        \n        __m256i negated = _mm256_xor_si256(v_src, v_ones);\n        \n        __m256i result = _mm256_blendv_epi8(negated, rotated, mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        if ((src[i] & 0xF) == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_epsilon = fabsf(epsilon);\n    __m256 v_abs_eps = _mm256_set1_ps(abs_epsilon);\n    __m256 v_neg_abs_eps = _mm256_set1_ps(-abs_epsilon);\n    __m256i ones_32 = _mm256_set1_epi32(1);\n    __m256i neg_ones_32 = _mm256_set1_epi32(-1);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 mask_positive = _mm256_cmp_ps(diff, v_abs_eps, _CMP_GT_OS);\n        __m256 mask_negative = _mm256_cmp_ps(diff, v_neg_abs_eps, _CMP_LT_OS);\n        \n        __m256i step_positive = _mm256_and_si256(_mm256_castps_si256(mask_positive), ones_32);\n        __m256i step_negative = _mm256_and_si256(_mm256_castps_si256(mask_negative), neg_ones_32);\n        __m256i result_int32 = _mm256_or_si256(step_positive, step_negative);\n        \n        __m128i low128 = _mm256_castsi256_si128(result_int32);\n        __m128i high128 = _mm256_extracti128_si256(result_int32, 1);\n        __m128i packed16 = _mm_packs_epi32(low128, high128);\n        __m128i packed8 = _mm_packs_epi16(packed16, _mm_setzero_si128());\n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > abs_epsilon) {\n            out[i] = 1;\n        } else if (diff < -abs_epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_eps = epsilon;\n    if (abs_eps < 0) {\n        abs_eps = -abs_eps;\n    }\n    float neg_abs_eps = -abs_eps;\n    \n    __m256 abs_eps_vec = _mm256_set1_ps(abs_eps);\n    __m256 neg_abs_eps_vec = _mm256_set1_ps(neg_abs_eps);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(A + i);\n        __m256 b_val = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a_val, b_val);\n        \n        __m256 gt_mask = _mm256_cmp_ps(diff, abs_eps_vec, _CMP_GT_OS);\n        __m256 lt_mask = _mm256_cmp_ps(diff, neg_abs_eps_vec, _CMP_LT_OS);\n        \n        __m256i ones_i32 = _mm256_set1_epi32(1);\n        __m256i neg_ones_i32 = _mm256_set1_epi32(-1);\n        __m256i part1 = _mm256_and_si256(_mm256_castps_si256(gt_mask), ones_i32);\n        __m256i part2 = _mm256_and_si256(_mm256_castps_si256(lt_mask), neg_ones_i32);\n        __m256i res_i32 = _mm256_or_si256(part1, part2);\n        \n        __m128i low_128 = _mm256_castsi256_si128(res_i32);\n        __m128i high_128 = _mm256_extracti128_si256(res_i32, 1);\n        __m128i packed16 = _mm_packs_epi32(low_128, high_128);\n        __m128i packed8 = _mm_packs_epi16(packed16, _mm_setzero_si128());\n        \n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > abs_eps) {\n            out[i] = 1;\n        } else if (diff < neg_abs_eps) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_epsilon = (epsilon < 0) ? -epsilon : epsilon;\n    __m256 vabs_eps = _mm256_set1_ps(abs_epsilon);\n    __m256 vneg_abs_eps = _mm256_set1_ps(-abs_epsilon);\n    __m256i vones = _mm256_set1_epi32(1);\n    __m256i vminus_ones = _mm256_set1_epi32(-1);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        \n        __m256 mask_gt = _mm256_cmp_ps(diff, vabs_eps, _CMP_GT_OS);\n        __m256 mask_lt = _mm256_cmp_ps(diff, vneg_abs_eps, _CMP_LT_OS);\n        \n        __m256i res32 = _mm256_setzero_si256();\n        res32 = _mm256_blendv_epi8(res32, vones, _mm256_castps_si256(mask_gt));\n        res32 = _mm256_blendv_epi8(res32, vminus_ones, _mm256_castps_si256(mask_lt));\n        \n        __m256i packed16 = _mm256_packs_epi32(res32, _mm256_setzero_si256());\n        __m256i packed8 = _mm256_packs_epi16(packed16, _mm256_setzero_si256());\n        __m128i result = _mm256_castsi256_si128(packed8);\n        _mm_storel_epi64((__m128i*)(out + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > abs_epsilon) {\n            out[i] = 1;\n        } else if (diff < -abs_epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_epsilon = epsilon < 0 ? -epsilon : epsilon;\n    __m256 v_eps = _mm256_set1_ps(abs_epsilon);\n    __m256 v_neg_eps = _mm256_set1_ps(-abs_epsilon);\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a_vec, b_vec);\n        \n        __m256 mask_gt = _mm256_cmp_ps(diff, v_eps, _CMP_GT_OQ);\n        __m256 mask_lt = _mm256_cmp_ps(diff, v_neg_eps, _CMP_LT_OQ);\n        \n        __m256i i_mask_gt = _mm256_castps_si256(mask_gt);\n        __m256i i_mask_lt = _mm256_castps_si256(mask_lt);\n        \n        __m256i gt_shifted = _mm256_srli_epi32(i_mask_gt, 31);\n        __m256i lt_shifted = _mm256_srli_epi32(i_mask_lt, 31);\n        __m256i res32 = _mm256_sub_epi32(gt_shifted, lt_shifted);\n        \n        __m128i low32 = _mm256_castsi256_si128(res32);\n        __m128i high32 = _mm256_extracti128_si256(res32, 1);\n        __m128i packed16 = _mm_packs_epi32(low32, high32);\n        __m128i packed8 = _mm_packs_epi16(packed16, _mm_setzero_si128());\n        \n        _mm_storel_epi64((__m128i*)(out + i), packed8);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > abs_epsilon) {\n            out[i] = 1;\n        } else if (diff < -abs_epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_eps = fabsf(epsilon);\n    __m256 eps_vec = _mm256_set1_ps(abs_eps);\n    __m256 neg_eps_vec = _mm256_set1_ps(-abs_eps);\n    const __m256i shuffle_mask = _mm256_set_epi8(\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n        28, 24, 20, 16, 12, 8, 4, 0\n    );\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a_vec, b_vec);\n        \n        __m256 mask_pos = _mm256_cmp_ps(diff, eps_vec, _CMP_GT_OQ);\n        __m256 mask_neg = _mm256_cmp_ps(diff, neg_eps_vec, _CMP_LT_OQ);\n        \n        __m256i mask_pos_int = _mm256_castps_si256(mask_pos);\n        __m256i mask_neg_int = _mm256_castps_si256(mask_neg);\n        \n        __m256i pos_part = _mm256_and_si256(mask_pos_int, ones);\n        __m256i neg_part = _mm256_and_si256(mask_neg_int, neg_ones);\n        __m256i result_int32 = _mm256_or_si256(pos_part, neg_part);\n        \n        __m256i packed_result = _mm256_shuffle_epi8(result_int32, shuffle_mask);\n        __m128i low_packed = _mm256_castsi256_si128(packed_result);\n        _mm_storel_epi64((__m128i*)(out + i), low_packed);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > abs_eps) {\n            out[i] = 1;\n        } else if (diff < -abs_eps) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n    const __m256i sat_max_vec = _mm256_set1_epi32(INT32_MAX);\n    const __m256i sat_min_vec = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 v = _mm256_loadu_ps(A + i);\n        __m256 cmp_ge = _mm256_cmp_ps(v, max_val, _CMP_GE_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(v, min_val, _CMP_LT_OS);\n        __m256i vi = _mm256_cvttps_epi32(v);\n        __m256i mask_ge = _mm256_castps_si256(cmp_ge);\n        __m256i mask_lt = _mm256_castps_si256(cmp_lt);\n        vi = _mm256_blendv_epi8(vi, sat_max_vec, mask_ge);\n        vi = _mm256_blendv_epi8(vi, sat_min_vec, mask_lt);\n        _mm256_storeu_si256((__m256i*)(B + i), vi);\n    }\n\n    for (; i < size; i++) {\n        float a_val = A[i];\n        if (a_val >= (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (a_val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)a_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const float float_max_threshold = (float)INT32_MAX + 1.0f;\n    const float float_min_threshold = (float)INT32_MIN;\n    const __m256 max_float = _mm256_set1_ps(float_max_threshold);\n    const __m256 min_float = _mm256_set1_ps(float_min_threshold);\n    const __m256i vmax = _mm256_set1_epi32(INT32_MAX);\n    const __m256i vmin = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(A + i);\n        __m256i b_val = _mm256_cvtps_epi32(a_val);\n\n        __m256 mask_pos = _mm256_cmp_ps(a_val, max_float, _CMP_GE_OS);\n        __m256 mask_neg = _mm256_cmp_ps(a_val, min_float, _CMP_LT_OS);\n        __m256i mask_pos_i = _mm256_castps_si256(mask_pos);\n        __m256i mask_neg_i = _mm256_castps_si256(mask_neg);\n\n        b_val = _mm256_blendv_epi8(b_val, vmax, mask_pos_i);\n        b_val = _mm256_blendv_epi8(b_val, vmin, mask_neg_i);\n\n        _mm256_storeu_si256((__m256i*)(B + i), b_val);\n    }\n\n    for (; i < size; i++) {\n        float a_val = A[i];\n        if (a_val >= float_max_threshold) {\n            B[i] = INT32_MAX;\n        } else if (a_val < float_min_threshold) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)a_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 v_max = _mm256_set1_ps(2147483647.0f);\n    const __m256 v_min = _mm256_set1_ps(-2147483648.0f);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(A + i);\n        a_val = _mm256_min_ps(a_val, v_max);\n        a_val = _mm256_max_ps(a_val, v_min);\n        __m256i b_val = _mm256_cvtps_epi32(a_val);\n        _mm256_storeu_si256((__m256i*)(B + i), b_val);\n    }\n    \n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > 2147483647.0f) {\n            B[i] = 2147483647;\n        } else if (val < -2147483648.0f) {\n            B[i] = -2147483648;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const size_t vector_size = 8;\n    size_t num_vectors = size / vector_size;\n    \n    for (; i < num_vectors * vector_size; i += vector_size) {\n        __m256 x = _mm256_loadu_ps(A + i);\n        __m256i v = _mm256_cvtps_epi32(x);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask_positive = _mm256_cmp_ps(x, zero, _CMP_GT_OS);\n        __m256i mask_overflow = _mm256_cmpeq_epi32(v, _mm256_set1_epi32(0x80000000));\n        __m256i mask_pos_overflow = _mm256_and_si256(mask_overflow, _mm256_castps_si256(mask_positive));\n        v = _mm256_blendv_epi8(v, _mm256_set1_epi32(0x7FFFFFFF), mask_pos_overflow);\n        _mm256_storeu_si256((__m256i*)(B + i), v);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > 2147483647.0f) {\n            B[i] = 0x7FFFFFFF;\n        } else if (val < -2147483648.0f) {\n            B[i] = 0x80000000;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const float float_upper = 2147483648.0f;\n    const float float_lower = -2147483648.0f;\n    const __m256 v_float_upper = _mm256_set1_ps(float_upper);\n    const __m256 v_float_lower = _mm256_set1_ps(float_lower);\n    const __m256i v_int32_max = _mm256_set1_epi32(INT32_MAX);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 v = _mm256_loadu_ps(A + i);\n        v = _mm256_min_ps(v, v_float_upper);\n        v = _mm256_max_ps(v, v_float_lower);\n        __m256i vi = _mm256_cvttps_epi32(v);\n        __m256 mask = _mm256_cmp_ps(v, v_float_upper, _CMP_EQ_OS);\n        __m256i res = _mm256_blendv_epi8(vi, v_int32_max, _mm256_castps_si256(mask));\n        _mm256_storeu_si256((__m256i*)(B + i), res);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val >= float_upper) {\n            B[i] = INT32_MAX;\n        } else if (val < float_lower) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float factor = __builtin_exp2f(-scale);\n    __m256 factor_vec = _mm256_set1_ps(factor);\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 b_vec = _mm256_cvtepi32_ps(a_vec);\n        b_vec = _mm256_mul_ps(b_vec, factor_vec);\n        _mm256_storeu_ps(B + i, b_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i]) * factor;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_factor = (float)scale;\n    __m256 scale_vec = _mm256_set1_ps(scale_factor);\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale_factor;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_factor = (float)scale;\n    __m256 v_scale = _mm256_set1_ps(scale_factor);\n    size_t i = 0;\n    \n    for (; i + 8 <= size; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 v_float = _mm256_cvtepi32_ps(v_int);\n        __m256 v_scaled = _mm256_mul_ps(v_float, v_scale);\n        _mm256_storeu_ps(B + i, v_scaled);\n    }\n\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * scale_factor;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_f = static_cast<float>(scale);\n    __m256 v_scale = _mm256_set1_ps(scale_f);\n    size_t i = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, v_scale);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale_f;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_f = (float)scale;\n    __m256 v_scale = _mm256_set1_ps(scale_f);\n    size_t i = 0;\n    \n    for (; i + 7 < size; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 v_float = _mm256_cvtepi32_ps(v_int);\n        __m256 v_scaled = _mm256_mul_ps(v_float, v_scale);\n        _mm256_storeu_ps(B + i, v_scaled);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (float)A[i] * scale_f;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_val = _mm256_cvttps_epi32(rounded);\n        _mm256_storeu_si256((__m256i*)(C + i), int_val);\n    }\n    for (; i < size; i++) {\n        __m128 a_val = _mm_load_ss(A + i);\n        __m128 b_val = _mm_load_ss(B + i);\n        __m128 div = _mm_div_ss(a_val, b_val);\n        __m128 rounded = _mm_round_ss(div, div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        int32_t n = _mm_cvttss_si32(rounded);\n        C[i] = n;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i integer = _mm256_cvtps_epi32(rounded);\n        _mm256_storeu_si256((__m256i*)(C + i), integer);\n    }\n\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div >= 0 ? div + 0.5f : div - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    const size_t vec_size = size - 7;\n\n    for (; i < vec_size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n        __m256i round_vec = _mm256_cvtps_epi32(div_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), round_vec);\n    }\n\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div >= 0 ? div + 0.5f : div - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    const size_t vec_floats = 8;\n    const size_t limit = size - vec_floats + 1;\n    \n    for (; i < limit; i += vec_floats) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i epi32 = _mm256_cvtps_epi32(rounded);\n        _mm256_storeu_si256((__m256i*)(C + i), epi32);\n    }\n    \n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        int32_t rounded;\n        if (div >= 0) {\n            rounded = (int32_t)(div + 0.5f);\n        } else {\n            rounded = (int32_t)(div - 0.5f);\n        }\n        C[i] = rounded;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 div = _mm256_div_ps(a_vec, b_vec);\n        __m256i int_vec = _mm256_cvtps_epi32(div);\n        _mm256_storeu_si256((__m256i*)(C + i), int_vec);\n    }\n    for (; i < size; i++) {\n        __m128 a = _mm_load_ss(A + i);\n        __m128 b = _mm_load_ss(B + i);\n        __m128 div = _mm_div_ss(a, b);\n        __m128i div_int = _mm_cvtps_epi32(div);\n        C[i] = _mm_cvtsi128_si32(div_int);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const double flt_max = (double)FLT_MAX;\n    const double neg_flt_max = -flt_max;\n    const __m256d flt_max_vec = _mm256_set1_pd(flt_max);\n    const __m256d neg_flt_max_vec = _mm256_set1_pd(neg_flt_max);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a_vec = _mm256_loadu_pd(A + i);\n        a_vec = _mm256_min_pd(a_vec, flt_max_vec);\n        a_vec = _mm256_max_pd(a_vec, neg_flt_max_vec);\n        __m128 f_vec = _mm256_cvtpd_ps(a_vec);\n        _mm_storeu_ps(B + i, f_vec);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > flt_max) \n            val = flt_max;\n        else if (val < neg_flt_max) \n            val = neg_flt_max;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d flt_max = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d neg_flt_max = _mm256_set1_pd((double)-FLT_MAX);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a_vec = _mm256_loadu_pd(A + i);\n        a_vec = _mm256_max_pd(a_vec, neg_flt_max);\n        a_vec = _mm256_min_pd(a_vec, flt_max);\n        __m128 f_vec = _mm256_cvtpd_ps(a_vec);\n        _mm_storeu_ps(B + i, f_vec);\n    }\n    \n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > (double)FLT_MAX) \n            B[i] = FLT_MAX;\n        else if (val < (double)-FLT_MAX)\n            B[i] = -FLT_MAX;\n        else\n            B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_double = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d min_double = _mm256_set1_pd((double)-FLT_MAX);\n    size_t i = 0;\n    \n    for (; i + 3 < size; i += 4) {\n        __m256d a_val = _mm256_loadu_pd(A + i);\n        a_val = _mm256_min_pd(a_val, max_double);\n        a_val = _mm256_max_pd(a_val, min_double);\n        __m128 f_val = _mm256_cvtpd_ps(a_val);\n        _mm_storeu_ps(B + i, f_val);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > (double)FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < (double)-FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d d_flt_max = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d d_minus_flt_max = _mm256_set1_pd((double)-FLT_MAX);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d src = _mm256_loadu_pd(A + i);\n        __m256d gt_mask = _mm256_cmp_pd(src, d_flt_max, _CMP_GT_OS);\n        __m256d lt_mask = _mm256_cmp_pd(src, d_minus_flt_max, _CMP_LT_OS);\n        __m256d clamped = _mm256_blendv_pd(src, d_flt_max, gt_mask);\n        clamped = _mm256_blendv_pd(clamped, d_minus_flt_max, lt_mask);\n        __m128 converted = _mm256_cvtpd_ps(clamped);\n        _mm_storeu_ps(B + i, converted);\n    }\n    \n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > (double)FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < (double)-FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d flt_max = _mm256_set1_pd(FLT_MAX);\n    const __m256d neg_flt_max = _mm256_set1_pd(-FLT_MAX);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d dbl_vec = _mm256_loadu_pd(A + i);\n        dbl_vec = _mm256_min_pd(dbl_vec, flt_max);\n        dbl_vec = _mm256_max_pd(dbl_vec, neg_flt_max);\n        __m128 float_vec = _mm256_cvtpd_ps(dbl_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) \n            B[i] = FLT_MAX;\n        else if (val < -FLT_MAX) \n            B[i] = -FLT_MAX;\n        else \n            B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 15 < size; i += 16) {\n        __m256i in = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m128i low_128 = _mm256_castsi256_si128(in);\n        __m128i high_128 = _mm256_extractf128_si256(in, 1);\n        __m256i lo = _mm256_cvtepi16_epi32(low_128);\n        __m256i hi = _mm256_cvtepi16_epi32(high_128);\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elements = 16;\n    \n    for (; i + simd_elements <= size; i += simd_elements) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(a_vec));\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a_vec, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const size_t vector_size = 16;\n    \n    for (; i + vector_size - 1 < size; i += vector_size) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec));\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n\n    for (; i < size; i++) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    for (; i + 15 < size; i += 16) {\n        __m128i chunk_low = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i chunk_high = _mm_loadu_si128((const __m128i*)(A + i + 8));\n        __m256i extended_low = _mm256_cvtepi16_epi32(chunk_low);\n        __m256i extended_high = _mm256_cvtepi16_epi32(chunk_high);\n        _mm256_storeu_si256((__m256i*)(B + i), extended_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), extended_high);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 15 < size; i += 16) {\n        __m128i chunk0 = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i chunk1 = _mm_loadu_si128((const __m128i*)(A + i + 8));\n        __m256i extended0 = _mm256_cvtepi16_epi32(chunk0);\n        __m256i extended1 = _mm256_cvtepi16_epi32(chunk1);\n        _mm256_storeu_si256((__m256i*)(B + i), extended0);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), extended1);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float denom = max_val - min_val;\n    float denom_rcp = (denom == 0.0f) ? 0.0f : (1.0f / denom);\n    __m256 v_min = _mm256_set1_ps(min_val);\n    __m256 v_denom_rcp = _mm256_set1_ps(denom_rcp);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n    __m256i v_zero_epi32 = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 v_a = _mm256_loadu_ps(A + i);\n        __m256i v_control = _mm256_loadu_si256((const __m256i*)(control + i));\n        __m256i v_gt0 = _mm256_cmpgt_epi32(v_control, v_zero_epi32);\n        __m256 v_normalized = _mm256_sub_ps(v_a, v_min);\n        v_normalized = _mm256_mul_ps(v_normalized, v_denom_rcp);\n        v_normalized = _mm256_max_ps(v_normalized, v_zero);\n        v_normalized = _mm256_min_ps(v_normalized, v_one);\n        __m256 v_result = _mm256_blendv_ps(v_a, v_normalized, _mm256_castsi256_ps(v_gt0));\n        _mm256_storeu_ps(B + i, v_result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) * denom_rcp;\n            val = (val < 0.0f) ? 0.0f : val;\n            val = (val > 1.0f) ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256i zero_int = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl_val = _mm256_loadu_si256((const __m256i*)&control[i]);\n        __m256i mask_int = _mm256_cmpgt_epi32(ctrl_val, zero_int);\n        __m256 mask_float = _mm256_castsi256_ps(mask_int);\n\n        __m256 normalized = _mm256_sub_ps(a_val, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 res = _mm256_blendv_ps(a_val, normalized, mask_float);\n        _mm256_storeu_ps(&B[i], res);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    if (size == 0) return;\n    \n    const float range = max_val - min_val;\n    const float inv_range = 1.0f / range;\n    \n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 inv_range_vec = _mm256_set1_ps(inv_range);\n    __m256 zero_vec = _mm256_set1_ps(0.0f);\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256i zero_int = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl = _mm256_loadu_si256((__m256i const*)&control[i]);\n        \n        __m256i mask_int = _mm256_cmpgt_epi32(ctrl, zero_int);\n        __m256 mask = _mm256_castsi256_ps(mask_int);\n        \n        __m256 normalized = _mm256_sub_ps(a_val, min_vec);\n        normalized = _mm256_mul_ps(normalized, inv_range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n        \n        __m256 result = _mm256_blendv_ps(a_val, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) * inv_range;\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float range = max_val - min_val;\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 range_vec = _mm256_set1_ps(range);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256i zero_int = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256i ctrl_vec = _mm256_loadu_si256((const __m256i*)(control + i));\n        __m256i mask = _mm256_cmpgt_epi32(ctrl_vec, zero_int);\n\n        __m256 normalized = _mm256_sub_ps(a_vec, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, _mm256_castsi256_ps(mask));\n        _mm256_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / range;\n            if (val < 0.0f) val = 0.0f;\n            else if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float denom = max_val - min_val;\n    if (denom == 0.0f) {\n        denom = 1.0f;\n    }\n    \n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 denom_vec = _mm256_set1_ps(denom);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256i zero_int = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_val = _mm256_loadu_ps(A + i);\n        __m256i c_val = _mm256_loadu_si256((const __m256i*)(control + i));\n        \n        __m256i mask = _mm256_cmpgt_epi32(c_val, zero_int);\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_val, min_vec), denom_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n        \n        __m256 res = _mm256_blendv_ps(a_val, normalized, _mm256_castsi256_ps(mask));\n        _mm256_storeu_ps(B + i, res);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / denom;\n            val = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load 8 uint32_t values\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        // Shift right by 16 bits to isolate high 16 bits\n        a_vec = _mm256_srli_epi32(a_vec, 16);\n        // Convert integers to floats\n        __m256 b_vec = _mm256_cvtepi32_ps(a_vec);\n        // Compute square root of floats\n        b_vec = _mm256_sqrt_ps(b_vec);\n        // Store results\n        _mm256_storeu_ps(B + i, b_vec);\n    }\n    // Process remaining elements using SSE\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        // Convert to float and compute sqrt using SSE intrinsics\n        __m128 x = _mm_cvtsi32_ss(_mm_setzero_ps(), high16);\n        x = _mm_sqrt_ss(x);\n        _mm_store_ss(B + i, x);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i uint_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(uint_vec, 16);\n        __m256 float_vec = _mm256_cvtepu32_ps(shifted);\n        __m256 sqrt_vec = _mm256_sqrt_ps(float_vec);\n        _mm256_storeu_ps(B + i, sqrt_vec);\n    }\n    \n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float val = (float)high_bits;\n        B[i] = sqrtf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t vectorizable_elements = size - (size % 8);\n    \n    for (; i < vectorizable_elements; i += 8) {\n        __m256i uint_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i hi_bits = _mm256_srli_epi32(uint_vec, 16);\n        __m256 float_vec = _mm256_cvtepi32_ps(hi_bits);\n        __m256 sqrt_vec = _mm256_sqrt_ps(float_vec);\n        _mm256_storeu_ps(B + i, sqrt_vec);\n    }\n    \n    for (; i < size; ++i) {\n        uint32_t hi = A[i] >> 16;\n        B[i] = _mm_cvtss_f32(_mm_sqrt_ss(_mm_set_ss((float)hi)));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX processes 8 floats per vector\n\n    for (; i + vec_size <= size; i += vec_size) {\n        __m256i uint_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(uint_vec, 16);\n        __m256 float_vec = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_vec = _mm256_sqrt_ps(float_vec);\n        _mm256_storeu_ps(B + i, sqrt_vec);\n    }\n\n    for (; i < size; i++) {\n        uint32_t high_bits = A[i] >> 16;\n        B[i] = sqrtf((float)high_bits);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i v_uint = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i v_high16 = _mm256_srli_epi32(v_uint, 16);\n        __m256 v_float = _mm256_cvtepi32_ps(v_high16);\n        __m256 v_sqrt = _mm256_sqrt_ps(v_float);\n        _mm256_storeu_ps(B + i, v_sqrt);\n    }\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t vec_elements = 8;\n    size_t vec_end = size - (size % vec_elements);\n\n    for (; i < vec_end; i += vec_elements) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        *(B + i) = *(const float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t num_chunks = size / 8;\n    \n    for (; i < num_chunks * 8; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; i++) {\n        *((uint32_t*)&B[i]) = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t num_avx_elements = 8;\n    size_t avx_iters = size / num_avx_elements;\n\n    for (; i < avx_iters * num_avx_elements; i += num_avx_elements) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        *(B + i) = *(const float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t vec_elements = 8;\n    size_t end = size / vec_elements * vec_elements;\n\n    for (; i < end; i += vec_elements) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i];\n        float* float_ptr = (float*)&val;\n        B[i] = *float_ptr;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    \n    // Process 8 elements at a time using AVX\n    for (; i + simd_elements <= size; i += simd_elements) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    \n    // Process remaining elements individually\n    for (; i < size; i++) {\n        union {\n            uint32_t u;\n            float f;\n        } converter;\n        converter.u = A[i];\n        B[i] = converter.f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) {\n        return;\n    }\n\n    size_t i = 1;\n    for (; i <= size - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i diff_i = _mm256_sub_epi32(current, previous);\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        __m128i cond_vec = _mm_loadl_epi64((const __m128i*)(cond + i));\n        cond_vec = _mm_cmpgt_epi8(cond_vec, _mm_setzero_si128());\n\n        __m128i cond_low = _mm_cvtepi8_epi32(cond_vec);\n        __m128i cond_high = _mm_cvtepi8_epi32(_mm_srli_si128(cond_vec, 4));\n\n        __m256i cond_mask32 = _mm256_setzero_si256();\n        cond_mask32 = _mm256_insertf128_si256(cond_mask32, cond_low, 0);\n        cond_mask32 = _mm256_insertf128_si256(cond_mask32, cond_high, 1);\n\n        __m256 mask = _mm256_castsi256_ps(cond_mask32);\n        __m256 res = _mm256_and_ps(diff_f, mask);\n\n        _mm256_storeu_ps(diff + i, res);\n    }\n\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) {\n        return;\n    }\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m128i cond8 = _mm_loadl_epi64((const __m128i*)(cond + i));\n        __m256i cond32 = _mm256_cvtepu8_epi32(cond8);\n        __m256i mask = _mm256_cmpgt_epi32(cond32, _mm256_setzero_si256());\n        \n        __m256i current = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i prev = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i diff_i32 = _mm256_sub_epi32(current, prev);\n        \n        __m256 diff_f32 = _mm256_cvtepi32_ps(diff_i32);\n        __m256 blended = _mm256_blendv_ps(_mm256_setzero_ps(), diff_f32, _mm256_castsi256_ps(mask));\n        \n        _mm256_storeu_ps(diff + i, blended);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) {\n        return;\n    }\n\n    size_t i = 1;\n    for (; i <= size - 8; i += 8) {\n        __m128i cond8 = _mm_loadl_epi64((const __m128i*)(cond + i));\n        __m128i cond_low = cond8;\n        __m128i cond_high = _mm_srli_si128(cond8, 4);\n        __m128i cond_low_32 = _mm_cvtepu8_epi32(cond_low);\n        __m128i cond_high_32 = _mm_cvtepu8_epi32(cond_high);\n        __m256i cond_mask_32 = _mm256_set_m128i(cond_high_32, cond_low_32);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i cond_cmp = _mm256_cmpeq_epi32(cond_mask_32, ones);\n        \n        __m256i A_prev = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i A_curr = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i diff_i32 = _mm256_sub_epi32(A_curr, A_prev);\n        __m256 vdiff = _mm256_cvtepi32_ps(diff_i32);\n        __m256 res = _mm256_and_ps(vdiff, _mm256_castsi256_ps(cond_cmp));\n        \n        _mm256_storeu_ps(diff + i, res);\n    }\n\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) {\n        return;\n    }\n\n    size_t i = 1;\n    if (size >= 9) {\n        for (; i <= size - 8; i += 8) {\n            __m128i cond_vec = _mm_loadl_epi64((const __m128i*)(cond + i));\n            __m256i cond_mask = _mm256_cvtepu8_epi32(cond_vec);\n            __m256i cond_cmp = _mm256_cmpgt_epi32(cond_mask, _mm256_setzero_si256());\n            \n            __m256i current_A = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i prev_A = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n            __m256i diff_i = _mm256_sub_epi32(current_A, prev_A);\n            \n            __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n            __m256 res = _mm256_blendv_ps(_mm256_setzero_ps(), diff_f, _mm256_castsi256_ps(cond_cmp));\n            _mm256_storeu_ps(diff + i, res);\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i <= size - 8; i += 8) {\n        __m128i cond8 = _mm_loadl_epi64((const __m128i*)(cond + i));\n        __m256i cond_mask = _mm256_cvtepu8_epi32(cond8);\n        cond_mask = _mm256_cmpgt_epi32(cond_mask, _mm256_setzero_si256());\n\n        __m256i current = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i diff_i = _mm256_sub_epi32(current, previous);\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n        __m256 zeros = _mm256_setzero_ps();\n        __m256 res = _mm256_blendv_ps(zeros, diff_f, _mm256_castsi256_ps(cond_mask));\n        _mm256_storeu_ps(diff + i, res);\n    }\n\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t unroll = 8;\n    \n    if (length >= unroll) {\n        __m256i high32_set = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n        \n        for (; i <= length - unroll; i += unroll) {\n            __m128i in0 = _mm_loadu_si128((__m128i const*)(src + i));\n            __m128i in1 = _mm_loadu_si128((__m128i const*)(src + i + 4));\n            \n            __m256i out0 = _mm256_cvtepu32_epi64(in0);\n            __m256i out1 = _mm256_cvtepu32_epi64(in1);\n            \n            out0 = _mm256_or_si256(out0, high32_set);\n            out1 = _mm256_or_si256(out1, high32_set);\n            \n            _mm256_storeu_si256((__m256i*)(dst + i), out0);\n            _mm256_storeu_si256((__m256i*)(dst + i + 4), out1);\n        }\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | (uint64_t)src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i vec32 = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m128i low128 = _mm256_castsi256_si128(vec32);\n        __m128i high128 = _mm256_extracti128_si256(vec32, 1);\n        \n        __m256i low64 = _mm256_cvtepu32_epi64(low128);\n        __m256i high64 = _mm256_cvtepu32_epi64(high128);\n        \n        low64 = _mm256_or_si256(low64, mask);\n        high64 = _mm256_or_si256(high64, mask);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), low64);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high64);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i v32 = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m128i low128 = _mm256_castsi256_si128(v32);\n        __m128i high128 = _mm256_extracti128_si256(v32, 1);\n        \n        __m256i low64 = _mm256_cvtepu32_epi64(low128);\n        __m256i high64 = _mm256_cvtepu32_epi64(high128);\n        \n        low64 = _mm256_or_si256(low64, mask);\n        high64 = _mm256_or_si256(high64, mask);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), low64);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high64);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    // Create mask with high 32 bits set (0xFFFFFFFF00000000)\n    const __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n    \n    size_t i = 0;\n    // Process 8 elements per iteration\n    for (; i + 7 < length; i += 8) {\n        // Load 8 uint32_t elements\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        // Extract low and high 128-bit lanes\n        __m128i low128 = _mm256_castsi256_si128(chunk);\n        __m128i high128 = _mm256_extracti128_si256(chunk, 1);\n        \n        // Zero-extend lanes to 256-bit (4x64 each)\n        __m256i extended_low = _mm256_cvtepu32_epi64(low128);\n        __m256i extended_high = _mm256_cvtepu32_epi64(high128);\n        \n        // Apply mask to set high 32 bits\n        extended_low = _mm256_or_si256(extended_low, mask);\n        extended_high = _mm256_or_si256(extended_high, mask);\n        \n        // Store results\n        _mm256_storeu_si256((__m256i*)(dst + i), extended_low);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), extended_high);\n    }\n    \n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    \n    for (; i + simd_elements <= length; i += simd_elements) {\n        __m256i vec32 = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m128i low128 = _mm256_castsi256_si128(vec32);\n        __m128i high128 = _mm256_extractf128_si256(vec32, 1);\n        \n        __m256i low64 = _mm256_cvtepu32_epi64(low128);\n        __m256i high64 = _mm256_cvtepu32_epi64(high128);\n        \n        low64 = _mm256_or_si256(low64, mask);\n        high64 = _mm256_or_si256(high64, mask);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), low64);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high64);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec_low = _mm256_setzero_pd();\n    __m256d sum_vec_high = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m128i idx16 = _mm_loadu_si128((const __m128i*)(index + i));\n        __m128i idx32_low = _mm_cvtepi16_epi32(idx16);\n        __m128i idx32_high = _mm_cvtepi16_epi32(_mm_srli_si128(idx16, 8));\n\n        __m256d gathered_low = _mm256_i32gather_pd(vec, idx32_low, 8);\n        __m256d gathered_high = _mm256_i32gather_pd(vec, idx32_high, 8);\n\n        sum_vec_low = _mm256_add_pd(sum_vec_low, gathered_low);\n        sum_vec_high = _mm256_add_pd(sum_vec_high, gathered_high);\n    }\n\n    __m256d sum_vec = _mm256_add_pd(sum_vec_low, sum_vec_high);\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < length; i++) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d acc = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i idx16 = _mm_loadl_epi64((const __m128i*)(index + i));\n        __m128i idx32 = _mm_cvtepu16_epi32(idx16);\n        __m256d vals = _mm256_i32gather_pd(vec, idx32, 8);\n        acc = _mm256_add_pd(acc, vals);\n    }\n\n    __m128d low_acc = _mm256_extractf128_pd(acc, 0);\n    __m128d high_acc = _mm256_extractf128_pd(acc, 1);\n    low_acc = _mm_add_pd(low_acc, high_acc);\n    double sum = _mm_cvtsd_f64(_mm_hadd_pd(low_acc, low_acc));\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    if (length == 0) return 0.0;\n    __m256d acc = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m128i idx8 = _mm_loadu_si128((const __m128i*)(index + i));\n        __m128i low4_16 = idx8;\n        __m128i high4_16 = _mm_srli_si128(idx8, 8);\n        __m128i low4_32 = _mm_cvtepu16_epi32(low4_16);\n        __m128i high4_32 = _mm_cvtepu16_epi32(high4_16);\n        __m256d gather0 = _mm256_i32gather_pd(vec, low4_32, 8);\n        __m256d gather1 = _mm256_i32gather_pd(vec, high4_32, 8);\n        __m256d gathered_sum = _mm256_add_pd(gather0, gather1);\n        acc = _mm256_add_pd(acc, gathered_sum);\n    }\n\n    __m128d low = _mm256_castpd256_pd128(acc);\n    __m128d high = _mm256_extractf128_pd(acc, 1);\n    low = _mm_add_pd(low, high);\n    __m128d high64 = _mm_unpackhi_pd(low, low);\n    double sum = _mm_cvtsd_f64(_mm_add_sd(low, high64));\n\n    for (; i < length; i++) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    if (length == 0) \n        return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i idx16 = _mm_loadl_epi64((const __m128i*)(index + i));\n        __m128i idx32 = _mm_cvtepu16_epi32(idx16);\n        __m256d gathered = _mm256_i32gather_pd(vec, idx32, 8);\n        sum_vec = _mm256_add_pd(sum_vec, gathered);\n    }\n\n    __m128d low128 = _mm256_castpd256_pd128(sum_vec);\n    __m128d high128 = _mm256_extractf128_pd(sum_vec, 1);\n    __m128d sum128 = _mm_add_pd(low128, high128);\n    __m128d hi = _mm_unpackhi_pd(sum128, sum128);\n    double sum = _mm_cvtsd_f64(_mm_add_sd(sum128, hi));\n\n    for (; i < length; i++) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    double total_sum = 0.0;\n    size_t i = 0;\n\n    if (length >= 8) {\n        __m256d sum_vec0 = _mm256_setzero_pd();\n        __m256d sum_vec1 = _mm256_setzero_pd();\n\n        for (; i <= length - 8; i += 8) {\n            __m128i idx16 = _mm_loadu_si128((const __m128i*)(index + i));\n            __m256i idx32 = _mm256_cvtepi16_epi32(idx16);\n\n            __m128i idx32_low = _mm256_castsi256_si128(idx32);\n            __m128i idx32_high = _mm256_extractf128_si256(idx32, 1);\n\n            __m256d gathered_low = _mm256_i32gather_pd(vec, _mm256_castsi128_si256(idx32_low), 8);\n            __m256d gathered_high = _mm256_i32gather_pd(vec, _mm256_castsi128_si256(idx32_high), 8);\n\n            sum_vec0 = _mm256_add_pd(sum_vec0, gathered_low);\n            sum_vec1 = _mm256_add_pd(sum_vec1, gathered_high);\n        }\n\n        __m256d sum_vec = _mm256_add_pd(sum_vec0, sum_vec1);\n        double tmp[4];\n        _mm256_storeu_pd(tmp, sum_vec);\n        total_sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n    }\n\n    for (; i < length; i++) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t out_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < out_size; i++) {\n        for (size_t j = 0; j < out_size; j++) {\n            __m256d acc_vec = _mm256_setzero_pd();\n            double total = 0.0;\n\n            for (size_t k = 0; k < kernel_size; k++) {\n                const double* input_row = input + (i + k) * input_size + j;\n                const double* kernel_row = kernel + k * kernel_size;\n                size_t l = 0;\n\n                while (l + 4 <= kernel_size) {\n                    __m256d input_block = _mm256_loadu_pd(input_row + l);\n                    __m256d kernel_block = _mm256_loadu_pd(kernel_row + l);\n                    acc_vec = _mm256_fmadd_pd(input_block, kernel_block, acc_vec);\n                    l += 4;\n                }\n\n                for (; l < kernel_size; l++) {\n                    total += input_row[l] * kernel_row[l];\n                }\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, acc_vec);\n            for (int t = 0; t < 4; t++) {\n                total += temp[t];\n            }\n\n            if (total < 0.0) {\n                total = 0.0;\n            }\n\n            output[i * out_size + j] = total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t total_output_elements = output_size * output_size;\n\n    // Initialize output to zero\n    __m256d zero_vec = _mm256_setzero_pd();\n    size_t idx = 0;\n    for (; idx + 3 < total_output_elements; idx += 4) {\n        _mm256_storeu_pd(output + idx, zero_vec);\n    }\n    for (; idx < total_output_elements; idx++) {\n        output[idx] = 0.0;\n    }\n\n    // Convolution\n    for (size_t k = 0; k < kernel_size; k++) {\n        for (size_t l = 0; l < kernel_size; l++) {\n            double k_val = kernel[k * kernel_size + l];\n            __m256d k_vec = _mm256_set1_pd(k_val);\n            for (size_t i = 0; i < output_size; i++) {\n                const double* in_ptr = input + (i + k) * input_size + l;\n                double* out_ptr = output + i * output_size;\n                size_t j = 0;\n                for (; j + 3 < output_size; j += 4) {\n                    __m256d in_vec = _mm256_loadu_pd(in_ptr + j);\n                    __m256d mul = _mm256_mul_pd(k_vec, in_vec);\n                    __m256d out_vec = _mm256_loadu_pd(out_ptr + j);\n                    out_vec = _mm256_add_pd(out_vec, mul);\n                    _mm256_storeu_pd(out_ptr + j, out_vec);\n                }\n                for (; j < output_size; j++) {\n                    out_ptr[j] += k_val * in_ptr[j];\n                }\n            }\n        }\n    }\n\n    // Apply ReLU\n    __m256d zero = _mm256_setzero_pd();\n    idx = 0;\n    for (; idx + 3 < total_output_elements; idx += 4) {\n        __m256d out_vec = _mm256_loadu_pd(output + idx);\n        out_vec = _mm256_max_pd(zero, out_vec);\n        _mm256_storeu_pd(output + idx, out_vec);\n    }\n    for (; idx < total_output_elements; idx++) {\n        if (output[idx] < 0.0) {\n            output[idx] = 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            double remainder_sum = 0.0;\n\n            for (size_t m = 0; m < kernel_size; m++) {\n                const double* kernel_row = kernel + m * kernel_size;\n                const double* input_row = input + (i + m) * input_size + j;\n\n                size_t n = 0;\n                for (; n + 4 <= kernel_size; n += 4) {\n                    __m256d kernel_seg = _mm256_loadu_pd(kernel_row + n);\n                    __m256d input_seg = _mm256_loadu_pd(input_row + n);\n                    __m256d product = _mm256_mul_pd(kernel_seg, input_seg);\n                    sum_vec = _mm256_add_pd(sum_vec, product);\n                }\n\n                for (; n < kernel_size; n++) {\n                    remainder_sum += kernel_row[n] * input_row[n];\n                }\n            }\n\n            __m128d vlow = _mm256_castpd256_pd128(sum_vec);\n            __m128d vhigh = _mm256_extractf128_pd(sum_vec, 1);\n            vlow = _mm_add_pd(vlow, vhigh);\n            __m128d high64 = _mm_unpackhi_pd(vlow, vlow);\n            double vector_sum = _mm_cvtsd_f64(_mm_add_sd(vlow, high64));\n\n            double total = vector_sum + remainder_sum;\n            total = total > 0.0 ? total : 0.0;\n            output[i * output_size + j] = total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; i++) {\n        size_t j = 0;\n        for (; j + 3 < output_size; j += 4) {\n            __m256d accum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj++) {\n                    __m256d input_vec = _mm256_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[ki * kernel_size + kj]);\n                    accum = _mm256_fmadd_pd(input_vec, kernel_val, accum);\n                }\n            }\n            accum = _mm256_max_pd(accum, _mm256_setzero_pd());\n            _mm256_storeu_pd(&output[i * output_size + j], accum);\n        }\n\n        for (; j < output_size; j++) {\n            double sum = 0.0;\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj++) {\n                    sum += input[(i + ki) * input_size + j + kj] * kernel[ki * kernel_size + kj];\n                }\n            }\n            output[i * output_size + j] = (sum > 0) ? sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    \n    for (size_t i = 0; i < output_size; i++) {\n        size_t j = 0;\n        for (; j + 3 < output_size; j += 4) {\n            __m256d acc = _mm256_setzero_pd();\n            \n            for (size_t k = 0; k < kernel_size; k++) {\n                for (size_t l = 0; l < kernel_size; l++) {\n                    double kernel_val = kernel[k * kernel_size + l];\n                    __m256d kernel_broadcast = _mm256_set1_pd(kernel_val);\n                    __m256d input_vec = _mm256_loadu_pd(&input[(i + k) * input_size + j + l]);\n                    acc = _mm256_fmadd_pd(kernel_broadcast, input_vec, acc);\n                }\n            }\n            \n            acc = _mm256_max_pd(acc, _mm256_setzero_pd());\n            _mm256_storeu_pd(&output[i * output_size + j], acc);\n        }\n        \n        for (; j < output_size; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < kernel_size; k++) {\n                for (size_t l = 0; l < kernel_size; l++) {\n                    sum += kernel[k * kernel_size + l] * input[(i + k) * input_size + j + l];\n                }\n            }\n            output[i * output_size + j] = (sum < 0.0) ? 0.0 : sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <immintrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    std::vector<double> sorted_numbers(numbers);\n    std::sort(sorted_numbers.begin(), sorted_numbers.end());\n\n    size_t i = 0;\n    for (; i <= n-5; i += 4) {\n        __m256d v1 = _mm256_loadu_pd(&sorted_numbers[i]);\n        __m256d v2 = _mm256_loadu_pd(&sorted_numbers[i+1]);\n        __m256d diff = _mm256_sub_pd(v2, v1);\n        __m256d thresh_vec = _mm256_set1_pd(threshold);\n        __m256d mask = _mm256_cmp_pd(diff, thresh_vec, _CMP_LT_OS);\n        if (_mm256_movemask_pd(mask)) {\n            return true;\n        }\n    }\n\n    for (; i < n-1; ++i) {\n        if (sorted_numbers[i+1] - sorted_numbers[i] < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d sign_mask = _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000));\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m256d v_current = _mm256_set1_pd(numbers[i]);\n        size_t j = i + 1;\n\n        while (j + 3 < n) {\n            __m256d v_block = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(v_current, v_block);\n            __m256d abs_diff = _mm256_andnot_pd(sign_mask, diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, v_threshold, _CMP_LT_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0) \n                return true;\n            j += 4;\n        }\n\n        for (; j < n; ++j) {\n            double diff = numbers[i] - numbers[j];\n            double abs_diff = diff < 0.0 ? -diff : diff;\n            if (abs_diff < threshold) \n                return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <algorithm>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    std::vector<double> copy = numbers;\n    std::sort(copy.begin(), copy.end());\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d abs_mask = _mm256_set1_pd(-0.0); \n    size_t i = 0;\n\n    for (; i <= n-5; i += 4) {\n        __m256d a = _mm256_loadu_pd(&copy[i]);\n        __m256d b = _mm256_loadu_pd(&copy[i+1]);\n        __m256d diff = _mm256_sub_pd(a, b);\n        __m256d abs_diff = _mm256_andnot_pd(abs_mask, diff);\n        __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < n-1; ++i) {\n        double diff = copy[i] - copy[i+1];\n        if (diff < 0) \n            diff = -diff;\n        if (diff < threshold) \n            return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n\n    size_t i = 0;\n    if (n >= 5) {\n        __m256d thresh_vec = _mm256_set1_pd(threshold);\n        __m256d sign_mask = _mm256_set1_pd(-0.0);\n\n        for (; i <= n - 5; i += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n            __m256d vec2 = _mm256_loadu_pd(&numbers[i+1]);\n\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(sign_mask, diff);\n\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    for (size_t j = i; j < n - 1; ++j) {\n        if (std::fabs(numbers[j] - numbers[j+1]) < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n    std::vector<double> sorted(numbers);\n    std::sort(sorted.begin(), sorted.end());\n    \n    size_t i = 0;\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    for (; i <= n - 5; i += 4) {\n        __m256d a = _mm256_loadu_pd(&sorted[i]);\n        __m256d b = _mm256_loadu_pd(&sorted[i+1]);\n        __m256d diff = _mm256_sub_pd(b, a);\n        __m256d cmp = _mm256_cmp_pd(diff, v_threshold, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n    \n    for (; i < n - 1; ++i) {\n        if (sorted[i+1] - sorted[i] < threshold) {\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    if (n == 0) \n        return false;\n    \n    int running_total = 0;\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(operations.data() + i));\n        __m128i low = _mm256_castsi256_si128(chunk);\n        __m128i high = _mm256_extractf128_si256(chunk, 1);\n\n        __m128i low_scan = low;\n        __m128i shift1 = _mm_slli_si128(low_scan, 4);\n        low_scan = _mm_add_epi32(low_scan, shift1);\n        __m128i shift2 = _mm_slli_si128(low_scan, 8);\n        low_scan = _mm_add_epi32(low_scan, shift2);\n\n        __m128i high_scan = high;\n        shift1 = _mm_slli_si128(high_scan, 4);\n        high_scan = _mm_add_epi32(high_scan, shift1);\n        shift2 = _mm_slli_si128(high_scan, 8);\n        high_scan = _mm_add_epi32(high_scan, shift2);\n\n        __m128i total_low = _mm_shuffle_epi32(low_scan, _MM_SHUFFLE(3, 3, 3, 3));\n        high_scan = _mm_add_epi32(high_scan, total_low);\n\n        __m256i prefix_chunk = _mm256_set_m128i(high_scan, low_scan);\n        __m256i carry_vec = _mm256_set1_epi32(running_total);\n        __m256i global_prefix = _mm256_add_epi32(prefix_chunk, carry_vec);\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi32(zero, global_prefix);\n        int test = _mm256_movemask_epi8(mask);\n        if (test != 0) \n            return true;\n\n        running_total += _mm_extract_epi32(high_scan, 3);\n    }\n\n    for (; i < n; i++) {\n        running_total += operations[i];\n        if (running_total < 0)\n            return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int current_balance = 0;\n    int i = 0;\n\n    if (n == 0) {\n        return false;\n    }\n\n    for (; i <= n - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(operations.data() + i));\n        __m256i scan = _mm256_add_epi32(v, _mm256_slli_si256(v, 4));\n        scan = _mm256_add_epi32(scan, _mm256_slli_si256(scan, 8));\n        \n        __m128i low_lane = _mm256_extractf128_si256(scan, 0);\n        __m128i low_total = _mm_shuffle_epi32(low_lane, _MM_SHUFFLE(3, 3, 3, 3));\n        __m256i adjust = _mm256_set_m128i(low_total, _mm_setzero_si128());\n        scan = _mm256_add_epi32(scan, adjust);\n        \n        __m256i balance_vec = _mm256_set1_epi32(current_balance);\n        __m256i running_balances = _mm256_add_epi32(balance_vec, scan);\n        \n        __m256i zeros = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi32(zeros, running_balances);\n        \n        if (!_mm256_testz_si256(mask, mask)) {\n            return true;\n        }\n        \n        current_balance += _mm256_extract_epi32(scan, 7);\n    }\n\n    for (; i < n; i++) {\n        current_balance += operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int base = 0;\n    int n = operations.size();\n    int i = 0;\n\n    if (n == 0) {\n        return false;\n    }\n\n    for (; i + 7 < n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(operations.data() + i));\n        __m256i v1 = _mm256_add_epi32(v, _mm256_slli_si256(v, 4));\n        __m256i v2 = _mm256_add_epi32(v1, _mm256_slli_si256(v1, 8));\n        int sum_first_lane = _mm256_extract_epi32(v2, 3);\n        __m256i offset = _mm256_setr_epi32(0, 0, 0, 0, sum_first_lane, sum_first_lane, sum_first_lane, sum_first_lane);\n        __m256i scan_chunk = _mm256_add_epi32(v2, offset);\n        __m256i base_vec = _mm256_set1_epi32(base);\n        __m256i balances = _mm256_add_epi32(base_vec, scan_chunk);\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi32(zeros, balances);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n        base += _mm256_extract_epi32(scan_chunk, 7);\n    }\n\n    for (; i < n; i++) {\n        base += operations[i];\n        if (base < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    if (n == 0) \n        return false;\n\n    int current_sum = 0;\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        __m128i low = _mm256_castsi256_si128(v);\n        __m128i high = _mm256_extracti128_si256(v, 1);\n\n        __m128i low_scan = low;\n        __m128i shifted = _mm_slli_si128(low_scan, 4);\n        low_scan = _mm_add_epi32(low_scan, shifted);\n        shifted = _mm_slli_si128(low_scan, 8);\n        low_scan = _mm_add_epi32(low_scan, shifted);\n\n        __m128i last_low = _mm_shuffle_epi32(low_scan, _MM_SHUFFLE(3, 3, 3, 3));\n\n        __m128i high_scan = high;\n        shifted = _mm_slli_si128(high_scan, 4);\n        high_scan = _mm_add_epi32(high_scan, shifted);\n        shifted = _mm_slli_si128(high_scan, 8);\n        high_scan = _mm_add_epi32(high_scan, shifted);\n\n        high_scan = _mm_add_epi32(high_scan, last_low);\n\n        __m256i chunk_prefix = _mm256_castsi128_si256(low_scan);\n        chunk_prefix = _mm256_inserti128_si256(chunk_prefix, high_scan, 1);\n\n        __m256i base_vec = _mm256_set1_epi32(current_sum);\n        __m256i chunk_prefix_with_base = _mm256_add_epi32(chunk_prefix, base_vec);\n\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi32(zeros, chunk_prefix_with_base);\n\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n\n        current_sum += _mm_extract_epi32(high_scan, 3);\n    }\n\n    for (; i < n; i++) {\n        current_sum += operations[i];\n        if (current_sum < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    if (n == 0) \n        return false;\n\n    int carry = 0;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(operations.data() + i));\n        __m256i v1 = _mm256_add_epi32(chunk, _mm256_slli_si256(chunk, 4));\n        __m256i v2 = _mm256_add_epi32(v1, _mm256_slli_si256(v1, 8));\n        \n        __m128i low = _mm256_extractf128_si256(v2, 0);\n        __m128i high = _mm256_extractf128_si256(v2, 1);\n        __m128i last_low = _mm_shuffle_epi32(low, _MM_SHUFFLE(3,3,3,3));\n        high = _mm_add_epi32(high, last_low);\n        \n        __m256i prefix_chunk = _mm256_insertf128_si256(v2, high, 1);\n        __m256i carry_vec = _mm256_set1_epi32(carry);\n        __m256i running_sums = _mm256_add_epi32(prefix_chunk, carry_vec);\n        \n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), running_sums);\n        if (!_mm256_testz_si256(mask, mask)) {\n            return true;\n        }\n        \n        carry = _mm256_extract_epi32(running_sums, 7);\n    }\n\n    long long current = carry;\n    for (; i < n; i++) {\n        current += operations[i];\n        if (current < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    if (numbers.empty()) return 0.0;\n    size_t n = numbers.size();\n    double sum = 0.0;\n\n    // Compute sum using AVX\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    // Horizontal sum\n    __m128d sum_low = _mm256_castpd256_pd128(sum_vec);\n    __m128d sum_high = _mm256_extractf128_pd(sum_vec, 1);\n    sum_low = _mm_add_pd(sum_low, sum_high);\n    __m128d sum_temp = _mm_unpackhi_pd(sum_low, sum_low);\n    sum = _mm_cvtsd_f64(_mm_add_sd(sum_low, sum_temp));\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Calculate mean\n    double mean = sum / n;\n    __m256d mean_vec = _mm256_set1_pd(mean);\n\n    // Compute absolute deviations\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n    i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum for absolute deviations\n    __m128d abs_low = _mm256_castpd256_pd128(abs_sum_vec);\n    __m128d abs_high = _mm256_extractf128_pd(abs_sum_vec, 1);\n    abs_low = _mm_add_pd(abs_low, abs_high);\n    __m128d abs_temp = _mm_unpackhi_pd(abs_low, abs_low);\n    double abs_sum = _mm_cvtsd_f64(_mm_add_sd(abs_low, abs_temp));\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double total_sum = 0.0;\n    __m256d vsum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        vsum = _mm256_add_pd(vsum, v);\n    }\n    for (; i < n; i++) {\n        total_sum += numbers[i];\n    }\n\n    __m128d low_sum = _mm256_castpd256_pd128(vsum);\n    __m128d high_sum = _mm256_extractf128_pd(vsum, 1);\n    low_sum = _mm_add_pd(low_sum, high_sum);\n    __m128d high64 = _mm_unpackhi_pd(low_sum, low_sum);\n    total_sum += _mm_cvtsd_f64(_mm_add_sd(low_sum, high64));\n\n    double mean = total_sum / n;\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d abs_mask = _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));\n    __m256d vabs_sum = _mm256_setzero_pd();\n    double total_abs_dev = 0.0;\n\n    i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, vmean);\n        __m256d abs_diff = _mm256_and_pd(diff, abs_mask);\n        vabs_sum = _mm256_add_pd(vabs_sum, abs_diff);\n    }\n\n    __m128d low_abs = _mm256_castpd256_pd128(vabs_sum);\n    __m128d high_abs = _mm256_extractf128_pd(vabs_sum, 1);\n    low_abs = _mm_add_pd(low_abs, high_abs);\n    __m128d high64_abs = _mm_unpackhi_pd(low_abs, low_abs);\n    total_abs_dev = _mm_cvtsd_f64(_mm_add_sd(low_abs, high64_abs));\n\n    for (; i < n; i++) {\n        total_abs_dev += std::fabs(numbers[i] - mean);\n    }\n\n    return total_abs_dev / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    double sum = 0.0;\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n    for (; i < n; i++) {\n        sum += numbers[i];\n    }\n    double mean = sum / n;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_mask = _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));\n    __m256d mad_vec = _mm256_setzero_pd();\n    i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(diff, abs_mask);\n        mad_vec = _mm256_add_pd(mad_vec, abs_diff);\n    }\n    double mad_sum = 0.0;\n    _mm256_storeu_pd(temp, mad_vec);\n    mad_sum = temp[0] + temp[1] + temp[2] + temp[3];\n    for (; i < n; i++) {\n        double diff = numbers[i] - mean;\n        mad_sum += std::abs(diff);\n    }\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Compute the mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    double sum = 0.0;\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n\n    // Horizontal sum of the vector\n    __m128d low = _mm256_extractf128_pd(sum_vec, 0);\n    __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n    low = _mm_add_pd(low, high);\n    __m128d high64 = _mm_unpackhi_pd(low, low);\n    low = _mm_add_sd(low, high64);\n    sum += _mm_cvtsd_f64(low);\n\n    // Handle remaining elements\n    for (; i < n; i++) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Step 2: Compute sum of absolute deviations\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n    i = 0;\n\n    // Create absolute value mask (clears sign bit)\n    __m256d abs_mask = _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));\n\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_val = _mm256_and_pd(diff, abs_mask);\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_val);\n    }\n\n    // Horizontal sum of absolute deviations\n    __m128d abs_low = _mm256_extractf128_pd(abs_sum_vec, 0);\n    __m128d abs_high = _mm256_extractf128_pd(abs_sum_vec, 1);\n    abs_low = _mm_add_pd(abs_low, abs_high);\n    __m128d abs_high64 = _mm_unpackhi_pd(abs_low, abs_low);\n    abs_low = _mm_add_sd(abs_low, abs_high64);\n    double abs_sum = _mm_cvtsd_f64(abs_low);\n\n    // Handle remaining elements\n    for (; i < n; i++) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return 0.0;\n    }\n\n    // Step 1: Compute the mean\n    __m256d sum_vec = _mm256_setzero_pd();\n    double total_sum = 0.0;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n    }\n    // Horizontal sum of the vector\n    __m128d low = _mm256_extractf128_pd(sum_vec, 0);\n    __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n    low = _mm_add_pd(low, high);\n    __m128d shuf = _mm_shuffle_pd(low, low, 1);\n    __m128d hsum = _mm_add_pd(low, shuf);\n    total_sum += _mm_cvtsd_f64(hsum);\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n    double mean = total_sum / n;\n\n    // Step 2: Compute absolute deviations sum\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n    __m256d sign_mask = _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));\n    double abs_sum = 0.0;\n    i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 3 < n; i += 4) {\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(diff, sign_mask); // Absolute value\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_diff);\n    }\n    // Horizontal sum of absolute deviations\n    low = _mm256_extractf128_pd(abs_sum_vec, 0);\n    high = _mm256_extractf128_pd(abs_sum_vec, 1);\n    low = _mm_add_pd(low, high);\n    shuf = _mm_shuffle_pd(low, low, 1);\n    hsum = _mm_add_pd(low, shuf);\n    abs_sum += _mm_cvtsd_f64(hsum);\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    if (numbers.empty()) {\n        return {};\n    }\n    size_t n = numbers.size();\n    size_t output_size = n * 2 - 1;\n    std::vector<int> output(output_size);\n    \n    int* in_ptr = numbers.data();\n    int* out_ptr = output.data();\n    size_t i = 0;\n\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i in8 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(in_ptr));\n            __m128i in_lo = _mm256_extracti128_si256(in8, 0);\n            __m128i in_hi = _mm256_extracti128_si256(in8, 1);\n            __m128i d128 = _mm_set1_epi32(delimeter);\n            \n            __m128i lo1 = _mm_unpacklo_epi32(in_lo, d128);\n            __m128i lo2 = _mm_unpackhi_epi32(in_lo, d128);\n            __m256i out1 = _mm256_set_m128i(lo2, lo1);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(out_ptr), out1);\n            \n            __m128i hi1 = _mm_unpacklo_epi32(in_hi, d128);\n            __m128i hi2 = _mm_unpackhi_epi32(in_hi, d128);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(out_ptr + 8), hi1);\n            \n            alignas(16) int temp[4];\n            _mm_store_si128(reinterpret_cast<__m128i*>(temp), hi2);\n            out_ptr[12] = temp[0];\n            out_ptr[13] = temp[1];\n            out_ptr[14] = temp[2];\n            \n            in_ptr += 8;\n            if (i + 8 < n) {\n                out_ptr[15] = delimeter;\n                out_ptr += 16;\n            } else {\n                out_ptr += 15;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        *out_ptr++ = numbers[i];\n        if (i < n - 1) {\n            *out_ptr++ = delimeter;\n        }\n    }\n    \n    return output;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n    std::vector<int> result(2 * n - 1);\n    size_t nm1 = n - 1;\n    size_t in = 0;\n    size_t out = 0;\n\n    for (; in + 8 <= nm1; in += 8, out += 16) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(numbers.data() + in));\n        __m256i delim_vec = _mm256_set1_epi32(delimeter);\n\n        __m256i unpack_lo = _mm256_unpacklo_epi32(vec, delim_vec);\n        __m256i unpack_hi = _mm256_unpackhi_epi32(vec, delim_vec);\n\n        __m256i permuted_lo = _mm256_permute2x128_si256(unpack_lo, unpack_hi, 0x20);\n        __m256i permuted_hi = _mm256_permute2x128_si256(unpack_lo, unpack_hi, 0x31);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + out), permuted_lo);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + out + 8), permuted_hi);\n    }\n\n    for (; in < nm1; ++in, out += 2) {\n        result[out] = numbers[in];\n        result[out + 1] = delimeter;\n    }\n\n    result[out] = numbers[in];\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return std::vector<int>();\n    }\n    size_t output_size = 2 * n - 1;\n    std::vector<int> output(output_size);\n    size_t i = 0;\n    size_t j = 0;\n\n    __m256i vdelim = _mm256_set1_epi32(delimeter);\n    __m256i store_mask = _mm256_set_epi32(0, -1, -1, -1, -1, -1, -1, -1);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i in = _mm256_loadu_si256((__m256i*)(numbers.data() + i));\n        __m256i interleaved_lo = _mm256_unpacklo_epi32(in, vdelim);\n        __m256i interleaved_hi = _mm256_unpackhi_epi32(in, vdelim);\n        __m256i permuted1 = _mm256_permute2x128_si256(interleaved_lo, interleaved_hi, 0x20);\n        __m256i permuted2 = _mm256_permute2x128_si256(interleaved_lo, interleaved_hi, 0x31);\n        _mm256_storeu_si256((__m256i*)(output.data() + j), permuted1);\n        j += 8;\n        _mm256_maskstore_epi32(output.data() + j, store_mask, permuted2);\n        j += 7;\n    }\n\n    for (; i < n; i++) {\n        if (i > 0) {\n            output[j++] = delimeter;\n        }\n        output[j++] = numbers[i];\n    }\n\n    return output;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    int n = numbers.size();\n    if (n == 0) {\n        return std::vector<int>();\n    }\n    std::vector<int> output(2 * n - 1);\n    int full_blocks = n / 8;\n    int tail = n % 8;\n    int j = 0;\n    __m256i deli_vec = _mm256_set1_epi32(delimeter);\n    __m256i mask = _mm256_set_epi32(0, -1, -1, -1, -1, -1, -1, -1);\n\n    for (int iblock = 0; iblock < full_blocks; iblock++) {\n        int i = iblock * 8;\n        __m256i in_vec = _mm256_loadu_si256((__m256i const*)(numbers.data() + i));\n        __m256i low_interleaved = _mm256_unpacklo_epi32(in_vec, deli_vec);\n        __m256i high_interleaved = _mm256_unpackhi_epi32(in_vec, deli_vec);\n\n        _mm256_storeu_si256((__m256i*)(output.data() + j), low_interleaved);\n        _mm256_maskstore_epi32(output.data() + j + 8, mask, high_interleaved);\n        j += 15;\n\n        if (iblock < full_blocks - 1 || tail > 0) {\n            output[j++] = delimeter;\n        }\n    }\n\n    if (tail > 0) {\n        int i = full_blocks * 8;\n        for (int k = 0; k < tail; k++) {\n            output[j++] = numbers[i + k];\n            if (k < tail - 1) {\n                output[j++] = delimeter;\n            }\n        }\n    }\n\n    return output;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return std::vector<int>();\n    }\n    std::vector<int> result(2 * n - 1);\n    result[0] = numbers[0];\n    size_t rest_count = n - 1;\n    if (rest_count > 0) {\n        int* input_rest = numbers.data() + 1;\n        int* output_rest = result.data() + 1;\n        __m256i delim_vec = _mm256_set1_epi32(delimeter);\n        size_t i = 0;\n        for (; i <= rest_count - 8; i += 8) {\n            __m256i numbers_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(input_rest + i));\n            __m256i low = _mm256_unpacklo_epi32(delim_vec, numbers_vec);\n            __m256i high = _mm256_unpackhi_epi32(delim_vec, numbers_vec);\n            __m256i out1 = _mm256_permute2x128_si256(low, high, 0x20);\n            __m256i out2 = _mm256_permute2x128_si256(low, high, 0x31);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(output_rest + 2 * i), out1);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(output_rest + 2 * i + 8), out2);\n        }\n        for (; i < rest_count; i++) {\n            output_rest[2 * i] = delimeter;\n            output_rest[2 * i + 1] = input_rest[i];\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_acc0 = _mm256_setzero_si256();\n    __m256i sum_acc1 = _mm256_setzero_si256();\n    __m256i prod_acc = _mm256_set1_epi64x(1);\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(numbers.data() + i));\n        __m128i v_low = _mm256_extracti128_si256(v, 0);\n        __m128i v_high = _mm256_extracti128_si256(v, 1);\n\n        __m256i v32_low = _mm256_cvtepi16_epi32(v_low);\n        __m256i v32_high = _mm256_cvtepi16_epi32(v_high);\n\n        __m128i v32_low_low = _mm256_extracti128_si256(v32_low, 0);\n        __m128i v32_low_high = _mm256_extracti128_si256(v32_low, 1);\n        __m256i v64_low0 = _mm256_cvtepi32_epi64(v32_low_low);\n        __m256i v64_low1 = _mm256_cvtepi32_epi64(v32_low_high);\n\n        __m128i v32_high_low = _mm256_extracti128_si256(v32_high, 0);\n        __m128i v32_high_high = _mm256_extracti128_si256(v32_high, 1);\n        __m256i v64_high0 = _mm256_cvtepi32_epi64(v32_high_low);\n        __m256i v64_high1 = _mm256_cvtepi32_epi64(v32_high_high);\n\n        sum_acc0 = _mm256_add_epi64(sum_acc0, v64_low0);\n        sum_acc0 = _mm256_add_epi64(sum_acc0, v64_low1);\n        sum_acc1 = _mm256_add_epi64(sum_acc1, v64_high0);\n        sum_acc1 = _mm256_add_epi64(sum_acc1, v64_high1);\n\n        for (size_t j = 0; j < 16; j += 4) {\n            __m128i four_shorts = _mm_loadl_epi64((const __m128i*)(numbers.data() + i + j));\n            __m128i four_int32 = _mm_cvtepi16_epi32(four_shorts);\n            __m256i four_int64 = _mm256_cvtepi32_epi64(four_int32);\n            \n            alignas(32) int64_t prod_arr[4];\n            alignas(32) int64_t four_arr[4];\n            _mm256_store_si256((__m256i*)prod_arr, prod_acc);\n            _mm256_store_si256((__m256i*)four_arr, four_int64);\n            \n            for (int k = 0; k < 4; k++) {\n                prod_arr[k] *= four_arr[k];\n            }\n            prod_acc = _mm256_load_si256((const __m256i*)prod_arr);\n        }\n    }\n\n    sum_acc0 = _mm256_add_epi64(sum_acc0, sum_acc1);\n    alignas(32) int64_t sum_arr[4];\n    _mm256_store_si256((__m256i*)sum_arr, sum_acc0);\n    int64_t total_sum = 0;\n    for (int j = 0; j < 4; j++) {\n        total_sum += sum_arr[j];\n    }\n\n    alignas(32) int64_t prod_arr[4];\n    _mm256_store_si256((__m256i*)prod_arr, prod_acc);\n    int64_t total_prod = 1;\n    for (int j = 0; j < 4; j++) {\n        total_prod *= prod_arr[j];\n    }\n\n    for (; i < n; i++) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    const int16_t* data = numbers.data();\n    __m256i sum_acc0 = _mm256_setzero_si256();\n    __m256i sum_acc1 = _mm256_setzero_si256();\n    __m256i sum_acc2 = _mm256_setzero_si256();\n    __m256i sum_acc3 = _mm256_setzero_si256();\n    int64_t product = 1;\n    int64_t total_sum = 0;\n\n    size_t i = 0;\n    if (n >= 16) {\n        size_t aligned_size = n - (n % 16);\n        for (; i < aligned_size; i += 16) {\n            __m256i chunk = _mm256_loadu_si256((const __m256i*)(data + i));\n            __m128i low128 = _mm256_extracti128_si256(chunk, 0);\n            __m128i high128 = _mm256_extracti128_si256(chunk, 1);\n            \n            __m256i low0 = _mm256_cvtepi16_epi64(low128);\n            __m128i low_next = _mm_srli_si128(low128, 8);\n            __m256i low1 = _mm256_cvtepi16_epi64(low_next);\n            \n            __m256i high0 = _mm256_cvtepi16_epi64(high128);\n            __m128i high_next = _mm_srli_si128(high128, 8);\n            __m256i high1 = _mm256_cvtepi16_epi64(high_next);\n            \n            sum_acc0 = _mm256_add_epi64(sum_acc0, low0);\n            sum_acc1 = _mm256_add_epi64(sum_acc1, low1);\n            sum_acc2 = _mm256_add_epi64(sum_acc2, high0);\n            sum_acc3 = _mm256_add_epi64(sum_acc3, high1);\n            \n            alignas(32) int16_t arr[16];\n            _mm256_store_si256((__m256i*)arr, chunk);\n            for (int j = 0; j < 16; j++) {\n                product *= arr[j];\n            }\n        }\n        int64_t s0[4], s1[4], s2[4], s3[4];\n        _mm256_storeu_si256((__m256i*)s0, sum_acc0);\n        _mm256_storeu_si256((__m256i*)s1, sum_acc1);\n        _mm256_storeu_si256((__m256i*)s2, sum_acc2);\n        _mm256_storeu_si256((__m256i*)s3, sum_acc3);\n        for (int j = 0; j < 4; j++) {\n            total_sum += s0[j] + s1[j] + s2[j] + s3[j];\n        }\n    }\n    \n    for (; i < n; i++) {\n        total_sum += data[i];\n        product *= data[i];\n    }\n    \n    return {total_sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum_acc_vec = _mm256_setzero_si256();\n    int64_t product_acc[4] = {1, 1, 1, 1};\n    size_t n = numbers.size();\n    size_t i = 0;\n\n    for (; i + 15 < n; i += 16) {\n        for (int j = 0; j < 4; j++) {\n            size_t index = i + j * 4;\n            __m128i group = _mm_loadl_epi64((__m128i*)(numbers.data() + index));\n            __m256i group64 = _mm256_cvtepi16_epi64(group);\n            sum_acc_vec = _mm256_add_epi64(sum_acc_vec, group64);\n\n            int64_t g[4];\n            _mm256_storeu_si256((__m256i*)g, group64);\n            int64_t group_prod = g[0] * g[1] * g[2] * g[3];\n            product_acc[j] *= group_prod;\n        }\n    }\n\n    int64_t total_sum = 0;\n    int64_t sum_buf[4];\n    _mm256_storeu_si256((__m256i*)sum_buf, sum_acc_vec);\n    for (int j = 0; j < 4; j++) {\n        total_sum += sum_buf[j];\n    }\n\n    int64_t total_product = 1;\n    for (int j = 0; j < 4; j++) {\n        total_product *= product_acc[j];\n    }\n\n    for (size_t j = i; j < n; j++) {\n        total_sum += numbers[j];\n        total_product *= numbers[j];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    std::vector<int64_t> result(2);\n    if (numbers.empty()) {\n        result[0] = 0;\n        result[1] = 1;\n        return result;\n    }\n\n    size_t n = numbers.size();\n    int64_t total_sum = 0;\n    int64_t total_product = 1;\n    __m256i sum64_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 7 < n; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)(numbers.data() + i));\n        __m128i high64 = _mm_srli_si128(data, 8);\n        __m256i low4 = _mm256_cvtepi16_epi64(data);\n        __m256i high4 = _mm256_cvtepi16_epi64(high64);\n\n        sum64_vec = _mm256_add_epi64(sum64_vec, low4);\n        sum64_vec = _mm256_add_epi64(sum64_vec, high4);\n\n        int64_t low_arr[4];\n        int64_t high_arr[4];\n        _mm256_storeu_si256((__m256i*)low_arr, low4);\n        _mm256_storeu_si256((__m256i*)high_arr, high4);\n        int64_t low_prod = low_arr[0] * low_arr[1] * low_arr[2] * low_arr[3];\n        int64_t high_prod = high_arr[0] * high_arr[1] * high_arr[2] * high_arr[3];\n        total_product *= low_prod * high_prod;\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(sum64_vec);\n    __m128i high128 = _mm256_extracti128_si256(sum64_vec, 1);\n    __m128i sum128 = _mm_add_epi64(low128, high128);\n    uint64_t t[2];\n    _mm_storeu_si128((__m128i*)t, sum128);\n    total_sum = t[0] + t[1];\n\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    result[0] = total_sum;\n    result[1] = total_product;\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n    \n    int64_t sum = 0;\n    size_t n = numbers.size();\n    size_t i = 0;\n    \n    if (n >= 16) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        \n        for (; i <= n - 16; i += 16) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&numbers[i]);\n            __m128i low128 = _mm256_castsi256_si128(vec);\n            __m128i high128 = _mm256_extractf128_si256(vec, 1);\n            \n            __m256i low4 = _mm256_cvtepi16_epi64(low128);\n            __m256i high4 = _mm256_cvtepi16_epi64(_mm_srli_si128(low128, 8));\n            __m256i low4_high = _mm256_cvtepi16_epi64(high128);\n            __m256i high4_high = _mm256_cvtepi16_epi64(_mm_srli_si128(high128, 8));\n            \n            sum_vec = _mm256_add_epi64(sum_vec, low4);\n            sum_vec = _mm256_add_epi64(sum_vec, high4);\n            sum_vec = _mm256_add_epi64(sum_vec, low4_high);\n            sum_vec = _mm256_add_epi64(sum_vec, high4_high);\n        }\n        \n        int64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        sum = temp[0] + temp[1] + temp[2] + temp[3];\n    }\n    \n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n    \n    int64_t product = 1;\n    int64_t prod_acc[4] = {1, 1, 1, 1};\n    for (size_t j = 0; j < n; ++j) {\n        prod_acc[j & 3] *= numbers[j];\n    }\n    product = prod_acc[0] * prod_acc[1] * prod_acc[2] * prod_acc[3];\n    \n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) {\n        return result;\n    }\n    int current_max = std::numeric_limits<int>::min();\n    int i = 0;\n    constexpr int vec_size = 8;\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(numbers.data() + i));\n        int chunk[vec_size];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(chunk), data);\n        \n        for (int j = 0; j < vec_size; ++j) {\n            if (chunk[j] > current_max) {\n                current_max = chunk[j];\n            }\n            result[i + j] = current_max;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (numbers[i] > current_max) {\n            current_max = numbers[i];\n        }\n        result[i] = current_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "To solve this problem, we need to compute the rolling maximum of a sequence of integers efficiently using AVX2 intrinsics for parallelism. The rolling maximum at each position is the maximum value encountered from the start of the sequence up to the current position.\n\n### Approach\n1. **Problem Analysis**: The task involves generating a vector where each element at position `i` is the maximum value from the start of the vector up to `i`. This operation is inherently sequential, but we can leverage AVX2 to process multiple elements in parallel within chunks of 8 integers.\n2. **SIMD Processing**: We process the input vector in chunks of 8 integers. For each chunk:\n   - **Initialization**: The initial value for the chunk is the rolling maximum from the previous chunk (or `INT_MIN` for the first chunk).\n   - **Element-wise Maximum**: Compute the element-wise maximum between the initial value and the current chunk elements.\n   - **Inclusive Maximum Scan**: Perform an inclusive scan to propagate the maximum value through the chunk. This involves three passes with strides of 1, 2, and 4 to cover all positions.\n3. **Remainder Handling**: After processing all full chunks, any remaining elements (less than 8) are processed sequentially.\n4. **Efficiency**: The use of AVX2 allows processing 8 elements in parallel, significantly speeding up the computation for large vectors.\n\n### Solution Code\n```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstatic inline __m256i inclusive_max_scan(__m256i v) {\n    __m256i t = v;\n    __m256i shifted1 = _mm256_permutevar8x32_epi32(t, _mm256_set_epi32(6, 5, 4, 3, 2, 1, 0, 0));\n    t = _mm256_max_epi32(t, shifted1);\n    __m256i shifted2 = _mm256_permutevar8x32_epi32(t, _mm256_set_epi32(5, 4, 3, 2, 1, 0, 1, 0));\n    t = _mm256_max_epi32(t, shifted2);\n    __m256i shifted3 = _mm256_permutevar8x32_epi32(t, _mm256_set_epi32(3, 2, 1, 0, 3, 2, 1, 0));\n    t = _mm256_max_epi32(t, shifted3);\n    return t;\n}\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    int i = 0;\n    int current_max = INT_MIN;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i initial_vec = _mm256_set1_epi32(current_max);\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i maxes = _mm256_max_epi32(initial_vec, chunk);\n        __m256i scan = inclusive_max_scan(maxes);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), scan);\n        current_max = _mm256_extract_epi32(scan, 7);\n    }\n\n    for (; i < n; ++i) {\n        if (numbers[i] > current_max) {\n            current_max = numbers[i];\n        }\n        result[i] = current_max;\n    }\n\n    return result;\n}\n```\n\n### Explanation\n1. **Inclusive Maximum Scan**: The `inclusive_max_scan` function processes an 8-integer vector to compute the rolling maximum within the vector. It uses three passes with strides of 1, 2, and 4 to propagate the maximum value efficiently.\n2. **Chunk Processing**: The main loop processes the input vector in chunks of 8 integers. For each chunk:\n   - **Initial Vector**: The current rolling maximum from the previous chunk is broadcasted to all 8 lanes of an AVX2 register.\n   - **Element-wise Maximum**: Each element in the chunk is compared with the initial value to start the rolling maximum computation.\n   - **Scan Application**: The inclusive scan propagates the maximum value through the chunk, updating each position to reflect the maximum up to that point.\n   - **Storage and Update**: The result for the chunk is stored, and the last element's value becomes the new rolling maximum for the next chunk.\n3. **Remainder Handling**: Any remaining elements after processing full chunks are handled sequentially to update the rolling maximum and complete the result vector.\n\nThis approach efficiently leverages AVX2 intrinsics to parallelize the rolling maximum computation, providing significant performance improvements for large input vectors."}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n    \n    int current_max = std::numeric_limits<int>::min();\n    int i = 0;\n    \n    while (i < n && (i % 8 != 0)) {\n        current_max = std::max(current_max, numbers[i]);\n        result[i] = current_max;\n        i++;\n    }\n    \n    if (i == n) return result;\n    \n    for (; i <= n - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        __m256i v1 = _mm256_permutevar8x32_epi32(chunk, _mm256_set_epi32(6, 5, 4, 3, 2, 1, 0, 0));\n        v1 = _mm256_max_epi32(chunk, v1);\n        \n        __m256i v2 = _mm256_permutevar8x32_epi32(v1, _mm256_set_epi32(5, 4, 3, 2, 1, 0, 0, 0));\n        v2 = _mm256_max_epi32(v1, v2);\n        \n        __m256i v3 = _mm256_permutevar8x32_epi32(v2, _mm256_set_epi32(3, 2, 1, 0, 0, 0, 0, 0));\n        v3 = _mm256_max_epi32(v2, v3);\n        \n        __m256i max_vec = _mm256_set1_epi32(current_max);\n        __m256i rolling_vec = _mm256_max_epi32(max_vec, v3);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), rolling_vec);\n        \n        current_max = _mm256_extract_epi32(rolling_vec, 7);\n    }\n    \n    while (i < n) {\n        current_max = std::max(current_max, numbers[i]);\n        result[i] = current_max;\n        i++;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    const int n = numbers.size();\n    std::vector<int> result;\n    if (n == 0) return result;\n    result.reserve(n);\n    \n    int i = 0;\n    int current_max = std::numeric_limits<int>::min();\n    \n    while (i < n && i % 8 != 0) {\n        if (i == 0) {\n            current_max = numbers[0];\n        } else {\n            current_max = std::max(current_max, numbers[i]);\n        }\n        result.push_back(current_max);\n        i++;\n    }\n    \n    while (i < n) {\n        int remaining = n - i;\n        if (remaining < 8) {\n            break;\n        }\n        \n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(numbers.data() + i));\n        int temp[8];\n        int local_max = current_max;\n        for (int j = 0; j < 8; j++) {\n            local_max = std::max(local_max, numbers[i + j]);\n            temp[j] = local_max;\n        }\n        current_max = temp[7];\n        for (int j = 0; j < 8; j++) {\n            result.push_back(temp[j]);\n        }\n        i += 8;\n    }\n    \n    while (i < n) {\n        current_max = std::max(current_max, numbers[i]);\n        result.push_back(current_max);\n        i++;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstatic inline __m256i scan_max_8(__m256i vec) {\n    __m256i shifted1 = _mm256_permutevar8x32_epi32(vec, _mm256_set_epi32(6, 5, 4, 3, 2, 1, 0, 0));\n    __m256i max1 = _mm256_max_epi32(vec, shifted1);\n    __m256i shifted2 = _mm256_permutevar8x32_epi32(max1, _mm256_set_epi32(5, 4, 3, 2, 1, 0, 1, 0));\n    __m256i max2 = _mm256_max_epi32(max1, shifted2);\n    __m256i shifted4 = _mm256_permutevar8x32_epi32(max2, _mm256_set_epi32(3, 2, 1, 0, 3, 2, 1, 0));\n    __m256i max4 = _mm256_max_epi32(max2, shifted4);\n    return max4;\n}\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) \n        return result;\n\n    int current_max = numbers[0];\n    size_t i = 0;\n    for (; i < n; i += 8) {\n        size_t len = std::min<size_t>(8, n - i);\n        if (len < 8) {\n            for (size_t j = i; j < i + len; ++j) {\n                current_max = std::max(current_max, numbers[j]);\n                result[j] = current_max;\n            }\n            break;\n        }\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(numbers.data() + i));\n        if (i == 0) {\n            __m256i cum_chunk = scan_max_8(chunk);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data()), cum_chunk);\n            current_max = result[i + 7];\n        } else {\n            __m256i broadcast_prev = _mm256_set1_epi32(current_max);\n            __m256i cum_chunk = scan_max_8(chunk);\n            __m256i res_chunk = _mm256_max_epi32(broadcast_prev, cum_chunk);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), res_chunk);\n            current_max = result[i + 7];\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t n = a.size();\n    if (n != b.size()) {\n        return \"\";\n    }\n    std::string result;\n    result.resize(n);\n    size_t i = 0;\n    for (; i + 31 < n; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i zeros = _mm256_set1_epi8('0');\n        __m256i a_val = _mm256_sub_epi8(a_vec, zeros);\n        __m256i b_val = _mm256_sub_epi8(b_vec, zeros);\n        __m256i res_val = _mm256_xor_si256(a_val, b_val);\n        __m256i res_vec = _mm256_add_epi8(res_val, zeros);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), res_vec);\n    }\n    for (; i < n; i++) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    if (a.size() != b.size()) {\n        return \"\";\n    }\n    \n    size_t n = a.size();\n    std::string result(n, '0');\n    size_t i = 0;\n    const size_t block_size = 32;\n    \n    for (; i + block_size <= n; i += block_size) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i vec_xor = _mm256_xor_si256(vec_a, vec_b);\n        __m256i vec_zero = _mm256_set1_epi8('0');\n        __m256i vec_res = _mm256_add_epi8(vec_xor, vec_zero);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec_res);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = (a[i] ^ b[i]) + '0';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    if (a.size() != b.size()) {\n        return \"\";\n    }\n    \n    size_t n = a.size();\n    std::string res(n, '\\0');\n    size_t i = 0;\n    \n    const size_t vecSize = 32;\n    for (; i + vecSize <= n; i += vecSize) {\n        __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        \n        __m256i zeros = _mm256_set1_epi8('0');\n        __m256i a_bits = _mm256_sub_epi8(a_vec, zeros);\n        __m256i b_bits = _mm256_sub_epi8(b_vec, zeros);\n        \n        __m256i xor_result = _mm256_xor_si256(a_bits, b_bits);\n        __m256i res_vec = _mm256_add_epi8(xor_result, zeros);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), res_vec);\n    }\n    \n    for (; i < n; ++i) {\n        res[i] = '0' + ((a[i] - '0') ^ (b[i] - '0'));\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t n = std::min(a.size(), b.size());\n    std::string res(n, '0');\n    size_t i = 0;\n\n    for (; i + 31 < n; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i b_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i xor_result = _mm256_xor_si256(a_vec, b_vec);\n        __m256i char_result = _mm256_add_epi8(xor_result, _mm256_set1_epi8('0'));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), char_result);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = (a[i] ^ b[i]) + '0';\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        return \"\";\n    }\n    std::string res(len, '0');\n    const char* a_ptr = a.data();\n    const char* b_ptr = b.data();\n    char* res_ptr = &res[0];\n    size_t i = 0;\n    constexpr size_t vec_size = 32;\n    size_t vec_end = len - (len % vec_size);\n    __m256i zero_char = _mm256_set1_epi8('0');\n    for (; i < vec_end; i += vec_size) {\n        __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a_ptr + i));\n        __m256i b_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b_ptr + i));\n        __m256i xor_vec = _mm256_xor_si256(a_vec, b_vec);\n        __m256i result_vec = _mm256_add_epi8(xor_vec, zero_char);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res_ptr + i), result_vec);\n    }\n    for (; i < len; i++) {\n        res_ptr[i] = (a_ptr[i] ^ b_ptr[i]) + '0';\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    const size_t n = strings.size();\n    if (n == 0) {\n        return \"\";\n    }\n    size_t global_max_len = 0;\n    size_t global_max_index = 0;\n    size_t i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m256i chunk = _mm256_set_epi64x(\n            static_cast<long long>(strings[i+3].size()),\n            static_cast<long long>(strings[i+2].size()),\n            static_cast<long long>(strings[i+1].size()),\n            static_cast<long long>(strings[i].size())\n        );\n\n        for (int j = 0; j < 4; j++) {\n            size_t len = static_cast<size_t>(_mm256_extract_epi64(chunk, j));\n            if (len > global_max_len) {\n                global_max_len = len;\n                global_max_index = i + j;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        size_t len = strings[i].size();\n        if (len > global_max_len) {\n            global_max_len = len;\n            global_max_index = i;\n        }\n    }\n\n    return strings[global_max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return std::string();\n    }\n    \n    size_t n = strings.size();\n    std::vector<int> lengths;\n    lengths.reserve(n);\n    for (const auto& s : strings) {\n        lengths.push_back(static_cast<int>(s.size()));\n    }\n    \n    int global_max = INT_MIN;\n    int global_idx = -1;\n    \n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lengths.data() + i));\n        \n        __m256i v_perm = _mm256_permute2f128_si256(vals, vals, 0x01);\n        __m256i v_max1 = _mm256_max_epi32(vals, v_perm);\n        __m128i v_low = _mm256_castsi256_si128(v_max1);\n        __m128i v1 = _mm_shuffle_epi32(v_low, _MM_SHUFFLE(2,3,0,1));\n        __m128i v2 = _mm_max_epi32(v_low, v1);\n        __m128i v3 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(1,0,3,2));\n        __m128i v4 = _mm_max_epi32(v2, v3);\n        int max_in_chunk = _mm_extract_epi32(v4, 0);\n        \n        __m256i max_bc = _mm256_set1_epi32(max_in_chunk);\n        __m256i mask_vec = _mm256_cmpeq_epi32(vals, max_bc);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(mask_vec));\n        if (mask) {\n            int pos = __builtin_ffs(mask) - 1;\n            int candidate_idx = i + pos;\n            if (max_in_chunk > global_max) {\n                global_max = max_in_chunk;\n                global_idx = candidate_idx;\n            }\n        }\n    }\n    \n    for (; i < n; ++i) {\n        if (lengths[i] > global_max) {\n            global_max = lengths[i];\n            global_idx = i;\n        }\n    }\n    \n    return strings[global_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n    long long max_val = static_cast<long long>(strings[0].size());\n    int max_idx = 0;\n    int i = 1;\n    int n = strings.size();\n\n    while (i < n) {\n        int remaining = n - i;\n        int chunk_size = (remaining < 4) ? remaining : 4;\n        long long temp[4] = {-1, -1, -1, -1};\n        for (int j = 0; j < chunk_size; ++j) {\n            temp[j] = static_cast<long long>(strings[i+j].size());\n        }\n\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp));\n        __m256i current_max_vec = _mm256_set1_epi64x(max_val);\n        __m256i cmp = _mm256_cmpgt_epi64(data, current_max_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            unsigned int byte_index = __builtin_ctz(mask);\n            int in_chunk_index = byte_index / 8;\n            int abs_index = i + in_chunk_index;\n            max_val = temp[in_chunk_index];\n            max_idx = abs_index;\n\n            for (int j = in_chunk_index + 1; j < chunk_size; ++j) {\n                if (temp[j] > max_val) {\n                    max_val = temp[j];\n                    max_idx = i + j;\n                }\n            }\n        }\n        i += chunk_size;\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return std::string();\n    }\n\n    std::vector<int> lengths;\n    lengths.reserve(strings.size());\n    for (const auto& s : strings) {\n        lengths.push_back(static_cast<int>(s.size()));\n    }\n\n    int n = static_cast<int>(lengths.size());\n    int max_val = -1;\n    int max_idx = -1;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i v_len = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lengths[i]));\n        __m256i v = v_len;\n        __m256i v_perm = _mm256_shuffle_epi32(v, _MM_SHUFFLE(2, 3, 0, 1));\n        v = _mm256_max_epi32(v, v_perm);\n        v_perm = _mm256_shuffle_epi32(v, _MM_SHUFFLE(1, 0, 3, 2));\n        v = _mm256_max_epi32(v, v_perm);\n        \n        __m128i v_low = _mm256_castsi256_si128(v);\n        __m128i v_high = _mm256_extracti128_si256(v, 1);\n        __m128i v_max128 = _mm_max_epi32(v_low, v_high);\n        \n        __m128i v1 = _mm_shuffle_epi32(v_max128, _MM_SHUFFLE(2, 3, 0, 1));\n        v_max128 = _mm_max_epi32(v_max128, v1);\n        v1 = _mm_shuffle_epi32(v_max128, _MM_SHUFFLE(1, 0, 3, 2));\n        v_max128 = _mm_max_epi32(v_max128, v1);\n        \n        int chunk_max = _mm_extract_epi32(v_max128, 0);\n        \n        if (chunk_max > max_val) {\n            max_val = chunk_max;\n            for (int j = 0; j < 8; j++) {\n                if (lengths[i + j] == chunk_max) {\n                    max_idx = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (lengths[i] > max_val) {\n            max_val = lengths[i];\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    size_t n = strings.size();\n    if (n == 0) {\n        return std::string();\n    }\n\n    unsigned int max_val = 0;\n    size_t i = 0;\n\n    if (n >= 8) {\n        __m256i sign_bit = _mm256_set1_epi32(0x80000000);\n        __m256i max_vec = _mm256_set1_epi32(0x80000000);\n\n        for (; i <= n-8; i += 8) {\n            __m256i vec = _mm256_setr_epi32(\n                static_cast<int>(strings[i].size()),\n                static_cast<int>(strings[i+1].size()),\n                static_cast<int>(strings[i+2].size()),\n                static_cast<int>(strings[i+3].size()),\n                static_cast<int>(strings[i+4].size()),\n                static_cast<int>(strings[i+5].size()),\n                static_cast<int>(strings[i+6].size()),\n                static_cast<int>(strings[i+7].size())\n            );\n            __m256i vec_adj = _mm256_xor_si256(vec, sign_bit);\n            max_vec = _mm256_max_epi32(max_vec, vec_adj);\n        }\n        max_vec = _mm256_xor_si256(max_vec, sign_bit);\n        int max_arr[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(max_arr), max_vec);\n        for (int j = 0; j < 8; j++) {\n            if (static_cast<unsigned int>(max_arr[j]) > max_val) {\n                max_val = static_cast<unsigned int>(max_arr[j]);\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        unsigned int len = static_cast<unsigned int>(strings[i].size());\n        if (len > max_val) {\n            max_val = len;\n        }\n    }\n\n    for (size_t j = 0; j < n; j++) {\n        if (static_cast<unsigned int>(strings[j].size()) == max_val) {\n            return strings[j];\n        }\n    }\n\n    return std::string();\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    __m128i v = _mm_set_epi64x(b, a);\n    uint64_t a_val = _mm_extract_epi64(v, 0);\n    uint64_t b_val = _mm_extract_epi64(v, 1);\n    \n    int shift = 0;\n    while (((a_val | b_val) & 1) == 0) {\n        a_val >>= 1;\n        b_val >>= 1;\n        ++shift;\n    }\n\n    while (a_val != b_val) {\n        while ((a_val & 1) == 0) a_val >>= 1;\n        while ((b_val & 1) == 0) b_val >>= 1;\n        if (a_val > b_val) {\n            a_val -= b_val;\n        } else {\n            b_val -= a_val;\n        }\n    }\n    \n    return a_val << shift;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    uint64_t shift = 0;\n    while (((a | b) & 1) == 0) {\n        a >>= 1;\n        b >>= 1;\n        shift++;\n    }\n\n    while ((a & 1) == 0) {\n        a >>= 1;\n    }\n\n    do {\n        while ((b & 1) == 0) {\n            b >>= 1;\n        }\n        __m256i v = _mm256_set_epi64x(0, 0, b, a);\n        __m256i v_swapped = _mm256_permute4x64_epi64(v, 0xE1);\n        __m256i min_val = _mm256_min_epu64(v, v_swapped);\n        __m256i max_val = _mm256_max_epu64(v, v_swapped);\n        __m256i diff = _mm256_sub_epi64(max_val, min_val);\n        __m256i new_v = _mm256_unpacklo_epi64(min_val, diff);\n        a = _mm256_extract_epi64(new_v, 0);\n        b = _mm256_extract_epi64(new_v, 1);\n    } while (b != 0);\n\n    return a << shift;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    int a_shift = __builtin_ctzll(a);\n    int b_shift = __builtin_ctzll(b);\n    int shift = a_shift < b_shift ? a_shift : b_shift;\n    a >>= a_shift;\n    b >>= b_shift;\n\n    __m256i v = _mm256_set_epi64x(0, 0, b, a);\n\n    while (1) {\n        uint64_t b_val = _mm256_extract_epi64(v, 1);\n        if (b_val == 0) {\n            uint64_t a_val = _mm256_extract_epi64(v, 0);\n            return a_val << shift;\n        }\n\n        uint64_t a_val = _mm256_extract_epi64(v, 0);\n        if (a_val > b_val) {\n            uint64_t temp = a_val;\n            a_val = b_val;\n            b_val = temp;\n            v = _mm256_set_epi64x(0, 0, b_val, a_val);\n        }\n\n        b_val = b_val - a_val;\n        if (b_val == 0) {\n            v = _mm256_set_epi64x(0, 0, b_val, a_val);\n        } else {\n            int tz = __builtin_ctzll(b_val);\n            b_val >>= tz;\n            v = _mm256_set_epi64x(0, 0, b_val, a_val);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    __m256i v = _mm256_set_epi64x(0, 0, b, a);\n    __m256i ones = _mm256_set1_epi64x(1);\n    int shift_count = 0;\n\n    while (1) {\n        __m256i ands = _mm256_and_si256(v, ones);\n        uint64_t a_low = _mm256_extract_epi64(ands, 0);\n        uint64_t b_low = _mm256_extract_epi64(ands, 1);\n        if (a_low == 0 && b_low == 0) {\n            shift_count++;\n            v = _mm256_srli_epi64(v, 1);\n        } else {\n            break;\n        }\n    }\n\n    uint64_t a_val = _mm256_extract_epi64(v, 0);\n    uint64_t b_val = _mm256_extract_epi64(v, 1);\n\n    while (b_val != 0) {\n        uint64_t temp = b_val;\n        b_val = a_val % b_val;\n        a_val = temp;\n    }\n\n    return a_val << shift_count;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    int shift = 0;\n    __m128i vec = _mm_set_epi64x(b, a);\n\n    while (1) {\n        uint64_t u = (uint64_t)_mm_cvtsi128_si64(vec);\n        uint64_t v = (uint64_t)_mm_extract_epi64(vec, 1);\n\n        if (u == 0) return v << shift;\n        if (v == 0) return u << shift;\n\n        __m128i ones = _mm_set1_epi64x(1);\n        __m128i lsb = _mm_and_si128(vec, ones);\n        __m128i is_even = _mm_cmpeq_epi64(lsb, _mm_setzero_si128());\n\n        uint64_t u_even = (uint64_t)_mm_cvtsi128_si64(is_even);\n        uint64_t v_even = (uint64_t)_mm_extract_epi64(is_even, 1);\n\n        if (u_even && v_even) {\n            vec = _mm_srli_epi64(vec, 1);\n            shift++;\n        } else if (u_even) {\n            __m128i shifted = _mm_srli_epi64(vec, 1);\n            __m128i mask = _mm_set_epi64x(-1, 0);\n            vec = _mm_blendv_epi8(shifted, vec, mask);\n        } else if (v_even) {\n            __m128i shifted = _mm_srli_epi64(vec, 1);\n            __m128i mask = _mm_set_epi64x(0, -1);\n            vec = _mm_blendv_epi8(shifted, vec, mask);\n        } else {\n            if (u >= v) {\n                u = (u - v) >> 1;\n            } else {\n                v = (v - u) >> 1;\n            }\n            vec = _mm_set_epi64x(v, u);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <algorithm>\n#include <cfloat>\n#include <vector>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    if (n < 2) {\n        return {};\n    }\n    std::sort(numbers.begin(), numbers.end());\n    float* arr = numbers.data();\n\n    float global_min = FLT_MAX;\n    int global_index = -1;\n\n    int i = 0;\n    for (; i <= n - 9; i += 8) {\n        __m256 a = _mm256_loadu_ps(arr + i);\n        __m256 b = _mm256_loadu_ps(arr + i + 1);\n        __m256 diff = _mm256_sub_ps(b, a);\n        float temp[8];\n        _mm256_storeu_ps(temp, diff);\n\n        for (int j = 0; j < 8; j++) {\n            if (temp[j] < global_min) {\n                global_min = temp[j];\n                global_index = i + j;\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        float diff = arr[i + 1] - arr[i];\n        if (diff < global_min) {\n            global_min = diff;\n            global_index = i;\n        }\n    }\n\n    return { arr[global_index], arr[global_index + 1] };\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <immintrin.h>\n#include <cfloat>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    int n = numbers.size();\n    float global_min = FLT_MAX;\n    int global_index = -1;\n    int i = 0;\n\n    for (; i <= n - 9; i += 8) {\n        __m256 a = _mm256_loadu_ps(&numbers[i]);\n        __m256 b = _mm256_loadu_ps(&numbers[i + 1]);\n        __m256 diff = _mm256_sub_ps(b, a);\n        float temp[8];\n        _mm256_storeu_ps(temp, diff);\n        for (int j = 0; j < 8; j++) {\n            if (temp[j] < global_min) {\n                global_min = temp[j];\n                global_index = i + j;\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        float diff = numbers[i + 1] - numbers[i];\n        if (diff < global_min) {\n            global_min = diff;\n            global_index = i;\n        }\n    }\n\n    return { numbers[global_index], numbers[global_index + 1] };\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <algorithm>\n#include <cfloat>\n#include <vector>\n#include <immintrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    int min_index = 0;\n\n    int i = 0;\n    for (; i <= n - 9; i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i+1]);\n        __m256 diff = _mm256_sub_ps(next, current);\n        float temp[8];\n        _mm256_storeu_ps(temp, diff);\n        for (int j = 0; j < 8; j++) {\n            if (temp[j] < min_diff) {\n                min_diff = temp[j];\n                min_index = i + j;\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        float diff = numbers[i+1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = i;\n        }\n    }\n\n    return {numbers[min_index], numbers[min_index+1]};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <cfloat>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    const int n = numbers.size();\n    if (n == 2) {\n        return {numbers[0], numbers[1]};\n    }\n\n    float min_diff = FLT_MAX;\n    int min_index = -1;\n\n    int i = 0;\n    for (; i <= n - 9; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&numbers[i]);\n        __m256 v2 = _mm256_loadu_ps(&numbers[i+1]);\n        __m256 diff = _mm256_sub_ps(v2, v1);\n        float temp[8];\n        _mm256_storeu_ps(temp, diff);\n\n        for (int j = 0; j < 8; j++) {\n            if (temp[j] < min_diff) {\n                min_diff = temp[j];\n                min_index = i + j;\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        float diff = numbers[i+1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = i;\n        }\n    }\n\n    return { numbers[min_index], numbers[min_index+1] };\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    int n = numbers.size();\n    if (n < 2) {\n        return {};\n    }\n    float min_diff = FLT_MAX;\n    int min_index = 0;\n    __m256 abs_mask = _mm256_set1_ps(-0.0f);\n    int i = 0;\n    for (; i <= n - 9; i += 8) {\n        __m256 a = _mm256_loadu_ps(&numbers[i]);\n        __m256 b = _mm256_loadu_ps(&numbers[i+1]);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_andnot_ps(abs_mask, diff);\n        float temp[8];\n        _mm256_storeu_ps(temp, abs_diff);\n        for (int j = 0; j < 8; j++) {\n            if (temp[j] < min_diff) {\n                min_diff = temp[j];\n                min_index = i + j;\n            }\n        }\n    }\n    for (int j = i; j < n - 1; j++) {\n        float diff = std::fabs(numbers[j] - numbers[j+1]);\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = j;\n        }\n    }\n    return {numbers[min_index], numbers[min_index+1]};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n    float min_val, max_val;\n    int i = 0;\n\n    if (n < 8) {\n        min_val = numbers[0];\n        max_val = numbers[0];\n        for (i = 1; i < n; ++i) {\n            if (numbers[i] < min_val) min_val = numbers[i];\n            if (numbers[i] > max_val) max_val = numbers[i];\n        }\n    } else {\n        __m256 min_acc = _mm256_loadu_ps(&numbers[0]);\n        __m256 max_acc = min_acc;\n        i = 8;\n        for (; i <= n - 8; i += 8) {\n            __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n            min_acc = _mm256_min_ps(min_acc, chunk);\n            max_acc = _mm256_max_ps(max_acc, chunk);\n        }\n        float min_arr[8], max_arr[8];\n        _mm256_storeu_ps(min_arr, min_acc);\n        _mm256_storeu_ps(max_arr, max_acc);\n        min_val = min_arr[0];\n        max_val = max_arr[0];\n        for (int j = 1; j < 8; ++j) {\n            if (min_arr[j] < min_val) min_val = min_arr[j];\n            if (max_arr[j] > max_val) max_val = max_arr[j];\n        }\n        for (int j = i; j < n; ++j) {\n            if (numbers[j] < min_val) min_val = numbers[j];\n            if (numbers[j] > max_val) max_val = numbers[j];\n        }\n    }\n\n    float scale = (max_val == min_val) ? 0.0f : 1.0f / (max_val - min_val);\n    std::vector<float> result(n);\n    i = 0;\n\n    if (n >= 8) {\n        __m256 vmin = _mm256_set1_ps(min_val);\n        __m256 vscale = _mm256_set1_ps(scale);\n        for (; i <= n - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&numbers[i]);\n            __m256 transformed = _mm256_mul_ps(_mm256_sub_ps(vec, vmin), vscale);\n            _mm256_storeu_ps(&result[i], transformed);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n    \n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    size_t i = 0;\n    \n    if (n >= 8) {\n        __m256 vmin = _mm256_loadu_ps(&numbers[0]);\n        __m256 vmax = vmin;\n        \n        for (i = 8; i <= n - 8; i += 8) {\n            __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n            vmin = _mm256_min_ps(vmin, chunk);\n            vmax = _mm256_max_ps(vmax, chunk);\n        }\n        \n        alignas(32) float temp_min[8], temp_max[8];\n        _mm256_store_ps(temp_min, vmin);\n        _mm256_store_ps(temp_max, vmax);\n        \n        for (int j = 0; j < 8; ++j) {\n            if (temp_min[j] < min_val) min_val = temp_min[j];\n            if (temp_max[j] > max_val) max_val = temp_max[j];\n        }\n    }\n    \n    for (; i < n; ++i) {\n        if (numbers[i] < min_val) min_val = numbers[i];\n        if (numbers[i] > max_val) max_val = numbers[i];\n    }\n    \n    float scale = (max_val != min_val) ? 1.0f / (max_val - min_val) : 0.0f;\n    __m256 vmin_bc = _mm256_set1_ps(min_val);\n    __m256 vscale_bc = _mm256_set1_ps(scale);\n    \n    std::vector<float> result(n);\n    i = 0;\n    \n    for (; i <= n - 8; i += 8) {\n        __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n        __m256 adjusted = _mm256_sub_ps(chunk, vmin_bc);\n        __m256 scaled = _mm256_mul_ps(adjusted, vscale_bc);\n        _mm256_storeu_ps(&result[i], scaled);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    size_t i = 0;\n\n    if (n >= 8) {\n        __m256 min_vec = _mm256_loadu_ps(&numbers[0]);\n        __m256 max_vec = min_vec;\n        i = 8;\n\n        for (; i <= n - 8; i += 8) {\n            __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n            min_vec = _mm256_min_ps(min_vec, chunk);\n            max_vec = _mm256_max_ps(max_vec, chunk);\n        }\n\n        __m128 low_min = _mm256_castps256_ps128(min_vec);\n        __m128 high_min = _mm256_extractf128_ps(min_vec, 1);\n        __m128 min128 = _mm_min_ps(low_min, high_min);\n        __m128 min1 = _mm_min_ps(min128, _mm_shuffle_ps(min128, min128, _MM_SHUFFLE(1, 0, 3, 2)));\n        __m128 min2 = _mm_min_ps(min1, _mm_shuffle_ps(min1, min1, _MM_SHUFFLE(2, 3, 0, 1)));\n        min_val = _mm_cvtss_f32(min2);\n\n        __m128 low_max = _mm256_castps256_ps128(max_vec);\n        __m128 high_max = _mm256_extractf128_ps(max_vec, 1);\n        __m128 max128 = _mm_max_ps(low_max, high_max);\n        __m128 max1 = _mm_max_ps(max128, _mm_shuffle_ps(max128, max128, _MM_SHUFFLE(1, 0, 3, 2)));\n        __m128 max2 = _mm_max_ps(max1, _mm_shuffle_ps(max1, max1, _MM_SHUFFLE(2, 3, 0, 1)));\n        max_val = _mm_cvtss_f32(max2);\n    }\n\n    for (; i < n; ++i) {\n        if (numbers[i] < min_val) min_val = numbers[i];\n        if (numbers[i] > max_val) max_val = numbers[i];\n    }\n\n    float range = max_val - min_val;\n    float scale = (range != 0.0f) ? (1.0f / range) : 0.0f;\n\n    std::vector<float> result(n);\n    __m256 min_bc = _mm256_set1_ps(min_val);\n    __m256 scale_bc = _mm256_set1_ps(scale);\n    i = 0;\n\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n            __m256 res = _mm256_sub_ps(chunk, min_bc);\n            res = _mm256_mul_ps(res, scale_bc);\n            _mm256_storeu_ps(&result[i], res);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return std::vector<float>();\n    }\n    \n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    size_t i = 0;\n\n    if (n >= 8) {\n        __m256 min_reg = _mm256_set1_ps(min_val);\n        __m256 max_reg = _mm256_set1_ps(max_val);\n        for (; i <= n - 8; i += 8) {\n            __m256 data = _mm256_loadu_ps(&numbers[i]);\n            min_reg = _mm256_min_ps(min_reg, data);\n            max_reg = _mm256_max_ps(max_reg, data);\n        }\n\n        float min_temp[8], max_temp[8];\n        _mm256_storeu_ps(min_temp, min_reg);\n        _mm256_storeu_ps(max_temp, max_reg);\n        for (int j = 0; j < 8; ++j) {\n            if (min_temp[j] < min_val) min_val = min_temp[j];\n            if (max_temp[j] > max_val) max_val = max_temp[j];\n        }\n    }\n\n    size_t start = (n >= 8) ? i : 1;\n    for (size_t j = start; j < n; ++j) {\n        float val = numbers[j];\n        if (val < min_val) min_val = val;\n        if (val > max_val) max_val = val;\n    }\n\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        return std::vector<float>(n, 0.5f);\n    }\n\n    float scale = 1.0f / range;\n    std::vector<float> result(n);\n    i = 0;\n\n    if (n >= 8) {\n        __m256 vmin = _mm256_set1_ps(min_val);\n        __m256 vscale = _mm256_set1_ps(scale);\n        for (; i <= n - 8; i += 8) {\n            __m256 data = _mm256_loadu_ps(&numbers[i]);\n            data = _mm256_sub_ps(data, vmin);\n            data = _mm256_mul_ps(data, vscale);\n            _mm256_storeu_ps(&result[i], data);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    if (n == 0) {\n        return numbers;\n    }\n\n    __m256 min_reg = _mm256_set1_ps(FLT_MAX);\n    __m256 max_reg = _mm256_set1_ps(-FLT_MAX);\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n        min_reg = _mm256_min_ps(min_reg, chunk);\n        max_reg = _mm256_max_ps(max_reg, chunk);\n    }\n\n    __m128 min_low = _mm256_castps256_ps128(min_reg);\n    __m128 min_high = _mm256_extractf128_ps(min_reg, 1);\n    __m128 min128 = _mm_min_ps(min_low, min_high);\n    __m128 min_shuf = _mm_movehl_ps(min128, min128);\n    min128 = _mm_min_ps(min128, min_shuf);\n    min_shuf = _mm_shuffle_ps(min128, min128, 1);\n    min128 = _mm_min_ss(min128, min_shuf);\n    float min_scalar = _mm_cvtss_f32(min128);\n\n    __m128 max_low = _mm256_castps256_ps128(max_reg);\n    __m128 max_high = _mm256_extractf128_ps(max_reg, 1);\n    __m128 max128 = _mm_max_ps(max_low, max_high);\n    __m128 max_shuf = _mm_movehl_ps(max128, max128);\n    max128 = _mm_max_ps(max128, max_shuf);\n    max_shuf = _mm_shuffle_ps(max128, max128, 1);\n    max128 = _mm_max_ss(max128, max_shuf);\n    float max_scalar = _mm_cvtss_f32(max128);\n\n    for (int j = i; j < n; j++) {\n        if (numbers[j] < min_scalar) min_scalar = numbers[j];\n        if (numbers[j] > max_scalar) max_scalar = numbers[j];\n    }\n\n    if (min_scalar == max_scalar) {\n        for (int j = 0; j < n; j++) {\n            numbers[j] = 0.5f;\n        }\n    } else {\n        float scale = 1.0f / (max_scalar - min_scalar);\n        __m256 min_broadcast = _mm256_set1_ps(min_scalar);\n        __m256 scale_broadcast = _mm256_set1_ps(scale);\n\n        i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256 chunk = _mm256_loadu_ps(&numbers[i]);\n            __m256 res = _mm256_sub_ps(chunk, min_broadcast);\n            res = _mm256_mul_ps(res, scale_broadcast);\n            _mm256_storeu_ps(&numbers[i], res);\n        }\n\n        for (int j = i; j < n; j++) {\n            numbers[j] = (numbers[j] - min_scalar) * scale;\n        }\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "To solve this problem, we need to find the largest divisor of a given positive number `n` that is smaller than `n` itself. The solution leverages SIMD (Single Instruction, Multiple Data) instructions, specifically AVX/AVX2 intrinsics, to parallelize the computation and improve efficiency.\n\n### Approach\n1. **Problem Analysis**: The largest divisor of `n` (other than `n` itself) is `n` divided by its smallest prime factor. If `n` is a prime number, the largest divisor is 1.\n2. **Key Insight**: The smallest factor of `n` (if it exists) must be between 2 and the square root of `n`. If no factor is found in this range, `n` is prime.\n3. **Algorithm Selection**: \n   - **Initial Checks**: Handle small values of `n` (0, 1, 2, 3) directly.\n   - **Finding Smallest Factor**: Iterate from 2 up to the square root of `n` in steps of 4, checking each candidate divisor in parallel.\n   - **SIMD Utilization**: For each set of 4 consecutive candidates, compute the remainders when `n` is divided by each candidate. Use AVX2 intrinsics to compare these remainders to zero in parallel.\n   - **Early Termination**: If any remainder is zero, the corresponding candidate is a factor. The smallest such candidate is used to compute the largest divisor as `n / candidate`.\n   - **Prime Handling**: If no factors are found, `n` is prime, and the largest divisor is 1.\n\n### Solution Code\n```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) {\n        return 0;\n    }\n    if (n <= 3) {\n        return 1;\n    }\n    \n    uint64_t sqrt_n = static_cast<uint64_t>(std::sqrt(n));\n    uint64_t k = 2;\n    while (k <= sqrt_n) {\n        uint64_t remaining = sqrt_n - k + 1;\n        if (remaining > 4) {\n            remaining = 4;\n        }\n        \n        uint64_t candidates[4] = {0, 0, 0, 0};\n        uint64_t mods[4] = {1, 1, 1, 1};\n        \n        for (uint64_t i = 0; i < remaining; i++) {\n            candidates[i] = k + i;\n            mods[i] = n % candidates[i];\n        }\n        \n        __m256i mod_vec = _mm256_loadu_si256((__m256i const*)mods);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpeq_epi64(mod_vec, zero);\n        \n        if (_mm256_movemask_epi8(mask) != 0) {\n            for (uint64_t i = 0; i < remaining; i++) {\n                if (mods[i] == 0) {\n                    return n / candidates[i];\n                }\n            }\n        }\n        \n        k += remaining;\n    }\n    \n    return 1;\n}\n```\n\n### Explanation\n1. **Initial Checks**: The function first checks if `n` is 0 or 1 (returning 0) or 2 or 3 (returning 1), as these cases have straightforward results.\n2. **Square Root Calculation**: Computes the integer square root of `n` to determine the upper bound for potential factors.\n3. **Loop Through Candidates**: The loop processes candidates in batches of 4. For each batch, it calculates the remainders of `n` divided by each candidate.\n4. **SIMD Comparison**: Uses AVX2 intrinsics to load the remainders into a vector and compare them to zero in parallel. If any remainder is zero, the corresponding candidate is a valid factor.\n5. **Result Calculation**: The first valid factor encountered (smallest in the batch) is used to compute the largest divisor as `n / candidate`. If no factors are found, the function returns 1, indicating `n` is prime.\n\nThis approach efficiently leverages parallel processing to check multiple candidates simultaneously, significantly speeding up the factorization process for large numbers."}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n == 1) {\n        return 0;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n    uint64_t sqrt_n = (uint64_t)sqrt(n);\n    if (sqrt_n * sqrt_n < n) {\n        sqrt_n++;\n    }\n    for (uint64_t d = 3; d <= sqrt_n; d += 8) {\n        uint64_t divisors[4] = {d, d+2, d+4, d+6};\n        uint64_t rems[4] = {1, 1, 1, 1};\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] <= sqrt_n) {\n                rems[i] = n % divisors[i];\n            }\n        }\n        __m256i rem_vec = _mm256_loadu_si256((const __m256i*)rems);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(rem_vec, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            for (int i = 0; i < 4; i++) {\n                if (rems[i] == 0) {\n                    return n / divisors[i];\n                }\n            }\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n == 1) {\n        return 0;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n\n    uint64_t d = 3;\n    while (d <= n / d) {\n        uint64_t divisors[4] = {d, d+2, d+4, d+6};\n        __m256i div_vec = _mm256_set_epi64x(divisors[3], divisors[2], divisors[1], divisors[0]);\n\n        for (int i = 0; i < 4; i++) {\n            uint64_t di = _mm256_extract_epi64(div_vec, i);\n            if (di > n / di) {\n                break;\n            }\n            if (n % di == 0) {\n                return n / di;\n            }\n        }\n        d += 8;\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n % 2 == 0) {\n        if (n == 2) {\n            return 1;\n        } else {\n            return n / 2;\n        }\n    }\n    \n    uint64_t low = 1;\n    uint64_t high = (1ULL << 32);\n    if (high > n) {\n        high = n;\n    }\n    while (low <= high) {\n        uint64_t mid = low + (high - low) / 2;\n        if (mid > n / mid) {\n            high = mid - 1;\n        } else {\n            low = mid + 1;\n        }\n    }\n    uint64_t limit = high;\n    \n    uint64_t candidate = 0;\n    uint64_t d = 3;\n    while (d <= limit) {\n        uint64_t divisors[4] = {d, d+2, d+4, d+6};\n        uint64_t rems[4] = {0};\n        int valid_count = 4;\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] > limit) {\n                valid_count = i;\n                break;\n            }\n        }\n        for (int i = 0; i < valid_count; i++) {\n            rems[i] = n % divisors[i];\n        }\n        for (int i = valid_count; i < 4; i++) {\n            rems[i] = 1;\n        }\n        \n        __m256i rems_vec = _mm256_set_epi64x(rems[3], rems[2], rems[1], rems[0]);\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(rems_vec, zeros);\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(cmp));\n        \n        if (mask) {\n            for (int i = 0; i < valid_count; i++) {\n                if (rems[i] == 0) {\n                    candidate = divisors[i];\n                    goto found;\n                }\n            }\n        }\n        d += 8;\n    }\n    return 1;\n    \nfound:\n    return n / candidate;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n == 1) {\n        return 0;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n    for (uint64_t base = 3; ; base += 8) {\n        if (base > n / base) {\n            break;\n        }\n        uint64_t divisors[4] = { base, base+2, base+4, base+6 };\n        uint64_t rems[4] = { 1, 1, 1, 1 };\n\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] > n / divisors[i]) {\n                continue;\n            }\n            rems[i] = n % divisors[i];\n        }\n\n        __m256i rem_vec = _mm256_loadu_si256((const __m256i*)rems);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(rem_vec, zero_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask) {\n            for (int i = 0; i < 4; i++) {\n                if (divisors[i] > n / divisors[i]) {\n                    continue;\n                }\n                if (rems[i] == 0) {\n                    return n / divisors[i];\n                }\n            }\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\ninline int check_block4(int n, int* cands) {\n    __m128i cand_vec = _mm_loadu_si128((const __m128i*)cands);\n    __m256d cand_d = _mm256_cvtepi32_pd(cand_vec);\n    __m256d n_vec = _mm256_set1_pd(static_cast<double>(n));\n    __m256d div = _mm256_div_pd(n_vec, cand_d);\n    __m256d div_round = _mm256_round_pd(div, _MM_FROUND_TO_ZERO);\n    __m128i div_int = _mm256_cvttpd_epi32(div_round);\n    __m128i product = _mm_mullo_epi32(div_int, cand_vec);\n    __m128i rem_vec = _mm_sub_epi32(_mm_set1_epi32(n), product);\n    int rem[4];\n    _mm_storeu_si128((__m128i*)rem, rem_vec);\n    for (int i = 0; i < 4; ++i) {\n        if (rem[i] == 0) {\n            return cands[i];\n        }\n    }\n    return 0;\n}\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n    int f = 3;\n    while (n > 1) {\n        if (static_cast<long long>(f) * f > n) {\n            factors.push_back(n);\n            break;\n        }\n        int cands1[4] = {f, f+2, f+4, f+6};\n        int cands2[4] = {f+8, f+10, f+12, f+14};\n        int divisor = check_block4(n, cands1);\n        if (!divisor) {\n            divisor = check_block4(n, cands2);\n        }\n        if (divisor) {\n            factors.push_back(divisor);\n            n /= divisor;\n            continue;\n        }\n        f += 16;\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    if (n == 1) {\n        return factors;\n    }\n\n    int d = 3;\n    while (n > 1) {\n        int max_div = static_cast<int>(std::sqrt(n));\n        if (d > max_div) {\n            factors.push_back(n);\n            break;\n        }\n\n        bool found = false;\n        for (int base = d; base <= max_div; ) {\n            int block_end = base + 6;\n            int count = 4;\n            if (block_end > max_div) {\n                count = (max_div - base) / 2 + 1;\n            }\n\n            __m128i divisors_i;\n            if (count == 4) {\n                divisors_i = _mm_set_epi32(base+6, base+4, base+2, base);\n            } else if (count == 3) {\n                divisors_i = _mm_set_epi32(1, base+4, base+2, base);\n            } else if (count == 2) {\n                divisors_i = _mm_set_epi32(1, 1, base+2, base);\n            } else {\n                divisors_i = _mm_set_epi32(1, 1, 1, base);\n            }\n\n            __m256d divisors_d = _mm256_cvtepi32_pd(divisors_i);\n            __m256d n_vec = _mm256_set1_pd(static_cast<double>(n));\n            __m256d quotients_d = _mm256_div_pd(n_vec, divisors_d);\n            __m256d quotients_trunc = _mm256_round_pd(quotients_d, _MM_FROUND_TO_ZERO);\n            __m256d products = _mm256_mul_pd(divisors_d, quotients_trunc);\n            __m256d remainders_d = _mm256_sub_pd(n_vec, products);\n\n            __m256d zero = _mm256_setzero_pd();\n            __m256d mask_eq = _mm256_cmp_pd(remainders_d, zero, _CMP_EQ_OQ);\n\n            __m256i valid_mask_i;\n            if (count == 4) {\n                valid_mask_i = _mm256_set_epi64x(~0LL, ~0LL, ~0LL, ~0LL);\n            } else if (count == 3) {\n                valid_mask_i = _mm256_set_epi64x(0, ~0LL, ~0LL, ~0LL);\n            } else if (count == 2) {\n                valid_mask_i = _mm256_set_epi64x(0, 0, ~0LL, ~0LL);\n            } else {\n                valid_mask_i = _mm256_set_epi64x(0, 0, 0, ~0LL);\n            }\n            __m256d valid_mask = _mm256_castsi256_pd(valid_mask_i);\n            __m256d final_mask = _mm256_and_pd(mask_eq, valid_mask);\n            int mask = _mm256_movemask_pd(final_mask);\n\n            if (mask) {\n                for (int idx = 0; idx < count; ++idx) {\n                    if (mask & (1 << idx)) {\n                        int divisor = base + 2 * idx;\n                        factors.push_back(divisor);\n                        n /= divisor;\n                        found = true;\n                        break;\n                    }\n                }\n                break;\n            } else {\n                base += 8;\n            }\n        }\n\n        if (found) {\n            continue;\n        }\n\n        factors.push_back(n);\n        break;\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    if (n == 1) {\n        return factors;\n    }\n\n    int f = 3;\n    while (n > 1) {\n        if (f * f > n) {\n            factors.push_back(n);\n            break;\n        }\n\n        __m256i divisors = _mm256_setr_epi32(f, f+2, f+4, f+6, f+8, f+10, f+12, f+14);\n        __m256 divs_f = _mm256_cvtepi32_ps(divisors);\n        __m256 n_vec = _mm256_set1_ps(static_cast<float>(n));\n        __m256 quot_f = _mm256_div_ps(n_vec, divs_f);\n        __m256i quot_i = _mm256_cvttps_epi32(quot_f);\n        __m256i product = _mm256_mullo_epi32(quot_i, divisors);\n        __m256i rem_vec = _mm256_sub_epi32(_mm256_set1_epi32(n), product);\n        __m256i mask = _mm256_cmpeq_epi32(rem_vec, _mm256_setzero_si256());\n\n        int mask_arr[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(mask_arr), mask);\n\n        bool found = false;\n        for (int i = 0; i < 8; i++) {\n            if (mask_arr[i] != 0) {\n                int divisor = f + 2 * i;\n                factors.push_back(divisor);\n                n /= divisor;\n                found = true;\n                break;\n            }\n        }\n\n        if (!found) {\n            f += 16;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n    int d = 3;\n    while (n > 1) {\n        if (d > n / d) {\n            factors.push_back(n);\n            break;\n        }\n        __m256i divisors = _mm256_setr_epi32(d, d+2, d+4, d+6, d+8, d+10, d+12, d+14);\n        __m128i div_lo = _mm256_extracti128_si256(divisors, 0);\n        __m128i div_hi = _mm256_extracti128_si256(divisors, 1);\n        \n        __m256d div_lo_d = _mm256_cvtepi32_pd(div_lo);\n        __m256d n_lo_d = _mm256_set1_pd((double)n);\n        __m256d quot_lo_d = _mm256_div_pd(n_lo_d, div_lo_d);\n        __m128i quot_lo_i = _mm256_cvttpd_epi32(quot_lo_d);\n        __m128i product_lo = _mm_mullo_epi32(quot_lo_i, div_lo);\n        __m128i rem_lo = _mm_sub_epi32(_mm_set1_epi32(n), product_lo);\n        \n        __m256d div_hi_d = _mm256_cvtepi32_pd(div_hi);\n        __m256d n_hi_d = _mm256_set1_pd((double)n);\n        __m256d quot_hi_d = _mm256_div_pd(n_hi_d, div_hi_d);\n        __m128i quot_hi_i = _mm256_cvttpd_epi32(quot_hi_d);\n        __m128i product_hi = _mm_mullo_epi32(quot_hi_i, div_hi);\n        __m128i rem_hi = _mm_sub_epi32(_mm_set1_epi32(n), product_hi);\n        \n        int rems[8];\n        _mm_storeu_si128((__m128i*)rems, rem_lo);\n        _mm_storeu_si128((__m128i*)(rems+4), rem_hi);\n        \n        bool found = false;\n        int found_divisor = 0;\n        for (int i = 0; i < 8; i++) {\n            int current_divisor = d + 2*i;\n            if (rems[i] == 0) {\n                found = true;\n                found_divisor = current_divisor;\n                break;\n            }\n        }\n        if (found) {\n            factors.push_back(found_divisor);\n            n /= found_divisor;\n        } else {\n            d += 16;\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) {\n        return factors;\n    }\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    if (n == 1) {\n        return factors;\n    }\n\n    int f = 3;\n    while (n > 1) {\n        if (static_cast<long long>(f) * f > n) {\n            factors.push_back(n);\n            break;\n        }\n\n        __m256i vdiv = _mm256_set_epi32(f, f+2, f+4, f+6, f+8, f+10, f+12, f+14);\n        __m256i vn = _mm256_set1_epi32(n);\n        __m256 vn_float = _mm256_set1_ps(static_cast<float>(n));\n        __m256 vdiv_float = _mm256_cvtepi32_ps(vdiv);\n        __m256 vquot = _mm256_div_ps(vn_float, vdiv_float);\n        vquot = _mm256_floor_ps(vquot);\n        __m256i vquot_int = _mm256_cvtps_epi32(vquot);\n        __m256i vproduct = _mm256_mullo_epi32(vquot_int, vdiv);\n        __m256i vrem = _mm256_sub_epi32(vn, vproduct);\n        __m256i vmask = _mm256_cmpeq_epi32(vrem, _mm256_setzero_si256());\n\n        int mask_arr[8];\n        int div_arr[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(mask_arr), vmask);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(div_arr), vdiv);\n\n        bool found_factor = false;\n        for (int i = 0; i < 8; ++i) {\n            long long d = div_arr[i];\n            if (d * d > static_cast<long long>(n)) {\n                factors.push_back(n);\n                n = 1;\n                found_factor = true;\n                break;\n            }\n            if (mask_arr[i] != 0) {\n                factors.push_back(d);\n                n /= d;\n                found_factor = true;\n                break;\n            }\n        }\n\n        if (n == 1) {\n            break;\n        }\n\n        if (!found_factor) {\n            f += 16;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> output;\n    output.reserve(l.size());\n\n    size_t i = 0;\n    for (; i + 8 <= l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        int bitmask = _mm256_movemask_ps(mask);\n        float buffer[8];\n        _mm256_storeu_ps(buffer, vec);\n        for (int j = 0; j < 8; j++) {\n            if (bitmask & (1 << j)) {\n                output.push_back(buffer[j]);\n            }\n        }\n    }\n\n    for (; i < l.size(); i++) {\n        if (l[i] > 0) {\n            output.push_back(l[i]);\n        }\n    }\n\n    return output;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    static const struct Tables {\n        uint32_t shuffle[256][8];\n        uint8_t popcount[256];\n        constexpr Tables() : shuffle(), popcount() {\n            for (int m = 0; m < 256; m++) {\n                int count = 0;\n                for (int i = 0; i < 8; i++) {\n                    if (m & (1 << i)) {\n                        shuffle[m][count] = i;\n                        count++;\n                    }\n                }\n                popcount[m] = count;\n                for (; count < 8; count++) {\n                    shuffle[m][count] = 0;\n                }\n            }\n        }\n    } tables;\n\n    std::vector<float> res;\n    res.reserve(l.size());\n    const float* data = l.data();\n    size_t n = l.size();\n    size_t i = 0;\n\n    for (; i + 7 < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        __m256 cmp = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OS);\n        int mask = _mm256_movemask_ps(cmp);\n        __m256i perm = _mm256_loadu_si256((const __m256i*)tables.shuffle[mask]);\n        __m256 selected = _mm256_permutevar8x32_ps(vec, perm);\n        int count = tables.popcount[mask];\n        float buffer[8];\n        _mm256_storeu_ps(buffer, selected);\n        for (int j = 0; j < count; j++) {\n            res.push_back(buffer[j]);\n        }\n    }\n\n    for (; i < n; i++) {\n        if (l[i] > 0) {\n            res.push_back(l[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    static int32_t perm_table[256][8];\n    static bool initialized = false;\n    if (!initialized) {\n        for (int mask = 0; mask < 256; mask++) {\n            int j = 0;\n            for (int i = 0; i < 8; i++) {\n                if (mask & (1 << i)) {\n                    perm_table[mask][j] = i;\n                    j++;\n                }\n            }\n            for (; j < 8; j++) {\n                perm_table[mask][j] = 0;\n            }\n        }\n        initialized = true;\n    }\n\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    size_t i = 0;\n    const size_t n = l.size();\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask_vec = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        int mask = _mm256_movemask_ps(mask_vec);\n        int popcount = __builtin_popcount(mask);\n\n        if (popcount > 0) {\n            __m256i perm_idx = _mm256_loadu_si256((const __m256i*)(perm_table[mask]));\n            __m256 selected = _mm256_permutevar8x32_ps(vec, perm_idx);\n            float temp[8];\n            _mm256_storeu_ps(temp, selected);\n\n            for (int j = 0; j < popcount; j++) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    static const int (*permTable)[8] = [] {\n        static int table[256][8];\n        for (int m = 0; m < 256; m++) {\n            int pos_list[8], neg_list[8];\n            int p = 0, n = 0;\n            for (int i = 0; i < 8; i++) {\n                if (m & (1 << i)) {\n                    pos_list[p++] = i;\n                } else {\n                    neg_list[n++] = i;\n                }\n            }\n            int j = 0;\n            for (int i = 0; i < p; i++) {\n                table[m][j++] = pos_list[i];\n            }\n            for (int i = 0; i < n; i++) {\n                table[m][j++] = neg_list[i];\n            }\n        }\n        return (const int (*)[8])table;\n    }();\n\n    std::vector<float> res;\n    const size_t n = l.size();\n    if (n == 0) return res;\n    res.reserve(n);\n\n    const float* data = l.data();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(data + i);\n        __m256 cmp = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        int mask = _mm256_movemask_ps(cmp);\n        const int* indices = permTable[mask];\n        __m256i perm_idx = _mm256_loadu_si256((const __m256i*)indices);\n        __m256 permuted = _mm256_permutevar8x32_ps(vec, perm_idx);\n        int k = __builtin_popcount(mask);\n        float temp[8];\n        _mm256_storeu_ps(temp, permuted);\n        res.insert(res.end(), temp, temp + k);\n    }\n\n    for (; i < n; i++) {\n        if (data[i] > 0.0f) {\n            res.push_back(data[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> res;\n    size_t i = 0;\n    const size_t size = l.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&l[i]);\n            __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n            int bitmask = _mm256_movemask_ps(mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (bitmask & (1 << j)) {\n                    res.push_back(l[i + j]);\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            res.push_back(l[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) \n        return false;\n    if (n == 2) \n        return true;\n    if (n % 2 == 0) \n        return false;\n\n    int64_t sqrt_n = static_cast<int64_t>(sqrt(static_cast<double>(n)));\n    if (sqrt_n * sqrt_n < n) \n        sqrt_n++;\n\n    for (int64_t d = 3; d <= sqrt_n; d += 8) {\n        int64_t divisors[4] = {d, d + 2, d + 4, d + 6};\n        uint64_t remainders[4] = {1, 1, 1, 1};\n        int count = 0;\n\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] > sqrt_n) \n                break;\n            count++;\n            remainders[i] = n % divisors[i];\n        }\n\n        if (count == 0) \n            break;\n\n        __m256i rem_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(remainders));\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(rem_vec, zero);\n\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) \n        return false;\n    if (n == 2) \n        return true;\n    if (n % 2 == 0) \n        return false;\n\n    unsigned long long un = (unsigned long long)n;\n    double sqrt_val = sqrt((double)un);\n    unsigned long long limit = (unsigned long long)sqrt_val;\n\n    // Adjust limit to be the floor of the square root\n    if (limit * limit > un) {\n        limit--;\n    }\n    while ((limit + 1) * (limit + 1) <= un) {\n        limit++;\n    }\n\n    if (limit < 3) \n        return true;\n\n    for (unsigned long long d = 3; d <= limit - 14; d += 16) {\n        unsigned long long divisors[8] = { d, d+2, d+4, d+6, d+8, d+10, d+12, d+14 };\n        unsigned int rems[8] = {0};\n\n        for (int j = 0; j < 8; j++) {\n            rems[j] = (unsigned int)(un % divisors[j]);\n        }\n\n        __m256i rem_vec = _mm256_loadu_si256((const __m256i*)rems);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(rem_vec, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n\n    for (unsigned long long i = (limit - 14) + 16; i <= limit; i += 2) {\n        if (un % i == 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n\n    int64_t sqrt_n = 0;\n    if (n > 0) {\n        int64_t low = 1, high = n;\n        if (n > 10000000000) {\n            high = (1LL << 32) - 1;\n            if (high > n) high = n;\n        }\n        while (low <= high) {\n            int64_t mid = low + (high - low) / 2;\n            if (mid > 0 && mid <= n / mid) {\n                sqrt_n = mid;\n                low = mid + 1;\n            } else {\n                high = mid - 1;\n            }\n        }\n    }\n\n    bool found_exceeding = false;\n    for (int64_t d = 3; d <= sqrt_n && !found_exceeding; d += 8) {\n        __m256i div_vec = _mm256_set_epi64x(d+6, d+4, d+2, d);\n        for (int i = 0; i < 4; i++) {\n            int64_t di = _mm256_extract_epi64(div_vec, i);\n            if (di > sqrt_n) {\n                found_exceeding = true;\n                break;\n            }\n            if (n % di == 0) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n    \n    int64_t limit = (int64_t)(sqrt((double)n)) + 1;\n    int64_t base = 3;\n    __m256i v_offsets = _mm256_setr_epi64x(0, 2, 4, 6);\n    \n    while (base <= limit) {\n        __m256i v_base_vec = _mm256_set1_epi64x(base);\n        __m256i v_divs = _mm256_add_epi64(v_base_vec, v_offsets);\n        \n        for (int j = 0; j < 4; j++) {\n            int64_t div = _mm256_extract_epi64(v_divs, j);\n            if (div > limit) \n                break;\n            if (n % div == 0) \n                return false;\n        }\n        base += 8;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool is_prime_simd(int64_t n) {\n    if (n < 2) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n\n    int64_t limit = static_cast<int64_t>(std::sqrt(n));\n\n    for (int64_t base = 3; base <= limit; base += 8) {\n        __m256i v_base = _mm256_set1_epi64x(base);\n        __m256i v_offsets = _mm256_setr_epi64x(0, 2, 4, 6);\n        __m256i v_divisors = _mm256_add_epi64(v_base, v_offsets);\n\n        int64_t divisors[4];\n        _mm256_storeu_si256((__m256i*)divisors, v_divisors);\n\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] > limit) break;\n            if (n % divisors[i] == 0) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n == 0) return s;\n\n    bool has_letter = false;\n    size_t i = 0;\n    for (; i + 31 < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i a_minus = _mm256_set1_epi8('A' - 1);\n        __m256i z_val = _mm256_set1_epi8('Z');\n        __m256i cmp_ge_A = _mm256_cmpgt_epi8(chunk, a_minus);\n        __m256i cmp_le_Z = _mm256_cmpgt_epi8(z_val, chunk);\n        __m256i uppercase_mask = _mm256_and_si256(cmp_ge_A, cmp_le_Z);\n\n        __m256i a_lower = _mm256_set1_epi8('a' - 1);\n        __m256i z_lower_val = _mm256_set1_epi8('z');\n        __m256i cmp_ge_a = _mm256_cmpgt_epi8(chunk, a_lower);\n        __m256i cmp_le_z = _mm256_cmpgt_epi8(z_lower_val, chunk);\n        __m256i lowercase_mask = _mm256_and_si256(cmp_ge_a, cmp_le_z);\n\n        __m256i letter_mask = _mm256_or_si256(uppercase_mask, lowercase_mask);\n        if (_mm256_movemask_epi8(letter_mask) != 0) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        for (; i < n; i++) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n\n    if (has_letter) {\n        std::string result;\n        result.resize(n);\n        size_t j = 0;\n        for (; j + 31 < n; j += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + j));\n            __m256i a_minus = _mm256_set1_epi8('A' - 1);\n            __m256i z_val = _mm256_set1_epi8('Z');\n            __m256i cmp_ge_A = _mm256_cmpgt_epi8(chunk, a_minus);\n            __m256i cmp_le_Z = _mm256_cmpgt_epi8(z_val, chunk);\n            __m256i uppercase_mask = _mm256_and_si256(cmp_ge_A, cmp_le_Z);\n\n            __m256i a_lower = _mm256_set1_epi8('a' - 1);\n            __m256i z_lower_val = _mm256_set1_epi8('z');\n            __m256i cmp_ge_a = _mm256_cmpgt_epi8(chunk, a_lower);\n            __m256i cmp_le_z = _mm256_cmpgt_epi8(z_lower_val, chunk);\n            __m256i lowercase_mask = _mm256_and_si256(cmp_ge_a, cmp_le_z);\n\n            __m256i letter_mask = _mm256_or_si256(uppercase_mask, lowercase_mask);\n            __m256i flip_vector = _mm256_and_si256(letter_mask, _mm256_set1_epi8(0x20));\n            __m256i flipped_chunk = _mm256_xor_si256(chunk, flip_vector);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[j]), flipped_chunk);\n        }\n        for (; j < n; j++) {\n            char c = s[j];\n            if (c >= 'A' && c <= 'Z') {\n                result[j] = c + 32;\n            } else if (c >= 'a' && c <= 'z') {\n                result[j] = c - 32;\n            } else {\n                result[j] = c;\n            }\n        }\n        return result;\n    } else {\n        std::string result = s;\n        size_t left = 0, right = n - 1;\n        while (left < right) {\n            char tmp = result[left];\n            result[left] = result[right];\n            result[right] = tmp;\n            left++;\n            right--;\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n == 0) return s;\n    \n    bool has_letter = false;\n    size_t i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i A = _mm256_set1_epi8('A' - 1);\n        __m256i Z = _mm256_set1_epi8('Z');\n        __m256i a = _mm256_set1_epi8('a' - 1);\n        __m256i z = _mm256_set1_epi8('z');\n        \n        __m256i geA = _mm256_cmpgt_epi8(chunk, A);\n        __m256i leZ = _mm256_cmpgt_epi8(Z, chunk);\n        __m256i upper = _mm256_and_si256(geA, leZ);\n        \n        __m256i gea = _mm256_cmpgt_epi8(chunk, a);\n        __m256i lez = _mm256_cmpgt_epi8(z, chunk);\n        __m256i lower = _mm256_and_si256(gea, lez);\n        \n        __m256i letter = _mm256_or_si256(upper, lower);\n        if (!_mm256_testz_si256(letter, letter)) {\n            has_letter = true;\n            break;\n        }\n    }\n    \n    if (!has_letter) {\n        for (; i < n; i++) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n    \n    if (!has_letter) {\n        std::string result(n, '\\0');\n        const char* src = s.data();\n        char* dest = result.data();\n        const __m256i rev_mask = _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        );\n        \n        size_t remaining = n;\n        const char* src_ptr = src + n - 32;\n        char* dest_ptr = dest;\n        \n        while (remaining >= 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src_ptr));\n            __m256i reversed = _mm256_shuffle_epi8(chunk, rev_mask);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest_ptr), reversed);\n            src_ptr -= 32;\n            dest_ptr += 32;\n            remaining -= 32;\n        }\n        \n        if (remaining > 0) {\n            const char* tail_src = src;\n            char* tail_dest = dest + n - 1;\n            for (size_t j = 0; j < remaining; j++) {\n                *tail_dest-- = *tail_src++;\n            }\n        }\n        return result;\n    } else {\n        std::string result = s;\n        char* data = result.data();\n        i = 0;\n        for (; i + 32 <= n; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i A = _mm256_set1_epi8('A' - 1);\n            __m256i Z = _mm256_set1_epi8('Z');\n            __m256i a = _mm256_set1_epi8('a' - 1);\n            __m256i z = _mm256_set1_epi8('z');\n            \n            __m256i geA = _mm256_cmpgt_epi8(chunk, A);\n            __m256i leZ = _mm256_cmpgt_epi8(Z, chunk);\n            __m256i upper = _mm256_and_si256(geA, leZ);\n            \n            __m256i gea = _mm256_cmpgt_epi8(chunk, a);\n            __m256i lez = _mm256_cmpgt_epi8(z, chunk);\n            __m256i lower = _mm256_and_si256(gea, lez);\n            \n            __m256i letter = _mm256_or_si256(upper, lower);\n            __m256i flip = _mm256_and_si256(letter, _mm256_set1_epi8(0x20));\n            __m256i flipped = _mm256_xor_si256(chunk, flip);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(data + i), flipped);\n        }\n        \n        for (; i < n; i++) {\n            char c = data[i];\n            if (c >= 'A' && c <= 'Z') {\n                data[i] = c + 0x20;\n            } else if (c >= 'a' && c <= 'z') {\n                data[i] = c - 0x20;\n            }\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <algorithm>\n#include <string>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    std::string res(s);\n    bool has_letter = false;\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n        __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n        __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n        __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n\n        __m256i cmp_upper1 = _mm256_cmpgt_epi8(chunk, A_minus1);\n        __m256i cmp_upper2 = _mm256_cmpgt_epi8(Z_plus1, chunk);\n        __m256i upper = _mm256_and_si256(cmp_upper1, cmp_upper2);\n\n        __m256i cmp_lower1 = _mm256_cmpgt_epi8(chunk, a_minus1);\n        __m256i cmp_lower2 = _mm256_cmpgt_epi8(z_plus1, chunk);\n        __m256i lower = _mm256_and_si256(cmp_lower1, cmp_lower2);\n\n        __m256i mask_letter = _mm256_or_si256(upper, lower);\n        int mask = _mm256_movemask_epi8(mask_letter);\n        if (mask != 0) {\n            has_letter = true;\n        }\n\n        __m256i flip_val = _mm256_and_si256(mask_letter, _mm256_set1_epi8(0x20));\n        __m256i flipped = _mm256_xor_si256(chunk, flip_val);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), flipped);\n    }\n\n    for (; i < len; i++) {\n        char c = s[i];\n        if (c >= 'A' && c <= 'Z') {\n            res[i] = c + 32;\n            has_letter = true;\n        } else if (c >= 'a' && c <= 'z') {\n            res[i] = c - 32;\n            has_letter = true;\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(res.begin(), res.end());\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    if (s.empty()) {\n        return s;\n    }\n\n    const size_t n = s.size();\n    std::string result = s;\n    constexpr size_t vec_size = 32;\n    size_t i = 0;\n    bool has_letter = false;\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i low_data = _mm256_or_si256(data, _mm256_set1_epi8(0x20));\n        __m256i a_minus = _mm256_set1_epi8('a' - 1);\n        __m256i z_plus = _mm256_set1_epi8('z' + 1);\n        __m256i ge_a = _mm256_cmpgt_epi8(low_data, a_minus);\n        __m256i le_z = _mm256_cmpgt_epi8(z_plus, low_data);\n        __m256i mask = _mm256_and_si256(ge_a, le_z);\n\n        if (!has_letter) {\n            if (_mm256_movemask_epi8(mask) != 0) {\n                has_letter = true;\n            }\n        }\n\n        __m256i to_flip = _mm256_and_si256(mask, _mm256_set1_epi8(0x20));\n        __m256i flipped = _mm256_xor_si256(data, to_flip);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), flipped);\n    }\n\n    for (; i < n; i++) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            result[i] = c ^ 0x20;\n            has_letter = true;\n        }\n    }\n\n    if (!has_letter) {\n        return std::string(s.rbegin(), s.rend());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    if (s.empty()) {\n        return s;\n    }\n    const size_t n = s.size();\n    bool has_letter = false;\n    size_t i = 0;\n\n    for (; i + 32 <= n; i += 32) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i a_min = _mm256_set1_epi8('A' - 1);\n        __m256i z_max = _mm256_set1_epi8('Z' + 1);\n        __m256i mask_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(v, a_min),\n            _mm256_cmpgt_epi8(z_max, v)\n        );\n        __m256i a_min2 = _mm256_set1_epi8('a' - 1);\n        __m256i z_max2 = _mm256_set1_epi8('z' + 1);\n        __m256i mask_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(v, a_min2),\n            _mm256_cmpgt_epi8(z_max2, v)\n        );\n        __m256i mask_letter = _mm256_or_si256(mask_upper, mask_lower);\n        if (_mm256_movemask_epi8(mask_letter) != 0) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        for (; i < n; i++) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n\n    if (!has_letter) {\n        std::string result = s;\n        std::reverse(result.begin(), result.end());\n        return result;\n    } else {\n        std::string result = s;\n        i = 0;\n        for (; i + 32 <= n; i += 32) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i a_min = _mm256_set1_epi8('A' - 1);\n            __m256i z_max = _mm256_set1_epi8('Z' + 1);\n            __m256i mask_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(v, a_min),\n                _mm256_cmpgt_epi8(z_max, v)\n            );\n            __m256i a_min2 = _mm256_set1_epi8('a' - 1);\n            __m256i z_max2 = _mm256_set1_epi8('z' + 1);\n            __m256i mask_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(v, a_min2),\n                _mm256_cmpgt_epi8(z_max2, v)\n            );\n            __m256i mask_letter = _mm256_or_si256(mask_upper, mask_lower);\n            __m256i flip = _mm256_set1_epi8(0x20);\n            __m256i flipped = _mm256_xor_si256(v, _mm256_and_si256(flip, mask_letter));\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), flipped);\n        }\n        for (; i < n; i++) {\n            char c = s[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 32;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 32;\n            }\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n == 0) return l;\n    int m = (n + 2) / 3;\n    std::vector<int> temp_arr(m);\n    int i = 0;\n    for (; i + 8 <= m; i += 8) {\n        __m256i base = _mm256_set1_epi32(3 * i);\n        __m256i offsets = _mm256_set_epi32(21, 18, 15, 12, 9, 6, 3, 0);\n        __m256i vindex = _mm256_add_epi32(base, offsets);\n        __m256i gathered = _mm256_i32gather_epi32(l.data(), vindex, 4);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_arr.data() + i), gathered);\n    }\n    for (; i < m; i++) {\n        temp_arr[i] = l[3 * i];\n    }\n    std::sort(temp_arr.begin(), temp_arr.end());\n    i = 0;\n    for (; i + 8 <= m; i += 8) {\n        __m256i sorted_chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp_arr.data() + i));\n        int chunk[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(chunk), sorted_chunk);\n        for (int j = 0; j < 8; j++) {\n            l[3 * (i + j)] = chunk[j];\n        }\n    }\n    for (; i < m; i++) {\n        l[3 * i] = temp_arr[i];\n    }\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) {\n        return l;\n    }\n\n    const __m256i offset_vector = _mm256_set_epi32(21, 18, 15, 12, 9, 6, 3, 0);\n    size_t i = 0;\n\n    for (; i + 24 <= n; i += 24) {\n        int* base = l.data() + i;\n        __m256i values = _mm256_i32gather_epi32(base, offset_vector, 4);\n        \n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, values);\n        \n        for (int a = 1; a < 8; a++) {\n            int temp = arr[a];\n            int b = a - 1;\n            while (b >= 0 && arr[b] > temp) {\n                arr[b + 1] = arr[b];\n                b--;\n            }\n            arr[b + 1] = temp;\n        }\n        \n        __m256i sorted = _mm256_loadu_si256((__m256i*)arr);\n        _mm256_i32scatter_epi32(base, offset_vector, sorted, 4);\n    }\n\n    if (i < n) {\n        int arr_rem[8];\n        int count = 0;\n        for (size_t j = i; j < n; j++) {\n            if (j % 3 == 0) {\n                arr_rem[count++] = l[j];\n            }\n        }\n\n        for (int a = 1; a < count; a++) {\n            int temp = arr_rem[a];\n            int b = a - 1;\n            while (b >= 0 && arr_rem[b] > temp) {\n                arr_rem[b + 1] = arr_rem[b];\n                b--;\n            }\n            arr_rem[b + 1] = temp;\n        }\n\n        int k = 0;\n        for (size_t j = i; j < n; j++) {\n            if (j % 3 == 0) {\n                l[j] = arr_rem[k++];\n            }\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> result = l;\n    int n = l.size();\n    int m = (n + 2) / 3;\n\n    if (m == 0) {\n        return result;\n    }\n\n    std::vector<int> extracted(m);\n    const int* base_ptr = l.data();\n\n    int i = 0;\n    for (; i + 8 <= m; i += 8) {\n        __m256i indices = _mm256_set_epi32(\n            3*(i+7), 3*(i+6), 3*(i+5), 3*(i+4),\n            3*(i+3), 3*(i+2), 3*(i+1), 3*i\n        );\n        __m256i gathered = _mm256_i32gather_epi32(base_ptr, indices, 4);\n        _mm256_storeu_si256((__m256i*)(extracted.data() + i), gathered);\n    }\n\n    for (; i < m; i++) {\n        extracted[i] = l[3*i];\n    }\n\n    std::vector<int> sorted_extracted = extracted;\n    std::sort(sorted_extracted.begin(), sorted_extracted.end());\n\n    for (i = 0; i < m; i++) {\n        result[3*i] = sorted_extracted[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n == 0) {\n        return l;\n    }\n    std::vector<int> result = l;\n    int n_tri = (n + 2) / 3;\n    if (n_tri == 0) {\n        return result;\n    }\n    std::vector<int> tri_values(n_tri);\n    int i = 0;\n    for (; i + 8 <= n_tri; i += 8) {\n        __m256i indices = _mm256_setr_epi32(3*i, 3*i+3, 3*i+6, 3*i+9, 3*i+12, 3*i+15, 3*i+18, 3*i+21);\n        __m256i gathered = _mm256_i32gather_epi32(l.data(), indices, 4);\n        _mm256_storeu_si256((__m256i*)(tri_values.data() + i), gathered);\n    }\n    for (; i < n_tri; i++) {\n        tri_values[i] = l[3*i];\n    }\n    std::sort(tri_values.begin(), tri_values.end());\n    for (int j = 0; j < n_tri; j++) {\n        result[3*j] = tri_values[j];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n == 0) {\n        return l;\n    }\n\n    int num_triples = (n + 2) / 3;\n    std::vector<int> to_sort(num_triples);\n\n    int* data = l.data();\n    int* dest = to_sort.data();\n\n    for (int j0 = 0; j0 < num_triples; j0 += 8) {\n        int remaining = num_triples - j0;\n        if (remaining >= 8) {\n            __m256i vindex = _mm256_set_epi32(\n                (j0+7)*3, (j0+6)*3, (j0+5)*3, (j0+4)*3,\n                (j0+3)*3, (j0+2)*3, (j0+1)*3, j0*3\n            );\n            __m256i gathered = _mm256_i32gather_epi32(data, vindex, 4);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest + j0), gathered);\n        } else {\n            for (int j = j0; j < num_triples; j++) {\n                dest[j] = data[j*3];\n            }\n        }\n    }\n\n    std::sort(to_sort.begin(), to_sort.end());\n\n    for (int j = 0; j < num_triples; j++) {\n        l[j*3] = to_sort[j];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n == 0) return 0.0f;\n    if (n < 8) {\n        float max_val = l[0];\n        for (int i = 1; i < n; i++) {\n            if (l[i] > max_val) max_val = l[i];\n        }\n        return max_val;\n    }\n    const float* data = l.data();\n    __m256 max_vec = _mm256_loadu_ps(data);\n    int i = 8;\n    while (i <= n - 8) {\n        __m256 chunk = _mm256_loadu_ps(data + i);\n        max_vec = _mm256_max_ps(max_vec, chunk);\n        i += 8;\n    }\n    __m128 low128 = _mm256_castps256_ps128(max_vec);\n    __m128 high128 = _mm256_extractf128_ps(max_vec, 1);\n    __m128 max128 = _mm_max_ps(low128, high128);\n    __m128 tmp = _mm_movehl_ps(max128, max128);\n    __m128 max2 = _mm_max_ps(max128, tmp);\n    __m128 max1 = _mm_max_ss(max2, _mm_movehdup_ps(max2));\n    float result = _mm_cvtss_f32(max1);\n    for (; i < n; i++) {\n        if (data[i] > result) {\n            result = data[i];\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return 0.0f;\n    }\n    __m256 max_reg = _mm256_set1_ps(-FLT_MAX);\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 chunk = _mm256_loadu_ps(&l[i]);\n        max_reg = _mm256_max_ps(max_reg, chunk);\n    }\n\n    __m128 low = _mm256_castps256_ps128(max_reg);\n    __m128 high = _mm256_extractf128_ps(max_reg, 1);\n    __m128 max128 = _mm_max_ps(low, high);\n\n    __m128 hi64 = _mm_movehl_ps(max128, max128);\n    __m128 max64 = _mm_max_ps(max128, hi64);\n    __m128 max32 = _mm_max_ss(max64, _mm_shuffle_ps(max64, max64, _MM_SHUFFLE(0, 0, 0, 1)));\n    float max_val = _mm_cvtss_f32(max32);\n\n    for (; i < n; i++) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    float* ptr = l.data();\n\n    if (n < 8) {\n        float max_val = ptr[0];\n        for (size_t i = 1; i < n; ++i) {\n            if (ptr[i] > max_val) \n                max_val = ptr[i];\n        }\n        return max_val;\n    }\n\n    __m256 max_vec = _mm256_loadu_ps(ptr);\n    size_t i = 8;\n    while (i <= n - 8) {\n        __m256 chunk = _mm256_loadu_ps(ptr + i);\n        max_vec = _mm256_max_ps(max_vec, chunk);\n        i += 8;\n    }\n\n    float max_arr[8];\n    _mm256_storeu_ps(max_arr, max_vec);\n    float max_val = max_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_arr[j] > max_val)\n            max_val = max_arr[j];\n    }\n\n    while (i < n) {\n        if (ptr[i] > max_val)\n            max_val = ptr[i];\n        ++i;\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    const size_t n = l.size();\n    float max_val = l[0];\n    if (n < 8) {\n        for (size_t i = 1; i < n; ++i) {\n            if (l[i] > max_val) max_val = l[i];\n        }\n        return max_val;\n    }\n    __m256 max_vec = _mm256_loadu_ps(l.data());\n    size_t i = 8;\n    for (; i <= n - 8; i += 8) {\n        __m256 chunk = _mm256_loadu_ps(l.data() + i);\n        max_vec = _mm256_max_ps(max_vec, chunk);\n    }\n    __m128 low128 = _mm256_castps256_ps128(max_vec);\n    __m128 high128 = _mm256_extractf128_ps(max_vec, 1);\n    __m128 max128 = _mm_max_ps(low128, high128);\n    __m128 high64 = _mm_movehl_ps(max128, max128);\n    __m128 max64 = _mm_max_ps(max128, high64);\n    __m128 high32 = _mm_shuffle_ps(max64, max64, 0x01);\n    __m128 max32 = _mm_max_ss(max64, high32);\n    float simd_max = _mm_cvtss_f32(max32);\n    for (; i < n; ++i) {\n        if (l[i] > simd_max) {\n            simd_max = l[i];\n        }\n    }\n    return simd_max;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return 0;\n    }\n    float max_val = l[0];\n    if (n < 8) {\n        for (size_t i = 1; i < n; i++) {\n            if (l[i] > max_val) {\n                max_val = l[i];\n            }\n        }\n        return max_val;\n    }\n    __m256 max_vec = _mm256_loadu_ps(&l[0]);\n    size_t i = 8;\n    for (; i <= n - 8; i += 8) {\n        __m256 chunk = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, chunk);\n    }\n    __m128 vlow = _mm256_castps256_ps128(max_vec);\n    __m128 vhigh = _mm256_extractf128_ps(max_vec, 1);\n    __m128 max128 = _mm_max_ps(vlow, vhigh);\n    __m128 shuf = _mm_movehl_ps(max128, max128);\n    __m128 max1 = _mm_max_ps(max128, shuf);\n    __m128 max2 = _mm_max_ss(max1, _mm_shuffle_ps(max1, max1, 0x01));\n    max_val = _mm_cvtss_f32(max2);\n    for (; i < n; i++) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) {\n        return 0;\n    }\n\n    int total_count = 0;\n\n    for (int i = 0; i < n; i += 8) {\n        int base = i;\n        __m256i indices = _mm256_set_epi32(base + 7, base + 6, base + 5, base + 4,\n                                          base + 3, base + 2, base + 1, base);\n\n        __m256i rem11 = _mm256_set_epi32(\n            (base + 7) % 11, (base + 6) % 11, (base + 5) % 11, (base + 4) % 11,\n            (base + 3) % 11, (base + 2) % 11, (base + 1) % 11, base % 11\n        );\n        __m256i rem13 = _mm256_set_epi32(\n            (base + 7) % 13, (base + 6) % 13, (base + 5) % 13, (base + 4) % 13,\n            (base + 3) % 13, (base + 2) % 13, (base + 1) % 13, base % 13\n        );\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask11 = _mm256_cmpeq_epi32(rem11, zero);\n        __m256i mask13 = _mm256_cmpeq_epi32(rem13, zero);\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        int mask_arr[8];\n        _mm256_storeu_si256((__m256i*)mask_arr, mask);\n\n        for (int j = 0; j < 8; j++) {\n            int num = base + j;\n            if (num < n && mask_arr[j] != 0) {\n                int count = 0;\n                int x = num;\n                if (x == 0) {\n                    // Skip digit count for 0\n                } else {\n                    while (x != 0) {\n                        if (x % 10 == 7) {\n                            count++;\n                        }\n                        x /= 10;\n                    }\n                }\n                total_count += count;\n            }\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_digit_7(int num) {\n    if (num == 0) return 0;\n    int count = 0;\n    while (num > 0) {\n        if (num % 10 == 7) {\n            count++;\n        }\n        num /= 10;\n    }\n    return count;\n}\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    int total_count = 0;\n    const int chunk_size = 8;\n    const __m256i v11 = _mm256_set1_epi32(11);\n    const __m256i v13 = _mm256_set1_epi32(13);\n    const __m256i vzero = _mm256_setzero_si256();\n    \n    for (int i = 0; i < n; i += chunk_size) {\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask_lt_n = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), indices);\n        \n        __m256i rem11 = _mm256_sub_epi32(indices, \n            _mm256_mullo_epi32(\n                _mm256_srli_epi32(\n                    _mm256_mul_epu32_emu(indices, v11), \n                32),\n            v11);\n        rem11 = _mm256_mask_condition(rem11, v11);\n        \n        __m256i rem13 = _mm256_sub_epi32(indices, \n            _mm256_mullo_epi32(\n                _mm256_srli_epi32(\n                    _mm256_mul_epu32_emu(indices, v13), \n                32),\n            v13);\n        rem13 = _mm256_mask_condition(rem13, v13);\n        \n        __m256i div11 = _mm256_cmpeq_epi32(rem11, vzero);\n        __m256i div13 = _mm256_cmpeq_epi32(rem13, vzero);\n        __m256i cond = _mm256_or_si256(div11, div13);\n        cond = _mm256_and_si256(cond, mask_lt_n);\n        \n        int mask = _mm256_movemask_epi8(cond);\n        if (mask) {\n            int vals[8];\n            _mm256_storeu_si256((__m256i*)vals, indices);\n            for (int j = 0; j < 8; j++) {\n                if (mask & (1 << (j*4))) {\n                    total_count += count_digit_7(vals[j]);\n                }\n            }\n        }\n    }\n    return total_count;\n}\n\n// Helper functions for emulating 32-bit unsigned multiplication\n__m256i _mm256_mul_epu32_emu(__m256i a, __m256i b) {\n    __m128i a_low = _mm256_extractf128_si256(a, 0);\n    __m128i a_high = _mm256_extractf128_si256(a, 1);\n    __m128i b_low = _mm256_extractf128_si256(b, 0);\n    __m128i b_high = _mm256_extractf128_si256(b, 1);\n    \n    __m128i res_low = _mm_mul_epu32(a_low, b_low);\n    __m128i res_high = _mm_mul_epu32(a_high, b_high);\n    \n    return _mm256_set_m128i(res_high, res_low);\n}\n\n__m256i _mm256_mask_condition(__m256i rem, __m256i divisor) {\n    __m256i cmp = _mm256_cmpgt_epi32(rem, divisor);\n    return _mm256_sub_epi32(rem, _mm256_and_si256(cmp, divisor));\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int global_count = 0;\n    for (int i = 0; i < n; i += 8) {\n        int base = i;\n        __m256i v = _mm256_setr_epi32(base, base+1, base+2, base+3, base+4, base+5, base+6, base+7);\n        __m256i count_vec = _mm256_setzero_si256();\n        __m256i zero = _mm256_setzero_si256();\n        __m256i ten = _mm256_set1_epi32(10);\n        __m256i seven = _mm256_set1_epi32(7);\n        \n        for (int digit_index = 0; digit_index < 10; digit_index++) {\n            __m256i q = _mm256_setr_epi32(\n                v.m256i_i32[0] / 10, v.m256i_i32[1] / 10,\n                v.m256i_i32[2] / 10, v.m256i_i32[3] / 10,\n                v.m256i_i32[4] / 10, v.m256i_i32[5] / 10,\n                v.m256i_i32[6] / 10, v.m256i_i32[7] / 10\n            );\n            __m256i r = _mm256_sub_epi32(v, _mm256_mullo_epi32(q, ten));\n            __m256i mask = _mm256_cmpeq_epi32(r, seven);\n            mask = _mm256_and_si256(mask, _mm256_set1_epi32(1));\n            count_vec = _mm256_add_epi32(count_vec, mask);\n            v = q;\n            if (_mm256_testz_si256(v, v)) break;\n        }\n        \n        int counts[8];\n        _mm256_storeu_si256((__m256i*)counts, count_vec);\n        \n        for (int j = 0; j < 8; j++) {\n            int num = base + j;\n            if (num >= n) continue;\n            if (num % 11 == 0 || num % 13 == 0) {\n                global_count += counts[j];\n            }\n        }\n    }\n    return global_count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) {\n        return 0;\n    }\n    int bases[] = {11, 13, 143};\n    int signs[] = {1, 1, -1};\n    int total_count = 0;\n\n    for (int idx = 0; idx < 3; idx++) {\n        int base = bases[idx];\n        int sign = signs[idx];\n        int count = 0;\n        int k = 0;\n        __m256i base_vec = _mm256_set1_epi32(base);\n        __m256i n_vec = _mm256_set1_epi32(n);\n\n        while (1) {\n            __m256i k_vec = _mm256_setr_epi32(k, k+1, k+2, k+3, k+4, k+5, k+6, k+7);\n            __m256i multiples = _mm256_mullo_epi32(k_vec, base_vec);\n            __m256i mask = _mm256_cmpgt_epi32(n_vec, multiples);\n\n            if (_mm256_testz_si256(mask, mask)) {\n                break;\n            }\n\n            int multiples_arr[8];\n            int mask_arr[8];\n            _mm256_storeu_si256((__m256i*)multiples_arr, multiples);\n            _mm256_storeu_si256((__m256i*)mask_arr, mask);\n\n            for (int i = 0; i < 8; i++) {\n                if (mask_arr[i]) {\n                    int num = multiples_arr[i];\n                    if (num == 0) continue;\n                    int cnt = 0;\n                    while (num) {\n                        if (num % 10 == 7) {\n                            cnt++;\n                        }\n                        num /= 10;\n                    }\n                    count += cnt;\n                }\n            }\n            k += 8;\n        }\n        total_count += sign * count;\n    }\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "To solve this problem, we need to count the number of times the digit '7' appears in all non-negative integers less than a given integer `n` that are divisible by either 11 or 13. The solution leverages AVX2 intrinsics to process multiple integers in parallel, enhancing performance through vectorization.\n\n### Approach\n1. **Problem Analysis**: The task involves iterating through all integers from 0 to `n-1`, checking if each integer is divisible by 11 or 13. For each valid integer, we count the occurrences of the digit '7' in its decimal representation.\n2. **Vectorization with AVX2**: We process integers in blocks of 8 (the size of an AVX2 vector) to utilize SIMD (Single Instruction, Multiple Data) capabilities. For each block, we:\n   - Load 8 consecutive integers into an AVX2 vector.\n   - Extract each integer from the vector and check divisibility by 11 or 13 using scalar operations.\n   - For integers meeting the divisibility condition, count the digit '7' occurrences in their decimal representation.\n3. **Efficiency**: By processing blocks of integers, we minimize the overhead of loop control and leverage AVX2 for efficient data loading. The digit counting remains scalar due to the complexity of vectorizing decimal digit extraction.\n\n### Solution Code\n```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    int total = 0;\n    for (int i = 0; i < n; i += 8) {\n        int end = (i + 8) < n ? (i + 8) : n;\n        int count_in_block = end - i;\n        __m256i v = _mm256_set_epi32(\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i\n        );\n        int nums[8];\n        _mm256_storeu_si256((__m256i*)nums, v);\n        for (int j = 0; j < count_in_block; j++) {\n            int num = nums[j];\n            if (num % 11 == 0 || num % 13 == 0) {\n                int t = num;\n                int count7 = 0;\n                if (t == 0) {\n                    count7 = 0;\n                } else {\n                    while (t) {\n                        if (t % 10 == 7) {\n                            count7++;\n                        }\n                        t /= 10;\n                    }\n                }\n                total += count7;\n            }\n        }\n    }\n    return total;\n}\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if `n` is non-positive, returning 0 immediately if true.\n2. **Block Processing**: The loop processes integers in blocks of 8. For each block:\n   - **Vector Creation**: An AVX2 vector is created containing 8 consecutive integers starting from the current index `i`.\n   - **Storage**: The vector is stored into an array `nums` for easy access to individual elements.\n   - **Divisibility Check**: Each integer in the block is checked for divisibility by 11 or 13. If divisible, the digit '7' count in its decimal representation is computed.\n3. **Digit Counting**: For each valid integer, the digit '7' count is determined by repeatedly checking the last digit (using modulus 10) and removing it (using integer division by 10).\n4. **Result Accumulation**: The counts of digit '7' for all valid integers are accumulated into `total`, which is returned as the final result.\n\nThis approach efficiently processes integers in blocks, leveraging AVX2 for data loading while using scalar operations for divisibility checks and digit counting, ensuring correctness and performance."}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return {};\n    }\n    size_t even_size = (n + 1) / 2;\n    size_t odd_size = n / 2;\n    std::vector<float> even_arr(even_size);\n    std::vector<float> odd_arr(odd_size);\n\n    for (size_t i = 0; i < n; i += 8) {\n        if (i + 8 <= n) {\n            __m256 v = _mm256_loadu_ps(&l[i]);\n            __m256i idx = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);\n            __m256 permuted = _mm256_permutevar8x32_ps(v, idx);\n            __m128 even_chunk = _mm256_castps256_ps128(permuted);\n            __m128 odd_chunk = _mm256_extractf128_ps(permuted, 1);\n            _mm_storeu_ps(&even_arr[i / 2], even_chunk);\n            _mm_storeu_ps(&odd_arr[i / 2], odd_chunk);\n        } else {\n            for (size_t j = i; j < n; ++j) {\n                if (j % 2 == 0) {\n                    even_arr[j / 2] = l[j];\n                } else {\n                    odd_arr[j / 2] = l[j];\n                }\n            }\n        }\n    }\n\n    std::sort(even_arr.begin(), even_arr.end());\n\n    std::vector<float> result(n);\n    size_t i = 0;\n    for (; i + 4 <= odd_size; i += 4) {\n        __m128 evens = _mm_loadu_ps(&even_arr[i]);\n        __m128 odds = _mm_loadu_ps(&odd_arr[i]);\n        __m128 low = _mm_unpacklo_ps(evens, odds);\n        __m128 high = _mm_unpackhi_ps(evens, odds);\n        __m256 res = _mm256_castps128_ps256(low);\n        res = _mm256_insertf128_ps(res, high, 1);\n        _mm256_storeu_ps(&result[2 * i], res);\n    }\n\n    for (size_t j = i; j < odd_size; ++j) {\n        result[2 * j] = even_arr[j];\n        result[2 * j + 1] = odd_arr[j];\n    }\n\n    if (even_size > odd_size) {\n        result[2 * odd_size] = even_arr[odd_size];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return l;\n    \n    std::vector<float> even;\n    std::vector<float> odd;\n    even.reserve((n+1)/2);\n    odd.reserve(n/2);\n    \n    __m256i deinterleave_mask = _mm256_set_epi32(7,5,3,1,6,4,2,0);\n    __m256i interleave_mask = _mm256_set_epi32(7,3,6,2,5,1,4,0);\n    \n    float temp_even[4];\n    float temp_odd[4];\n    \n    size_t i = 0;\n    for (; i+8<=n; i+=8) {\n        __m256 v = _mm256_loadu_ps(&l[i]);\n        __m256 permuted = _mm256_permutevar8x32_ps(v, deinterleave_mask);\n        __m128 even128 = _mm256_castps256_ps128(permuted);\n        __m128 odd128 = _mm256_extractf128_ps(permuted, 1);\n        _mm_storeu_ps(temp_even, even128);\n        _mm_storeu_ps(temp_odd, odd128);\n        for (int j=0; j<4; j++) {\n            even.push_back(temp_even[j]);\n            odd.push_back(temp_odd[j]);\n        }\n    }\n    \n    for (; i<n; i++) {\n        if (i % 2 == 0) {\n            even.push_back(l[i]);\n        } else {\n            odd.push_back(l[i]);\n        }\n    }\n    \n    std::sort(even.begin(), even.end());\n    \n    std::vector<float> result(n);\n    size_t even_index = 0;\n    size_t odd_index = 0;\n    i = 0;\n    \n    for (; even_index+4<=even.size() && odd_index+4<=odd.size(); even_index+=4, odd_index+=4, i+=8) {\n        __m128 even128 = _mm_loadu_ps(&even[even_index]);\n        __m128 odd128 = _mm_loadu_ps(&odd[odd_index]);\n        __m256 both = _mm256_castps128_ps256(even128);\n        both = _mm256_insertf128_ps(both, odd128, 1);\n        __m256 interleaved = _mm256_permutevar8x32_ps(both, interleave_mask);\n        _mm256_storeu_ps(&result[i], interleaved);\n    }\n    \n    while (even_index < even.size() && odd_index < odd.size()) {\n        result[i++] = even[even_index++];\n        result[i++] = odd[odd_index++];\n    }\n    if (even_index < even.size()) {\n        result[i++] = even[even_index++];\n    }\n    if (odd_index < odd.size()) {\n        result[i++] = odd[odd_index++];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    int half_even = (n + 1) / 2;\n    int half_odd = n / 2;\n    std::vector<float> even(half_even);\n    std::vector<float> odd(half_odd);\n\n    if (n == 0) {\n        return l;\n    }\n\n    __m256i permute_indices = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 permuted = _mm256_permutevar8x32_ps(vec, permute_indices);\n        __m128 even_part = _mm256_castps256_ps128(permuted);\n        __m128 odd_part = _mm256_extractf128_ps(permuted, 1);\n        int base = i / 2;\n        _mm_storeu_ps(&even[base], even_part);\n        _mm_storeu_ps(&odd[base], odd_part);\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 0) {\n            even[i / 2] = l[i];\n        } else {\n            odd[i / 2] = l[i];\n        }\n    }\n\n    std::sort(even.begin(), even.end());\n\n    std::vector<float> result(n);\n    i = 0;\n    for (; i <= half_odd - 4; i += 4) {\n        __m128 evens = _mm_loadu_ps(&even[i]);\n        __m128 odds = _mm_loadu_ps(&odd[i]);\n        __m128 low = _mm_unpacklo_ps(evens, odds);\n        __m128 high = _mm_unpackhi_ps(evens, odds);\n        __m256 res = _mm256_castps128_ps256(low);\n        res = _mm256_insertf128_ps(res, high, 1);\n        _mm256_storeu_ps(&result[2 * i], res);\n    }\n\n    for (; i < half_odd; i++) {\n        result[2 * i] = even[i];\n        result[2 * i + 1] = odd[i];\n    }\n\n    if (n % 2 == 1) {\n        result[n - 1] = even[half_odd];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n == 0) \n        return l;\n    \n    int num_even = (n + 1) / 2;\n    std::vector<int> even_indices(num_even);\n    for (int i = 0; i < num_even; i++) {\n        even_indices[i] = 2 * i;\n    }\n\n    std::vector<float> even_values(num_even);\n    float* base = l.data();\n    int i = 0;\n    for (; i + 8 <= num_even; i += 8) {\n        __m256i vindex = _mm256_loadu_si256((__m256i*)(even_indices.data() + i));\n        __m256 data = _mm256_i32gather_ps(base, vindex, 4);\n        _mm256_storeu_ps(even_values.data() + i, data);\n    }\n\n    for (; i < num_even; i++) {\n        even_values[i] = base[even_indices[i]];\n    }\n\n    std::sort(even_values.begin(), even_values.end());\n\n    for (int i = 0; i < num_even; i++) {\n        l[even_indices[i]] = even_values[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return l;\n    \n    size_t even_count = (n + 1) / 2;\n    size_t odd_count = n / 2;\n    std::vector<float> even_array(even_count);\n    std::vector<float> odd_array(odd_count);\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256 v = _mm256_loadu_ps(&l[i]);\n        __m256i idx = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);\n        __m256 permuted = _mm256_permutevar8x32_ps(v, idx);\n        __m128 even_part = _mm256_castps256_ps128(permuted);\n        __m128 odd_part = _mm256_extractf128_ps(permuted, 1);\n        _mm_storeu_ps(&even_array[i/2], even_part);\n        _mm_storeu_ps(&odd_array[i/2], odd_part);\n    }\n    for (; i < n; ++i) {\n        if (i % 2 == 0) {\n            even_array[i/2] = l[i];\n        } else {\n            odd_array[i/2] = l[i];\n        }\n    }\n\n    std::sort(even_array.begin(), even_array.end());\n\n    std::vector<float> result(n);\n    i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m128 even_chunk = _mm_loadu_ps(&even_array[i/2]);\n        __m128 odd_chunk = _mm_loadu_ps(&odd_array[i/2]);\n        __m128 low_interleaved = _mm_unpacklo_ps(even_chunk, odd_chunk);\n        __m128 high_interleaved = _mm_unpackhi_ps(even_chunk, odd_chunk);\n        __m256 res = _mm256_setzero_ps();\n        res = _mm256_insertf128_ps(res, low_interleaved, 0);\n        res = _mm256_insertf128_ps(res, high_interleaved, 1);\n        _mm256_storeu_ps(&result[i], res);\n    }\n    for (; i < n; ++i) {\n        if (i % 2 == 0) {\n            result[i] = even_array[i/2];\n        } else {\n            result[i] = odd_array[i/2];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) \n        return false;\n    \n    std::sort(l.begin(), l.end());\n    \n    for (int i = 0; i < n - 2; i++) {\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n        \n        while (j < k) {\n            if (k - j >= 8) {\n                __m256i v_j = _mm256_set1_epi32(l[j]);\n                __m256i v_ks = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[k - 7]));\n                __m256i v_target = _mm256_set1_epi32(target);\n                __m256i v_sum = _mm256_add_epi32(v_j, v_ks);\n                \n                __m256i v_cmp = _mm256_cmpeq_epi32(v_sum, v_target);\n                if (_mm256_movemask_epi8(v_cmp) != 0) {\n                    return true;\n                }\n                \n                __m256i v_gt = _mm256_cmpgt_epi32(v_sum, v_target);\n                if (_mm256_movemask_epi8(v_gt) == -1) {\n                    k -= 8;\n                    continue;\n                }\n                \n                __m256i v_lt = _mm256_cmpgt_epi32(v_target, v_sum);\n                if (_mm256_movemask_epi8(v_lt) == -1) {\n                    j++;\n                    continue;\n                }\n                \n                bool updated = false;\n                for (int t = 0; t < 8; t++) {\n                    int idx = k - t;\n                    if (idx <= j) break;\n                    \n                    int sum_scalar = l[j] + l[idx];\n                    if (sum_scalar == target) {\n                        return true;\n                    } else if (sum_scalar < target) {\n                        j++;\n                        updated = true;\n                        break;\n                    } else {\n                        k = idx - 1;\n                        updated = true;\n                        break;\n                    }\n                }\n                if (updated) \n                    continue;\n                else \n                    k -= 8;\n            } else {\n                int sum = l[j] + l[k];\n                if (sum == target) {\n                    return true;\n                } else if (sum < target) {\n                    j++;\n                } else {\n                    k--;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) {\n        return false;\n    }\n    std::sort(l.begin(), l.end());\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int target = -(l[i] + l[j]);\n            if (target < l[j + 1]) {\n                break;\n            }\n            if (target > l[n - 1]) {\n                continue;\n            }\n\n            int start = j + 1;\n            int len = n - start;\n            int* base = l.data() + start;\n            __m256i target_vec = _mm256_set1_epi32(target);\n            \n            int k = 0;\n            for (; k <= len - 8; k += 8) {\n                __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(base + k));\n                __m256i cmp = _mm256_cmpeq_epi32(data, target_vec);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            for (; k < len; ++k) {\n                if (base[k] == target) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return false;\n    }\n    std::vector<int> l_sorted = l;\n    std::sort(l_sorted.begin(), l_sorted.end());\n    int n = l_sorted.size();\n    for (int i = 0; i < n - 2; i++) {\n        int target = -l_sorted[i];\n        int left = i + 1;\n        int right = n - 1;\n        while (left < right) {\n            if (right - left >= 8) {\n                __m256i left_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l_sorted.data() + left));\n                __m256i right_vec = _mm256_set1_epi32(l_sorted[right]);\n                __m256i sum_vec = _mm256_add_epi32(left_vec, right_vec);\n                __m256i target_vec = _mm256_set1_epi32(target);\n                __m256i cmp = _mm256_cmpeq_epi32(sum_vec, target_vec);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n                __m256i cmp_lt = _mm256_cmpgt_epi32(target_vec, sum_vec);\n                int mask_lt = _mm256_movemask_epi8(cmp_lt);\n                if (mask_lt == 0xFFFFFFFF) {\n                    left += 8;\n                } else {\n                    int sum = l_sorted[left] + l_sorted[right];\n                    if (sum == target) {\n                        return true;\n                    } else if (sum < target) {\n                        left++;\n                    } else {\n                        right--;\n                    }\n                }\n            } else {\n                int sum = l_sorted[left] + l_sorted[right];\n                if (sum == target) {\n                    return true;\n                } else if (sum < target) {\n                    left++;\n                } else {\n                    right--;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <algorithm>\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) \n        return false;\n    std::sort(l.begin(), l.end());\n    \n    for (int i = 0; i < n - 2; i++) {\n        if (i > 0 && l[i] == l[i-1])\n            continue;\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n        \n        while (j < k) {\n            if (k - j >= 8) {\n                __m256i j_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[j]));\n                __m256i k_broadcast = _mm256_set1_epi32(l[k]);\n                __m256i sum_vec = _mm256_add_epi32(j_vec, k_broadcast);\n                __m256i target_vec = _mm256_set1_epi32(target);\n                __m256i cmp_eq = _mm256_cmpeq_epi32(sum_vec, target_vec);\n                if (_mm256_movemask_epi8(cmp_eq) != 0) {\n                    return true;\n                }\n                \n                __m256i cmp_less = _mm256_cmpgt_epi32(target_vec, sum_vec);\n                if (_mm256_testc_si256(cmp_less, _mm256_set1_epi32(-1))) {\n                    j += 8;\n                    continue;\n                }\n                \n                __m256i cmp_greater = _mm256_cmpgt_epi32(sum_vec, target_vec);\n                if (_mm256_testc_si256(cmp_greater, _mm256_set1_epi32(-1))) {\n                    k--;\n                    continue;\n                }\n                \n                if (l[j] + l[k] < target) {\n                    j++;\n                } else {\n                    k--;\n                }\n            } else {\n                int sum = l[j] + l[k];\n                if (sum == target) {\n                    return true;\n                } else if (sum < target) {\n                    j++;\n                } else {\n                    k--;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <immintrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) \n        return false;\n    \n    std::sort(l.begin(), l.end());\n    \n    for (int i = 0; i < n - 2; ++i) {\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n        \n        while (j < k) {\n            if (j + 7 < k) {\n                __m256i j_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[j]));\n                __m256i k_val = _mm256_set1_epi32(l[k]);\n                __m256i sum_jk = _mm256_add_epi32(j_vec, k_val);\n                __m256i target_vec = _mm256_set1_epi32(target);\n                __m256i cmp_jk = _mm256_cmpeq_epi32(sum_jk, target_vec);\n                if (!_mm256_testz_si256(cmp_jk, cmp_jk)) {\n                    return true;\n                }\n\n                __m256i k_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[k - 7]));\n                __m256i j_val = _mm256_set1_epi32(l[j]);\n                __m256i sum_kj = _mm256_add_epi32(j_val, k_vec);\n                __m256i cmp_kj = _mm256_cmpeq_epi32(sum_kj, target_vec);\n                if (!_mm256_testz_si256(cmp_kj, cmp_kj)) {\n                    return true;\n                }\n\n                if (l[j + 7] + l[k] < target) {\n                    j += 8;\n                    continue;\n                } else if (l[j] + l[k - 7] > target) {\n                    k -= 8;\n                    continue;\n                }\n            }\n            \n            int sum = l[j] + l[k];\n            if (sum == target) {\n                return true;\n            } else if (sum < target) {\n                ++j;\n            } else {\n                --k;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    int* data = l.data();\n    int i = 0;\n\n    __m256i one = _mm256_set1_epi32(1);\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(data + i));\n        vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)(data + i), vec);\n    }\n\n    for (; i < n; i++) {\n        data[i]++;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t i = 0;\n    std::vector<int> result(n);\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i res = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&result[i], res);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i res_vec = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for (; i + 7 < n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i result = _mm256_add_epi32(data, ones);\n        _mm256_storeu_si256((__m256i*)&l[i], result);\n    }\n\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t i = 0;\n    const size_t vec_size = 8; // AVX2 processes 8 ints at once\n    size_t end_vec = n - (n % vec_size);\n\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for (; i < end_vec; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i result = _mm256_add_epi32(data, ones);\n        _mm256_storeu_si256((__m256i*)&l[i], result);\n    }\n\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    for (int i = 0; i < n; i++) {\n        int target = -l[i];\n        __m256i target_vec = _mm256_set1_epi32(target);\n\n        int j = 0;\n        for (; j <= n - 8; j += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i const*)(l.data() + j));\n            __m256i cmp = _mm256_cmpeq_epi32(target_vec, data);\n            unsigned int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                for (int k = 0; k < 8; k++) {\n                    if (l[j + k] == target && (j + k) != i) {\n                        return true;\n                    }\n                }\n            }\n        }\n\n        for (; j < n; j++) {\n            if (j == i) continue;\n            if (l[j] == target) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) {\n        return false;\n    }\n    for (int i = 0; i < n; i++) {\n        int x = l[i];\n        int neg_x = -x;\n        __m256i v_neg_x = _mm256_set1_epi32(neg_x);\n        int j = 0;\n        for (; j <= n - 8; j += 8) {\n            __m256i chunk = _mm256_loadu_si256((__m256i const*)(l.data() + j));\n            __m256i cmp = _mm256_cmpeq_epi32(chunk, v_neg_x);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            if (mask != 0) {\n                if (i >= j && i < j + 8) {\n                    mask &= ~(1 << (i - j));\n                }\n                if (mask != 0) {\n                    return true;\n                }\n            }\n        }\n        for (; j < n; j++) {\n            if (j == i) {\n                continue;\n            }\n            if (l[j] == neg_x) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) \n        return false;\n\n    __m256i base_indices = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n    \n    for (int i = 0; i < n; i++) {\n        int a = l[i];\n        __m256i neg_a_vec = _mm256_set1_epi32(-a);\n        int j = 0;\n        \n        for (; j <= n - 8; j += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i const*)(l.data() + j));\n            __m256i cmp = _mm256_cmpeq_epi32(data, neg_a_vec);\n            \n            if (j <= i && i < j + 8) {\n                int pos = i - j;\n                __m256i pos_v = _mm256_set1_epi32(pos);\n                __m256i mask_skip = _mm256_cmpeq_epi32(base_indices, pos_v);\n                cmp = _mm256_andnot_si256(mask_skip, cmp);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        \n        if (j < n) {\n            int remaining = n - j;\n            __m256i valid_mask_vec = _mm256_setr_epi32(\n                (j + 0 < n) ? -1 : 0,\n                (j + 1 < n) ? -1 : 0,\n                (j + 2 < n) ? -1 : 0,\n                (j + 3 < n) ? -1 : 0,\n                (j + 4 < n) ? -1 : 0,\n                (j + 5 < n) ? -1 : 0,\n                (j + 6 < n) ? -1 : 0,\n                (j + 7 < n) ? -1 : 0\n            );\n            __m256i data = _mm256_maskload_epi32(l.data() + j, valid_mask_vec);\n            __m256i cmp = _mm256_cmpeq_epi32(data, neg_a_vec);\n            cmp = _mm256_and_si256(cmp, valid_mask_vec);\n            \n            if (j <= i && i < n) {\n                int pos = i - j;\n                __m256i pos_v = _mm256_set1_epi32(pos);\n                __m256i mask_skip = _mm256_cmpeq_epi32(base_indices, pos_v);\n                cmp = _mm256_andnot_si256(mask_skip, cmp);\n            }\n            \n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) {\n        return false;\n    }\n\n    for (int i = 0; i < n - 1; ++i) {\n        int a = l[i];\n        int neg_a_val = -a;\n        __m256i neg_a = _mm256_set1_epi32(neg_a_val);\n        int j = i + 1;\n\n        while (j < n) {\n            int remaining = n - j;\n            if (remaining < 8) {\n                for (int k = j; k < n; ++k) {\n                    if (l[k] == neg_a_val) {\n                        return true;\n                    }\n                }\n                break;\n            }\n\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l.data() + j));\n            __m256i cmp = _mm256_cmpeq_epi32(chunk, neg_a);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n            j += 8;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) \n        return false;\n\n    for (int i = 0; i < n; i++) {\n        int a = l[i];\n        if (a == INT_MIN)\n            continue;\n\n        int neg_a = -a;\n        __m256i needle = _mm256_set1_epi32(neg_a);\n        bool found = false;\n        int j = 0;\n        for (; j <= n - 8; j += 8) {\n            __m256i haystack = _mm256_loadu_si256((__m256i*)(l.data() + j));\n            __m256i cmp = _mm256_cmpeq_epi32(needle, haystack);\n\n            if (j <= i && i < j + 8) {\n                __m256i base = _mm256_setr_epi32(j, j+1, j+2, j+3, j+4, j+5, j+6, j+7);\n                __m256i i_vec = _mm256_set1_epi32(i);\n                __m256i mask_i = _mm256_cmpeq_epi32(base, i_vec);\n                mask_i = _mm256_xor_si256(mask_i, _mm256_set1_epi32(-1));\n                cmp = _mm256_and_si256(cmp, mask_i);\n            }\n\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                found = true;\n                break;\n            }\n        }\n\n        if (found) \n            return true;\n\n        for (; j < n; j++) {\n            if (j == i) \n                continue;\n            if (l[j] == neg_a) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::vector<uint8_t> digits;\n    int64_t num = x;\n    while (num) {\n        digits.push_back(num % base);\n        num = num / base;\n    }\n    size_t n = digits.size();\n    uint8_t* arr = digits.data();\n\n    const __m128i mask128 = _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n    const __m256i mask = _mm256_broadcastsi128_si256(mask128);\n    size_t i = 0, j = n - 1;\n\n    while (i < j && i + 32 <= j - 31) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<__m256i*>(arr + i));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<__m256i*>(arr + j - 31));\n\n        __m256i rev1 = _mm256_shuffle_epi8(vec1, mask);\n        rev1 = _mm256_permute2x128_si256(rev1, rev1, 0x01);\n        __m256i rev2 = _mm256_shuffle_epi8(vec2, mask);\n        rev2 = _mm256_permute2x128_si256(rev2, rev2, 0x01);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(arr + i), rev2);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(arr + j - 31), rev1);\n\n        i += 32;\n        j -= 32;\n    }\n\n    while (i < j) {\n        std::swap(arr[i], arr[j]);\n        i++;\n        j--;\n    }\n\n    __m256i zero_char = _mm256_set1_epi8('0');\n    size_t idx = 0;\n    for (; idx + 32 <= n; idx += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<__m256i*>(arr + idx));\n        vec = _mm256_add_epi8(vec, zero_char);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(arr + idx), vec);\n    }\n    for (; idx < n; idx++) {\n        arr[idx] = '0' + arr[idx];\n    }\n\n    return std::string(arr, arr + n);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    if (x < 0 || base < 2 || base > 9) {\n        return \"\";\n    }\n\n    uint8_t digits[64] = {0};\n    int n = 0;\n    int64_t num = x;\n    while (num) {\n        digits[n] = num % base;\n        num /= base;\n        n++;\n    }\n\n    uint8_t reversed_digits[64] = {0};\n    int num_chunks = (n + 31) / 32;\n    const __m256i rev_mask = _mm256_set_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n    );\n\n    for (int i = 0; i < num_chunks; i++) {\n        int start_index = n - (i+1)*32;\n        int actual = 32;\n        if (start_index < 0) {\n            actual = 32 + start_index;\n            start_index = 0;\n        }\n        uint8_t buffer[32] = {0};\n        for (int j = 0; j < actual; j++) {\n            buffer[32 - actual + j] = digits[start_index + j];\n        }\n        __m256i chunk = _mm256_loadu_si256((__m256i*)buffer);\n        __m256i reversed_chunk = _mm256_shuffle_epi8(chunk, rev_mask);\n        uint8_t reversed_buffer[32];\n        _mm256_storeu_si256((__m256i*)reversed_buffer, reversed_chunk);\n        for (int j = 0; j < actual; j++) {\n            reversed_digits[i*32 + j] = reversed_buffer[j];\n        }\n    }\n\n    std::string res;\n    res.resize(n);\n    __m256i zero_vec = _mm256_set1_epi8('0');\n    int char_chunks = (n + 31) / 32;\n    for (int i = 0; i < char_chunks; i++) {\n        int start = i * 32;\n        int actual = (n - start) < 32 ? (n - start) : 32;\n        uint8_t digit_buffer[32] = {0};\n        for (int j = 0; j < actual; j++) {\n            digit_buffer[j] = reversed_digits[start+j];\n        }\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)digit_buffer);\n        __m256i char_vec = _mm256_add_epi8(digit_vec, zero_vec);\n        uint8_t char_buffer[32];\n        _mm256_storeu_si256((__m256i*)char_buffer, char_vec);\n        for (int j = 0; j < actual; j++) {\n            res[start+j] = char_buffer[j];\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    \n    uint64_t base8 = 1;\n    for (int i = 0; i < 8; ++i) {\n        base8 *= base;\n    }\n    \n    std::string result;\n    \n    uint64_t ux = static_cast<uint64_t>(x);\n    while (ux >= base8) {\n        uint64_t rem = ux % base8;\n        ux = ux / base8;\n        \n        uint64_t d[8] = {0};\n        uint64_t temp = rem;\n        for (int i = 0; i < 8; ++i) {\n            d[i] = temp % base;\n            temp = temp / base;\n        }\n        \n        uint64_t packed = static_cast<uint64_t>(d[0]) |\n                          (static_cast<uint64_t>(d[1]) << 8 |\n                          (static_cast<uint64_t>(d[2]) << 16 |\n                          (static_cast<uint64_t>(d[3]) << 24 |\n                          (static_cast<uint64_t>(d[4]) << 32 |\n                          (static_cast<uint64_t>(d[5]) << 40 |\n                          (static_cast<uint64_t>(d[6]) << 48 |\n                          (static_cast<uint64_t>(d[7]) << 56;\n        \n        __m128i low_part = _mm_set_epi64x(0, packed);\n        __m256i digit_vec = _mm256_castsi128_si256(low_part);\n        __m256i char_vec = _mm256_add_epi8(digit_vec, _mm256_set1_epi8('0'));\n        \n        char temp_chars[8];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp_chars), _mm256_castsi256_si128(char_vec));\n        result.append(temp_chars, 8);\n    }\n    \n    uint64_t remaining = ux;\n    while (remaining) {\n        uint64_t digit = remaining % base;\n        remaining = remaining / base;\n        result.push_back('0' + static_cast<char>(digit));\n    }\n    \n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    if (x < 0) {\n        return \"\";\n    }\n\n    uint8_t digits[65];\n    int idx = 0;\n    int64_t num = x;\n\n    while (num) {\n        digits[idx++] = static_cast<uint8_t>(num % base);\n        num = num / base;\n    }\n\n    for (int i = 0; i < idx / 2; i++) {\n        uint8_t temp = digits[i];\n        digits[i] = digits[idx - 1 - i];\n        digits[idx - 1 - i] = temp;\n    }\n\n    char buffer[65];\n    buffer[idx] = '\\0';\n\n    constexpr int chunk_size = 32;\n    __m256i ascii_zero = _mm256_set1_epi8('0');\n\n    int i = 0;\n    for (; i + chunk_size <= idx; i += chunk_size) {\n        __m256i digit_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(digits + i));\n        __m256i char_vec = _mm256_add_epi8(digit_vec, ascii_zero);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(buffer + i), char_vec);\n    }\n\n    for (; i < idx; i++) {\n        buffer[i] = '0' + digits[i];\n    }\n\n    return std::string(buffer);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n\n    uint64_t base8 = 1;\n    for (int i = 0; i < 8; i++) {\n        base8 *= base;\n    }\n\n    std::string res;\n    do {\n        uint32_t chunk = static_cast<uint32_t>(x % base8);\n        x = x / base8;\n\n        uint8_t the_digits[8] = {0};\n        for (int i = 0; i < 8; i++) {\n            the_digits[i] = chunk % base;\n            chunk /= base;\n        }\n\n        __m128i vec_digits_in = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(the_digits));\n        __m128i reversal_mask = _mm_set_epi8(\n            -128, -128, -128, -128, -128, -128, -128, -128,\n            7, 6, 5, 4, 3, 2, 1, 0\n        );\n        __m128i vec_reversed = _mm_shuffle_epi8(vec_digits_in, reversal_mask);\n        __m128i vec_zero = _mm_set1_epi8('0');\n        __m128i vec_chars = _mm_add_epi8(vec_reversed, vec_zero);\n\n        alignas(16) uint8_t char_array[16];\n        _mm_store_si128(reinterpret_cast<__m128i*>(char_array), vec_chars);\n\n        std::string chunk_str;\n        if (x == 0) {\n            bool leading = true;\n            for (int i = 0; i < 8; i++) {\n                if (leading && char_array[i] == '0') {\n                    continue;\n                }\n                leading = false;\n                chunk_str += static_cast<char>(char_array[i]);\n            }\n            if (chunk_str.empty()) {\n                chunk_str = \"0\";\n            }\n        } else {\n            for (int i = 0; i < 8; i++) {\n                chunk_str += static_cast<char>(char_array[i]);\n            }\n        }\n\n        res = chunk_str + res;\n    } while (x);\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) \n        return true;\n\n    const char* data = text.data();\n    size_t end = len / 2;\n    size_t i = 0;\n\n    const __m256i rev_mask = _mm256_set_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n    );\n    const __m256i rev16_mask = _mm256_set_epi8(\n        15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n        15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n    );\n\n    for (; i + 32 <= end; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + len - i - 32));\n        \n        __m256i swapped = _mm256_permute2x128_si256(right, right, 0x01);\n        __m256i right_rev = _mm256_shuffle_epi8(swapped, rev16_mask);\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left, right_rev);\n        unsigned mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF)\n            return false;\n    }\n\n    for (size_t j = len - i - 1; i < end; ++i, --j) {\n        if (data[i] != data[j])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.size();\n    if (n == 0) \n        return true;\n\n    const char* str = text.data();\n    size_t left = 0;\n    size_t right = n - 1;\n\n    const __m256i shuffle_mask = _mm256_set_epi8(\n        0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,\n        0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n    );\n\n    while (right - left + 1 >= 32) {\n        __m256i left_vec = _mm256_loadu_si256((const __m256i*)(str + left));\n        __m256i right_vec = _mm256_loadu_si256((const __m256i*)(str + right - 31));\n        \n        __m256i reversed_right_vec = _mm256_shuffle_epi8(right_vec, shuffle_mask);\n        reversed_right_vec = _mm256_permute2x128_si256(reversed_right_vec, reversed_right_vec, 0x01);\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left_vec, reversed_right_vec);\n        unsigned mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) \n            return false;\n        \n        left += 32;\n        right -= 32;\n    }\n\n    while (left < right) {\n        if (str[left] != str[right])\n            return false;\n        left++;\n        right--;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.size();\n    if (n == 0) \n        return true;\n    \n    const char* str = text.data();\n    size_t half = n / 2;\n    size_t i = 0;\n    \n    static const uint8_t reverse_mask_array[32] = {\n        15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n        15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n    };\n    const __m256i reverse_mask = _mm256_loadu_si256((const __m256i*)reverse_mask_array);\n\n    while (i + 32 <= half) {\n        __m256i left = _mm256_loadu_si256((const __m256i*)(str + i));\n        __m256i right = _mm256_loadu_si256((const __m256i*)(str + n - i - 32));\n        \n        __m256i reversed_right = _mm256_shuffle_epi8(right, reverse_mask);\n        reversed_right = _mm256_permute4x64_epi64(reversed_right, 0x4E);\n        \n        __m256i cmp = _mm256_cmpeq_epi8(left, reversed_right);\n        unsigned int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) \n            return false;\n        \n        i += 32;\n    }\n\n    while (i < half) {\n        if (str[i] != str[n - 1 - i]) \n            return false;\n        i++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.length();\n    if (n == 0) \n        return true;\n\n    const char* left_ptr = text.data();\n    const char* right_ptr = text.data() + n - 1;\n\n    const __m256i rev_mask = _mm256_set_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n    );\n\n    while (left_ptr + 63 <= right_ptr) {\n        __m256i front = _mm256_loadu_si256((const __m256i*)left_ptr);\n        __m256i back = _mm256_loadu_si256((const __m256i*)(right_ptr - 31));\n        __m256i reversed_back = _mm256_shuffle_epi8(back, rev_mask);\n\n        __m256i cmp = _mm256_cmpeq_epi8(front, reversed_back);\n        unsigned mask = (unsigned)_mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n\n        left_ptr += 32;\n        right_ptr -= 32;\n    }\n\n    while (left_ptr < right_ptr) {\n        if (*left_ptr != *right_ptr)\n            return false;\n        left_ptr++;\n        right_ptr--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.size();\n    if (n == 0) \n        return true;\n\n    size_t half = n / 2;\n    size_t i = 0;\n\n    static const __m256i reverse_mask = _mm256_set_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n    );\n\n    while (i + 32 <= half) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.data() + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.data() + n - i - 32));\n        __m256i right_reversed = _mm256_shuffle_epi8(right, reverse_mask);\n        __m256i cmp = _mm256_cmpeq_epi8(left, right_reversed);\n        unsigned mask = (unsigned)_mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n        i += 32;\n    }\n\n    for (; i < half; i++) {\n        if (text[i] != text[n - i - 1]) \n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, '\\0');\n    size_t i = 0;\n\n    const __m256i shift_val = _mm256_set1_epi8(5);\n    const __m256i threshold = _mm256_set1_epi8('a');\n    const __m256i wrap_adjust = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(data, shift_val);\n        __m256i mask = _mm256_cmpgt_epi8(threshold, shifted);\n        __m256i adjust = _mm256_and_si256(mask, wrap_adjust);\n        __m256i result = _mm256_add_epi8(shifted, adjust);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.data() + i), result);\n    }\n\n    for (; i < len; ++i) {\n        char c = s[i];\n        char shifted = c - 5;\n        if (shifted < 'a') {\n            out[i] = shifted + 26;\n        } else {\n            out[i] = shifted;\n        }\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    size_t len = s.length();\n    std::string out(len, '\\0');\n    size_t i = 0;\n    const size_t vectorSize = 32;\n    size_t vectorEnd = len - (len % vectorSize);\n\n    for (; i < vectorEnd; i += vectorSize) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(vec, _mm256_set1_epi8(5));\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a' - 1), shifted);\n        __m256i to_add = _mm256_and_si256(mask, _mm256_set1_epi8(26));\n        shifted = _mm256_add_epi8(shifted, to_add);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.data() + i), shifted);\n    }\n\n    for (; i < len; ++i) {\n        unsigned char c = s[i];\n        c -= 5;\n        if (c < 'a') {\n            c += 26;\n        }\n        out[i] = c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.size());\n    size_t len = s.size();\n    size_t i = 0;\n\n    const __m256i five = _mm256_set1_epi8(5);\n    const __m256i a_vec = _mm256_set1_epi8('a');\n    const __m256i twenty_six = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(data, five);\n        __m256i mask = _mm256_cmpgt_epi8(a_vec, shifted);\n        __m256i adjustment = _mm256_and_si256(mask, twenty_six);\n        __m256i result = _mm256_add_epi8(shifted, adjustment);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.data() + i), result);\n    }\n\n    for (; i < len; ++i) {\n        char c = s[i];\n        char t = c - 5;\n        if (t < 'a') {\n            t += 26;\n        }\n        out[i] = t;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, '\\0');\n    size_t i = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i shifted = _mm256_sub_epi8(data, _mm256_set1_epi8('a'));\n            shifted = _mm256_add_epi8(shifted, _mm256_set1_epi8(21));\n            __m256i mask = _mm256_cmpgt_epi8(shifted, _mm256_set1_epi8(25));\n            __m256i adjust = _mm256_and_si256(mask, _mm256_set1_epi8(26));\n            shifted = _mm256_sub_epi8(shifted, adjust);\n            __m256i result = _mm256_add_epi8(shifted, _mm256_set1_epi8('a'));\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.data() + i), result);\n        }\n    }\n\n    for (; i < len; i++) {\n        int x = s[i] - 'a';\n        x = (x + 21) % 26;\n        out[i] = x + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    if (s.empty()) {\n        return out;\n    }\n    out.resize(s.length());\n    size_t i = 0;\n    const size_t len = s.length();\n    const size_t simd_size = 32;\n\n    for (; i + simd_size <= len; i += simd_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i a_vec = _mm256_set1_epi8('a');\n        __m256i x = _mm256_sub_epi8(chunk, a_vec);\n        __m256i twenty_one = _mm256_set1_epi8(21);\n        __m256i y = _mm256_add_epi8(x, twenty_one);\n        __m256i twenty_five = _mm256_set1_epi8(25);\n        __m256i mask = _mm256_cmpgt_epi8(y, twenty_five);\n        __m256i twenty_six = _mm256_set1_epi8(26);\n        __m256i adjust = _mm256_and_si256(mask, twenty_six);\n        __m256i z = _mm256_sub_epi8(y, adjust);\n        __m256i decoded_vec = _mm256_add_epi8(z, a_vec);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), decoded_vec);\n    }\n\n    for (; i < len; i++) {\n        unsigned char t = s[i] - 'a';\n        t += 21;\n        if (t >= 26) {\n            t -= 26;\n        }\n        out[i] = t + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    const size_t n = l.size();\n    if (n == 0) \n        return true;\n\n    __m256i t_vec = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(-1);\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(l.data() + i));\n        __m256i cmp = _mm256_cmpgt_epi32(t_vec, data);\n        if (!_mm256_testc_si256(cmp, all_ones)) \n            return false;\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) \n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int n = l.size();\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i thresh_vec = _mm256_set1_epi32(t);\n        __m256i all_ones = _mm256_set1_epi32(-1);\n\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l.data() + i));\n            __m256i mask_lt = _mm256_cmpgt_epi32(thresh_vec, data);\n            if (!_mm256_testc_si256(mask_lt, all_ones)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t n = l.size();\n    size_t i = 0;\n\n    if (n >= 8) {\n        __m256i thresh_vec = _mm256_set1_epi32(t);\n        __m256i all_ones = _mm256_set1_epi32(-1);\n\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l.data() + i));\n            __m256i mask = _mm256_cmpgt_epi32(thresh_vec, data);\n            if (!_mm256_testc_si256(mask, all_ones)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    if (l.empty()) {\n        return true;\n    }\n    __m256i thresh = _mm256_set1_epi32(t);\n    size_t i = 0;\n    const size_t n = l.size();\n    const size_t aligned_end = n & ~static_cast<size_t>(7);\n\n    for (; i < aligned_end; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l.data() + i));\n        __m256i cmp = _mm256_cmpgt_epi32(thresh, vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != -1) {\n            return false;\n        }\n    }\n\n    for (; i < n; i++) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t len = l.size();\n    size_t i = 0;\n    \n    if (len >= 8) {\n        __m256i thresh_vec = _mm256_set1_epi32(t);\n        \n        for (; i <= len - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(l.data() + i));\n            __m256i cmp = _mm256_cmpgt_epi32(thresh_vec, data);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != -1) {\n                return false;\n            }\n        }\n    }\n    \n    for (; i < len; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.length();\n    int running_total = 0;\n    int pos = 0;\n\n    // Process full chunks of 32 bytes\n    for (; pos + 31 < len; pos += 32) {\n        __m256i v_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + pos));\n        __m256i v_gt = _mm256_cmpeq_epi8(v_data, _mm256_set1_epi8('>'));\n        __m256i v_lt = _mm256_cmpeq_epi8(v_data, _mm256_set1_epi8('<'));\n\n        __m256i v_ones = _mm256_set1_epi8(1);\n        __m256i v_delta = _mm256_sub_epi8(\n            _mm256_and_si256(v_lt, v_ones),\n            _mm256_and_si256(v_gt, v_ones)\n        );\n\n        alignas(32) int8_t delta_arr[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(delta_arr), v_delta);\n\n        int chunk_sum = 0;\n        int min_in_chunk = 0;\n        int current = 0;\n        for (int i = 0; i < 32; i++) {\n            current += delta_arr[i];\n            if (current < min_in_chunk) {\n                min_in_chunk = current;\n            }\n        }\n        chunk_sum = current;\n\n        if (running_total + min_in_chunk < 0) {\n            return false;\n        }\n        running_total += chunk_sum;\n    }\n\n    // Process remaining bytes\n    for (; pos < len; pos++) {\n        if (brackets[pos] == '<') {\n            running_total++;\n        } else if (brackets[pos] == '>') {\n            running_total--;\n        }\n        if (running_total < 0) {\n            return false;\n        }\n    }\n\n    return running_total == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.length();\n    const char* ptr = brackets.c_str();\n    int total = 0;\n\n    int i = 0;\n    for (; i <= len - 8; i += 8) {\n        __m128i chunk = _mm_loadl_epi64((const __m128i*)(ptr + i));\n        __m256i bytes = _mm256_cvtepu8_epi32(chunk);\n        __m256i mask_lt = _mm256_cmpeq_epi32(bytes, _mm256_set1_epi32('<'));\n        __m256i mask_gt = _mm256_cmpeq_epi32(bytes, _mm256_set1_epi32('>'));\n        __m256i deltas = _mm256_sub_epi32(mask_gt, mask_lt);\n\n        alignas(32) int delta_arr[8];\n        _mm256_store_si256((__m256i*)delta_arr, deltas);\n\n        for (int j = 0; j < 8; ++j) {\n            total += delta_arr[j];\n            if (total < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (ptr[i] == '<') {\n            total++;\n        } else if (ptr[i] == '>') {\n            total--;\n        }\n        if (total < 0) {\n            return false;\n        }\n    }\n\n    return total == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    if (len == 0) {\n        return true;\n    }\n\n    for (size_t i = 0; i < len; i += 32) {\n        int chunk_len = (i + 32 <= len) ? 32 : len - i;\n        char buffer[32] = {0};\n        for (int j = 0; j < chunk_len; j++) {\n            buffer[j] = brackets[i + j];\n        }\n\n        __m256i chunk_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(buffer));\n        __m256i mask_less = _mm256_cmpeq_epi8(chunk_vec, _mm256_set1_epi8('<'));\n        __m256i mask_greater = _mm256_cmpeq_epi8(chunk_vec, _mm256_set1_epi8('>'));\n\n        unsigned int movemask_less = _mm256_movemask_epi8(mask_less);\n        unsigned int movemask_greater = _mm256_movemask_epi8(mask_greater);\n        int n_less = _mm_popcnt_u32(movemask_less);\n        int n_greater = _mm_popcnt_u32(movemask_greater);\n        int chunk_sum = n_less - n_greater;\n\n        int rel_min = 0;\n        int current_rel = 0;\n        for (int j = 0; j < chunk_len; j++) {\n            if (buffer[j] == '<') {\n                current_rel++;\n            } else if (buffer[j] == '>') {\n                current_rel--;\n            }\n            if (current_rel < rel_min) {\n                rel_min = current_rel;\n            }\n        }\n\n        if (balance + rel_min < 0) {\n            return false;\n        }\n        balance += chunk_sum;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int current_balance = 0;\n    size_t i = 0;\n    size_t n = brackets.size();\n    \n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n            __m256i gt_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n            __m256i lt_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n            __m256i values = _mm256_sub_epi8(gt_mask, lt_mask);\n            \n            alignas(32) int8_t values_arr[32];\n            _mm256_store_si256(reinterpret_cast<__m256i*>(values_arr), values);\n            \n            int s = current_balance;\n            int min_in_chunk = s;\n            for (int j = 0; j < 32; j++) {\n                s += values_arr[j];\n                if (s < min_in_chunk) {\n                    min_in_chunk = s;\n                }\n            }\n            \n            if (min_in_chunk < 0) {\n                return false;\n            }\n            current_balance = s;\n        }\n    }\n    \n    for (; i < n; i++) {\n        if (brackets[i] == '<') {\n            current_balance++;\n        } else if (brackets[i] == '>') {\n            current_balance--;\n        }\n        if (current_balance < 0) {\n            return false;\n        }\n    }\n    \n    return current_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int base = 0;\n    int i = 0;\n\n    for (; i + 32 <= n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        __m256i gt = _mm256_set1_epi8('>');\n        __m256i lt = _mm256_set1_epi8('<');\n        __m256i mask_gt = _mm256_cmpeq_epi8(chunk, gt);\n        __m256i mask_lt = _mm256_cmpeq_epi8(chunk, lt);\n        __m256i values = _mm256_sub_epi8(\n            _mm256_and_si256(mask_lt, _mm256_set1_epi8(1)),\n            _mm256_and_si256(mask_gt, _mm256_set1_epi8(1))\n        );\n\n        __m128i low_128 = _mm256_castsi256_si128(values);\n        __m128i high_128 = _mm256_extracti128_si256(values, 1);\n\n        __m256i v0 = _mm256_cvtepi8_epi32(low_128);\n        __m256i v1 = _mm256_cvtepi8_epi32(_mm_srli_si128(low_128, 8));\n        __m256i v2 = _mm256_cvtepi8_epi32(high_128);\n        __m256i v3 = _mm256_cvtepi8_epi32(_mm_srli_si128(high_128, 8));\n\n        __m256i vecs[4] = {v0, v1, v2, v3};\n        for (int j = 0; j < 4; ++j) {\n            __m256i v = vecs[j];\n            __m256i shifted1 = _mm256_slli_si256(v, 4);\n            v = _mm256_add_epi32(v, shifted1);\n            __m256i shifted2 = _mm256_slli_si256(v, 8);\n            v = _mm256_add_epi32(v, shifted2);\n            __m128i low_v = _mm256_castsi256_si128(v);\n            __m128i high_v = _mm256_extracti128_si256(v, 1);\n            __m128i last = _mm_shuffle_epi32(low_v, _MM_SHUFFLE(3, 3, 3, 3));\n            high_v = _mm_add_epi32(high_v, last);\n            v = _mm256_inserti128_si256(v, high_v, 1);\n\n            __m128i low = _mm256_castsi256_si128(v);\n            __m128i high = _mm256_extracti128_si256(v, 1);\n            __m128i min128 = _mm_min_epi32(low, high);\n            __m128i min64 = _mm_shuffle_epi32(min128, _MM_SHUFFLE(1, 0, 3, 2));\n            min128 = _mm_min_epi32(min128, min64);\n            __m128i min32 = _mm_shuffle_epi32(min128, _MM_SHUFFLE(2, 3, 0, 1));\n            min128 = _mm_min_epi32(min128, min32);\n            int min_prefix = _mm_extract_epi32(min128, 0);\n            int sum_vector = _mm256_extract_epi32(v, 7);\n\n            if (base + min_prefix < 0) {\n                return false;\n            }\n            base += sum_vector;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            base++;\n        } else if (brackets[i] == '>') {\n            base--;\n        }\n        if (base < 0) {\n            return false;\n        }\n    }\n\n    return base == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const char* str = brackets.c_str();\n    size_t len = brackets.length();\n    size_t i = 0;\n    int global_sum = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n        __m256i open_val = _mm256_sub_epi8(_mm256_setzero_si256(), open_mask);\n        __m256i values = _mm256_add_epi8(open_val, close_mask);\n\n        alignas(32) int8_t values_arr[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(values_arr), values);\n\n        for (int j = 0; j < 32; ++j) {\n            global_sum += values_arr[j];\n            if (global_sum < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (str[i] == '(') {\n            ++global_sum;\n        } else if (str[i] == ')') {\n            --global_sum;\n        }\n        if (global_sum < 0) {\n            return false;\n        }\n    }\n\n    return global_sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int current_sum = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i open_vals = _mm256_and_si256(open_mask, ones);\n        __m256i close_vals = _mm256_and_si256(close_mask, ones);\n        __m256i values = _mm256_sub_epi8(open_vals, close_vals);\n\n        alignas(32) int8_t values_arr[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(values_arr), values);\n\n        for (int j = 0; j < 32; ++j) {\n            current_sum += values_arr[j];\n            if (current_sum < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            ++current_sum;\n        } else if (brackets[i] == ')') {\n            --current_sum;\n        }\n        if (current_sum < 0) {\n            return false;\n        }\n    }\n\n    return current_sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.size();\n    int base = 0;\n    int global_min = 0;\n    size_t i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i vals = _mm256_sub_epi8(\n            _mm256_and_si256(open_mask, ones),\n            _mm256_andnot_si256(open_mask, ones)\n        );\n\n        __m256i scan = vals;\n        scan = _mm256_add_epi8(scan, _mm256_slli_si256(scan, 1));\n        scan = _mm256_add_epi8(scan, _mm256_slli_si256(scan, 2));\n        scan = _mm256_add_epi8(scan, _mm256_slli_si256(scan, 4));\n        scan = _mm256_add_epi8(scan, _mm256_slli_si256(scan, 8));\n\n        __m128i low_lane = _mm256_castsi256_si128(scan);\n        __m128i high_lane = _mm256_extracti128_si256(scan, 1);\n        int8_t carry_val = (int8_t)_mm_extract_epi8(low_lane, 15);\n        __m128i carry_vec = _mm_set1_epi8(carry_val);\n        high_lane = _mm_add_epi8(high_lane, carry_vec);\n\n        int8_t total_chunk_val = (int8_t)_mm_extract_epi8(high_lane, 15);\n        __m128i min_temp = _mm_min_epi8(low_lane, high_lane);\n        __m128i min1 = _mm_min_epi8(min_temp, _mm_srli_si128(min_temp, 8));\n        __m128i min2 = _mm_min_epi8(min1, _mm_srli_si128(min1, 4));\n        __m128i min3 = _mm_min_epi8(min2, _mm_srli_si128(min2, 2));\n        __m128i min4 = _mm_min_epi8(min3, _mm_srli_si128(min3, 1));\n        int8_t min_in_chunk_val = (int8_t)_mm_extract_epi8(min4, 0);\n\n        if (base + min_in_chunk_val < global_min) {\n            global_min = base + min_in_chunk_val;\n        }\n        base += total_chunk_val;\n    }\n\n    for (; i < len; i++) {\n        if (brackets[i] == '(') {\n            base++;\n        } else if (brackets[i] == ')') {\n            base--;\n        }\n        if (base < global_min) {\n            global_min = base;\n        }\n    }\n\n    return (base == 0 && global_min >= 0);\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int depth = 0;\n    size_t n = brackets.size();\n    size_t i = 0;\n\n    // Helper function for 128-bit lane processing\n    auto scan_min_128 = [](__m128i v, int& total_sum) -> int {\n        // Inclusive prefix scan\n        __m128i scan = v;\n        __m128i shifted = _mm_bslli_si128(v, 1);\n        scan = _mm_add_epi8(scan, shifted);\n        shifted = _mm_bslli_si128(scan, 2);\n        scan = _mm_add_epi8(scan, shifted);\n        shifted = _mm_bslli_si128(scan, 4);\n        scan = _mm_add_epi8(scan, shifted);\n        shifted = _mm_bslli_si128(scan, 8);\n        scan = _mm_add_epi8(scan, shifted);\n\n        // Total sum is last element\n        total_sum = _mm_extract_epi8(scan, 15);\n\n        // Include zero in min calculation\n        __m128i zero = _mm_setzero_si128();\n        __m128i scan_with_zero = _mm_min_epi8(scan, zero);\n\n        // Tree reduction for minimum value\n        __m128i min1 = _mm_min_epi8(scan_with_zero, _mm_srli_si128(scan_with_zero, 8));\n        __m128i min2 = _mm_min_epi8(min1, _mm_srli_si128(min1, 4));\n        __m128i min3 = _mm_min_epi8(min2, _mm_srli_si128(min2, 2));\n        __m128i min4 = _mm_min_epi8(min3, _mm_srli_si128(min3, 1));\n        return _mm_extract_epi8(min4, 0);\n    };\n\n    // Process 32-byte chunks with AVX2\n    for (; i + 32 <= n; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        __m256i mask_open = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i mask_close = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n        __m256i increments = _mm256_sub_epi8(mask_close, mask_open);\n\n        // Split into 128-bit lanes\n        __m128i low_lane = _mm256_castsi256_si128(increments);\n        __m128i high_lane = _mm256_extracti128_si256(increments, 1);\n\n        int total_low, total_high;\n        int min_low = scan_min_128(low_lane, total_low);\n        int min_high = scan_min_128(high_lane, total_high);\n\n        int chunk_min = std::min(min_low, total_low + min_high);\n        if (depth + chunk_min < 0) {\n            return false;\n        }\n        depth += total_low + total_high;\n    }\n\n    // Process remaining bytes\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            depth++;\n        } else if (brackets[i] == ')') {\n            depth--;\n        }\n        if (depth < 0) {\n            return false;\n        }\n    }\n\n    return depth == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    int global_sum = 0;\n    size_t i = 0;\n\n    while (i < n) {\n        int chunk_size = std::min(32, static_cast<int>(n - i));\n        __m256i v;\n        if (chunk_size == 32) {\n            v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        } else {\n            alignas(32) char buffer[32] = {0};\n            std::memcpy(buffer, brackets.data() + i, chunk_size);\n            v = _mm256_load_si256(reinterpret_cast<const __m256i*>(buffer));\n        }\n        i += chunk_size;\n\n        __m256i mask_open = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('('));\n        __m256i mask_close = _mm256_cmpeq_epi8(v, _mm256_set1_epi8(')'));\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i signs = _mm256_sub_epi8(\n            _mm256_and_si256(mask_open, ones),\n            _mm256_and_si256(mask_close, ones)\n        );\n\n        __m256i v1 = _mm256_add_epi8(signs, _mm256_srli_si256(signs, 1));\n        __m256i v2 = _mm256_add_epi8(v1, _mm256_srli_si256(v1, 2));\n        __m256i v3 = _mm256_add_epi8(v2, _mm256_srli_si256(v2, 4));\n        __m256i v4 = _mm256_add_epi8(v3, _mm256_srli_si256(v3, 8));\n\n        __m128i low_lane = _mm256_castsi256_si128(v4);\n        __m128i high_lane = _mm256_extracti128_si256(v4, 1);\n        int total_low = static_cast<signed char>(_mm_extract_epi8(low_lane, 15));\n        __m128i total_low_vec = _mm_set1_epi8(static_cast<char>(total_low));\n        __m128i high_lane_adjusted = _mm_add_epi8(high_lane, total_low_vec);\n        __m256i v5_full = _mm256_set_m128i(high_lane_adjusted, low_lane);\n\n        __m256i min_val = v5_full;\n        __m256i perm = _mm256_permute2x128_si256(min_val, min_val, 0x81);\n        min_val = _mm256_min_epi8(min_val, perm);\n        __m128i low128 = _mm256_castsi256_si128(min_val);\n        __m128i shifted = _mm_srli_si128(low128, 8);\n        low128 = _mm_min_epi8(low128, shifted);\n        shifted = _mm_srli_si128(low128, 4);\n        low128 = _mm_min_epi8(low128, shifted);\n        shifted = _mm_srli_si128(low128, 2);\n        low128 = _mm_min_epi8(low128, shifted);\n        shifted = _mm_srli_si128(low128, 1);\n        low128 = _mm_min_epi8(low128, shifted);\n        int min_in_chunk = static_cast<signed char>(_mm_extract_epi8(low128, 0));\n\n        int total_in_chunk = static_cast<signed char>(_mm_extract_epi8(high_lane_adjusted, 15));\n\n        if (global_sum + min_in_chunk < 0) {\n            return false;\n        }\n        global_sum += total_in_chunk;\n    }\n\n    return global_sum == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    std::vector<float> result(xs.size() - 1);\n    int n = result.size();\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256i base = _mm256_set1_epi32(i);\n        __m256i steps = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i indices_int = _mm256_add_epi32(base, steps);\n        indices_int = _mm256_add_epi32(indices_int, _mm256_set1_epi32(1));\n        __m256 indices_float = _mm256_cvtepi32_ps(indices_int);\n        __m256 res = _mm256_mul_ps(coeffs, indices_float);\n        _mm256_storeu_ps(&result[i], res);\n    }\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * static_cast<float>(i + 1);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) {\n        return std::vector<float>();\n    }\n    int res_size = n - 1;\n    std::vector<float> res(res_size);\n    const __m256i step_vector = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    \n    int i = 0;\n    for (; i <= res_size - 8; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256i base = _mm256_set1_epi32(i + 1);\n        __m256i indices = _mm256_add_epi32(base, step_vector);\n        __m256 multipliers = _mm256_cvtepi32_ps(indices);\n        __m256 result_vec = _mm256_mul_ps(coeffs, multipliers);\n        _mm256_storeu_ps(&res[i], result_vec);\n    }\n    \n    for (; i < res_size; i++) {\n        res[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) {\n        return std::vector<float>();\n    }\n    std::vector<float> res(n-1);\n    float* in_ptr = xs.data() + 1;\n    float* out_ptr = res.data();\n\n    static const __m256 step = _mm256_setr_ps(0.0f, 1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= n-1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(in_ptr + i);\n        __m256 base = _mm256_set1_ps(static_cast<float>(1 + i));\n        __m256 indices = _mm256_add_ps(base, step);\n        __m256 product = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(out_ptr + i, product);\n    }\n\n    for (; i < n-1; ++i) {\n        out_ptr[i] = in_ptr[i] * static_cast<float>(1 + i);\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    size_t n = xs.size();\n    std::vector<float> result(n - 1);\n    size_t i = 0;\n\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256 multiplier = _mm256_cvtepi32_ps(indices);\n        __m256 data = _mm256_loadu_ps(&xs[i+1]);\n        __m256 res = _mm256_mul_ps(multiplier, data);\n        _mm256_storeu_ps(&result[i], res);\n    }\n\n    for (; i < n - 1; ++i) {\n        result[i] = (i+1) * xs[i+1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    size_t n = xs.size() - 1;\n    std::vector<float> res(n);\n    const __m256 step_float = _mm256_setr_ps(0.0f, 1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f);\n    \n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256 base = _mm256_set1_ps(static_cast<float>(i+1));\n        __m256 mult_vec = _mm256_add_ps(base, step_float);\n        __m256 coeffs = _mm256_loadu_ps(&xs[i+1]);\n        __m256 product = _mm256_mul_ps(mult_vec, coeffs);\n        _mm256_storeu_ps(&res[i], product);\n    }\n\n    for (; i < n; i++) {\n        res[i] = static_cast<float>(i+1) * xs[i+1];\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    size_t n = s.size();\n    if (n == 0) {\n        return 0;\n    }\n    int count = 0;\n    size_t i = 0;\n\n    __m256i a_vec = _mm256_set1_epi8('a');\n    __m256i e_vec = _mm256_set1_epi8('e');\n    __m256i i_vec = _mm256_set1_epi8('i');\n    __m256i o_vec = _mm256_set1_epi8('o');\n    __m256i u_vec = _mm256_set1_epi8('u');\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n\n    for (; i + 31 < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i low_chunk = _mm256_or_si256(chunk, lower_mask);\n\n        __m256i eq_a = _mm256_cmpeq_epi8(low_chunk, a_vec);\n        __m256i eq_e = _mm256_cmpeq_epi8(low_chunk, e_vec);\n        __m256i eq_i = _mm256_cmpeq_epi8(low_chunk, i_vec);\n        __m256i eq_o = _mm256_cmpeq_epi8(low_chunk, o_vec);\n        __m256i eq_u = _mm256_cmpeq_epi8(low_chunk, u_vec);\n\n        __m256i vowel_any = _mm256_or_si256(eq_a, eq_e);\n        vowel_any = _mm256_or_si256(vowel_any, eq_i);\n        vowel_any = _mm256_or_si256(vowel_any, eq_o);\n        vowel_any = _mm256_or_si256(vowel_any, eq_u);\n\n        int mask = _mm256_movemask_epi8(vowel_any);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < n; i++) {\n        char c = s[i] | 0x20;\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    if ((s[n-1] | 0x20) == 'y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    size_t n = s.size();\n    if (n == 0) {\n        return 0;\n    }\n    const char* data = s.data();\n    int total_std = 0;\n    size_t i = 0;\n    \n    const __m256i mask_0x20 = _mm256_set1_epi8(0x20);\n    const __m256i vowels_a = _mm256_set1_epi8('a');\n    const __m256i vowels_e = _mm256_set1_epi8('e');\n    const __m256i vowels_i = _mm256_set1_epi8('i');\n    const __m256i vowels_o = _mm256_set1_epi8('o');\n    const __m256i vowels_u = _mm256_set1_epi8('u');\n    \n    for (; i + 32 <= n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i low_chunk = _mm256_or_si256(chunk, mask_0x20);\n        \n        __m256i eq_a = _mm256_cmpeq_epi8(low_chunk, vowels_a);\n        __m256i eq_e = _mm256_cmpeq_epi8(low_chunk, vowels_e);\n        __m256i eq_i = _mm256_cmpeq_epi8(low_chunk, vowels_i);\n        __m256i eq_o = _mm256_cmpeq_epi8(low_chunk, vowels_o);\n        __m256i eq_u = _mm256_cmpeq_epi8(low_chunk, vowels_u);\n        \n        __m256i any1 = _mm256_or_si256(eq_a, eq_e);\n        __m256i any2 = _mm256_or_si256(eq_i, eq_o);\n        __m256i any3 = _mm256_or_si256(any1, any2);\n        __m256i vowel_any = _mm256_or_si256(any3, eq_u);\n        \n        unsigned int mask = _mm256_movemask_epi8(vowel_any);\n        total_std += __builtin_popcount(mask);\n    }\n    \n    for (; i < n; i++) {\n        char c = data[i] | 0x20;\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            total_std++;\n        }\n    }\n    \n    char last_char = data[n-1];\n    if (last_char == 'y' || last_char == 'Y') {\n        total_std++;\n    }\n    \n    return total_std;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int n = s.size();\n    if (n == 0) \n        return 0;\n    \n    int count = 0;\n    size_t i = 0;\n    constexpr int vec_size = 32;\n    \n    for (; i + vec_size <= static_cast<size_t>(n); i += vec_size) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i low_chunk = _mm256_or_si256(chunk, _mm256_set1_epi8(0x20));\n        \n        __m256i eq_a = _mm256_cmpeq_epi8(low_chunk, _mm256_set1_epi8('a'));\n        __m256i eq_e = _mm256_cmpeq_epi8(low_chunk, _mm256_set1_epi8('e'));\n        __m256i eq_i = _mm256_cmpeq_epi8(low_chunk, _mm256_set1_epi8('i'));\n        __m256i eq_o = _mm256_cmpeq_epi8(low_chunk, _mm256_set1_epi8('o'));\n        __m256i eq_u = _mm256_cmpeq_epi8(low_chunk, _mm256_set1_epi8('u'));\n        \n        __m256i vowel_mask = _mm256_or_si256(\n            eq_a, _mm256_or_si256(\n                eq_e, _mm256_or_si256(\n                    eq_i, _mm256_or_si256(eq_o, eq_u)\n                )\n            )\n        );\n        \n        unsigned mask = _mm256_movemask_epi8(vowel_mask);\n        count += _mm_popcnt_u32(mask);\n    }\n    \n    for (; i < static_cast<size_t>(n); ++i) {\n        char c = s[i] | 0x20;\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') \n            count++;\n    }\n    \n    char last_char = s[n-1];\n    if (last_char == 'y' || last_char == 'Y') \n        count++;\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int total_vowels = 0;\n    size_t n = s.length();\n    size_t i = 0;\n\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i vowel_mask = _mm256_set1_epi8(0x20);\n    __m256i a_char = _mm256_set1_epi8('a');\n    __m256i e_char = _mm256_set1_epi8('e');\n    __m256i i_char = _mm256_set1_epi8('i');\n    __m256i o_char = _mm256_set1_epi8('o');\n    __m256i u_char = _mm256_set1_epi8('u');\n\n    for (; i + 31 < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i lower_chunk = _mm256_or_si256(chunk, vowel_mask);\n\n        __m256i mask_a = _mm256_cmpeq_epi8(lower_chunk, a_char);\n        __m256i mask_e = _mm256_cmpeq_epi8(lower_chunk, e_char);\n        __m256i mask_i = _mm256_cmpeq_epi8(lower_chunk, i_char);\n        __m256i mask_o = _mm256_cmpeq_epi8(lower_chunk, o_char);\n        __m256i mask_u = _mm256_cmpeq_epi8(lower_chunk, u_char);\n\n        __m256i combined_mask = _mm256_or_si256(mask_a, mask_e);\n        combined_mask = _mm256_or_si256(combined_mask, mask_i);\n        combined_mask = _mm256_or_si256(combined_mask, mask_o);\n        combined_mask = _mm256_or_si256(combined_mask, mask_u);\n\n        int bitmask = _mm256_movemask_epi8(combined_mask);\n        total_vowels += __builtin_popcount(bitmask);\n    }\n\n    for (; i < n; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'A' || c == 'e' || c == 'E' || \n            c == 'i' || c == 'I' || c == 'o' || c == 'O' || \n            c == 'u' || c == 'U') {\n            total_vowels++;\n        }\n    }\n\n    char last_char = s[n-1];\n    if (last_char == 'y' || last_char == 'Y') {\n        total_vowels++;\n    }\n\n    return total_vowels;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    size_t n = s.size();\n    if (n == 0) return 0;\n    \n    int count = 0;\n    size_t i = 0;\n    const __m256i a_vec = _mm256_set1_epi8('a');\n    const __m256i e_vec = _mm256_set1_epi8('e');\n    const __m256i i_vec = _mm256_set1_epi8('i');\n    const __m256i o_vec = _mm256_set1_epi8('o');\n    const __m256i u_vec = _mm256_set1_epi8('u');\n    const __m256i lower_mask = _mm256_set1_epi8(0x20);\n\n    for (; i + 31 < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i lowcase = _mm256_or_si256(chunk, lower_mask);\n\n        __m256i eq_a = _mm256_cmpeq_epi8(lowcase, a_vec);\n        __m256i eq_e = _mm256_cmpeq_epi8(lowcase, e_vec);\n        __m256i eq_i = _mm256_cmpeq_epi8(lowcase, i_vec);\n        __m256i eq_o = _mm256_cmpeq_epi8(lowcase, o_vec);\n        __m256i eq_u = _mm256_cmpeq_epi8(lowcase, u_vec);\n\n        __m256i vowel_any = _mm256_or_si256(_mm256_or_si256(eq_a, eq_e), \n                  _mm256_or_si256(_mm256_or_si256(eq_i, eq_o), eq_u));\n        unsigned int mask = _mm256_movemask_epi8(vowel_any);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < n; i++) {\n        char c = s[i] | 0x20;\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    char last_char = s[n-1];\n    if (last_char == 'y' || last_char == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    const char* p = s.c_str();\n    size_t len = s.length();\n    size_t i = 0;\n    uint64_t total = 0;\n    \n    if (len < 32) {\n        for (; i < len; i++) {\n            if (p[i] >= 'A' && p[i] <= 'Z') {\n                total += (unsigned char)p[i];\n            }\n        }\n        return (int)total;\n    }\n    \n    __m256i v_A_minus1 = _mm256_set1_epi8('A' - 1);\n    __m256i v_Z = _mm256_set1_epi8('Z');\n    __m256i v_zero = _mm256_setzero_si256();\n    \n    for (; i <= len - 32; i += 32) {\n        __m256i v_chars = _mm256_loadu_si256((const __m256i*)(p + i));\n        __m256i mask1 = _mm256_cmpgt_epi8(v_chars, v_A_minus1);\n        __m256i mask2 = _mm256_cmpgt_epi8(v_chars, v_Z);\n        __m256i mask = _mm256_andnot_si256(mask2, mask1);\n        __m256i selected = _mm256_and_si256(v_chars, mask);\n        __m256i partial_sums = _mm256_sad_epu8(selected, v_zero);\n        \n        __m128i low_sums = _mm256_castsi256_si128(partial_sums);\n        __m128i high_sums = _mm256_extracti128_si256(partial_sums, 1);\n        __m128i v_sum = _mm_add_epi64(low_sums, high_sums);\n        \n        uint64_t t[2];\n        _mm_store_si128((__m128i*)t, v_sum);\n        total += t[0] + t[1];\n    }\n    \n    for (; i < len; i++) {\n        if (p[i] >= 'A' && p[i] <= 'Z') {\n            total += (unsigned char)p[i];\n        }\n    }\n    \n    return (int)total;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nint digitSum_simd(std::string s) {\n    const char* ptr = s.data();\n    size_t len = s.length();\n    size_t i = 0;\n    long long total = 0;\n\n    __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n    __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n    __m128i zero128 = _mm_setzero_si128();\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr + i));\n        __m256i gt_A = _mm256_cmpgt_epi8(chunk, A_minus1);\n        __m256i lt_Z = _mm256_cmpgt_epi8(Z_plus1, chunk);\n        __m256i mask = _mm256_and_si256(gt_A, lt_Z);\n        __m256i upper_chars = _mm256_and_si256(chunk, mask);\n\n        __m128i low128 = _mm256_castsi256_si128(upper_chars);\n        __m128i high128 = _mm256_extractf128_si256(upper_chars, 1);\n\n        __m128i sum_low = _mm_sad_epu8(low128, zero128);\n        __m128i sum_high = _mm_sad_epu8(high128, zero128);\n\n        uint64_t low0 = _mm_extract_epi64(sum_low, 0);\n        uint64_t low1 = _mm_extract_epi64(sum_low, 1);\n        uint64_t high0 = _mm_extract_epi64(sum_high, 0);\n        uint64_t high1 = _mm_extract_epi64(sum_high, 1);\n\n        total += low0 + low1 + high0 + high1;\n    }\n\n    for (; i < len; i++) {\n        char c = ptr[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += c;\n        }\n    }\n\n    return static_cast<int>(total);\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nint digitSum_simd(std::string s) {\n    const char* data = s.data();\n    size_t size = s.size();\n    uint64_t total = 0;\n    size_t i = 0;\n\n    if (size >= 32) {\n        for (; i <= size - 32; i += 32) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i mask_ge_A = _mm256_cmpgt_epi8(v, _mm256_set1_epi8('A' - 1));\n            __m256i mask_le_Z = _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), v);\n            __m256i mask_upper = _mm256_and_si256(mask_ge_A, mask_le_Z);\n            __m256i uppercase_bytes = _mm256_and_si256(v, mask_upper);\n            __m256i sum_vec = _mm256_sad_epu8(uppercase_bytes, _mm256_setzero_si256());\n            __m128i low128 = _mm256_castsi256_si128(sum_vec);\n            __m128i high128 = _mm256_extracti128_si256(sum_vec, 1);\n            __m128i sum128 = _mm_add_epi64(low128, high128);\n            total += _mm_extract_epi64(sum128, 0) + _mm_extract_epi64(sum128, 1);\n        }\n    }\n\n    for (; i < size; ++i) {\n        char c = data[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += static_cast<unsigned char>(c);\n        }\n    }\n\n    return static_cast<int>(total);\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    size_t i = 0;\n    uint64_t total = 0;\n\n    if (len == 0) {\n        return 0;\n    }\n\n    __m256i vA = _mm256_set1_epi8('A' - 1);\n    __m256i vZ = _mm256_set1_epi8('Z');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i cmp1 = _mm256_cmpgt_epi8(v, vA);\n        __m256i cmp2 = _mm256_cmpgt_epi8(vZ, v);\n        __m256i mask = _mm256_and_si256(cmp1, cmp2);\n        __m256i selected = _mm256_and_si256(v, mask);\n\n        __m128i low128 = _mm256_castsi256_si128(selected);\n        __m128i high128 = _mm256_extracti128_si256(selected, 1);\n        __m128i sum_low = _mm_sad_epu8(low128, _mm_setzero_si128());\n        __m128i sum_high = _mm_sad_epu8(high128, _mm_setzero_si128());\n        __m128i sum_total = _mm_add_epi64(sum_low, sum_high);\n\n        alignas(16) uint64_t temp[2];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum_total);\n        total += temp[0] + temp[1];\n    }\n\n    for (; i < len; i++) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += static_cast<unsigned char>(c);\n        }\n    }\n\n    return static_cast<int>(total);\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int total_sum = 0;\n    const char* data = s.data();\n    size_t len = s.length();\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i cmp_ge_A = _mm256_cmpgt_epi8(v, _mm256_set1_epi8('A' - 1));\n        __m256i cmp_le_Z = _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), v);\n        __m256i mask = _mm256_and_si256(cmp_ge_A, cmp_le_Z);\n        __m256i masked_v = _mm256_and_si256(v, mask);\n        __m256i sum_vec = _mm256_sad_epu8(masked_v, _mm256_setzero_si256());\n        \n        total_sum += static_cast<int>(_mm256_extract_epi64(sum_vec, 0));\n        total_sum += static_cast<int>(_mm256_extract_epi64(sum_vec, 1));\n        total_sum += static_cast<int>(_mm256_extract_epi64(sum_vec, 2));\n        total_sum += static_cast<int>(_mm256_extract_epi64(sum_vec, 3));\n    }\n\n    for (; i < len; ++i) {\n        char c = data[i];\n        if (c >= 'A' && c <= 'Z') {\n            total_sum += c;\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    size_t i = 0;\n    const int* data = arr.data();\n\n    for (; i + 7 < arr.size(); i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(data + i));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(v, ones);\n        __m256i even_mask = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        __m256i big = _mm256_set1_epi32(INT_MAX);\n        __m256i candidate_vals = _mm256_blendv_epi8(big, v, even_mask);\n\n        int vals[8];\n        _mm256_storeu_si256((__m256i*)vals, candidate_vals);\n\n        for (int j = 0; j < 8; j++) {\n            if (vals[j] < min_even) {\n                min_even = vals[j];\n                min_index = i + j;\n            }\n        }\n    }\n\n    for (; i < arr.size(); i++) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n    return { min_even, min_index };\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n    int n = arr.size();\n    int global_min = INT_MAX;\n    int global_min_index = -1;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i lsb = _mm256_and_si256(chunk, one);\n        __m256i mask_even = _mm256_cmpeq_epi32(lsb, _mm256_setzero_si256());\n        __m256i intmax_vec = _mm256_set1_epi32(INT_MAX);\n        __m256i chunk_evens = _mm256_blendv_epi8(intmax_vec, chunk, mask_even);\n\n        __m128i low = _mm256_castsi256_si128(chunk_evens);\n        __m128i high = _mm256_extracti128_si256(chunk_evens, 1);\n\n        __m128i low_shuffled = _mm_shuffle_epi32(low, _MM_SHUFFLE(2,3,0,1));\n        __m128i min_low = _mm_min_epi32(low, low_shuffled);\n        low_shuffled = _mm_shuffle_epi32(min_low, _MM_SHUFFLE(1,0,3,2));\n        min_low = _mm_min_epi32(min_low, low_shuffled);\n        int min_low_val = _mm_extract_epi32(min_low, 0);\n\n        __m128i high_shuffled = _mm_shuffle_epi32(high, _MM_SHUFFLE(2,3,0,1));\n        __m128i min_high = _mm_min_epi32(high, high_shuffled);\n        high_shuffled = _mm_shuffle_epi32(min_high, _MM_SHUFFLE(1,0,3,2));\n        min_high = _mm_min_epi32(min_high, high_shuffled);\n        int min_high_val = _mm_extract_epi32(min_high, 0);\n\n        int min_candidate = min_low_val < min_high_val ? min_low_val : min_high_val;\n\n        if (min_candidate < global_min) {\n            global_min = min_candidate;\n            int values[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(values), chunk_evens);\n            for (int j = 0; j < 8; j++) {\n                if (values[j] == min_candidate) {\n                    global_min_index = i + j;\n                    break;\n                }\n            }\n        } else if (min_candidate == INT_MAX && global_min == INT_MAX) {\n            int values[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(values), chunk);\n            int even_flags[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(even_flags), mask_even);\n            for (int j = 0; j < 8; j++) {\n                if (values[j] == INT_MAX && even_flags[j] != 0) {\n                    global_min = INT_MAX;\n                    global_min_index = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (int i = (n / 8) * 8; i < n; i++) {\n        if (arr[i] % 2 == 0) {\n            if (global_min_index == -1 || arr[i] < global_min) {\n                global_min = arr[i];\n                global_min_index = i;\n            }\n        }\n    }\n\n    if (global_min_index == -1) {\n        return {};\n    }\n    return {global_min, global_min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n    \n    int n = arr.size();\n    int min_even = INT_MAX;\n    int min_index = -1;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(arr.data() + i));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_res = _mm256_and_si256(vec, ones);\n        __m256i mask = _mm256_cmpeq_epi32(and_res, _mm256_setzero_si256());\n        \n        int values[8];\n        int mask_arr[8];\n        _mm256_storeu_si256((__m256i*)values, vec);\n        _mm256_storeu_si256((__m256i*)mask_arr, mask);\n        \n        for (int j = 0; j < 8; j++) {\n            if (mask_arr[j] != 0) {\n                if (values[j] < min_even) {\n                    min_even = values[j];\n                    min_index = i + j;\n                }\n            }\n        }\n    }\n    \n    for (; i < n; i++) {\n        if ((arr[i] & 1) == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n    \n    if (min_index == -1) {\n        return {};\n    } else {\n        return { min_even, min_index };\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return {};\n    }\n\n    int current_min = INT_MAX;\n    int current_index = -1;\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(v, ones);\n        __m256i mask = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        __m256i intmax = _mm256_set1_epi32(INT_MAX);\n        __m256i v_even = _mm256_blendv_epi8(intmax, v, mask);\n\n        int chunk_values[8];\n        _mm256_storeu_si256((__m256i*)chunk_values, v_even);\n\n        int min_val = chunk_values[0];\n        for (int j = 1; j < 8; j++) {\n            if (chunk_values[j] < min_val) {\n                min_val = chunk_values[j];\n            }\n        }\n\n        if (min_val == INT_MAX) {\n            continue;\n        }\n\n        if (min_val < current_min) {\n            for (int j = 0; j < 8; j++) {\n                if (chunk_values[j] == min_val) {\n                    current_min = min_val;\n                    current_index = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        int val = arr[i];\n        if (val % 2 == 0) {\n            if (val < current_min) {\n                current_min = val;\n                current_index = i;\n            }\n        }\n    }\n\n    if (current_index == -1) {\n        return {};\n    }\n    return {current_min, current_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <climits>\n\ninline int horizontal_min_epi32(__m256i a) {\n    __m128i vlow = _mm256_extracti128_si256(a, 0);\n    __m128i vhigh = _mm256_extracti128_si256(a, 1);\n    vlow = _mm_min_epi32(vlow, _mm_shuffle_epi32(vlow, _MM_SHUFFLE(2,3,0,1)));\n    vlow = _mm_min_epi32(vlow, _mm_shuffle_epi32(vlow, _MM_SHUFFLE(1,0,3,2)));\n    vhigh = _mm_min_epi32(vhigh, _mm_shuffle_epi32(vhigh, _MM_SHUFFLE(2,3,0,1)));\n    vhigh = _mm_min_epi32(vhigh, _mm_shuffle_epi32(vhigh, _MM_SHUFFLE(1,0,3,2)));\n    __m128i res = _mm_min_epi32(vlow, vhigh);\n    return _mm_extract_epi32(res, 0);\n}\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return {};\n    }\n\n    int global_min_value = INT_MAX;\n    int global_min_index = -1;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(arr.data() + i));\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(v, one);\n        __m256i even_mask = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        __m256i v_candidate = _mm256_blendv_epi8(\n            _mm256_set1_epi32(INT_MAX),\n            v,\n            even_mask\n        );\n        int min_in_chunk = horizontal_min_epi32(v_candidate);\n        if (min_in_chunk == INT_MAX) {\n            continue;\n        }\n        __m256i v_min_val = _mm256_set1_epi32(min_in_chunk);\n        __m256i mask_eq = _mm256_cmpeq_epi32(v_candidate, v_min_val);\n        __m256i v_indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i candidate_indices = _mm256_blendv_epi8(\n            _mm256_set1_epi32(n),\n            v_indices,\n            mask_eq\n        );\n        int min_index_in_chunk = horizontal_min_epi32(candidate_indices);\n        if (min_in_chunk < global_min_value) {\n            global_min_value = min_in_chunk;\n            global_min_index = min_index_in_chunk;\n        } else if (min_in_chunk == global_min_value) {\n            if (min_index_in_chunk < global_min_index) {\n                global_min_index = min_index_in_chunk;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < global_min_value) {\n                global_min_value = arr[i];\n                global_min_index = i;\n            } else if (arr[i] == global_min_value) {\n                if (i < global_min_index) {\n                    global_min_index = i;\n                }\n            }\n        }\n    }\n\n    if (global_min_value == INT_MAX) {\n        return {};\n    }\n    return {global_min_value, global_min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Allocate 8 separate histograms\n    std::vector<std::vector<int>> freqs(8, std::vector<int>(n+1, 0));\n\n    int i = 0;\n    // Process in chunks of 8 using AVX2\n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        // Extract each element from the vector\n        int val0 = _mm256_extract_epi32(data, 0);\n        int val1 = _mm256_extract_epi32(data, 1);\n        int val2 = _mm256_extract_epi32(data, 2);\n        int val3 = _mm256_extract_epi32(data, 3);\n        int val4 = _mm256_extract_epi32(data, 4);\n        int val5 = _mm256_extract_epi32(data, 5);\n        int val6 = _mm256_extract_epi32(data, 6);\n        int val7 = _mm256_extract_epi32(data, 7);\n\n        // Update respective histograms conditionally\n        if (val0 >= 1 && val0 <= n) freqs[0][val0]++;\n        if (val1 >= 1 && val1 <= n) freqs[1][val1]++;\n        if (val2 >= 1 && val2 <= n) freqs[2][val2]++;\n        if (val3 >= 1 && val3 <= n) freqs[3][val3]++;\n        if (val4 >= 1 && val4 <= n) freqs[4][val4]++;\n        if (val5 >= 1 && val5 <= n) freqs[5][val5]++;\n        if (val6 >= 1 && val6 <= n) freqs[6][val6]++;\n        if (val7 >= 1 && val7 <= n) freqs[7][val7]++;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val >= 1 && val <= n) {\n            freqs[0][val]++; // Use first histogram for tail\n        }\n    }\n\n    // Combine histograms\n    std::vector<int> freq_total(n+1, 0);\n    for (int j = 1; j <= n; ++j) {\n        for (int k = 0; k < 8; ++k) {\n            freq_total[j] += freqs[k][j];\n        }\n    }\n\n    // Find the greatest integer satisfying freq[x] >= x\n    for (int x = n; x >= 1; --x) {\n        if (freq_total[x] >= x) {\n            return x;\n        }\n    }\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <cstring>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return -1;\n    }\n\n    int max_val = 0;\n    int i = 0;\n    __m256i max_vec = _mm256_setzero_si256();\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        max_vec = _mm256_max_epi32(max_vec, data);\n    }\n    alignas(32) int max_arr[8];\n    _mm256_store_si256((__m256i*)max_arr, max_vec);\n    for (int j = 0; j < 8; j++) {\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n    for (; i < n; i++) {\n        if (lst[i] > max_val) max_val = lst[i];\n    }\n\n    int* freq_arr = static_cast<int*>(_mm_malloc((max_val + 8) * sizeof(int), 32));\n    if (freq_arr == nullptr) {\n        return -1;\n    }\n    memset(freq_arr, 0, (max_val + 8) * sizeof(int));\n\n    for (int x : lst) {\n        if (x <= max_val) {\n            freq_arr[x]++;\n        }\n    }\n\n    int result = -1;\n    for (int base = max_val; base >= 1; base -= 8) {\n        int start = base - 7;\n        if (start < 1) start = 1;\n\n        __m256i f = _mm256_loadu_si256((__m256i const*)(freq_arr + start));\n        __m256i idx = _mm256_set_epi32(start + 7, start + 6, start + 5, start + 4, \n                                      start + 3, start + 2, start + 1, start);\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i idx_minus_one = _mm256_sub_epi32(idx, one);\n        __m256i cmp_cond = _mm256_cmpgt_epi32(f, idx_minus_one);\n\n        __m256i base_vec = _mm256_set1_epi32(base);\n        __m256i mask = _mm256_cmpgt_epi32(idx, base_vec);\n        cmp_cond = _mm256_andnot_si256(mask, cmp_cond);\n\n        alignas(32) int cmp_arr[8];\n        _mm256_store_si256((__m256i*)cmp_arr, cmp_cond);\n\n        for (int j = 7; j >= 0; j--) {\n            if (cmp_arr[j] != 0) {\n                result = start + j;\n                _mm_free(freq_arr);\n                return result;\n            }\n        }\n    }\n\n    _mm_free(freq_arr);\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) \n        return -1;\n\n    int max_val = 0;\n    size_t n = lst.size();\n    size_t i = 0;\n    __m256i max_vec = _mm256_set1_epi32(0);\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        max_vec = _mm256_max_epi32(max_vec, vec);\n    }\n    \n    int max_arr[8];\n    _mm256_storeu_si256((__m256i*)max_arr, max_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (max_arr[j] > max_val) \n            max_val = max_arr[j];\n    }\n    \n    for (; i < n; ++i) {\n        if (lst[i] > max_val) \n            max_val = lst[i];\n    }\n    \n    if (max_val <= 0) \n        return -1;\n        \n    int* freq = new int[max_val + 1]();\n    \n    for (i = 0; i < n; ++i) {\n        if (lst[i] <= max_val) \n            freq[lst[i]]++;\n    }\n    \n    int result = -1;\n    \n    for (i = max_val; i >= 8; i -= 8) {\n        __m256i indices = _mm256_set_epi32(i, i-1, i-2, i-3, i-4, i-5, i-6, i-7);\n        __m256i freqs_vec = _mm256_loadu_si256((__m256i const*)(freq + i - 7));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i indices_minus_one = _mm256_sub_epi32(indices, ones);\n        __m256i cmp = _mm256_cmpgt_epi32(freqs_vec, indices_minus_one);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        \n        if (mask) {\n            for (int j = 7; j >= 0; --j) {\n                if (mask & (1 << j)) {\n                    result = i - (7 - j);\n                    delete[] freq;\n                    return result;\n                }\n            }\n        }\n    }\n    \n    int start = (max_val >= 8) ? (max_val % 8) : max_val;\n    for (int i = start; i >= 1; --i) {\n        if (freq[i] >= i) {\n            result = i;\n            break;\n        }\n    }\n    \n    delete[] freq;\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) \n        return -1;\n\n    int* freq = (int*)_mm_malloc((n + 1) * sizeof(int), 32);\n    __m256i zero = _mm256_setzero_si256();\n    for (int i = 0; i <= n; i += 8) {\n        _mm256_store_si256((__m256i*)(freq + i), zero);\n    }\n\n    for (int x : lst) {\n        if (x >= 1 && x <= n) {\n            freq[x]++;\n        }\n    }\n\n    int candidate = -1;\n    int i = n;\n    for (; i >= 8; i -= 8) {\n        __m256i indices = _mm256_setr_epi32(i, i - 1, i - 2, i - 3, i - 4, i - 5, i - 6, i - 7);\n        __m256i freqs = _mm256_i32gather_epi32(freq, indices, 4);\n        __m256i thresholds = _mm256_sub_epi32(indices, _mm256_set1_epi32(1));\n        __m256i mask = _mm256_cmpgt_epi32(freqs, thresholds);\n        int bitmask = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        if (bitmask) {\n            for (int j = 0; j < 8; j++) {\n                if (bitmask & (1 << j)) {\n                    candidate = i - j;\n                    break;\n                }\n            }\n            if (candidate != -1) \n                break;\n        }\n    }\n\n    if (candidate == -1) {\n        for (int j = i; j >= 1; j--) {\n            if (freq[j] >= j) {\n                candidate = j;\n                break;\n            }\n        }\n    }\n\n    _mm_free(freq);\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return -1;\n    }\n\n    const int* data = lst.data();\n    __m256i v_max = _mm256_set1_epi32(0);\n    int max_val = 0;\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(data + i));\n        v_max = _mm256_max_epi32(v_max, v);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, v_max);\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if (data[i] > max_val) {\n            max_val = data[i];\n        }\n    }\n\n    if (max_val < 1) {\n        return -1;\n    }\n\n    std::vector<int> freq_vec(max_val + 1, 0);\n    int* freq = freq_vec.data();\n    for (i = 0; i < n; i++) {\n        if (lst[i] <= max_val) {\n            freq[lst[i]]++;\n        }\n    }\n\n    int candidate = -1;\n    i = max_val;\n\n    for (; i >= 8; i -= 8) {\n        __m256i v_index = _mm256_set_epi32(i, i - 1, i - 2, i - 3, i - 4, i - 5, i - 6, i - 7);\n        __m256i v_freq = _mm256_loadu_si256((__m256i const*)(freq + i - 7));\n        __m256i v_diff = _mm256_sub_epi32(v_freq, v_index);\n        __m256i v_sign = _mm256_srai_epi32(v_diff, 31);\n        __m256i v_non_negative = _mm256_cmpeq_epi32(v_sign, _mm256_setzero_si256());\n        int mask8 = _mm256_movemask_ps(_mm256_castsi256_ps(v_non_negative));\n\n        if (mask8) {\n            for (int j = 7; j >= 0; j--) {\n                if (mask8 & (1 << j)) {\n                    candidate = i - 7 + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    if (candidate != -1) {\n        return candidate;\n    }\n\n    for (int j = i; j >= 1; j--) {\n        if (freq[j] >= j) {\n            candidate = j;\n            break;\n        }\n    }\n\n    if (candidate != -1) {\n        return candidate;\n    } else {\n        return -1;\n    }\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int i = 0;\n    int j = n - 1;\n    __m256i rev_idx = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    while (j - i >= 14) {\n        __m256i front = _mm256_loadu_si256((__m256i*)(q.data() + i));\n        __m256i back = _mm256_loadu_si256((__m256i*)(q.data() + j - 7));\n        __m256i rev_back = _mm256_permutevar8x32_epi32(back, rev_idx);\n        __m256i cmp = _mm256_cmpeq_epi32(front, rev_back);\n        unsigned mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n        i += 8;\n        j -= 8;\n    }\n\n    while (i < j) {\n        if (q[i] != q[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    int k = 0;\n    int n8 = n & ~7;\n    for (; k < n8; k += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(q.data() + k));\n        sum_vec = _mm256_add_epi32(sum_vec, v);\n    }\n\n    __m128i low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n    low = _mm_add_epi32(low, high);\n    low = _mm_hadd_epi32(low, low);\n    low = _mm_hadd_epi32(low, low);\n    int total = _mm_extract_epi32(low, 0);\n\n    for (; k < n; k++) {\n        total += q[k];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    __m256i sum_total = _mm256_setzero_si256();\n    int left = 0;\n    int right = n - 1;\n    \n    const __m256i rev_mask = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    \n    while (right - left >= 15) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i const*)(q.data() + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i const*)(q.data() + right - 7));\n        \n        sum_total = _mm256_add_epi32(sum_total, left_vec);\n        sum_total = _mm256_add_epi32(sum_total, right_vec);\n        \n        __m256i right_reversed = _mm256_permutevar8x32_epi32(right_vec, rev_mask);\n        __m256i cmp = _mm256_cmpeq_epi32(left_vec, right_reversed);\n        unsigned mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n        \n        left += 8;\n        right -= 8;\n    }\n    \n    int center_sum = 0;\n    while (left < right) {\n        if (q[left] != q[right]) {\n            return false;\n        }\n        center_sum += q[left] + q[right];\n        left++;\n        right--;\n    }\n    if (left == right) {\n        center_sum += q[left];\n    }\n    \n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_total);\n    int total = center_sum;\n    for (int i = 0; i < 8; i++) {\n        total += sum_arr[i];\n    }\n    \n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    int i = 0;\n    int j = n - 1;\n    const __m256i rev_index = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    while (i < j) {\n        if (j - i + 1 >= 8) {\n            __m256i front = _mm256_loadu_si256((__m256i*)(q.data() + i));\n            __m256i back = _mm256_loadu_si256((__m256i*)(q.data() + j - 7));\n            __m256i rev_back = _mm256_permutevar8x32_epi32(back, rev_index);\n            __m256i cmp = _mm256_cmpeq_epi32(front, rev_back);\n            unsigned mask = (unsigned)_mm256_movemask_epi8(cmp);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n            i += 8;\n            j -= 8;\n        } else {\n            while (i < j) {\n                if (q[i] != q[j]) {\n                    return false;\n                }\n                i++;\n                j--;\n            }\n            break;\n        }\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t k = 0;\n    for (; k + 8 <= n; k += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(q.data() + k));\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(sum_vec);\n    __m128i high128 = _mm256_extractf128_si256(sum_vec, 1);\n    low128 = _mm_add_epi32(low128, high128);\n\n    __m128i shifted = _mm_srli_si128(low128, 8);\n    __m128i sum64 = _mm_add_epi32(low128, shifted);\n    __m128i shifted2 = _mm_srli_si128(sum64, 4);\n    __m128i sum32 = _mm_add_epi32(sum64, shifted2);\n    int total_sum = _mm_extract_epi32(sum32, 0);\n\n    for (; k < n; k++) {\n        total_sum += q[k];\n    }\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int i = 0;\n    int j = n - 1;\n    \n    __m256i rev_index = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    \n    while (j - i + 1 >= 16) {\n        __m256i front = _mm256_loadu_si256((__m256i const*)(q.data() + i));\n        __m256i back = _mm256_loadu_si256((__m256i const*)(q.data() + j - 7));\n        __m256i reversed_back = _mm256_permutevar8x32_epi32(back, rev_index);\n        \n        __m256i cmp = _mm256_cmpeq_epi32(front, reversed_back);\n        unsigned mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            return false;\n        }\n        i += 8;\n        j -= 8;\n    }\n    \n    while (i < j) {\n        if (q[i] != q[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    int k = 0;\n    for (; k + 8 <= n; k += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i const*)(q.data() + k));\n        sum_vec = _mm256_add_epi32(sum_vec, chunk);\n    }\n    \n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int total = 0;\n    for (int t = 0; t < 8; t++) {\n        total += sum_arr[t];\n    }\n    \n    for (; k < n; k++) {\n        total += q[k];\n    }\n    \n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    size_t half = n / 2;\n    __m256i rev_mask = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    size_t i = 0;\n    for (; i + 8 <= half; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(q.data() + i));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(q.data() + n - i - 8));\n        __m256i right_rev = _mm256_permutevar8x32_epi32(right, rev_mask);\n        __m256i cmp = _mm256_cmpeq_epi32(left, right_rev);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < half; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            return false;\n        }\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i_sum = 0;\n    for (; i_sum + 8 <= n; i_sum += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(q.data() + i_sum));\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int total = 0;\n    for (int j = 0; j < 8; ++j) {\n        total += sum_arr[j];\n    }\n    for (size_t j = i_sum; j < n; ++j) {\n        total += q[j];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int half = n / 2;\n    int total_mismatches = 0;\n    int i = 0;\n\n    if (half >= 8) {\n        __m256i rev_index = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        \n        for (; i + 8 <= half; i += 8) {\n            __m256i left = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n            __m256i right = _mm256_loadu_si256((__m256i*)(arr.data() + n - 8 - i));\n            __m256i right_rev = _mm256_permutevar8x32_epi32(right, rev_index);\n            __m256i cmp = _mm256_cmpeq_epi32(left, right_rev);\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i mismatch_flags = _mm256_andnot_si256(cmp, ones);\n\n            __m128i lo = _mm256_castsi256_si128(mismatch_flags);\n            __m128i hi = _mm256_extracti128_si256(mismatch_flags, 1);\n            __m128i sum128 = _mm_add_epi32(lo, hi);\n            sum128 = _mm_hadd_epi32(sum128, sum128);\n            sum128 = _mm_hadd_epi32(sum128, sum128);\n            int block_mismatches = _mm_extract_epi32(sum128, 0);\n            total_mismatches += block_mismatches;\n        }\n    }\n\n    for (; i < half; i++) {\n        if (arr[i] != arr[n - 1 - i]) {\n            total_mismatches++;\n        }\n    }\n\n    return total_mismatches;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <nmmintrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    size_t n = arr.size();\n    if (n == 0) return 0;\n    size_t i = 0;\n    size_t j = n - 1;\n    int total_count = 0;\n    const __m256i rev_index = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    while (j >= i + 15) {\n        __m256i front = _mm256_loadu_si256((const __m256i*)(arr.data() + i));\n        __m256i back_load = _mm256_loadu_si256((const __m256i*)(arr.data() + j - 7));\n        __m256i back_reversed = _mm256_permutevar8x32_epi32(back_load, rev_index);\n        __m256i cmp = _mm256_cmpeq_epi32(front, back_reversed);\n        unsigned int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        total_count += 8 - _mm_popcnt_u32(mask & 0xFF);\n        i += 8;\n        j -= 8;\n    }\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            total_count++;\n        }\n        i++;\n        j--;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <nmmintrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int half = n / 2;\n    int count = 0;\n    size_t i = 0;\n\n    if (half >= 8) {\n        __m256i rev_index = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        for (; i + 8 <= half; i += 8) {\n            __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n            __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + n - 1 - i - 7));\n            __m256i rev_right = _mm256_permutevar8x32_epi32(right, rev_index);\n            __m256i cmp = _mm256_cmpeq_epi32(left, rev_right);\n            unsigned int mask = static_cast<unsigned int>(_mm256_movemask_epi8(cmp));\n            int equal_count = _mm_popcnt_u32(mask) / 4;\n            count += (8 - equal_count);\n        }\n    }\n\n    for (; i < half; i++) {\n        if (arr[i] != arr[n - 1 - i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int n_pairs = n / 2;\n    int count = 0;\n    if (n_pairs == 0) {\n        return 0;\n    }\n    const int* left_ptr = arr.data();\n    const int* right_ptr = arr.data() + n - 1;\n    const __m256i rev_index = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    int i = 0;\n    for (; i + 8 <= n_pairs; i += 8, left_ptr += 8, right_ptr -= 8) {\n        __m256i left_vec = _mm256_loadu_si256((const __m256i*)left_ptr);\n        __m256i right_vec = _mm256_loadu_si256((const __m256i*)(right_ptr - 7));\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right_vec, rev_index);\n        __m256i cmp = _mm256_cmpeq_epi32(left_vec, reversed_right);\n        __m256i match_flags = _mm256_and_si256(cmp, _mm256_set1_epi32(1));\n        __m128i low_flags = _mm256_castsi256_si128(match_flags);\n        __m128i high_flags = _mm256_extracti128_si256(match_flags, 1);\n        __m128i sum128 = _mm_add_epi32(low_flags, high_flags);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        int s = _mm_extract_epi32(sum128, 0);\n        count += (8 - s);\n    }\n    for (; i < n_pairs; ++i, ++left_ptr, --right_ptr) {\n        if (*left_ptr != *right_ptr) {\n            ++count;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int i = 0;\n    int j = n - 1;\n    int total_mismatches = 0;\n\n    const __m256i permute_mask = _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    const __m256i ones = _mm256_set1_epi32(1);\n\n    while (j - i + 1 >= 16) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + j - 7));\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, permute_mask);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n        __m256i match_vec = _mm256_and_si256(cmp, ones);\n\n        __m128i low128 = _mm256_castsi256_si128(match_vec);\n        __m128i high128 = _mm256_extracti128_si256(match_vec, 1);\n        __m128i sum128 = _mm_add_epi32(low128, high128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        int matches = _mm_extract_epi32(sum128, 0);\n        total_mismatches += (8 - matches);\n\n        i += 8;\n        j -= 8;\n    }\n\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            total_mismatches++;\n        }\n        i++;\n        j--;\n    }\n\n    return total_mismatches;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto compute_total = [](const std::vector<std::string>& vec) -> uint64_t {\n        if (vec.empty()) {\n            return 0;\n        }\n        __m256i sum_vec = _mm256_setzero_si256();\n        size_t i = 0;\n        size_t n = vec.size();\n        for (; i + 3 < n; i += 4) {\n            uint64_t s0 = vec[i].size();\n            uint64_t s1 = vec[i+1].size();\n            uint64_t s2 = vec[i+2].size();\n            uint64_t s3 = vec[i+3].size();\n            __m256i data = _mm256_set_epi64x(s3, s2, s1, s0);\n            sum_vec = _mm256_add_epi64(sum_vec, data);\n        }\n        uint64_t temp[4] = {0};\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        uint64_t total = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; i < n; ++i) {\n            total += vec[i].size();\n        }\n        return total;\n    };\n\n    uint64_t total1 = compute_total(lst1);\n    uint64_t total2 = compute_total(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto total_length = [](const std::vector<std::string>& vec) -> unsigned long long {\n        unsigned long long total = 0;\n        size_t n = vec.size();\n        size_t i = 0;\n        if (n >= 4) {\n            __m256i vsum = _mm256_setzero_si256();\n            for (; i + 4 <= n; i += 4) {\n                unsigned long long len0 = vec[i].length();\n                unsigned long long len1 = vec[i+1].length();\n                unsigned long long len2 = vec[i+2].length();\n                unsigned long long len3 = vec[i+3].length();\n                __m256i v = _mm256_setr_epi64x(len0, len1, len2, len3);\n                vsum = _mm256_add_epi64(vsum, v);\n            }\n            __m128i low = _mm256_castsi256_si128(vsum);\n            __m128i high = _mm256_extracti128_si256(vsum, 1);\n            low = _mm_add_epi64(low, high);\n            unsigned long long temp[2];\n            _mm_storeu_si128((__m128i*)temp, low);\n            total = temp[0] + temp[1];\n        }\n        for (; i < n; i++) {\n            total += vec[i].length();\n        }\n        return total;\n    };\n\n    unsigned long long total1 = total_length(lst1);\n    unsigned long long total2 = total_length(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto total_chars = [](const std::vector<std::string>& vec) -> uint64_t {\n        if (vec.empty()) {\n            return 0;\n        }\n        std::vector<uint64_t> lengths;\n        lengths.reserve(vec.size());\n        for (const auto& s : vec) {\n            lengths.push_back(static_cast<uint64_t>(s.size()));\n        }\n        size_t n = lengths.size();\n        uint64_t total = 0;\n        size_t i = 0;\n        if (n >= 4) {\n            __m256i sum_vec = _mm256_setzero_si256();\n            for (; i <= n - 4; i += 4) {\n                __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lengths[i]));\n                sum_vec = _mm256_add_epi64(sum_vec, data);\n            }\n            uint64_t temp[4];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n            total = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n        for (; i < n; ++i) {\n            total += lengths[i];\n        }\n        return total;\n    };\n    uint64_t total1 = total_chars(lst1);\n    uint64_t total2 = total_chars(lst2);\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto total_chars_simd = [](const std::vector<std::string>& vec) -> uint64_t {\n        const size_t n = vec.size();\n        if (n == 0) \n            return 0;\n        \n        const size_t aligned_n = n - (n % 4);\n        __m256i sum_vec = _mm256_setzero_si256();\n        \n        for (size_t i = 0; i < aligned_n; i += 4) {\n            __m256i v = _mm256_set_epi64x(\n                static_cast<int64_t>(vec[i+3].size()),\n                static_cast<int64_t>(vec[i+2].size()),\n                static_cast<int64_t>(vec[i+1].size()),\n                static_cast<int64_t>(vec[i].size())\n            );\n            sum_vec = _mm256_add_epi64(sum_vec, v);\n        }\n        \n        uint64_t temp[4];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n        uint64_t total = temp[0] + temp[1] + temp[2] + temp[3];\n        \n        for (size_t i = aligned_n; i < n; ++i) {\n            total += vec[i].size();\n        }\n        return total;\n    };\n    \n    uint64_t total1 = total_chars_simd(lst1);\n    uint64_t total2 = total_chars_simd(lst2);\n    \n    if (total1 < total2) {\n        return lst1;\n    } else if (total1 > total2) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto total_length = [](const std::vector<std::string>& v) -> size_t {\n        size_t n = v.size();\n        if (n == 0) \n            return 0;\n\n        size_t total = 0;\n        size_t i = 0;\n\n        if (n >= 4) {\n            __m256i sum_vec = _mm256_setzero_si256();\n            for (; i <= n-4; i += 4) {\n                uint64_t len0 = v[i].size();\n                uint64_t len1 = v[i+1].size();\n                uint64_t len2 = v[i+2].size();\n                uint64_t len3 = v[i+3].size();\n\n                __m256i vec = _mm256_set_epi64x(len3, len2, len1, len0);\n                sum_vec = _mm256_add_epi64(sum_vec, vec);\n            }\n\n            __m128i low128 = _mm256_castsi256_si128(sum_vec);\n            __m128i high128 = _mm256_extracti128_si256(sum_vec, 1);\n            __m128i sum128 = _mm_add_epi64(low128, high128);\n            __m128i shifted = _mm_srli_si128(sum128, 8);\n            __m128i result128 = _mm_add_epi64(sum128, shifted);\n            total = _mm_cvtsi128_si64(result128);\n        }\n\n        for (; i < n; i++) {\n            total += v[i].size();\n        }\n\n        return total;\n    };\n\n    size_t total1 = total_length(lst1);\n    size_t total2 = total_length(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return (x == 0);\n    }\n    if (n == 1) {\n        return (x == 1);\n    }\n    if (x == 0) {\n        return false;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t current_power = n;\n    for (uint64_t exp = 1; exp <= 64; exp += 4) {\n        if (current_power == 0 || current_power > x) {\n            break;\n        }\n\n        uint64_t powers[4];\n        powers[0] = current_power;\n\n        if (current_power > (~(uint64_t)0) / n) {\n            powers[1] = 0;\n            powers[2] = 0;\n            powers[3] = 0;\n        } else {\n            powers[1] = current_power * n;\n            if (powers[1] > (~(uint64_t)0) / n) {\n                powers[2] = 0;\n                powers[3] = 0;\n            } else {\n                powers[2] = powers[1] * n;\n                if (powers[2] > (~(uint64_t)0) / n) {\n                    powers[3] = 0;\n                } else {\n                    powers[3] = powers[2] * n;\n                }\n            }\n        }\n\n        __m256i vec_power = _mm256_set_epi64x(powers[3], powers[2], powers[1], powers[0]);\n        __m256i vec_x = _mm256_set1_epi64x(x);\n        __m256i cmp = _mm256_cmpeq_epi64(vec_power, vec_x);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return true;\n        }\n\n        if (powers[3] == 0) {\n            break;\n        }\n        if (powers[3] > (~(uint64_t)0) / n) {\n            break;\n        }\n        current_power = powers[3] * n;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return (x == 0);\n    }\n    if (n == 1) {\n        return (x == 1);\n    }\n    if (x == 0) {\n        return false;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t powers[4];\n    powers[0] = n;\n    powers[1] = powers[0] * n;\n    powers[2] = powers[1] * n;\n    powers[3] = powers[2] * n;\n\n    int exp_start = 1;\n    while (exp_start <= 64) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i p_vec = _mm256_loadu_si256((__m256i const*)powers);\n        __m256i cmp = _mm256_cmpeq_epi64(p_vec, x_vec);\n        uint32_t mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n\n        if (powers[3] > x || powers[3] > (uint64_t)-1 / n) {\n            break;\n        }\n\n        uint64_t next_base = powers[3] * n;\n        powers[0] = next_base;\n        powers[1] = powers[0] * n;\n        powers[2] = powers[1] * n;\n        powers[3] = powers[2] * n;\n\n        exp_start += 4;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <limits.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 0) {\n        return (n == 0);\n    }\n    if (n == 0) {\n        return false;\n    }\n    if (x == 1) {\n        return true;\n    }\n    if (n == 1) {\n        return false;\n    }\n\n    uint64_t last_power = 1;\n    for (int start = 1; start <= 64; start += 4) {\n        uint64_t powers[4] = {0, 0, 0, 0};\n\n        if (last_power > UINT64_MAX / n) {\n            break;\n        }\n        powers[0] = last_power * n;\n\n        if (powers[0] > UINT64_MAX / n) {\n            powers[1] = powers[2] = powers[3] = 0;\n        } else {\n            powers[1] = powers[0] * n;\n            if (powers[1] > UINT64_MAX / n) {\n                powers[2] = powers[3] = 0;\n            } else {\n                powers[2] = powers[1] * n;\n                if (powers[2] > UINT64_MAX / n) {\n                    powers[3] = 0;\n                } else {\n                    powers[3] = powers[2] * n;\n                }\n            }\n        }\n\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i p_vec = _mm256_setr_epi64x(powers[0], powers[1], powers[2], powers[3]);\n        __m256i cmp = _mm256_cmpeq_epi64(p_vec, x_vec);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return true;\n        }\n\n        if (powers[0] > x) {\n            break;\n        }\n\n        if (powers[3] == 0) {\n            break;\n        }\n\n        last_power = powers[3];\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return (x == 0);\n    }\n    if (n == 1) {\n        return (x == 1);\n    }\n    if (x == 0) {\n        return false;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t current = n;\n    for (uint64_t base_exp = 1; base_exp <= 64; base_exp += 4) {\n        uint64_t p0 = current;\n        uint64_t p1 = p0 * n;\n        uint64_t p2 = p1 * n;\n        uint64_t p3 = p2 * n;\n\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i p_vec = _mm256_set_epi64x(p3, p2, p1, p0);\n        __m256i cmp = _mm256_cmpeq_epi64(p_vec, x_vec);\n\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return true;\n        }\n\n        if (p0 > x) {\n            break;\n        }\n\n        current = p3 * n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) \n        return true;\n    if (n == 1) \n        return false;\n\n    uint64_t current = 1;\n    for (uint64_t exp = 0; exp <= 64; exp += 4) {\n        if (current > x) \n            break;\n\n        uint64_t powers[4] = {0};\n        powers[0] = current;\n        bool overflow = false;\n\n        if (powers[0] > (~(uint64_t)0) / n) {\n            overflow = true;\n        } else {\n            powers[1] = powers[0] * n;\n        }\n\n        if (!overflow) {\n            if (powers[1] > (~(uint64_t)0) / n) {\n                overflow = true;\n            } else {\n                powers[2] = powers[1] * n;\n            }\n        }\n\n        if (!overflow) {\n            if (powers[2] > (~(uint64_t)0) / n) {\n                overflow = true;\n            } else {\n                powers[3] = powers[2] * n;\n            }\n        }\n\n        __m256i vp = _mm256_loadu_si256((__m256i*)powers);\n        __m256i vx = _mm256_set1_epi64x(x);\n        __m256i neq = _mm256_xor_si256(vp, vx);\n\n        __m128i neq_low = _mm256_castsi256_si128(neq);\n        __m128i neq_high = _mm256_extractf128_si256(neq, 1);\n        __m128i zero128 = _mm_setzero_si128();\n        __m128i cmp_low = _mm_cmpeq_epi64(neq_low, zero128);\n        __m128i cmp_high = _mm_cmpeq_epi64(neq_high, zero128);\n        __m128i cmp = _mm_or_si128(cmp_low, cmp_high);\n\n        if (!_mm_testz_si128(cmp, cmp)) \n            return true;\n\n        if (overflow) \n            break;\n\n        if (powers[3] > (~(uint64_t)0) / n) \n            break;\n\n        current = powers[3] * n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    int n = num.size();\n    if (n == 0) \n        return 0;\n\n    const char* str = num.c_str();\n    int count = 0;\n    int i = 0;\n\n    const __m256i prime2 = _mm256_set1_epi8('2');\n    const __m256i prime3 = _mm256_set1_epi8('3');\n    const __m256i prime5 = _mm256_set1_epi8('5');\n    const __m256i prime7 = _mm256_set1_epi8('7');\n    const __m256i primeB = _mm256_set1_epi8('B');\n    const __m256i primeD = _mm256_set1_epi8('D');\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, prime2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk, prime3);\n        __m256i cmp5 = _mm256_cmpeq_epi8(chunk, prime5);\n        __m256i cmp7 = _mm256_cmpeq_epi8(chunk, prime7);\n        __m256i cmpB = _mm256_cmpeq_epi8(chunk, primeB);\n        __m256i cmpD = _mm256_cmpeq_epi8(chunk, primeD);\n\n        __m256i any = _mm256_or_si256(cmp2, cmp3);\n        any = _mm256_or_si256(any, cmp5);\n        any = _mm256_or_si256(any, cmp7);\n        any = _mm256_or_si256(any, cmpB);\n        any = _mm256_or_si256(any, cmpD);\n\n        unsigned int mask = static_cast<unsigned int>(_mm256_movemask_epi8(any));\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < n; ++i) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    const char* s = num.c_str();\n    size_t len = num.length();\n    int count = 0;\n\n    if (len == 0) \n        return 0;\n\n    __m256i prime2 = _mm256_set1_epi8('2');\n    __m256i prime3 = _mm256_set1_epi8('3');\n    __m256i prime5 = _mm256_set1_epi8('5');\n    __m256i prime7 = _mm256_set1_epi8('7');\n    __m256i primeB = _mm256_set1_epi8('B');\n    __m256i primeD = _mm256_set1_epi8('D');\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s + i));\n        \n        __m256i any = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_or_si256(\n                    _mm256_cmpeq_epi8(chunk, prime2),\n                    _mm256_cmpeq_epi8(chunk, prime3)\n                ),\n                _mm256_or_si256(\n                    _mm256_cmpeq_epi8(chunk, prime5),\n                    _mm256_cmpeq_epi8(chunk, prime7)\n                )\n            ),\n            _mm256_or_si256(\n                _mm256_cmpeq_epi8(chunk, primeB),\n                _mm256_cmpeq_epi8(chunk, primeD)\n            )\n        );\n        \n        int mask = _mm256_movemask_epi8(any);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < len; i++) {\n        char c = s[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* str = num.data();\n    size_t len = num.size();\n    int total_count = 0;\n    size_t i = 0;\n\n    const __m256i v2 = _mm256_set1_epi8('2');\n    const __m256i v3 = _mm256_set1_epi8('3');\n    const __m256i v5 = _mm256_set1_epi8('5');\n    const __m256i v7 = _mm256_set1_epi8('7');\n    const __m256i vB = _mm256_set1_epi8('B');\n    const __m256i vD = _mm256_set1_epi8('D');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        \n        __m256i cmp2 = _mm256_cmpeq_epi8(data, v2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(data, v3);\n        __m256i cmp5 = _mm256_cmpeq_epi8(data, v5);\n        __m256i cmp7 = _mm256_cmpeq_epi8(data, v7);\n        __m256i cmpB = _mm256_cmpeq_epi8(data, vB);\n        __m256i cmpD = _mm256_cmpeq_epi8(data, vD);\n        \n        __m256i any = _mm256_or_si256(cmp2, cmp3);\n        any = _mm256_or_si256(any, cmp5);\n        any = _mm256_or_si256(any, cmp7);\n        any = _mm256_or_si256(any, cmpB);\n        any = _mm256_or_si256(any, cmpD);\n        \n        unsigned mask = _mm256_movemask_epi8(any);\n        total_count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <nmmintrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int total = num.size();\n    if (total == 0) return 0;\n    \n    int count = 0;\n    int i = 0;\n\n    __m256i zero_char = _mm256_set1_epi8('0');\n    __m256i nine_threshold = _mm256_set1_epi8(9);\n    __m256i seven = _mm256_set1_epi8(7);\n    \n    __m256i prime2 = _mm256_set1_epi8(2);\n    __m256i prime3 = _mm256_set1_epi8(3);\n    __m256i prime5 = _mm256_set1_epi8(5);\n    __m256i prime7 = _mm256_set1_epi8(7);\n    __m256i prime11 = _mm256_set1_epi8(11);\n    __m256i prime13 = _mm256_set1_epi8(13);\n\n    for (; i + 32 <= total; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n        __m256i converted = _mm256_sub_epi8(chunk, zero_char);\n        __m256i mask_letter = _mm256_cmpgt_epi8(converted, nine_threshold);\n        converted = _mm256_sub_epi8(converted, _mm256_and_si256(mask_letter, seven));\n        \n        __m256i cmp2 = _mm256_cmpeq_epi8(converted, prime2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(converted, prime3);\n        __m256i cmp5 = _mm256_cmpeq_epi8(converted, prime5);\n        __m256i cmp7 = _mm256_cmpeq_epi8(converted, prime7);\n        __m256i cmp11 = _mm256_cmpeq_epi8(converted, prime11);\n        __m256i cmp13 = _mm256_cmpeq_epi8(converted, prime13);\n        \n        __m256i any = _mm256_or_si256(cmp2, cmp3);\n        any = _mm256_or_si256(any, cmp5);\n        any = _mm256_or_si256(any, cmp7);\n        any = _mm256_or_si256(any, cmp11);\n        any = _mm256_or_si256(any, cmp13);\n        \n        unsigned int mask = _mm256_movemask_epi8(any);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total; i++) {\n        char c = num[i];\n        int val;\n        if (c > '9') \n            val = c - 'A' + 10;\n        else \n            val = c - '0';\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) \n            count++;\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    size_t len = num.length();\n    size_t i = 0;\n    int count = 0;\n\n    if (len == 0) \n        return 0;\n\n    __m256i prime2 = _mm256_set1_epi8('2');\n    __m256i prime3 = _mm256_set1_epi8('3');\n    __m256i prime5 = _mm256_set1_epi8('5');\n    __m256i prime7 = _mm256_set1_epi8('7');\n    __m256i primeB = _mm256_set1_epi8('B');\n    __m256i primeD = _mm256_set1_epi8('D');\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, prime2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk, prime3);\n        __m256i cmp5 = _mm256_cmpeq_epi8(chunk, prime5);\n        __m256i cmp7 = _mm256_cmpeq_epi8(chunk, prime7);\n        __m256i cmpB = _mm256_cmpeq_epi8(chunk, primeB);\n        __m256i cmpD = _mm256_cmpeq_epi8(chunk, primeD);\n        __m256i any = _mm256_or_si256(cmp2, cmp3);\n        any = _mm256_or_si256(any, cmp5);\n        any = _mm256_or_si256(any, cmp7);\n        any = _mm256_or_si256(any, cmpB);\n        any = _mm256_or_si256(any, cmpD);\n        unsigned mask = _mm256_movemask_epi8(any);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) \n        return false;\n\n    size_t i = 0;\n    while (i + 33 < n) {\n        __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i + 1));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i + 2));\n\n        __m256i eq01 = _mm256_cmpeq_epi8(v0, v1);\n        __m256i eq02 = _mm256_cmpeq_epi8(v0, v2);\n        __m256i eq12 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i any = _mm256_or_si256(eq01, _mm256_or_si256(eq02, eq12));\n\n        if (_mm256_movemask_epi8(any) != 0) {\n            return false;\n        }\n        i += 32;\n    }\n\n    for (; i <= n - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.length();\n    if (n < 3) \n        return false;\n\n    const char* data = s.data();\n\n    // Step 2: Check adjacent pairs (i, i+1) for all i in [0, n-2]\n    size_t i = 0;\n    size_t len_step2 = n - 1;\n    for (; i + 32 <= len_step2; i += 32) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        __m256i cmp = _mm256_cmpeq_epi8(current, next);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    for (; i < len_step2; ++i) {\n        if (data[i] == data[i+1])\n            return false;\n    }\n\n    // Step 3: Check pairs (i, i+2) for all i in [0, n-3]\n    size_t j = 0;\n    size_t len_step3 = n - 2;\n    for (; j + 32 <= len_step3; j += 32) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + j));\n        __m256i next2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + j + 2));\n        __m256i cmp = _mm256_cmpeq_epi8(current, next2);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    for (; j < len_step3; ++j) {\n        if (data[j] == data[j+2])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n    size_t i = 0;\n    if (n >= 34) {\n        size_t limit = n - 34;\n        for (; i <= limit; i += 32) {\n            const char* base = s.data() + i;\n            __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(base));\n            __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(base + 1));\n            __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(base + 2));\n            __m256i eq01 = _mm256_cmpeq_epi8(v0, v1);\n            __m256i eq02 = _mm256_cmpeq_epi8(v0, v2);\n            __m256i eq12 = _mm256_cmpeq_epi8(v1, v2);\n            __m256i any_eq = _mm256_or_si256(_mm256_or_si256(eq01, eq02), eq12);\n            if (!_mm256_testz_si256(any_eq, any_eq)) {\n                return false;\n            }\n        }\n    }\n    for (; i <= n - 3; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n\n    size_t j = 0;\n    const char* data = s.data();\n    while (j + 33 < n) {\n        __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + j));\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + j + 1));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + j + 2));\n\n        __m256i eq1 = _mm256_cmpeq_epi8(v0, v1);\n        __m256i eq2 = _mm256_cmpeq_epi8(v0, v2);\n        __m256i eq3 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i any_fail = _mm256_or_si256(eq1, _mm256_or_si256(eq2, eq3));\n\n        if (_mm256_movemask_epi8(any_fail) != 0) {\n            return false;\n        }\n        j += 32;\n    }\n\n    for (; j <= n - 3; ++j) {\n        if (data[j] == data[j+1] || data[j] == data[j+2] || data[j+1] == data[j+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t nval = s.size();\n    if (nval < 3) {\n        return false;\n    }\n    size_t num_triplets = nval - 2;\n    size_t i = 0;\n\n    if (nval >= 34) {\n        for (; i <= nval - 34; i += 32) {\n            __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i + 1));\n            __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i + 2));\n            \n            __m256i eq01 = _mm256_cmpeq_epi8(v0, v1);\n            __m256i eq02 = _mm256_cmpeq_epi8(v0, v2);\n            __m256i eq12 = _mm256_cmpeq_epi8(v1, v2);\n            \n            __m256i any_eq = _mm256_or_si256(_mm256_or_si256(eq01, eq02), eq12);\n            \n            if (!_mm256_testz_si256(any_eq, any_eq)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < num_triplets; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int n_val = (int)str.length();\n    if (n_val <= 1) return false;\n    if (n_val == 2) return true;\n    if (n_val % 2 == 0) return false;\n\n    int limit = (int)std::sqrt(n_val);\n\n    for (int base = 3; base <= limit; base += 16) {\n        __m256i base_vec = _mm256_set1_epi32(base);\n        __m256i step_vec = _mm256_setr_epi32(0,2,4,6,8,10,12,14);\n        __m256i divisors = _mm256_add_epi32(base_vec, step_vec);\n\n        __m128i div_low = _mm256_castsi256_si128(divisors);\n        __m128i limit_vec = _mm_set1_epi32(limit);\n        __m128i mask_low = _mm_cmpgt_epi32(div_low, limit_vec);\n\n        __m256d div_low_d = _mm256_cvtepi32_pd(div_low);\n        __m256d n_d = _mm256_set1_pd((double)n_val);\n        __m256d quot_low_d = _mm256_div_pd(n_d, div_low_d);\n        quot_low_d = _mm256_round_pd(quot_low_d, _MM_FROUND_TO_ZERO);\n        __m128i quot_low_i = _mm256_cvttpd_epi32(quot_low_d);\n\n        __m128i product_low = _mm_mullo_epi32(div_low, quot_low_i);\n        __m128i rem_low = _mm_sub_epi32(_mm_set1_epi32(n_val), product_low);\n        rem_low = _mm_blendv_epi8(rem_low, _mm_set1_epi32(1), mask_low);\n\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp_low = _mm_cmpeq_epi32(rem_low, zero);\n        if (_mm_movemask_epi8(cmp_low) != 0) {\n            return false;\n        }\n\n        __m128i div_high = _mm256_extractf128_si256(divisors, 1);\n        __m128i mask_high = _mm_cmpgt_epi32(div_high, limit_vec);\n\n        __m256d div_high_d = _mm256_cvtepi32_pd(div_high);\n        __m256d quot_high_d = _mm256_div_pd(n_d, div_high_d);\n        quot_high_d = _mm256_round_pd(quot_high_d, _MM_FROUND_TO_ZERO);\n        __m128i quot_high_i = _mm256_cvttpd_epi32(quot_high_d);\n\n        __m128i product_high = _mm_mullo_epi32(div_high, quot_high_i);\n        __m128i rem_high = _mm_sub_epi32(_mm_set1_epi32(n_val), product_high);\n        rem_high = _mm_blendv_epi8(rem_high, _mm_set1_epi32(1), mask_high);\n\n        __m128i cmp_high = _mm_cmpeq_epi32(rem_high, zero);\n        if (_mm_movemask_epi8(cmp_high) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <string>\n\nbool prime_length_simd(const std::string & str) {\n    unsigned int n = str.length();\n    if (n <= 1) {\n        return false;\n    }\n    if (n == 2) {\n        return true;\n    }\n    if (n % 2 == 0) {\n        return false;\n    }\n    unsigned int max_div = static_cast<unsigned int>(std::sqrt(n));\n    bool break_outer = false;\n    for (unsigned int i = 3; i <= max_div; i += 16) {\n        __m256i base_vec = _mm256_set1_epi32(i);\n        __m256i step_vec = _mm256_set_epi32(14, 12, 10, 8, 6, 4, 2, 0);\n        __m256i divs = _mm256_add_epi32(base_vec, step_vec);\n        unsigned int div_array[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(div_array), divs);\n        for (int j = 0; j < 8; ++j) {\n            if (div_array[j] > max_div) {\n                break_outer = true;\n                break;\n            }\n            if (n % div_array[j] == 0) {\n                return false;\n            }\n        }\n        if (break_outer) {\n            break;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <string>\n#include <cstdint>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len == 2) return true;\n    if (len % 2 == 0) return false;\n\n    if (len > UINT32_MAX) {\n        size_t root = (size_t)std::sqrt(len);\n        for (size_t i = 3; i <= root; i += 2) {\n            if (len % i == 0) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    uint32_t n = (uint32_t)len;\n    uint32_t root = (uint32_t)std::sqrt(n);\n    \n    for (uint32_t i = 3; i <= root; i += 8) {\n        __m128i divisors = _mm_setr_epi32(i, i+2, i+4, i+6);\n        __m128i root_vec = _mm_set1_epi32(root);\n        __m128i mask_valid = _mm_cmple_epi32(divisors, root_vec);\n        \n        __m256d div_double = _mm256_cvtepi32_pd(divisors);\n        __m256d n_double = _mm256_set1_pd((double)n);\n        __m256d quot_double = _mm256_div_pd(n_double, div_double);\n        __m128i quot_int = _mm256_cvttpd_epi32(quot_double);\n        \n        __m128i product = _mm_mullo_epi32(quot_int, divisors);\n        __m128i rem = _mm_sub_epi32(_mm_set1_epi32(n), product);\n        __m128i mask = _mm_cmpeq_epi32(rem, _mm_setzero_si128());\n        __m128i final_mask = _mm_and_si128(mask, mask_valid);\n        \n        if (!_mm_testz_si128(final_mask, final_mask)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <cmath>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int n = str.length();\n    if (n <= 1) \n        return false;\n    if (n == 2)\n        return true;\n    if (n % 2 == 0)\n        return false;\n    \n    int limit = static_cast<int>(std::sqrt(n));\n    __m256i step_vec = _mm256_set1_epi32(16);\n    __m256i divisors = _mm256_setr_epi32(3, 5, 7, 9, 11, 13, 15, 17);\n    \n    while (true) {\n        int first_div = _mm256_extract_epi32(divisors, 0);\n        if (first_div > limit) \n            break;\n        \n        alignas(32) int div_arr[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(div_arr), divisors);\n        \n        for (int i = 0; i < 8; i++) {\n            int d = div_arr[i];\n            if (d > limit) \n                break;\n            if (n % d == 0) \n                return false;\n        }\n        divisors = _mm256_add_epi32(divisors, step_vec);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <string>\n#include <climits>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len > INT_MAX) {\n        if (len <= 1) return false;\n        if (len == 2) return true;\n        if (len % 2 == 0) return false;\n        unsigned long long n = len;\n        unsigned long long limit = (unsigned long long)sqrt(n) + 1;\n        for (unsigned long long i = 3; i <= limit; i += 2) {\n            if (n % i == 0) {\n                return false;\n            }\n        }\n        return true;\n    }\n    \n    int n = (int)len;\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n    \n    int limit = (int)std::sqrt(n) + 1;\n    if (n > (1 << 24)) {\n        for (int i = 3; i <= limit; i += 2) {\n            if (n % i == 0) {\n                return false;\n            }\n        }\n        return true;\n    }\n    \n    for (int i = 3; i <= limit; i += 16) {\n        __m256i divisors = _mm256_set_epi32(i+14, i+12, i+10, i+8, i+6, i+4, i+2, i);\n        __m256 div_float = _mm256_cvtepi32_ps(divisors);\n        __m256 inv_approx = _mm256_rcp_ps(div_float);\n        __m256 one_step = _mm256_mul_ps(div_float, inv_approx);\n        one_step = _mm256_sub_ps(_mm256_set1_ps(2.0f), one_step);\n        __m256 inv = _mm256_mul_ps(inv_approx, one_step);\n        __m256 n_float = _mm256_set1_ps((float)n);\n        __m256 q_float = _mm256_mul_ps(n_float, inv);\n        __m256i q = _mm256_cvttps_epi32(q_float);\n        __m256i q_times_d = _mm256_mullo_epi32(q, divisors);\n        __m256i rem = _mm256_sub_epi32(_mm256_set1_epi32(n), q_times_d);\n        __m256i mask_negative = _mm256_cmpgt_epi32(_mm256_setzero_si256(), rem);\n        rem = _mm256_add_epi32(rem, _mm256_and_si256(divisors, mask_negative));\n        __m256i mask_valid = _mm256_cmpgt_epi32(_mm256_set1_epi32(limit+1), divisors);\n        __m256i mask_zero = _mm256_cmpeq_epi32(rem, _mm256_setzero_si256());\n        __m256i mask = _mm256_and_si256(mask_zero, mask_valid);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    char buffer[32] = {0};\n    int len = 0;\n    uint64_t n = N;\n    while (n) {\n        buffer[len++] = '0' + (n % 10);\n        n /= 10;\n    }\n\n    __m256i indices = _mm256_setr_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7,\n        8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23,\n        24, 25, 26, 27, 28, 29, 30, 31\n    );\n    __m256i len_vec = _mm256_set1_epi8(len);\n    __m256i mask = _mm256_cmpgt_epi8(len_vec, indices);\n\n    __m256i chars = _mm256_loadu_si256((const __m256i*)buffer);\n    __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0'));\n    digits = _mm256_and_si256(digits, mask);\n\n    __m256i sum_vec = _mm256_sad_epu8(digits, _mm256_setzero_si256());\n    uint64_t sum_arr[4];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    uint64_t total_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    if (total_sum == 0) {\n        return \"0\";\n    }\n    std::string bin_str;\n    while (total_sum) {\n        bin_str = char('0' + (total_sum & 1)) + bin_str;\n        total_sum >>= 1;\n    }\n    return bin_str;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    char buffer[32] = {0};\n    if (N == 0) {\n        return \"0\";\n    }\n\n    int index = 0;\n    uint64_t num = N;\n    while (num) {\n        buffer[index++] = '0' + (num % 10);\n        num /= 10;\n    }\n    std::reverse(buffer, buffer + index);\n\n    char padded[32] = {0};\n    std::memcpy(padded, buffer, index);\n\n    __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(padded));\n    __m256i zeros_char = _mm256_set1_epi8('0');\n    __m256i digits = _mm256_sub_epi8(v, zeros_char);\n\n    __m128i low = _mm256_castsi256_si128(digits);\n    __m128i high = _mm256_extracti128_si256(digits, 1);\n    __m128i zero = _mm_setzero_si128();\n\n    __m128i sum_low = _mm_sad_epu8(low, zero);\n    __m128i sum_high = _mm_sad_epu8(high, zero);\n\n    uint64_t s0 = _mm_extract_epi64(sum_low, 0);\n    uint64_t s1 = _mm_extract_epi64(sum_low, 1);\n    uint64_t s2 = _mm_extract_epi64(sum_high, 0);\n    uint64_t s3 = _mm_extract_epi64(sum_high, 1);\n    uint64_t total = s0 + s1 + s2 + s3;\n\n    if (total == 0) {\n        return \"0\";\n    }\n\n    std::string bin_str;\n    while (total) {\n        bin_str = char('0' + (total & 1)) + bin_str;\n        total >>= 1;\n    }\n    return bin_str;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdio>\n#include <cstring>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    char temp[21];\n    snprintf(temp, sizeof(temp), \"%020llu\", (unsigned long long)N);\n\n    char buffer[32];\n    memset(buffer, '0', sizeof(buffer));\n    memcpy(buffer, temp, 20);\n\n    __m256i v = _mm256_loadu_si256(reinterpret_cast<__m256i*>(buffer));\n    __m256i ascii0 = _mm256_set1_epi8('0');\n    v = _mm256_sub_epi8(v, ascii0);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sums = _mm256_sad_epu8(v, zero);\n\n    uint64_t sum_arr[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_arr), sums);\n    uint64_t total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    if (total == 0) {\n        return \"0\";\n    }\n\n    std::string bin;\n    while (total) {\n        bin = static_cast<char>('0' + (total & 1)) + bin;\n        total >>= 1;\n    }\n    return bin;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    std::string num_str = std::to_string(N);\n    int len = num_str.length();\n    char buffer[32];\n    \n    for (int i = 0; i < 32; ++i) {\n        buffer[i] = '0';\n    }\n    for (int i = 0; i < len; ++i) {\n        buffer[i] = num_str[i];\n    }\n\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(buffer));\n    __m256i ascii_zero = _mm256_set1_epi8('0');\n    __m256i digits = _mm256_sub_epi8(vec, ascii_zero);\n    \n    __m256i sum_sad = _mm256_sad_epu8(digits, _mm256_setzero_si256());\n    \n    uint64_t sums[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sums), sum_sad);\n    uint64_t total_digit_sum = sums[0] + sums[1] + sums[2] + sums[3];\n    \n    if (total_digit_sum == 0) {\n        return \"0\";\n    }\n    \n    std::string binary_str = \"\";\n    uint64_t n = total_digit_sum;\n    while (n) {\n        binary_str = static_cast<char>('0' + (n & 1)) + binary_str;\n        n >>= 1;\n    }\n    \n    return binary_str;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(uint64_t N) {\n    if (N == 0) {\n        return \"0\";\n    }\n    std::string str = std::to_string(N);\n    size_t len = str.length();\n    if (len > 20) len = 20;\n\n    std::string padded = (len < 20) ? std::string(20 - len, '0') + str : str;\n\n    char buffer[32];\n    std::memset(buffer, '0', 32);\n    std::memcpy(buffer, padded.c_str(), 20);\n\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<__m256i*>(buffer));\n    __m256i ascii_zero = _mm256_set1_epi8('0');\n    __m256i digits = _mm256_sub_epi8(vec, ascii_zero);\n\n    __m256i sum_vector = _mm256_sad_epu8(digits, _mm256_setzero_si256());\n    \n    uint64_t sum_arr[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_arr), sum_vector);\n    \n    uint64_t total_digit_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    if (total_digit_sum == 0) {\n        return \"0\";\n    }\n\n    std::string bin_str;\n    uint64_t num = total_digit_sum;\n    while (num) {\n        bin_str.push_back('0' + (num & 1));\n        num >>= 1;\n    }\n    std::reverse(bin_str.begin(), bin_str.end());\n    return bin_str;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n    int i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_odd_index = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n    \n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i lsb = _mm256_and_si256(data, ones);\n        __m256i mask_even_value = _mm256_cmpeq_epi32(lsb, _mm256_setzero_si256());\n        __m256i mask_combined = _mm256_and_si256(mask_odd_index, mask_even_value);\n        __m256i selected = _mm256_and_si256(data, mask_combined);\n        sum_vec = _mm256_add_epi32(sum_vec, selected);\n    }\n    \n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    int sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    \n    for (; i < n; ++i) {\n        if ((i & 1) && !(lst[i] & 1)) {\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n    int total = 0;\n    size_t i = 0;\n\n    const __m256i offset_parity = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n    const __m256i one = _mm256_set1_epi32(1);\n    const __m256i zero = _mm256_setzero_si256();\n\n    for (; i + 8 <= n; i += 8) {\n        int base_parity = i & 1;\n        __m256i base_parity_vec = _mm256_set1_epi32(base_parity);\n        __m256i index_parity = _mm256_xor_si256(base_parity_vec, offset_parity);\n        __m256i odd_index_mask = _mm256_cmpeq_epi32(index_parity, one);\n\n        __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        __m256i value_parity = _mm256_and_si256(data, one);\n        __m256i even_value_mask = _mm256_cmpeq_epi32(value_parity, zero);\n\n        __m256i mask = _mm256_and_si256(odd_index_mask, even_value_mask);\n        __m256i selected = _mm256_and_si256(data, mask);\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, selected);\n        for (int j = 0; j < 8; j++) {\n            total += temp[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if ((i & 1) && (lst[i] % 2 == 0)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int n = lst.size();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i base_pattern = _mm256_setr_epi32(0,1,0,1,0,1,0,1);\n    __m256i sum_vec = zero;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        int parity = 1 ^ (i & 1);\n        __m256i desired = _mm256_set1_epi32(parity);\n        __m256i odd_index_mask = _mm256_cmpeq_epi32(base_pattern, desired);\n        __m256i data_lsb = _mm256_and_si256(data, one);\n        __m256i even_mask = _mm256_cmpeq_epi32(data_lsb, zero);\n        __m256i mask = _mm256_and_si256(odd_index_mask, even_mask);\n        __m256i selected = _mm256_and_si256(data, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, selected);\n    }\n\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum_vec);\n    int sum = 0;\n    for (int j = 0; j < 8; j++) {\n        sum += sum_arr[j];\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 1 && (lst[i] % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int total = 0;\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i index_mask = _mm256_setr_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i const*)&lst[i]);\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i lsb = _mm256_and_si256(data, ones);\n            __m256i even_mask = _mm256_cmpeq_epi32(lsb, _mm256_setzero_si256());\n            __m256i mask = _mm256_and_si256(even_mask, index_mask);\n            __m256i selected = _mm256_and_si256(data, mask);\n            \n            __m128i low = _mm256_castsi256_si128(selected);\n            __m128i high = _mm256_extracti128_si256(selected, 1);\n            __m128i sum128 = _mm_add_epi32(low, high);\n            __m128i hadd1 = _mm_hadd_epi32(sum128, sum128);\n            __m128i hadd2 = _mm_hadd_epi32(hadd1, hadd1);\n            total += _mm_extract_epi32(hadd2, 0);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if ((i % 2 == 1) && (lst[i] % 2 == 0)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int sum = 0;\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i offset_base = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        for (; i <= n - 8; i += 8) {\n            __m256i chunk = _mm256_loadu_si256((__m256i const*)&lst[i]);\n            __m256i base_vec = _mm256_set1_epi32(i);\n            __m256i indices = _mm256_add_epi32(base_vec, offset_base);\n            \n            __m256i mod2_indices = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n            __m256i mask_a = _mm256_cmpeq_epi32(mod2_indices, _mm256_set1_epi32(1));\n            \n            __m256i mod2_chunk = _mm256_and_si256(chunk, _mm256_set1_epi32(1));\n            __m256i mask_b = _mm256_cmpeq_epi32(mod2_chunk, _mm256_set1_epi32(0));\n            \n            __m256i mask = _mm256_and_si256(mask_a, mask_b);\n            __m256i selected = _mm256_and_si256(chunk, mask);\n            \n            __m128i low128 = _mm256_castsi256_si128(selected);\n            __m128i high128 = _mm256_extracti128_si256(selected, 1);\n            low128 = _mm_add_epi32(low128, high128);\n            \n            __m128i shifted8 = _mm_srli_si128(low128, 8);\n            __m128i sum64 = _mm_add_epi32(low128, shifted8);\n            __m128i shifted4 = _mm_srli_si128(sum64, 4);\n            __m128i sum32 = _mm_add_epi32(sum64, shifted4);\n            \n            sum += _mm_cvtsi128_si32(sum32);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if ((i % 2 == 1) && (lst[i] % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n    std::string res(len, '\\0');\n    \n    size_t i = 0;\n    const size_t block_size = 32;\n    if (len >= block_size) {\n        for (; i <= len - block_size; i += block_size) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i base = _mm256_set1_epi8('a');\n            __m256i shifted = _mm256_sub_epi8(chunk, base);\n            __m256i add_val = _mm256_set1_epi8(4);\n            shifted = _mm256_add_epi8(shifted, add_val);\n            __m256i cmp = _mm256_cmpgt_epi8(shifted, _mm256_set1_epi8(25));\n            __m256i adjust = _mm256_and_si256(cmp, _mm256_set1_epi8(26));\n            shifted = _mm256_sub_epi8(shifted, adjust);\n            __m256i result = _mm256_add_epi8(shifted, base);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), result);\n        }\n    }\n    \n    for (; i < len; ++i) {\n        char c = s[i];\n        c = (c - 'a' + 4) % 26 + 'a';\n        res[i] = c;\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) \n        return s;\n\n    std::string result(s.size(), 0);\n    size_t i = 0;\n    const size_t n = s.size();\n    const size_t simd_size = 32;\n\n    for (; i + simd_size <= n; i += simd_size) {\n        __m256i block = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_add_epi8(block, _mm256_set1_epi8(4));\n        __m256i mask = _mm256_cmpgt_epi8(shifted, _mm256_set1_epi8('z'));\n        __m256i correction = _mm256_and_si256(mask, _mm256_set1_epi8(26));\n        __m256i adjusted = _mm256_sub_epi8(shifted, correction);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), adjusted);\n    }\n\n    for (; i < n; ++i) {\n        char c = s[i] + 4;\n        if (c > 'z') \n            c -= 26;\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) {\n        return \"\";\n    }\n\n    std::string result(s.size(), '\\0');\n    const char* src = s.data();\n    char* dst = &result[0];\n    size_t len = s.size();\n    size_t i = 0;\n\n    const __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n    const __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n    const __m256i z_char = _mm256_set1_epi8('z');\n    const __m256i four = _mm256_set1_epi8(4);\n    const __m256i twenty_six = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        __m256i mask_ge_a = _mm256_cmpgt_epi8(v, a_minus1);\n        __m256i mask_le_z = _mm256_cmpgt_epi8(z_plus1, v);\n        __m256i mask_letter = _mm256_and_si256(mask_ge_a, mask_le_z);\n        __m256i v1 = _mm256_add_epi8(v, four);\n        __m256i mask_wrap = _mm256_and_si256(_mm256_cmpgt_epi8(v1, z_char), mask_letter);\n        __m256i v2 = _mm256_sub_epi8(v1, twenty_six);\n        __m256i v_temp = _mm256_blendv_epi8(v1, v2, mask_wrap);\n        __m256i v_result = _mm256_blendv_epi8(v, v_temp, mask_letter);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dst + i), v_result);\n    }\n\n    for (; i < len; ++i) {\n        char c = src[i];\n        if (c >= 'a' && c <= 'z') {\n            c += 4;\n            if (c > 'z') {\n                c -= 26;\n            }\n        }\n        dst[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n    std::string res(len, 0);\n    const char* src = s.c_str();\n    char* dest = &res[0];\n    size_t i = 0;\n\n    // Process 32-byte chunks\n    constexpr size_t vec_size = 32;\n    for (; i + vec_size <= len; i += vec_size) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        \n        // Identify lowercase letters\n        __m256i a_minus = _mm256_set1_epi8('a' - 1);\n        __m256i z_plus = _mm256_set1_epi8('z' + 1);\n        __m256i mask_lo = _mm256_cmpgt_epi8(v, a_minus);\n        __m256i mask_hi = _mm256_cmpgt_epi8(z_plus, v);\n        __m256i mask_alpha = _mm256_and_si256(mask_lo, mask_hi);\n        \n        // Add 4 to all characters\n        __m256i add4 = _mm256_set1_epi8(4);\n        __m256i shifted = _mm256_add_epi8(v, add4);\n        \n        // Check for overflow beyond 'z'\n        __m256i z_val = _mm256_set1_epi8('z');\n        __m256i overflow = _mm256_cmpgt_epi8(shifted, z_val);\n        __m256i wrap = _mm256_and_si256(overflow, _mm256_set1_epi8(26));\n        __m256i corrected = _mm256_sub_epi8(shifted, wrap);\n        \n        // Blend: keep non-letters unchanged, apply shift+wrap to letters\n        __m256i result = _mm256_blendv_epi8(v, corrected, mask_alpha);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest + i), result);\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        char c = src[i];\n        if (c >= 'a' && c <= 'z') {\n            c += 4;\n            if (c > 'z') c -= 26;\n        }\n        dest[i] = c;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n    std::string result_str(len, '\\0');\n    char* dest = &result_str[0];\n    const char* src = s.c_str();\n\n    const __m256i a_bound = _mm256_set1_epi8('a' - 1);\n    const __m256i z_bound = _mm256_set1_epi8('z' + 1);\n    const __m256i add_val = _mm256_set1_epi8(4);\n    const __m256i wrap_val_vec = _mm256_set1_epi8(26);\n    const __m256i z_limit = _mm256_set1_epi8('z');\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        __m256i low_mask = _mm256_cmpgt_epi8(data, a_bound);\n        __m256i high_mask = _mm256_cmpgt_epi8(z_bound, data);\n        __m256i letter_mask = _mm256_and_si256(low_mask, high_mask);\n        __m256i rotated = _mm256_add_epi8(data, add_val);\n        __m256i wrap_mask = _mm256_cmpgt_epi8(rotated, z_limit);\n        __m256i wrap_amount = _mm256_and_si256(wrap_mask, wrap_val_vec);\n        rotated = _mm256_sub_epi8(rotated, wrap_amount);\n        __m256i res = _mm256_blendv_epi8(data, rotated, letter_mask);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest + i), res);\n    }\n\n    for (; i < len; ++i) {\n        char c = src[i];\n        if (c > 'a' - 1 && c < 'z' + 1) {\n            c += 4;\n            if (c > 'z') {\n                c -= 26;\n            }\n        }\n        dest[i] = c;\n    }\n\n    return result_str;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.length();\n    std::string result(len, '\\0');\n    size_t i = 0;\n\n    const char a_minus1 = 'a' - 1;\n    const char z_plus1 = 'z' + 1;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n        \n        __m256i lowcase_for_letter = _mm256_or_si256(data, _mm256_set1_epi8(0x20));\n        __m256i mask_ge_a = _mm256_cmpgt_epi8(lowcase_for_letter, _mm256_set1_epi8(a_minus1));\n        __m256i mask_le_z = _mm256_cmpgt_epi8(_mm256_set1_epi8(z_plus1), lowcase_for_letter);\n        __m256i mask_letter = _mm256_and_si256(mask_ge_a, mask_le_z);\n        \n        __m256i swapped = _mm256_xor_si256(data, _mm256_set1_epi8(0x20));\n        __m256i temp = _mm256_blendv_epi8(data, swapped, mask_letter);\n        \n        __m256i lower_temp = _mm256_or_si256(temp, _mm256_set1_epi8(0x20));\n        __m256i mask_a = _mm256_cmpeq_epi8(lower_temp, _mm256_set1_epi8('a'));\n        __m256i mask_e = _mm256_cmpeq_epi8(lower_temp, _mm256_set1_epi8('e'));\n        __m256i mask_i = _mm256_cmpeq_epi8(lower_temp, _mm256_set1_epi8('i'));\n        __m256i mask_o = _mm256_cmpeq_epi8(lower_temp, _mm256_set1_epi8('o'));\n        __m256i mask_u = _mm256_cmpeq_epi8(lower_temp, _mm256_set1_epi8('u'));\n        __m256i vowel_mask = _mm256_or_si256(mask_a, mask_e);\n        vowel_mask = _mm256_or_si256(vowel_mask, mask_i);\n        vowel_mask = _mm256_or_si256(vowel_mask, mask_o);\n        vowel_mask = _mm256_or_si256(vowel_mask, mask_u);\n        \n        __m256i shifted = _mm256_add_epi8(temp, _mm256_set1_epi8(2));\n        __m256i res = _mm256_blendv_epi8(temp, shifted, vowel_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n\n    for (; i < len; i++) {\n        char c = message[i];\n        char lowcase_for_letter = c | 0x20;\n        bool is_letter = (lowcase_for_letter > a_minus1) && (lowcase_for_letter < z_plus1);\n        if (is_letter) {\n            c ^= 0x20;\n            char lower_temp = c | 0x20;\n            if (lower_temp == 'a' || lower_temp == 'e' || lower_temp == 'i' || lower_temp == 'o' || lower_temp == 'u') {\n                c += 2;\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    if (message.empty()) return result;\n    result.resize(message.size());\n    size_t len = message.size();\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n        \n        __m256i cmp_a = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('a'));\n        __m256i cmp_e = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('e'));\n        __m256i cmp_i = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('i'));\n        __m256i cmp_o = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('o'));\n        __m256i cmp_u = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('u'));\n        __m256i cmp_A = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('A'));\n        __m256i cmp_E = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('E'));\n        __m256i cmp_I = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('I'));\n        __m256i cmp_O = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('O'));\n        __m256i cmp_U = _mm256_cmpeq_epi8(v, _mm256_set1_epi8('U'));\n        \n        __m256i vowel_mask = _mm256_or_si256(cmp_a, cmp_e);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_i);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_o);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_u);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_A);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_E);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_I);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_O);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmp_U);\n        \n        __m256i addend = _mm256_and_si256(vowel_mask, _mm256_set1_epi8(2));\n        __m256i after_add = _mm256_add_epi8(v, addend);\n        __m256i flipped = _mm256_xor_si256(after_add, _mm256_set1_epi8(0x20));\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), flipped);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n        c ^= 0x20;\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n    const char* src = message.data();\n    char* dest = &result[0];\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        \n        // Create lowercase version for letter detection\n        __m256i lowcase = _mm256_or_si256(chunk, _mm256_set1_epi8(0x20));\n        \n        // Check if character is a letter\n        __m256i a_bound = _mm256_set1_epi8('a' - 1);\n        __m256i z_bound = _mm256_set1_epi8('z' + 1);\n        __m256i ge_a = _mm256_cmpgt_epi8(lowcase, a_bound);\n        __m256i le_z = _mm256_cmpgt_epi8(z_bound, lowcase);\n        __m256i mask_letter = _mm256_and_si256(ge_a, le_z);\n        \n        // Swap case for letters\n        __m256i swap_vec = _mm256_and_si256(mask_letter, _mm256_set1_epi8(0x20));\n        __m256i swapped = _mm256_xor_si256(chunk, swap_vec);\n        \n        // Force to uppercase for vowel check\n        __m256i force_upper = _mm256_and_si256(swapped, _mm256_set1_epi8(0xDF));\n        \n        // Compare with vowels\n        __m256i vowel_A = _mm256_set1_epi8('A');\n        __m256i vowel_E = _mm256_set1_epi8('E');\n        __m256i vowel_I = _mm256_set1_epi8('I');\n        __m256i vowel_O = _mm256_set1_epi8('O');\n        __m256i vowel_U = _mm256_set1_epi8('U');\n        \n        __m256i cmpA = _mm256_cmpeq_epi8(force_upper, vowel_A);\n        __m256i cmpE = _mm256_cmpeq_epi8(force_upper, vowel_E);\n        __m256i cmpI = _mm256_cmpeq_epi8(force_upper, vowel_I);\n        __m256i cmpO = _mm256_cmpeq_epi8(force_upper, vowel_O);\n        __m256i cmpU = _mm256_cmpeq_epi8(force_upper, vowel_U);\n        \n        __m256i vowel_mask = _mm256_or_si256(cmpA, cmpE);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmpI);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmpO);\n        vowel_mask = _mm256_or_si256(vowel_mask, cmpU);\n        \n        // Replace vowels\n        __m256i add2 = _mm256_add_epi8(swapped, _mm256_set1_epi8(2));\n        __m256i res = _mm256_blendv_epi8(swapped, add2, vowel_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest + i), res);\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        char c = src[i];\n        char low = c | 0x20;\n        if (low >= 'a' && low <= 'z') {\n            c ^= 0x20;\n            char up = c & 0xDF;\n            if (up == 'A' || up == 'E' || up == 'I' || up == 'O' || up == 'U') {\n                c += 2;\n            }\n        }\n        dest[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    if (message.empty()) {\n        return result;\n    }\n    result.resize(message.size());\n    const char* src = message.data();\n    char* dst = &result[0];\n    size_t len = message.size();\n    \n    const __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n    const __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n    const __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n    const __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n    const __m256i bit20 = _mm256_set1_epi8(0x20);\n    const __m256i vowel_a = _mm256_set1_epi8('a');\n    const __m256i vowel_e = _mm256_set1_epi8('e');\n    const __m256i vowel_i = _mm256_set1_epi8('i');\n    const __m256i vowel_o = _mm256_set1_epi8('o');\n    const __m256i vowel_u = _mm256_set1_epi8('u');\n    const __m256i two = _mm256_set1_epi8(2);\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        \n        __m256i gt_A = _mm256_cmpgt_epi8(chunk, A_minus1);\n        __m256i lt_Z = _mm256_cmplt_epi8(chunk, Z_plus1);\n        __m256i is_upper = _mm256_and_si256(gt_A, lt_Z);\n        \n        __m256i gt_a = _mm256_cmpgt_epi8(chunk, a_minus1);\n        __m256i lt_z = _mm256_cmplt_epi8(chunk, z_plus1);\n        __m256i is_lower = _mm256_and_si256(gt_a, lt_z);\n        \n        __m256i is_letter = _mm256_or_si256(is_upper, is_lower);\n        __m256i swap_mask = _mm256_and_si256(is_letter, bit20);\n        __m256i swapped = _mm256_xor_si256(chunk, swap_mask);\n        \n        __m256i low_swapped = _mm256_or_si256(swapped, bit20);\n        \n        __m256i mask_a = _mm256_cmpeq_epi8(low_swapped, vowel_a);\n        __m256i mask_e = _mm256_cmpeq_epi8(low_swapped, vowel_e);\n        __m256i mask_i = _mm256_cmpeq_epi8(low_swapped, vowel_i);\n        __m256i mask_o = _mm256_cmpeq_epi8(low_swapped, vowel_o);\n        __m256i mask_u = _mm256_cmpeq_epi8(low_swapped, vowel_u);\n        __m256i vowel_mask = _mm256_or_si256(mask_a, _mm256_or_si256(mask_e, _mm256_or_si256(mask_i, _mm256_or_si256(mask_o, mask_u))));\n        \n        __m256i replaced = _mm256_add_epi8(swapped, two);\n        __m256i res = _mm256_blendv_epi8(swapped, replaced, vowel_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dst + i), res);\n    }\n    \n    for (; i < len; i++) {\n        char c = src[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            c ^= 0x20;\n            char low_c = c | 0x20;\n            if (low_c == 'a' || low_c == 'e' || low_c == 'i' || low_c == 'o' || low_c == 'u') {\n                c += 2;\n            }\n        }\n        dst[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    size_t len = message.size();\n    std::string result(len, '\\0');\n    const char* src = message.data();\n    char* dest = &result[0];\n    size_t i = 0;\n\n    __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n    __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n    __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n    __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n    __m256i flip_vec = _mm256_set1_epi8(0x20);\n    __m256i two_vec = _mm256_set1_epi8(2);\n\n    __m256i vowel_a = _mm256_set1_epi8('a');\n    __m256i vowel_e = _mm256_set1_epi8('e');\n    __m256i vowel_i = _mm256_set1_epi8('i');\n    __m256i vowel_o = _mm256_set1_epi8('o');\n    __m256i vowel_u = _mm256_set1_epi8('u');\n    __m256i vowel_A = _mm256_set1_epi8('A');\n    __m256i vowel_E = _mm256_set1_epi8('E');\n    __m256i vowel_I = _mm256_set1_epi8('I');\n    __m256i vowel_O = _mm256_set1_epi8('O');\n    __m256i vowel_U = _mm256_set1_epi8('U');\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src + i));\n        \n        __m256i ge_A = _mm256_cmpgt_epi8(data, A_minus1);\n        __m256i le_Z = _mm256_cmpgt_epi8(Z_plus1, data);\n        __m256i upper = _mm256_and_si256(ge_A, le_Z);\n        \n        __m256i ge_a = _mm256_cmpgt_epi8(data, a_minus1);\n        __m256i le_z = _mm256_cmpgt_epi8(z_plus1, data);\n        __m256i lower = _mm256_and_si256(ge_a, le_z);\n        \n        __m256i mask_letter = _mm256_or_si256(upper, lower);\n        __m256i to_flip = _mm256_and_si256(mask_letter, flip_vec);\n        __m256i swapped = _mm256_xor_si256(data, to_flip);\n        \n        __m256i mask_v = _mm256_cmpeq_epi8(swapped, vowel_a);\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_e));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_i));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_o));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_u));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_A));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_E));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_I));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_O));\n        mask_v = _mm256_or_si256(mask_v, _mm256_cmpeq_epi8(swapped, vowel_U));\n        \n        __m256i to_add = _mm256_and_si256(mask_v, two_vec);\n        __m256i result_vec = _mm256_add_epi8(swapped, to_add);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dest + i), result_vec);\n    }\n    \n    for (; i < len; i++) {\n        char c = src[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            c ^= 0x20;\n        }\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n        dest[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_keys_lower = true;\n    bool all_keys_upper = true;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        if (key.empty()) {\n            all_keys_lower = false;\n            all_keys_upper = false;\n            break;\n        }\n\n        bool this_lower = true;\n        bool this_upper = true;\n        size_t len = key.size();\n        const char* ptr = key.c_str();\n        size_t i = 0;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr + i));\n            \n            __m256i cmp_low = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1));\n            __m256i cmp_high = _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk);\n            __m256i in_lower = _mm256_and_si256(cmp_low, cmp_high);\n            unsigned mask_lower = (unsigned)_mm256_movemask_epi8(in_lower);\n            if (mask_lower != 0xFFFFFFFF) {\n                this_lower = false;\n            }\n\n            cmp_low = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1));\n            cmp_high = _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk);\n            __m256i in_upper = _mm256_and_si256(cmp_low, cmp_high);\n            unsigned mask_upper = (unsigned)_mm256_movemask_epi8(in_upper);\n            if (mask_upper != 0xFFFFFFFF) {\n                this_upper = false;\n            }\n\n            if (!this_lower && !this_upper) {\n                break;\n            }\n        }\n\n        for (; i < len; i++) {\n            char c = ptr[i];\n            if (c >= 'a' && c <= 'z') {\n                if (this_upper) {\n                    this_upper = false;\n                }\n            } else if (c >= 'A' && c <= 'Z') {\n                if (this_lower) {\n                    this_lower = false;\n                }\n            } else {\n                this_lower = false;\n                this_upper = false;\n                break;\n            }\n            if (!this_lower && !this_upper) {\n                break;\n            }\n        }\n\n        all_keys_lower = all_keys_lower && this_lower;\n        all_keys_upper = all_keys_upper && this_upper;\n\n        if (!all_keys_lower && !all_keys_upper) {\n            break;\n        }\n    }\n\n    return all_keys_lower || all_keys_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool found_lower = false;\n    bool found_upper = false;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        if (key.empty()) {\n            return false;\n        }\n\n        bool has_lower_key = false;\n        bool has_upper_key = false;\n        const char* p = key.c_str();\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n        __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n        __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n        __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(p + i));\n            __m256i mask_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(data, a_minus1),\n                _mm256_cmpgt_epi8(z_plus1, data)\n            );\n            __m256i mask_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(data, A_minus1),\n                _mm256_cmpgt_epi8(Z_plus1, data)\n            );\n            __m256i mask_alpha = _mm256_or_si256(mask_lower, mask_upper);\n\n            if (_mm256_movemask_epi8(mask_alpha) != -1) {\n                return false;\n            }\n\n            if (_mm256_movemask_epi8(mask_lower)) {\n                has_lower_key = true;\n            }\n            if (_mm256_movemask_epi8(mask_upper)) {\n                has_upper_key = true;\n            }\n        }\n\n        for (; i < len; ++i) {\n            char c = p[i];\n            if (!((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z'))) {\n                return false;\n            }\n            if (c >= 'a' && c <= 'z') {\n                has_lower_key = true;\n            } else {\n                has_upper_key = true;\n            }\n        }\n\n        if (has_lower_key && has_upper_key) {\n            return false;\n        }\n\n        if (has_lower_key) {\n            if (found_upper) {\n                return false;\n            }\n            found_lower = true;\n        } else if (has_upper_key) {\n            if (found_lower) {\n                return false;\n            }\n            found_upper = true;\n        } else {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool first_case_set = false;\n    int first_case = 0;\n    __m256i all_ones = _mm256_set1_epi8(-1);\n    __m256i A_minus1 = _mm256_set1_epi8('A' - 1);\n    __m256i Z_plus1 = _mm256_set1_epi8('Z' + 1);\n    __m256i a_minus1 = _mm256_set1_epi8('a' - 1);\n    __m256i z_plus1 = _mm256_set1_epi8('z' + 1);\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                      16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        size_t len = key.length();\n        if (len == 0) {\n            return false;\n        }\n\n        bool key_has_lower = false;\n        bool key_has_upper = false;\n        size_t i = 0;\n\n        while (i < len) {\n            size_t remaining = len - i;\n            if (remaining >= 32) {\n                __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n                __m256i ge_A = _mm256_cmpgt_epi8(chunk, A_minus1);\n                __m256i le_Z = _mm256_cmpgt_epi8(Z_plus1, chunk);\n                __m256i in_upper = _mm256_and_si256(ge_A, le_Z);\n                __m256i ge_a = _mm256_cmpgt_epi8(chunk, a_minus1);\n                __m256i le_z = _mm256_cmpgt_epi8(z_plus1, chunk);\n                __m256i in_lower = _mm256_and_si256(ge_a, le_z);\n                __m256i is_alpha = _mm256_or_si256(in_upper, in_lower);\n\n                if (_mm256_movemask_epi8(is_alpha) != 0xFFFFFFFF) {\n                    return false;\n                }\n\n                if (!key_has_lower && _mm256_movemask_epi8(in_lower) != 0) {\n                    key_has_lower = true;\n                }\n                if (!key_has_upper && _mm256_movemask_epi8(in_upper) != 0) {\n                    key_has_upper = true;\n                }\n                i += 32;\n            } else {\n                __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8(remaining), indices);\n                __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n                __m256i ge_A = _mm256_cmpgt_epi8(chunk, A_minus1);\n                __m256i le_Z = _mm256_cmpgt_epi8(Z_plus1, chunk);\n                __m256i in_upper = _mm256_and_si256(ge_A, le_Z);\n                __m256i ge_a = _mm256_cmpgt_epi8(chunk, a_minus1);\n                __m256i le_z = _mm256_cmpgt_epi8(z_plus1, chunk);\n                __m256i in_lower = _mm256_and_si256(ge_a, le_z);\n                __m256i is_alpha = _mm256_or_si256(in_upper, in_lower);\n                __m256i check = _mm256_or_si256(is_alpha, _mm256_xor_si256(mask, all_ones));\n\n                if (_mm256_movemask_epi8(check) != 0xFFFFFFFF) {\n                    return false;\n                }\n\n                __m256i lower_valid = _mm256_and_si256(in_lower, mask);\n                __m256i upper_valid = _mm256_and_si256(in_upper, mask);\n\n                if (!key_has_lower && _mm256_movemask_epi8(lower_valid) != 0) {\n                    key_has_lower = true;\n                }\n                if (!key_has_upper && _mm256_movemask_epi8(upper_valid) != 0) {\n                    key_has_upper = true;\n                }\n                i = len;\n            }\n\n            if (key_has_lower && key_has_upper) {\n                return false;\n            }\n        }\n\n        if (!first_case_set) {\n            first_case_set = true;\n            first_case = key_has_lower ? 0 : 1;\n        } else {\n            if (first_case == 0 && key_has_upper) {\n                return false;\n            }\n            if (first_case == 1 && key_has_lower) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty())\n        return false;\n\n    int expected = 0;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        bool key_is_all_lower = true;\n        bool key_is_all_upper = true;\n        size_t len = key.length();\n\n        if (len == 0) {\n            key_is_all_lower = false;\n            key_is_all_upper = false;\n        } else {\n            __m256i a_vec = _mm256_set1_epi8('a' - 1);\n            __m256i z_vec = _mm256_set1_epi8('z' + 1);\n            __m256i A_vec = _mm256_set1_epi8('A' - 1);\n            __m256i Z_vec = _mm256_set1_epi8('Z' + 1);\n            size_t i = 0;\n\n            for (; i < len; i += 32) {\n                size_t remaining = len - i;\n                __m256i v;\n                __m256i valid_cmp;\n\n                if (remaining >= 32) {\n                    v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n                    valid_cmp = _mm256_set1_epi8(-1);\n                } else {\n                    char buffer[32] = {0};\n                    memcpy(buffer, key.c_str() + i, remaining);\n                    v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(buffer));\n                    __m256i indices = _mm256_setr_epi8(\n                        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n                    );\n                    valid_cmp = _mm256_cmpgt_epi8(_mm256_set1_epi8(remaining), indices);\n                }\n\n                __m256i cmp_ge_a = _mm256_cmpgt_epi8(v, a_vec);\n                __m256i cmp_le_z = _mm256_cmpgt_epi8(z_vec, v);\n                __m256i mask_lower = _mm256_and_si256(cmp_ge_a, cmp_le_z);\n\n                __m256i cmp_ge_A = _mm256_cmpgt_epi8(v, A_vec);\n                __m256i cmp_le_Z = _mm256_cmpgt_epi8(Z_vec, v);\n                __m256i mask_upper = _mm256_and_si256(cmp_ge_A, cmp_le_Z);\n\n                __m256i mask_any_letter = _mm256_or_si256(mask_lower, mask_upper);\n                if (!_mm256_testc_si256(mask_any_letter, valid_cmp)) {\n                    key_is_all_lower = false;\n                    key_is_all_upper = false;\n                    break;\n                }\n\n                if (key_is_all_lower && !_mm256_testc_si256(mask_lower, valid_cmp))\n                    key_is_all_lower = false;\n\n                if (key_is_all_upper && !_mm256_testc_si256(mask_upper, valid_cmp))\n                    key_is_all_upper = false;\n\n                if (!key_is_all_lower && !key_is_all_upper)\n                    break;\n            }\n        }\n\n        if (key_is_all_lower) {\n            if (expected == 0) {\n                expected = 1;\n            } else if (expected != 1) {\n                return false;\n            }\n        } else if (key_is_all_upper) {\n            if (expected == 0) {\n                expected = 2;\n            } else if (expected != 2) {\n                return false;\n            }\n        } else {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool global_lower = true;\n    bool global_upper = true;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        bool has_letter = false;\n        bool has_lower = false;\n        bool has_upper = false;\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m256i A_vec = _mm256_set1_epi8('A' - 1);\n        __m256i Z_vec = _mm256_set1_epi8('Z' + 1);\n        __m256i a_vec = _mm256_set1_epi8('a' - 1);\n        __m256i z_vec = _mm256_set1_epi8('z' + 1);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n\n            __m256i upper_mask1 = _mm256_cmpgt_epi8(chunk, A_vec);\n            __m256i upper_mask2 = _mm256_cmpgt_epi8(Z_vec, chunk);\n            __m256i upper_mask = _mm256_and_si256(upper_mask1, upper_mask2);\n            int upper_result = _mm256_movemask_epi8(upper_mask);\n\n            __m256i lower_mask1 = _mm256_cmpgt_epi8(chunk, a_vec);\n            __m256i lower_mask2 = _mm256_cmpgt_epi8(z_vec, chunk);\n            __m256i lower_mask = _mm256_and_si256(lower_mask1, lower_mask2);\n            int lower_result = _mm256_movemask_epi8(lower_mask);\n\n            if (upper_result) {\n                has_upper = true;\n                has_letter = true;\n            }\n            if (lower_result) {\n                has_lower = true;\n                has_letter = true;\n            }\n\n            if (has_upper && has_lower) {\n                break;\n            }\n        }\n\n        for (; i < len; i++) {\n            char c = key[i];\n            if (c >= 'A' && c <= 'Z') {\n                has_upper = true;\n                has_letter = true;\n            } else if (c >= 'a' && c <= 'z') {\n                has_lower = true;\n                has_letter = true;\n            }\n            if (has_upper && has_lower) {\n                break;\n            }\n        }\n\n        if (!has_letter) {\n            global_lower = false;\n            global_upper = false;\n        } else if (has_upper && has_lower) {\n            global_lower = false;\n            global_upper = false;\n        } else if (has_lower) {\n            global_upper = false;\n        } else if (has_upper) {\n            global_lower = false;\n        }\n\n        if (!global_lower && !global_upper) {\n            break;\n        }\n    }\n\n    return global_lower || global_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return std::vector<int>();\n    }\n\n    int total_bytes = (n + 7) / 8;\n    int rounded_bytes = (total_bytes + 31) & ~31;\n    uint8_t* bits = (uint8_t*)_mm_malloc(rounded_bytes, 32);\n    if (bits == nullptr) {\n        return std::vector<int>();\n    }\n\n    for (int i = 0; i < rounded_bytes; ++i) {\n        bits[i] = 0xFF;\n    }\n\n    bits[0] &= ~0x03;\n\n    int sqrt_n = sqrt(n);\n    for (int p = 2; p <= sqrt_n; ++p) {\n        int byte_idx = p / 8;\n        int bit_idx = p % 8;\n        if (!(bits[byte_idx] & (1 << bit_idx))) {\n            continue;\n        }\n\n        if (p < 256) {\n            long long start = static_cast<long long>(p) * p;\n            long long base = start & ~255LL;\n            for (; base < n; base += 256) {\n                unsigned char block_mask[32];\n                for (int i = 0; i < 32; ++i) {\n                    block_mask[i] = 0xFF;\n                }\n\n                long long first = (base < start) ? start : base;\n                if (first >= n) break;\n\n                long long offset = first - base;\n                long long pos = offset;\n                while (pos < 256 && (base + pos) < n) {\n                    int block_byte = pos / 8;\n                    int block_bit = pos % 7;\n                    block_mask[block_byte] &= ~(1 << block_bit);\n                    pos += p;\n                }\n\n                long long base_byte = base / 8;\n                if (base_byte + 32 <= rounded_bytes) {\n                    __m256i mask = _mm256_load_si256((__m256i*)block_mask);\n                    __m256i data = _mm256_load_si256((__m256i*)(bits + base_byte));\n                    data = _mm256_and_si256(data, mask);\n                    _mm256_store_si256((__m256i*)(bits + base_byte), data);\n                } else {\n                    for (int i = 0; i < 32; ++i) {\n                        if (base_byte + i < total_bytes) {\n                            bits[base_byte + i] &= block_mask[i];\n                        }\n                    }\n                }\n            }\n        } else {\n            for (long long j = static_cast<long long>(p) * p; j < n; j += p) {\n                int byte_idx = j / 8;\n                int bit_idx = j % 8;\n                if (byte_idx < total_bytes) {\n                    bits[byte_idx] &= ~(1 << bit_idx);\n                }\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; ++i) {\n        int byte_idx = i / 8;\n        int bit_idx = i % 8;\n        if (bits[byte_idx] & (1 << bit_idx)) {\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(bits);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return {};\n    }\n\n    std::vector<char> sieve(n, 0);\n    sieve[0] = sieve[1] = 1;\n\n    for (int i = 2; static_cast<long long>(i) * static_cast<long long>(i) < n; i++) {\n        if (sieve[i] == 0) {\n            long long start = static_cast<long long>(i) * static_cast<long long>(i);\n            long long step = static_cast<long long>(i);\n\n            for (long long j = start; j <= n - 4 * step; j += 4 * step) {\n                __m256i vbase = _mm256_set1_epi64x(j);\n                __m256i voffset = _mm256_set_epi64x(3 * step, 2 * step, step, 0);\n                __m256i vindices = _mm256_add_epi64(vbase, voffset);\n\n                long long indices[4];\n                _mm256_storeu_si256(reinterpret_cast<__m256i*>(indices), vindices);\n\n                sieve[indices[0]] = 1;\n                sieve[indices[1]] = 1;\n                sieve[indices[2]] = 1;\n                sieve[indices[3]] = 1;\n            }\n\n            for (long long j = start + (4 * step) * ((n - start) / (4 * step)); j < n; j += step) {\n                sieve[j] = 1;\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; i++) {\n        if (sieve[i] == 0) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) {\n        return primes;\n    }\n\n    uint8_t *flags = (uint8_t*)std::malloc(n);\n    if (flags == nullptr) {\n        return primes;\n    }\n\n    flags[0] = 0;\n    flags[1] = 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    int i = 2;\n    for (; i + 31 < n; i += 32) {\n        _mm256_storeu_si256((__m256i*)(flags + i), ones);\n    }\n    for (; i < n; i++) {\n        flags[i] = 1;\n    }\n\n    if (n > 4) {\n        for (int j = 4; j < 32 && j < n; j += 2) {\n            flags[j] = 0;\n        }\n\n        const uint8_t mask2_data[32] = {\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF,\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF,\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF,\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF\n        };\n        __m256i mask2 = _mm256_loadu_si256((const __m256i*)mask2_data);\n\n        int base = 32;\n        for (; base + 32 <= n; base += 32) {\n            __m256i block = _mm256_loadu_si256((__m256i*)(flags + base));\n            block = _mm256_and_si256(block, mask2);\n            _mm256_storeu_si256((__m256i*)(flags + base), block);\n        }\n\n        for (int j = base; j < n; j++) {\n            if (j % 2 == 0) {\n                flags[j] = 0;\n            }\n        }\n    }\n\n    for (int p = 3; p * p < n; p += 2) {\n        if (flags[p]) {\n            for (int j = p * p; j < n; j += p) {\n                flags[j] = 0;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; i++) {\n        if (flags[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    std::free(flags);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return {};\n    }\n    std::vector<unsigned char> sieve(n, 1);\n    sieve[0] = 0;\n    sieve[1] = 0;\n    int sqrt_n = static_cast<int>(std::sqrt(n)) + 1;\n\n    for (int p = 2; p < sqrt_n; ++p) {\n        if (sieve[p]) {\n            int start = p * p;\n            if (start >= n) {\n                continue;\n            }\n            int block_index_start = start / 32;\n            int block_index_end = (n + 31) / 32;\n\n            for (int block_index = block_index_start; block_index < block_index_end; ++block_index) {\n                int base = block_index * 32;\n                int block_end = base + 32;\n                if (block_end > n) {\n                    block_end = n;\n                }\n\n                int r = base % p;\n                int offset = (r == 0) ? 0 : (p - r);\n                int first_multiple = base + offset;\n\n                if (first_multiple < start) {\n                    int k = (start - first_multiple + p - 1) / p;\n                    first_multiple += k * p;\n                }\n\n                if (first_multiple >= block_end) {\n                    continue;\n                }\n\n                if (block_end - base == 32) {\n                    int start_offset = first_multiple - base;\n                    unsigned char mask_array[32];\n                    for (int i = 0; i < 32; ++i) {\n                        mask_array[i] = 0xFF;\n                    }\n                    for (int i = start_offset; i < 32; i += p) {\n                        mask_array[i] = 0x00;\n                    }\n                    __m256i block = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(sieve.data() + base));\n                    __m256i mask_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(mask_array));\n                    block = _mm256_and_si256(block, mask_vec);\n                    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sieve.data() + base), block);\n                } else {\n                    for (int i = first_multiple; i < block_end; i += p) {\n                        sieve[i] = 0;\n                    }\n                }\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; ++i) {\n        if (sieve[i]) {\n            primes.push_back(i);\n        }\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return {};\n    }\n    \n    int sqrt_n = static_cast<int>(std::sqrt(n)) + 1;\n    std::vector<int> small_primes;\n    if (sqrt_n >= 2) {\n        std::vector<unsigned char> small_sieve(sqrt_n + 1, 0xFF);\n        small_sieve[0] = 0;\n        small_sieve[1] = 0;\n        for (int i = 2; i <= sqrt_n; i++) {\n            if (small_sieve[i]) {\n                small_primes.push_back(i);\n                for (int j = i * i; j <= sqrt_n; j += i) {\n                    small_sieve[j] = 0;\n                }\n            }\n        }\n    }\n    \n    std::vector<int> primes;\n    for (int base = 0; base < n; base += 32) {\n        int block_size = std::min(32, n - base);\n        unsigned char block_sieve[32];\n        \n        for (int i = 0; i < block_size; i++) {\n            if (base + i < 2) {\n                block_sieve[i] = 0;\n            } else {\n                block_sieve[i] = 0xFF;\n            }\n        }\n        \n        for (int p : small_primes) {\n            int start_num = std::max(base, p * p);\n            if (start_num >= base + block_size) {\n                continue;\n            }\n            int first = start_num;\n            if (first % p != 0) {\n                first += p - (first % p);\n            }\n            int offset = first - base;\n            while (offset < block_size) {\n                block_sieve[offset] = 0;\n                offset += p;\n            }\n        }\n        \n        __m256i block_vec = _mm256_loadu_si256((__m256i*)block_sieve);\n        __m256i ones = _mm256_set1_epi8(0xFF);\n        __m256i mask = _mm256_cmpeq_epi8(block_vec, ones);\n        unsigned int bitmask = _mm256_movemask_epi8(mask);\n        \n        if (block_size < 32) {\n            bitmask &= (1U << block_size) - 1;\n        }\n        \n        for (int i = 0; i < block_size; i++) {\n            if (bitmask & (1U << i)) {\n                int num = base + i;\n                if (num < n) {\n                    primes.push_back(num);\n                }\n            }\n        }\n    }\n    \n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <nmmintrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.size();\n    const char* p = s.data();\n\n    if (len == 0) {\n        return 0;\n    }\n\n    __m256i vA = _mm256_set1_epi8('A');\n    __m256i vE = _mm256_set1_epi8('E');\n    __m256i vI = _mm256_set1_epi8('I');\n    __m256i vO = _mm256_set1_epi8('O');\n    __m256i vU = _mm256_set1_epi8('U');\n    __m256i mask_even = _mm256_set_epi8(\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1\n    );\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(p + i));\n        __m256i eqA = _mm256_cmpeq_epi8(data, vA);\n        __m256i eqE = _mm256_cmpeq_epi8(data, vE);\n        __m256i eqI = _mm256_cmpeq_epi8(data, vI);\n        __m256i eqO = _mm256_cmpeq_epi8(data, vO);\n        __m256i eqU = _mm256_cmpeq_epi8(data, vU);\n\n        __m256i any = _mm256_or_si256(eqA, eqE);\n        any = _mm256_or_si256(any, eqI);\n        any = _mm256_or_si256(any, eqO);\n        any = _mm256_or_si256(any, eqU);\n\n        __m256i masked = _mm256_and_si256(any, mask_even);\n        int m = _mm256_movemask_epi8(masked);\n        count += _mm_popcnt_u32(m);\n    }\n\n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = p[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n == 0) \n        return 0;\n\n    int total = 0;\n    size_t i = 0;\n\n    if (n >= 32) {\n        const __m256i mask_odd = _mm256_set1_epi16(0x00FF);\n        const __m256i vA = _mm256_set1_epi8('A');\n        const __m256i vE = _mm256_set1_epi8('E');\n        const __m256i vI = _mm256_set1_epi8('I');\n        const __m256i vO = _mm256_set1_epi8('O');\n        const __m256i vU = _mm256_set1_epi8('U');\n\n        for (; i + 32 <= n; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i even_chunk = _mm256_and_si256(chunk, mask_odd);\n\n            __m256i eqA = _mm256_cmpeq_epi8(even_chunk, vA);\n            __m256i eqE = _mm256_cmpeq_epi8(even_chunk, vE);\n            __m256i eqI = _mm256_cmpeq_epi8(even_chunk, vI);\n            __m256i eqO = _mm256_cmpeq_epi8(even_chunk, vO);\n            __m256i eqU = _mm256_cmpeq_epi8(even_chunk, vU);\n\n            __m256i vowel_mask = _mm256_or_si256(\n                eqA, \n                _mm256_or_si256(\n                    eqE, \n                    _mm256_or_si256(\n                        eqI, \n                        _mm256_or_si256(eqO, eqU)\n                    )\n                )\n            );\n\n            unsigned int mask = static_cast<unsigned int>(_mm256_movemask_epi8(vowel_mask));\n            total += __builtin_popcount(mask);\n        }\n    }\n\n    for (; i < n; i += 2) {\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            total++;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.size();\n    size_t i = 0;\n    int count = 0;\n\n    if (len >= 32) {\n        const __m256i vowel_A = _mm256_set1_epi8('A');\n        const __m256i vowel_E = _mm256_set1_epi8('E');\n        const __m256i vowel_I = _mm256_set1_epi8('I');\n        const __m256i vowel_O = _mm256_set1_epi8('O');\n        const __m256i vowel_U = _mm256_set1_epi8('U');\n        const uint32_t even_mask = 0x55555555;\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n\n            __m256i cmpA = _mm256_cmpeq_epi8(data, vowel_A);\n            __m256i cmpE = _mm256_cmpeq_epi8(data, vowel_E);\n            __m256i cmpI = _mm256_cmpeq_epi8(data, vowel_I);\n            __m256i cmpO = _mm256_cmpeq_epi8(data, vowel_O);\n            __m256i cmpU = _mm256_cmpeq_epi8(data, vowel_U);\n\n            __m256i cmpOr = _mm256_or_si256(cmpA, cmpE);\n            cmpOr = _mm256_or_si256(cmpOr, cmpI);\n            cmpOr = _mm256_or_si256(cmpOr, cmpO);\n            cmpOr = _mm256_or_si256(cmpOr, cmpU);\n\n            int mask = _mm256_movemask_epi8(cmpOr);\n            count += __builtin_popcount(mask & even_mask);\n        }\n    }\n\n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = s[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) \n        return 0;\n        \n    const char* p = s.data();\n    int count = 0;\n\n    __m256i full_even_mask = _mm256_setr_epi8(\n        -1, 0, -1, 0, -1, 0, -1, 0, \n        -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0, \n        -1, 0, -1, 0, -1, 0, -1, 0\n    );\n\n    __m256i vowel_A = _mm256_set1_epi8('A');\n    __m256i vowel_E = _mm256_set1_epi8('E');\n    __m256i vowel_I = _mm256_set1_epi8('I');\n    __m256i vowel_O = _mm256_set1_epi8('O');\n    __m256i vowel_U = _mm256_set1_epi8('U');\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(p + i));\n\n        __m256i cmpA = _mm256_cmpeq_epi8(data, vowel_A);\n        __m256i cmpE = _mm256_cmpeq_epi8(data, vowel_E);\n        __m256i cmpI = _mm256_cmpeq_epi8(data, vowel_I);\n        __m256i cmpO = _mm256_cmpeq_epi8(data, vowel_O);\n        __m256i cmpU = _mm256_cmpeq_epi8(data, vowel_U);\n\n        __m256i anyVowel = _mm256_or_si256(cmpA, cmpE);\n        anyVowel = _mm256_or_si256(anyVowel, cmpI);\n        anyVowel = _mm256_or_si256(anyVowel, cmpO);\n        anyVowel = _mm256_or_si256(anyVowel, cmpU);\n\n        __m256i vowel_even = _mm256_and_si256(anyVowel, full_even_mask);\n        int mask = _mm256_movemask_epi8(vowel_even);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = p[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return 0;\n\n    const __m256i even_mask = _mm256_setr_epi8(\n        -1, 0, -1, 0, -1, 0, -1, 0, \n        -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0\n    );\n    const __m256i odd_mask = _mm256_setr_epi8(\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1,\n        0, -1, 0, -1, 0, -1, 0, -1\n    );\n    const __m256i indices = _mm256_setr_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, \n        8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23,\n        24, 25, 26, 27, 28, 29, 30, 31\n    );\n    const __m256i v_A = _mm256_set1_epi8('A');\n    const __m256i v_E = _mm256_set1_epi8('E');\n    const __m256i v_I = _mm256_set1_epi8('I');\n    const __m256i v_O = _mm256_set1_epi8('O');\n    const __m256i v_U = _mm256_set1_epi8('U');\n\n    const char* str = s.data();\n    int total = 0;\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t rem = len - i;\n        if (rem > 32) rem = 32;\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i rem_vec = _mm256_set1_epi8(rem);\n        __m256i valid_mask = _mm256_cmpgt_epi8(rem_vec, indices);\n\n        __m256i pattern_mask = (i % 2 == 0) ? even_mask : odd_mask;\n        __m256i final_mask = _mm256_and_si256(pattern_mask, valid_mask);\n\n        __m256i eq_A = _mm256_cmpeq_epi8(chunk, v_A);\n        __m256i eq_E = _mm256_cmpeq_epi8(chunk, v_E);\n        __m256i eq_I = _mm256_cmpeq_epi8(chunk, v_I);\n        __m256i eq_O = _mm256_cmpeq_epi8(chunk, v_O);\n        __m256i eq_U = _mm256_cmpeq_epi8(chunk, v_U);\n\n        __m256i vowel = _mm256_or_si256(eq_A, eq_E);\n        vowel = _mm256_or_si256(vowel, eq_I);\n        vowel = _mm256_or_si256(vowel, eq_O);\n        vowel = _mm256_or_si256(vowel, eq_U);\n\n        __m256i result = _mm256_and_si256(vowel, final_mask);\n        int bitmask = _mm256_movemask_epi8(result);\n        total += __builtin_popcount(bitmask);\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) {\n        return res;\n    }\n\n    int i = 0;\n    __m256i v_offsets = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n\n    for (; i + 7 < n; i += 8) {\n        int base_val = n + 2 * i;\n        __m256i v_base = _mm256_set1_epi32(base_val);\n        __m256i v_data = _mm256_add_epi32(v_base, v_offsets);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), v_data);\n    }\n\n    for (; i < n; i++) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) \n        return res;\n\n    int i = 0;\n    const __m256i step = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n    \n    for (; i + 7 < n; i += 8) {\n        int base_val = n + 2 * i;\n        __m256i base = _mm256_set1_epi32(base_val);\n        __m256i vec = _mm256_add_epi32(base, step);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), vec);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) \n        return res;\n\n    const int vector_size = 8;\n    int i = 0;\n    __m256i step = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n\n    for (; i + vector_size <= n; i += vector_size) {\n        int base_val = n + 2 * i;\n        __m256i base = _mm256_set1_epi32(base_val);\n        __m256i chunk = _mm256_add_epi32(base, step);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&res[i]), chunk);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> vec(n);\n    if (n <= 0) {\n        return vec;\n    }\n    int* data = vec.data();\n    const __m256i v_indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    \n    for (int i0 = 0; i0 < n; i0 += 8) {\n        int remaining = n - i0;\n        if (remaining >= 8) {\n            __m256i base = _mm256_set1_epi32(n + 2 * i0);\n            __m256i values = _mm256_add_epi32(base, _mm256_slli_epi32(v_indices, 1));\n            _mm256_storeu_si256((__m256i*)(data + i0), values);\n        } else {\n            for (int j = 0; j < remaining; j++) {\n                data[i0 + j] = n + 2 * (i0 + j);\n            }\n        }\n    }\n    return vec;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) \n        return res;\n\n    int i = 0;\n    __m256i step = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n\n    for (; i + 8 <= n; i += 8) {\n        int base_val = n + 2 * i;\n        __m256i base_vec = _mm256_set1_epi32(base_val);\n        __m256i values = _mm256_add_epi32(base_vec, step);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res.data() + i), values);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i v_n = _mm_cvtsi64_si128(n);\n    __m128i v_m = _mm_cvtsi64_si128(m);\n    __m128i and_vec = _mm_and_si128(v_n, v_m);\n    __m128i xor_vec = _mm_xor_si128(v_n, v_m);\n    __m128i shift_xor = _mm_srli_epi64(xor_vec, 1);\n    __m128i avg_vec = _mm_add_epi64(and_vec, shift_xor);\n    int64_t avg = _mm_cvtsi128_si64(avg_vec);\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string binary;\n    while (avg) {\n        binary.insert(binary.begin(), '0' + (avg & 1));\n        avg >>= 1;\n    }\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i n_vec = _mm_cvtsi64_si128(n);\n    __m128i m_vec = _mm_cvtsi64_si128(m);\n    __m128i sum = _mm_add_epi64(n_vec, m_vec);\n    __m128i avg_vec = _mm_srli_epi64(sum, 1);\n    int64_t avg = _mm_cvtsi128_si64(avg_vec);\n    uint64_t u_avg = static_cast<uint64_t>(avg);\n    std::string res;\n    for (int i = 63; i >= 0; i--) {\n        uint64_t mask = (1ULL << i);\n        if (u_avg & mask) {\n            res += '1';\n        } else if (!res.empty()) {\n            res += '0';\n        }\n    }\n    return res.empty() ? \"0\" : res;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n    __m128i and_part = _mm_and_si128(v_n, v_m);\n    __m128i xor_part = _mm_xor_si128(v_n, v_m);\n    xor_part = _mm_srli_epi64(xor_part, 1);\n    __m128i avg_v = _mm_add_epi64(and_part, xor_part);\n    int64_t avg = _mm_cvtsi128_si64(avg_v);\n\n    if (avg == 0) {\n        return \"0\";\n    }\n    std::string bin;\n    int64_t num = avg;\n    while (num) {\n        bin = (num & 1 ? '1' : '0') + bin;\n        num >>= 1;\n    }\n    return bin;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n    uint64_t un = n;\n    uint64_t um = m;\n    __m256i vn = _mm256_set1_epi64x(un);\n    __m256i vm = _mm256_set1_epi64x(um);\n    __m256i vsum = _mm256_add_epi64(vn, vm);\n    __m256i vavg = _mm256_srli_epi64(vsum, 1);\n    uint64_t avg_arr[4];\n    _mm256_storeu_si256((__m256i*)avg_arr, vavg);\n    uint64_t avg = avg_arr[0];\n\n    std::string bin_str = \"\";\n    while (avg) {\n        bin_str = char('0' + (avg & 1)) + bin_str;\n        avg >>= 1;\n    }\n    return bin_str;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "To solve this problem, we need to compute the average of integers from `n` to `m` (inclusive), round it to the nearest integer (with ties rounding to the smaller integer), and convert the result to a binary string. If `n` is greater than `m`, we return \"-1\".\n\n### Approach\n1. **Check for Valid Range**: If `n` is greater than `m`, return \"-1\".\n2. **Compute Average**: The average of integers from `n` to `m` can be efficiently computed using the formula `(n + m) / 2`. This formula works because the sequence of integers from `n` to `m` is arithmetic, and the average of the entire sequence is the same as the average of the first and last terms.\n3. **Handle Zero Case**: If the computed average is zero, return \"0\".\n4. **Convert to Binary Using SIMD**: \n   - **Buffer Initialization**: Create a buffer to store the binary representation of the 64-bit average.\n   - **Bit Processing**: For each byte in the 64-bit average, use SIMD instructions to convert the byte into a string of 8 binary digits. This involves:\n     - **Broadcasting the Byte**: Duplicate the byte across a 128-bit vector.\n     - **Bit Masking**: Use a pre-defined bit mask to isolate each bit in the byte.\n     - **Comparison and Blending**: Compare each isolated bit to zero and blend the result with '0' or '1' to form the binary digits.\n     - **Store Results**: Store the resulting 8 binary digits into the buffer.\n5. **Remove Leading Zeros**: After processing all bytes, skip any leading zeros in the buffer to form the final binary string.\n\n### Solution Code\n```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n    int64_t avg = (n + m) / 2;\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    char buffer[65];\n    __m128i ascii0 = _mm_set1_epi8('0');\n    __m128i ascii1 = _mm_set1_epi8('1');\n    __m128i bit_mask = _mm_set_epi8(0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, \n                                    0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < 8; ++i) {\n        int shift = 56 - i * 8;\n        uint8_t byte = 0;\n        if (shift >= 0) {\n            byte = static_cast<uint8_t>((avg >> shift) & 0xFF);\n        }\n        __m128i byte_vec = _mm_set1_epi8(static_cast<char>(byte));\n        __m128i anded = _mm_and_si128(byte_vec, bit_mask);\n        __m128i cmp = _mm_cmpeq_epi8(anded, _mm_setzero_si128());\n        __m128i res = _mm_blendv_epi8(ascii1, ascii0, cmp);\n        _mm_storel_epi64(reinterpret_cast<__m128i*>(buffer + i * 8), res);\n    }\n    buffer[64] = '\\0';\n\n    int start = 0;\n    while (start < 64 && buffer[start] == '0') {\n        start++;\n    }\n    if (start == 64) {\n        return \"0\";\n    }\n    return std::string(buffer + start);\n}\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if `n` is greater than `m` and returns \"-1\" if true.\n2. **Average Calculation**: The average is computed as `(n + m) / 2` using integer arithmetic, which naturally truncates any fractional part (equivalent to rounding down for non-negative numbers).\n3. **Zero Handling**: If the average is zero, the function returns \"0\".\n4. **SIMD-Based Binary Conversion**:\n   - **Buffer Setup**: A buffer of 65 characters (64 for bits + 1 for null terminator) is initialized.\n   - **Byte Processing Loop**: For each of the 8 bytes in the 64-bit average:\n     - The current byte is extracted by shifting the average appropriately.\n     - The byte is broadcast into a 128-bit vector.\n     - Each bit in the byte is isolated using a bit mask and compared to zero.\n     - Based on the comparison, each bit is converted to '0' or '1' using vector blending.\n     - The resulting 8 binary digits are stored in the buffer.\n5. **Leading Zero Removal**: The buffer is scanned to skip leading zeros, and the remaining characters form the binary string result. If all bits are zero, \"0\" is returned.\n\nThis approach efficiently leverages SIMD instructions for parallel bit processing, ensuring optimal performance while handling large integers. The solution is both concise and effective, meeting the problem requirements."}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) {\n        return res;\n    }\n\n    __m256i step_vec = _mm256_set_epi32(14, 12, 10, 8, 6, 4, 2, 0);\n    for (int base_i = 1; base_i <= n; base_i += 16) {\n        __m256i base_v = _mm256_set1_epi32(base_i);\n        __m256i i_vals = _mm256_add_epi32(base_v, step_vec);\n        __m256i i_plus1 = _mm256_add_epi32(i_vals, _mm256_set1_epi32(1));\n        __m256i prod = _mm256_mullo_epi32(i_vals, i_plus1);\n        __m256i tri = _mm256_srli_epi32(prod, 1);\n        __m256i indices = _mm256_sub_epi32(i_vals, _mm256_set1_epi32(1));\n\n        int tri_arr[8];\n        int idx_arr[8];\n        _mm256_storeu_si256((__m256i*)tri_arr, tri);\n        _mm256_storeu_si256((__m256i*)idx_arr, indices);\n\n        for (int k = 0; k < 8; k++) {\n            int i_val = base_i + 2 * k;\n            if (i_val > n) break;\n            if (idx_arr[k] < n) {\n                res[idx_arr[k]] = tri_arr[k];\n            }\n        }\n    }\n\n    if (n >= 2) {\n        int current = 1;\n        for (int j = 1; j <= n / 2; j++) {\n            current = (int)((static_cast<long long>(current) * (2 * j - 1)) % 10000;\n            current = (int)((static_cast<long long>(current) * (2 * j)) % 10000;\n            int index = 2 * j - 1;\n            if (index < n) {\n                res[index] = current;\n            } else {\n                break;\n            }\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) {\n        return res;\n    }\n    if (n >= 1) {\n        res[0] = 1;\n    }\n    if (n >= 2) {\n        res[1] = 2;\n    }\n    if (n <= 2) {\n        return res;\n    }\n\n    int m = (n + 1) / 2;\n    int p = n / 2;\n    int mod = 10000;\n    int k_start = 1;\n\n    while (k_start < m) {\n        int k_remaining = m - k_start;\n        int chunk_size = (k_remaining > 8) ? 8 : k_remaining;\n        int k_arr[8] = {0};\n        for (int i = 0; i < chunk_size; i++) {\n            k_arr[i] = k_start + i;\n        }\n\n        __m256i k_vec = _mm256_setr_epi32(k_arr[0], k_arr[1], k_arr[2], k_arr[3],\n                                          k_arr[4], k_arr[5], k_arr[6], k_arr[7]);\n        __m256i two = _mm256_set1_epi32(2);\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i i_vec = _mm256_add_epi32(_mm256_mullo_epi32(k_vec, two), one);\n        __m256i k_plus1 = _mm256_add_epi32(k_vec, one);\n        __m256i T_vec = _mm256_mullo_epi32(i_vec, k_plus1);\n\n        int T_arr[8];\n        _mm256_storeu_si256((__m256i*)T_arr, T_vec);\n\n        for (int i = 0; i < chunk_size; i++) {\n            int pos = 2 * k_arr[i];\n            if (pos < n) {\n                res[pos] = T_arr[i];\n            }\n        }\n        k_start += chunk_size;\n    }\n\n    int current_fact = 2;\n    for (int k = 1; k < p; k++) {\n        current_fact = (current_fact * (2 * k + 1)) % mod;\n        current_fact = (current_fact * (2 * k + 2)) % mod;\n        int pos = 2 * k + 1;\n        if (pos < n) {\n            res[pos] = current_fact;\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    static const int fact_mod[9] = {2, 24, 720, 2320, 8800, 1600, 1200, 8000, 8000};\n\n    // Process even indices\n    int j_even = 0;\n    for (; j_even + 14 < n; j_even += 16) {\n        __m256i j_vec = _mm256_setr_epi32(j_even, j_even+2, j_even+4, j_even+6, j_even+8, j_even+10, j_even+12, j_even+14);\n        __m256i i1 = _mm256_add_epi32(j_vec, _mm256_set1_epi32(1));\n        __m256i i2 = _mm256_add_epi32(j_vec, _mm256_set1_epi32(2));\n        __m256i prod = _mm256_mullo_epi32(i1, i2);\n        __m256i tri = _mm256_srli_epi32(prod, 1);\n        _mm256_i32scatter_epi32(res.data(), j_vec, tri, 4);\n    }\n    for (; j_even < n; j_even += 2) {\n        res[j_even] = (j_even + 1) * (j_even + 2) / 2;\n    }\n\n    // Process odd indices\n    int j_odd = 1;\n    for (; j_odd + 14 < n; j_odd += 16) {\n        __m256i j_vec = _mm256_setr_epi32(j_odd, j_odd+2, j_odd+4, j_odd+6, j_odd+8, j_odd+10, j_odd+12, j_odd+14);\n        __m256i k_vec = _mm256_sub_epi32(j_vec, _mm256_set1_epi32(1));\n        k_vec = _mm256_srli_epi32(k_vec, 1);\n        __m256i mask = _mm256_cmplt_epi32(k_vec, _mm256_set1_epi32(9));\n        __m256i fact_vals = _mm256_i32gather_epi32(fact_mod, k_vec, 4);\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i result_fact = _mm256_blendv_epi8(zeros, fact_vals, mask);\n        _mm256_i32scatter_epi32(res.data(), j_vec, result_fact, 4);\n    }\n    for (; j_odd < n; j_odd += 2) {\n        int k = (j_odd - 1) / 2;\n        if (k < 9) {\n            res[j_odd] = fact_mod[k];\n        } else {\n            res[j_odd] = 0;\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) {\n        return res;\n    }\n\n    for (int j0 = 0; j0 < n; j0 += 16) {\n        __m256i i_vals = _mm256_setr_epi32(\n            j0 + 1, j0 + 3, j0 + 5, j0 + 7,\n            j0 + 9, j0 + 11, j0 + 13, j0 + 15\n        );\n        __m256i next = _mm256_add_epi32(i_vals, _mm256_set1_epi32(1));\n        __m256i prod = _mm256_mullo_epi32(i_vals, next);\n        __m256i tri = _mm256_srli_epi32(prod, 1);\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, tri);\n\n        for (int k = 0; k < 8; k++) {\n            int j_index = j0 + 2 * k;\n            if (j_index < n) {\n                res[j_index] = temp[k];\n            }\n        }\n    }\n\n    long long fact = 1;\n    for (int i_val = 2; i_val <= n; i_val += 2) {\n        int j_index = i_val - 1;\n        if (j_index < n) {\n            fact = (fact * (i_val - 1)) % 10000;\n            fact = (fact * i_val) % 10000;\n            res[j_index] = static_cast<int>(fact);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i v_i = _mm256_set_epi32(i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1);\n        __m256i v_i1 = _mm256_set_epi32(i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2);\n        __m256i v_prod = _mm256_mullo_epi32(v_i, v_i1);\n        __m256i v_half = _mm256_srli_epi32(v_prod, 1);\n        \n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, v_half);\n        \n        for (int j = 0; j < 8; j++) {\n            temp[j] %= 10000;\n            result[i + j] = temp[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        uint64_t idx = i + 1;\n        uint64_t next = idx + 1;\n        uint64_t prod = idx * next;\n        prod /= 2;\n        result[i] = static_cast<int>(prod % 10000);\n    }\n\n    int fact = 1;\n    for (int k = 1; k <= n; k++) {\n        fact = (fact * k) % 10000;\n        if (k % 2 == 0) {\n            result[k-1] = fact;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int padded_size = ((n + 7) / 8) * 8;\n    int* rev_arr = new int[padded_size + 1];\n    rev_arr[0] = 0;\n    for (int i = 1; i <= n; i++) {\n        int rev = 0;\n        int temp = i;\n        while (temp) {\n            rev = rev * 10 + temp % 10;\n            temp /= 10;\n        }\n        rev_arr[i] = rev;\n    }\n    for (int i = n + 1; i <= padded_size; i++) {\n        rev_arr[i] = 0;\n    }\n\n    __m256i even_sum_vec = _mm256_setzero_si256();\n    __m256i odd_sum_vec = _mm256_setzero_si256();\n\n    for (int i = 1; i <= padded_size; i += 8) {\n        __m256i valid_mask = _mm256_set_epi32(\n            (i + 7) <= n ? -1 : 0,\n            (i + 6) <= n ? -1 : 0,\n            (i + 5) <= n ? -1 : 0,\n            (i + 4) <= n ? -1 : 0,\n            (i + 3) <= n ? -1 : 0,\n            (i + 2) <= n ? -1 : 0,\n            (i + 1) <= n ? -1 : 0,\n            i <= n ? -1 : 0\n        );\n\n        __m256i vx = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vrev = _mm256_loadu_si256((__m256i const*)(rev_arr + i));\n        __m256i mask_palindrome = _mm256_cmpeq_epi32(vx, vrev);\n        __m256i mask = _mm256_and_si256(mask_palindrome, valid_mask);\n\n        __m256i v_parity = _mm256_and_si256(vx, _mm256_set1_epi32(1));\n        __m256i even_inc = _mm256_and_si256(\n            _mm256_and_si256(mask, _mm256_cmpeq_epi32(v_parity, _mm256_setzero_si256())),\n            _mm256_set1_epi32(1)\n        );\n        __m256i odd_inc = _mm256_and_si256(\n            _mm256_and_si256(mask, _mm256_cmpeq_epi32(v_parity, _mm256_set1_epi32(1))),\n            _mm256_set1_epi32(1)\n        );\n\n        even_sum_vec = _mm256_add_epi32(even_sum_vec, even_inc);\n        odd_sum_vec = _mm256_add_epi32(odd_sum_vec, odd_inc);\n    }\n\n    alignas(32) int even_tmp[8], odd_tmp[8];\n    _mm256_store_si256((__m256i*)even_tmp, even_sum_vec);\n    _mm256_store_si256((__m256i*)odd_tmp, odd_sum_vec);\n    int even_sum = 0, odd_sum = 0;\n    for (int i = 0; i < 8; i++) {\n        even_sum += even_tmp[i];\n        odd_sum += odd_tmp[i];\n    }\n\n    delete[] rev_arr;\n\n    return {even_sum, odd_sum};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n    \n    for (int i = 1; i <= n; i += 8) {\n        int num_in_chunk = (i + 7 <= n) ? 8 : n - i + 1;\n        int base[8] = {0};\n        int rev[8] = {0};\n        int valid_arr[8] = {0};\n        \n        for (int j = 0; j < num_in_chunk; j++) {\n            base[j] = i + j;\n            int x = base[j];\n            int r = 0;\n            while (x) {\n                r = r * 10 + x % 10;\n                x /= 10;\n            }\n            rev[j] = r;\n            valid_arr[j] = -1;\n        }\n        \n        __m256i orig_vec = _mm256_loadu_si256((__m256i const*)base);\n        __m256i rev_vec = _mm256_loadu_si256((__m256i const*)rev);\n        __m256i valid_mask = _mm256_loadu_si256((__m256i const*)valid_arr);\n        \n        __m256i cmp = _mm256_cmpeq_epi32(orig_vec, rev_vec);\n        cmp = _mm256_and_si256(cmp, valid_mask);\n        \n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i lsb = _mm256_and_si256(orig_vec, ones);\n        __m256i even_mask = _mm256_cmpeq_epi32(lsb, _mm256_setzero_si256());\n        __m256i odd_mask = _mm256_cmpeq_epi32(lsb, ones);\n        \n        __m256i even_pal = _mm256_and_si256(cmp, even_mask);\n        __m256i odd_pal = _mm256_and_si256(cmp, odd_mask);\n        \n        int even_pal_mask = _mm256_movemask_ps(_mm256_castsi256_ps(even_pal));\n        int odd_pal_mask = _mm256_movemask_ps(_mm256_castsi256_ps(odd_pal));\n        \n        even_count += _mm_popcnt_u32(even_pal_mask & 0xFF);\n        odd_count += _mm_popcnt_u32(odd_pal_mask & 0xFF);\n    }\n    \n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n    int i = 1;\n    for (; i <= n; i += 8) {\n        __m256i v = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i limit = _mm256_set1_epi32(n+1);\n        __m256i valid_mask = _mm256_cmpgt_epi32(limit, v);\n        alignas(32) int arr[8];\n        alignas(32) int mask_arr[8];\n        _mm256_store_si256((__m256i*)arr, v);\n        _mm256_store_si256((__m256i*)mask_arr, valid_mask);\n        for (int j = 0; j < 8; j++) {\n            if (mask_arr[j] != 0) {\n                int x = arr[j];\n                int reversed = 0;\n                int temp = x;\n                while (temp) {\n                    reversed = reversed * 10 + temp % 10;\n                    temp /= 10;\n                }\n                if (reversed == x) {\n                    if (x % 2 == 0) {\n                        even_count++;\n                    } else {\n                        odd_count++;\n                    }\n                }\n            }\n        }\n    }\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    if (n < 1) {\n        return {0, 0};\n    }\n    int even_count = 0;\n    int odd_count = 0;\n    int i = 1;\n    for (; i <= n - 7; i += 8) {\n        __m256i x_vec = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256 x_f = _mm256_cvtepi32_ps(x_vec);\n        __m256 div10 = _mm256_set1_ps(10.0f);\n        __m256i q1 = _mm256_cvttps_epi32(_mm256_div_ps(x_f, div10));\n        __m256i d0 = _mm256_sub_epi32(x_vec, _mm256_mullo_epi32(q1, _mm256_set1_epi32(10)));\n        __m256 q1_f = _mm256_cvtepi32_ps(q1);\n        __m256i q2 = _mm256_cvttps_epi32(_mm256_div_ps(q1_f, div10));\n        __m256i d1 = _mm256_sub_epi32(q1, _mm256_mullo_epi32(q2, _mm256_set1_epi32(10)));\n        __m256 q2_f = _mm256_cvtepi32_ps(q2);\n        __m256i q3 = _mm256_cvttps_epi32(_mm256_div_ps(q2_f, div10));\n        __m256i d2 = _mm256_sub_epi32(q2, _mm256_mullo_epi32(q3, _mm256_set1_epi32(10)));\n        __m256 q3_f = _mm256_cvtepi32_ps(q3);\n        __m256i q4 = _mm256_cvttps_epi32(_mm256_div_ps(q3_f, div10));\n        __m256i d3 = _mm256_sub_epi32(q3, _mm256_mullo_epi32(q4, _mm256_set1_epi32(10)));\n        __m256i ten = _mm256_set1_epi32(10);\n        __m256i hundred = _mm256_set1_epi32(100);\n        __m256i thousand = _mm256_set1_epi32(1000);\n        __m256i is_1_digit = _mm256_cmpgt_epi32(ten, x_vec);\n        __m256i is_2_digit = _mm256_and_si256(_mm256_cmpgt_epi32(x_vec, _mm256_set1_epi32(9)), _mm256_cmpgt_epi32(hundred, x_vec));\n        __m256i is_3_digit = _mm256_and_si256(_mm256_cmpgt_epi32(x_vec, _mm256_set1_epi32(99)), _mm256_cmpgt_epi32(thousand, x_vec));\n        __m256i is_4_digit = _mm256_cmpgt_epi32(x_vec, _mm256_set1_epi32(999)));\n        __m256i cond_1 = _mm256_set1_epi32(-1);\n        __m256i cond_2 = _mm256_cmpeq_epi32(d0, d1);\n        __m256i cond_3 = _mm256_cmpeq_epi32(d0, d2);\n        __m256i cond_4 = _mm256_and_si256(_mm256_cmpeq_epi32(d0, d3), _mm256_cmpeq_epi32(d1, d2));\n        __m256i cond = _mm256_setzero_si256();\n        cond = _mm256_or_si256(cond, _mm256_and_si256(is_1_digit, cond_1));\n        cond = _mm256_or_si256(cond, _mm256_and_si256(is_2_digit, cond_2));\n        cond = _mm256_or_si256(cond, _mm256_and_si256(is_3_digit, cond_3));\n        cond = _mm256_or_si256(cond, _mm256_and_si256(is_4_digit, cond_4));\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(x_vec, one), _mm256_setzero_si256());\n        __m256i odd_mask = _mm256_cmpeq_epi32(_mm256_and_si256(x_vec, one), one);\n        __m256i even_pal = _mm256_and_si256(cond, even_mask);\n        __m256i odd_pal = _mm256_and_si256(cond, odd_mask);\n        int mask_even = _mm256_movemask_ps(_mm256_castsi256_ps(even_pal));\n        int mask_odd = _mm256_movemask_ps(_mm256_castsi256_ps(odd_pal));\n        even_count += __builtin_popcount(mask_even);\n        odd_count += __builtin_popcount(mask_odd);\n    }\n    for (; i <= n; ++i) {\n        int x = i;\n        int d[4] = {0};\n        int idx = 0;\n        while (x) {\n            d[idx++] = x % 10;\n            x /= 10;\n        }\n        bool is_pal = false;\n        if (idx == 1) {\n            is_pal = true;\n        } else if (idx == 2) {\n            if (d[0] == d[1]) \n                is_pal = true;\n        } else if (idx == 3) {\n            if (d[0] == d[2]) \n                is_pal = true;\n        } else if (idx == 4) {\n            if (d[0] == d[3] && d[1] == d[2])\n                is_pal = true;\n        }\n        if (is_pal) {\n            if (i % 2 == 0) \n                even_count++;\n            else \n                odd_count++;\n        }\n    }\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    if (n < 1) {\n        return {0, 0};\n    }\n\n    __m256i v_even_count = _mm256_setzero_si256();\n    __m256i v_odd_count = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(\n            i, i+1, i+2, i+3, i+4, i+5, i+6, i+7\n        );\n        __m256i n_vec = _mm256_set1_epi32(n);\n        __m256i valid_mask = _mm256_cmpgt_epi32(\n            _mm256_add_epi32(n_vec, _mm256_set1_epi32(1)), indices\n        );\n\n        uint32_t d0[8], d1[8], d2[8], d3[8];\n        for (int j = 0; j < 8; j++) {\n            int num = i + j;\n            if (num > n) {\n                d0[j] = d1[j] = d2[j] = d3[j] = 0;\n            } else {\n                d0[j] = num % 10;\n                num /= 10;\n                d1[j] = num % 10;\n                num /= 10;\n                d2[j] = num % 10;\n                num /= 10;\n                d3[j] = num % 10;\n            }\n        }\n\n        __m256i v_d0 = _mm256_setr_epi32(d0[0], d0[1], d0[2], d0[3], d0[4], d0[5], d0[6], d0[7]);\n        __m256i v_d1 = _mm256_setr_epi32(d1[0], d1[1], d1[2], d1[3], d1[4], d1[5], d1[6], d1[7]);\n        __m256i v_d2 = _mm256_setr_epi32(d2[0], d2[1], d2[2], d2[3], d2[4], d2[5], d2[6], d2[7]);\n        __m256i v_d3 = _mm256_setr_epi32(d3[0], d3[1], d3[2], d3[3], d3[4], d3[5], d3[6], d3[7]);\n\n        __m256i mask1 = _mm256_cmpgt_epi32(_mm256_set1_epi32(10), indices);\n        __m256i mask2 = _mm256_and_si256(\n            _mm256_cmpgt_epi32(_mm256_set1_epi32(100), indices),\n            _mm256_cmpgt_epi32(indices, _mm256_set1_epi32(9))\n        );\n        __m256i mask2_pal = _mm256_and_si256(mask2, _mm256_cmpeq_epi32(v_d0, v_d1));\n        __m256i mask3 = _mm256_and_si256(\n            _mm256_cmpgt_epi32(_mm256_set1_epi32(1000), indices),\n            _mm256_cmpgt_epi32(indices, _mm256_set1_epi32(99))\n        );\n        __m256i mask3_pal = _mm256_and_si256(mask3, _mm256_cmpeq_epi32(v_d0, v_d2));\n        __m256i mask4 = _mm256_cmpgt_epi32(indices, _mm256_set1_epi32(999));\n        __m256i mask4_pal = _mm256_and_si256(\n            mask4, \n            _mm256_and_si256(\n                _mm256_cmpeq_epi32(v_d0, v_d3),\n                _mm256_cmpeq_epi32(v_d1, v_d2)\n            )\n        );\n        __m256i is_pal = _mm256_or_si256(\n            mask1, \n            _mm256_or_si256(mask2_pal, _mm256_or_si256(mask3_pal, mask4_pal))\n        );\n        is_pal = _mm256_and_si256(is_pal, valid_mask);\n\n        __m256i even_mask = _mm256_cmpeq_epi32(\n            _mm256_and_si256(indices, _mm256_set1_epi32(1)),\n            _mm256_setzero_si256()\n        );\n        __m256i odd_mask = _mm256_cmpeq_epi32(\n            _mm256_and_si256(indices, _mm256_set1_epi32(1)),\n            _mm256_set1_epi32(1)\n        );\n\n        __m256i even_inc = _mm256_and_si256(is_pal, even_mask);\n        __m256i odd_inc = _mm256_and_si256(is_pal, odd_mask);\n\n        even_inc = _mm256_srli_epi32(even_inc, 31);\n        odd_inc = _mm256_srli_epi32(odd_inc, 31);\n\n        v_even_count = _mm256_add_epi32(v_even_count, even_inc);\n        v_odd_count = _mm256_add_epi32(v_odd_count, odd_inc);\n    }\n\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(even_arr), v_even_count);\n    _mm256_store_si256(reinterpret_cast<__m256i*>(odd_arr), v_odd_count);\n\n    int even_total = 0, odd_total = 0;\n    for (int i = 0; i < 8; i++) {\n        even_total += even_arr[i];\n        odd_total += odd_arr[i];\n    }\n\n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    for (int i = 0; i < size; i += 8) {\n        int batch_size = std::min(8, size - i);\n        int values[8] = {0};\n        for (int j = 0; j < batch_size; j++) {\n            values[j] = n[i+j];\n        }\n        __m256i v = _mm256_loadu_si256((__m256i*)values);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        int abs_v_arr[8];\n        _mm256_storeu_si256((__m256i*)abs_v_arr, abs_v);\n        int first_digit_arr[8] = {0};\n        const int thresholds[10] = {1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000};\n        for (int j = 0; j < 8; j++) {\n            if (abs_v_arr[j] == 0) {\n                first_digit_arr[j] = 0;\n            } else {\n                for (int k = 9; k >= 0; k--) {\n                    if (abs_v_arr[j] >= thresholds[k]) {\n                        first_digit_arr[j] = abs_v_arr[j] / thresholds[k];\n                        break;\n                    }\n                }\n            }\n        }\n        int total_sum_arr[8] = {0};\n        int current[8];\n        for (int j = 0; j < 8; j++) {\n            current[j] = abs_v_arr[j];\n        }\n        for (int iter = 0; iter < 10; iter++) {\n            for (int j = 0; j < 8; j++) {\n                if (current[j] != 0) {\n                    int digit = current[j] % 10;\n                    total_sum_arr[j] += digit;\n                    current[j] = current[j] / 10;\n                }\n            }\n        }\n        for (int j = 0; j < 8; j++) {\n            if (values[j] < 0) {\n                total_sum_arr[j] -= 2 * first_digit_arr[j];\n            }\n        }\n        for (int j = 0; j < batch_size; j++) {\n            if (total_sum_arr[j] > 0) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    const size_t size = n.size();\n    \n    if (size >= 8) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i one = _mm256_set1_epi32(1);\n        \n        for (; i <= size - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n            __m256i sign_mask = _mm256_cmpgt_epi32(zero, v);\n            __m256i abs_v = _mm256_abs_epi32(v);\n            \n            int abs_arr[8];\n            int sign_arr[8];\n            _mm256_storeu_si256((__m256i*)abs_arr, abs_v);\n            _mm256_storeu_si256((__m256i*)sign_arr, sign_mask);\n            \n            int sums[8] = {0};\n            for (int j = 0; j < 8; j++) {\n                if (abs_arr[j] == INT_MIN) {\n                    sums[j] = 43;\n                } else if (abs_arr[j] != 0) {\n                    int num = abs_arr[j];\n                    int base = 1000000000;\n                    bool found_msd = false;\n                    while (base) {\n                        if (num >= base) {\n                            int digit = num / base;\n                            if (!found_msd) {\n                                sums[j] += sign_arr[j] ? -digit : digit;\n                                found_msd = true;\n                            } else {\n                                sums[j] += digit;\n                            }\n                            num %= base;\n                        }\n                        base /= 10;\n                    }\n                }\n            }\n            \n            __m256i sum_vec = _mm256_loadu_si256((__m256i*)sums);\n            __m256i cmp = _mm256_cmpgt_epi32(sum_vec, zero);\n            __m256i count_vec = _mm256_and_si256(cmp, one);\n            \n            __m128i low = _mm256_castsi256_si128(count_vec);\n            __m128i high = _mm256_extracti128_si256(count_vec, 1);\n            low = _mm_add_epi32(low, high);\n            low = _mm_hadd_epi32(low, low);\n            low = _mm_hadd_epi32(low, low);\n            count += _mm_extract_epi32(low, 0);\n        }\n    }\n    \n    for (; i < size; i++) {\n        int num = n[i];\n        if (num == 0) continue;\n        \n        int sum = 0;\n        if (num == INT_MIN) {\n            sum = 43;\n        } else {\n            int sign = (num < 0) ? -1 : 1;\n            int temp = (num < 0) ? -num : num;\n            int base = 1000000000;\n            bool found_msd = false;\n            while (base) {\n                if (temp >= base) {\n                    int digit = temp / base;\n                    if (!found_msd) {\n                        sum += sign * digit;\n                        found_msd = true;\n                    } else {\n                        sum += digit;\n                    }\n                    temp %= base;\n                }\n                base /= 10;\n            }\n        }\n        if (sum > 0) count++;\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int i = 0;\n    int size = n.size();\n    \n    if (size == 0) \n        return 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i zero = _mm256_setzero_si256();\n        __m256i sign_mask = _mm256_cmpgt_epi32(zero, v);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        \n        int abs_arr[8];\n        int sign_arr[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(abs_arr), abs_v);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(sign_arr), sign_mask);\n        \n        for (int j = 0; j < 8; j++) {\n            int num = abs_arr[j];\n            int is_negative = sign_arr[j];\n            int digit_sum = 0;\n            int first_digit = 0;\n            \n            if (num != 0) {\n                while (num) {\n                    int digit = num % 10;\n                    num /= 10;\n                    digit_sum += digit;\n                    if (num == 0) {\n                        first_digit = digit;\n                    }\n                }\n                if (is_negative) {\n                    digit_sum -= 2 * first_digit;\n                }\n            }\n            if (digit_sum > 0) {\n                count++;\n            }\n        }\n    }\n    \n    for (; i < size; i++) {\n        int num = n[i];\n        if (num == 0) {\n            continue;\n        }\n        int sign = (num < 0) ? -1 : 1;\n        int abs_num = (num < 0) ? -num : num;\n        int digit_sum = 0;\n        int first_digit = 0;\n        \n        while (abs_num) {\n            int digit = abs_num % 10;\n            abs_num /= 10;\n            digit_sum += digit;\n            if (abs_num == 0) {\n                first_digit = digit;\n            }\n        }\n        if (sign < 0) {\n            digit_sum -= 2 * first_digit;\n        }\n        if (digit_sum > 0) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    for (int i = 0; i < size; i += 8) {\n        if (i + 8 <= size) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n            __m256i abs_v = _mm256_abs_epi32(v);\n            __m256i sign = _mm256_cmpgt_epi32(_mm256_setzero_si256(), v);\n            \n            unsigned int abs_arr[8];\n            int sign_arr[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(abs_arr), abs_v);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(sign_arr), sign);\n            \n            for (int j = 0; j < 8; j++) {\n                if (abs_arr[j] == 0) {\n                    continue;\n                }\n                unsigned int a = abs_arr[j];\n                int total_sum = 0;\n                int most_significant = 0;\n                for (int k = 0; k < 10; k++) {\n                    int digit = a % 10;\n                    total_sum += digit;\n                    if (digit != 0) {\n                        most_significant = digit;\n                    }\n                    a /= 10;\n                }\n                if (sign_arr[j]) {\n                    total_sum -= 2 * most_significant;\n                }\n                if (total_sum > 0) {\n                    count++;\n                }\n            }\n        } else {\n            for (int j = i; j < size; j++) {\n                int num = n[j];\n                if (num == 0) {\n                    continue;\n                }\n                unsigned int a;\n                int is_negative = 0;\n                if (num == INT_MIN) {\n                    a = 2147483648u;\n                    is_negative = 1;\n                } else {\n                    is_negative = (num < 0);\n                    a = is_negative ? -num : num;\n                }\n                int total_sum = 0;\n                int most_significant = 0;\n                for (int k = 0; k < 10; k++) {\n                    int digit = a % 10;\n                    total_sum += digit;\n                    if (digit != 0) {\n                        most_significant = digit;\n                    }\n                    a /= 10;\n                }\n                if (is_negative) {\n                    total_sum -= 2 * most_significant;\n                }\n                if (total_sum > 0) {\n                    count++;\n                }\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <climits>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)&n[i]);\n        __m256i zero = _mm256_setzero_si256();\n        \n        __m256i gt_zero = _mm256_cmpgt_epi32(v, zero);\n        __m256i count_pos_vec = _mm256_srli_epi32(gt_zero, 31);\n        __m128i low = _mm256_castsi256_si128(count_pos_vec);\n        __m128i high = _mm256_extractf128_si256(count_pos_vec, 1);\n        low = _mm_add_epi32(low, high);\n        low = _mm_hadd_epi32(low, low);\n        low = _mm_hadd_epi32(low, low);\n        count += _mm_cvtsi128_si32(low);\n\n        __m256i lt_zero = _mm256_cmpgt_epi32(zero, v);\n        alignas(32) int arr[8];\n        alignas(32) int neg_mask[8];\n        _mm256_store_si256((__m256i*)arr, v);\n        _mm256_store_si256((__m256i*)neg_mask, lt_zero);\n\n        for (int j = 0; j < 8; j++) {\n            if (neg_mask[j]) {\n                int num = arr[j];\n                unsigned int u;\n                if (num == INT_MIN) {\n                    u = 2147483648u;\n                } else {\n                    u = (unsigned int)(-num);\n                }\n                if (u == 0) continue;\n                unsigned int divisor = 1;\n                unsigned int temp = u;\n                while (temp >= 10) {\n                    divisor *= 10;\n                    temp /= 10;\n                }\n                unsigned int first_digit = 0;\n                unsigned int rest_sum = 0;\n                temp = u;\n                bool is_first = true;\n                while (divisor != 0) {\n                    unsigned int digit = temp / divisor;\n                    temp %= divisor;\n                    divisor /= 10;\n                    if (is_first) {\n                        first_digit = digit;\n                        is_first = false;\n                    } else {\n                        rest_sum += digit;\n                    }\n                }\n                if (rest_sum > first_digit) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    for (; i < size; i++) {\n        int num = n[i];\n        if (num > 0) {\n            count++;\n        } else if (num < 0) {\n            unsigned int u;\n            if (num == INT_MIN) {\n                u = 2147483648u;\n            } else {\n                u = (unsigned int)(-num);\n            }\n            if (u == 0) continue;\n            unsigned int divisor = 1;\n            unsigned int temp = u;\n            while (temp >= 10) {\n                divisor *= 10;\n                temp /= 10;\n            }\n            unsigned int first_digit = 0;\n            unsigned int rest_sum = 0;\n            temp = u;\n            bool is_first = true;\n            while (divisor != 0) {\n                unsigned int digit = temp / divisor;\n                temp %= divisor;\n                divisor /= 10;\n                if (is_first) {\n                    first_digit = digit;\n                    is_first = false;\n                } else {\n                    rest_sum += digit;\n                }\n            }\n            if (rest_sum > first_digit) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "To solve this problem, we need to determine if a given vector of unique integers can be sorted in non-decreasing order by performing any number of right shift operations. A right shift operation moves each element one position to the right, with the last element moving to the first position. \n\n### Approach\n1. **Problem Analysis**: The key observation is that after performing any number of right shifts, the vector becomes a rotation of the original vector. Therefore, the problem reduces to checking if the original vector is a rotation of a sorted vector. \n2. **Insight**: For a rotated sorted vector, there should be at most one descent (a position where an element is greater than the next element) when considering the vector in a circular manner. \n3. **Algorithm**:\n   - **Edge Cases**: If the vector is empty or has only one element, it is trivially sorted.\n   - **Circular Check**: For vectors larger than one element, we need to check two types of pairs:\n     - **Contiguous Pairs**: Check each consecutive pair in the original vector (from index 0 to index n-2) to see if the current element is greater than the next.\n     - **Circular Pair**: Check the last element and the first element to see if the last element is greater than the first.\n   - **Count Descents**: The total number of such descents should be at most one for the vector to be sortable via rotations.\n4. **Optimization with SIMD**: To efficiently count the descents in contiguous pairs, we use AVX2 intrinsics for parallel processing:\n   - **Vector Processing**: Process the vector in blocks of 8 integers using AVX2 instructions to compare elements and count descents.\n   - **Mask Handling**: For the remaining elements (if the vector size is not a multiple of 8), use a mask to apply comparisons only to the relevant elements.\n   - **Summing Results**: Aggregate the results from vector operations to get the total count of descents.\n\n### Solution Code\n```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) \n        return true;\n    int n = arr.size();\n    if (n == 1) \n        return true;\n\n    int total_count = 0;\n\n    if (arr[n-1] > arr[0]) \n        total_count++;\n\n    const int* data = arr.data();\n    int m = n - 1;\n\n    if (m > 0) {\n        __m256i count_vec = _mm256_setzero_si256();\n        int full_blocks = m / 8;\n        int remainder = m % 8;\n\n        for (int i = 0; i < full_blocks; i++) {\n            int offset = i * 8;\n            __m256i a = _mm256_loadu_si256((const __m256i*)(data + offset));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(data + offset + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(a, b);\n            count_vec = _mm256_sub_epi32(count_vec, cmp);\n        }\n\n        if (remainder > 0) {\n            int offset = full_blocks * 8;\n            __m256i a = _mm256_loadu_si256((const __m256i*)(data + offset));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(data + offset + 1));\n            int mask_arr[8] = {0};\n            for (int i = 0; i < remainder; i++) {\n                mask_arr[i] = -1;\n            }\n            __m256i mask = _mm256_loadu_si256((const __m256i*)mask_arr);\n            __m256i cmp = _mm256_cmpgt_epi32(a, b);\n            cmp = _mm256_and_si256(cmp, mask);\n            count_vec = _mm256_sub_epi32(count_vec, cmp);\n        }\n\n        alignas(32) int count_arr[8];\n        _mm256_store_si256((__m256i*)count_arr, count_vec);\n        for (int i = 0; i < 8; i++) {\n            total_count += count_arr[i];\n        }\n    }\n\n    return (total_count <= 1);\n}\n```\n\n### Explanation\n1. **Edge Handling**: The function first checks if the vector is empty or has only one element, returning `true` immediately in these cases since such vectors are trivially sorted.\n2. **Circular Pair Check**: The last element is compared with the first element to check for a descent in the circular sense.\n3. **Contiguous Pairs Check**: The function processes the vector in blocks of 8 integers using AVX2 intrinsics:\n   - **Vector Loads**: Load consecutive elements into vectors for comparison.\n   - **Comparison**: Use `_mm256_cmpgt_epi32` to find elements greater than their next element.\n   - **Masking**: For the remaining elements (if the vector size isn't a multiple of 8), apply a mask to ensure only valid elements are considered.\n4. **Result Aggregation**: The results from vector comparisons are aggregated into a total descent count. If this count is at most one, the vector can be sorted via rotations, and the function returns `true`; otherwise, it returns `false`.\n\nThis approach efficiently leverages SIMD instructions to parallelize the descent checks, making it suitable for large vectors while ensuring correctness for all edge cases."}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) \n        return true;\n\n    int* ext = (int*)std::malloc((n + 7) * sizeof(int));\n    for (int i = 0; i < n; i++) {\n        ext[i] = arr[i];\n    }\n    for (int i = 0; i < 7; i++) {\n        ext[n + i] = arr[i];\n    }\n\n    int* base_ext = ext;\n    int* base_shifted = ext + 1;\n\n    int total_descents = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)(base_ext + i));\n        __m256i shifted_vec = _mm256_loadu_si256((__m256i*)(base_shifted + i));\n        __m256i cmp = _mm256_cmpgt_epi32(current_vec, shifted_vec);\n\n        int remaining = n - i;\n        if (remaining < 8) {\n            __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n            __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(remaining), indices);\n            cmp = _mm256_and_si256(cmp, mask);\n        }\n        int mask_bits = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        total_descents += _mm_popcnt_u32(mask_bits);\n    }\n\n    std::free(ext);\n\n    return (total_descents == 0 || total_descents == 1);\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) \n        return true;\n    int n = arr.size();\n    if (n == 1) \n        return true;\n\n    std::vector<int> next(n);\n    for (int i = 0; i < n - 1; ++i) {\n        next[i] = arr[i + 1];\n    }\n    next[n - 1] = arr[0];\n\n    int count = 0;\n    int i = 0;\n    __m256i total = _mm256_setzero_si256();\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v_arr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i v_next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&next[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(v_arr, v_next);\n        cmp = _mm256_and_si256(cmp, _mm256_set1_epi32(1));\n        total = _mm256_add_epi32(total, cmp);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), total);\n    for (int j = 0; j < 8; ++j) {\n        count += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] > next[i]) \n            ++count;\n    }\n\n    return count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n    std::vector<int> extended_arr = arr;\n    extended_arr.push_back(arr[0]);\n    int count = 0;\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i total_vec = _mm256_setzero_si256();\n        __m256i ones = _mm256_set1_epi32(1);\n        for (; i <= n - 8; i += 8) {\n            __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n            __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(extended_arr.data() + i + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(a, b);\n            __m256i masked = _mm256_and_si256(cmp, ones);\n            total_vec = _mm256_add_epi32(total_vec, masked);\n        }\n        int counts[8] __attribute__((aligned(32)));\n        _mm256_store_si256(reinterpret_cast<__m256i*>(counts), total_vec);\n        for (int j = 0; j < 8; j++) {\n            count += counts[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] > extended_arr[i+1]) {\n            count++;\n        }\n    }\n    return count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n    int consecutive_drops = 0;\n    int i = 0;\n    \n    if (n >= 9) {\n        for (; i <= n - 9; i += 8) {\n            __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n            __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(vec1, vec2);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            consecutive_drops += _mm_popcnt_u32(mask);\n        }\n    }\n    \n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            consecutive_drops++;\n        }\n    }\n    \n    int circular_drop = (arr[n - 1] > arr[0]) ? 1 : 0;\n    int total_drops = consecutive_drops + circular_drop;\n    return total_drops <= 1;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    size_t n1 = lst1.size();\n    size_t n2 = lst2.size();\n    size_t total_even = 0;\n    const int* data1 = lst1.data();\n    const int* data2 = lst2.data();\n    \n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zeros = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 8 <= n1; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(data1 + i));\n        __m256i and_res = _mm256_and_si256(a, ones);\n        __m256i mask = _mm256_cmpeq_epi32(and_res, zeros);\n        int m = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        total_even += _mm_popcnt_u32(m);\n    }\n    for (; i < n1; ++i) {\n        total_even += (data1[i] % 2 == 0);\n    }\n    \n    i = 0;\n    for (; i + 8 <= n2; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(data2 + i));\n        __m256i and_res = _mm256_and_si256(a, ones);\n        __m256i mask = _mm256_cmpeq_epi32(and_res, zeros);\n        int m = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        total_even += _mm_popcnt_u32(m);\n    }\n    for (; i < n2; ++i) {\n        total_even += (data2[i] % 2 == 0);\n    }\n    \n    if (total_even >= n1) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    const size_t n1 = lst1.size();\n    const size_t n2 = lst2.size();\n    const int* data1 = lst1.data();\n    const int* data2 = lst2.data();\n    uint64_t total_odd1 = 0;\n    uint64_t total_odd2 = 0;\n    __m256i ones = _mm256_set1_epi32(1);\n    \n    size_t i = 0;\n    if (n1 >= 8) {\n        __m256i sum1 = _mm256_setzero_si256();\n        for (; i <= n1 - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data1 + i));\n            __m256i lsb = _mm256_and_si256(v, ones);\n            sum1 = _mm256_add_epi32(sum1, lsb);\n        }\n        alignas(32) int temp1[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp1), sum1);\n        for (int j = 0; j < 8; ++j) {\n            total_odd1 += temp1[j];\n        }\n    }\n    for (; i < n1; ++i) {\n        total_odd1 += (data1[i] & 1);\n    }\n    \n    i = 0;\n    if (n2 >= 8) {\n        __m256i sum2 = _mm256_setzero_si256();\n        for (; i <= n2 - 8; i += 8) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data2 + i));\n            __m256i lsb = _mm256_and_si256(v, ones);\n            sum2 = _mm256_add_epi32(sum2, lsb);\n        }\n        alignas(32) int temp2[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp2), sum2);\n        for (int j = 0; j < 8; ++j) {\n            total_odd2 += temp2[j];\n        }\n    }\n    for (; i < n2; ++i) {\n        total_odd2 += (data2[i] & 1);\n    }\n    \n    if (total_odd1 + total_odd2 <= n2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    auto count_evens = [](const std::vector<int>& vec) -> int {\n        int cnt = 0;\n        int i = 0;\n        int size = vec.size();\n        if (size == 0) \n            return 0;\n        __m256i count_vec = _mm256_setzero_si256();\n        for (; i + 8 <= size; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(vec.data() + i));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i lsb = _mm256_and_si256(data, ones);\n            __m256i cmp = _mm256_cmpeq_epi32(lsb, _mm256_setzero_si256());\n            __m256i even_flags = _mm256_and_si256(cmp, ones);\n            count_vec = _mm256_add_epi32(count_vec, even_flags);\n        }\n        __m128i low = _mm256_castsi256_si128(count_vec);\n        __m128i high = _mm256_extracti128_si256(count_vec, 1);\n        low = _mm_add_epi32(low, high);\n        low = _mm_hadd_epi32(low, low);\n        low = _mm_hadd_epi32(low, low);\n        cnt = _mm_extract_epi32(low, 0);\n        for (; i < size; i++) {\n            if ((vec[i] & 1) == 0) {\n                cnt++;\n            }\n        }\n        return cnt;\n    };\n\n    int total_even = count_evens(lst1) + count_evens(lst2);\n    if (total_even >= static_cast<int>(lst1.size())) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    auto count_even = [](const std::vector<int>& arr) -> int {\n        int cnt = 0;\n        int n = arr.size();\n        int i = 0;\n        if (n >= 8) {\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i zeros = _mm256_setzero_si256();\n            for (; i <= n-8; i += 8) {\n                __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n                __m256i lsb = _mm256_and_si256(data, ones);\n                __m256i mask = _mm256_cmpeq_epi32(lsb, zeros);\n                __m256i even_count_vec = _mm256_and_si256(mask, ones);\n                __m128i low128 = _mm256_castsi256_si128(even_count_vec);\n                __m128i high128 = _mm256_extracti128_si256(even_count_vec, 1);\n                low128 = _mm_add_epi32(low128, high128);\n                __m128i hadd1 = _mm_hadd_epi32(low128, low128);\n                __m128i hadd2 = _mm_hadd_epi32(hadd1, hadd1);\n                cnt += _mm_extract_epi32(hadd2, 0);\n            }\n        }\n        for (; i < n; i++) {\n            if ((arr[i] & 1) == 0) {\n                cnt++;\n            }\n        }\n        return cnt;\n    };\n\n    int total_even = count_even(lst1) + count_even(lst2);\n    if (total_even >= static_cast<int>(lst1.size())) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n1 = lst1.size();\n    size_t n2 = lst2.size();\n    int count_odds_lst1 = 0;\n    int count_evens_lst2 = 0;\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 8 <= n1; i += 8) {\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i and_result = _mm256_and_si256(v1, ones);\n        __m256i cmp = _mm256_cmpeq_epi32(and_result, ones);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        count_odds_lst1 += __builtin_popcount(mask);\n    }\n    for (; i < n1; ++i) {\n        count_odds_lst1 += (lst1[i] & 1);\n    }\n\n    i = 0;\n    for (; i + 8 <= n2; i += 8) {\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i and_result = _mm256_and_si256(v2, ones);\n        __m256i cmp = _mm256_cmpeq_epi32(and_result, zeros);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        count_evens_lst2 += __builtin_popcount(mask);\n    }\n    for (; i < n2; ++i) {\n        count_evens_lst2 += !(lst2[i] & 1);\n    }\n\n    return (count_evens_lst2 >= count_odds_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <cstddef>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const std::string& s : lst) {\n        size_t total_count = 0;\n        const size_t len = s.size();\n        const char* data = s.data();\n\n        if (len > 0) {\n            __m256i zero = _mm256_set1_epi8('0');\n            __m256i one = _mm256_set1_epi8(1);\n            size_t i = 0;\n\n            for (; i + 31 < len; i += 32) {\n                __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n                __m256i digits = _mm256_sub_epi8(chunk, zero);\n                __m256i odd_flags = _mm256_and_si256(digits, one);\n                __m256i cmp = _mm256_cmpeq_epi8(odd_flags, one);\n                int mask = _mm256_movemask_epi8(cmp);\n                total_count += __builtin_popcount(mask);\n            }\n\n            for (; i < len; ++i) {\n                int digit = data[i] - '0';\n                if (digit & 1) {\n                    ++total_count;\n                }\n            }\n        }\n\n        std::string count_str = std::to_string(total_count);\n        std::string template_str = \"the number of odd elements in the string i of the input.\";\n        std::string new_str;\n        for (char c : template_str) {\n            if (c == 'i') {\n                new_str += count_str;\n            } else {\n                new_str += c;\n            }\n        }\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const std::string template_str = \"the number of odd elements in the string i of the input.\";\n    std::vector<std::string> result_vec;\n    for (const auto& s : lst) {\n        uint64_t total_count = 0;\n        size_t len = s.length();\n        const char* p = s.c_str();\n        size_t i = 0;\n        __m256i total_vec = _mm256_setzero_si256();\n        for (; i + 31 < len; i += 32) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(p + i));\n            __m256i mask = _mm256_and_si256(v, _mm256_set1_epi8(1));\n            __m256i sum_part = _mm256_sad_epu8(mask, _mm256_setzero_si256());\n            total_vec = _mm256_add_epi64(total_vec, sum_part);\n        }\n        alignas(32) uint64_t total_arr[4];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(total_arr), total_vec);\n        total_count = total_arr[0] + total_arr[1] + total_arr[2] + total_arr[3];\n        for (; i < len; ++i) {\n            if (s[i] & 1) {\n                total_count++;\n            }\n        }\n        std::string count_str = std::to_string(total_count);\n        std::string result = template_str;\n        size_t pos = 0;\n        while ((pos = result.find('i', pos)) != std::string::npos) {\n            result.replace(pos, 1, count_str);\n            pos += count_str.length();\n        }\n        result_vec.push_back(result);\n    }\n    return result_vec;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <cstdint>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    const std::string base_template = \"the number of odd elements in the string i of the input.\";\n    std::vector<std::string> res;\n    res.reserve(lst.size());\n    for (const auto& s : lst) {\n        size_t len = s.length();\n        size_t i = 0;\n        uint64_t total_count = 0;\n        if (len >= 32) {\n            __m256i ones = _mm256_set1_epi8(1);\n            __m256i zeros = _mm256_setzero_si256();\n            for (; i <= len - 32; i += 32) {\n                __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n                __m256i and_result = _mm256_and_si256(chunk, ones);\n                __m256i sum_part = _mm256_sad_epu8(and_result, zeros);\n                uint64_t sums[4];\n                _mm256_storeu_si256(reinterpret_cast<__m256i*>(sums), sum_part);\n                total_count += sums[0] + sums[1] + sums[2] + sums[3];\n            }\n        }\n        for (; i < len; ++i) {\n            if (s[i] & 1) {\n                total_count++;\n            }\n        }\n        std::string count_str = std::to_string(total_count);\n        std::string out_str;\n        out_str.reserve(base_template.length() + count_str.length() * 4);\n        for (char c : base_template) {\n            if (c == 'i') {\n                out_str += count_str;\n            } else {\n                out_str.push_back(c);\n            }\n        }\n        res.push_back(std::move(out_str));\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstddef>\n\nstatic int count_odd_digits(const std::string& s) {\n    size_t len = s.size();\n    size_t i = 0;\n    int total_odd = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i lsb = _mm256_and_si256(chunk, _mm256_set1_epi8(1));\n            __m256i mask_vec = _mm256_cmpeq_epi8(lsb, _mm256_set1_epi8(1));\n            int mask = _mm256_movemask_epi8(mask_vec);\n            total_odd += __builtin_popcount(mask);\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] & 1) {\n            total_odd++;\n        }\n    }\n\n    return total_odd;\n}\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string>& lst) {\n    std::vector<std::string> result;\n    const std::string base = \"the number of odd elements in the string i of the input.\";\n\n    for (const std::string& s : lst) {\n        int count = count_odd_digits(s);\n        std::string count_str = std::to_string(count);\n        std::string new_string;\n        for (char c : base) {\n            if (c == 'i') {\n                new_string += count_str;\n            } else {\n                new_string.push_back(c);\n            }\n        }\n        result.push_back(new_string);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <nmmintrin.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const std::string fixed = \"the number of odd elements in the string i of the input.\";\n\n    for (const std::string& s : lst) {\n        size_t len = s.size();\n        const char* ptr = s.data();\n        unsigned int count = 0;\n        size_t i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr + i));\n                __m256i mask = _mm256_set1_epi8(1);\n                __m256i lsb = _mm256_and_si256(chunk, mask);\n                __m256i cmp = _mm256_cmpeq_epi8(lsb, mask);\n                int mask32 = _mm256_movemask_epi8(cmp);\n                count += _mm_popcnt_u32(static_cast<unsigned int>(mask32));\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (ptr[i] & 1) {\n                count++;\n            }\n        }\n\n        std::string count_str = std::to_string(count);\n        std::string output;\n        for (char c : fixed) {\n            if (c == 'i') {\n                output += count_str;\n            } else {\n                output.push_back(c);\n            }\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) {\n        return 0;\n    }\n    std::vector<int64_t> P(n + 1);\n    P[0] = 0;\n    size_t i = 0;\n    int64_t carry = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i shifted1 = _mm256_slli_si256(v, 8);\n        __m256i sum1 = _mm256_add_epi64(v, shifted1);\n        __m128i low_lane = _mm256_extractf128_si256(sum1, 0);\n        __m128i high_lane = _mm256_extractf128_si256(sum1, 1);\n        __m128i low_lane_high = _mm_unpackhi_epi64(low_lane, low_lane);\n        __m128i new_high = _mm_add_epi64(high_lane, low_lane_high);\n        __m256i scan = _mm256_insertf128_si256(_mm256_castsi128_si256(low_lane), new_high, 1);\n        __m256i carry_vec = _mm256_set1_epi64x(carry);\n        scan = _mm256_add_epi64(scan, carry_vec);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&P[i + 1]), scan);\n        carry = _mm256_extract_epi64(scan, 3);\n    }\n\n    for (; i < n; ++i) {\n        carry += nums[i];\n        P[i + 1] = carry;\n    }\n\n    int64_t min_diff = LLONG_MAX;\n    int64_t max_prefix = P[0];\n    for (size_t j = 1; j <= n; ++j) {\n        int64_t candidate = P[j] - max_prefix;\n        if (candidate < min_diff) {\n            min_diff = candidate;\n        }\n        if (P[j] > max_prefix) {\n            max_prefix = P[j];\n        }\n    }\n\n    return min_diff;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstruct State {\n    int64_t total;\n    int64_t min_prefix;\n    int64_t min_suffix;\n    int64_t min_total;\n};\n\nState combine(State left, State right) {\n    State res;\n    res.total = left.total + right.total;\n    res.min_prefix = std::min(left.min_prefix, left.total + right.min_prefix);\n    res.min_suffix = std::min(right.min_suffix, right.total + left.min_suffix);\n    res.min_total = std::min({left.min_total, right.min_total, left.min_suffix + right.min_prefix});\n    return res;\n}\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    int num_chunks = (n + 3) / 4;\n    std::vector<State> states(num_chunks);\n\n    for (int i = 0; i < num_chunks; i++) {\n        int start = i * 4;\n        int end = std::min(start + 4, n);\n        int count = end - start;\n\n        if (count == 4) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[start]));\n            __m128i low = _mm256_castsi256_si128(chunk);\n            __m128i high = _mm256_extracti128_si256(chunk, 1);\n            __m128i sum128 = _mm_add_epi64(low, high);\n            __m128i sum64 = _mm_add_epi64(sum128, _mm_unpackhi_epi64(sum128, sum128));\n            int64_t total = _mm_extract_epi64(sum64, 0);\n\n            int64_t arr[4];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(arr), chunk);\n\n            int64_t min_prefix = LLONG_MAX;\n            int64_t prefix = 0;\n            for (int j = 0; j < 4; j++) {\n                prefix += arr[j];\n                if (prefix < min_prefix) min_prefix = prefix;\n            }\n\n            int64_t min_suffix = LLONG_MAX;\n            int64_t suffix = 0;\n            for (int j = 3; j >= 0; j--) {\n                suffix += arr[j];\n                if (suffix < min_suffix) min_suffix = suffix;\n            }\n\n            int64_t min_total = LLONG_MAX;\n            for (int j = 0; j < 4; j++) {\n                int64_t sum = 0;\n                for (int k = j; k < 4; k++) {\n                    sum += arr[k];\n                    if (sum < min_total) min_total = sum;\n                }\n            }\n\n            states[i] = {total, min_prefix, min_suffix, min_total};\n        } else {\n            int64_t total = 0;\n            int64_t min_prefix = LLONG_MAX;\n            int64_t prefix = 0;\n            for (int j = 0; j < count; j++) {\n                total += nums[start + j];\n                prefix += nums[start + j];\n                if (prefix < min_prefix) min_prefix = prefix;\n            }\n\n            int64_t min_suffix = LLONG_MAX;\n            int64_t suffix = 0;\n            for (int j = count - 1; j >= 0; j--) {\n                suffix += nums[start + j];\n                if (suffix < min_suffix) min_suffix = suffix;\n            }\n\n            int64_t min_total = LLONG_MAX;\n            for (int j = 0; j < count; j++) {\n                int64_t sum = 0;\n                for (int k = j; k < count; k++) {\n                    sum += nums[start + k];\n                    if (sum < min_total) min_total = sum;\n                }\n            }\n            states[i] = {total, min_prefix, min_suffix, min_total};\n        }\n    }\n\n    while (num_chunks > 1) {\n        int new_num_chunks = (num_chunks + 1) / 2;\n        for (int i = 0; i < new_num_chunks; i++) {\n            if (2 * i + 1 < num_chunks) {\n                states[i] = combine(states[2 * i], states[2 * i + 1]);\n            } else {\n                states[i] = states[2 * i];\n            }\n        }\n        num_chunks = new_num_chunks;\n    }\n\n    return states[0].min_total;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n#include <algorithm>\n\nstatic inline __m256i inclusive_scan_epi64(__m256i v) {\n    __m256i shifted = _mm256_slli_si256(v, 8);\n    __m256i scan = _mm256_add_epi64(v, shifted);\n    __m128i lane0 = _mm256_extracti128_si256(scan, 0);\n    __m128i high_lane0 = _mm_unpackhi_epi64(lane0, lane0);\n    __m256i add_vec = _mm256_insertf128_si256(_mm256_castsi128_si256(_mm_setzero_si128()), high_lane0, 1);\n    scan = _mm256_add_epi64(scan, add_vec);\n    return scan;\n}\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t min_sum = LLONG_MAX;\n    int64_t current_prefix = 0;\n    int64_t current_max_prefix = 0;\n    size_t i = 0;\n    size_t n = nums.size();\n\n    for (; i + 4 <= n; i += 4) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i scan = inclusive_scan_epi64(v);\n        __m256i s = _mm256_set1_epi64x(current_prefix);\n        __m256i abs_prefix_vec = _mm256_add_epi64(scan, s);\n        int64_t abs_prefix_arr[4];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(abs_prefix_arr), abs_prefix_vec);\n\n        for (int j = 0; j < 4; ++j) {\n            int64_t candidate = abs_prefix_arr[j] - current_max_prefix;\n            if (candidate < min_sum) {\n                min_sum = candidate;\n            }\n            if (abs_prefix_arr[j] > current_max_prefix) {\n                current_max_prefix = abs_prefix_arr[j];\n            }\n        }\n        current_prefix = abs_prefix_arr[3];\n    }\n\n    for (; i < n; ++i) {\n        current_prefix += nums[i];\n        int64_t candidate = current_prefix - current_max_prefix;\n        if (candidate < min_sum) {\n            min_sum = candidate;\n        }\n        if (current_prefix > current_max_prefix) {\n            current_max_prefix = current_prefix;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstruct State {\n    int64_t min_prefix;\n    int64_t min_suffix;\n    int64_t min_total;\n    int64_t total;\n};\n\nstatic State combine(const State& left, const State& right) {\n    State res;\n    res.total = left.total + right.total;\n    res.min_prefix = std::min(left.min_prefix, left.total + right.min_prefix);\n    res.min_suffix = std::min(right.min_suffix, right.total + left.min_suffix);\n    res.min_total = std::min({left.min_total, right.min_total, left.min_suffix + right.min_prefix});\n    return res;\n}\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n    size_t n = nums.size();\n    size_t num_segments = (n + 3) / 4;\n    std::vector<State> states(num_segments);\n\n    for (size_t i = 0; i < num_segments; ++i) {\n        size_t start = i * 4;\n        size_t end = std::min(start + 4, n);\n        size_t count = end - start;\n\n        if (count == 4) {\n            __m256i v = _mm256_loadu_si256((const __m256i*)&nums[start]);\n            __m256i p0 = _mm256_set1_epi64x(0);\n            __m256i p = v;\n            __m256i shift1 = _mm256_permute4x64_epi64(p, _MM_SHUFFLE(2, 1, 0, 3));\n            p = _mm256_add_epi64(p, shift1);\n            __m256i shift2 = _mm256_permute4x64_epi64(shift1, _MM_SHUFFLE(2, 1, 0, 3));\n            p = _mm256_add_epi64(p, shift2);\n            __m256i shift3 = _mm256_permute4x64_epi64(shift2, _MM_SHUFFLE(2, 1, 0, 3));\n            p = _mm256_add_epi64(p, shift3);\n\n            int64_t arr[4];\n            _mm256_storeu_si256((__m256i*)arr, p);\n            states[i].min_prefix = std::min({arr[0], arr[1], arr[2], arr[3]});\n            states[i].min_suffix = std::min({arr[3], arr[3]-arr[2]+arr[3], arr[3]-arr[1]+arr[3], arr[3]-arr[0]+arr[3]});\n            states[i].total = arr[3];\n\n            __m256i vmin = _mm256_min_epi64(v, _mm256_set1_epi64x(LLONG_MAX));\n            vmin = _mm256_min_epi64(vmin, _mm256_permute4x64_epi64(vmin, _MM_SHUFFLE(2, 1, 0, 3)));\n            vmin = _mm256_min_epi64(vmin, _mm256_permute4x64_epi64(vmin, _MM_SHUFFLE(1, 0, 3, 2)));\n            int64_t min_vals[4];\n            _mm256_storeu_si256((__m256i*)min_vals, vmin);\n            states[i].min_total = min_vals[0];\n        } else {\n            int64_t min_prefix = LLONG_MAX, min_suffix = LLONG_MAX, min_total = LLONG_MAX, total = 0;\n            for (size_t j = start; j < end; ++j) {\n                total += nums[j];\n                min_prefix = std::min(min_prefix, total);\n            }\n            int64_t suffix = 0;\n            for (int j = end - 1; j >= static_cast<int>(start); --j) {\n                suffix += nums[j];\n                min_suffix = std::min(min_suffix, suffix);\n            }\n            int64_t cur = 0;\n            min_total = LLONG_MAX;\n            for (size_t j = start; j < end; ++j) {\n                cur = std::min(nums[j], cur + nums[j]);\n                min_total = std::min(min_total, cur);\n            }\n            states[i] = {min_prefix, min_suffix, min_total, total};\n        }\n    }\n\n    State res = states[0];\n    for (size_t i = 1; i < num_segments; ++i) {\n        res = combine(res, states[i]);\n    }\n    return res.min_total;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n    std::vector<int64_t> prefix(n + 1);\n    prefix[0] = 0;\n    for (int i = 0; i < n; ++i) {\n        prefix[i + 1] = prefix[i] + nums[i];\n    }\n\n    int64_t current_max_prefix = prefix[0];\n    int64_t global_min = INT64_MAX;\n    int i = 1;\n\n    for (; i + 3 <= n; i += 4) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&prefix[i]));\n        int64_t p0 = _mm256_extract_epi64(v, 0);\n        int64_t p1 = _mm256_extract_epi64(v, 1);\n        int64_t p2 = _mm256_extract_epi64(v, 2);\n        int64_t p3 = _mm256_extract_epi64(v, 3);\n\n        int64_t candidate0 = p0 - current_max_prefix;\n        if (candidate0 < global_min) global_min = candidate0;\n        if (p0 > current_max_prefix) current_max_prefix = p0;\n\n        int64_t candidate1 = p1 - current_max_prefix;\n        if (candidate1 < global_min) global_min = candidate1;\n        if (p1 > current_max_prefix) current_max_prefix = p1;\n\n        int64_t candidate2 = p2 - current_max_prefix;\n        if (candidate2 < global_min) global_min = candidate2;\n        if (p2 > current_max_prefix) current_max_prefix = p2;\n\n        int64_t candidate3 = p3 - current_max_prefix;\n        if (candidate3 < global_min) global_min = candidate3;\n        if (p3 > current_max_prefix) current_max_prefix = p3;\n    }\n\n    for (; i <= n; ++i) {\n        int64_t candidate = prefix[i] - current_max_prefix;\n        if (candidate < global_min) global_min = candidate;\n        if (prefix[i] > current_max_prefix) current_max_prefix = prefix[i];\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    long long total_ops = 0;\n    int R = grid.size();\n    if (R == 0) \n        return 0;\n    int C = grid[0].size();\n\n    for (int i = 0; i < R; i++) {\n        const std::vector<int>& row = grid[i];\n        const int* ptr = row.data();\n        int count = 0;\n        int j = 0;\n\n        if (C >= 8) {\n            __m256i sum8 = _mm256_setzero_si256();\n            for (; j <= C - 8; j += 8) {\n                __m256i v = _mm256_loadu_si256((const __m256i*)(ptr + j));\n                sum8 = _mm256_add_epi32(sum8, v);\n            }\n            __m128i low128 = _mm256_castsi256_si128(sum8);\n            __m128i high128 = _mm256_extractf128_si256(sum8, 1);\n            __m128i sum128 = _mm_add_epi32(low128, high128);\n            sum128 = _mm_hadd_epi32(sum128, sum128);\n            sum128 = _mm_hadd_epi32(sum128, sum128);\n            count += _mm_extract_epi32(sum128, 0);\n        }\n\n        for (; j < C; j++) {\n            count += ptr[j];\n        }\n\n        if (count > 0) {\n            total_ops += (count + capacity - 1) / capacity;\n        }\n    }\n\n    return static_cast<int>(total_ops);\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty()) {\n        return 0;\n    }\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n    int total_bucket_lowers = 0;\n\n    for (int i = 0; i < num_rows; i++) {\n        const std::vector<int>& row = grid[i];\n        int row_sum = 0;\n        int j = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; j <= num_cols - 8; j += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[j]));\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        __m128i low128 = _mm256_castsi256_si128(sum_vec);\n        __m128i high128 = _mm256_extracti128_si256(sum_vec, 1);\n        __m128i sum128 = _mm_add_epi32(low128, high128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        row_sum += _mm_cvtsi128_si32(sum128);\n\n        for (; j < num_cols; j++) {\n            row_sum += row[j];\n        }\n\n        total_bucket_lowers += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total_bucket_lowers;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    long long total_lowers = 0;\n    int num_rows = grid.size();\n\n    for (int i = 0; i < num_rows; i++) {\n        const std::vector<int>& row = grid[i];\n        int n = row.size();\n        if (n == 0) {\n            continue;\n        }\n        const int* data = row.data();\n        int j = 0;\n        __m256i sum8 = _mm256_setzero_si256();\n\n        for (; j <= n - 8; j += 8) {\n            __m256i v = _mm256_loadu_si256((__m256i const*)(data + j));\n            sum8 = _mm256_add_epi32(sum8, v);\n        }\n\n        __m128i low128 = _mm256_castsi256_si128(sum8);\n        __m128i high128 = _mm256_extracti128_si256(sum8, 1);\n        __m128i sum128 = _mm_add_epi32(low128, high128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        int vector_sum = _mm_extract_epi32(sum128, 0);\n\n        int remainder_sum = 0;\n        for (; j < n; j++) {\n            remainder_sum += data[j];\n        }\n\n        int sum_row = vector_sum + remainder_sum;\n        total_lowers += (static_cast<long long>(sum_row) + capacity - 1) / capacity;\n    }\n\n    return static_cast<int>(total_lowers);\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_ops = 0;\n    for (const auto& row : grid) {\n        int n = row.size();\n        const int* ptr = row.data();\n        int count = 0;\n        int i = 0;\n\n        if (n >= 8) {\n            __m256i sum8 = _mm256_setzero_si256();\n            for (; i <= n - 8; i += 8) {\n                __m256i chunk = _mm256_loadu_si256((const __m256i*)(ptr + i));\n                sum8 = _mm256_add_epi32(sum8, chunk);\n            }\n\n            __m128i low128 = _mm256_castsi256_si128(sum8);\n            __m128i high128 = _mm256_extracti128_si256(sum8, 1);\n            __m128i sum128 = _mm_add_epi32(low128, high128);\n            __m128i shuf128 = _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1));\n            __m128i sum_temp = _mm_add_epi32(sum128, shuf128);\n            __m128i shuf128_2 = _mm_shuffle_epi32(sum_temp, _MM_SHUFFLE(1, 0, 3, 2));\n            __m128i total128 = _mm_add_epi32(sum_temp, shuf128_2);\n            count += _mm_cvtsi128_si32(total128);\n        }\n\n        for (; i < n; ++i) {\n            count += ptr[i];\n        }\n\n        total_ops += (count + capacity - 1) / capacity;\n    }\n    return total_ops;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    long long total_lowers = 0;\n    int n_rows = grid.size();\n    if (n_rows == 0) \n        return 0;\n    int n_cols = grid[0].size();\n\n    for (int r = 0; r < n_rows; ++r) {\n        const std::vector<int>& row = grid[r];\n        int i = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n        while (i + 8 <= n_cols) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum_vec = _mm256_add_epi32(sum_vec, v);\n            i += 8;\n        }\n        int row_sum = 0;\n        alignas(32) int temp[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            row_sum += temp[k];\n        }\n        for (; i < n_cols; ++i) {\n            row_sum += row[i];\n        }\n        total_lowers += (static_cast<long long>(row_sum) + capacity - 1) / capacity;\n    }\n    return static_cast<int>(total_lowers);\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t total = 0;\n    size_t n = lst.size();\n    if (n == 0) \n        return 0;\n\n    __m256i ones16 = _mm256_set1_epi16(1);\n    __m256i mask_even = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    size_t i = 0;\n\n    for (; i + 16 <= n; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(&lst[i]));\n        __m256i odd_mask = _mm256_and_si256(v, ones16);\n        __m256i condition = _mm256_and_si256(odd_mask, mask_even);\n        __m256i cmp_result = _mm256_cmpeq_epi16(condition, ones16);\n        __m256i selected = _mm256_and_si256(v, cmp_result);\n\n        int16_t temp[16];\n        _mm256_storeu_si256((__m256i*)temp, selected);\n\n        for (int j = 0; j < 16; ++j) {\n            total += temp[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t total = 0;\n    int n = lst.size();\n    int i = 0;\n\n    if (n == 0) return 0;\n\n    __m256i even_mask = _mm256_setr_epi16(\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0,\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0\n    );\n    __m256i one_vec = _mm256_set1_epi16(1);\n    __m256i sum32 = _mm256_setzero_si256();\n\n    for (; i + 15 < n; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        __m256i odd_test = _mm256_and_si256(data, one_vec);\n        __m256i condition = _mm256_cmpeq_epi16(odd_test, one_vec);\n        condition = _mm256_and_si256(condition, even_mask);\n        __m256i selected = _mm256_and_si256(data, condition);\n        __m256i products = _mm256_madd_epi16(selected, one_vec);\n        sum32 = _mm256_add_epi32(sum32, products);\n    }\n\n    int32_t sums[8];\n    _mm256_storeu_si256((__m256i*)sums, sum32);\n    for (int j = 0; j < 8; j++) {\n        total += (int64_t)sums[j];\n    }\n\n    for (; i < n; i += 2) {\n        if (lst[i] % 2 != 0) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    if (size == 0) {\n        return 0;\n    }\n    int64_t total = 0;\n    size_t i = 0;\n\n    const __m256i base_even_mask = _mm256_setr_epi16(\n        -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0\n    );\n    const __m256i ones16 = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        __m256i odd_flag = _mm256_and_si256(data, ones16);\n        __m256i odd_mask = _mm256_cmpeq_epi16(odd_flag, ones16);\n        __m256i combined_mask = _mm256_and_si256(base_even_mask, odd_mask);\n        __m256i selected = _mm256_and_si256(data, combined_mask);\n        __m256i sum32 = _mm256_madd_epi16(selected, ones16);\n\n        __m128i low128 = _mm256_castsi256_si128(sum32);\n        __m128i high128 = _mm256_extracti128_si256(sum32, 1);\n        __m128i sum128 = _mm_add_epi32(low128, high128);\n\n        int32_t s0 = _mm_extract_epi32(sum128, 0);\n        int32_t s1 = _mm_extract_epi32(sum128, 1);\n        int32_t s2 = _mm_extract_epi32(sum128, 2);\n        int32_t s3 = _mm_extract_epi32(sum128, 3);\n        total += static_cast<int64_t>(s0) + s1 + s2 + s3;\n    }\n\n    for (size_t j = 0; j < size - i; j += 2) {\n        int16_t val = lst[i + j];\n        if (val & 1) {\n            total += val;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t total = 0;\n    int n = lst.size();\n    int i = 0;\n\n    if (n == 0) {\n        return 0;\n    }\n\n    const __m256i even_mask = _mm256_set_epi16(\n        0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF,\n        0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF\n    );\n    __m256i ones = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= n; i += 16) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(lst.data() + i));\n        __m256i even_masked = _mm256_and_si256(vec, even_mask);\n        __m256i and_result = _mm256_and_si256(even_masked, ones);\n        __m256i odd_mask = _mm256_cmpeq_epi16(and_result, ones);\n        __m256i candidate = _mm256_and_si256(even_masked, odd_mask);\n\n        __m256i ones16 = _mm256_set1_epi16(1);\n        __m256i product = _mm256_madd_epi16(candidate, ones16);\n\n        __m128i low = _mm256_castsi256_si128(product);\n        __m128i high = _mm256_extracti128_si256(product, 1);\n        __m128i hadd1 = _mm_hadd_epi32(low, high);\n        __m128i hadd2 = _mm_hadd_epi32(hadd1, hadd1);\n        __m128i shifted = _mm_srli_si128(hadd2, 8);\n        __m128i sum32 = _mm_add_epi32(hadd2, shifted);\n        int32_t s = _mm_extract_epi32(sum32, 0);\n        total += s;\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    if (lst.empty()) return 0;\n    \n    int64_t total_sum = 0;\n    const int16_t* p = lst.data();\n    size_t i = 0;\n    size_t n = lst.size();\n\n    __m256i even_mask_even_base = _mm256_set_epi16(\n        0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF,\n        0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF\n    );\n    __m256i even_mask_odd_base = _mm256_set_epi16(\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0,\n        0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0\n    );\n    __m256i one = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= n; i += 16) {\n        __m256i mask = (i % 2 == 0) ? even_mask_even_base : even_mask_odd_base;\n        __m256i data = _mm256_loadu_si256((const __m256i*)(p + i));\n        __m256i odd_vector = _mm256_and_si256(data, one);\n        __m256i condition = _mm256_and_si256(mask, odd_vector);\n        condition = _mm256_cmpeq_epi16(condition, one);\n        __m256i masked_data = _mm256_and_si256(data, condition);\n        __m256i sum32_vec = _mm256_madd_epi16(masked_data, one);\n        __m128i sum32_low = _mm256_castsi256_si128(sum32_vec);\n        __m128i sum32_high = _mm256_extracti128_si256(sum32_vec, 1);\n        __m128i sum32 = _mm_add_epi32(sum32_low, sum32_high);\n        alignas(16) int32_t temp[4];\n        _mm_store_si128((__m128i*)temp, sum32);\n        total_sum += static_cast<int64_t>(temp[0]) + temp[1] + temp[2] + temp[3];\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 0 && (p[i] & 1)) {\n            total_sum += p[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n\n    for (; i + 8 <= k; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(arr.data() + i));\n        __m256i abs_data = _mm256_abs_epi32(data);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_data);\n        __m256i selected = _mm256_and_si256(data, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, selected);\n    }\n\n    int tail_sum = 0;\n    for (; i < k; i++) {\n        int val = arr[i];\n        if (val >= -99 && val <= 99) {\n            tail_sum += val;\n        }\n    }\n\n    alignas(32) int simd_sum[8];\n    _mm256_store_si256((__m256i*)simd_sum, sum_vec);\n    int total = tail_sum;\n    for (int j = 0; j < 8; j++) {\n        total += simd_sum[j];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int i = 0;\n    __m256i sum_vector = _mm256_setzero_si256();\n    if (k >= 8) {\n        const int last_index = k - 8;\n        for (i = 0; i <= last_index; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n            __m256i mask1 = _mm256_cmpgt_epi32(data, _mm256_set1_epi32(-100));\n            __m256i mask2 = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), data);\n            __m256i condition_mask = _mm256_and_si256(mask1, mask2);\n            __m256i selected = _mm256_and_si256(data, condition_mask);\n            sum_vector = _mm256_add_epi32(sum_vector, selected);\n        }\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(sum_vector);\n    __m128i high128 = _mm256_extracti128_si256(sum_vector, 1);\n    low128 = _mm_add_epi32(low128, high128);\n    low128 = _mm_hadd_epi32(low128, low128);\n    low128 = _mm_hadd_epi32(low128, low128);\n    int total = _mm_cvtsi128_si32(low128);\n\n    for (; i < k; i++) {\n        int x = arr[i];\n        if (x >= -99 && x <= 99) {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int total = 0;\n    int i = 0;\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for (; i <= k - 8; i += 8) {\n        __m256i v_data = _mm256_loadu_si256((__m256i const*)(arr.data() + i));\n        __m256i v_low_bound = _mm256_set1_epi32(-100);\n        __m256i v_high_bound = _mm256_set1_epi32(100);\n        __m256i mask_low = _mm256_cmpgt_epi32(v_data, v_low_bound);\n        __m256i mask_high = _mm256_cmpgt_epi32(v_high_bound, v_data);\n        __m256i mask = _mm256_and_si256(mask_low, mask_high);\n        __m256i selected = _mm256_and_si256(v_data, mask);\n        v_sum = _mm256_add_epi32(v_sum, selected);\n    }\n\n    __m128i v_low = _mm256_castsi256_si128(v_sum);\n    __m128i v_high = _mm256_extractf128_si256(v_sum, 1);\n    __m128i v_sum128 = _mm_add_epi32(v_low, v_high);\n    v_sum128 = _mm_hadd_epi32(v_sum128, v_sum128);\n    v_sum128 = _mm_hadd_epi32(v_sum128, v_sum128);\n    total = _mm_extract_epi32(v_sum128, 0);\n\n    for (; i < k; i++) {\n        int x = arr[i];\n        if (x >= -99 && x <= 99) {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    if (k <= 0) {\n        return 0;\n    }\n    const int* data = arr.data();\n    int i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i <= k - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(data + i));\n        __m256i mask_low = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(-100));\n        __m256i mask_high = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), v);\n        __m256i mask = _mm256_and_si256(mask_low, mask_high);\n        __m256i masked_v = _mm256_and_si256(v, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_v);\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(sum_vec);\n    __m128i high128 = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi32(low128, high128);\n    __m128i hadd1 = _mm_hadd_epi32(sum128, sum128);\n    __m128i hadd2 = _mm_hadd_epi32(hadd1, hadd1);\n    int total = _mm_extract_epi32(hadd2, 0);\n\n    for (; i < k; i++) {\n        int val = data[i];\n        if (val >= -99 && val <= 99) {\n            total += val;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i total_vec = _mm256_setzero_si256();\n    __m256i neg_100 = _mm256_set1_epi32(-100);\n    __m256i pos_100 = _mm256_set1_epi32(100);\n    int i = 0;\n\n    for (; i <= k - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(arr.data() + i));\n        __m256i cmp_low = _mm256_cmpgt_epi32(v, neg_100);\n        __m256i cmp_high = _mm256_cmpgt_epi32(pos_100, v);\n        __m256i cmp = _mm256_and_si256(cmp_low, cmp_high);\n        __m256i selected = _mm256_and_si256(v, cmp);\n        total_vec = _mm256_add_epi32(total_vec, selected);\n    }\n\n    int remaining = k - i;\n    if (remaining > 0) {\n        __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i rem_vec = _mm256_set1_epi32(remaining);\n        __m256i load_mask = _mm256_cmpgt_epi32(rem_vec, indices);\n        __m256i v = _mm256_maskload_epi32(arr.data() + i, load_mask);\n        __m256i cmp_low = _mm256_cmpgt_epi32(v, neg_100);\n        __m256i cmp_high = _mm256_cmpgt_epi32(pos_100, v);\n        __m256i cmp = _mm256_and_si256(cmp_low, cmp_high);\n        __m256i selected = _mm256_and_si256(v, cmp);\n        total_vec = _mm256_add_epi32(total_vec, selected);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, total_vec);\n    int total = 0;\n    for (int j = 0; j < 8; j++) {\n        total += temp[j];\n    }\n    return total;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n    const int* data = lst.data();\n    size_t i = 0;\n\n    if (n >= 9) {\n        for (; i <= n - 9; i += 8) {\n            __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        if (data[i] > data[i + 1]) {\n            return false;\n        }\n    }\n\n    if (n < 3) return true;\n    i = 0;\n\n    if (n >= 10) {\n        for (; i <= n - 10; i += 8) {\n            __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i two_ahead = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n            __m256i cmp = _mm256_cmpeq_epi32(current, two_ahead);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i <= n - 3; i++) {\n        if (data[i] == data[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    const size_t n = lst.size();\n    if (n <= 1) \n        return true;\n    const int* ptr = lst.data();\n\n    // Condition 1: non-decreasing order\n    size_t i = 0;\n    for (; i + 8 < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)(ptr + i));\n        __m256i next = _mm256_loadu_si256((const __m256i*)(ptr + i + 1));\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n    for (; i < n - 1; i++) {\n        if (ptr[i] > ptr[i+1]) {\n            return false;\n        }\n    }\n\n    // Condition 2: no triple duplicates\n    if (n < 3) {\n        return true;\n    }\n\n    i = 0;\n    for (; i + 9 < n; i += 8) {\n        __m256i v0 = _mm256_loadu_si256((const __m256i*)(ptr + i));\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(ptr + i + 1));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(ptr + i + 2));\n        __m256i eq01 = _mm256_cmpeq_epi32(v0, v1);\n        __m256i eq02 = _mm256_cmpeq_epi32(v0, v2);\n        __m256i both = _mm256_and_si256(eq01, eq02);\n        if (_mm256_movemask_epi8(both) != 0) {\n            return false;\n        }\n    }\n    for (; i < n - 2; i++) {\n        if (ptr[i] == ptr[i+1] && ptr[i] == ptr[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    const int* data = lst.data();\n    size_t i = 0;\n\n    if (n >= 10) {\n        for (; i <= n - 10; i += 8) {\n            __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n            __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n            __m256i cmp_gt = _mm256_cmpgt_epi32(v0, v1);\n            int mask_gt = _mm256_movemask_epi8(cmp_gt);\n            if (mask_gt != 0) {\n                return false;\n            }\n\n            __m256i cmp_eq1 = _mm256_cmpeq_epi32(v0, v1);\n            __m256i cmp_eq2 = _mm256_cmpeq_epi32(v0, v2);\n            __m256i cmp_triple = _mm256_and_si256(cmp_eq1, cmp_eq2);\n            int mask_triple = _mm256_movemask_epi8(cmp_triple);\n            if (mask_triple != 0) {\n                return false;\n            }\n        }\n    }\n\n    for (size_t j = i; j < n - 1; j++) {\n        if (data[j] > data[j + 1]) {\n            return false;\n        }\n    }\n\n    for (size_t j = i; j < n - 2; j++) {\n        if (data[j] == data[j + 1] && data[j] == data[j + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n < 2) {\n        return true;\n    }\n\n    size_t i = 0;\n    if (n > 8) {\n        for (; i <= n - 9; i += 8) {\n            __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n            __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(v0, v1);\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    if (n < 3) {\n        return true;\n    }\n\n    i = 0;\n    if (n > 9) {\n        for (; i <= n - 10; i += 8) {\n            __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n            __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 1));\n            __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 2));\n            __m256i eq1 = _mm256_cmpeq_epi32(v0, v1);\n            __m256i eq2 = _mm256_cmpeq_epi32(v0, v2);\n            __m256i mask = _mm256_and_si256(eq1, eq2);\n            if (!_mm256_testz_si256(mask, mask)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < n - 2; ++i) {\n        if (lst[i] == lst[i + 1] && lst[i] == lst[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    size_t n = lst.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    size_t pairs = n - 1;\n    size_t num_simd_pairs = pairs / 8;\n    size_t tail_pairs = pairs % 8;\n    size_t i = 0;\n\n    for (; i < num_simd_pairs * 8; i += 8) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 1));\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n\n    for (size_t j = i; j < n - 1; ++j) {\n        if (lst[j] > lst[j+1]) {\n            return false;\n        }\n    }\n\n    size_t triples = n - 2;\n    if (triples == 0) {\n        return true;\n    }\n\n    i = 0;\n    size_t num_simd_triples = triples / 8;\n    size_t tail_triples = triples % 8;\n\n    for (; i < num_simd_triples * 8; i += 8) {\n        __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 1));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i + 2));\n        __m256i cmp01 = _mm256_cmpeq_epi32(v0, v1);\n        __m256i cmp12 = _mm256_cmpeq_epi32(v1, v2);\n        __m256i triple_dup = _mm256_and_si256(cmp01, cmp12);\n        if (!_mm256_testz_si256(triple_dup, triple_dup)) {\n            return false;\n        }\n    }\n\n    for (size_t j = i; j < n - 2; ++j) {\n        if (lst[j] == lst[j+1] && lst[j+1] == lst[j+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n    int n = arr.size();\n    int any_zero = 0;\n    int total_neg_count = 0;\n    __m256i global_abs_sum = _mm256_setzero_si256();\n    int i = 0;\n    for (; i < n; i += 8) {\n        int remaining = n - i;\n        __m256i chunk;\n        __m256i mask_valid;\n        if (remaining >= 8) {\n            chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n            mask_valid = _mm256_set1_epi32(-1);\n        } else {\n            int tmp[8] = {0};\n            for (int j = 0; j < remaining; ++j) {\n                tmp[j] = arr[i + j];\n            }\n            chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(tmp));\n            int mask_arr[8] = {0};\n            for (int j = 0; j < remaining; ++j) {\n                mask_arr[j] = -1;\n            }\n            mask_valid = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(mask_arr));\n        }\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i mask_zero = _mm256_cmpeq_epi32(chunk, zero_vec);\n        mask_zero = _mm256_and_si256(mask_zero, mask_valid);\n        if (!_mm256_testz_si256(mask_zero, mask_zero)) {\n            any_zero = 1;\n            break;\n        }\n        __m256i mask_neg = _mm256_cmpgt_epi32(zero_vec, chunk);\n        mask_neg = _mm256_and_si256(mask_neg, mask_valid);\n        mask_neg = _mm256_and_si256(mask_neg, _mm256_set1_epi32(1));\n        __m256i sum_neg = _mm256_hadd_epi32(mask_neg, mask_neg);\n        sum_neg = _mm256_hadd_epi32(sum_neg, sum_neg);\n        int neg_count_chunk = _mm256_extract_epi32(sum_neg, 0) + _mm256_extract_epi32(sum_neg, 4);\n        total_neg_count += neg_count_chunk;\n        __m256i abs_chunk = _mm256_abs_epi32(chunk);\n        abs_chunk = _mm256_and_si256(abs_chunk, mask_valid);\n        global_abs_sum = _mm256_add_epi32(global_abs_sum, abs_chunk);\n    }\n    if (any_zero) {\n        return 0;\n    }\n    __m256i sum_abs = _mm256_hadd_epi32(global_abs_sum, global_abs_sum);\n    sum_abs = _mm256_hadd_epi32(sum_abs, sum_abs);\n    int abs_total = _mm256_extract_epi32(sum_abs, 0) + _mm256_extract_epi32(sum_abs, 4);\n    int sign_product = (total_neg_count & 1) ? -1 : 1;\n    return sign_product * abs_total;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    bool any_zero = false;\n    uint32_t scalar_sign_xor = 0;\n    __m256i abs_sum = _mm256_setzero_si256();\n\n    size_t i = 0;\n    const size_t n = arr.size();\n    for (; i + 7 < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(arr.data() + i));\n\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(v, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            any_zero = true;\n            break;\n        }\n\n        __m256i abs_v = _mm256_abs_epi32(v);\n        abs_sum = _mm256_add_epi32(abs_sum, abs_v);\n\n        __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n        __m256i sign_bits = _mm256_and_si256(v, sign_mask);\n\n        alignas(32) uint32_t signs[8];\n        _mm256_store_si256((__m256i*)signs, sign_bits);\n        scalar_sign_xor ^= signs[0] ^ signs[1] ^ signs[2] ^ signs[3] ^ signs[4] ^ signs[5] ^ signs[6] ^ signs[7];\n    }\n\n    if (any_zero) {\n        return 0;\n    }\n\n    int tail_abs_sum = 0;\n    for (; i < n; i++) {\n        int x = arr[i];\n        if (x == 0) {\n            return 0;\n        }\n        tail_abs_sum += (x < 0) ? -x : x;\n        if (x < 0) {\n            scalar_sign_xor ^= 0x80000000;\n        }\n    }\n\n    __m128i low128 = _mm256_castsi256_si128(abs_sum);\n    __m128i high128 = _mm256_extractf128_si256(abs_sum, 1);\n    __m128i sum128 = _mm_add_epi32(low128, high128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    int total_abs_simd = _mm_extract_epi32(sum128, 0);\n    int total_abs = total_abs_simd + tail_abs_sum;\n\n    int sign_factor = (scalar_sign_xor == 0) ? 1 : -1;\n    return sign_factor * total_abs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    size_t i = 0;\n    __m256i v_abs_sum = _mm256_setzero_si256();\n    int any_zero = 0;\n    int global_neg_parity = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask_zero = _mm256_cmpeq_epi32(v, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(mask_zero)) {\n            any_zero = 1;\n        }\n        int sign_mask = _mm256_movemask_ps(_mm256_castsi256_ps(v));\n        global_neg_parity ^= (__builtin_popcount(sign_mask) & 1;\n        __m256i abs_v = _mm256_abs_epi32(v);\n        v_abs_sum = _mm256_add_epi32(v_abs_sum, abs_v);\n    }\n\n    __m128i low = _mm256_castsi256_si128(v_abs_sum);\n    __m128i high = _mm256_extracti128_si256(v_abs_sum, 1);\n    __m128i sum128 = _mm_add_epi32(low, high);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    int abs_total = _mm_extract_epi32(sum128, 0);\n\n    for (; i < n; ++i) {\n        int x = arr[i];\n        if (x == 0) {\n            any_zero = 1;\n        } else {\n            if (x < 0) {\n                global_neg_parity ^= 1;\n            }\n            abs_total += (x < 0) ? -x : x;\n        }\n    }\n\n    if (any_zero) {\n        return 0;\n    }\n\n    int sign = global_neg_parity ? -1 : 1;\n    return sign * abs_total;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    bool has_zero = false;\n    int neg_count = 0;\n    int abs_sum = 0;\n    int i = 0;\n\n    for (; i + 7 < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)&arr[i]);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i zero_mask = _mm256_cmpeq_epi32(v, zero_vec);\n        int mask = _mm256_movemask_epi8(zero_mask);\n        if (mask != 0) {\n            has_zero = true;\n            break;\n        }\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero_vec, v);\n        __m256i neg_counts = _mm256_and_si256(neg_mask, _mm256_set1_epi32(1));\n        __m128i low_neg = _mm256_castsi256_si128(neg_counts);\n        __m128i high_neg = _mm256_extracti128_si256(neg_counts, 1);\n        low_neg = _mm_add_epi32(low_neg, high_neg);\n        low_neg = _mm_hadd_epi32(low_neg, low_neg);\n        low_neg = _mm_hadd_epi32(low_neg, low_neg);\n        neg_count += _mm_extract_epi32(low_neg, 0);\n\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m128i low_abs = _mm256_castsi256_si128(abs_v);\n        __m128i high_abs = _mm256_extracti128_si256(abs_v, 1);\n        low_abs = _mm_add_epi32(low_abs, high_abs);\n        low_abs = _mm_hadd_epi32(low_abs, low_abs);\n        low_abs = _mm_hadd_epi32(low_abs, low_abs);\n        abs_sum += _mm_extract_epi32(low_abs, 0);\n    }\n\n    if (has_zero) {\n        return 0;\n    }\n\n    for (; i < n; i++) {\n        int val = arr[i];\n        if (val == 0) {\n            return 0;\n        }\n        if (val < 0) {\n            neg_count++;\n        }\n        abs_sum += std::abs(val);\n    }\n\n    return (neg_count % 2 == 0) ? abs_sum : -abs_sum;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n    \n    int n = arr.size();\n    int i = 0;\n    \n    __m256i global_zero = _mm256_setzero_si256();\n    __m256i global_xor = _mm256_setzero_si256();\n    __m256i abs_sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask_zero = _mm256_cmpeq_epi32(v, _mm256_setzero_si256());\n        global_zero = _mm256_or_si256(global_zero, mask_zero);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        abs_sum_vec = _mm256_add_epi32(abs_sum_vec, abs_v);\n        __m256i sign_v = _mm256_srai_epi32(v, 31);\n        global_xor = _mm256_xor_si256(global_xor, sign_v);\n    }\n    \n    __m128i low_abs = _mm256_castsi256_si128(abs_sum_vec);\n    __m128i high_abs = _mm256_extracti128_si256(abs_sum_vec, 1);\n    __m128i sum_abs_128 = _mm_add_epi32(low_abs, high_abs);\n    __m128i hadd1 = _mm_hadd_epi32(sum_abs_128, sum_abs_128);\n    __m128i hadd2 = _mm_hadd_epi32(hadd1, hadd1);\n    int sum_abs_vector = _mm_extract_epi32(hadd2, 0);\n    \n    bool any_zero_vector = !_mm256_testz_si256(global_zero, global_zero);\n    \n    __m128i low_xor = _mm256_castsi256_si128(global_xor);\n    __m128i high_xor = _mm256_extracti128_si256(global_xor, 1);\n    low_xor = _mm_xor_si128(low_xor, high_xor);\n    __m128i low_xor64 = _mm_shuffle_epi32(low_xor, _MM_SHUFFLE(2, 3, 0, 1));\n    low_xor = _mm_xor_si128(low_xor, low_xor64);\n    __m128i low_xor32 = _mm_shuffle_epi32(low_xor, _MM_SHUFFLE(1, 0, 3, 2));\n    low_xor = _mm_xor_si128(low_xor, low_xor32);\n    int total_xor_vector = _mm_extract_epi32(low_xor, 0);\n    \n    bool any_zero_tail = false;\n    int total_xor_tail = 0;\n    int sum_abs_tail = 0;\n    for (; i < n; ++i) {\n        int x = arr[i];\n        if (x == 0) {\n            any_zero_tail = true;\n        }\n        sum_abs_tail += std::abs(x);\n        if (x < 0) {\n            total_xor_tail ^= 0xFFFFFFFF;\n        }\n    }\n    \n    bool any_zero = any_zero_vector || any_zero_tail;\n    int total_xor = total_xor_vector ^ total_xor_tail;\n    int total_abs = sum_abs_vector + sum_abs_tail;\n    \n    int sign_product;\n    if (any_zero) {\n        sign_product = 0;\n    } else {\n        sign_product = (total_xor == 0) ? 1 : -1;\n    }\n    \n    return sign_product * total_abs;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    if (k == 1) {\n        int min_val = grid[0][0];\n        int min_i = 0, min_j = 0;\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (grid[i][j] < min_val) {\n                    min_val = grid[i][j];\n                    min_i = i;\n                    min_j = j;\n                }\n            }\n        }\n        return {min_val};\n    }\n    std::vector<std::vector<std::vector<int>>> dp(k, std::vector<std::vector<int>>(N, std::vector<int>()));\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            dp[0][i][j] = {grid[i][j]};\n        }\n    }\n    const int di[4] = {-1, 0, 1, 0};\n    const int dj[4] = {0, -1, 0, 1};\n    auto less = [](const std::vector<int>& a, const std::vector<int>& b) -> bool {\n        size_t L = a.size();\n        size_t i = 0;\n        for (; i < L; i += 8) {\n            size_t len = (L - i) < 8 ? (L - i) : 8;\n            if (len < 8) {\n                break;\n            }\n            __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n            __m256i b_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n            __m256i eq = _mm256_cmpeq_epi32(a_vec, b_vec);\n            unsigned mask = static_cast<unsigned>(_mm256_movemask_epi8(eq));\n            if (mask != 0xFFFFFFFF) {\n                unsigned not_mask = ~mask;\n                unsigned long index;\n#ifdef _MSC_VER\n                if (_BitScanForward(&index, not_mask)) {\n#else\n                index = __builtin_ctz(not_mask);\n                if (1) {\n#endif\n                    size_t pos = i + (index >> 2);\n                    return a[pos] < b[pos];\n                }\n            }\n        }\n        for (; i < L; i++) {\n            if (a[i] != b[i]) {\n                return a[i] < b[i];\n            }\n        }\n        return false;\n    };\n    for (int step = 1; step < k; step++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                std::vector<int> best;\n                for (int d = 0; d < 4; d++) {\n                    int ni = i + di[d];\n                    int nj = j + dj[d];\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        std::vector<int> candidate = dp[step - 1][ni][nj];\n                        candidate.push_back(grid[i][j]);\n                        if (best.empty() || less(candidate, best)) {\n                            best = candidate;\n                        }\n                    }\n                }\n                dp[step][i][j] = best;\n            }\n        }\n    }\n    std::vector<int> ans = dp[k - 1][0][0];\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (less(dp[k - 1][i][j], ans)) {\n                ans = dp[k - 1][i][j];\n            }\n        }\n    }\n    return ans;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int n = grid.size();\n    int cur_i = -1, cur_j = -1;\n    __m256i one = _mm256_set1_epi32(1);\n    for (int i = 0; i < n; i++) {\n        int j = 0;\n        for (; j < n; j += 8) {\n            if (j + 8 > n) {\n                break;\n            }\n            __m256i v = _mm256_loadu_si256((__m256i const*)&grid[i][j]);\n            __m256i cmp = _mm256_cmpeq_epi32(v, one);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                for (int jj = j; jj < j + 8; jj++) {\n                    if (grid[i][jj] == 1) {\n                        cur_i = i;\n                        cur_j = jj;\n                        goto found;\n                    }\n                }\n            }\n        }\n        for (; j < n; j++) {\n            if (grid[i][j] == 1) {\n                cur_i = i;\n                cur_j = j;\n                goto found;\n            }\n        }\n    }\nfound:\n    std::vector<int> path;\n    path.push_back(1);\n    if (k == 1) {\n        return path;\n    }\n    const int di[4] = {-1, 1, 0, 0};\n    const int dj[4] = {0, 0, -1, 1};\n    for (int step = 1; step < k; step++) {\n        int neighbor_vals[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n        int next_i[4], next_j[4];\n        int valid_count = 0;\n        for (int d = 0; d < 4; d++) {\n            int ni = cur_i + di[d];\n            int nj = cur_j + dj[d];\n            if (ni >= 0 && ni < n && nj >= 0 && nj < n) {\n                neighbor_vals[valid_count] = grid[ni][nj];\n                next_i[valid_count] = ni;\n                next_j[valid_count] = nj;\n                valid_count++;\n            }\n        }\n        if (valid_count == 0) {\n            break;\n        }\n        int min_val;\n        if (valid_count > 1) {\n            __m128i v = _mm_loadu_si128((__m128i*)neighbor_vals);\n            __m128i v1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(2, 3, 0, 1));\n            __m128i min1 = _mm_min_epi32(v, v1);\n            __m128i min2 = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1, 0, 3, 2));\n            __m128i min3 = _mm_min_epi32(min1, min2);\n            min_val = _mm_extract_epi32(min3, 0);\n        } else {\n            min_val = neighbor_vals[0];\n        }\n        int min_index = -1;\n        for (int i = 0; i < valid_count; i++) {\n            if (neighbor_vals[i] == min_val) {\n                min_index = i;\n                break;\n            }\n        }\n        cur_i = next_i[min_index];\n        cur_j = next_j[min_index];\n        path.push_back(min_val);\n    }\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int n = grid.size();\n    if (k == 0) {\n        return std::vector<int>();\n    }\n    int min_val = grid[0][0];\n    for (int i = 0; i < n; i++) {\n        const int* row = grid[i].data();\n        int j = 0;\n        __m256i min_vec = _mm256_set1_epi32(min_val);\n        if (n >= 8) {\n            for (; j <= n - 8; j += 8) {\n                __m256i data = _mm256_loadu_si256((__m256i const*)(row + j));\n                __m256i min_temp = _mm256_min_epi32(min_vec, data);\n                min_vec = min_temp;\n            }\n        }\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, min_vec);\n        for (int t = 0; t < 8; t++) {\n            if (arr[t] < min_val) {\n                min_val = arr[t];\n            }\n        }\n        for (; j < n; j++) {\n            if (row[j] < min_val) {\n                min_val = row[j];\n            }\n        }\n    }\n    int min_i = -1;\n    int min_j = -1;\n    bool found = false;\n    __m256i target = _mm256_set1_epi32(min_val);\n    for (int i = 0; i < n && !found; i++) {\n        const int* row = grid[i].data();\n        int j = 0;\n        if (n >= 8) {\n            for (; j <= n - 8; j += 8) {\n                __m256i data = _mm256_loadu_si256((__m256i const*)(row + j));\n                __m256i cmp = _mm256_cmpeq_epi32(data, target);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask) {\n                    for (int t = 0; t < 8; t++) {\n                        if (row[j + t] == min_val) {\n                            min_i = i;\n                            min_j = j + t;\n                            found = true;\n                            break;\n                        }\n                    }\n                    if (found) break;\n                }\n            }\n        }\n        if (!found) {\n            for (; j < n; j++) {\n                if (row[j] == min_val) {\n                    min_i = i;\n                    min_j = j;\n                    found = true;\n                    break;\n                }\n            }\n        }\n    }\n    std::vector<int> path;\n    path.push_back(min_val);\n    int cur_i = min_i;\n    int cur_j = min_j;\n    int dx[] = {-1, 1, 0, 0};\n    int dy[] = {0, 0, -1, 1};\n    for (int step = 1; step < k; step++) {\n        int next_val;\n        int next_i = -1;\n        int next_j = -1;\n        bool found_neighbor = false;\n        for (int d = 0; d < 4; d++) {\n            int ni = cur_i + dx[d];\n            int nj = cur_j + dy[d];\n            if (ni >= 0 && ni < n && nj >= 0 && nj < n) {\n                if (!found_neighbor || grid[ni][nj] < next_val) {\n                    next_val = grid[ni][nj];\n                    next_i = ni;\n                    next_j = nj;\n                    found_neighbor = true;\n                }\n            }\n        }\n        path.push_back(next_val);\n        cur_i = next_i;\n        cur_j = next_j;\n    }\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    int r0 = -1, c0 = -1;\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (grid[i][j] == 1) {\n                r0 = i;\n                c0 = j;\n                break;\n            }\n        }\n        if (r0 != -1) break;\n    }\n\n    std::vector<int> path;\n    path.push_back(1);\n    if (k == 1) {\n        return path;\n    }\n\n    int r = r0, c = c0;\n    const int dr[4] = {-1, 1, 0, 0};\n    const int dc[4] = {0, 0, -1, 1};\n\n    for (int step = 1; step < k; step++) {\n        int candidate_values[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n\n        for (int i = 0; i < 4; i++) {\n            int nr = r + dr[i];\n            int nc = c + dc[i];\n            if (nr >= 0 && nr < N && nc >= 0 && nc < N) {\n                candidate_values[i] = grid[nr][nc];\n            }\n        }\n\n        __m128i vec = _mm_loadu_si128((__m128i*)candidate_values);\n        __m128i shuffled = _mm_shuffle_epi32(vec, _MM_SHUFFLE(2,3,0,1));\n        __m128i min1 = _mm_min_epi32(vec, shuffled);\n        shuffled = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1,0,3,2));\n        __m128i min2 = _mm_min_epi32(min1, shuffled);\n        int min_val = _mm_extract_epi32(min2, 0);\n\n        __m128i min_broadcast = _mm_set1_epi32(min_val);\n        __m128i cmp = _mm_cmpeq_epi32(vec, min_broadcast);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n\n        int idx = 0;\n        if (mask & 1) idx = 0;\n        else if (mask & 2) idx = 1;\n        else if (mask & 4) idx = 2;\n        else if (mask & 8) idx = 3;\n\n        r += dr[idx];\n        c += dc[idx];\n        path.push_back(min_val);\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int n = grid.size();\n    int start_i = -1, start_j = -1;\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            if (grid[i][j] == 1) {\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    std::unordered_map<int, int> visited;\n    std::vector<int> path;\n    path.push_back(1);\n    int linear_start = start_i * n + start_j;\n    visited[linear_start] = 0;\n    int current_i = start_i, current_j = start_j;\n    int cycle_start_index = -1;\n    int step;\n\n    for (step = 1; step < k; step++) {\n        int candidates[4] = {\n            (current_i - 1 >= 0) ? grid[current_i - 1][current_j] : INT_MAX,\n            (current_i + 1 < n)  ? grid[current_i + 1][current_j] : INT_MAX,\n            (current_j - 1 >= 0) ? grid[current_i][current_j - 1] : INT_MAX,\n            (current_j + 1 < n)  ? grid[current_i][current_j + 1] : INT_MAX\n        };\n\n        __m128i vec = _mm_loadu_si128((__m128i*)candidates);\n        __m128i v_swapped = _mm_shuffle_epi32(vec, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i v_min1 = _mm_min_epi32(vec, v_swapped);\n        __m128i v_swapped2 = _mm_shuffle_epi32(v_min1, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i v_min2 = _mm_min_epi32(v_min1, v_swapped2);\n        int min_val = _mm_extract_epi32(v_min2, 0);\n\n        __m128i v_cmp = _mm_cmpeq_epi32(vec, _mm_set1_epi32(min_val));\n        int mask = _mm_movemask_epi8(v_cmp);\n        int index;\n        if (mask & 0x000F) {\n            index = 0;\n        } else if (mask & 0x00F0) {\n            index = 1;\n        } else if (mask & 0x0F00) {\n            index = 2;\n        } else {\n            index = 3;\n        }\n\n        int next_i, next_j;\n        if (index == 0) {\n            next_i = current_i - 1;\n            next_j = current_j;\n        } else if (index == 1) {\n            next_i = current_i + 1;\n            next_j = current_j;\n        } else if (index == 2) {\n            next_i = current_i;\n            next_j = current_j - 1;\n        } else {\n            next_i = current_i;\n            next_j = current_j + 1;\n        }\n\n        int next_linear = next_i * n + next_j;\n        if (visited.find(next_linear) != visited.end()) {\n            cycle_start_index = visited[next_linear];\n            break;\n        }\n\n        current_i = next_i;\n        current_j = next_j;\n        path.push_back(min_val);\n        visited[next_linear] = path.size() - 1;\n    }\n\n    if (step < k) {\n        int rem = k - path.size();\n        int cycle_length = path.size() - cycle_start_index;\n        for (int i = 0; i < rem; i++) {\n            int pos_in_cycle = i % cycle_length;\n            path.push_back(path[cycle_start_index + pos_in_cycle]);\n        }\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t digits[32];\n    for (int i = 0; i < 32; i++) {\n        digits[i] = 1;\n    }\n\n    int count = 0;\n    uint64_t num = n;\n    while (num) {\n        digits[count] = num % 10;\n        num /= 10;\n        count++;\n    }\n\n    uint8_t valid_array[32];\n    for (int i = 0; i < 32; i++) {\n        valid_array[i] = (i < count) ? 0xFF : 0x00;\n    }\n\n    __m256i v = _mm256_loadu_si256((__m256i*)digits);\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i mask = _mm256_and_si256(v, ones);\n    __m256i even_mask = _mm256_cmpeq_epi8(mask, _mm256_setzero_si256());\n    __m256i blended = _mm256_blendv_epi8(v, ones, even_mask);\n\n    __m256i valid_vec = _mm256_loadu_si256((__m256i*)valid_array);\n    __m256i valid_odd_flags = _mm256_and_si256(mask, valid_vec);\n    if (_mm256_testz_si256(valid_odd_flags, valid_odd_flags)) {\n        return 0;\n    }\n\n    uint8_t blended_array[32];\n    _mm256_storeu_si256((__m256i*)blended_array, blended);\n    uint64_t product = 1;\n    for (int i = 0; i < 32; i++) {\n        product *= blended_array[i];\n    }\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t digits[32] = {1};\n    int len = 0;\n    uint64_t temp = n;\n    if (temp == 0) {\n        digits[0] = 0;\n        len = 1;\n    } else {\n        while (temp) {\n            digits[len++] = temp % 10;\n            temp /= 10;\n        }\n    }\n\n    __m256i v = _mm256_loadu_si256((__m256i*)digits);\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i and_result = _mm256_and_si256(v, ones);\n    __m256i mask = _mm256_cmpeq_epi8(and_result, ones);\n    __m256i indices = _mm256_setr_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7,\n        8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23,\n        24, 25, 26, 27, 28, 29, 30, 31\n    );\n    __m256i len_vec = _mm256_set1_epi8(len);\n    __m256i active_mask = _mm256_cmpgt_epi8(len_vec, indices);\n    mask = _mm256_and_si256(mask, active_mask);\n    v = _mm256_blendv_epi8(ones, v, mask);\n    uint8_t result_digits[32];\n    _mm256_storeu_si256((__m256i*)result_digits, v);\n\n    uint64_t product = 1;\n    for (int i = 0; i < 32; i++) {\n        product *= result_digits[i];\n    }\n\n    if (_mm256_movemask_epi8(mask) == 0) {\n        return 0;\n    } else {\n        return product;\n    }\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nstatic inline __m128i product4(__m128i v) {\n    __m128i shuf1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128i prod1 = _mm_mullo_epi32(v, shuf1);\n    __m128i shuf2 = _mm_shuffle_epi32(prod1, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i prod2 = _mm_mullo_epi32(prod1, shuf2);\n    return prod2;\n}\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    int digits[24] = {0};\n    int count = 0;\n    uint64_t temp = n;\n    while (temp) {\n        digits[count++] = temp % 10;\n        temp /= 10;\n    }\n    for (int i = count; i < 24; i++) {\n        digits[i] = 0;\n    }\n\n    uint64_t product = 1;\n    int any_odd = 0;\n\n    for (int i = 0; i < 24; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(digits + i));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(v, ones);\n        __m256i cmp = _mm256_cmpeq_epi32(mask, ones);\n\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            any_odd = 1;\n        }\n\n        __m256i blended = _mm256_or_si256(\n            _mm256_and_si256(cmp, v),\n            _mm256_andnot_si256(cmp, ones)\n        );\n\n        __m128i low = _mm256_castsi256_si128(blended);\n        __m128i high = _mm256_extracti128_si256(blended, 1);\n        __m128i plow = product4(low);\n        __m128i phigh = product4(high);\n\n        uint32_t p0 = _mm_extract_epi32(plow, 0);\n        uint32_t p1 = _mm_extract_epi32(phigh, 0);\n        product *= (uint64_t)p0 * p1;\n    }\n\n    return any_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t digits[32] = {0};\n    uint64_t t = n;\n    for (int i = 0; i < 32; i++) {\n        if (t != 0) {\n            digits[i] = t % 10;\n            t /= 10;\n        }\n    }\n\n    __m256i v_digits = _mm256_loadu_si256((__m256i*)digits);\n    __m256i v_ones = _mm256_set1_epi8(1);\n    __m256i v_mask = _mm256_and_si256(v_digits, v_ones);\n    __m256i v_odd = _mm256_cmpeq_epi8(v_mask, v_ones);\n    __m256i v_adjusted = _mm256_blendv_epi8(v_ones, v_digits, v_odd);\n\n    int any_odd = _mm256_movemask_epi8(v_odd) != 0;\n    uint8_t adjusted_arr[32];\n    _mm256_storeu_si256((__m256i*)adjusted_arr, v_adjusted);\n\n    uint64_t product = 1;\n    for (int i = 0; i < 32; i++) {\n        product *= adjusted_arr[i];\n    }\n\n    return any_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nstatic inline __m128i horizontal_product4(__m128i v) {\n    __m128i shuf = _mm_shuffle_epi32(v, _MM_SHUFFLE(2,3,0,1));\n    __m128i prod1 = _mm_mullo_epi32(v, shuf);\n    __m128i high = _mm_unpackhi_epi32(prod1, prod1);\n    return _mm_mul_epu32(prod1, high);\n}\n\nuint64_t digits_simd(uint64_t n) {\n    uint32_t digits[24] = {0};\n    int count = 0;\n    uint64_t temp = n;\n    int any_odd = 0;\n\n    if (n == 0) {\n        digits[count++] = 0;\n    } else {\n        while (temp) {\n            uint32_t d = temp % 10;\n            digits[count++] = d;\n            if (d % 2 == 1) {\n                any_odd = 1;\n            }\n            temp /= 10;\n        }\n    }\n\n    if (!any_odd) {\n        return 0;\n    }\n\n    for (int i = 0; i < count; i++) {\n        if (digits[i] % 2 == 0) {\n            digits[i] = 1;\n        }\n    }\n\n    int padded_count = ((count + 7) / 8) * 8;\n    for (int i = count; i < padded_count; i++) {\n        digits[i] = 1;\n    }\n\n    uint64_t total = 1;\n    for (int i = 0; i < padded_count; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(digits + i));\n        __m128i low = _mm256_castsi256_si128(v);\n        __m128i high = _mm256_extracti128_si256(v, 1);\n        __m128i plow = horizontal_product4(low);\n        __m128i phigh = horizontal_product4(high);\n        uint64_t prod_low = _mm_extract_epi64(plow, 0);\n        uint64_t prod_high = _mm_extract_epi64(phigh, 0);\n        total *= prod_low * prod_high;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t total = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 ceil_vec = _mm256_ceil_ps(vec);\n            __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n            \n            __m128i low = _mm256_castsi256_si128(int_vec);\n            __m128i high = _mm256_extracti128_si256(int_vec, 1);\n            \n            __m128i abs_low = _mm_abs_epi32(low);\n            __m128i even_low = _mm_shuffle_epi32(abs_low, _MM_SHUFFLE(2,2,0,0));\n            __m128i odd_low = _mm_shuffle_epi32(abs_low, _MM_SHUFFLE(3,3,1,1));\n            __m128i sq_even_low = _mm_mul_epu32(even_low, even_low);\n            __m128i sq_odd_low = _mm_mul_epu32(odd_low, odd_low);\n            __m128i sq_low = _mm_add_epi64(sq_even_low, sq_odd_low);\n            __m128i sum_low = _mm_add_epi64(sq_low, _mm_srli_si128(sq_low, 8));\n            \n            __m128i abs_high = _mm_abs_epi32(high);\n            __m128i even_high = _mm_shuffle_epi32(abs_high, _MM_SHUFFLE(2,2,0,0));\n            __m128i odd_high = _mm_shuffle_epi32(abs_high, _MM_SHUFFLE(3,3,1,1));\n            __m128i sq_even_high = _mm_mul_epu32(even_high, even_high);\n            __m128i sq_odd_high = _mm_mul_epu32(odd_high, odd_high);\n            __m128i sq_high = _mm_add_epi64(sq_even_high, sq_odd_high);\n            __m128i sum_high = _mm_add_epi64(sq_high, _mm_srli_si128(sq_high, 8));\n            \n            total += (uint64_t)_mm_cvtsi128_si64(sum_low);\n            total += (uint64_t)_mm_cvtsi128_si64(sum_high);\n        }\n    }\n    \n    for (; i < n; ++i) {\n        int64_t val = static_cast<int64_t>(std::ceil(lst[i]));\n        total += val * val;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i acc = _mm256_setzero_si256();\n        __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF);\n\n        for (; i <= n-8; i+=8) {\n            __m256 v = _mm256_loadu_ps(&lst[i]);\n            __m256 ceil_v = _mm256_ceil_ps(v);\n            __m256i vi = _mm256_cvtps_epi32(ceil_v);\n\n            __m256i even = _mm256_and_si256(vi, mask);\n            __m256i odd = _mm256_srli_epi64(vi, 32);\n\n            __m256i even_sq = _mm256_mul_epi32(even, even);\n            __m256i odd_sq = _mm256_mul_epi32(odd, odd);\n\n            __m256i sq_total = _mm256_add_epi64(even_sq, odd_sq);\n            acc = _mm256_add_epi64(acc, sq_total);\n        }\n\n        uint64_t tmp[4];\n        _mm256_storeu_si256((__m256i*)tmp, acc);\n        sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n    }\n\n    for (; i < n; i++) {\n        float x = lst[i];\n        int32_t y = (int32_t)x;\n        if (x > (float)y) {\n            y++;\n        }\n        uint64_t square = (uint64_t)y * (uint64_t)y;\n        sum += square;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t i = 0;\n    __m256i total_low = _mm256_setzero_si256();\n    __m256i total_high = _mm256_setzero_si256();\n\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256i int32_vec = _mm256_cvtps_epi32(ceil_vec);\n        \n        __m128i low128 = _mm256_castsi256_si128(int32_vec);\n        __m128i high128 = _mm256_extractf128_si256(int32_vec, 1);\n        \n        __m256i low64 = _mm256_cvtepi32_epi64(low128);\n        __m256i high64 = _mm256_cvtepi32_epi64(high128);\n        \n        __m256i sq_low = _mm256_mul_epi32(low64, low64);\n        __m256i sq_high = _mm256_mul_epi32(high64, high64);\n        \n        total_low = _mm256_add_epi64(total_low, sq_low);\n        total_high = _mm256_add_epi64(total_high, sq_high);\n    }\n\n    __m256i total = _mm256_add_epi64(total_low, total_high);\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, total);\n    uint64_t sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        float x = lst[i];\n        int32_t t = (int32_t)x;\n        if (x > 0 && (float)t < x) {\n            t++;\n        }\n        sum += (uint64_t)t * (uint64_t)t;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    if (lst.empty()) return 0;\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n    const size_t vec_size = 8;\n    size_t n = lst.size();\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256 v = _mm256_loadu_ps(&lst[i]);\n        __m256 rounded = _mm256_ceil_ps(v);\n        __m256i vi = _mm256_cvtps_epi32(rounded);\n        __m256i even_squares = _mm256_mul_epu32(vi, vi);\n        __m256i vi_shifted = _mm256_srli_epi64(vi, 32);\n        __m256i odd_squares = _mm256_mul_epu32(vi_shifted, vi_shifted);\n        __m256i squares = _mm256_add_epi64(even_squares, odd_squares);\n        sum_vec = _mm256_add_epi64(sum_vec, squares);\n    }\n\n    __m128i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi64(sum_low, sum_high);\n    uint64_t sum_array[2];\n    _mm_store_si128((__m128i*)sum_array, sum128);\n    uint64_t total = sum_array[0] + sum_array[1];\n\n    for (; i < n; ++i) {\n        int rounded = (int)ceil(lst[i]);\n        total += (uint64_t)rounded * (uint64_t)rounded;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t total_sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    const int round_mode = _MM_FROUND_CEIL;\n\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 rounded = _mm256_round_ps(vec, round_mode);\n            __m256i vi32 = _mm256_cvtps_epi32(rounded);\n            \n            __m128i low128 = _mm256_castsi256_si128(vi32);\n            __m128i high128 = _mm256_extractf128_si256(vi32, 1);\n\n            __m128i low_shifted = _mm_srli_epi64(low128, 32);\n            __m128i low_sq_even = _mm_mul_epu32(low128, low128);\n            __m128i low_sq_odd = _mm_mul_epu32(low_shifted, low_shifted);\n            __m128i low_sum = _mm_add_epi64(low_sq_even, low_sq_odd);\n\n            __m128i high_shifted = _mm_srli_epi64(high128, 32);\n            __m128i high_sq_even = _mm_mul_epu32(high128, high128);\n            __m128i high_sq_odd = _mm_mul_epu32(high_shifted, high_shifted);\n            __m128i high_sum = _mm_add_epi64(high_sq_even, high_sq_odd);\n\n            __m128i chunk_sum_vec = _mm_add_epi64(low_sum, high_sum);\n            uint64_t sum1 = _mm_extract_epi64(chunk_sum_vec, 0);\n            uint64_t sum2 = _mm_extract_epi64(chunk_sum_vec, 1);\n            total_sum += sum1 + sum2;\n        }\n    }\n\n    for (; i < n; i++) {\n        float val = lst[i];\n        int64_t ceiled;\n        if (val == (float)(int64_t)val) {\n            ceiled = (int64_t)val;\n        } else if (val > 0) {\n            ceiled = (int64_t)val + 1;\n        } else {\n            ceiled = (int64_t)val;\n        }\n        total_sum += ceiled * ceiled;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return -1;\n    }\n    int pairs = n - 1;\n    int candidate = -1;\n    const int* data = arr.data();\n\n    int i = 0;\n    for (; i + 8 <= pairs; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(data + i));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(data + i + 1));\n        __m256i cmp = _mm256_cmpgt_epi32(left, right);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        for (int j = 0; j < 8; j++) {\n            if (mask & (1 << j)) {\n                candidate = i + j + 1;\n            }\n        }\n    }\n\n    for (; i < pairs; i++) {\n        if (data[i+1] < data[i]) {\n            candidate = i + 1;\n        }\n    }\n\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int candidate = -1;\n    int i = 1;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i prev_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i - 1));\n        __m256i curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n        __m256i cmp = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        \n        if (mask) {\n            for (int j = 7; j >= 0; j--) {\n                if (mask & (1 << j)) {\n                    candidate = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] < arr[i-1]) {\n            candidate = i;\n        }\n    }\n\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int last_index = -1;\n    const int* data = arr.data();\n\n    size_t i = 1;\n    for (; i <= n - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256((const __m256i*)(data + i - 1));\n        __m256i curr = _mm256_loadu_si256((const __m256i*)(data + i));\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int bitmask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if (bitmask) {\n            int k = 7;\n            while (k >= 0) {\n                if (bitmask & (1 << k)) {\n                    last_index = i + k;\n                    break;\n                }\n                k--;\n            }\n        }\n    }\n\n    for (size_t j = i; j < n; j++) {\n        if (data[j] < data[j-1]) {\n            last_index = j;\n        }\n    }\n\n    return last_index;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n <= 1) \n        return -1;\n\n    int candidate = -1;\n    int i = 1;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i - 1));\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        if (mask) {\n            int j = 7;\n            while (j >= 0) {\n                if (mask & (1 << j)) {\n                    if (i + j > candidate) \n                        candidate = i + j;\n                    break;\n                }\n                j--;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] < arr[i-1]) {\n            candidate = i;\n        }\n    }\n\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int last_drop = -1;\n    int num_compare = n - 1;\n    int i = 0;\n    for (; i <= num_compare - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i));\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(arr.data() + i + 1));\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if (mask) {\n            for (int j = 7; j >= 0; j--) {\n                if (mask & (1 << j)) {\n                    last_drop = i + j + 1;\n                    break;\n                }\n            }\n        }\n    }\n    for (; i < num_compare; i++) {\n        if (arr[i] > arr[i+1]) {\n            last_drop = i+1;\n        }\n    }\n    return last_drop;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i current_neg_max = _mm256_set1_epi32(INT_MIN);\n    __m256i current_pos_min = _mm256_set1_epi32(INT_MAX);\n    __m256i any_neg_global = _mm256_setzero_si256();\n    __m256i any_pos_global = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        \n        __m256i mask_neg = _mm256_cmpgt_epi32(zero, vec);\n        any_neg_global = _mm256_or_si256(any_neg_global, mask_neg);\n        __m256i vec_neg = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MIN), vec, mask_neg);\n        current_neg_max = _mm256_max_epi32(current_neg_max, vec_neg);\n        \n        __m256i mask_pos = _mm256_cmpgt_epi32(vec, zero);\n        any_pos_global = _mm256_or_si256(any_pos_global, mask_pos);\n        __m256i vec_pos = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), vec, mask_pos);\n        current_pos_min = _mm256_min_epi32(current_pos_min, vec_pos);\n    }\n\n    __m128i low_neg = _mm256_castsi256_si128(current_neg_max);\n    __m128i high_neg = _mm256_extracti128_si256(current_neg_max, 1);\n    low_neg = _mm_max_epi32(low_neg, high_neg);\n    high_neg = _mm_shuffle_epi32(low_neg, _MM_SHUFFLE(1, 0, 3, 2));\n    low_neg = _mm_max_epi32(low_neg, high_neg);\n    high_neg = _mm_shuffle_epi32(low_neg, _MM_SHUFFLE(2, 3, 0, 1));\n    low_neg = _mm_max_epi32(low_neg, high_neg);\n    int max_neg = _mm_extract_epi32(low_neg, 0);\n    \n    __m128i low_pos = _mm256_castsi256_si128(current_pos_min);\n    __m128i high_pos = _mm256_extracti128_si256(current_pos_min, 1);\n    low_pos = _mm_min_epi32(low_pos, high_pos);\n    high_pos = _mm_shuffle_epi32(low_pos, _MM_SHUFFLE(1, 0, 3, 2));\n    low_pos = _mm_min_epi32(low_pos, high_pos);\n    high_pos = _mm_shuffle_epi32(low_pos, _MM_SHUFFLE(2, 3, 0, 1));\n    low_pos = _mm_min_epi32(low_pos, high_pos);\n    int min_pos = _mm_extract_epi32(low_pos, 0);\n    \n    __m128i low_any_neg = _mm256_castsi256_si128(any_neg_global);\n    __m128i high_any_neg = _mm256_extracti128_si256(any_neg_global, 1);\n    low_any_neg = _mm_or_si128(low_any_neg, high_any_neg);\n    low_any_neg = _mm_or_si128(low_any_neg, _mm_shuffle_epi32(low_any_neg, _MM_SHUFFLE(1, 0, 3, 2)));\n    low_any_neg = _mm_or_si128(low_any_neg, _mm_shuffle_epi32(low_any_neg, _MM_SHUFFLE(2, 3, 0, 1)));\n    int any_negative_simd = _mm_extract_epi32(low_any_neg, 0);\n    \n    __m128i low_any_pos = _mm256_castsi256_si128(any_pos_global);\n    __m128i high_any_pos = _mm256_extracti128_si256(any_pos_global, 1);\n    low_any_pos = _mm_or_si128(low_any_pos, high_any_pos);\n    low_any_pos = _mm_or_si128(low_any_pos, _mm_shuffle_epi32(low_any_pos, _MM_SHUFFLE(1, 0, 3, 2)));\n    low_any_pos = _mm_or_si128(low_any_pos, _mm_shuffle_epi32(low_any_pos, _MM_SHUFFLE(2, 3, 0, 1)));\n    int any_positive_simd = _mm_extract_epi32(low_any_pos, 0);\n    \n    int scalar_neg_max = INT_MIN;\n    int scalar_pos_min = INT_MAX;\n    bool any_neg_scalar = false;\n    bool any_pos_scalar = false;\n    \n    for (; i < lst.size(); i++) {\n        int x = lst[i];\n        if (x < 0) {\n            if (!any_neg_scalar || x > scalar_neg_max) {\n                scalar_neg_max = x;\n                any_neg_scalar = true;\n            }\n        } else if (x > 0) {\n            if (!any_pos_scalar || x < scalar_pos_min) {\n                scalar_pos_min = x;\n                any_pos_scalar = true;\n            }\n        }\n    }\n    \n    if (any_neg_scalar) {\n        if (scalar_neg_max > max_neg) {\n            max_neg = scalar_neg_max;\n        }\n    }\n    \n    if (any_pos_scalar) {\n        if (scalar_pos_min < min_pos) {\n            min_pos = scalar_pos_min;\n        }\n    }\n    \n    bool any_neg_total = (any_negative_simd != 0) || any_neg_scalar;\n    bool any_pos_total = (any_positive_simd != 0) || any_pos_scalar;\n    \n    if (!any_neg_total) {\n        max_neg = 0;\n    }\n    if (!any_pos_total) {\n        min_pos = 0;\n    }\n    \n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <vector>\n#include <climits>\n#include <immintrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int neg_max = INT_MIN;\n    int pos_min = INT_MAX;\n    \n    if (n == 0) {\n        return {0, 0};\n    }\n\n    __m256i v_neg_max = _mm256_set1_epi32(INT_MIN);\n    __m256i v_pos_min = _mm256_set1_epi32(INT_MAX);\n    __m256i zero = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i x = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        \n        __m256i mask_neg = _mm256_cmpgt_epi32(zero, x);\n        __m256i candidate_neg = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MIN), x, mask_neg);\n        v_neg_max = _mm256_max_epi32(v_neg_max, candidate_neg);\n        \n        __m256i mask_pos = _mm256_cmpgt_epi32(x, zero);\n        __m256i candidate_pos = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), x, mask_pos);\n        v_pos_min = _mm256_min_epi32(v_pos_min, candidate_pos);\n    }\n    \n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val < 0) {\n            if (val > neg_max) neg_max = val;\n        } else if (val > 0) {\n            if (val < pos_min) pos_min = val;\n        }\n    }\n    \n    __m128i v_neg_low = _mm256_extracti128_si256(v_neg_max, 0);\n    __m128i v_neg_high = _mm256_extracti128_si256(v_neg_max, 1);\n    v_neg_low = _mm_max_epi32(v_neg_low, v_neg_high);\n    __m128i v_neg_shuf = _mm_shuffle_epi32(v_neg_low, _MM_SHUFFLE(2, 3, 0, 1));\n    v_neg_low = _mm_max_epi32(v_neg_low, v_neg_shuf);\n    v_neg_shuf = _mm_shuffle_epi32(v_neg_low, _MM_SHUFFLE(1, 0, 3, 2));\n    v_neg_low = _mm_max_epi32(v_neg_low, v_neg_shuf);\n    int simd_neg_max = _mm_extract_epi32(v_neg_low, 0);\n    if (simd_neg_max > neg_max) neg_max = simd_neg_max;\n    \n    __m128i v_pos_low = _mm256_extracti128_si256(v_pos_min, 0);\n    __m128i v_pos_high = _mm256_extracti128_si256(v_pos_min, 1);\n    v_pos_low = _mm_min_epi32(v_pos_low, v_pos_high);\n    __m128i v_pos_shuf = _mm_shuffle_epi32(v_pos_low, _MM_SHUFFLE(2, 3, 0, 1));\n    v_pos_low = _mm_min_epi32(v_pos_low, v_pos_shuf);\n    v_pos_shuf = _mm_shuffle_epi32(v_pos_low, _MM_SHUFFLE(1, 0, 3, 2));\n    v_pos_low = _mm_min_epi32(v_pos_low, v_pos_shuf);\n    int simd_pos_min = _mm_extract_epi32(v_pos_low, 0);\n    if (simd_pos_min < pos_min) pos_min = simd_pos_min;\n    \n    if (neg_max == INT_MIN) neg_max = 0;\n    if (pos_min == INT_MAX) pos_min = 0;\n    \n    return {neg_max, pos_min};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    __m256i v_max_neg = _mm256_set1_epi32(INT_MIN);\n    __m256i v_min_pos = _mm256_set1_epi32(INT_MAX);\n    __m256i v_any_neg = _mm256_setzero_si256();\n    __m256i v_any_pos = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    \n    size_t i = 0;\n    size_t n = lst.size();\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst.data() + i));\n        \n        __m256i mask_neg = _mm256_cmpgt_epi32(zero, data);\n        v_any_neg = _mm256_or_si256(v_any_neg, mask_neg);\n        __m256i data_neg = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MIN), data, mask_neg);\n        v_max_neg = _mm256_max_epi32(v_max_neg, data_neg);\n        \n        __m256i mask_pos = _mm256_cmpgt_epi32(data, zero);\n        v_any_pos = _mm256_or_si256(v_any_pos, mask_pos);\n        __m256i data_pos = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, mask_pos);\n        v_min_pos = _mm256_min_epi32(v_min_pos, data_pos);\n    }\n    \n    __m128i v_max_neg_low = _mm256_castsi256_si128(v_max_neg);\n    __m128i v_max_neg_high = _mm256_extractf128_si256(v_max_neg, 1);\n    __m128i v_max_neg128 = _mm_max_epi32(v_max_neg_low, v_max_neg_high);\n    v_max_neg128 = _mm_max_epi32(v_max_neg128, _mm_shuffle_epi32(v_max_neg128, _MM_SHUFFLE(1, 0, 3, 2)));\n    v_max_neg128 = _mm_max_epi32(v_max_neg128, _mm_shuffle_epi32(v_max_neg128, _MM_SHUFFLE(2, 3, 0, 1)));\n    int max_neg_simd = _mm_extract_epi32(v_max_neg128, 0);\n    \n    __m128i v_min_pos_low = _mm256_castsi256_si128(v_min_pos);\n    __m128i v_min_pos_high = _mm256_extractf128_si256(v_min_pos, 1);\n    __m128i v_min_pos128 = _mm_min_epi32(v_min_pos_low, v_min_pos_high);\n    v_min_pos128 = _mm_min_epi32(v_min_pos128, _mm_shuffle_epi32(v_min_pos128, _MM_SHUFFLE(1, 0, 3, 2)));\n    v_min_pos128 = _mm_min_epi32(v_min_pos128, _mm_shuffle_epi32(v_min_pos128, _MM_SHUFFLE(2, 3, 0, 1)));\n    int min_pos_simd = _mm_extract_epi32(v_min_pos128, 0);\n    \n    __m128i v_any_neg_low = _mm256_castsi256_si128(v_any_neg);\n    __m128i v_any_neg_high = _mm256_extractf128_si256(v_any_neg, 1);\n    __m128i v_any_neg128 = _mm_or_si128(v_any_neg_low, v_any_neg_high);\n    bool has_neg_simd = !_mm_testz_si128(v_any_neg128, v_any_neg128);\n    \n    __m128i v_any_pos_low = _mm256_castsi256_si128(v_any_pos);\n    __m128i v_any_pos_high = _mm256_extractf128_si256(v_any_pos, 1);\n    __m128i v_any_pos128 = _mm_or_si128(v_any_pos_low, v_any_pos_high);\n    bool has_pos_simd = !_mm_testz_si128(v_any_pos128, v_any_pos128);\n    \n    int max_neg_tail = INT_MIN;\n    int min_pos_tail = INT_MAX;\n    bool has_neg_tail = false;\n    bool has_pos_tail = false;\n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            has_neg_tail = true;\n            if (x > max_neg_tail) max_neg_tail = x;\n        } else if (x > 0) {\n            has_pos_tail = true;\n            if (x < min_pos_tail) min_pos_tail = x;\n        }\n    }\n    \n    bool any_neg = has_neg_simd || has_neg_tail;\n    bool any_pos = has_pos_simd || has_pos_tail;\n    \n    int global_max_neg = max_neg_simd;\n    if (has_neg_tail && max_neg_tail > global_max_neg) {\n        global_max_neg = max_neg_tail;\n    }\n    \n    int global_min_pos = min_pos_simd;\n    if (has_pos_tail && min_pos_tail < global_min_pos) {\n        global_min_pos = min_pos_tail;\n    }\n    \n    int a = any_neg ? global_max_neg : 0;\n    int b = any_pos ? global_min_pos : 0;\n    \n    return {a, b};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    size_t n = lst.size();\n    __m256i max_neg_vec = _mm256_set1_epi32(INT_MIN);\n    __m256i min_pos_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i found_neg_vec = _mm256_setzero_si256();\n    __m256i found_pos_vec = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(lst.data() + i));\n        \n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, data);\n        __m256i pos_mask = _mm256_cmpgt_epi32(data, zero);\n        \n        found_neg_vec = _mm256_or_si256(found_neg_vec, neg_mask);\n        found_pos_vec = _mm256_or_si256(found_pos_vec, pos_mask);\n        \n        __m256i neg_data = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MIN), data, neg_mask);\n        __m256i pos_data = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, pos_mask);\n        \n        max_neg_vec = _mm256_max_epi32(max_neg_vec, neg_data);\n        min_pos_vec = _mm256_min_epi32(min_pos_vec, pos_data);\n    }\n\n    int max_neg_arr[8];\n    int min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg_vec);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos_vec);\n    \n    int max_neg = max_neg_arr[0];\n    int min_pos = min_pos_arr[0];\n    for (int j = 1; j < 8; j++) {\n        if (max_neg_arr[j] > max_neg) max_neg = max_neg_arr[j];\n        if (min_pos_arr[j] < min_pos) min_pos = min_pos_arr[j];\n    }\n\n    int any_negative_simd = !_mm256_testz_si256(found_neg_vec, found_neg_vec);\n    int any_positive_simd = !_mm256_testz_si256(found_pos_vec, found_pos_vec);\n\n    bool any_neg_rem = false;\n    bool any_pos_rem = false;\n    int max_neg_rem = INT_MIN;\n    int min_pos_rem = INT_MAX;\n    \n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            any_neg_rem = true;\n            if (x > max_neg_rem) max_neg_rem = x;\n        } else if (x > 0) {\n            any_pos_rem = true;\n            if (x < min_pos_rem) min_pos_rem = x;\n        }\n    }\n    \n    if (any_neg_rem) {\n        if (max_neg_rem > max_neg) max_neg = max_neg_rem;\n    }\n    if (any_pos_rem) {\n        if (min_pos_rem < min_pos) min_pos = min_pos_rem;\n    }\n    \n    bool any_negative = any_negative_simd || any_neg_rem;\n    bool any_positive = any_positive_simd || any_pos_rem;\n    \n    if (!any_negative) max_neg = 0;\n    if (!any_positive) min_pos = 0;\n    \n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return {0, 0};\n    }\n    \n    int global_max_neg = INT_MIN;\n    int global_min_pos = INT_MAX;\n    bool global_found_neg = false;\n    bool global_found_pos = false;\n    \n    __m256i zero = _mm256_setzero_si256();\n    __m256i int_min_vec = _mm256_set1_epi32(INT_MIN);\n    __m256i int_max_vec = _mm256_set1_epi32(INT_MAX);\n    \n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(lst.data() + i));\n        \n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, v);\n        int any_neg = _mm256_movemask_epi8(neg_mask);\n        if (any_neg) global_found_neg = true;\n        __m256i v_neg = _mm256_blendv_epi8(int_min_vec, v, neg_mask);\n        \n        __m128i v_neg_low = _mm256_castsi256_si128(v_neg);\n        __m128i v_neg_high = _mm256_extracti128_si256(v_neg, 1);\n        __m128i v_neg_max1 = _mm_max_epi32(v_neg_low, v_neg_high);\n        __m128i v_neg_shuf1 = _mm_shuffle_epi32(v_neg_max1, _MM_SHUFFLE(2,3,0,1));\n        __m128i v_neg_max2 = _mm_max_epi32(v_neg_max1, v_neg_shuf1);\n        __m128i v_neg_shuf2 = _mm_shuffle_epi32(v_neg_max2, _MM_SHUFFLE(1,0,3,2));\n        __m128i v_neg_max3 = _mm_max_epi32(v_neg_max2, v_neg_shuf2);\n        int chunk_max_neg = _mm_extract_epi32(v_neg_max3, 0);\n        if (chunk_max_neg > global_max_neg) global_max_neg = chunk_max_neg;\n        \n        __m256i pos_mask = _mm256_cmpgt_epi32(v, zero);\n        int any_pos = _mm256_movemask_epi8(pos_mask);\n        if (any_pos) global_found_pos = true;\n        __m256i v_pos = _mm256_blendv_epi8(int_max_vec, v, pos_mask);\n        \n        __m128i v_pos_low = _mm256_castsi256_si128(v_pos);\n        __m128i v_pos_high = _mm256_extracti128_si256(v_pos, 1);\n        __m128i v_pos_min1 = _mm_min_epi32(v_pos_low, v_pos_high);\n        __m128i v_pos_shuf1 = _mm_shuffle_epi32(v_pos_min1, _MM_SHUFFLE(2,3,0,1));\n        __m128i v_pos_min2 = _mm_min_epi32(v_pos_min1, v_pos_shuf1);\n        __m128i v_pos_shuf2 = _mm_shuffle_epi32(v_pos_min2, _MM_SHUFFLE(1,0,3,2));\n        __m128i v_pos_min3 = _mm_min_epi32(v_pos_min2, v_pos_shuf2);\n        int chunk_min_pos = _mm_extract_epi32(v_pos_min3, 0);\n        if (chunk_min_pos < global_min_pos) global_min_pos = chunk_min_pos;\n    }\n    \n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            global_found_neg = true;\n            if (x > global_max_neg) global_max_neg = x;\n        } \n        else if (x > 0) {\n            global_found_pos = true;\n            if (x < global_min_pos) global_min_pos = x;\n        }\n    }\n    \n    if (!global_found_neg) global_max_neg = 0;\n    if (!global_found_pos) global_min_pos = 0;\n    \n    return {global_max_neg, global_min_pos};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <x86intrin.h>\n\nstatic inline __m256i multiply_epu64_vec(__m256i a, __m256i b) {\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF);\n    __m256i a_low = _mm256_and_si256(a, mask);\n    __m256i b_low = _mm256_and_si256(b, mask);\n    __m256i a_high = _mm256_srli_epi64(a, 32);\n    __m256i b_high = _mm256_srli_epi64(b, 32);\n    \n    __m256i low_prod = _mm256_mul_epu32(a_low, b_low);\n    __m256i cross1 = _mm256_mul_epu32(a_low, b_high);\n    __m256i cross2 = _mm256_mul_epu32(a_high, b_low);\n    \n    cross1 = _mm256_slli_epi64(cross1, 32);\n    cross2 = _mm256_slli_epi64(cross2, 32);\n    \n    __m256i sum = _mm256_add_epi64(low_prod, cross1);\n    return _mm256_add_epi64(sum, cross2);\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n >= 13) {\n        return 0;\n    }\n    \n    uint64_t facts[12];\n    uint64_t current = 1;\n    for (uint64_t k = 1; k <= n; k++) {\n        current *= k;\n        facts[k - 1] = current;\n    }\n    \n    __m256i vec_prod = _mm256_set1_epi64x(1);\n    for (uint64_t i = 0; i < n; i += 4) {\n        uint64_t num_left = n - i;\n        uint64_t temp[4] = {1, 1, 1, 1};\n        for (uint64_t j = 0; j < num_left && j < 4; j++) {\n            temp[j] = facts[i + j];\n        }\n        __m256i data = _mm256_loadu_si256((__m256i*)temp);\n        vec_prod = multiply_epu64_vec(vec_prod, data);\n    }\n    \n    uint64_t result_arr[4];\n    _mm256_storeu_si256((__m256i*)result_arr, vec_prod);\n    uint64_t result = 1;\n    for (int j = 0; j < 4; j++) {\n        result *= result_arr[j];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n >= 22) {\n        return 0;\n    }\n    \n    uint64_t product = 1;\n    for (uint64_t i = 1; i <= n; i += 4) {\n        uint64_t count = (i + 3 <= n) ? 4 : n - i + 1;\n        uint64_t bases[4] = {1, 1, 1, 1};\n        uint64_t exponents[4] = {0, 0, 0, 0};\n        \n        for (uint64_t k = 0; k < count; k++) {\n            bases[k] = i + k;\n            exponents[k] = n - (i + k) + 1;\n        }\n        \n        __m256i base_vec = _mm256_set_epi64x(bases[3], bases[2], bases[1], bases[0]);\n        __m256i exp_vec = _mm256_set_epi64x(exponents[3], exponents[2], exponents[1], exponents[0]);\n        __m256i res_vec = _mm256_set1_epi64x(1);\n        \n        for (int bit = 0; bit < 64; bit++) {\n            uint64_t exp_arr[4], base_arr[4], res_arr[4];\n            _mm256_storeu_si256((__m256i*)exp_arr, exp_vec);\n            _mm256_storeu_si256((__m256i*)base_arr, base_vec);\n            _mm256_storeu_si256((__m256i*)res_arr, res_vec);\n            \n            for (int k = 0; k < 4; k++) {\n                if (exp_arr[k] & 1) {\n                    res_arr[k] = res_arr[k] * base_arr[k];\n                }\n                base_arr[k] = base_arr[k] * base_arr[k];\n                exp_arr[k] >>= 1;\n            }\n            \n            exp_vec = _mm256_loadu_si256((__m256i*)exp_arr);\n            base_vec = _mm256_loadu_si256((__m256i*)base_arr);\n            res_vec = _mm256_loadu_si256((__m256i*)res_arr);\n        }\n        \n        uint64_t res_values[4];\n        _mm256_storeu_si256((__m256i*)res_values, res_vec);\n        \n        uint64_t chunk_product = 1;\n        for (uint64_t k = 0; k < count; k++) {\n            chunk_product *= res_values[k];\n        }\n        \n        product *= chunk_product;\n    }\n    \n    return product;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t factorials[n+1];\n    factorials[0] = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        factorials[i] = factorials[i-1] * i;\n    }\n\n    uint64_t *arr = factorials + 1;\n    uint64_t prod = 1;\n    uint64_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        uint64_t a0 = _mm256_extract_epi64(vec, 0);\n        uint64_t a1 = _mm256_extract_epi64(vec, 1);\n        uint64_t a2 = _mm256_extract_epi64(vec, 2);\n        uint64_t a3 = _mm256_extract_epi64(vec, 3);\n        prod = prod * a0 * a1 * a2 * a3;\n    }\n    for (; i < n; i++) {\n        prod = prod * arr[i];\n    }\n    return prod;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) {\n        return 1;\n    }\n    uint64_t P = 1;\n    for (uint64_t k = 1; k <= n; k++) {\n        uint64_t fact = 1;\n        uint64_t i = 1;\n        while (i <= k) {\n            uint32_t nums[4] = {1, 1, 1, 1};\n            for (int j = 0; j < 4; j++) {\n                if (i + j <= k) {\n                    nums[j] = (uint32_t)(i + j);\n                }\n            }\n            __m128i v = _mm_loadu_si128((__m128i const*)nums);\n            __m128i shuf = _mm_shuffle_epi32(v, _MM_SHUFFLE(2, 3, 0, 1));\n            __m128i mul = _mm_mul_epu32(v, shuf);\n            uint64_t p0 = _mm_extract_epi64(mul, 0);\n            uint64_t p1 = _mm_extract_epi64(mul, 1);\n            fact *= p0 * p1;\n            i += 4;\n        }\n        P *= fact;\n    }\n    return P;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstatic inline __m256i multiply_epu64(__m256i a, __m256i b) {\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF);\n    __m256i a_hi = _mm256_srli_epi64(a, 32);\n    __m256i a_lo = _mm256_and_si256(a, mask);\n    __m256i b_hi = _mm256_srli_epi64(b, 32);\n    __m256i b_lo = _mm256_and_si256(b, mask);\n\n    __m256i prod_lo = _mm256_mul_epu32(a_lo, b_lo);\n    __m256i prod_hi_lo = _mm256_mul_epu32(a_hi, b_lo);\n    __m256i prod_lo_hi = _mm256_mul_epu32(a_lo, b_hi);\n    __m256i sum_hi_lo = _mm256_add_epi64(prod_hi_lo, prod_lo_hi);\n    __m256i sum_shift = _mm256_slli_epi64(sum_hi_lo, 32);\n    return _mm256_add_epi64(prod_lo, sum_shift);\n}\n\nstatic inline __m256i vector_pow(__m256i v_base, __m256i v_expo) {\n    __m256i ones = _mm256_set1_epi64x(1);\n    __m256i counts = _mm256_setzero_si256();\n    __m256i v_terms = ones;\n\n    while (1) {\n        __m256i not_done = _mm256_cmpgt_epi64(v_expo, counts);\n        if (_mm256_testz_si256(not_done, not_done)) {\n            break;\n        }\n\n        __m256i multiplier = _mm256_or_si256(\n            _mm256_and_si256(not_done, v_base),\n            _mm256_andnot_si256(not_done, ones)\n        );\n        v_terms = multiply_epu64(v_terms, multiplier);\n        counts = _mm256_add_epi64(counts, _mm256_and_si256(not_done, ones));\n    }\n    return v_terms;\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) {\n        return 1;\n    }\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i += 4) {\n        uint64_t bases[4] = {1, 1, 1, 1};\n        uint64_t exponents[4] = {0, 0, 0, 0};\n        for (int j = 0; j < 4; j++) {\n            if (i + j <= n) {\n                bases[j] = i + j;\n                exponents[j] = n - (i + j) + 1;\n            }\n        }\n\n        __m256i v_base = _mm256_setr_epi64x(bases[0], bases[1], bases[2], bases[3]);\n        __m256i v_expo = _mm256_setr_epi64x(exponents[0], exponents[1], exponents[2], exponents[3]);\n        __m256i v_terms = vector_pow(v_base, v_expo);\n\n        uint64_t terms[4];\n        _mm256_storeu_si256((__m256i*)terms, v_terms);\n        for (int j = 0; j < 4; j++) {\n            if (i + j <= n) {\n                result *= terms[j];\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstddef>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n\n    static const __m256i mask3_table[12] = {\n        _mm256_setr_epi32(-1, 0, 0, -1, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, -1, 0, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, -1, 0, 0, -1),\n        _mm256_setr_epi32(-1, 0, 0, -1, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, -1, 0, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, -1, 0, 0, -1),\n        _mm256_setr_epi32(-1, 0, 0, -1, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, -1, 0, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, -1, 0, 0, -1),\n        _mm256_setr_epi32(-1, 0, 0, -1, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, -1, 0, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, -1, 0, 0, -1)\n    };\n\n    static const __m256i mask4_table[12] = {\n        _mm256_setr_epi32(0, 0, 0, 0, -1, 0, 0, 0),\n        _mm256_setr_epi32(0, 0, 0, -1, 0, 0, 0, -1),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, 0, -1, 0, 0),\n        _mm256_setr_epi32(-1, 0, 0, 0, -1, 0, 0, 0),\n        _mm256_setr_epi32(0, 0, 0, -1, 0, 0, 0, 0),\n        _mm256_setr_epi32(0, 0, -1, 0, 0, 0, 0, 0),\n        _mm256_setr_epi32(0, -1, 0, 0, 0, 0, 0, 0),\n        _mm256_setr_epi32(-1, 0, 0, 0, 0, 0, 0, 0),\n        _mm256_setr_epi32(0, 0, 0, 0, 0, 0, 0, -1),\n        _mm256_setr_epi32(0, 0, 0, 0, 0, 0, -1, 0),\n        _mm256_setr_epi32(0, 0, 0, 0, 0, -1, 0, 0)\n    };\n\n    __m256i v_sum = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        int residue = i % 12;\n        __m256i mask3 = mask3_table[residue];\n        __m256i mask4 = mask4_table[residue];\n        \n        __m256i v_square = _mm256_mullo_epi32(v, v);\n        __m256i v_cube = _mm256_mullo_epi32(v_square, v);\n        \n        __m256i v_temp = _mm256_blendv_epi8(v, v_cube, mask4);\n        __m256i v_transformed = _mm256_blendv_epi8(v_temp, v_square, mask3);\n        \n        v_sum = _mm256_add_epi32(v_sum, v_transformed);\n    }\n\n    __m128i low = _mm256_castsi256_si128(v_sum);\n    __m128i high = _mm256_extracti128_si256(v_sum, 1);\n    __m128i sum128 = _mm_add_epi32(low, high);\n    \n    __m128i shuf1 = _mm_shuffle_epi32(sum128, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sum1 = _mm_add_epi32(sum128, shuf1);\n    __m128i shuf2 = _mm_shuffle_epi32(sum1, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128i sum2 = _mm_add_epi32(sum1, shuf2);\n    int result = _mm_cvtsi128_si32(sum2);\n\n    for (; i < n; ++i) {\n        int x = lst[i];\n        if (i % 3 == 0) {\n            result += x * x;\n        } else if (i % 4 == 0) {\n            result += x * x * x;\n        } else {\n            result += x;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int total = 0;\n    int n = lst.size();\n    if (n == 0) \n        return 0;\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(lst.data() + i));\n        \n        int base_mod = i % 12;\n        int mod0 = (base_mod) % 12;\n        int mod1 = (base_mod + 1) % 12;\n        int mod2 = (base_mod + 2) % 12;\n        int mod3 = (base_mod + 3) % 12;\n        int mod4 = (base_mod + 4) % 12;\n        int mod5 = (base_mod + 5) % 12;\n        int mod6 = (base_mod + 6) % 12;\n        int mod7 = (base_mod + 7) % 12;\n        \n        __m256i mod12_vec = _mm256_setr_epi32(mod0, mod1, mod2, mod3, mod4, mod5, mod6, mod7);\n        \n        __m256i zero = _mm256_set1_epi32(0);\n        __m256i three = _mm256_set1_epi32(3);\n        __m256i four = _mm256_set1_epi32(4);\n        __m256i six = _mm256_set1_epi32(6);\n        __m256i eight = _mm256_set1_epi32(8);\n        __m256i nine = _mm256_set1_epi32(9);\n        \n        __m256i mask0 = _mm256_cmpeq_epi32(mod12_vec, zero);\n        __m256i mask3 = _mm256_cmpeq_epi32(mod12_vec, three);\n        __m256i mask6 = _mm256_cmpeq_epi32(mod12_vec, six);\n        __m256i mask9 = _mm256_cmpeq_epi32(mod12_vec, nine);\n        __m256i mask_square = _mm256_or_si256(mask0, _mm256_or_si256(mask3, _mm256_or_si256(mask6, mask9)));\n        \n        __m256i mask4 = _mm256_cmpeq_epi32(mod12_vec, four);\n        __m256i mask8 = _mm256_cmpeq_epi32(mod12_vec, eight);\n        __m256i mask_cube = _mm256_or_si256(mask4, mask8);\n        \n        __m256i square = _mm256_mullo_epi32(data, data);\n        __m256i cube = _mm256_mullo_epi32(square, data);\n        \n        __m256i temp = _mm256_blendv_epi8(data, square, mask_square);\n        __m256i res = _mm256_blendv_epi8(temp, cube, mask_cube);\n        \n        sum_vec = _mm256_add_epi32(sum_vec, res);\n    }\n    \n    __m128i low = _mm256_castsi256_si128(sum_vec);\n    __m128i high = _mm256_extractf128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi32(low, high);\n    \n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    total = _mm_extract_epi32(sum128, 0);\n    \n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value = value * value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        total += value;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n\n    int total = 0;\n    int i = 0;\n\n    if (n >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; i <= n - 8; i += 8) {\n            int base = i;\n            int base_mod = base % 12;\n            __m256i v = _mm256_loadu_si256((__m256i const*)(lst.data() + base));\n\n            __m256i offsets = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n            __m256i base_v = _mm256_set1_epi32(base_mod);\n            __m256i res_vec = _mm256_add_epi32(base_v, offsets);\n\n            __m256i cmp_ge12 = _mm256_cmpgt_epi32(res_vec, _mm256_set1_epi32(11));\n            __m256i subtract = _mm256_and_si256(cmp_ge12, _mm256_set1_epi32(12));\n            res_vec = _mm256_sub_epi32(res_vec, subtract);\n\n            __m256i mask_square = _mm256_setzero_si256();\n            mask_square = _mm256_or_si256(mask_square, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(0)));\n            mask_square = _mm256_or_si256(mask_square, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(3)));\n            mask_square = _mm256_or_si256(mask_square, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(6)));\n            mask_square = _mm256_or_si256(mask_square, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(9)));\n\n            __m256i mask_cube = _mm256_setzero_si256();\n            mask_cube = _mm256_or_si256(mask_cube, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(4)));\n            mask_cube = _mm256_or_si256(mask_cube, _mm256_cmpeq_epi32(res_vec, _mm256_set1_epi32(8)));\n\n            __m256i v_square = _mm256_mullo_epi32(v, v);\n            __m256i v_cube = _mm256_mullo_epi32(v, v_square);\n\n            __m256i temp = _mm256_blendv_epi8(v, v_square, mask_square);\n            __m256i result = _mm256_blendv_epi8(temp, v_cube, mask_cube);\n\n            sum_vec = _mm256_add_epi32(sum_vec, result);\n        }\n\n        __m128i low128 = _mm256_castsi256_si128(sum_vec);\n        __m128i high128 = _mm256_extracti128_si256(sum_vec, 1);\n        __m128i sum128 = _mm_add_epi32(low128, high128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        total = _mm_cvtsi128_si32(sum128);\n    }\n\n    for (; i < n; i++) {\n        int x = lst[i];\n        if (i % 3 == 0) {\n            total += x * x;\n        }\n        else if (i % 4 == 0) {\n            total += x * x * x;\n        }\n        else {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) \n        return 0;\n\n    static bool precomputed = false;\n    static __m256i mask3_vectors[12];\n    static __m256i mask4_vectors[12];\n\n    if (!precomputed) {\n        int mask3_pattern[12] = {1,0,0,1,0,0,1,0,0,1,0,0};\n        int mask4_pattern[12] = {0,0,0,0,1,0,0,0,1,0,0,0};\n        for (int offset = 0; offset < 12; offset++) {\n            int m3[8], m4[8];\n            for (int i = 0; i < 8; i++) {\n                int r = (offset + i) % 12;\n                m3[i] = mask3_pattern[r] ? -1 : 0;\n                m4[i] = mask4_pattern[r] ? -1 : 0;\n            }\n            mask3_vectors[offset] = _mm256_loadu_si256((const __m256i*)m3);\n            mask4_vectors[offset] = _mm256_loadu_si256((const __m256i*)m4);\n        }\n        precomputed = true;\n    }\n\n    __m256i total_vec = _mm256_setzero_si256();\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        int offset = i % 12;\n        __m256i in = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i mask3_vec = mask3_vectors[offset];\n        __m256i mask4_vec = mask4_vectors[offset];\n\n        __m256i squared = _mm256_mullo_epi32(in, in);\n        __m256i cubed = _mm256_mullo_epi32(squared, in);\n\n        __m256i mask3or4 = _mm256_or_si256(mask3_vec, mask4_vec);\n        __m256i not_mask3or4 = _mm256_andnot_si256(mask3or4, _mm256_set1_epi32(-1));\n\n        __m256i part1 = _mm256_and_si256(not_mask3or4, in);\n        __m256i part2 = _mm256_and_si256(mask3_vec, squared);\n        __m256i part3 = _mm256_and_si256(mask4_vec, cubed);\n\n        __m256i res = _mm256_or_si256(part1, _mm256_or_si256(part2, part3));\n        total_vec = _mm256_add_epi32(total_vec, res);\n    }\n\n    int temp[8] = {0};\n    _mm256_storeu_si256((__m256i*)temp, total_vec);\n    int sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (; i < n; i++) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i v_sum = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        int base_mod = i % 12;\n        __m256i v_base_mod = _mm256_set1_epi32(base_mod);\n        __m256i v_offsets = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n        __m256i v_mod12 = _mm256_add_epi32(v_base_mod, v_offsets);\n        __m256i v_ge12 = _mm256_cmpgt_epi32(v_mod12, _mm256_set1_epi32(11));\n        v_mod12 = _mm256_sub_epi32(v_mod12, _mm256_and_si256(_mm256_set1_epi32(12), v_ge12));\n\n        __m256i cond1 = _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(0));\n        cond1 = _mm256_or_si256(cond1, _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(3)));\n        cond1 = _mm256_or_si256(cond1, _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(6)));\n        cond1 = _mm256_or_si256(cond1, _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(9)));\n\n        __m256i cond2 = _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(4));\n        cond2 = _mm256_or_si256(cond2, _mm256_cmpeq_epi32(v_mod12, _mm256_set1_epi32(8)));\n\n        __m256i v_square = _mm256_mullo_epi32(v, v);\n        __m256i v_cube = _mm256_mullo_epi32(v_square, v);\n\n        __m256i temp = _mm256_blendv_epi8(v, v_square, cond1);\n        __m256i result = _mm256_blendv_epi8(temp, v_cube, cond2);\n\n        v_sum = _mm256_add_epi32(v_sum, result);\n    }\n\n    __m128i low_sum = _mm256_castsi256_si128(v_sum);\n    __m128i high_sum = _mm256_extracti128_si256(v_sum, 1);\n    __m128i sum128 = _mm_add_epi32(low_sum, high_sum);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    int total = _mm_extract_epi32(sum128, 0);\n\n    for (; i < n; ++i) {\n        int idx = i;\n        int val = lst[i];\n        if (idx % 3 == 0) {\n            total += val * val;\n        } else if (idx % 4 == 0) {\n            total += val * val * val;\n        } else {\n            total += val;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int total_count = 0;\n    int i = 0;\n    int size = nums.size();\n    for (; i + 8 <= size; i += 8) {\n        __m256i vn = _mm256_loadu_si256((const __m256i*)(nums.data() + i));\n        __m256i vcond1 = _mm256_cmpgt_epi32(vn, _mm256_set1_epi32(10));\n        __m256i vabs = _mm256_abs_epi32(vn);\n        __m256 vf = _mm256_cvtepi32_ps(vabs);\n        __m256 vd = _mm256_div_ps(vf, _mm256_set1_ps(10.0f));\n        __m256i vq = _mm256_cvttps_epi32(vd);\n        __m256i vq10 = _mm256_mullo_epi32(vq, _mm256_set1_epi32(10));\n        __m256i vrem = _mm256_sub_epi32(vabs, vq10);\n        __m256i vlowbit = _mm256_and_si256(vrem, _mm256_set1_epi32(1));\n        __m256i vcond2a = _mm256_cmpeq_epi32(vlowbit, _mm256_set1_epi32(1));\n        __m256i vmask = _mm256_and_si256(vcond1, vcond2a);\n        alignas(32) int vmask_arr[8];\n        alignas(32) int vabs_arr[8];\n        _mm256_store_si256((__m256i*)vmask_arr, vmask);\n        _mm256_store_si256((__m256i*)vabs_arr, vabs);\n        for (int j = 0; j < 8; j++) {\n            if (vmask_arr[j]) {\n                int x = vabs_arr[j];\n                while (x >= 10) {\n                    x /= 10;\n                }\n                if (x & 1) {\n                    total_count++;\n                }\n            }\n        }\n    }\n    for (; i < size; i++) {\n        int n = nums[i];\n        if (n > 10) {\n            int last_digit = std::abs(n) % 10;\n            if (last_digit & 1) {\n                int x = std::abs(n);\n                while (x >= 10) {\n                    x /= 10;\n                }\n                if (x & 1) {\n                    total_count++;\n                }\n            }\n        }\n    }\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\ninline __m256i div10_epi32(__m256i v) {\n    __m256 vf = _mm256_cvtepi32_ps(v);\n    vf = _mm256_div_ps(vf, _mm256_set1_ps(10.0f));\n    return _mm256_cvttps_epi32(vf);\n}\n\ninline __m256i mod10_epi32(__m256i v) {\n    __m256i quot = div10_epi32(v);\n    __m256i rem = _mm256_sub_epi32(v, _mm256_mullo_epi32(quot, _mm256_set1_epi32(10)));\n    return rem;\n}\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) \n        return 0;\n\n    int count = 0;\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)&nums[i]);\n        __m256i mask1 = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(10));\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m256i last_digit = mod10_epi32(abs_v);\n        __m256i current = abs_v;\n        __m256i mask_loop = _mm256_cmpgt_epi32(current, _mm256_set1_epi32(9));\n        int iter = 0;\n        while (iter < 10) {\n            if (_mm256_testz_si256(mask_loop, mask_loop))\n                break;\n            __m256i next = div10_epi32(current);\n            current = _mm256_blendv_epi8(current, next, mask_loop);\n            mask_loop = _mm256_cmpgt_epi32(current, _mm256_set1_epi32(9));\n            iter++;\n        }\n        __m256i last_odd_mask = _mm256_cmpeq_epi32(_mm256_and_si256(last_digit, _mm256_set1_epi32(1)), _mm256_set1_epi32(1));\n        __m256i first_odd_mask = _mm256_cmpeq_epi32(_mm256_and_si256(current, _mm256_set1_epi32(1)), _mm256_set1_epi32(1));\n        __m256i mask2 = _mm256_and_si256(last_odd_mask, first_odd_mask);\n        __m256i total_mask = _mm256_and_si256(mask1, mask2);\n        int bitmask = _mm256_movemask_ps(_mm256_castsi256_ps(total_mask));\n        count += __builtin_popcount(bitmask & 0xFF);\n    }\n\n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last = abs_num % 10;\n            int first = abs_num;\n            while (first >= 10) {\n                first /= 10;\n            }\n            if ((last % 2 == 1) && (first % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int n = nums.size();\n    if (n == 0) \n        return 0;\n\n    int count = 0;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(10));\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        \n        if (mask == 0) \n            continue;\n\n        __m256i abs_v = _mm256_abs_epi32(v);\n        alignas(32) int avx_abs[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(avx_abs), abs_v);\n\n        for (int j = 0; j < 8; j++) {\n            if (mask & (1 << j)) {\n                int num_abs = avx_abs[j];\n                int last_digit = num_abs % 10;\n                int first_digit = num_abs;\n                while (first_digit >= 10) \n                    first_digit /= 10;\n                \n                if (last_digit % 2 == 1 && first_digit % 2 == 1) \n                    count++;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10) {\n            int num_abs = std::abs(num);\n            int last_digit = num_abs % 10;\n            int first_digit = num_abs;\n            while (first_digit >= 10) \n                first_digit /= 10;\n            \n            if (last_digit % 2 == 1 && first_digit % 2 == 1) \n                count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n    int count = 0;\n    int i = 0;\n\n    static const float powers10f[10] = {\n        1.0f, 10.0f, 100.0f, 1000.0f, 10000.0f, \n        100000.0f, 1000000.0f, 10000000.0f, 100000000.0f, 1000000000.0f\n    };\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)&nums[i]);\n        __m256i ten = _mm256_set1_epi32(10);\n        __m256i mask_gt10 = _mm256_cmpgt_epi32(v, ten);\n\n        __m256i sign = _mm256_srai_epi32(v, 31);\n        __m256i v_abs = _mm256_sub_epi32(_mm256_xor_si256(v, sign), sign);\n\n        __m256 v_abs_f = _mm256_cvtepi32_ps(v_abs);\n        __m256 v_div10 = _mm256_div_ps(v_abs_f, _mm256_set1_ps(10.0f));\n        __m256 v_floor = _mm256_floor_ps(v_div10);\n        __m256 v_remainder = _mm256_sub_ps(v_abs_f, _mm256_mul_ps(v_floor, _mm256_set1_ps(10.0f)));\n        __m256i last_digit = _mm256_cvttps_epi32(v_remainder);\n\n        __m256i digits = _mm256_set1_epi32(1);\n        for (int exp = 1; exp < 10; exp++) {\n            __m256 thresh = _mm256_set1_ps(powers10f[exp]);\n            __m256 mask_ge = _mm256_cmp_ps(v_abs_f, thresh, _CMP_GE_OS);\n            __m256i mask_ge_i = _mm256_castps_si256(mask_ge);\n            __m256i next_digit = _mm256_set1_epi32(exp + 1);\n            digits = _mm256_blendv_epi8(digits, next_digit, mask_ge_i);\n        }\n\n        __m256i indices = _mm256_sub_epi32(digits, _mm256_set1_epi32(1));\n        __m256 v_divisors = _mm256_i32gather_ps(powers10f, indices, 4);\n        __m256 v_first_digit = _mm256_div_ps(v_abs_f, v_divisors);\n        __m256i first_digit = _mm256_cvttps_epi32(v_first_digit);\n\n        __m256i last_odd = _mm256_and_si256(last_digit, _mm256_set1_epi32(1));\n        __m256i first_odd = _mm256_and_si256(first_digit, _mm256_set1_epi32(1));\n        __m256i both_odd = _mm256_and_si256(last_odd, first_odd);\n\n        __m256i mask_result = _mm256_and_si256(both_odd, mask_gt10);\n\n        alignas(32) int res[8];\n        _mm256_store_si256((__m256i*)res, mask_result);\n        for (int j = 0; j < 8; j++) {\n            count += res[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10) {\n            int n_abs = abs(num);\n            int last = n_abs % 10;\n            int first = n_abs;\n            while (first >= 10) {\n                first /= 10;\n            }\n            if ((last % 2 == 1) && (first % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    const int n = nums.size();\n    if (n == 0) return 0;\n    int count = 0;\n    int i = 0;\n    const int thresholds[9] = {10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000};\n\n    for (; i + 7 < n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i mask1 = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(10));\n        __m256i mask2 = _mm256_and_si256(v, _mm256_set1_epi32(1));\n        mask2 = _mm256_cmpeq_epi32(mask2, _mm256_set1_epi32(1));\n        __m256i divisor_vec = _mm256_set1_epi32(thresholds[0]);\n        for (int j = 1; j < 9; j++) {\n            __m256i thresh = _mm256_set1_epi32(thresholds[j]);\n            __m256i mask_update = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(thresholds[j] - 1));\n            divisor_vec = _mm256_blendv_epi8(divisor_vec, thresh, mask_update);\n        }\n        __m256 vf = _mm256_cvtepi32_ps(v);\n        __m256 divisor_f = _mm256_cvtepi32_ps(divisor_vec);\n        __m256 first_digit_f = _mm256_div_ps(vf, divisor_f);\n        __m256i first_digit = _mm256_cvttps_epi32(first_digit_f);\n        __m256i mask3 = _mm256_and_si256(first_digit, _mm256_set1_epi32(1));\n        mask3 = _mm256_cmpeq_epi32(mask3, _mm256_set1_epi32(1));\n        __m256i mask = _mm256_and_si256(mask1, mask2);\n        mask = _mm256_and_si256(mask, mask3);\n        alignas(32) int mask_array[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(mask_array), mask);\n        for (int j = 0; j < 8; j++) {\n            if (mask_array[j]) count++;\n        }\n    }\n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10 && (num % 2) == 1) {\n            int divisor = thresholds[0];\n            for (int j = 1; j < 9; j++) {\n                if (num >= thresholds[j]) divisor = thresholds[j];\n                else break;\n            }\n            int first_digit = num / divisor;\n            if (first_digit % 2 == 1) count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": ""}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count0 = (n + 1) / 3;\n    uint64_t count1 = n - count0;\n    uint64_t total = 0;\n\n    if (count0 >= 3) {\n        total += count0 * (count0 - 1) * (count0 - 2) / 6;\n    }\n    if (count1 >= 3) {\n        total += count1 * (count1 - 1) * (count1 - 2) / 6;\n    }\n\n    __m256i dummy = _mm256_setzero_si256();\n    (void)dummy;\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count0 = (n + 1) / 3;\n    uint64_t count1 = n - count0;\n    uint64_t c0 = 0;\n    uint64_t c1 = 0;\n\n    if (count0 >= 3) {\n        c0 = count0;\n        c0 *= (count0 - 1);\n        c0 /= 2;\n        c0 *= (count0 - 2);\n        c0 /= 3;\n    }\n\n    if (count1 >= 3) {\n        c1 = count1;\n        c1 *= (count1 - 1);\n        c1 /= 2;\n        c1 *= (count1 - 2);\n        c1 /= 3;\n    }\n\n    __m128i v0 = _mm_cvtsi64_si128(c0);\n    __m128i v1 = _mm_cvtsi64_si128(c1);\n    __m128i vsum = _mm_add_epi64(v0, v1);\n    uint64_t ans = _mm_cvtsi128_si64(vsum);\n    return ans;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nalignas(32) static const uint8_t patterns[3][32] = {\n    {1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1},\n    {1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0},\n    {0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1}\n};\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) {\n        return 0;\n    }\n    uint64_t count0 = 0;\n    uint64_t i = 1;\n\n    for (; i + 31 <= n; i += 32) {\n        uint64_t base = i;\n        int r0 = base % 3;\n        __m256i v_pattern = _mm256_loadu_si256((const __m256i*)patterns[r0]);\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi8(v_pattern, v_zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        count0 += _mm_popcnt_u32((unsigned int)mask);\n    }\n\n    for (; i <= n; i++) {\n        if (i % 3 == 2) {\n            count0++;\n        }\n    }\n\n    uint64_t count1 = n - count0;\n    uint64_t triples = 0;\n    if (count0 >= 3) {\n        triples = (count0 * (count0 - 1) * (count0 - 2)) / 6;\n    }\n    if (count1 >= 3) {\n        triples += (count1 * (count1 - 1) * (count1 - 2)) / 6;\n    }\n    return triples;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 2) {\n        return 0;\n    }\n    uint64_t count0 = (n - 2) / 3 + 1;\n    uint64_t count1 = n - count0;\n\n    auto comb = [](uint64_t x) -> uint64_t {\n        if (x < 3) \n            return 0;\n        return x * (x - 1) / 2 * (x - 2) / 3;\n    };\n    return comb(count0) + comb(count1);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n\n    int limit = (int)sqrt((double)n) + 1;\n\n    for (int base = 3; base <= limit; base += 8) {\n        __m128i divs = _mm_set_epi32(base+6, base+4, base+2, base);\n        __m128i limit_vec = _mm_set1_epi32(limit);\n        __m128i mask = _mm_cmple_epi32(divs, limit_vec);\n\n        __m256d divs_double = _mm256_cvtepi32_pd(divs);\n        __m256d n_vec_double = _mm256_set1_pd((double)n);\n        __m256d quot_double = _mm256_div_pd(n_vec_double, divs_double);\n        __m128i quot_int = _mm256_cvttpd_epi32(quot_double);\n\n        __m128i product = _mm_mullo_epi32(divs, quot_int);\n        __m128i n_vec = _mm_set1_epi32(n);\n        __m128i cmp = _mm_cmpeq_epi32(product, n_vec);\n        __m128i final_cmp = _mm_and_si128(cmp, mask);\n\n        if (!_mm_testz_si128(final_cmp, final_cmp)) {\n            return y;\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n\n    int limit = (int)sqrt((double)n) + 1;\n\n    for (int base = 3; base <= limit; base += 16) {\n        __m256d divs1 = _mm256_setr_pd(base, base+2, base+4, base+6);\n        __m256d limit_vec = _mm256_set1_pd(limit + 0.1);\n        __m256d active1 = _mm256_cmp_pd(divs1, limit_vec, _CMP_LE_OQ);\n        __m256d safe_divs1 = _mm256_blendv_pd(_mm256_set1_pd(n+1.0), divs1, active1);\n\n        __m256d n_vec = _mm256_set1_pd((double)n);\n        __m256d quot1 = _mm256_div_pd(n_vec, safe_divs1);\n        quot1 = _mm256_round_pd(quot1, _MM_FROUND_TO_ZERO);\n        __m256d prod1 = _mm256_mul_pd(safe_divs1, quot1);\n        __m256d rem1 = _mm256_sub_pd(n_vec, prod1);\n\n        __m256d zero = _mm256_setzero_pd();\n        __m256d cmp1 = _mm256_cmp_pd(rem1, zero, _CMP_EQ_OQ);\n        int mask1 = _mm256_movemask_pd(cmp1);\n        if (mask1 != 0) {\n            return y;\n        }\n\n        __m256d divs2 = _mm256_setr_pd(base+8, base+10, base+12, base+14);\n        __m256d active2 = _mm256_cmp_pd(divs2, limit_vec, _CMP_LE_OQ);\n        __m256d safe_divs2 = _mm256_blendv_pd(_mm256_set1_pd(n+1.0), divs2, active2);\n\n        __m256d quot2 = _mm256_div_pd(n_vec, safe_divs2);\n        quot2 = _mm256_round_pd(quot2, _MM_FROUND_TO_ZERO);\n        __m256d prod2 = _mm256_mul_pd(safe_divs2, quot2);\n        __m256d rem2 = _mm256_sub_pd(n_vec, prod2);\n\n        __m256d cmp2 = _mm256_cmp_pd(rem2, zero, _CMP_EQ_OQ);\n        int mask2 = _mm256_movemask_pd(cmp2);\n        if (mask2 != 0) {\n            return y;\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) \n        return y;\n    if (n == 2) \n        return x;\n    if (n % 2 == 0) \n        return y;\n    int d = 3;\n    while (d <= n / d) {\n        int last_batch1 = d + 6;\n        if (last_batch1 > n / last_batch1) {\n            break;\n        }\n        __m128i divs1 = _mm_setr_epi32(d, d + 2, d + 4, d + 6);\n        __m256d divs1_d = _mm256_cvtepi32_pd(divs1);\n        __m256d n_vec = _mm256_set1_pd((double)n);\n        __m256d quot1 = _mm256_div_pd(n_vec, divs1_d);\n        __m128i quot1_i = _mm256_cvttpd_epi32(quot1);\n        __m256d quot1_d = _mm256_cvtepi32_pd(quot1_i);\n        __m256d rem1 = _mm256_fnmadd_pd(divs1_d, quot1_d, n_vec);\n        __m256d zero = _mm256_setzero_pd();\n        __m256d mask1 = _mm256_cmp_pd(rem1, zero, _CMP_EQ_OQ);\n        if (_mm256_movemask_pd(mask1) != 0) {\n            return y;\n        }\n        int last_batch2 = d + 14;\n        if (last_batch2 > n / last_batch2) {\n            d += 8;\n            break;\n        }\n        __m128i divs2 = _mm_setr_epi32(d + 8, d + 10, d + 12, d + 14);\n        __m256d divs2_d = _mm256_cvtepi32_pd(divs2);\n        __m256d quot2 = _mm256_div_pd(n_vec, divs2_d);\n        __m128i quot2_i = _mm256_cvttpd_epi32(quot2);\n        __m256d quot2_d = _mm256_cvtepi32_pd(quot2_i);\n        __m256d rem2 = _mm256_fnmadd_pd(divs2_d, quot2_d, n_vec);\n        __m256d mask2 = _mm256_cmp_pd(rem2, zero, _CMP_EQ_OQ);\n        if (_mm256_movemask_pd(mask2) != 0) {\n            return y;\n        }\n        d += 16;\n    }\n    while (d <= n / d) {\n        if (n % d == 0) {\n            return y;\n        }\n        d += 2;\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n    \n    int i_sqrt = (int)sqrt(n);\n    for (int base = 3; base <= i_sqrt; base += 16) {\n        __m256i v_offsets = _mm256_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14);\n        __m256i v_base = _mm256_set1_epi32(base);\n        __m256i v_factors = _mm256_add_epi32(v_base, v_offsets);\n        \n        int factors[8];\n        _mm256_storeu_si256((__m256i*)factors, v_factors);\n        \n        for (int i = 0; i < 8; i++) {\n            if (factors[i] > i_sqrt) break;\n            if (n % factors[i] == 0) return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n\n    int limit = 1;\n    if (n > 1) {\n        while ((long long)limit * limit <= n) {\n            limit++;\n        }\n        limit--;\n    }\n\n    for (int i = 3; i <= limit; i += 16) {\n        __m256i base = _mm256_set1_epi32(i);\n        __m256i offsets = _mm256_set_epi32(14, 12, 10, 8, 6, 4, 2, 0);\n        __m256i divisors = _mm256_add_epi32(base, offsets);\n        int div_array[8] __attribute__((aligned(32)));\n        _mm256_store_si256((__m256i*)div_array, divisors);\n\n        for (int j = 0; j < 8; j++) {\n            if (div_array[j] > limit) {\n                break;\n            }\n            if (n % div_array[j] == 0) {\n                return y;\n            }\n        }\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    size_t i = 0;\n    size_t len = lst.size();\n    __m256i acc_low = _mm256_setzero_si256();\n    __m256i acc_high = _mm256_setzero_si256();\n\n    for (; i + 8 <= len; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 mask_positive = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GE_OQ);\n        __m256 int_part = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO);\n        __m256 mask_int = _mm256_cmp_ps(vec, int_part, _CMP_EQ_OQ);\n        __m256i int32_vec = _mm256_cvttps_epi32(int_part);\n        __m256i one_i32 = _mm256_set1_epi32(1);\n        __m256i and1 = _mm256_and_si256(int32_vec, one_i32);\n        __m256i mask_odd_i32 = _mm256_cmpeq_epi32(and1, one_i32);\n        __m256 mask_odd = _mm256_castsi256_ps(mask_odd_i32);\n        __m256 mask = _mm256_and_ps(mask_positive, _mm256_and_ps(mask_int, mask_odd));\n        __m256i mask_i32 = _mm256_castps_si256(mask);\n        __m256i selected_ints = _mm256_and_si256(int32_vec, mask_i32);\n        __m128i low_128 = _mm256_castsi256_si128(selected_ints);\n        __m128i high_128 = _mm256_extracti128_si256(selected_ints, 1);\n        __m256i low_64 = _mm256_cvtepu32_epi64(low_128);\n        __m256i high_64 = _mm256_cvtepu32_epi64(high_128);\n        __m256i squares_low = _mm256_mul_epu32(low_64, low_64);\n        __m256i squares_high = _mm256_mul_epu32(high_64, high_64);\n        acc_low = _mm256_add_epi64(acc_low, squares_low);\n        acc_high = _mm256_add_epi64(acc_high, squares_high);\n    }\n\n    __m128i low_low = _mm256_extracti128_si256(acc_low, 0);\n    __m128i low_high = _mm256_extracti128_si256(acc_low, 1);\n    __m128i high_low = _mm256_extracti128_si256(acc_high, 0);\n    __m128i high_high = _mm256_extracti128_si256(acc_high, 1);\n    __m128i sum128 = _mm_add_epi64(low_low, low_high);\n    sum128 = _mm_add_epi64(sum128, high_low);\n    sum128 = _mm_add_epi64(sum128, high_high);\n    uint64_t total = _mm_extract_epi64(sum128, 0) + _mm_extract_epi64(sum128, 1);\n\n    for (; i < len; i++) {\n        float x = lst[i];\n        if (x >= 0 && x < 2147483648.0f) {\n            int32_t n = static_cast<int32_t>(x);\n            if (static_cast<float>(n) == x) {\n                if (n & 1) {\n                    uint64_t square = static_cast<uint64_t>(n) * static_cast<uint64_t>(n);\n                    total += square;\n                }\n            }\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) return 0;\n\n    const size_t n = lst.size();\n    const float* data = lst.data();\n    double total = 0.0;\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256 v = _mm256_loadu_ps(data + i);\n        __m256 mask_non_neg = _mm256_cmp_ps(v, _mm256_setzero_ps(), _CMP_GE_OQ);\n        __m256i integer_vec = _mm256_cvttps_epi32(v);\n        __m256 truncated_float = _mm256_cvtepi32_ps(integer_vec);\n        __m256 mask_integer = _mm256_cmp_ps(v, truncated_float, _CMP_EQ_OQ);\n        __m256 mask_valid = _mm256_and_ps(mask_non_neg, mask_integer);\n\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_bits = _mm256_and_si256(integer_vec, one);\n        __m256i mask_odd_int = _mm256_cmpeq_epi32(odd_bits, one);\n        __m256 mask_odd = _mm256_castsi256_ps(mask_odd_int);\n\n        __m256 condition_mask = _mm256_and_ps(mask_valid, mask_odd);\n        __m256 v_square = _mm256_mul_ps(v, v);\n        __m256 selected = _mm256_and_ps(condition_mask, v_square);\n\n        float buffer[8];\n        _mm256_storeu_ps(buffer, selected);\n        for (int j = 0; j < 8; j++) {\n            total += static_cast<double>(buffer[j]);\n        }\n    }\n\n    for (; i < n; i++) {\n        float x = data[i];\n        if (x >= 0 && x == static_cast<float>(static_cast<int32_t>(x))) {\n            int32_t integer = static_cast<int32_t>(x);\n            if (integer % 2 != 0) {\n                total += static_cast<double>(x) * static_cast<double>(x);\n            }\n        }\n    }\n\n    return static_cast<uint64_t>(total);\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n    \n    __m256d accum_d = _mm256_setzero_pd();\n    size_t i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        \n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask_positive = _mm256_cmp_ps(data, zero, _CMP_GE_OQ);\n        \n        __m256 integer_part = _mm256_round_ps(data, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        __m256 mask_integer = _mm256_cmp_ps(data, integer_part, _CMP_EQ_OQ);\n        __m256 mask_valid = _mm256_and_ps(mask_positive, mask_integer);\n        \n        __m256 half = _mm256_mul_ps(integer_part, _mm256_set1_ps(0.5f));\n        __m256 half_rounded = _mm256_round_ps(half, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        __m256 remainder = _mm256_sub_ps(integer_part, _mm256_mul_ps(half_rounded, _mm256_set1_ps(2.0f)));\n        __m256 mask_odd = _mm256_cmp_ps(remainder, _mm256_set1_ps(1.0f), _CMP_EQ_OQ);\n        \n        __m256 mask = _mm256_and_ps(mask_valid, mask_odd);\n        __m256 squares = _mm256_mul_ps(data, data);\n        __m256 masked_squares = _mm256_and_ps(squares, mask);\n        \n        __m128 low_squares = _mm256_castps256_ps128(masked_squares);\n        __m128 high_squares = _mm256_extractf128_ps(masked_squares, 1);\n        \n        __m256d low_d = _mm256_cvtps_pd(low_squares);\n        __m256d high_d = _mm256_cvtps_pd(high_squares);\n        \n        accum_d = _mm256_add_pd(accum_d, low_d);\n        accum_d = _mm256_add_pd(accum_d, high_d);\n    }\n    \n    double temp[4];\n    _mm256_storeu_pd(temp, accum_d);\n    double vector_sum = temp[0] + temp[1] + temp[2] + temp[3];\n    uint64_t total = static_cast<uint64_t>(vector_sum + 0.5);\n    \n    for (; i < n; ++i) {\n        float x = lst[i];\n        if (x >= 0.0f) {\n            float truncated = truncf(x);\n            if (x == truncated) {\n                uint64_t n_val = static_cast<uint64_t>(truncated);\n                if (n_val % 2 == 1) {\n                    total += n_val * n_val;\n                }\n            }\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    if (size == 0) {\n        return 0;\n    }\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 x = _mm256_loadu_ps(&lst[i]);\n        \n        __m256 positive_mask = _mm256_cmp_ps(x, _mm256_setzero_ps(), _CMP_GT_OQ);\n        \n        __m256 truncated = _mm256_round_ps(x, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        __m256 integer_mask = _mm256_cmp_ps(truncated, x, _CMP_EQ_OQ);\n        \n        __m256i int_vec = _mm256_cvttps_epi32(truncated);\n        __m256i and1 = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i odd_mask_i = _mm256_cmpeq_epi32(and1, _mm256_set1_epi32(1));\n        __m256 odd_mask = _mm256_castsi256_ps(odd_mask_i);\n        \n        __m256 mask = _mm256_and_ps(positive_mask, _mm256_and_ps(integer_mask, odd_mask));\n        unsigned bitmask = _mm256_movemask_ps(mask);\n        \n        alignas(32) int32_t int32_arr[8];\n        _mm256_store_si256((__m256i*)int32_arr, int_vec);\n        \n        for (int j = 0; j < 8; j++) {\n            if (bitmask & (1 << j)) {\n                int32_t num = int32_arr[j];\n                result += (uint64_t)num * (uint64_t)num;\n            }\n        }\n    }\n    \n    for (; i < size; i++) {\n        float x = lst[i];\n        if (x > 0 && x == static_cast<int32_t>(x)) {\n            int32_t num = static_cast<int32_t>(x);\n            if (num % 2 != 0) {\n                result += (uint64_t)num * (uint64_t)num;\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    uint64_t total = 0;\n    const size_t size = lst.size();\n    const float int_max = static_cast<float>(INT_MAX);\n    const __m256 v_zero = _mm256_setzero_ps();\n    const __m256 v_int_max = _mm256_set1_ps(int_max);\n    const __m256i v_ones = _mm256_set1_epi32(1);\n    \n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        __m256 mask_non_neg = _mm256_cmp_ps(vec, v_zero, _CMP_GE_OQ);\n        __m256 mask_upper = _mm256_cmp_ps(vec, v_int_max, _CMP_LE_OQ);\n        __m256 mask_range = _mm256_and_ps(mask_non_neg, mask_upper);\n        \n        __m256 trun = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        __m256 mask_int = _mm256_cmp_ps(vec, trun, _CMP_EQ_OQ);\n        __m256 mask_non_neg_int = _mm256_and_ps(mask_range, mask_int);\n        \n        __m256i ints = _mm256_cvtps_epi32(vec);\n        __m256i and_result = _mm256_and_si256(ints, v_ones);\n        __m256i odd_mask = _mm256_cmpeq_epi32(and_result, v_ones);\n        __m256 mask_odd_float = _mm256_castsi256_ps(odd_mask);\n        \n        __m256 final_mask = _mm256_and_ps(mask_non_neg_int, mask_odd_float);\n        int valid_mask = _mm256_movemask_ps(final_mask);\n        \n        alignas(32) int int_arr[8];\n        _mm256_store_si256((__m256i*)int_arr, ints);\n        \n        for (int j = 0; j < 8; ++j) {\n            if (valid_mask & (1 << j)) {\n                int val = int_arr[j];\n                total += static_cast<uint64_t>(val) * val;\n            }\n        }\n    }\n    \n    for (; i < size; ++i) {\n        float x = lst[i];\n        if (x >= 0 && x <= int_max && x == static_cast<int>(x)) {\n            int int_val = static_cast<int>(x);\n            if (int_val % 2 != 0) {\n                total += static_cast<uint64_t>(int_val) * int_val;\n            }\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    for (; i + 7 < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n        \n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n    const size_t vec_size = 8;\n\n    for (; i + vec_size <= n; i += vec_size) {\n        __m256i v_game = _mm256_loadu_si256((const __m256i*)(game.data() + i));\n        __m256i v_guess = _mm256_loadu_si256((const __m256i*)(guess.data() + i));\n        __m256i v_diff = _mm256_sub_epi32(v_game, v_guess);\n        __m256i v_abs = _mm256_abs_epi32(v_diff);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), v_abs);\n    }\n\n    for (; i < n; i++) {\n        int diff = game[i] - guess[i];\n        result[i] = std::abs(diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i v_game = _mm256_loadu_si256((const __m256i*)(game.data() + i));\n            __m256i v_guess = _mm256_loadu_si256((const __m256i*)(guess.data() + i));\n            __m256i v_diff = _mm256_sub_epi32(v_game, v_guess);\n            __m256i v_abs = _mm256_abs_epi32(v_diff);\n            _mm256_storeu_si256((__m256i*)(result.data() + i), v_abs);\n        }\n    }\n\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = diff < 0 ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    \n    if (n == 0) {\n        return result;\n    }\n\n    size_t i = 0;\n    const size_t simd_size = 8;\n    const size_t aligned_end = n - (n % simd_size);\n    \n    for (; i < aligned_end; i += simd_size) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff < 0) ? -diff : diff;\n    }\n\n    return result;\n}\n```"}

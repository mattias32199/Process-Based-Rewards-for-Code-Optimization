{"task_id": "SimdBench_0_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t aligned_length = length & ~(size_t)1;\n    size_t i = 0;\n\n    for (; i < aligned_length; i += 2) {\n        int64_t m0 = -(int64_t)mask[i];\n        int64_t m1 = -(int64_t)mask[i+1];\n        __m128i vmask = _mm_set_epi64x(m1, m0);\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i vdst = _mm_loadu_si128((const __m128i*)(dst + i));\n        __m128i res = _mm_or_si128(_mm_and_si128(vmask, vsrc), \n                                   _mm_andnot_si128(vmask, vdst));\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        uint16_t mask_bits = (uint16_t)(mask[i]) | ((uint16_t)(mask[i+1]) << 8);\n        int64_t mask0 = (int64_t)(-(int64_t)(mask_bits & 0xFF)) >> 63;\n        int64_t mask1 = (int64_t)(-(int64_t)((mask_bits >> 8) & 0xFF)) >> 63;\n        __m128i mask_vec = _mm_set_epi64x(mask1, mask0);\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i*)(dst + i));\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(mask_vec, src_vec),\n            _mm_andnot_si128(mask_vec, dst_vec)\n        );\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    if (i < length) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        bool m0 = mask[i];\n        bool m1 = mask[i + 1];\n        __m128i mask_vec = _mm_set_epi64x(m1 ? -1 : 0, m0 ? -1 : 0);\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const *)(dst + i));\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(src_vec, mask_vec),\n            _mm_andnot_si128(mask_vec, dst_vec)\n        );\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n    if (i < length) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    if (length == 0) {\n        return;\n    }\n\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        uint16_t mask16;\n        __builtin_memcpy(&mask16, mask + i, sizeof(uint16_t));\n\n        int64_t mask0 = -((int64_t)(mask16 & 0xFF));\n        int64_t mask1 = -((int64_t)((mask16 >> 8) & 0xFF));\n\n        __m128i mask_vec = _mm_set_epi64x(mask1, mask0);\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((const __m128i*)(dst + i));\n\n        __m128i not_mask = _mm_andnot_si128(mask_vec, dst_vec);\n        __m128i src_masked = _mm_and_si128(mask_vec, src_vec);\n        __m128i result = _mm_or_si128(src_masked, not_mask);\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        int64_t mask0 = -static_cast<int64_t>(mask[i]);\n        int64_t mask1 = -static_cast<int64_t>(mask[i+1]);\n        __m128i mask_vec = _mm_set_epi64x(mask1, mask0);\n        __m128i src_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i dst_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(dst + i));\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(mask_vec, src_vec),\n            _mm_andnot_si128(mask_vec, dst_vec)\n        );\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), res);\n    }\n    if (i < length && mask[i]) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    if (length >= 2) {\n        for (; i <= length - 2; i += 2) {\n            __m128i x_vec = _mm_loadl_epi64((const __m128i*)(src + i));\n            __m128d x_double = _mm_cvtepi32_pd(x_vec);\n            __m128d scale_double = _mm_set1_pd((double)scale);\n            __m128d scaled = _mm_mul_pd(x_double, scale_double);\n            \n            __m128d scaled_div8 = _mm_mul_pd(scaled, _mm_set1_pd(1.0 / 8.0));\n            __m128d tmp1 = _mm_floor_pd(scaled_div8);\n            \n            __m128d tmp2 = _mm_mul_pd(scaled, _mm_set1_pd(8.0));\n            \n            __m128d product = _mm_mul_pd(tmp1, tmp2);\n            __m128d sum = _mm_add_pd(scaled, product);\n            \n            __m128d result_double = _mm_mul_pd(sum, _mm_set1_pd(1.0 / 7.0));\n            __m128i result_int = _mm_cvttpd_epi32(result_double);\n            \n            _mm_storel_epi64((__m128i*)(dst + i), result_int);\n        }\n    }\n    if (i < length) {\n        int element = src[i];\n        int scaled_val = element * scale;\n        int tmp1_val = scaled_val >> 3;\n        int tmp2_val = scaled_val << 3;\n        int product_val = tmp1_val * tmp2_val;\n        int sum_val = scaled_val + product_val;\n        dst[i] = sum_val / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m128i v_scale = _mm_set1_epi32(scale);\n        __m128d v_divisor = _mm_set1_pd(7.0);\n        \n        for (; i + 4 <= length; i += 4) {\n            __m128i v_src = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i v_x = _mm_mullo_epi32(v_src, v_scale);\n            __m128i v_tmp1 = _mm_srai_epi32(v_x, 3);\n            __m128i v_tmp2 = _mm_slli_epi32(v_x, 3);\n            __m128i v_product = _mm_mullo_epi32(v_tmp1, v_tmp2);\n            __m128i v_sum = _mm_add_epi32(v_x, v_product);\n            \n            __m128i sign = _mm_srai_epi32(v_sum, 31);\n            __m128i v_abs = _mm_sub_epi32(_mm_xor_si128(v_sum, sign), sign);\n            \n            __m128i v_abs_hi = _mm_srli_si128(v_abs, 8);\n            __m128d v_low = _mm_cvtepi32_pd(v_abs);\n            __m128d v_high = _mm_cvtepi32_pd(v_abs_hi);\n            \n            v_low = _mm_div_pd(v_low, v_divisor);\n            v_high = _mm_div_pd(v_high, v_divisor);\n            \n            __m128i v_res_low = _mm_cvttpd_epi32(v_low);\n            __m128i v_res_high = _mm_cvttpd_epi32(v_high);\n            v_res_high = _mm_slli_si128(v_res_high, 8);\n            \n            __m128i v_res = _mm_or_si128(v_res_low, v_res_high);\n            v_res = _mm_sub_epi32(_mm_xor_si128(v_res, sign), sign);\n            \n            _mm_storeu_si128((__m128i*)(dst + i), v_res);\n        }\n    }\n    for (; i < length; ++i) {\n        int x = src[i] * scale;\n        int tmp1 = x >> 3;\n        int tmp2 = x << 3;\n        int product = tmp1 * tmp2;\n        int sum = x + product;\n        dst[i] = sum / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m128i scale_vec = _mm_set1_epi32(scale);\n        for (; i + 4 <= length; i += 4) {\n            __m128i data = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i x = _mm_mullo_epi32(data, scale_vec);\n            __m128i tmp1 = _mm_srai_epi32(x, 3);\n            __m128i tmp2 = _mm_slli_epi32(x, 3);\n            __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n            __m128i total = _mm_add_epi32(x, product);\n\n            __m128d low_d = _mm_cvtepi32_pd(total);\n            __m128i high_part = _mm_castps_si128(_mm_movehl_ps(_mm_setzero_ps(), _mm_castsi128_ps(total)));\n            __m128d high_d = _mm_cvtepi32_pd(high_part);\n            __m128d div7_low = _mm_div_pd(low_d, _mm_set1_pd(7.0));\n            __m128d div7_high = _mm_div_pd(high_d, _mm_set1_pd(7.0));\n            __m128i low_int = _mm_cvttpd_epi32(div7_low);\n            __m128i high_int = _mm_cvttpd_epi32(div7_high);\n            high_int = _mm_slli_si128(high_int, 8);\n            __m128i result = _mm_or_si128(low_int, high_int);\n            _mm_storeu_si128((__m128i*)(dst + i), result);\n        }\n    }\n    for (; i < length; i++) {\n        int x = src[i] * scale;\n        int tmp1 = x >> 3;\n        int tmp2 = x << 3;\n        int total = x + tmp1 * tmp2;\n        dst[i] = total / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i v = _mm_mullo_epi32(v_src, v_scale);\n        __m128i tmp1 = _mm_srai_epi32(v, 3);\n        __m128i tmp2 = _mm_slli_epi32(v, 3);\n        __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(v, product);\n        alignas(16) int sum_arr[4];\n        _mm_store_si128((__m128i*)sum_arr, sum);\n        for (int j = 0; j < 4; j++) {\n            dst[i + j] = sum_arr[j] / 7;\n        }\n    }\n    for (; i < length; i++) {\n        int n = src[i] * scale;\n        int tmp1 = n >> 3;\n        int tmp2 = n << 3;\n        int product = tmp1 * tmp2;\n        int sum = n + product;\n        dst[i] = sum / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m128i scale_vec = _mm_set1_epi32(scale);\n        for (; i <= length - 4; i += 4) {\n            __m128i vec = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i mul_scaled = _mm_mullo_epi32(vec, scale_vec);\n            __m128i tmp1 = _mm_srai_epi32(mul_scaled, 3);\n            __m128i tmp2 = _mm_slli_epi32(mul_scaled, 3);\n            __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n            __m128i sum = _mm_add_epi32(mul_scaled, product);\n            \n            alignas(16) int temp[4];\n            _mm_store_si128((__m128i*)temp, sum);\n            for (int j = 0; j < 4; j++) {\n                temp[j] /= 7;\n            }\n            __m128i result = _mm_load_si128((__m128i*)temp);\n            _mm_storeu_si128((__m128i*)(dst + i), result);\n        }\n    }\n    for (; i < length; i++) {\n        int mul = src[i] * scale;\n        int t1 = mul >> 3;\n        int t2 = mul << 3;\n        int prod = t1 * t2;\n        int s = mul + prod;\n        dst[i] = s / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r + 1 < rows; r += 2) {\n        __m128d v = _mm_setr_pd(src[r * cols + stride], src[(r + 1) * cols + stride]);\n        _mm_storeu_pd(dst + r, v);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r < rows - (rows % 2); r += 2) {\n        const double *addr0 = src + r * cols + stride;\n        const double *addr1 = src + (r + 1) * cols + stride;\n        __m128d v0 = _mm_load_sd(addr0);\n        __m128d v1 = _mm_load_sd(addr1);\n        __m128d packed = _mm_unpacklo_pd(v0, v1);\n        _mm_storeu_pd(dst + r, packed);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    if (rows == 0) return;\n\n    size_t i = 0;\n    for (; i + 1 < rows; i += 2) {\n        __m128d v = _mm_setr_pd(src[i * cols + stride], src[(i + 1) * cols + stride]);\n        _mm_storeu_pd(dst + i, v);\n    }\n\n    if (i < rows) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n    for (; r + 1 < rows; r += 2) {\n        __m128d v0 = _mm_load_sd(src + r * cols + stride);\n        __m128d v1 = _mm_load_sd(src + (r + 1) * cols + stride);\n        __m128d combined = _mm_unpacklo_pd(v0, v1);\n        _mm_storeu_pd(dst + r, combined);\n    }\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    if (rows == 0) {\n        return;\n    }\n\n    size_t r = 0;\n    for (; r < rows - 1; r += 2) {\n        double val0 = src[r * cols + stride];\n        double val1 = src[(r + 1) * cols + stride];\n        __m128d vec = _mm_setr_pd(val0, val1);\n        _mm_storeu_pd(&dst[r], vec);\n    }\n\n    if (r < rows) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i*)(indices + i));\n        int idx0 = _mm_cvtsi128_si32(idx_vec);\n        int idx1 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 4));\n        int idx2 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 8));\n        int idx3 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 12));\n        __m128 res = _mm_set_ps(src[idx3], src[idx2], src[idx1], src[idx0]);\n        _mm_storeu_ps(dst + i, res);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n    i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i*)(indices + i));\n        int idx0 = _mm_cvtsi128_si32(idx_vec);\n        int idx1 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 4));\n        int idx2 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 8));\n        int idx3 = _mm_cvtsi128_si32(_mm_srli_si128(idx_vec, 12));\n        __m128 vals = _mm_loadu_ps(src + i);\n        float temp[4];\n        _mm_storeu_ps(temp, vals);\n        dst[idx0] = temp[0];\n        dst[idx1] = temp[1];\n        dst[idx2] = temp[2];\n        dst[idx3] = temp[3];\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((const __m128i*)(indices + i));\n        int idx_arr[4];\n        _mm_storeu_si128((__m128i*)idx_arr, idx);\n        __m128 val = _mm_setr_ps(src[idx_arr[0]], src[idx_arr[1]], src[idx_arr[2]], src[idx_arr[3]]);\n        _mm_storeu_ps(dst + i, val);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n    for (i = 0; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    // Gather operation: dst[i] = src[indices[i]]\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((const __m128i*)(indices + i));\n        int idx_arr[4];\n        _mm_storeu_si128((__m128i*)idx_arr, idx);\n        __m128 res = _mm_setr_ps(src[idx_arr[0]], src[idx_arr[1]], src[idx_arr[2]], src[idx_arr[3]]);\n        _mm_storeu_ps(dst + i, res);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    // Scatter operation: dst[indices[i]] = src[i]\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((const __m128i*)(indices + i));\n        int idx_arr[4];\n        _mm_storeu_si128((__m128i*)idx_arr, idx);\n        __m128 vals = _mm_loadu_ps(src + i);\n        float val_arr[4];\n        _mm_storeu_ps(val_arr, vals);\n        dst[idx_arr[0]] = val_arr[0];\n        dst[idx_arr[1]] = val_arr[1];\n        dst[idx_arr[2]] = val_arr[2];\n        dst[idx_arr[3]] = val_arr[3];\n    }\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((const __m128i*)(indices + i));\n        int idx_arr[4];\n        _mm_storeu_si128((__m128i*)idx_arr, idx);\n        __m128 vec = _mm_set_ps(\n            src[idx_arr[3]],\n            src[idx_arr[2]],\n            src[idx_arr[1]],\n            src[idx_arr[0]]\n        );\n        _mm_storeu_ps(dst + i, vec);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n    for (i = 0; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((const __m128i*)(indices + i));\n        int tmp_idx[4];\n        _mm_storeu_si128((__m128i*)tmp_idx, idx_vec);\n        __m128 gathered = _mm_set_ps(\n            src[tmp_idx[3]],\n            src[tmp_idx[2]],\n            src[tmp_idx[1]],\n            src[tmp_idx[0]]\n        );\n        _mm_storeu_ps(dst + i, gathered);\n    }\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((const __m128i*)(indices + i));\n        int tmp_idx[4];\n        _mm_storeu_si128((__m128i*)tmp_idx, idx_vec);\n        __m128 vals = _mm_loadu_ps(src + i);\n        float tmp_vals[4];\n        _mm_storeu_ps(tmp_vals, vals);\n        dst[tmp_idx[0]] = tmp_vals[0];\n        dst[tmp_idx[1]] = tmp_vals[1];\n        dst[tmp_idx[2]] = tmp_vals[2];\n        dst[tmp_idx[3]] = tmp_vals[3];\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n    uint64_t left_index = start;\n    uint64_t right_index = end;\n    while (left_index + 15 <= right_index) {\n        __m128i left_block = _mm_loadu_si128((__m128i*)(arr + left_index));\n        __m128i right_block = _mm_loadu_si128((__m128i*)(arr + right_index - 7));\n        left_block = _mm_shuffle_epi32(left_block, _MM_SHUFFLE(1,0,3,2));\n        left_block = _mm_shufflelo_epi16(left_block, _MM_SHUFFLE(0,1,2,3));\n        left_block = _mm_shufflehi_epi16(left_block, _MM_SHUFFLE(0,1,2,3));\n        right_block = _mm_shuffle_epi32(right_block, _MM_SHUFFLE(1,0,3,2));\n        right_block = _mm_shufflelo_epi16(right_block, _MM_SHUFFLE(0,1,2,3));\n        right_block = _mm_shufflehi_epi16(right_block, _MM_SHUFFLE(0,1,2,3));\n        _mm_storeu_si128((__m128i*)(arr + left_index), right_block);\n        _mm_storeu_si128((__m128i*)(arr + right_index - 7), left_block);\n        left_index += 8;\n        right_index -= 8;\n    }\n    while (left_index < right_index) {\n        int16_t temp = arr[left_index];\n        arr[left_index] = arr[right_index];\n        arr[right_index] = temp;\n        left_index++;\n        right_index--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    const int mask_lo_hi = (3 << 0) | (2 << 2) | (1 << 4) | (0 << 6);\n\n    while (end - start + 1 >= 16) {\n        __m128i front_chunk = _mm_loadu_si128((__m128i*)(arr + start));\n        __m128i back_chunk = _mm_loadu_si128((__m128i*)(arr + end - 7));\n\n        __m128i rev_front = _mm_shuffle_epi32(front_chunk, _MM_SHUFFLE(1, 0, 3, 2));\n        rev_front = _mm_shufflelo_epi16(rev_front, mask_lo_hi);\n        rev_front = _mm_shufflehi_epi16(rev_front, mask_lo_hi);\n\n        __m128i rev_back = _mm_shuffle_epi32(back_chunk, _MM_SHUFFLE(1, 0, 3, 2));\n        rev_back = _mm_shufflelo_epi16(rev_back, mask_lo_hi);\n        rev_back = _mm_shufflehi_epi16(rev_back, mask_lo_hi);\n\n        _mm_storeu_si128((__m128i*)(arr + start), rev_back);\n        _mm_storeu_si128((__m128i*)(arr + end - 7), rev_front);\n\n        start += 8;\n        end -= 8;\n    }\n\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) {\n        return;\n    }\n    uint64_t left = start;\n    uint64_t right = end;\n    \n    const __m128i mask = _mm_set_epi8(1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14);\n    \n    while (left + 14 < right) {\n        __m128i left_vec = _mm_loadu_si128((__m128i*)(arr + left));\n        __m128i right_vec = _mm_loadu_si128((__m128i*)(arr + right - 7));\n        \n        __m128i rev_left = _mm_shuffle_epi8(left_vec, mask);\n        __m128i rev_right = _mm_shuffle_epi8(right_vec, mask);\n        \n        _mm_storeu_si128((__m128i*)(arr + left), rev_right);\n        _mm_storeu_si128((__m128i*)(arr + right - 7), rev_left);\n        \n        left += 8;\n        right -= 8;\n    }\n    \n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n    \n    int16_t *left_ptr = arr + start;\n    int16_t *right_ptr = arr + end;\n    \n    while ((uint64_t)(right_ptr - left_ptr) >= 15) {\n        __m128i left_val = _mm_loadu_si128((__m128i*)left_ptr);\n        __m128i right_val = _mm_loadu_si128((__m128i*)(right_ptr - 7));\n        \n        left_val = _mm_shufflelo_epi16(left_val, 0x1B);\n        left_val = _mm_shufflehi_epi16(left_val, 0x1B);\n        left_val = _mm_shuffle_epi32(left_val, 0x4E);\n        \n        right_val = _mm_shufflelo_epi16(right_val, 0x1B);\n        right_val = _mm_shufflehi_epi16(right_val, 0x1B);\n        right_val = _mm_shuffle_epi32(right_val, 0x4E);\n        \n        _mm_storeu_si128((__m128i*)left_ptr, right_val);\n        _mm_storeu_si128((__m128i*)(right_ptr - 7), left_val);\n        \n        left_ptr += 8;\n        right_ptr -= 8;\n    }\n    \n    while (left_ptr < right_ptr) {\n        int16_t temp = *left_ptr;\n        *left_ptr = *right_ptr;\n        *right_ptr = temp;\n        left_ptr++;\n        right_ptr--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) {\n        return;\n    }\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (left + 15 <= right) {\n        __m128i left_vec = _mm_loadu_si128((__m128i*)(arr + left));\n        __m128i right_vec = _mm_loadu_si128((__m128i*)(arr + right - 7));\n\n        left_vec = _mm_shufflelo_epi16(left_vec, _MM_SHUFFLE(0,1,2,3));\n        left_vec = _mm_shufflehi_epi16(left_vec, _MM_SHUFFLE(0,1,2,3));\n        left_vec = _mm_shuffle_epi32(left_vec, _MM_SHUFFLE(1,0,3,2));\n\n        right_vec = _mm_shufflelo_epi16(right_vec, _MM_SHUFFLE(0,1,2,3));\n        right_vec = _mm_shufflehi_epi16(right_vec, _MM_SHUFFLE(0,1,2,3));\n        right_vec = _mm_shuffle_epi32(right_vec, _MM_SHUFFLE(1,0,3,2));\n\n        _mm_storeu_si128((__m128i*)(arr + right - 7), left_vec);\n        _mm_storeu_si128((__m128i*)(arr + left), right_vec);\n\n        left += 8;\n        right -= 8;\n    }\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *row_base = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *dest_row = slice + i * dim2;\n        size_t j = 0;\n        for (; j + 16 <= dim2; j += 16) {\n            __m128i vec = _mm_set_epi8(\n                row_base[(j+15)*dim3],\n                row_base[(j+14)*dim3],\n                row_base[(j+13)*dim3],\n                row_base[(j+12)*dim3],\n                row_base[(j+11)*dim3],\n                row_base[(j+10)*dim3],\n                row_base[(j+9)*dim3],\n                row_base[(j+8)*dim3],\n                row_base[(j+7)*dim3],\n                row_base[(j+6)*dim3],\n                row_base[(j+5)*dim3],\n                row_base[(j+4)*dim3],\n                row_base[(j+3)*dim3],\n                row_base[(j+2)*dim3],\n                row_base[(j+1)*dim3],\n                row_base[j*dim3]\n            );\n            _mm_storeu_si128((__m128i*)(dest_row + j), vec);\n        }\n        for (; j < dim2; j++) {\n            dest_row[j] = row_base[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base_i = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *slice_i = slice + i * dim2;\n        size_t j = 0;\n\n        for (; j + 16 <= dim2; j += 16) {\n            __m128i vec = _mm_set_epi8(\n                base_i[(j + 15) * dim3],\n                base_i[(j + 14) * dim3],\n                base_i[(j + 13) * dim3],\n                base_i[(j + 12) * dim3],\n                base_i[(j + 11) * dim3],\n                base_i[(j + 10) * dim3],\n                base_i[(j + 9) * dim3],\n                base_i[(j + 8) * dim3],\n                base_i[(j + 7) * dim3],\n                base_i[(j + 6) * dim3],\n                base_i[(j + 5) * dim3],\n                base_i[(j + 4) * dim3],\n                base_i[(j + 3) * dim3],\n                base_i[(j + 2) * dim3],\n                base_i[(j + 1) * dim3],\n                base_i[j * dim3]\n            );\n            _mm_storeu_si128((__m128i*)(slice_i + j), vec);\n        }\n\n        for (; j < dim2; j++) {\n            slice_i[j] = base_i[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *out = slice + i * dim2;\n        size_t j = 0;\n        for (; j + 15 < dim2; j += 16) {\n            __m128i v = _mm_setr_epi8(\n                base[j * dim3],\n                base[(j+1) * dim3],\n                base[(j+2) * dim3],\n                base[(j+3) * dim3],\n                base[(j+4) * dim3],\n                base[(j+5) * dim3],\n                base[(j+6) * dim3],\n                base[(j+7) * dim3],\n                base[(j+8) * dim3],\n                base[(j+9) * dim3],\n                base[(j+10) * dim3],\n                base[(j+11) * dim3],\n                base[(j+12) * dim3],\n                base[(j+13) * dim3],\n                base[(j+14) * dim3],\n                base[(j+15) * dim3]\n            );\n            _mm_storeu_si128((__m128i*)(out + j), v);\n        }\n        for (; j < dim2; j++) {\n            out[j] = base[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *src_base = tensor + i * dim2 * dim3 + slice_idx;\n        uint8_t *dst_base = slice + i * dim2;\n        size_t j = 0;\n        for (; j + 15 < dim2; j += 16) {\n            __m128i v = _mm_set_epi8(\n                src_base[(j+15)*dim3],\n                src_base[(j+14)*dim3],\n                src_base[(j+13)*dim3],\n                src_base[(j+12)*dim3],\n                src_base[(j+11)*dim3],\n                src_base[(j+10)*dim3],\n                src_base[(j+9)*dim3],\n                src_base[(j+8)*dim3],\n                src_base[(j+7)*dim3],\n                src_base[(j+6)*dim3],\n                src_base[(j+5)*dim3],\n                src_base[(j+4)*dim3],\n                src_base[(j+3)*dim3],\n                src_base[(j+2)*dim3],\n                src_base[(j+1)*dim3],\n                src_base[j*dim3]\n            );\n            _mm_storeu_si128((__m128i*)(dst_base + j), v);\n        }\n        for (; j < dim2; j++) {\n            dst_base[j] = src_base[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    for (size_t i = 0; i < dim1; i++) {\n        const uint8_t *base = tensor + i * dim2 * dim3 + slice_idx;\n        size_t j = 0;\n        for (; j + 15 < dim2; j += 16) {\n            __m128i v = _mm_set_epi8(\n                base[(j+15)*dim3],\n                base[(j+14)*dim3],\n                base[(j+13)*dim3],\n                base[(j+12)*dim3],\n                base[(j+11)*dim3],\n                base[(j+10)*dim3],\n                base[(j+9)*dim3],\n                base[(j+8)*dim3],\n                base[(j+7)*dim3],\n                base[(j+6)*dim3],\n                base[(j+5)*dim3],\n                base[(j+4)*dim3],\n                base[(j+3)*dim3],\n                base[(j+2)*dim3],\n                base[(j+1)*dim3],\n                base[j*dim3]\n            );\n            _mm_storeu_si128((__m128i*)(slice + i * dim2 + j), v);\n        }\n        for (; j < dim2; j++) {\n            slice[i * dim2 + j] = base[j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i0 = 0; i0 < rows; i0 += block_size) {\n        size_t i_end = (i0 + block_size) < rows ? (i0 + block_size) : rows;\n        for (size_t j0 = 0; j0 < cols; j0 += block_size) {\n            size_t j_end = (j0 + block_size) < cols ? (j0 + block_size) : cols;\n            for (size_t i = i0; i < i_end; i += 8) {\n                size_t i_next = i + 8;\n                if (i_next > i_end) i_next = i_end;\n                for (size_t j = j0; j < j_end; j += 8) {\n                    size_t j_next = j + 8;\n                    if (j_next > j_end) j_next = j_end;\n                    if ((i_next - i == 8) && (j_next - j == 8)) {\n                        __m128i r0 = _mm_loadu_si128((const __m128i*)(src + i * cols + j));\n                        __m128i r1 = _mm_loadu_si128((const __m128i*)(src + (i+1) * cols + j));\n                        __m128i r2 = _mm_loadu_si128((const __m128i*)(src + (i+2) * cols + j));\n                        __m128i r3 = _mm_loadu_si128((const __m128i*)(src + (i+3) * cols + j));\n                        __m128i r4 = _mm_loadu_si128((const __m128i*)(src + (i+4) * cols + j));\n                        __m128i r5 = _mm_loadu_si128((const __m128i*)(src + (i+5) * cols + j));\n                        __m128i r6 = _mm_loadu_si128((const __m128i*)(src + (i+6) * cols + j));\n                        __m128i r7 = _mm_loadu_si128((const __m128i*)(src + (i+7) * cols + j));\n\n                        __m128i t0 = _mm_unpacklo_epi16(r0, r1);\n                        __m128i t1 = _mm_unpackhi_epi16(r0, r1);\n                        __m128i t2 = _mm_unpacklo_epi16(r2, r3);\n                        __m128i t3 = _mm_unpackhi_epi16(r2, r3);\n                        __m128i t4 = _mm_unpacklo_epi16(r4, r5);\n                        __m128i t5 = _mm_unpackhi_epi16(r4, r5);\n                        __m128i t6 = _mm_unpacklo_epi16(r6, r7);\n                        __m128i t7 = _mm_unpackhi_epi16(r6, r7);\n\n                        __m128i tt0 = _mm_unpacklo_epi32(t0, t2);\n                        __m128i tt1 = _mm_unpackhi_epi32(t0, t2);\n                        __m128i tt2 = _mm_unpacklo_epi32(t1, t3);\n                        __m128i tt3 = _mm_unpackhi_epi32(t1, t3);\n                        __m128i tt4 = _mm_unpacklo_epi32(t4, t6);\n                        __m128i tt5 = _mm_unpackhi_epi32(t4, t6);\n                        __m128i tt6 = _mm_unpacklo_epi32(t5, t7);\n                        __m128i tt7 = _mm_unpackhi_epi32(t5, t7);\n\n                        r0 = _mm_unpacklo_epi64(tt0, tt4);\n                        r1 = _mm_unpackhi_epi64(tt0, tt4);\n                        r2 = _mm_unpacklo_epi64(tt1, tt5);\n                        r3 = _mm_unpackhi_epi64(tt1, tt5);\n                        r4 = _mm_unpacklo_epi64(tt2, tt6);\n                        r5 = _mm_unpackhi_epi64(tt2, tt6);\n                        r6 = _mm_unpacklo_epi64(tt3, tt7);\n                        r7 = _mm_unpackhi_epi64(tt3, tt7);\n\n                        _mm_storeu_si128((__m128i*)(dst + j * rows + i), r0);\n                        _mm_storeu_si128((__m128i*)(dst + (j+1) * rows + i), r1);\n                        _mm_storeu_si128((__m128i*)(dst + (j+2) * rows + i), r2);\n                        _mm_storeu_si128((__m128i*)(dst + (j+3) * rows + i), r3);\n                        _mm_storeu_si128((__m128i*)(dst + (j+4) * rows + i), r4);\n                        _mm_storeu_si128((__m128i*)(dst + (j+5) * rows + i), r5);\n                        _mm_storeu_si128((__m128i*)(dst + (j+6) * rows + i), r6);\n                        _mm_storeu_si128((__m128i*)(dst + (j+7) * rows + i), r7);\n                    } else {\n                        for (size_t ii = i; ii < i_next; ii++) {\n                            for (size_t jj = j; jj < j_next; jj++) {\n                                dst[jj * rows + ii] = src[ii * cols + jj];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i0 = 0; i0 < rows; i0 += block_size) {\n        size_t block_i = (i0 + block_size > rows) ? rows - i0 : block_size;\n        for (size_t j0 = 0; j0 < cols; j0 += block_size) {\n            size_t block_j = (j0 + block_size > cols) ? cols - j0 : block_size;\n            for (size_t i_in_block = 0; i_in_block < block_i; i_in_block += 8) {\n                size_t i_remaining = (block_i - i_in_block >= 8) ? 8 : block_i - i_in_block;\n                for (size_t j_in_block = 0; j_in_block < block_j; j_in_block += 8) {\n                    size_t j_remaining = (block_j - j_in_block >= 8) ? 8 : block_j - j_in_block;\n                    if (i_remaining == 8 && j_remaining == 8) {\n                        const int16_t *src_base = src + (i0 + i_in_block) * cols + j0 + j_in_block;\n                        __m128i row0 = _mm_loadu_si128((__m128i const*)(src_base + 0*cols));\n                        __m128i row1 = _mm_loadu_si128((__m128i const*)(src_base + 1*cols));\n                        __m128i row2 = _mm_loadu_si128((__m128i const*)(src_base + 2*cols));\n                        __m128i row3 = _mm_loadu_si128((__m128i const*)(src_base + 3*cols));\n                        __m128i row4 = _mm_loadu_si128((__m128i const*)(src_base + 4*cols));\n                        __m128i row5 = _mm_loadu_si128((__m128i const*)(src_base + 5*cols));\n                        __m128i row6 = _mm_loadu_si128((__m128i const*)(src_base + 6*cols));\n                        __m128i row7 = _mm_loadu_si128((__m128i const*)(src_base + 7*cols));\n\n                        __m128i t0 = _mm_unpacklo_epi16(row0, row1);\n                        __m128i t1 = _mm_unpacklo_epi16(row2, row3);\n                        __m128i t2 = _mm_unpackhi_epi16(row0, row1);\n                        __m128i t3 = _mm_unpackhi_epi16(row2, row3);\n                        __m128i t4 = _mm_unpacklo_epi16(row4, row5);\n                        __m128i t5 = _mm_unpacklo_epi16(row6, row7);\n                        __m128i t6 = _mm_unpackhi_epi16(row4, row5);\n                        __m128i t7 = _mm_unpackhi_epi16(row6, row7);\n\n                        __m128i tmp0 = _mm_unpacklo_epi32(t0, t1);\n                        __m128i tmp1 = _mm_unpacklo_epi32(t4, t5);\n                        __m128i tmp2 = _mm_unpackhi_epi32(t0, t1);\n                        __m128i tmp3 = _mm_unpackhi_epi32(t4, t5);\n                        __m128i tmp4 = _mm_unpacklo_epi32(t2, t3);\n                        __m128i tmp5 = _mm_unpacklo_epi32(t6, t7);\n                        __m128i tmp6 = _mm_unpackhi_epi32(t2, t3);\n                        __m128i tmp7 = _mm_unpackhi_epi32(t6, t7);\n\n                        t0 = _mm_unpacklo_epi64(tmp0, tmp1);\n                        t1 = _mm_unpackhi_epi64(tmp0, tmp1);\n                        t2 = _mm_unpacklo_epi64(tmp2, tmp3);\n                        t3 = _mm_unpackhi_epi64(tmp2, tmp3);\n                        t4 = _mm_unpacklo_epi64(tmp4, tmp5);\n                        t5 = _mm_unpackhi_epi64(tmp4, tmp5);\n                        t6 = _mm_unpacklo_epi64(tmp6, tmp7);\n                        t7 = _mm_unpackhi_epi64(tmp6, tmp7);\n\n                        int16_t *dst_base = dst + (j0 + j_in_block) * rows + i0 + i_in_block;\n                        _mm_storeu_si128((__m128i*)(dst_base + 0*rows), t0);\n                        _mm_storeu_si128((__m128i*)(dst_base + 1*rows), t1);\n                        _mm_storeu_si128((__m128i*)(dst_base + 2*rows), t2);\n                        _mm_storeu_si128((__m128i*)(dst_base + 3*rows), t3);\n                        _mm_storeu_si128((__m128i*)(dst_base + 4*rows), t4);\n                        _mm_storeu_si128((__m128i*)(dst_base + 5*rows), t5);\n                        _mm_storeu_si128((__m128i*)(dst_base + 6*rows), t6);\n                        _mm_storeu_si128((__m128i*)(dst_base + 7*rows), t7);\n                    } else {\n                        for (size_t i = 0; i < i_remaining; i++) {\n                            for (size_t j = 0; j < j_remaining; j++) {\n                                dst[(j0 + j_in_block + j) * rows + i0 + i_in_block + i] = \n                                    src[(i0 + i_in_block + i) * cols + j0 + j_in_block + j];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i_block = 0; i_block < rows; i_block += block_size) {\n        size_t i_end = i_block + block_size;\n        if (i_end > rows) i_end = rows;\n\n        for (size_t j_block = 0; j_block < cols; j_block += block_size) {\n            size_t j_end = j_block + block_size;\n            if (j_end > cols) j_end = cols;\n\n            for (size_t i = i_block; i < i_end; i += 8) {\n                size_t i_next = i + 8;\n                if (i_next > i_end) i_next = i_end;\n                size_t i_count = i_next - i;\n\n                for (size_t j = j_block; j < j_end; j += 8) {\n                    size_t j_next = j + 8;\n                    if (j_next > j_end) j_next = j_end;\n                    size_t j_count = j_next - j;\n\n                    if (i_count == 8 && j_count == 8) {\n                        __m128i r0 = _mm_loadu_si128((__m128i*)(src + i * cols + j));\n                        __m128i r1 = _mm_loadu_si128((__m128i*)(src + (i+1) * cols + j));\n                        __m128i r2 = _mm_loadu_si128((__m128i*)(src + (i+2) * cols + j));\n                        __m128i r3 = _mm_loadu_si128((__m128i*)(src + (i+3) * cols + j));\n                        __m128i r4 = _mm_loadu_si128((__m128i*)(src + (i+4) * cols + j));\n                        __m128i r5 = _mm_loadu_si128((__m128i*)(src + (i+5) * cols + j));\n                        __m128i r6 = _mm_loadu_si128((__m128i*)(src + (i+6) * cols + j));\n                        __m128i r7 = _mm_loadu_si128((__m128i*)(src + (i+7) * cols + j));\n\n                        __m128i t0 = _mm_unpacklo_epi16(r0, r1);\n                        __m128i t1 = _mm_unpackhi_epi16(r0, r1);\n                        __m128i t2 = _mm_unpacklo_epi16(r2, r3);\n                        __m128i t3 = _mm_unpackhi_epi16(r2, r3);\n                        __m128i t4 = _mm_unpacklo_epi16(r4, r5);\n                        __m128i t5 = _mm_unpackhi_epi16(r4, r5);\n                        __m128i t6 = _mm_unpacklo_epi16(r6, r7);\n                        __m128i t7 = _mm_unpackhi_epi16(r6, r7);\n\n                        __m128i u0 = _mm_unpacklo_epi32(t0, t2);\n                        __m128i u1 = _mm_unpackhi_epi32(t0, t2);\n                        __m128i u2 = _mm_unpacklo_epi32(t1, t3);\n                        __m128i u3 = _mm_unpackhi_epi32(t1, t3);\n                        __m128i u4 = _mm_unpacklo_epi32(t4, t6);\n                        __m128i u5 = _mm_unpackhi_epi32(t4, t6);\n                        __m128i u6 = _mm_unpacklo_epi32(t5, t7);\n                        __m128i u7 = _mm_unpackhi_epi32(t5, t7);\n\n                        __m128i v0 = _mm_unpacklo_epi64(u0, u4);\n                        __m128i v1 = _mm_unpackhi_epi64(u0, u4);\n                        __m128i v2 = _mm_unpacklo_epi64(u1, u5);\n                        __m128i v3 = _mm_unpackhi_epi64(u1, u5);\n                        __m128i v4 = _mm_unpacklo_epi64(u2, u6);\n                        __m128i v5 = _mm_unpackhi_epi64(u2, u6);\n                        __m128i v6 = _mm_unpacklo_epi64(u3, u7);\n                        __m128i v7 = _mm_unpackhi_epi64(u3, u7);\n\n                        _mm_storeu_si128((__m128i*)(dst + j * rows + i), v0);\n                        _mm_storeu_si128((__m128i*)(dst + (j+1) * rows + i), v1);\n                        _mm_storeu_si128((__m128i*)(dst + (j+2) * rows + i), v2);\n                        _mm_storeu_si128((__m128i*)(dst + (j+3) * rows + i), v3);\n                        _mm_storeu_si128((__m128i*)(dst + (j+4) * rows + i), v4);\n                        _mm_storeu_si128((__m128i*)(dst + (j+5) * rows + i), v5);\n                        _mm_storeu_si128((__m128i*)(dst + (j+6) * rows + i), v6);\n                        _mm_storeu_si128((__m128i*)(dst + (j+7) * rows + i), v7);\n                    } else {\n                        for (size_t ii = i; ii < i + i_count; ii++) {\n                            for (size_t jj = j; jj < j + j_count; jj++) {\n                                dst[jj * rows + ii] = src[ii * cols + jj];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        size_t i_end = (i + block_size < rows) ? i + block_size : rows;\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t j_end = (j + block_size < cols) ? j + block_size : cols;\n            for (size_t ii = i; ii < i_end; ii += 8) {\n                size_t ii_next = (ii + 8 < i_end) ? ii + 8 : i_end;\n                for (size_t jj = j; jj < j_end; jj += 8) {\n                    size_t jj_next = (jj + 8 < j_end) ? jj + 8 : j_end;\n                    if ((ii_next - ii) == 8 && (jj_next - jj) == 8) {\n                        __m128i r0 = _mm_loadu_si128((__m128i*)(src + (ii+0)*cols + jj));\n                        __m128i r1 = _mm_loadu_si128((__m128i*)(src + (ii+1)*cols + jj));\n                        __m128i r2 = _mm_loadu_si128((__m128i*)(src + (ii+2)*cols + jj));\n                        __m128i r3 = _mm_loadu_si128((__m128i*)(src + (ii+3)*cols + jj));\n                        __m128i r4 = _mm_loadu_si128((__m128i*)(src + (ii+4)*cols + jj));\n                        __m128i r5 = _mm_loadu_si128((__m128i*)(src + (ii+5)*cols + jj));\n                        __m128i r6 = _mm_loadu_si128((__m128i*)(src + (ii+6)*cols + jj));\n                        __m128i r7 = _mm_loadu_si128((__m128i*)(src + (ii+7)*cols + jj));\n\n                        __m128i t0 = _mm_unpacklo_epi16(r0, r1);\n                        __m128i t1 = _mm_unpackhi_epi16(r0, r1);\n                        __m128i t2 = _mm_unpacklo_epi16(r2, r3);\n                        __m128i t3 = _mm_unpackhi_epi16(r2, r3);\n                        __m128i t4 = _mm_unpacklo_epi16(r4, r5);\n                        __m128i t5 = _mm_unpackhi_epi16(r4, r5);\n                        __m128i t6 = _mm_unpacklo_epi16(r6, r7);\n                        __m128i t7 = _mm_unpackhi_epi16(r6, r7);\n\n                        __m128i u0 = _mm_unpacklo_epi32(t0, t2);\n                        __m128i u1 = _mm_unpackhi_epi32(t0, t2);\n                        __m128i u2 = _mm_unpacklo_epi32(t1, t3);\n                        __m128i u3 = _mm_unpackhi_epi32(t1, t3);\n                        __m128i u4 = _mm_unpacklo_epi32(t4, t6);\n                        __m128i u5 = _mm_unpackhi_epi32(t4, t6);\n                        __m128i u6 = _mm_unpacklo_epi32(t5, t7);\n                        __m128i u7 = _mm_unpackhi_epi32(t5, t7);\n\n                        __m128i v0 = _mm_unpacklo_epi64(u0, u4);\n                        __m128i v1 = _mm_unpackhi_epi64(u0, u4);\n                        __m128i v2 = _mm_unpacklo_epi64(u1, u5);\n                        __m128i v3 = _mm_unpackhi_epi64(u1, u5);\n                        __m128i v4 = _mm_unpacklo_epi64(u2, u6);\n                        __m128i v5 = _mm_unpackhi_epi64(u2, u6);\n                        __m128i v6 = _mm_unpacklo_epi64(u3, u7);\n                        __m128i v7 = _mm_unpackhi_epi64(u3, u7);\n\n                        _mm_storeu_si128((__m128i*)(dst + (jj+0)*rows + ii), v0);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+1)*rows + ii), v1);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+2)*rows + ii), v2);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+3)*rows + ii), v3);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+4)*rows + ii), v4);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+5)*rows + ii), v5);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+6)*rows + ii), v6);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+7)*rows + ii), v7);\n                    } else {\n                        for (size_t k = ii; k < ii_next; k++) {\n                            for (size_t l = jj; l < jj_next; l++) {\n                                dst[l * rows + k] = src[k * cols + l];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i_block = 0; i_block < rows; i_block += block_size) {\n        size_t i_end_block = i_block + block_size;\n        if (i_end_block > rows) i_end_block = rows;\n\n        for (size_t j_block = 0; j_block < cols; j_block += block_size) {\n            size_t j_end_block = j_block + block_size;\n            if (j_end_block > cols) j_end_block = cols;\n\n            for (size_t ii = i_block; ii < i_end_block; ii += 8) {\n                size_t i_next = ii + 8;\n                if (i_next > i_end_block) i_next = i_end_block;\n\n                for (size_t jj = j_block; jj < j_end_block; jj += 8) {\n                    size_t j_next = jj + 8;\n                    if (j_next > j_end_block) j_next = j_end_block;\n\n                    if ((i_next - ii) == 8 && (j_next - jj) == 8) {\n                        __m128i a0 = _mm_loadu_si128((const __m128i*)(src + ii * cols + jj));\n                        __m128i a1 = _mm_loadu_si128((const __m128i*)(src + (ii+1)*cols + jj));\n                        __m128i a2 = _mm_loadu_si128((const __m128i*)(src + (ii+2)*cols + jj));\n                        __m128i a3 = _mm_loadu_si128((const __m128i*)(src + (ii+3)*cols + jj));\n                        __m128i a4 = _mm_loadu_si128((const __m128i*)(src + (ii+4)*cols + jj));\n                        __m128i a5 = _mm_loadu_si128((const __m128i*)(src + (ii+5)*cols + jj));\n                        __m128i a6 = _mm_loadu_si128((const __m128i*)(src + (ii+6)*cols + jj));\n                        __m128i a7 = _mm_loadu_si128((const __m128i*)(src + (ii+7)*cols + jj));\n\n                        __m128i t0 = _mm_unpacklo_epi16(a0, a1);\n                        __m128i t1 = _mm_unpacklo_epi16(a2, a3);\n                        __m128i t2 = _mm_unpackhi_epi16(a0, a1);\n                        __m128i t3 = _mm_unpackhi_epi16(a2, a3);\n                        __m128i t4 = _mm_unpacklo_epi16(a4, a5);\n                        __m128i t5 = _mm_unpacklo_epi16(a6, a7);\n                        __m128i t6 = _mm_unpackhi_epi16(a4, a5);\n                        __m128i t7 = _mm_unpackhi_epi16(a6, a7);\n\n                        __m128i u0 = _mm_unpacklo_epi32(t0, t1);\n                        __m128i u1 = _mm_unpacklo_epi32(t4, t5);\n                        __m128i u2 = _mm_unpackhi_epi32(t0, t1);\n                        __m128i u3 = _mm_unpackhi_epi32(t4, t5);\n                        __m128i u4 = _mm_unpacklo_epi32(t2, t3);\n                        __m128i u5 = _mm_unpackhi_epi32(t2, t3);\n                        __m128i u6 = _mm_unpacklo_epi32(t6, t7);\n                        __m128i u7 = _mm_unpackhi_epi32(t6, t7);\n\n                        a0 = _mm_unpacklo_epi64(u0, u1);\n                        a1 = _mm_unpackhi_epi64(u0, u1);\n                        a2 = _mm_unpacklo_epi64(u2, u3);\n                        a3 = _mm_unpackhi_epi64(u2, u3);\n                        a4 = _mm_unpacklo_epi64(u4, u5);\n                        a5 = _mm_unpackhi_epi64(u4, u5);\n                        a6 = _mm_unpacklo_epi64(u6, u7);\n                        a7 = _mm_unpackhi_epi64(u6, u7);\n\n                        _mm_storeu_si128((__m128i*)(dst + jj * rows + ii), a0);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+1) * rows + ii), a1);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+2) * rows + ii), a2);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+3) * rows + ii), a3);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+4) * rows + ii), a4);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+5) * rows + ii), a5);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+6) * rows + ii), a6);\n                        _mm_storeu_si128((__m128i*)(dst + (jj+7) * rows + ii), a7);\n                    } else {\n                        for (size_t i = ii; i < i_next; i++) {\n                            for (size_t j = jj; j < j_next; j++) {\n                                dst[j * rows + i] = src[i * cols + j];\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) return 0.0f;\n\n    size_t stride = dim * dim + dim + 1;\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 3 < dim; i += 4) {\n        float d0 = array[i * stride];\n        float d1 = array[(i + 1) * stride];\n        float d2 = array[(i + 2) * stride];\n        float d3 = array[(i + 3) * stride];\n        __m128 v = _mm_setr_ps(d0, d1, d2, d3);\n        sum_vec = _mm_add_ps(sum_vec, v);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(sum_vec, sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128 add1 = _mm_add_ps(sum_vec, shuf);\n    __m128 perm = _mm_shuffle_ps(add1, add1, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128 total_vec = _mm_add_ps(add1, perm);\n    float total = _mm_cvtss_f32(total_vec);\n\n    for (; i < dim; ++i) {\n        total += array[i * stride];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    size_t stride = dim * dim + dim + 1;\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 4 <= dim; i += 4) {\n        __m128 v = _mm_setr_ps(array[i * stride],\n                              array[(i + 1) * stride],\n                              array[(i + 2) * stride],\n                              array[(i + 3) * stride]);\n        sum_vec = _mm_add_ps(sum_vec, v);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(sum_vec, sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128 sums = _mm_add_ps(sum_vec, shuf);\n    __m128 perm = _mm_movehl_ps(sums, sums);\n    __m128 hsum = _mm_add_ss(sums, perm);\n    float total = _mm_cvtss_f32(hsum);\n\n    for (; i < dim; ++i) {\n        total += array[i * stride];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) \n        return 0.0f;\n    \n    size_t step = dim * dim + dim + 1;\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 4 <= dim; i += 4) {\n        float a0 = array[i * step];\n        float a1 = array[(i + 1) * step];\n        float a2 = array[(i + 2) * step];\n        float a3 = array[(i + 3) * step];\n        __m128 v = _mm_set_ps(a3, a2, a1, a0);\n        sum_vec = _mm_add_ps(sum_vec, v);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(sum_vec, sum_vec, _MM_SHUFFLE(2,3,0,1));\n    __m128 sums = _mm_add_ps(sum_vec, shuf);\n    __m128 perm = _mm_shuffle_ps(sums, sums, _MM_SHUFFLE(1,0,3,2));\n    __m128 total_vec = _mm_add_ps(sums, perm);\n    float total = _mm_cvtss_f32(total_vec);\n\n    for (; i < dim; i++) {\n        total += array[i * step];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t stride = dim * dim + dim + 1;\n    __m128 sum_vector = _mm_setzero_ps();\n    size_t i = 0;\n    \n    for (; i + 3 < dim; i += 4) {\n        size_t base_index = i * stride;\n        __m128 a = _mm_load_ss(array + base_index);\n        __m128 b = _mm_load_ss(array + base_index + stride);\n        __m128 c = _mm_load_ss(array + base_index + 2 * stride);\n        __m128 d = _mm_load_ss(array + base_index + 3 * stride);\n        \n        __m128 ab = _mm_unpacklo_ps(a, b);\n        __m128 cd = _mm_unpacklo_ps(c, d);\n        __m128 abcd = _mm_movelh_ps(ab, cd);\n        \n        sum_vector = _mm_add_ps(sum_vector, abcd);\n    }\n    \n    __m128 shuf = _mm_shuffle_ps(sum_vector, sum_vector, _MM_SHUFFLE(2,3,0,1));\n    __m128 sum1 = _mm_add_ps(sum_vector, shuf);\n    __m128 sum2 = _mm_movehl_ps(sum1, sum1);\n    __m128 sum3 = _mm_add_ss(sum1, sum2);\n    float sum = _mm_cvtss_f32(sum3);\n    \n    for (; i < dim; i++) {\n        sum += array[i * stride];\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    if (dim == 0) {\n        return 0.0f;\n    }\n    size_t stride = dim * dim + dim + 1;\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i + 3 < dim; i += 4) {\n        __m128 v = _mm_setr_ps(array[i * stride],\n                               array[(i+1) * stride],\n                               array[(i+2) * stride],\n                               array[(i+3) * stride]);\n        sum_vec = _mm_add_ps(sum_vec, v);\n    }\n\n    __m128 shuf = _mm_movehl_ps(sum_vec, sum_vec);\n    __m128 sums = _mm_add_ps(sum_vec, shuf);\n    shuf = _mm_shuffle_ps(sums, sums, 0x55);\n    sums = _mm_add_ss(sums, shuf);\n    float total;\n    _mm_store_ss(&total, sums);\n\n    for (; i < dim; i++) {\n        total += array[i * stride];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    size_t i = 0;\n\n    for (; i + 1 < total; i += 2) {\n        __m128d data = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(data, thresh_vec);\n        __m128d scaled = _mm_mul_pd(data, scale_vec);\n        __m128d result = _mm_or_pd(\n            _mm_and_pd(mask, scaled),\n            _mm_andnot_pd(mask, data)\n        );\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    if (i < total) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    const __m128d thresh_vec = _mm_set1_pd(threshold);\n    const __m128d scale_vec = _mm_set1_pd(scale);\n    const __m128d ones = _mm_set1_pd(1.0);\n    \n    size_t i = 0;\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d data = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(data, thresh_vec);\n        __m128d multiplier = _mm_or_pd(\n            _mm_and_pd(mask, scale_vec),\n            _mm_andnot_pd(mask, ones)\n        );\n        __m128d result = _mm_mul_pd(data, multiplier);\n        _mm_storeu_pd(dst + i, result);\n    }\n    \n    if (i < total_elements) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        size_t row_offset = i * cols;\n\n        for (; j + 1 < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(src + row_offset + j);\n            __m128d mask = _mm_cmpgt_pd(data, thresh_vec);\n            __m128d scaled = _mm_mul_pd(data, scale_vec);\n            __m128d result = _mm_or_pd(_mm_and_pd(mask, scaled), \n                                      _mm_andnot_pd(mask, data));\n            _mm_storeu_pd(dst + row_offset + j, result);\n        }\n\n        if (j < cols) {\n            double value = src[row_offset + j];\n            dst[row_offset + j] = (value > threshold) ? value * scale : value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t n = rows * cols;\n    size_t i = 0;\n    \n    const __m128d thresh_vec = _mm_set1_pd(threshold);\n    const __m128d scale_vec = _mm_set1_pd(scale);\n    \n    for (; i <= n - 2; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(src_vec, thresh_vec);\n        __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n        __m128d result = _mm_or_pd(_mm_and_pd(mask, scaled), \n                                   _mm_andnot_pd(mask, src_vec));\n        _mm_storeu_pd(dst + i, result);\n    }\n    \n    if (i < n) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    __m128d thresh = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    size_t i = 0;\n    \n    for (; i + 1 < total; i += 2) {\n        __m128d data = _mm_loadu_pd(src + i);\n        __m128d mask = _mm_cmpgt_pd(data, thresh);\n        __m128d scaled = _mm_mul_pd(data, scale_vec);\n        __m128d result = _mm_or_pd(\n            _mm_and_pd(mask, scaled),\n            _mm_andnot_pd(mask, data)\n        );\n        _mm_storeu_pd(dst + i, result);\n    }\n    \n    if (i < total) {\n        double val = src[i];\n        dst[i] = (val > threshold) ? val * scale : val;\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        const double *src_row = src + indices[i] * cols;\n        double *dst_row = dst + i * cols;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, vec);\n        }\n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        size_t src_row_idx = indices[i];\n        const double *src_row = src + src_row_idx * cols;\n        double *dst_row = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m128d chunk0 = _mm_loadu_pd(src_row + j);\n            __m128d chunk1 = _mm_loadu_pd(src_row + j + 2);\n            _mm_storeu_pd(dst_row + j, chunk0);\n            _mm_storeu_pd(dst_row + j + 2, chunk1);\n        }\n        \n        for (; j + 1 < cols; j += 2) {\n            __m128d chunk = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, chunk);\n        }\n        \n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        const double *src_row = src + indices[i] * cols;\n        double *dst_row = dst + i * cols;\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, data);\n        }\n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i++) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * cols;\n        double *dst_row = dst + i * cols;\n        \n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, data);\n        }\n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double *src_row = src + indices[i] * cols;\n        double *dst_row = dst + i * cols;\n        \n        size_t j = 0;\n        // Process two elements at a time using SSE2\n        for (; j + 1 < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, data);\n        }\n        // Handle remaining element if cols is odd\n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i sum = _mm_add_epi64(a, b);\n        sum = _mm_add_epi64(sum, scalar_vec);\n        \n        int64_t tmp[2];\n        _mm_storeu_si128((__m128i*)tmp, sum);\n        \n        for (int j = 0; j < 2; j++) {\n            int64_t n = tmp[j];\n            dst[i + j] = (n + (n < 0)) / 2;\n        }\n    }\n    for (; i < length; i++) {\n        int64_t n = src1[i] + src2[i] + scalar;\n        dst[i] = (n + (n < 0)) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i s1 = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i s2 = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i sum = _mm_add_epi64(s1, s2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), sum);\n    }\n    if (i < length) {\n        dst[i] = src1[i] + src2[i] + scalar;\n    }\n    for (size_t j = 0; j < length; j++) {\n        dst[j] = dst[j] / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    \n    for (; i + 1 < length; i += 2) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i sum = _mm_add_epi64(v1, v2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n        \n        int64_t temp[2];\n        _mm_storeu_si128((__m128i*)temp, sum);\n        \n        for (int j = 0; j < 2; j++) {\n            int64_t val = temp[j];\n            int64_t sign = val >> 63;\n            int64_t correction = (val & 1) & sign;\n            dst[i + j] = (val >> 1) + correction;\n        }\n    }\n    \n    for (; i < length; i++) {\n        int64_t val = src1[i] + src2[i] + scalar;\n        int64_t sign = val >> 63;\n        int64_t correction = (val & 1) & sign;\n        dst[i] = (val >> 1) + correction;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstatic inline __m128i mm_sra_epi64(const __m128i a) {\n    __m128i shifted = _mm_srli_epi64(a, 1);\n    __m128i sign_high = _mm_shuffle_epi32(a, _MM_SHUFFLE(3, 3, 1, 1));\n    sign_high = _mm_srai_epi32(sign_high, 31);\n    __m128i high_bit = _mm_slli_epi64(sign_high, 63);\n    return _mm_or_si128(shifted, high_bit);\n}\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i v_scalar = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i v_src2 = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i v_sum = _mm_add_epi64(v_src1, v_src2);\n        v_sum = _mm_add_epi64(v_sum, v_scalar);\n\n        __m128i one = _mm_set1_epi64x(1);\n        __m128i low_bit = _mm_and_si128(v_sum, one);\n        __m128i sign = _mm_shuffle_epi32(v_sum, _MM_SHUFFLE(3, 3, 1, 1));\n        sign = _mm_srai_epi32(sign, 31);\n        __m128i correction = _mm_and_si128(low_bit, sign);\n        __m128i adjusted = _mm_add_epi64(v_sum, correction);\n\n        __m128i result = mm_sra_epi64(adjusted);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        int64_t x = src1[i] + src2[i] + scalar;\n        if (x < 0 && (x & 1)) {\n            x++;\n        }\n        dst[i] = x / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    if (length == 0) return;\n    \n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n    \n    for (; i + 1 < length; i += 2) {\n        __m128i v1 = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i sum = _mm_add_epi64(v1, v2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n        \n        int64_t low = _mm_cvtsi128_si64(sum);\n        int64_t high = _mm_cvtsi128_si64(_mm_srli_si128(sum, 8));\n        \n        dst[i] = low / 2;\n        dst[i + 1] = high / 2;\n    }\n    \n    if (i < length) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i mask = _mm_setr_epi32(0, -1, 0, -1);\n    __m128i max_val_vec = _mm_set1_epi32(INT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i v_masked = _mm_or_si128(\n            _mm_and_si128(mask, max_val_vec),\n            _mm_andnot_si128(mask, v)\n        );\n        __m128i cmp = _mm_cmplt_epi32(v_masked, min_vec);\n        min_vec = _mm_or_si128(\n            _mm_and_si128(cmp, v_masked),\n            _mm_andnot_si128(cmp, min_vec)\n        );\n    }\n\n    int min_arr[4];\n    _mm_storeu_si128((__m128i*)min_arr, min_vec);\n    int min_val = (min_arr[0] < min_arr[2]) ? min_arr[0] : min_arr[2];\n\n    for (size_t j = i; j < length; j += 2) {\n        if (src[j] < min_val) {\n            min_val = src[j];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i even = _mm_shuffle_epi32(vec, _MM_SHUFFLE(2,0,2,0));\n        min_vec = _mm_min_epi32(min_vec, even);\n    }\n\n    __m128i shuf = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(2,3,0,1));\n    __m128i min_temp = _mm_min_epi32(min_vec, shuf);\n    int min_val = _mm_cvtsi128_si32(min_temp);\n\n    if (i < length) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n        if (i + 2 < length) {\n            if (src[i + 2] < min_val) {\n                min_val = src[i + 2];\n            }\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i keep_mask = _mm_set_epi32(0, -1, 0, -1);\n    __m128i max_vec = _mm_set1_epi32(INT_MAX);\n\n    size_t i = 0;\n    const size_t num_blocks = length / 4;\n    const size_t tail_start = num_blocks * 4;\n\n    for (i = 0; i < tail_start; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i candidate = _mm_or_si128(\n            _mm_and_si128(data, keep_mask),\n            _mm_andnot_si128(keep_mask, max_vec)\n        );\n        min_vec = _mm_min_epi32(min_vec, candidate);\n    }\n\n    __m128i v1 = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(2,3,0,1));\n    __m128i v2 = _mm_min_epi32(min_vec, v1);\n    __m128i v3 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(1,0,3,2));\n    __m128i v4 = _mm_min_epi32(v2, v3);\n    int min_val = _mm_cvtsi128_si32(v4);\n\n    for (size_t i = tail_start; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n    \n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i evens = _mm_shuffle_epi32(vec, _MM_SHUFFLE(2, 0, 2, 0));\n        __m128i mask = _mm_cmplt_epi32(evens, min_val);\n        min_val = _mm_or_si128(\n            _mm_and_si128(mask, evens),\n            _mm_andnot_si128(mask, min_val)\n        );\n    }\n    \n    int min_array[4];\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n    int min_temp = min_array[0];\n    if (min_array[1] < min_temp) min_temp = min_array[1];\n    if (min_array[2] < min_temp) min_temp = min_array[2];\n    if (min_array[3] < min_temp) min_temp = min_array[3];\n    \n    for (size_t j = i; j < length; j += 2) {\n        if (src[j] < min_temp) {\n            min_temp = src[j];\n        }\n    }\n    \n    return min_temp;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    size_t full_chunks = length / 4;\n    size_t tail_start = full_chunks * 4;\n    size_t remaining = length - tail_start;\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i max_vec = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_setr_epi32(-1, 0, -1, 0);\n\n    for (size_t i = 0; i < full_chunks * 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i even_chunk = _mm_or_si128(\n            _mm_and_si128(chunk, even_mask),\n            _mm_andnot_si128(even_mask, max_vec)\n        );\n        __m128i cmp = _mm_cmplt_epi32(even_chunk, min_vec);\n        min_vec = _mm_or_si128(\n            _mm_and_si128(cmp, even_chunk),\n            _mm_andnot_si128(cmp, min_vec)\n        );\n    }\n\n    int min_arr[4];\n    _mm_storeu_si128((__m128i*)min_arr, min_vec);\n    int result = min_arr[0];\n    for (int i = 1; i < 4; i++) {\n        if (min_arr[i] < result) {\n            result = min_arr[i];\n        }\n    }\n\n    if (remaining > 0) {\n        int tail_min = INT_MAX;\n        for (size_t j = 0; j < remaining; j += 2) {\n            int val = src[tail_start + j];\n            if (val < tail_min) {\n                tail_min = val;\n            }\n        }\n        if (tail_min < result) {\n            result = tail_min;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    shift &= 0x1F; // Mask to ensure shift is 0-31\n    size_t i = 0;\n    if (length >= 4) {\n        for (; i <= length - 4; i += 4) {\n            __m128i data = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i shift_vec = _mm_set1_epi32(shift);\n            __m128i arith = _mm_sra_epi32(data, shift_vec);\n            __m128i logical = _mm_srl_epi32(data, shift_vec);\n            __m128i result = _mm_add_epi32(arith, logical);\n            _mm_storeu_si128((__m128i*)(dst + i), result);\n        }\n    }\n    for (; i < length; i++) {\n        int val = src[i];\n        int arith_shifted = val >> shift;\n        unsigned int uval = (unsigned int)val;\n        unsigned int logical_shifted = uval >> shift;\n        dst[i] = arith_shifted + (int)logical_shifted;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i data = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i arith = _mm_sra_epi32(data, shift_vec);\n        __m128i logical = _mm_srl_epi32(data, shift_vec);\n        __m128i result = _mm_add_epi32(arith, logical);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        int logical = (int)((unsigned int)src[i] >> shift);\n        dst[i] = arith + logical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    if (shift >= 32) {\n        shift %= 32;\n    }\n    __m128i shift_vec = _mm_cvtsi32_si128(shift);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i data = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i arith = _mm_sra_epi32(data, shift_vec);\n        __m128i logic = _mm_srl_epi32(data, shift_vec);\n        __m128i result = _mm_add_epi32(arith, logic);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        unsigned int uval = (unsigned int)src[i];\n        int logic = (int)(uval >> shift);\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i data = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i arith = _mm_sra_epi32(data, shift_vec);\n        __m128i logic = _mm_srl_epi32(data, shift_vec);\n        __m128i res = _mm_add_epi32(arith, logic);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        unsigned int u = (unsigned int)(src[i]);\n        int logic = (int)(u >> shift);\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    size_t i = 0;\n    \n    for (; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&src[i]);\n        __m128i arith = _mm_sra_epi32(data, shift_vec);\n        __m128i logic = _mm_srl_epi32(data, shift_vec);\n        __m128i result = _mm_add_epi32(arith, logic);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    \n    for (; i < length; i++) {\n        int arith = src[i] >> shift;\n        int logic = (unsigned int)src[i] >> shift;\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i acc = _mm_setzero_si128();\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((const __m128i*)(matrix + i));\n        __m128i sign_extend = _mm_srai_epi32(vec, 31);\n        __m128i low64 = _mm_unpacklo_epi32(vec, sign_extend);\n        __m128i high64 = _mm_unpackhi_epi32(vec, sign_extend);\n        acc = _mm_add_epi64(acc, low64);\n        acc = _mm_add_epi64(acc, high64);\n    }\n\n    int64_t a0 = _mm_cvtsi128_si64(acc);\n    int64_t a1 = _mm_cvtsi128_si64(_mm_srli_si128(acc, 8));\n    int64_t total_sum = a0 + a1;\n\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m128i sum64 = _mm_setzero_si128();\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(matrix + i));\n        __m128i sign = _mm_srai_epi32(a, 31);\n        __m128i low64 = _mm_unpacklo_epi32(a, sign);\n        __m128i high64 = _mm_unpackhi_epi32(a, sign);\n        sum64 = _mm_add_epi64(sum64, low64);\n        sum64 = _mm_add_epi64(sum64, high64);\n    }\n\n    int64_t scalar_sum = 0;\n    for (; i < total_elements; i++) {\n        scalar_sum += matrix[i];\n    }\n\n    __m128i shifted = _mm_srli_si128(sum64, 8);\n    __m128i total_vec = _mm_add_epi64(sum64, shifted);\n    int64_t vector_sum = _mm_cvtsi128_si64(total_vec);\n\n    return vector_sum + scalar_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t total = 0;\n\n    if (total_elements < 4) {\n        for (; i < total_elements; i++) {\n            total += matrix[i];\n        }\n        return total;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)(matrix + i));\n        __m128i sign_mask = _mm_srai_epi32(v, 31);\n        __m128i low64 = _mm_unpacklo_epi32(v, sign_mask);\n        __m128i high64 = _mm_unpackhi_epi32(v, sign_mask);\n        sum = _mm_add_epi64(sum, low64);\n        sum = _mm_add_epi64(sum, high64);\n    }\n\n    int64_t tmp[2] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)tmp, sum);\n    total = tmp[0] + tmp[1];\n\n    for (; i < total_elements; i++) {\n        total += matrix[i];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    __m128i acc = _mm_setzero_si128();\n\n    for (; i + 3 < total; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(matrix + i));\n        __m128i signmask = _mm_srai_epi32(v, 31);\n        __m128i v0 = _mm_unpacklo_epi32(v, signmask);\n        __m128i v1 = _mm_unpackhi_epi32(v, signmask);\n        acc = _mm_add_epi64(acc, v0);\n        acc = _mm_add_epi64(acc, v1);\n    }\n\n    int64_t sum_lo = _mm_cvtsi128_si64(acc);\n    __m128i hi64 = _mm_srli_si128(acc, 8);\n    int64_t sum_hi = _mm_cvtsi128_si64(hi64);\n    int64_t total_sum = sum_lo + sum_hi;\n\n    for (; i < total; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_length = rows * cols;\n    size_t i = 0;\n    __m128i sum_low = _mm_setzero_si128();\n    __m128i sum_high = _mm_setzero_si128();\n\n    for (; i + 3 < total_length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(matrix + i));\n        __m128i sign = _mm_srai_epi32(v, 31);\n        __m128i low_64 = _mm_unpacklo_epi32(v, sign);\n        __m128i high_64 = _mm_unpackhi_epi32(v, sign);\n        sum_low = _mm_add_epi64(sum_low, low_64);\n        sum_high = _mm_add_epi64(sum_high, high_64);\n    }\n\n    __m128i total_vec = _mm_add_epi64(sum_low, sum_high);\n    int64_t total = _mm_cvtsi128_si64(total_vec);\n    total += _mm_cvtsi128_si64(_mm_unpackhi_epi64(total_vec, total_vec));\n\n    for (; i < total_length; i++) {\n        total += matrix[i];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    int8_t global_max = src[0];\n    size_t global_index = 0;\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i global_max_v = _mm_set1_epi8(global_max);\n        __m128i cmp = _mm_cmpgt_epi8(v, global_max_v);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask) {\n            int8_t chunk_max = global_max;\n            for (int j = 0; j < 16; j++) {\n                if (src[i + j] > chunk_max) {\n                    chunk_max = src[i + j];\n                }\n            }\n            for (int j = 0; j < 16; j++) {\n                if (src[i + j] == chunk_max) {\n                    global_index = i + j;\n                    break;\n                }\n            }\n            global_max = chunk_max;\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_index = i;\n        }\n    }\n\n    return global_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t global_index = 0;\n    int8_t global_max = src[0];\n    size_t i = 0;\n\n    for (; i <= length - 16; i += 16) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i v_max = v;\n        \n        __m128i v_shifted = _mm_srli_si128(v_max, 8);\n        __m128i cmp = _mm_cmpgt_epi8(v_max, v_shifted);\n        v_max = _mm_or_si128(_mm_and_si128(cmp, v_max), _mm_andnot_si128(cmp, v_shifted));\n        \n        v_shifted = _mm_srli_si128(v_max, 4);\n        cmp = _mm_cmpgt_epi8(v_max, v_shifted);\n        v_max = _mm_or_si128(_mm_and_si128(cmp, v_max), _mm_andnot_si128(cmp, v_shifted));\n        \n        v_shifted = _mm_srli_si128(v_max, 2);\n        cmp = _mm_cmpgt_epi8(v_max, v_shifted);\n        v_max = _mm_or_si128(_mm_and_si128(cmp, v_max), _mm_andnot_si128(cmp, v_shifted));\n        \n        v_shifted = _mm_srli_si128(v_max, 1);\n        cmp = _mm_cmpgt_epi8(v_max, v_shifted);\n        v_max = _mm_or_si128(_mm_and_si128(cmp, v_max), _mm_andnot_si128(cmp, v_shifted));\n        \n        int8_t block_max = (int8_t)(_mm_cvtsi128_si32(v_max) & 0xFF);\n\n        if (block_max > global_max) {\n            global_max = block_max;\n            __m128i v_broadcast = _mm_set1_epi8(global_max);\n            __m128i mask = _mm_cmpeq_epi8(v, v_broadcast);\n            unsigned int m = (unsigned int)_mm_movemask_epi8(mask);\n            int pos = __builtin_ctz(m);\n            global_index = i + pos;\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_index = i;\n        }\n    }\n\n    return global_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n    size_t global_max_index = 0;\n    int8_t global_max = src[0];\n    size_t i = 0;\n\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m128i vec = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i global_max_vec = _mm_set1_epi8(global_max);\n            __m128i cmp = _mm_cmpgt_epi8(vec, global_max_vec);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                for (int j = 0; j < 16; j++) {\n                    if (src[i + j] > global_max) {\n                        global_max = src[i + j];\n                        global_max_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max) {\n            global_max = src[i];\n            global_max_index = i;\n        }\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n    int8_t current_max = src[0];\n    size_t current_index = 0;\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i v_cur_max = _mm_set1_epi8(current_max);\n        __m128i cmp = _mm_cmpgt_epi8(chunk, v_cur_max);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            int8_t temp_max = src[i];\n            for (int j = 1; j < 16; j++) {\n                if (src[i + j] > temp_max) {\n                    temp_max = src[i + j];\n                }\n            }\n            if (temp_max > current_max) {\n                current_max = temp_max;\n                for (int j = 0; j < 16; j++) {\n                    if (src[i + j] == current_max) {\n                        current_index = i + j;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > current_max) {\n            current_max = src[i];\n            current_index = i;\n        }\n    }\n\n    return current_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    int8_t global_max_value = -128;\n    size_t global_max_index = 0;\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i zero = _mm_setzero_si128();\n        __m128i sign_mask = _mm_cmpgt_epi8(zero, chunk);\n        __m128i low_part = _mm_unpacklo_epi8(chunk, sign_mask);\n        __m128i high_part = _mm_unpackhi_epi8(chunk, sign_mask);\n        __m128i max16 = _mm_max_epi16(low_part, high_part);\n        __m128i max1 = _mm_max_epi16(max16, _mm_srli_si128(max16, 8));\n        __m128i max2 = _mm_max_epi16(max1, _mm_srli_si128(max1, 4));\n        __m128i max3 = _mm_max_epi16(max2, _mm_srli_si128(max2, 2));\n        int max_val_16 = _mm_extract_epi16(max3, 0);\n        int8_t max_in_chunk = (int8_t)max_val_16;\n\n        if (max_in_chunk > global_max_value) {\n            global_max_value = max_in_chunk;\n            __m128i v_max = _mm_set1_epi8(global_max_value);\n            __m128i cmp = _mm_cmpeq_epi8(chunk, v_max);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                int pos = __builtin_ctz(mask);\n                global_max_index = i + pos;\n            }\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > global_max_value) {\n            global_max_value = src[i];\n            global_max_index = i;\n        }\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        __m128i offset_base = _mm_set_epi32(3, 2, 1, 0);\n        __m128i four = _mm_set1_epi32(4);\n        __m128i zero32 = _mm_setzero_si128();\n        __m128i vmask = _mm_set1_epi32(mask);\n\n        for (; i <= length - 8; i += 8) {\n            __m128i base_vec = _mm_set1_epi32(i);\n            __m128i indices1 = _mm_add_epi32(base_vec, offset_base);\n            __m128i indices2 = _mm_add_epi32(indices1, four);\n            \n            __m128i and1 = _mm_and_si128(indices1, vmask);\n            __m128i and2 = _mm_and_si128(indices2, vmask);\n            \n            __m128i cmp1 = _mm_cmpeq_epi32(and1, zero32);\n            __m128i cmp2 = _mm_cmpeq_epi32(and2, zero32);\n            \n            __m128i blend_mask = _mm_packs_epi32(cmp1, cmp2);\n            \n            __m128i data1 = _mm_loadu_si128((const __m128i*)(src1 + i));\n            __m128i data2 = _mm_loadu_si128((const __m128i*)(src2 + i));\n            \n            __m128i res = _mm_or_si128(\n                _mm_and_si128(blend_mask, data2),\n                _mm_andnot_si128(blend_mask, data1)\n            );\n            _mm_storeu_si128((__m128i*)(dst + i), res);\n        }\n    }\n    \n    for (; i < length; i++) {\n        if (((uint32_t)i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t step = 8;\n    __m128i all_ones = _mm_set1_epi32(-1);\n\n    for (; i + step <= length; i += step) {\n        __m128i s1 = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i s2 = _mm_loadu_si128((const __m128i*)(src2 + i));\n\n        __m128i base_vec0 = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i base_vec1 = _mm_set_epi32(i+7, i+6, i+5, i+4);\n        __m128i vmask = _mm_set1_epi32(mask);\n\n        __m128i cond0 = _mm_and_si128(base_vec0, vmask);\n        __m128i cond1 = _mm_and_si128(base_vec1, vmask);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask32_0 = _mm_cmpeq_epi32(cond0, zero);\n        __m128i mask32_1 = _mm_cmpeq_epi32(cond1, zero);\n\n        mask32_0 = _mm_xor_si128(mask32_0, all_ones);\n        mask32_1 = _mm_xor_si128(mask32_1, all_ones);\n\n        __m128i mask16 = _mm_packs_epi32(mask32_0, mask32_1);\n        __m128i part1 = _mm_and_si128(mask16, s1);\n        __m128i part2 = _mm_andnot_si128(mask16, s2);\n        __m128i res = _mm_or_si128(part1, part2);\n\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; i++) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        for (; i <= length - 8; i += 8) {\n            uint32_t base_low = (uint32_t)i;\n            __m128i base_vec = _mm_set1_epi32(base_low);\n            __m128i offsets1 = _mm_setr_epi32(0, 1, 2, 3);\n            __m128i idx1 = _mm_add_epi32(base_vec, offsets1);\n            __m128i offsets2 = _mm_setr_epi32(4, 5, 6, 7);\n            __m128i idx2 = _mm_add_epi32(base_vec, offsets2);\n            \n            __m128i mask_vec = _mm_set1_epi32(mask);\n            __m128i and1 = _mm_and_si128(idx1, mask_vec);\n            __m128i and2 = _mm_and_si128(idx2, mask_vec);\n            \n            __m128i zero = _mm_setzero_si128();\n            __m128i cond1 = _mm_cmpeq_epi32(and1, zero);\n            __m128i cond2 = _mm_cmpeq_epi32(and2, zero);\n            \n            __m128i cond16 = _mm_packs_epi32(cond1, cond2);\n            \n            __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n            __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n            \n            __m128i result = _mm_or_si128(\n                _mm_and_si128(cond16, v2),\n                _mm_andnot_si128(cond16, v1)\n            );\n            \n            _mm_storeu_si128((__m128i*)(dst + i), result);\n        }\n    }\n    \n    for (; i < length; i++) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    if (length == 0) {\n        return;\n    }\n    \n    const __m128i base0 = _mm_set_epi32(3, 2, 1, 0);\n    const __m128i base1 = _mm_set_epi32(7, 6, 5, 4);\n    __m128i v_mask = _mm_set1_epi32(mask);\n    __m128i zero = _mm_setzero_si128();\n    __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i idx0 = _mm_add_epi32(v_i, base0);\n        __m128i idx1 = _mm_add_epi32(v_i, base1);\n        \n        __m128i and0 = _mm_and_si128(idx0, v_mask);\n        __m128i and1 = _mm_and_si128(idx1, v_mask);\n        \n        __m128i cmp0 = _mm_cmpeq_epi32(and0, zero);\n        __m128i cmp1 = _mm_cmpeq_epi32(and1, zero);\n        \n        cmp0 = _mm_xor_si128(cmp0, all_ones);\n        cmp1 = _mm_xor_si128(cmp1, all_ones);\n        \n        __m128i cond16 = _mm_packs_epi32(cmp0, cmp1);\n        \n        __m128i v_src1 = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i v_src2 = _mm_loadu_si128((const __m128i*)(src2 + i));\n        \n        __m128i blended = _mm_or_si128(\n            _mm_and_si128(v_src1, cond16),\n            _mm_andnot_si128(cond16, v_src2)\n        );\n        \n        _mm_storeu_si128((__m128i*)(dst + i), blended);\n    }\n    \n    for (; i < length; i++) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m128i mask_vec = _mm_set1_epi32(mask);\n        for (; i <= length - 4; i += 4) {\n            __m128i s1 = _mm_loadl_epi64((const __m128i*)(src1 + i));\n            __m128i s2 = _mm_loadl_epi64((const __m128i*)(src2 + i));\n            uint32_t i0 = (uint32_t)i;\n            uint32_t i1 = (uint32_t)(i + 1);\n            uint32_t i2 = (uint32_t)(i + 2);\n            uint32_t i3 = (uint32_t)(i + 3);\n            __m128i indices = _mm_setr_epi32(i0, i1, i2, i3);\n            __m128i and_res = _mm_and_si128(indices, mask_vec);\n            __m128i cmp = _mm_cmpeq_epi32(and_res, _mm_setzero_si128());\n            __m128i cond16 = _mm_packs_epi32(cmp, _mm_setzero_si128());\n            __m128i part1 = _mm_andnot_si128(cond16, s1);\n            __m128i part2 = _mm_and_si128(cond16, s2);\n            __m128i result = _mm_or_si128(part1, part2);\n            _mm_storel_epi64((__m128i*)(dst + i), result);\n        }\n    }\n    for (; i < length; i++) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(src + i));\n        const __m128i mask1 = _mm_set1_epi32(0x55555555);\n        const __m128i mask2 = _mm_set1_epi32(0x33333333);\n        const __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        const __m128i mask4 = _mm_set1_epi32(0x0000003F);\n\n        __m128i count = a;\n        count = _mm_add_epi32(_mm_and_si128(count, mask1), \n                _mm_and_si128(_mm_srli_epi32(count, 1), mask1));\n        count = _mm_add_epi32(_mm_and_si128(count, mask2), \n                _mm_and_si128(_mm_srli_epi32(count, 2), mask2));\n        count = _mm_add_epi32(count, _mm_srli_epi32(count, 4));\n        count = _mm_and_si128(count, mask3);\n        count = _mm_add_epi32(count, _mm_srli_epi32(count, 8));\n        count = _mm_add_epi32(count, _mm_srli_epi32(count, 16));\n        count = _mm_and_si128(count, mask4);\n\n        _mm_storeu_si128((__m128i*)(dst + i), count);\n    }\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        __m128i lookup = _mm_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3,\n            1, 2, 2, 3, 2, 3, 3, 4\n        );\n        for (; i <= length - 4; i += 4) {\n            __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n            __m128i lo_nib = _mm_and_si128(v, _mm_set1_epi8(0x0F));\n            __m128i hi_nib = _mm_and_si128(_mm_srli_epi16(v, 4), _mm_set1_epi8(0x0F));\n            __m128i pop_lo = _mm_shuffle_epi8(lookup, lo_nib);\n            __m128i pop_hi = _mm_shuffle_epi8(lookup, hi_nib);\n            __m128i pop_per_byte = _mm_add_epi8(pop_lo, pop_hi);\n            __m128i sum16 = _mm_maddubs_epi16(pop_per_byte, _mm_set1_epi8(1));\n            __m128i sum32 = _mm_madd_epi16(sum16, _mm_set1_epi16(1));\n            _mm_storeu_si128((__m128i*)(dst + i), sum32);\n        }\n    }\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i t = _mm_srli_epi32(v, 1);\n        t = _mm_and_si128(t, mask1);\n        v = _mm_sub_epi32(v, t);\n        t = _mm_srli_epi32(v, 2);\n        v = _mm_and_si128(v, mask2);\n        t = _mm_and_si128(t, mask2);\n        v = _mm_add_epi32(v, t);\n        t = _mm_srli_epi32(v, 4);\n        v = _mm_add_epi32(v, t);\n        v = _mm_and_si128(v, mask3);\n        uint32_t temp[4] __attribute__((aligned(16)));\n        _mm_store_si128((__m128i*)temp, v);\n        for (int j = 0; j < 4; ++j) {\n            dst[i + j] = (temp[j] * 0x01010101) >> 24;\n        }\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        dst[i] = (x * 0x01010101) >> 24;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t alignedLength = length & ~3;\n\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i mask5 = _mm_set1_epi32(0x000000FF);\n\n    for (; i < alignedLength; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i t = _mm_srli_epi32(v, 1);\n        t = _mm_and_si128(t, mask1);\n        v = _mm_sub_epi32(v, t);\n\n        t = _mm_srli_epi32(v, 2);\n        t = _mm_and_si128(t, mask2);\n        v = _mm_and_si128(v, mask2);\n        v = _mm_add_epi32(v, t);\n\n        v = _mm_add_epi32(v, _mm_srli_epi32(v, 4));\n        v = _mm_and_si128(v, mask3);\n\n        v = _mm_add_epi32(v, _mm_srli_epi32(v, 8));\n        v = _mm_and_si128(v, mask4);\n        v = _mm_add_epi32(v, _mm_srli_epi32(v, 16));\n        v = _mm_and_si128(v, mask5);\n\n        _mm_storeu_si128((__m128i*)(dst + i), v);\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x + (x >> 8)) & 0x00FF00FF;\n        x = (x + (x >> 16));\n        dst[i] = x & 0xFF;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const __m128i m1 = _mm_set1_epi32(0x55555555);\n    const __m128i m2 = _mm_set1_epi32(0x33333333);\n    const __m128i m3 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i m4 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i m5 = _mm_set1_epi32(0x0000FFFF);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i x = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i t = _mm_srli_epi32(x, 1);\n        t = _mm_and_si128(t, m1);\n        x = _mm_and_si128(x, m1);\n        x = _mm_add_epi32(x, t);\n\n        t = _mm_srli_epi32(x, 2);\n        t = _mm_and_si128(t, m2);\n        x = _mm_and_si128(x, m2);\n        x = _mm_add_epi32(x, t);\n\n        t = _mm_srli_epi32(x, 4);\n        t = _mm_and_si128(t, m3);\n        x = _mm_and_si128(x, m3);\n        x = _mm_add_epi32(x, t);\n\n        t = _mm_srli_epi32(x, 8);\n        t = _mm_and_si128(t, m4);\n        x = _mm_and_si128(x, m4);\n        x = _mm_add_epi32(x, t);\n\n        t = _mm_srli_epi32(x, 16);\n        t = _mm_and_si128(t, m5);\n        x = _mm_and_si128(x, m5);\n        x = _mm_add_epi32(x, t);\n\n        _mm_storeu_si128((__m128i*)(dst + i), x);\n    }\n\n    for (; i < length; i++) {\n        uint32_t w = src[i];\n        w = (w & 0x55555555) + ((w >> 1) & 0x55555555);\n        w = (w & 0x33333333) + ((w >> 2) & 0x33333333);\n        w = (w & 0x0F0F0F0F) + ((w >> 4) & 0x0F0F0F0F);\n        w = (w & 0x00FF00FF) + ((w >> 8) & 0x00FF00FF);\n        w = (w & 0x0000FFFF) + ((w >> 16) & 0x0000FFFF);\n        dst[i] = w;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    size_t simd_iters = length / simd_elements;\n\n    for (; i < simd_iters * simd_elements; i += simd_elements) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    const size_t simd_limit = length - (length % simd_elements);\n\n    for (; i < simd_limit; i += simd_elements) {\n        __m128i v1 = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i vsum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)(dst + i), vsum);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    size_t end = length & ~(simd_elements - 1);\n\n    for (; i < end; i += simd_elements) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i result = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    const size_t simd_steps = length / simd_elements;\n\n    for (; i < simd_steps * simd_elements; i += simd_elements) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i res = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 8;\n    const size_t simd_steps = length / simd_elements;\n\n    for (; i < simd_steps * simd_elements; i += simd_elements) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(src1 + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(src2 + i));\n        __m128i sum = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = sum > UINT16_MAX ? UINT16_MAX : sum;\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d low = _mm_set1_pd(-100.0);\n    __m128d high = _mm_set1_pd(100.0);\n    \n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            double sum = 0.0;\n            size_t k = 0;\n            \n            if (j < p - 1) {\n                __m128d sum_vec = _mm_setzero_pd();\n                for (; k < n; k++) {\n                    double a_val = A[i * n + k];\n                    __m128d a_vec = _mm_set1_pd(a_val);\n                    __m128d a_mask = _mm_and_pd(_mm_cmpge_pd(a_vec, low), _mm_cmple_pd(a_vec, high));\n                    \n                    __m128d b_vec = _mm_loadu_pd(B + k * p + j);\n                    __m128d b_mask = _mm_and_pd(_mm_cmpge_pd(b_vec, low), _mm_cmple_pd(b_vec, high));\n                    \n                    __m128d mask = _mm_and_pd(a_mask, b_mask);\n                    b_vec = _mm_and_pd(b_vec, mask);\n                    \n                    sum_vec = _mm_add_pd(sum_vec, _mm_mul_pd(a_vec, b_vec));\n                }\n                double tmp[2];\n                _mm_storeu_pd(tmp, sum_vec);\n                sum = tmp[0];\n                j++;\n            }\n            \n            for (; k < n; k++) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m * p; i++) {\n        C[i] = 0.0;\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t k = 0; k < n; k++) {\n            double a_val = A[i * n + k];\n            if (a_val >= -100.0 && a_val <= 100.0) {\n                __m128d a_vec = _mm_set1_pd(a_val);\n                size_t j = 0;\n                for (; j + 1 < p; j += 2) {\n                    __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n                    __m128d low_bound = _mm_set1_pd(-100.0);\n                    __m128d high_bound = _mm_set1_pd(100.0);\n                    __m128d low_mask = _mm_cmpge_pd(b_vec, low_bound);\n                    __m128d high_mask = _mm_cmple_pd(b_vec, high_bound);\n                    __m128d mask = _mm_and_pd(low_mask, high_mask);\n                    __m128d product = _mm_mul_pd(a_vec, b_vec);\n                    product = _mm_and_pd(product, mask);\n                    __m128d c_vec = _mm_loadu_pd(&C[i * p + j]);\n                    c_vec = _mm_add_pd(c_vec, product);\n                    _mm_storeu_pd(&C[i * p + j], c_vec);\n                }\n                if (j < p) {\n                    double b_val = B[k * p + j];\n                    if (b_val >= -100.0 && b_val <= 100.0) {\n                        C[i * p + j] += a_val * b_val;\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            C[i * p + j] = 0.0;\n        }\n    }\n\n    __m128d low_bound = _mm_set1_pd(-100.0);\n    __m128d high_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t k = 0; k < n; k++) {\n            double a_val = A[i * n + k];\n            if (a_val >= -100.0 && a_val <= 100.0) {\n                size_t j = 0;\n                for (; j + 1 < p; j += 2) {\n                    __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n                    __m128d mask_low = _mm_cmpge_pd(b_vec, low_bound);\n                    __m128d mask_high = _mm_cmple_pd(b_vec, high_bound);\n                    __m128d mask = _mm_and_pd(mask_low, mask_high);\n\n                    __m128d a_vec = _mm_set1_pd(a_val);\n                    __m128d prod = _mm_mul_pd(a_vec, b_vec);\n                    __m128d masked_prod = _mm_and_pd(prod, mask);\n\n                    __m128d c_vec = _mm_loadu_pd(&C[i * p + j]);\n                    c_vec = _mm_add_pd(c_vec, masked_prod);\n                    _mm_storeu_pd(&C[i * p + j], c_vec);\n                }\n                for (; j < p; j++) {\n                    double b_val = B[k * p + j];\n                    if (b_val >= -100.0 && b_val <= 100.0) {\n                        C[i * p + j] += a_val * b_val;\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    // Initialize C to zero\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            C[i*p + j] = 0.0;\n        }\n    }\n\n    // Constants for range checking\n    const __m128d low_bound = _mm_set1_pd(-100.0);\n    const __m128d high_bound = _mm_set1_pd(100.0);\n\n    // Matrix multiplication with condition\n    for (size_t i = 0; i < m; i++) {\n        for (size_t k = 0; k < n; k++) {\n            double a_val = A[i*n + k];\n            if (a_val < -100.0 || a_val > 100.0) {\n                continue;\n            }\n            __m128d a_vec = _mm_set1_pd(a_val);\n            size_t j = 0;\n            for (; j + 1 < p; j += 2) {\n                __m128d vb = _mm_loadu_pd(B + k*p + j);\n                __m128d maskB = _mm_and_pd(_mm_cmpge_pd(vb, low_bound), _mm_cmple_pd(vb, high_bound));\n                __m128d product = _mm_mul_pd(a_vec, vb);\n                product = _mm_and_pd(product, maskB);\n                __m128d c_vec = _mm_loadu_pd(C + i*p + j);\n                c_vec = _mm_add_pd(c_vec, product);\n                _mm_storeu_pd(C + i*p + j, c_vec);\n            }\n            if (j < p) {\n                double b_val = B[k*p + j];\n                if (b_val >= -100.0 && b_val <= 100.0) {\n                    C[i*p + j] += a_val * b_val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t total = m * p;\n    size_t idx = 0;\n    for (; idx + 1 < total; idx += 2) {\n        _mm_storeu_pd(C + idx, _mm_setzero_pd());\n    }\n    if (idx < total) {\n        C[idx] = 0.0;\n    }\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t k = 0; k < n; k++) {\n            double a_val = A[i * n + k];\n            if (a_val < -100.0 || a_val > 100.0) {\n                continue;\n            }\n            size_t j = 0;\n            __m128d a_vec = _mm_set1_pd(a_val);\n            __m128d low_bound = _mm_set1_pd(-100.0);\n            __m128d high_bound = _mm_set1_pd(100.0);\n            for (; j + 1 < p; j += 2) {\n                __m128d b_vec = _mm_loadu_pd(B + k * p + j);\n                __m128d mask1 = _mm_cmpge_pd(b_vec, low_bound);\n                __m128d mask2 = _mm_cmple_pd(b_vec, high_bound);\n                __m128d mask = _mm_and_pd(mask1, mask2);\n                __m128d prod = _mm_mul_pd(a_vec, b_vec);\n                __m128d masked_prod = _mm_and_pd(prod, mask);\n                __m128d c_vec = _mm_loadu_pd(C + i * p + j);\n                c_vec = _mm_add_pd(c_vec, masked_prod);\n                _mm_storeu_pd(C + i * p + j, c_vec);\n            }\n            if (j < p) {\n                double b_val = B[k * p + j];\n                if (b_val >= -100.0 && b_val <= 100.0) {\n                    C[i * p + j] += a_val * b_val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const size_t simd_elements = total_elements / 2;\n    \n    for (; i < simd_elements; ++i) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i * 2));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i * 2));\n        __m128i sum = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i * 2), sum);\n    }\n    \n    i *= 2;\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const size_t vectorized_size = total_elements - (total_elements % 2);\n\n    for (; i < vectorized_size; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c_vec = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n    \n    if (i < total) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total - 2; i += 2) {\n        __m128i a_chunk = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_chunk = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i c_chunk = _mm_add_epi64(a_chunk, b_chunk);\n        _mm_storeu_si128((__m128i*)(C + i), c_chunk);\n    }\n\n    if (i < total) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 1 < total; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c_vec = _mm_add_epi64(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < total; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m128 sign_mask = _mm_set1_ps(-0.0f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    size_t i = 0;\n    \n    for (; i + 3 < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(v1, v2);\n        \n        __m128 sign = _mm_and_ps(sum, sign_mask);\n        __m128 abs_val = _mm_andnot_ps(sign_mask, sum);\n        __m128 abs_plus_half = _mm_add_ps(abs_val, half);\n        __m128i trunc = _mm_cvttps_epi32(abs_plus_half);\n        __m128 rounded_abs = _mm_cvtepi32_ps(trunc);\n        __m128 result = _mm_or_ps(rounded_abs, sign);\n        \n        _mm_storeu_ps(dst + i, result);\n    }\n    \n    for (; i < length; i++) {\n        float s = src1[i] + src2[i];\n        if (s >= 0.0f) \n            dst[i] = (int)(s + 0.5f);\n        else \n            dst[i] = (int)(s - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const __m128 sign_mask = _mm_set1_ps(-0.0f);\n    const __m128 half = _mm_set1_ps(0.5f);\n    \n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        \n        __m128 sign = _mm_and_ps(sum, sign_mask);\n        __m128 abs_val = _mm_andnot_ps(sign, sum);\n        __m128 abs_plus_half = _mm_add_ps(abs_val, half);\n        __m128i truncated = _mm_cvttps_epi32(abs_plus_half);\n        __m128 rounded_abs = _mm_cvtepi32_ps(truncated);\n        __m128 result = _mm_or_ps(rounded_abs, sign);\n        \n        _mm_storeu_ps(dst + i, result);\n    }\n    \n    for (; i < length; i++) {\n        float sum = src1[i] + src2[i];\n        float abs_val = (sum < 0) ? -sum : sum;\n        float abs_plus_half = abs_val + 0.5f;\n        int truncated = (int)abs_plus_half;\n        float rounded_abs = (float)truncated;\n        dst[i] = (sum < 0) ? -rounded_abs : rounded_abs;\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 4;\n    size_t end = length / simd_width * simd_width;\n\n    for (; i < end; i += simd_width) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128i rounded_int = _mm_cvtps_epi32(sum);\n        __m128 rounded_float = _mm_cvtepi32_ps(rounded_int);\n        _mm_storeu_ps(dst + i, rounded_float);\n    }\n\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = static_cast<float>(static_cast<int>(sum + (sum >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 4;\n    const size_t simd_bound = length - (length % simd_width);\n\n    for (; i < simd_bound; i += simd_width) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128i int_sum = _mm_cvtps_epi32(sum);\n        __m128 rounded = _mm_cvtepi32_ps(int_sum);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = static_cast<float>(static_cast<int>(sum + (sum >= 0 ? 0.5f : -0.5f)));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t end = length & ~3;\n\n    for (; i < end; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(a, b);\n        __m128i sum_i = _mm_cvtps_epi32(sum);\n        __m128 rounded = _mm_cvtepi32_ps(sum_i);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; i++) {\n        __m128 a = _mm_load_ss(src1 + i);\n        __m128 b = _mm_load_ss(src2 + i);\n        __m128 sum = _mm_add_ss(a, b);\n        int32_t rounded_i = _mm_cvtss_si32(sum);\n        dst[i] = (float)rounded_i;\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m128 abs_mask = _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF));\n    const __m128i even_mask_i = _mm_set_epi32(0, -1, 0, -1);\n    const __m128 even_mask = _mm_castsi128_ps(even_mask_i);\n    const __m128 fifty = _mm_set1_ps(50.0f);\n    const __m128 minus_one = _mm_set1_ps(-1.0f);\n    const __m128 one = _mm_set1_ps(1.0f);\n    const __m128 zero = _mm_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_and_ps(diff, abs_mask);\n        __m128 cond1 = _mm_cmple_ps(abs_diff, fifty);\n        __m128 cond2 = even_mask;\n        __m128 combined_cond = _mm_and_ps(cond1, cond2);\n\n        __m128 product = _mm_mul_ps(a, b);\n        __m128i trunc_i = _mm_cvttps_epi32(product);\n        __m128 trunc_f = _mm_cvtepi32_ps(trunc_i);\n        __m128 fract = _mm_sub_ps(product, trunc_f);\n        __m128 mask_inc = _mm_cmpgt_ps(fract, zero);\n        __m128 inc = _mm_and_ps(mask_inc, one);\n        __m128 ceil_val = _mm_add_ps(trunc_f, inc);\n\n        __m128 res = _mm_or_ps(\n            _mm_and_ps(combined_cond, ceil_val),\n            _mm_andnot_ps(combined_cond, minus_one)\n        );\n        _mm_storeu_ps(dst + i, res);\n    }\n\n    for (; i < length; i++) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f) {\n            float product = src1[i] * src2[i];\n            dst[i] = ceilf(product);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    static const __m128i even_mask_i = _mm_setr_epi32(-1, 0, -1, 0);\n    __m128 mask_even = _mm_castsi128_ps(even_mask_i);\n    __m128 sign_mask = _mm_set1_ps(-0.0f);\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 zeros = _mm_setzero_ps();\n    __m128 ones = _mm_set1_ps(1.0f);\n    __m128 default_val = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_andnot_ps(sign_mask, diff);\n        __m128 cond1 = _mm_cmple_ps(abs_diff, threshold);\n        __m128 cond = _mm_and_ps(cond1, mask_even);\n        __m128 product = _mm_mul_ps(v1, v2);\n        __m128i int_part = _mm_cvttps_epi32(product);\n        __m128 t = _mm_cvtepi32_ps(int_part);\n        __m128 mask_positive = _mm_cmpge_ps(product, zeros);\n        __m128 mask_positive_non_integer = _mm_and_ps(mask_positive, _mm_cmpgt_ps(product, t));\n        __m128 increment = _mm_and_ps(mask_positive_non_integer, ones);\n        __m128 ceil_product = _mm_add_ps(t, increment);\n        __m128 result = _mm_or_ps(_mm_and_ps(cond, ceil_product), _mm_andnot_ps(cond, default_val));\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < length; i++) {\n        if (i % 2 == 0) {\n            float diff = src1[i] - src2[i];\n            float abs_diff = (diff < 0) ? -diff : diff;\n            if (abs_diff <= 50.0f) {\n                float prod = src1[i] * src2[i];\n                if (prod >= 0.0f) {\n                    float t_val = (float)((int)(prod));\n                    if (prod > t_val) {\n                        dst[i] = t_val + 1.0f;\n                    } else {\n                        dst[i] = t_val;\n                    }\n                } else {\n                    dst[i] = (float)((int)(prod));\n                }\n            } else {\n                dst[i] = -1.0f;\n            }\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(src1 + i);\n        __m128 v2 = _mm_loadu_ps(src2 + i);\n        \n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_max_ps(_mm_sub_ps(_mm_setzero_ps(), diff), diff);\n        __m128 cond_a = _mm_cmple_ps(abs_diff, _mm_set1_ps(50.0f));\n        \n        __m128i indices = _mm_setr_epi32(i, i+1, i+2, i+3);\n        __m128i one = _mm_set1_epi32(1);\n        __m128i even_bits = _mm_and_si128(indices, one);\n        __m128i cond_b = _mm_cmpeq_epi32(even_bits, _mm_setzero_si128());\n        __m128 cond_b_ps = _mm_castsi128_ps(cond_b);\n        \n        __m128 mask = _mm_and_ps(cond_a, cond_b_ps);\n        \n        __m128 product = _mm_mul_ps(v1, v2);\n        __m128i int_part = _mm_cvttps_epi32(product);\n        __m128 truncated = _mm_cvtepi32_ps(int_part);\n        \n        __m128 mask_positive = _mm_cmpgt_ps(product, _mm_setzero_ps());\n        __m128 mask_fraction = _mm_cmpneq_ps(truncated, product);\n        __m128 adjust = _mm_and_ps(mask_positive, mask_fraction);\n        adjust = _mm_and_ps(adjust, _mm_set1_ps(1.0f));\n        __m128 ceil_value = _mm_add_ps(truncated, adjust);\n        \n        __m128 result_vec = _mm_or_ps(\n            _mm_and_ps(mask, ceil_value),\n            _mm_andnot_ps(mask, _mm_set1_ps(-1.0f))\n        );\n        \n        _mm_storeu_ps(dst + i, result_vec);\n    }\n    \n    for (; i < length; i++) {\n        float diff = src1[i] - src2[i];\n        float abs_diff = diff;\n        if (abs_diff < 0) abs_diff = -abs_diff;\n        \n        if (i % 2 == 0 && abs_diff <= 50.0f) {\n            float product = src1[i] * src2[i];\n            float truncated = (float)((int)product);\n            if (product > 0.0f && truncated != product) {\n                truncated += 1.0f;\n            }\n            dst[i] = truncated;\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    const __m128 neg_zero = _mm_set1_ps(-0.0f);\n    const __m128 one = _mm_set1_ps(1.0f);\n    const __m128 minus_one = _mm_set1_ps(-1.0f);\n    const __m128 threshold = _mm_set1_ps(50.0f);\n    const __m128i one_epi32 = _mm_set1_epi32(1);\n    const __m128i zero_epi32 = _mm_setzero_si128();\n\n    for (; i + 3 < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        \n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 abs_diff = _mm_andnot_ps(neg_zero, diff);\n        __m128 mask1 = _mm_cmple_ps(abs_diff, threshold);\n        \n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mod_mask = _mm_and_si128(indices, one_epi32);\n        __m128i mask2_i = _mm_cmpeq_epi32(mod_mask, zero_epi32);\n        __m128 mask2 = _mm_castsi128_ps(mask2_i);\n        \n        __m128 cond_mask = _mm_and_ps(mask1, mask2);\n        __m128 product = _mm_mul_ps(v1, v2);\n        \n        __m128i trunc_int = _mm_cvttps_epi32(product);\n        __m128 truncated = _mm_cvtepi32_ps(trunc_int);\n        __m128 cmp_gt_zero = _mm_cmpgt_ps(product, _mm_setzero_ps());\n        __m128 cmp_neq = _mm_cmpneq_ps(product, truncated);\n        __m128 adjust_mask = _mm_and_ps(cmp_gt_zero, cmp_neq);\n        __m128 adjust = _mm_and_ps(adjust_mask, one);\n        __m128 ceil_val = _mm_add_ps(truncated, adjust);\n        \n        __m128 result = _mm_or_ps(\n            _mm_and_ps(cond_mask, ceil_val),\n            _mm_andnot_ps(cond_mask, minus_one)\n        );\n        \n        _mm_storeu_ps(&dst[i], result);\n    }\n    \n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) <= 50.0f)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m128 abs_threshold = _mm_set1_ps(50.0f);\n    const __m128 ones = _mm_set1_ps(1.0f);\n    const __m128 neg_ones = _mm_set1_ps(-1.0f);\n    const __m128 zero = _mm_setzero_ps();\n    const __m128i one = _mm_set1_epi32(1);\n    const __m128i zero_epi32 = _mm_setzero_si128();\n    const __m128 abs_mask = _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF));\n    \n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(src1 + i);\n        __m128 b = _mm_loadu_ps(src2 + i);\n        \n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_and_ps(diff, abs_mask);\n        __m128 cond1_mask = _mm_cmple_ps(abs_diff, abs_threshold);\n        \n        __m128i base_idx = _mm_set1_epi32(i);\n        __m128i offsets = _mm_set_epi32(3, 2, 1, 0);\n        __m128i indices = _mm_add_epi32(base_idx, offsets);\n        __m128i idx_mask = _mm_and_si128(indices, one);\n        __m128i even_mask_int = _mm_cmpeq_epi32(idx_mask, zero_epi32);\n        __m128 cond2_mask = _mm_castsi128_ps(even_mask_int);\n        \n        __m128 total_mask = _mm_and_ps(cond1_mask, cond2_mask);\n        \n        __m128 prod = _mm_mul_ps(a, b);\n        __m128i trunc_int = _mm_cvttps_epi32(prod);\n        __m128 trunc_val = _mm_cvtepi32_ps(trunc_int);\n        __m128 rem = _mm_sub_ps(prod, trunc_val);\n        __m128 ceil_mask = _mm_cmpgt_ps(rem, zero);\n        __m128 ceil_val = _mm_add_ps(trunc_val, _mm_and_ps(ceil_mask, ones));\n        \n        __m128 res = _mm_or_ps(_mm_and_ps(total_mask, ceil_val), \n                              _mm_andnot_ps(total_mask, neg_ones));\n        _mm_storeu_ps(dst + i, res);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & 1) == 0) {\n            float d = src1[i] - src2[i];\n            float abs_d = (d < 0) ? -d : d;\n            if (abs_d <= 50.0f) {\n                float product = src1[i] * src2[i];\n                int trunc_val_int = (int)product;\n                float trunc_val = (float)trunc_val_int;\n                float rem = product - trunc_val;\n                if (rem > 0) {\n                    dst[i] = trunc_val + 1.0f;\n                } else {\n                    dst[i] = trunc_val;\n                }\n            } else {\n                dst[i] = -1.0f;\n            }\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t total = m * n;\n    size_t i = 0;\n\n    for (; i + 1 < total; i += 2) {\n        __m128d data = _mm_loadu_pd(mat + i);\n        __m128d result = _mm_mul_pd(data, scalar_vec);\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    if (i < total) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 1 < total; i += 2) {\n        __m128d data = _mm_loadu_pd(mat + i);\n        __m128d result = _mm_mul_pd(data, scalar_vec);\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    if (i < total) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d v_scalar = _mm_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 1 < total; i += 2) {\n        __m128d v_mat = _mm_loadu_pd(&mat[i]);\n        __m128d v_res = _mm_mul_pd(v_mat, v_scalar);\n        _mm_storeu_pd(&dst[i], v_res);\n    }\n\n    if (i < total) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d vscalar = _mm_set1_pd(scalar);\n    size_t i = 0;\n\n    for (; i + 1 < total; i += 2) {\n        __m128d data = _mm_loadu_pd(mat + i);\n        __m128d result = _mm_mul_pd(data, vscalar);\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    if (i < total) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t rounded_down = total_elements & ~1ULL;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < rounded_down; i += 2) {\n        __m128d data = _mm_loadu_pd(mat + i);\n        __m128d result = _mm_mul_pd(data, scalar_vec);\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    if (rounded_down < total_elements) {\n        dst[rounded_down] = mat[rounded_down] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 x = _mm_loadu_ps(row + j);\n            sum_sq = _mm_add_ps(sum_sq, _mm_mul_ps(x, x));\n        }\n\n        __m128 swap = _mm_movehl_ps(sum_sq, sum_sq);\n        __m128 sum_temp = _mm_add_ps(sum_sq, swap);\n        __m128 shuf = _mm_shuffle_ps(sum_temp, sum_temp, _MM_SHUFFLE(0,0,0,1));\n        __m128 sum_scalar = _mm_add_ss(sum_temp, shuf);\n        float norm_sq = _mm_cvtss_f32(sum_scalar);\n\n        for (; j < n; j++) {\n            norm_sq += row[j] * row[j];\n        }\n\n        if (norm_sq == 0.0f) {\n            continue;\n        }\n\n        __m128 vsq = _mm_set_ss(norm_sq);\n        __m128 vnorm = _mm_sqrt_ss(vsq);\n        float norm = _mm_cvtss_f32(vnorm);\n        __m128 norm_vec = _mm_set1_ps(norm);\n\n        j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 x = _mm_loadu_ps(row + j);\n            __m128 normalized = _mm_div_ps(x, norm_vec);\n            _mm_storeu_ps(row + j, normalized);\n        }\n\n        for (; j < n; j++) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const float epsilon = 1e-8f;\n    \n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            sum_vec = _mm_add_ps(sum_vec, _mm_mul_ps(v, v));\n        }\n\n        __m128 high = _mm_movehl_ps(sum_vec, sum_vec);\n        __m128 sum1 = _mm_add_ps(sum_vec, high);\n        __m128 shuf = _mm_shuffle_ps(sum1, sum1, 0x1);\n        __m128 sum2 = _mm_add_ss(sum1, shuf);\n        float total_sq = _mm_cvtss_f32(sum2);\n\n        for (; j < n; j++) {\n            total_sq += row[j] * row[j];\n        }\n\n        float scale = (total_sq < epsilon) ? 0.0f : 1.0f / sqrtf(total_sq);\n        __m128 scale_v = _mm_set1_ps(scale);\n        j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            v = _mm_mul_ps(v, scale_v);\n            _mm_storeu_ps(row + j, v);\n        }\n\n        for (; j < n; j++) {\n            row[j] *= scale;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m128 acc0 = _mm_setzero_ps();\n        __m128 acc1 = _mm_setzero_ps();\n        __m128 acc2 = _mm_setzero_ps();\n        __m128 acc3 = _mm_setzero_ps();\n\n        size_t j = 0;\n        for (; j + 15 < n; j += 16) {\n            __m128 v0 = _mm_loadu_ps(row + j);\n            __m128 v1 = _mm_loadu_ps(row + j + 4);\n            __m128 v2 = _mm_loadu_ps(row + j + 8);\n            __m128 v3 = _mm_loadu_ps(row + j + 12);\n\n            acc0 = _mm_add_ps(acc0, _mm_mul_ps(v0, v0));\n            acc1 = _mm_add_ps(acc1, _mm_mul_ps(v1, v1));\n            acc2 = _mm_add_ps(acc2, _mm_mul_ps(v2, v2));\n            acc3 = _mm_add_ps(acc3, _mm_mul_ps(v3, v3));\n        }\n\n        acc0 = _mm_add_ps(acc0, acc1);\n        acc2 = _mm_add_ps(acc2, acc3);\n        acc0 = _mm_add_ps(acc0, acc2);\n\n        for (; j + 3 < n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            acc0 = _mm_add_ps(acc0, _mm_mul_ps(v, v));\n        }\n\n        __m128 shuf = _mm_shuffle_ps(acc0, acc0, _MM_SHUFFLE(2,3,0,1));\n        __m128 sums = _mm_add_ps(acc0, shuf);\n        shuf = _mm_movehl_ps(shuf, sums);\n        sums = _mm_add_ps(sums, shuf);\n        float sq_sum = _mm_cvtss_f32(sums);\n\n        for (; j < n; j++) {\n            float val = row[j];\n            sq_sum += val * val;\n        }\n\n        float scale = (sq_sum == 0.0f) ? 0.0f : 1.0f / sqrtf(sq_sum);\n        __m128 scale_v = _mm_set1_ps(scale);\n\n        j = 0;\n        for (; j + 3 < n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            v = _mm_mul_ps(v, scale_v);\n            _mm_storeu_ps(row + j, v);\n        }\n        for (; j < n; j++) {\n            row[j] *= scale;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        float* row = mat + i * n;\n        __m128 sum_sq = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process full SIMD chunks for squared sum\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            __m128 sq = _mm_mul_ps(vec, vec);\n            sum_sq = _mm_add_ps(sum_sq, sq);\n        }\n\n        // Horizontal sum of partial squared sums\n        __m128 shuf = _mm_movehl_ps(sum_sq, sum_sq);\n        __m128 sums = _mm_add_ps(sum_sq, shuf);\n        __m128 shuf2 = _mm_shuffle_ps(sums, sums, _MM_SHUFFLE(1,1,1,1));\n        __m128 total_vec = _mm_add_ss(sums, shuf2);\n        float total_sq = _mm_cvtss_f32(total_vec);\n\n        // Process remaining elements for squared sum\n        for (; j < n; j++) {\n            total_sq += row[j] * row[j];\n        }\n\n        // Compute scaling factor\n        float scale = (total_sq == 0.0f) ? 0.0f : 1.0f / sqrtf(total_sq);\n        __m128 scale_vec = _mm_set1_ps(scale);\n        j = 0;\n\n        // Process full SIMD chunks for scaling\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            vec = _mm_mul_ps(vec, scale_vec);\n            _mm_storeu_ps(row + j, vec);\n        }\n\n        // Process remaining elements for scaling\n        for (; j < n; j++) {\n            row[j] *= scale;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            __m128 sq = _mm_mul_ps(v, v);\n            sum_vec = _mm_add_ps(sum_vec, sq);\n        }\n\n        float sum = 0.0f;\n        for (; j < n; ++j) {\n            float val = row[j];\n            sum += val * val;\n        }\n\n        __m128 high = _mm_movehl_ps(sum_vec, sum_vec);\n        __m128 t1 = _mm_add_ps(sum_vec, high);\n        __m128 t2 = _mm_shuffle_ps(t1, t1, 0x01);\n        __m128 t3 = _mm_add_ss(t1, t2);\n        sum += _mm_cvtss_f32(t3);\n\n        float norm = sqrtf(sum);\n        float reciprocal_norm = (norm == 0.0f) ? 0.0f : (1.0f / norm);\n        __m128 norm_vec = _mm_set1_ps(reciprocal_norm);\n        j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 v = _mm_loadu_ps(row + j);\n            v = _mm_mul_ps(v, norm_vec);\n            _mm_storeu_ps(row + j, v);\n        }\n\n        for (; j < n; ++j) {\n            row[j] *= reciprocal_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t j_end = p & ~1;\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < j_end; j += 2) {\n            __m128d sum_vec = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m128d a = _mm_load1_pd(mat1 + i * n + k);\n                __m128d b = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d prod = _mm_mul_pd(a, b);\n                sum_vec = _mm_add_pd(sum_vec, prod);\n            }\n            __m128i int_vec = _mm_cvtpd_epi32(sum_vec);\n            __m128d rounded = _mm_cvtepi32_pd(int_vec);\n            _mm_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n\n    if (p % 2 != 0) {\n        size_t j = p - 1;\n        for (size_t i = 0; i < m; i++) {\n            __m128d sum_vec = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(mat1 + i * n + k);\n                double b0 = mat2[k * p + j];\n                double b1 = (k + 1 < n) ? mat2[(k + 1) * p + j] : 0.0;\n                __m128d b = _mm_set_pd(b1, b0);\n                __m128d prod = _mm_mul_pd(a, b);\n                sum_vec = _mm_add_pd(sum_vec, prod);\n            }\n            double tmp[2];\n            _mm_storeu_pd(tmp, sum_vec);\n            double total = tmp[0] + tmp[1];\n            if (n % 2 != 0) {\n                total += mat1[i * n + n - 1] * mat2[(n - 1) * p + j];\n            }\n            __m128d v = _mm_set_sd(total);\n            __m128i vi = _mm_cvtpd_epi32(v);\n            __m128d rounded_v = _mm_cvtepi32_pd(vi);\n            _mm_store_sd(dst + i * p + j, rounded_v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    const __m128d SIGNMASK = _mm_set1_pd(-0.0);\n    const __m128d HALF = _mm_set1_pd(0.5);\n    const __m128d MAGIC = _mm_set1_pd(6755399441055744.0);\n\n    for (size_t i = 0; i < m; i++) {\n        const double* mat1_row = mat1 + i * n;\n        double* dst_row = dst + i * p;\n        size_t j = 0;\n        \n        for (; j + 1 < p; j += 2) {\n            __m128d acc = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m128d a = _mm_set1_pd(mat1_row[k]);\n                __m128d b = _mm_loadu_pd(mat2 + k * p + j);\n                acc = _mm_add_pd(acc, _mm_mul_pd(a, b));\n            }\n            __m128d sign = _mm_and_pd(acc, SIGNMASK);\n            __m128d half_bias = _mm_xor_pd(HALF, sign);\n            __m128d t = _mm_add_pd(acc, half_bias);\n            __m128d rounded = _mm_sub_pd(_mm_add_pd(t, MAGIC), MAGIC);\n            _mm_storeu_pd(dst_row + j, rounded);\n        }\n        \n        if (j < p) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                sum += mat1_row[k] * mat2[k * p + j];\n            }\n            double sign = (sum < 0.0) ? -0.0 : 0.0;\n            double half_bias = (sum >= 0.0) ? 0.5 : -0.5;\n            double t = sum + half_bias;\n            double magic_val = 6755399441055744.0;\n            double rounded_val = (t + magic_val) - magic_val;\n            dst_row[j] = rounded_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j + 1 < p; j += 2) {\n            __m128d acc = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m128d a = _mm_load1_pd(mat1 + i * n + k);\n                __m128d b = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d prod = _mm_mul_pd(a, b);\n                acc = _mm_add_pd(acc, prod);\n            }\n            double sums[2];\n            _mm_storeu_pd(sums, acc);\n            for (int idx = 0; idx < 2; idx++) {\n                if (sums[idx] >= 0.0) {\n                    dst[i * p + j + idx] = (double)((long long)(sums[idx] + 0.5));\n                } else {\n                    dst[i * p + j + idx] = (double)((long long)(sums[idx] - 0.5));\n                }\n            }\n        }\n        if (j < p) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n            if (sum >= 0.0) {\n                dst[i * p + j] = (double)((long long)(sum + 0.5));\n            } else {\n                dst[i * p + j] = (double)((long long)(sum - 0.5));\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j <= p - 2; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m128d a = _mm_load1_pd(mat1 + i * n + k);\n                __m128d b = _mm_loadu_pd(mat2 + k * p + j);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double tmp[2];\n            _mm_storeu_pd(tmp, sum);\n            for (int idx = 0; idx < 2; idx++) {\n                if (tmp[idx] >= 0) {\n                    tmp[idx] = (double)((long long)(tmp[idx] + 0.5));\n                } else {\n                    tmp[idx] = (double)((long long)(tmp[idx] - 0.5));\n                }\n            }\n            _mm_storeu_pd(dst + i * p + j, _mm_loadu_pd(tmp));\n        }\n        if (p % 2 == 1) {\n            size_t j = p - 1;\n            double sum = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n            if (sum >= 0) {\n                dst[i * p + j] = (double)((long long)(sum + 0.5));\n            } else {\n                dst[i * p + j] = (double)((long long)(sum - 0.5));\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        size_t j = 0;\n        for (; j + 1 < p; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k++) {\n                __m128d a = _mm_set1_pd(mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128i int_val = _mm_cvtpd_epi32(sum);\n            __m128d rounded = _mm_cvtepi32_pd(int_val);\n            _mm_storeu_pd(&dst[i * p + j], rounded);\n        }\n        if (j < p) {\n            double s = 0.0;\n            for (size_t k = 0; k < n; k++) {\n                s += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = round(s);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const size_t i_upper = (rows / 4) * 4;\n    const size_t j_upper = (cols / 4) * 4;\n\n    for (size_t i = 0; i < i_upper; i += 4) {\n        for (size_t j = 0; j < j_upper; j += 4) {\n            __m128 row0 = _mm_loadu_ps(src + (i+0)*cols + j);\n            __m128 row1 = _mm_loadu_ps(src + (i+1)*cols + j);\n            __m128 row2 = _mm_loadu_ps(src + (i+2)*cols + j);\n            __m128 row3 = _mm_loadu_ps(src + (i+3)*cols + j);\n\n            __m128 scale = _mm_set1_ps(4.0f);\n            __m128 inv_scale = _mm_set1_ps(0.25f);\n\n            row0 = _mm_mul_ps(row0, scale);\n            row0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row0));\n            row0 = _mm_mul_ps(row0, inv_scale);\n\n            row1 = _mm_mul_ps(row1, scale);\n            row1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row1));\n            row1 = _mm_mul_ps(row1, inv_scale);\n\n            row2 = _mm_mul_ps(row2, scale);\n            row2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row2));\n            row2 = _mm_mul_ps(row2, inv_scale);\n\n            row3 = _mm_mul_ps(row3, scale);\n            row3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row3));\n            row3 = _mm_mul_ps(row3, inv_scale);\n\n            __m128 tmp0 = _mm_unpacklo_ps(row0, row1);\n            __m128 tmp1 = _mm_unpackhi_ps(row0, row1);\n            __m128 tmp2 = _mm_unpacklo_ps(row2, row3);\n            __m128 tmp3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 trans0 = _mm_movelh_ps(tmp0, tmp2);\n            __m128 trans1 = _mm_movehl_ps(tmp2, tmp0);\n            __m128 trans2 = _mm_movelh_ps(tmp1, tmp3);\n            __m128 trans3 = _mm_movehl_ps(tmp3, tmp1);\n\n            _mm_storeu_ps(dst + (j+0)*rows + i, trans0);\n            _mm_storeu_ps(dst + (j+1)*rows + i, trans1);\n            _mm_storeu_ps(dst + (j+2)*rows + i, trans2);\n            _mm_storeu_ps(dst + (j+3)*rows + i, trans3);\n        }\n    }\n\n    for (size_t i = 0; i < i_upper; i++) {\n        for (size_t j = j_upper; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v_scale = _mm_set_ss(val * 4.0f);\n            __m128i vi = _mm_cvtps_epi32(v_scale);\n            __m128 v_rounded = _mm_cvtepi32_ps(vi);\n            float rounded_scaled = _mm_cvtss_f32(v_rounded);\n            float rounded_val = rounded_scaled * 0.25f;\n            dst[j * rows + i] = rounded_val;\n        }\n    }\n\n    for (size_t i = i_upper; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v_scale = _mm_set_ss(val * 4.0f);\n            __m128i vi = _mm_cvtps_epi32(v_scale);\n            __m128 v_rounded = _mm_cvtepi32_ps(vi);\n            float rounded_scaled = _mm_cvtss_f32(v_rounded);\n            float rounded_val = rounded_scaled * 0.25f;\n            dst[j * rows + i] = rounded_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t main_rows = rows - (rows % 4);\n    size_t main_cols = cols - (cols % 4);\n    const __m128 factor = _mm_set1_ps(4.0f);\n    const __m128 inv_factor = _mm_set1_ps(0.25f);\n\n    for (size_t i = 0; i < main_rows; i += 4) {\n        for (size_t j = 0; j < main_cols; j += 4) {\n            __m128 r0 = _mm_loadu_ps(src + (i+0)*cols + j);\n            __m128 r1 = _mm_loadu_ps(src + (i+1)*cols + j);\n            __m128 r2 = _mm_loadu_ps(src + (i+2)*cols + j);\n            __m128 r3 = _mm_loadu_ps(src + (i+3)*cols + j);\n\n            r0 = _mm_mul_ps(r0, factor);\n            r1 = _mm_mul_ps(r1, factor);\n            r2 = _mm_mul_ps(r2, factor);\n            r3 = _mm_mul_ps(r3, factor);\n\n            __m128i ri0 = _mm_cvtps_epi32(r0);\n            __m128i ri1 = _mm_cvtps_epi32(r1);\n            __m128i ri2 = _mm_cvtps_epi32(r2);\n            __m128i ri3 = _mm_cvtps_epi32(r3);\n\n            r0 = _mm_cvtepi32_ps(ri0);\n            r1 = _mm_cvtepi32_ps(ri1);\n            r2 = _mm_cvtepi32_ps(ri2);\n            r3 = _mm_cvtepi32_ps(ri3);\n\n            r0 = _mm_mul_ps(r0, inv_factor);\n            r1 = _mm_mul_ps(r1, inv_factor);\n            r2 = _mm_mul_ps(r2, inv_factor);\n            r3 = _mm_mul_ps(r3, inv_factor);\n\n            _MM_TRANSPOSE4_PS(r0, r1, r2, r3);\n\n            _mm_storeu_ps(dst + (j+0)*rows + i, r0);\n            _mm_storeu_ps(dst + (j+1)*rows + i, r1);\n            _mm_storeu_ps(dst + (j+2)*rows + i, r2);\n            _mm_storeu_ps(dst + (j+3)*rows + i, r3);\n        }\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = main_cols; j < cols; j++) {\n            __m128 v = _mm_set_ss(src[i*cols + j] * 4.0f);\n            __m128i vi = _mm_cvtps_epi32(v);\n            __m128 rounded_v = _mm_cvtepi32_ps(vi);\n            float result = _mm_cvtss_f32(rounded_v) * 0.25f;\n            dst[j * rows + i] = result;\n        }\n    }\n\n    for (size_t i = main_rows; i < rows; i++) {\n        for (size_t j = 0; j < main_cols; j++) {\n            __m128 v = _mm_set_ss(src[i*cols + j] * 4.0f);\n            __m128i vi = _mm_cvtps_epi32(v);\n            __m128 rounded_v = _mm_cvtepi32_ps(vi);\n            float result = _mm_cvtss_f32(rounded_v) * 0.25f;\n            dst[j * rows + i] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t rows_aligned = rows & ~3;\n    size_t cols_aligned = cols & ~3;\n    const __m128 four = _mm_set1_ps(4.0f);\n    const __m128 quarter = _mm_set1_ps(0.25f);\n\n    for (size_t i = 0; i < rows_aligned; i += 4) {\n        for (size_t j = 0; j < cols_aligned; j += 4) {\n            __m128 r0 = _mm_loadu_ps(src + i * cols + j);\n            __m128 r1 = _mm_loadu_ps(src + (i + 1) * cols + j);\n            __m128 r2 = _mm_loadu_ps(src + (i + 2) * cols + j);\n            __m128 r3 = _mm_loadu_ps(src + (i + 3) * cols + j);\n\n            r0 = _mm_mul_ps(r0, four);\n            r0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r0));\n            r0 = _mm_mul_ps(r0, quarter);\n\n            r1 = _mm_mul_ps(r1, four);\n            r1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r1));\n            r1 = _mm_mul_ps(r1, quarter);\n\n            r2 = _mm_mul_ps(r2, four);\n            r2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r2));\n            r2 = _mm_mul_ps(r2, quarter);\n\n            r3 = _mm_mul_ps(r3, four);\n            r3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r3));\n            r3 = _mm_mul_ps(r3, quarter);\n\n            __m128 tmp0 = _mm_unpacklo_ps(r0, r1);\n            __m128 tmp1 = _mm_unpackhi_ps(r0, r1);\n            __m128 tmp2 = _mm_unpacklo_ps(r2, r3);\n            __m128 tmp3 = _mm_unpackhi_ps(r2, r3);\n\n            __m128 out0 = _mm_movelh_ps(tmp0, tmp2);\n            __m128 out1 = _mm_movehl_ps(tmp2, tmp0);\n            __m128 out2 = _mm_movelh_ps(tmp1, tmp3);\n            __m128 out3 = _mm_movehl_ps(tmp3, tmp1);\n\n            _mm_storeu_ps(dst + j * rows + i, out0);\n            _mm_storeu_ps(dst + (j + 1) * rows + i, out1);\n            _mm_storeu_ps(dst + (j + 2) * rows + i, out2);\n            _mm_storeu_ps(dst + (j + 3) * rows + i, out3);\n        }\n    }\n\n    for (size_t i = 0; i < rows_aligned; i += 4) {\n        for (size_t j = cols_aligned; j < cols; j++) {\n            __m128 t = _mm_setr_ps(\n                src[i * cols + j],\n                src[(i + 1) * cols + j],\n                src[(i + 2) * cols + j],\n                src[(i + 3) * cols + j]\n            );\n            t = _mm_mul_ps(t, four);\n            t = _mm_cvtepi32_ps(_mm_cvtps_epi32(t));\n            t = _mm_mul_ps(t, quarter);\n            _mm_storeu_ps(dst + j * rows + i, t);\n        }\n    }\n\n    for (size_t i = rows_aligned; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            float val = src[i * cols + j];\n            __m128 v = _mm_set_ss(val);\n            v = _mm_mul_ss(v, four);\n            v = _mm_cvtepi32_ps(_mm_cvtps_epi32(v));\n            v = _mm_mul_ss(v, quarter);\n            dst[j * rows + i] = _mm_cvtss_f32(v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 scale = _mm_set1_ps(4.0f);\n    const __m128 inv_scale = _mm_set1_ps(0.25f);\n\n    for (size_t r = 0; r < rows; r += 4) {\n        size_t r_block = (rows - r) >= 4 ? 4 : rows - r;\n        for (size_t c = 0; c < cols; c += 4) {\n            size_t c_block = (cols - c) >= 4 ? 4 : cols - c;\n\n            if (r_block == 4 && c_block == 4) {\n                __m128 row0 = _mm_loadu_ps(src + r * cols + c);\n                __m128 row1 = _mm_loadu_ps(src + (r + 1) * cols + c);\n                __m128 row2 = _mm_loadu_ps(src + (r + 2) * cols + c);\n                __m128 row3 = _mm_loadu_ps(src + (r + 3) * cols + c);\n\n                row0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(row0, scale)));\n                row1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(row1, scale)));\n                row2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(row2, scale)));\n                row3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(row3, scale)));\n\n                row0 = _mm_mul_ps(row0, inv_scale);\n                row1 = _mm_mul_ps(row1, inv_scale);\n                row2 = _mm_mul_ps(row2, inv_scale);\n                row3 = _mm_mul_ps(row3, inv_scale);\n\n                _MM_TRANSPOSE4_PS(row0, row1, row2, row3);\n\n                _mm_storeu_ps(dst + c * rows + r, row0);\n                _mm_storeu_ps(dst + (c + 1) * rows + r, row1);\n                _mm_storeu_ps(dst + (c + 2) * rows + r, row2);\n                _mm_storeu_ps(dst + (c + 3) * rows + r, row3);\n            } else {\n                for (size_t i = r; i < r + r_block; i++) {\n                    for (size_t j = c; j < c + c_block; j++) {\n                        float val = src[i * cols + j];\n                        __m128 v = _mm_set_ss(val * 4.0f);\n                        __m128i vi = _mm_cvtps_epi32(v);\n                        v = _mm_cvtepi32_ps(vi);\n                        float result = _mm_cvtss_f32(v) * 0.25f;\n                        dst[j * rows + i] = result;\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i_end = rows - (rows % 4);\n    size_t j_end = cols - (cols % 4);\n    const __m128 scale = _mm_set1_ps(4.0f);\n    const __m128 inv_scale = _mm_set1_ps(0.25f);\n\n    for (size_t i = 0; i < i_end; i += 4) {\n        for (size_t j = 0; j < j_end; j += 4) {\n            __m128 r0 = _mm_loadu_ps(src + i * cols + j);\n            __m128 r1 = _mm_loadu_ps(src + (i+1) * cols + j);\n            __m128 r2 = _mm_loadu_ps(src + (i+2) * cols + j);\n            __m128 r3 = _mm_loadu_ps(src + (i+3) * cols + j);\n\n            r0 = _mm_mul_ps(r0, scale);\n            r0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r0));\n            r0 = _mm_mul_ps(r0, inv_scale);\n\n            r1 = _mm_mul_ps(r1, scale);\n            r1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r1));\n            r1 = _mm_mul_ps(r1, inv_scale);\n\n            r2 = _mm_mul_ps(r2, scale);\n            r2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r2));\n            r2 = _mm_mul_ps(r2, inv_scale);\n\n            r3 = _mm_mul_ps(r3, scale);\n            r3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(r3));\n            r3 = _mm_mul_ps(r3, inv_scale);\n\n            _MM_TRANSPOSE4_PS(r0, r1, r2, r3);\n\n            _mm_storeu_ps(dst + j * rows + i, r0);\n            _mm_storeu_ps(dst + (j+1) * rows + i, r1);\n            _mm_storeu_ps(dst + (j+2) * rows + i, r2);\n            _mm_storeu_ps(dst + (j+3) * rows + i, r3);\n        }\n    }\n\n    for (size_t i = 0; i < i_end; i++) {\n        for (size_t j = j_end; j < cols; j++) {\n            __m128 v = _mm_load_ss(src + i * cols + j);\n            v = _mm_mul_ss(v, scale);\n            v = _mm_cvtepi32_ps(_mm_cvtps_epi32(v));\n            v = _mm_mul_ss(v, inv_scale);\n            _mm_store_ss(dst + j * rows + i, v);\n        }\n    }\n\n    for (size_t i = i_end; i < rows; i++) {\n        for (size_t j = 0; j < j_end; j++) {\n            __m128 v = _mm_load_ss(src + i * cols + j);\n            v = _mm_mul_ss(v, scale);\n            v = _mm_cvtepi32_ps(_mm_cvtps_epi32(v));\n            v = _mm_mul_ss(v, inv_scale);\n            _mm_store_ss(dst + j * rows + i, v);\n        }\n    }\n\n    for (size_t i = i_end; i < rows; i++) {\n        for (size_t j = j_end; j < cols; j++) {\n            __m128 v = _mm_load_ss(src + i * cols + j);\n            v = _mm_mul_ss(v, scale);\n            v = _mm_cvtepi32_ps(_mm_cvtps_epi32(v));\n            v = _mm_mul_ss(v, inv_scale);\n            _mm_store_ss(dst + j * rows + i, v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        __m128 a = _mm_loadu_ps(tensor1 + i);\n        __m128 b = _mm_loadu_ps(tensor2 + i);\n        __m128 sub = _mm_sub_ps(a, b);\n        __m128i sub_i = _mm_cvttps_epi32(sub);\n        __m128 result = _mm_cvtepi32_ps(sub_i);\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total; i++) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 sub = _mm_sub_ss(a, b);\n        __m128i sub_i = _mm_cvttps_epi32(sub);\n        __m128 result = _mm_cvtepi32_ps(sub_i);\n        _mm_store_ss(dst + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    for (; i + 3 < total; i += 4) {\n        __m128 a = _mm_loadu_ps(tensor1 + i);\n        __m128 b = _mm_loadu_ps(tensor2 + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128i diff_int = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(diff_int);\n        _mm_storeu_ps(dst + i, result);\n    }\n    for (; i < total; ++i) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 diff = _mm_sub_ss(a, b);\n        __m128i diff_int = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(diff_int);\n        _mm_store_ss(dst + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        __m128 a = _mm_loadu_ps(tensor1 + i);\n        __m128 b = _mm_loadu_ps(tensor2 + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128i trunc = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(trunc);\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total; i++) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 diff = _mm_sub_ss(a, b);\n        int trunc_val = _mm_cvttss_si32(diff);\n        dst[i] = (float)trunc_val;\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m128 t1 = _mm_loadu_ps(tensor1 + i);\n        __m128 t2 = _mm_loadu_ps(tensor2 + i);\n        __m128 diff = _mm_sub_ps(t1, t2);\n        __m128i truncated = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(truncated);\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total; i++) {\n        __m128 a = _mm_load_ss(tensor1 + i);\n        __m128 b = _mm_load_ss(tensor2 + i);\n        __m128 diff = _mm_sub_ss(a, b);\n        __m128i truncated = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(truncated);\n        _mm_store_ss(dst + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        __m128 a = _mm_loadu_ps(tensor1 + i);\n        __m128 b = _mm_loadu_ps(tensor2 + i);\n        __m128 c = _mm_sub_ps(a, b);\n        __m128i int_val = _mm_cvttps_epi32(c);\n        __m128 rounded = _mm_cvtepi32_ps(int_val);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < total; i++) {\n        float val = tensor1[i] - tensor2[i];\n        if (val >= 2147483648.0f) {\n            dst[i] = 2147483647.0f;\n        } else if (val < -2147483648.0f) {\n            dst[i] = -2147483648.0f;\n        } else {\n            dst[i] = (float)((int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    \n    for (; i + 1 < total; i += 2) {\n        __m128d a = _mm_loadu_pd(&mat1[i]);\n        __m128d b = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(a, b);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    if (i < total) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    for (; i + 1 < total; i += 2) {\n        __m128d a = _mm_loadu_pd(mat1 + i);\n        __m128d b = _mm_loadu_pd(mat2 + i);\n        __m128d c = _mm_mul_pd(a, b);\n        _mm_storeu_pd(dst + i, c);\n    }\n    if (i < total) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t total_simd = total & ~(size_t)1;\n\n    for (size_t j = 0; j < total_simd; j += 2) {\n        __m128d a = _mm_loadu_pd(mat1 + j);\n        __m128d b = _mm_loadu_pd(mat2 + j);\n        __m128d c = _mm_mul_pd(a, b);\n        _mm_storeu_pd(dst + j, c);\n    }\n\n    if (total_simd < total) {\n        dst[total_simd] = mat1[total_simd] * mat2[total_simd];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m256d a = _mm256_loadu_pd(mat1 + i);\n        __m256d b = _mm256_loadu_pd(mat2 + i);\n        __m256d c = _mm256_mul_pd(a, b);\n        _mm256_storeu_pd(dst + i, c);\n    }\n    \n    for (; i < total; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    if (i < total_elements) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 sum4 = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat4 = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec4 = _mm_loadu_ps(vec + j);\n            sum4 = _mm_add_ps(sum4, _mm_mul_ps(mat4, vec4));\n        }\n        float sum = 0.0f;\n        if (j > 0) {\n            __m128 hsum = _mm_movehl_ps(sum4, sum4);\n            __m128 temp_sum = _mm_add_ps(sum4, hsum);\n            __m128 shuf = _mm_shuffle_ps(temp_sum, temp_sum, 0x1);\n            sum = _mm_cvtss_f32(_mm_add_ss(temp_sum, shuf));\n        }\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 3 < n; j += 4) {\n            __m128 mat_chunk = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_chunk = _mm_loadu_ps(vec + j);\n            __m128 prod = _mm_mul_ps(mat_chunk, vec_chunk);\n            sum_vec = _mm_add_ps(sum_vec, prod);\n        }\n        __m128 shuf = _mm_movehl_ps(sum_vec, sum_vec);\n        __m128 sums = _mm_add_ps(sum_vec, shuf);\n        __m128 shuf2 = _mm_shuffle_ps(sums, sums, 1);\n        __m128 hsum = _mm_add_ss(sums, shuf2);\n        float total = _mm_cvtss_f32(hsum);\n        for (; j < n; j++) {\n            total += mat[i * n + j] * vec[j];\n        }\n        dst[i] = total;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 accum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vec = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_vec = _mm_loadu_ps(vec + j);\n            accum = _mm_add_ps(accum, _mm_mul_ps(mat_vec, vec_vec));\n        }\n        __m128 shuf = _mm_shuffle_ps(accum, accum, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128 sums = _mm_add_ps(accum, shuf);\n        __m128 shuf2 = _mm_shuffle_ps(sums, sums, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128 sum_total = _mm_add_ps(sums, shuf2);\n        float total = _mm_cvtss_f32(sum_total);\n        \n        for (; j < n; j++) {\n            total += mat[i * n + j] * vec[j];\n        }\n        \n        dst[i] = total;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 accum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vec = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_vec = _mm_loadu_ps(vec + j);\n            accum = _mm_add_ps(accum, _mm_mul_ps(mat_vec, vec_vec));\n        }\n        float temp[4];\n        _mm_storeu_ps(temp, accum);\n        float sum = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row_sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vec = _mm_loadu_ps(mat + i * n + j);\n            __m128 vec_vec = _mm_loadu_ps(vec + j);\n            row_sum = _mm_add_ps(row_sum, _mm_mul_ps(mat_vec, vec_vec));\n        }\n        __m128 shuf = _mm_shuffle_ps(row_sum, row_sum, _MM_SHUFFLE(2,3,0,1));\n        __m128 sums = _mm_add_ps(row_sum, shuf);\n        __m128 movhl = _mm_movehl_ps(sums, sums);\n        __m128 hadd = _mm_add_ss(sums, movhl);\n        float total = _mm_cvtss_f32(hadd);\n        for (; j < n; j++) {\n            total += mat[i * n + j] * vec[j];\n        }\n        dst[i] = total;\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    if (total == 0) return 0.0;\n    \n    size_t num_pairs = total / 2;\n    __m128d sum_vec = _mm_setzero_pd();\n    \n    for (size_t i = 0; i < num_pairs; i++) {\n        __m128d v = _mm_loadu_pd(mat + 2 * i);\n        sum_vec = _mm_add_pd(sum_vec, v);\n    }\n    \n    double low = _mm_cvtsd_f64(sum_vec);\n    double high = _mm_cvtsd_f64(_mm_unpackhi_pd(sum_vec, sum_vec));\n    double sum = low + high;\n    \n    if (total % 2 != 0) {\n        sum += mat[total - 1];\n    }\n    \n    return sum / total;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    if (total == 0) {\n        return 0.0;\n    }\n\n    __m128d vsum = _mm_setzero_pd();\n    size_t i = 0;\n    size_t even_end = total - (total % 2);\n\n    for (; i < even_end; i += 2) {\n        __m128d v = _mm_loadu_pd(mat + i);\n        vsum = _mm_add_pd(vsum, v);\n    }\n\n    double sum_arr[2];\n    _mm_storeu_pd(sum_arr, vsum);\n    double total_sum = sum_arr[0] + sum_arr[1];\n\n    if (total % 2 != 0) {\n        total_sum += mat[even_end];\n    }\n\n    return total_sum / total;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    if (total_elements == 0) {\n        return 0.0;\n    }\n    \n    const double* ptr = mat;\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t num_pairs = total_elements / 2;\n    \n    for (size_t i = 0; i < num_pairs; ++i) {\n        __m128d data = _mm_loadu_pd(ptr);\n        ptr += 2;\n        sum_vec = _mm_add_pd(sum_vec, data);\n    }\n    \n    __m128d shuf = _mm_shuffle_pd(sum_vec, sum_vec, 0x1);\n    __m128d sum_total_vec = _mm_add_pd(sum_vec, shuf);\n    double total_sum = _mm_cvtsd_f64(sum_total_vec);\n    \n    if (total_elements % 2 != 0) {\n        total_sum += *ptr;\n    }\n    \n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i;\n    const double* ptr = mat;\n    __m128d accum = _mm_setzero_pd();\n\n    for (i = 0; i < total_elements / 2; ++i) {\n        __m128d vec = _mm_loadu_pd(ptr);\n        accum = _mm_add_pd(accum, vec);\n        ptr += 2;\n    }\n\n    double temp[2];\n    _mm_storeu_pd(temp, accum);\n    double total_sum = temp[0] + temp[1];\n\n    if (total_elements % 2 != 0) {\n        total_sum += *ptr;\n    }\n\n    return total_sum / (double)total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t count = total / 2;\n    size_t remainder = total % 2;\n    \n    __m128d vsum = _mm_setzero_pd();\n    for (size_t i = 0; i < count; i++) {\n        __m128d v = _mm_loadu_pd(mat + 2 * i);\n        vsum = _mm_add_pd(vsum, v);\n    }\n    \n    __m128d high = _mm_unpackhi_pd(vsum, vsum);\n    __m128d result = _mm_add_sd(vsum, high);\n    double sum = _mm_cvtsd_f64(result);\n    \n    if (remainder) {\n        sum += mat[total - 1];\n    }\n    \n    return sum / total;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool asc = directions[i];\n        \n        if (cols <= 1) \n            continue;\n        \n        size_t j = 0;\n        if (cols >= 5) {\n            for (; j <= cols - 5; j += 4) {\n                __m128i left = _mm_loadu_si128((__m128i*)(row + j));\n                __m128i right = _mm_loadu_si128((__m128i*)(row + j + 1));\n                __m128i cmp;\n                if (asc) {\n                    cmp = _mm_cmpgt_epi32(left, right);\n                } else {\n                    cmp = _mm_cmpgt_epi32(right, left);\n                }\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n        \n        if (asc) {\n            for (; j < cols - 1; ++j) {\n                if (row[j] > row[j + 1]) \n                    return false;\n            }\n        } else {\n            for (; j < cols - 1; ++j) {\n                if (row[j] < row[j + 1]) \n                    return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) {\n        return true;\n    }\n    \n    for (size_t r = 0; r < rows; r++) {\n        bool dir = directions[r];\n        const int* row_ptr = matrix + r * cols;\n        const int* ptrA = row_ptr;\n        const int* ptrB = row_ptr + 1;\n        size_t num_pairs = cols - 1;\n        size_t i = 0;\n        \n        if (dir) {\n            for (; i + 4 <= num_pairs; i += 4) {\n                __m128i a = _mm_loadu_si128((const __m128i*)(ptrA + i));\n                __m128i b = _mm_loadu_si128((const __m128i*)(ptrB + i));\n                __m128i cmp = _mm_cmpgt_epi32(a, b);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return false;\n                }\n            }\n            for (; i < num_pairs; i++) {\n                if (ptrA[i] > ptrB[i]) {\n                    return false;\n                }\n            }\n        } else {\n            for (; i + 4 <= num_pairs; i += 4) {\n                __m128i a = _mm_loadu_si128((const __m128i*)(ptrA + i));\n                __m128i b = _mm_loadu_si128((const __m128i*)(ptrB + i));\n                __m128i cmp = _mm_cmplt_epi32(a, b);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return false;\n                }\n            }\n            for (; i < num_pairs; i++) {\n                if (ptrA[i] < ptrB[i]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool asc = directions[i];\n        size_t n = cols - 1;\n\n        if (n == 0) \n            continue;\n\n        size_t j = 0;\n\n        if (n >= 4) {\n            for (; j <= n - 4; j += 4) {\n                __m128i v0 = _mm_loadu_si128((__m128i const*)(row + j));\n                __m128i v1 = _mm_loadu_si128((__m128i const*)(row + j + 1));\n                __m128i cmp;\n\n                if (asc) {\n                    cmp = _mm_cmple_epi32(v0, v1);\n                } else {\n                    cmp = _mm_cmple_epi32(v1, v0);\n                }\n\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < n; ++j) {\n            if (asc) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (rows == 0 || cols <= 1) {\n        return true;\n    }\n\n    for (size_t r = 0; r < rows; r++) {\n        const int* row = matrix + r * cols;\n        bool asc = directions[r];\n        size_t n = cols - 1;\n\n        if (asc) {\n            size_t offset = 0;\n            for (; offset + 4 <= n; offset += 4) {\n                __m128i a = _mm_loadu_si128((const __m128i*)(row + offset));\n                __m128i b = _mm_loadu_si128((const __m128i*)(row + offset + 1));\n                __m128i cmp = _mm_cmpgt_epi32(a, b);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n            for (; offset < n; offset++) {\n                if (row[offset] > row[offset+1]) {\n                    return false;\n                }\n            }\n        } else {\n            size_t offset = 0;\n            for (; offset + 4 <= n; offset += 4) {\n                __m128i a = _mm_loadu_si128((const __m128i*)(row + offset));\n                __m128i b = _mm_loadu_si128((const __m128i*)(row + offset + 1));\n                __m128i cmp = _mm_cmplt_epi32(a, b);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n            for (; offset < n; offset++) {\n                if (row[offset] < row[offset+1]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        bool dir = directions[r];\n        const int* row = matrix + r * cols;\n        if (cols < 2) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j <= cols - 5; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp;\n            if (dir) {\n                cmp = _mm_cmpgt_epi32(current, next);\n            } else {\n                cmp = _mm_cmpgt_epi32(next, current);\n            }\n\n            if (_mm_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; j++) {\n            if (dir) {\n                if (row[j] > row[j+1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j+1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        __m128 base_ps = _mm_set1_ps((float)base);\n        __m128 ones = _mm_set1_ps(1.0f);\n        __m128i base_minus_one = _mm_set1_epi32(base - 1);\n        \n        for (; i <= length - 8; i += 8) {\n            __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n            \n            __m128i v_low = _mm_cvtepi16_epi32(v);\n            __m128i v_high = _mm_cvtepi16_epi32(_mm_srli_si128(v, 8));\n            \n            __m128i sign_low = _mm_srai_epi32(v_low, 31);\n            __m128i adj_low = _mm_and_si128(sign_low, base_minus_one);\n            __m128i temp_low = _mm_sub_epi32(v_low, adj_low);\n            __m128 temp_low_ps = _mm_cvtepi32_ps(temp_low);\n            __m128 quotient_low_ps = _mm_div_ps(temp_low_ps, base_ps);\n            __m128i quotient_trunc_low = _mm_cvttps_epi32(quotient_low_ps);\n            __m128 quotient_trunc_low_ps = _mm_cvtepi32_ps(quotient_trunc_low);\n            __m128 mask_neg_low = _mm_cmplt_ps(quotient_low_ps, _mm_setzero_ps());\n            __m128 mask_frac_low = _mm_cmpneq_ps(quotient_low_ps, quotient_trunc_low_ps);\n            __m128 mask_adj_low = _mm_and_ps(mask_neg_low, mask_frac_low);\n            __m128 adj_floor_low = _mm_and_ps(mask_adj_low, ones);\n            __m128 quotient_floor_low_ps = _mm_sub_ps(quotient_trunc_low_ps, adj_floor_low);\n            __m128 result_low_ps = _mm_mul_ps(quotient_floor_low_ps, base_ps);\n            __m128i result_low = _mm_cvttps_epi32(result_low_ps);\n            \n            __m128i sign_high = _mm_srai_epi32(v_high, 31);\n            __m128i adj_high = _mm_and_si128(sign_high, base_minus_one);\n            __m128i temp_high = _mm_sub_epi32(v_high, adj_high);\n            __m128 temp_high_ps = _mm_cvtepi32_ps(temp_high);\n            __m128 quotient_high_ps = _mm_div_ps(temp_high_ps, base_ps);\n            __m128i quotient_trunc_high = _mm_cvttps_epi32(quotient_high_ps);\n            __m128 quotient_trunc_high_ps = _mm_cvtepi32_ps(quotient_trunc_high);\n            __m128 mask_neg_high = _mm_cmplt_ps(quotient_high_ps, _mm_setzero_ps());\n            __m128 mask_frac_high = _mm_cmpneq_ps(quotient_high_ps, quotient_trunc_high_ps);\n            __m128 mask_adj_high = _mm_and_ps(mask_neg_high, mask_frac_high);\n            __m128 adj_floor_high = _mm_and_ps(mask_adj_high, ones);\n            __m128 quotient_floor_high_ps = _mm_sub_ps(quotient_trunc_high_ps, adj_floor_high);\n            __m128 result_high_ps = _mm_mul_ps(quotient_floor_high_ps, base_ps);\n            __m128i result_high = _mm_cvttps_epi32(result_high_ps);\n            \n            __m128i result = _mm_packs_epi32(result_low, result_high);\n            _mm_storeu_si128((__m128i*)(dst + i), result);\n        }\n    }\n    \n    for (; i < length; i++) {\n        int16_t s = src[i];\n        int16_t remainder = s % base;\n        int16_t r = s - remainder;\n        if (s < 0 && remainder != 0) {\n            r -= base;\n        }\n        dst[i] = r;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    if (length == 0) return;\n\n    __m128i base_epi16 = _mm_set1_epi16(static_cast<int16_t>(base));\n    __m128i base_minus_one = _mm_sub_epi16(base_epi16, _mm_set1_epi16(1));\n    __m128i zero_epi16 = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i mask = _mm_cmplt_epi16(src_vec, zero_epi16);\n        __m128i adjustment = _mm_and_si128(mask, base_minus_one);\n        __m128i adjusted_vec = _mm_sub_epi16(src_vec, adjustment);\n        __m128i sign_mask = _mm_cmpgt_epi16(zero_epi16, adjusted_vec);\n        __m128i low32 = _mm_unpacklo_epi16(adjusted_vec, sign_mask);\n        __m128i high32 = _mm_unpackhi_epi16(adjusted_vec, sign_mask);\n        __m128 low_ps = _mm_cvtepi32_ps(low32);\n        __m128 high_ps = _mm_cvtepi32_ps(high32);\n        __m128 base_ps = _mm_set1_ps(static_cast<float>(base));\n        low_ps = _mm_div_ps(low_ps, base_ps);\n        high_ps = _mm_div_ps(high_ps, base_ps);\n        __m128i quotient_low = _mm_cvttps_epi32(low_ps);\n        __m128i quotient_high = _mm_cvttps_epi32(high_ps);\n        int32_t low_arr[4], high_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(low_arr), quotient_low);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(high_arr), quotient_high);\n        for (int j = 0; j < 4; j++) {\n            low_arr[j] *= base;\n            high_arr[j] *= base;\n        }\n        quotient_low = _mm_loadu_si128(reinterpret_cast<const __m128i*>(low_arr));\n        quotient_high = _mm_loadu_si128(reinterpret_cast<const __m128i*>(high_arr));\n        __m128i result_vec = _mm_packs_epi32(quotient_low, quotient_high);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), result_vec);\n    }\n\n    for (; i < length; i++) {\n        int16_t x = src[i];\n        int16_t r = x % base;\n        if (r < 0) {\n            r += base;\n        }\n        dst[i] = x - r;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    float base_f = (float)base;\n    __m128 v_base_f = _mm_set1_ps(base_f);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)(src + i));\n        \n        __m128i v_src_low = _mm_srai_epi32(_mm_unpacklo_epi16(v_src, v_src), 16);\n        __m128i v_src_high = _mm_srai_epi32(_mm_unpackhi_epi16(v_src, v_src), 16);\n        \n        __m128 v_float_low = _mm_cvtepi32_ps(v_src_low);\n        __m128 v_float_high = _mm_cvtepi32_ps(v_src_high);\n        \n        v_float_low = _mm_div_ps(v_float_low, v_base_f);\n        v_float_high = _mm_div_ps(v_float_high, v_base_f);\n        \n        __m128i v_trunc_low = _mm_cvttps_epi32(v_float_low);\n        __m128i v_trunc_high = _mm_cvttps_epi32(v_float_high);\n        \n        __m128 v_trunc_f_low = _mm_cvtepi32_ps(v_trunc_low);\n        __m128 v_trunc_f_high = _mm_cvtepi32_ps(v_trunc_high);\n        \n        __m128 v_mask_low = _mm_cmplt_ps(v_float_low, v_trunc_f_low);\n        __m128 v_mask_high = _mm_cmplt_ps(v_float_high, v_trunc_f_high);\n        \n        __m128i v_floor_low = _mm_add_epi32(v_trunc_low, _mm_castps_si128(v_mask_low));\n        __m128i v_floor_high = _mm_add_epi32(v_trunc_high, _mm_castps_si128(v_mask_high));\n        \n        __m128 v_res_f_low = _mm_mul_ps(_mm_cvtepi32_ps(v_floor_low), v_base_f);\n        __m128 v_res_f_high = _mm_mul_ps(_mm_cvtepi32_ps(v_floor_high), v_base_f);\n        \n        __m128i v_res_low = _mm_cvtps_epi32(v_res_f_low);\n        __m128i v_res_high = _mm_cvtps_epi32(v_res_f_high);\n        \n        __m128i v_res = _mm_packs_epi32(v_res_low, v_res_high);\n        _mm_storeu_si128((__m128i*)(dst + i), v_res);\n    }\n\n    for (; i < length; i++) {\n        int16_t n = src[i];\n        int16_t r = n % base;\n        if (r < 0) r += base;\n        dst[i] = n - r;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    __m128i base_minus_one = _mm_set1_epi16((int16_t)(base - 1));\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i sign = _mm_cmpgt_epi16(zero, v);\n        __m128i offset = _mm_and_si128(sign, base_minus_one);\n        __m128i v_adjusted = _mm_sub_epi16(v, offset);\n        \n        __m128i quotient = _mm_setzero_si128();\n        for (int j = 0; j < 8; j++) {\n            int16_t elem = _mm_extract_epi16(v_adjusted, j);\n            int16_t q = elem / (int16_t)base;\n            quotient = _mm_insert_epi16(quotient, q, j);\n        }\n        \n        __m128i result = _mm_mullo_epi16(quotient, base_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        int16_t val = src[i];\n        if (val >= 0) {\n            dst[i] = (val / base) * base;\n        } else {\n            int16_t remainder = val % base;\n            if (remainder == 0) {\n                dst[i] = val;\n            } else {\n                dst[i] = val - (base + remainder);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    if (base == 1) {\n        for (size_t i = 0; i < length; i++) {\n            dst[i] = src[i];\n        }\n        return;\n    }\n\n    size_t i = 0;\n    if (length >= 8) {\n        __m128 base_f = _mm_set1_ps((float)base);\n        __m128i base_v = _mm_set1_epi16((int16_t)base);\n        __m128i zero = _mm_setzero_si128();\n\n        for (; i + 8 <= length; i += 8) {\n            __m128i v = _mm_loadu_si128((__m128i*)(src + i));\n            __m128i sign = _mm_cmpgt_epi16(zero, v);\n            __m128i v_low = _mm_unpacklo_epi16(v, sign);\n            __m128i v_high = _mm_unpackhi_epi16(v, sign);\n\n            __m128 v_low_f = _mm_cvtepi32_ps(v_low);\n            __m128 v_high_f = _mm_cvtepi32_ps(v_high);\n\n            __m128 q_low_f = _mm_div_ps(v_low_f, base_f);\n            __m128 q_high_f = _mm_div_ps(v_high_f, base_f);\n\n            __m128i q_low = _mm_cvttps_epi32(q_low_f);\n            __m128i q_high = _mm_cvttps_epi32(q_high_f);\n\n            __m128i q = _mm_packs_epi32(q_low, q_high);\n            __m128i product = _mm_mullo_epi16(q, base_v);\n            __m128i modulus = _mm_sub_epi16(v, product);\n            __m128i mask = _mm_cmplt_epi16(modulus, zero);\n            __m128i base_adjust = _mm_and_si128(mask, base_v);\n            __m128i res = _mm_sub_epi16(product, base_adjust);\n\n            _mm_storeu_si128((__m128i*)(dst + i), res);\n        }\n    }\n\n    for (; i < length; i++) {\n        int16_t x = src[i];\n        int16_t q = x / (int16_t)base;\n        int16_t product = q * (int16_t)base;\n        int16_t modulus = x - product;\n        if (modulus < 0) {\n            dst[i] = product - (int16_t)base;\n        } else {\n            dst[i] = product;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        // Process two elements at a time\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_chunk = _mm_loadu_pd(row_ptr + j);\n            __m128d vec_chunk = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_chunk, vec_chunk);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        \n        // Process last element if columns are odd\n        if (match && j < cols) {\n            if (row_ptr[j] != vector[j]) {\n                match = false;\n            }\n        }\n        \n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n\n        while (j + 1 < cols) {\n            __m128d row_vec = _mm_loadu_pd(row_ptr + j);\n            __m128d vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_vec, vec);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 3) {\n                match = false;\n                break;\n            }\n            j += 2;\n        }\n\n        if (match && j < cols) {\n            if (row_ptr[j] != vector[j]) {\n                match = false;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        // Process two elements at a time using SSE2\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(row_ptr + j);\n            __m128d vec_vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_vec, vec_vec);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 3) { // 0b11 = both elements equal\n                match = false;\n                break;\n            }\n        }\n        \n        // Handle last element if cols is odd\n        if (match && j < cols) {\n            if (row_ptr[j] != vector[j]) {\n                match = false;\n            }\n        }\n        \n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n        size_t even_cols = cols & ~(size_t)1;\n        \n        for (size_t j = 0; j < even_cols; j += 2) {\n            __m128d v = _mm_loadu_pd(vector + j);\n            __m128d r = _mm_loadu_pd(row_ptr + j);\n            __m128d cmp = _mm_cmpeq_pd(v, r);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 3) {\n                match = false;\n                break;\n            }\n        }\n        \n        if (match && even_cols < cols) {\n            if (vector[even_cols] != row_ptr[even_cols]) {\n                match = false;\n            }\n        }\n        \n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        size_t j = 0;\n        \n        for (; j + 1 < cols; j += 2) {\n            __m128d row_val = _mm_loadu_pd(row + j);\n            __m128d vec_val = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_val, vec_val);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        \n        if (match && j < cols) {\n            if (row[j] != vector[j]) {\n                match = false;\n            }\n        }\n        \n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t count = 0;\n    \n    __m128i ones = _mm_set1_epi8(1);\n    __m128i zero = _mm_setzero_si128();\n    \n    for (; i + 15 < total_elements; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a_vec, b_vec);\n        __m128i mask_bytes = _mm_and_si128(cmp, ones);\n        __m128i sum_vec = _mm_sad_epu8(mask_bytes, zero);\n        \n        uint64_t sum_arr[2];\n        _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n        count += sum_arr[0] + sum_arr[1];\n    }\n    \n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n    const size_t stride = 16;\n\n    for (; i + stride <= total_elements; i += stride) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a_vec, b_vec);\n        int mask = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t count = 0;\n\n    for (; i + 16 <= total; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a_vec, b_vec);\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i and_result = _mm_and_si128(cmp, ones);\n        __m128i sum = _mm_sad_epu8(and_result, _mm_setzero_si128());\n        uint64_t sum_arr[2];\n        _mm_storeu_si128((__m128i*)sum_arr, sum);\n        count += sum_arr[0] + sum_arr[1];\n    }\n\n    for (; i < total; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 16) {\n        for (; i <= total_elements - 16; i += 16) {\n            __m128i a_vec = _mm_loadu_si128((__m128i const*)(A + i));\n            __m128i b_vec = _mm_loadu_si128((__m128i const*)(B + i));\n            __m128i cmp = _mm_cmpeq_epi8(a_vec, b_vec);\n            __m128i ones = _mm_set1_epi8(1);\n            __m128i equal_bytes = _mm_and_si128(cmp, ones);\n            __m128i sum_vector = _mm_sad_epu8(equal_bytes, _mm_setzero_si128());\n            \n            uint64_t sums[2];\n            _mm_storeu_si128((__m128i*)sums, sum_vector);\n            count += sums[0] + sums[1];\n        }\n    }\n\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total == 0) {\n        return 0;\n    }\n\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 16 <= total; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(a_vec, b_vec);\n        __m128i anded = _mm_and_si128(cmp, ones);\n        __m128i sums = _mm_sad_epu8(anded, zero);\n        uint64_t low = _mm_cvtsi128_si64(sums);\n        uint64_t high = _mm_cvtsi128_si64(_mm_srli_si128(sums, 8));\n        count += low + high;\n    }\n\n    for (; i < total; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    if (length >= 2) {\n        for (; i <= length - 2; i += 2) {\n            int64_t prod0 = a * x[i];\n            int64_t prod1 = a * x[i+1];\n            __m128i prod_vec = _mm_set_epi64x(prod1, prod0);\n            __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n            __m128i zero = _mm_setzero_si128();\n            __m128i sign_bits = _mm_srli_epi64(y_vec, 63);\n            __m128i mask = _mm_sub_epi64(zero, sign_bits);\n            __m128i abs_vec = _mm_sub_epi64(_mm_xor_si128(y_vec, mask), mask);\n            __m128i result = _mm_sub_epi64(prod_vec, abs_vec);\n            _mm_storeu_si128((__m128i*)(y + i), result);\n        }\n    }\n    if (i < length) {\n        int64_t prod = a * x[i];\n        int64_t abs_val = y[i];\n        if (abs_val < 0) abs_val = -abs_val;\n        y[i] = prod - abs_val;\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i const*)(x + i));\n        int64_t x0 = _mm_cvtsi128_si64(x_vec);\n        int64_t x1 = _mm_cvtsi128_si64(_mm_srli_si128(x_vec, 8));\n        \n        int64_t p0 = a * x0;\n        int64_t p1 = a * x1;\n        \n        __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n        int64_t y0 = _mm_cvtsi128_si64(y_vec);\n        int64_t y1 = _mm_cvtsi128_si64(_mm_srli_si128(y_vec, 8));\n        \n        y0 = (y0 < 0) ? -y0 : y0;\n        y1 = (y1 < 0) ? -y1 : y1;\n        \n        int64_t r0 = p0 - y0;\n        int64_t r1 = p1 - y1;\n        \n        __m128i r_vec = _mm_set_epi64x(r1, r0);\n        _mm_storeu_si128((__m128i*)(y + i), r_vec);\n    }\n    \n    if (i < length) {\n        int64_t x0 = x[i];\n        int64_t y0 = y[i];\n        int64_t p0 = a * x0;\n        y0 = (y0 < 0) ? -y0 : y0;\n        y[i] = p0 - y0;\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i const*)(x + i));\n        __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n        \n        int64_t x0 = _mm_cvtsi128_si64(x_vec);\n        int64_t x1 = _mm_cvtsi128_si64(_mm_unpackhi_epi64(x_vec, x_vec));\n        \n        int64_t y0 = _mm_cvtsi128_si64(y_vec);\n        int64_t y1 = _mm_cvtsi128_si64(_mm_unpackhi_epi64(y_vec, y_vec));\n        \n        int64_t p0 = a * x0;\n        int64_t p1 = a * x1;\n        \n        y0 = (y0 < 0) ? -y0 : y0;\n        y1 = (y1 < 0) ? -y1 : y1;\n        \n        __m128i abs_y = _mm_set_epi64x(y1, y0);\n        __m128i prod = _mm_set_epi64x(p1, p0);\n        __m128i result = _mm_sub_epi64(abs_y, prod);\n        \n        _mm_storeu_si128((__m128i*)(y + i), result);\n    }\n    \n    if (i < length) {\n        int64_t temp = y[i];\n        temp = (temp < 0) ? -temp : temp;\n        y[i] = temp - (a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i x_vec = _mm_loadu_si128((__m128i const*)(x + i));\n        __m128i y_vec = _mm_loadu_si128((__m128i const*)(y + i));\n        \n        int64_t x0 = _mm_cvtsi128_si64(x_vec);\n        int64_t x1 = _mm_cvtsi128_si64(_mm_unpackhi_epi64(x_vec, x_vec));\n        int64_t y0 = _mm_cvtsi128_si64(y_vec);\n        int64_t y1 = _mm_cvtsi128_si64(_mm_unpackhi_epi64(y_vec, y_vec));\n        \n        int64_t p0 = a * x0;\n        int64_t p1 = a * x1;\n        y0 = (y0 < 0) ? -y0 : y0;\n        y1 = (y1 < 0) ? -y1 : y1;\n        int64_t r0 = p0 - y0;\n        int64_t r1 = p1 - y1;\n        \n        __m128i result = _mm_set_epi64x(r1, r0);\n        _mm_storeu_si128((__m128i*)(y + i), result);\n    }\n    for (; i < length; i++) {\n        int64_t temp = a * x[i];\n        int64_t abs_y = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = temp - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    if (length == 0) {\n        return;\n    }\n\n    if (length >= 2) {\n        __m128i a_vec = _mm_set1_epi64x(a);\n        __m128i mask_low32 = _mm_set1_epi64x(0xFFFFFFFF);\n        __m128i a_lo = _mm_and_si128(a_vec, mask_low32);\n        __m128i a_hi = _mm_srli_epi64(a_vec, 32);\n        __m128i zero = _mm_setzero_si128();\n\n        for (; i <= length - 2; i += 2) {\n            __m128i x_vec = _mm_loadu_si128((__m128i*)(x + i));\n            __m128i prod_low = _mm_mul_epu32(a_vec, x_vec);\n            __m128i x_hi = _mm_srli_epi64(x_vec, 32);\n            __m128i x_lo = _mm_and_si128(x_vec, mask_low32);\n            __m128i cross1 = _mm_mul_epi32(a_lo, x_hi);\n            __m128i cross2 = _mm_mul_epi32(a_hi, x_lo);\n            cross1 = _mm_slli_epi64(cross1, 32);\n            cross2 = _mm_slli_epi64(cross2, 32);\n            __m128i total = _mm_add_epi64(prod_low, cross1);\n            total = _mm_add_epi64(total, cross2);\n\n            __m128i y_vec = _mm_loadu_si128((__m128i*)(y + i));\n            __m128i sign_y = _mm_srli_epi64(y_vec, 63);\n            sign_y = _mm_sub_epi64(zero, sign_y);\n            __m128i abs_y = _mm_xor_si128(y_vec, sign_y);\n            abs_y = _mm_sub_epi64(abs_y, sign_y);\n            __m128i result = _mm_sub_epi64(abs_y, total);\n            _mm_storeu_si128((__m128i*)(y + i), result);\n        }\n    }\n\n    for (; i < length; i++) {\n        int64_t temp = a * x[i];\n        int64_t abs_val = y[i] < 0 ? -y[i] : y[i];\n        y[i] = abs_val - temp;\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    size_t i = 0;\n    __m128i min_vec, max_vec;\n\n    if (length >= 8) {\n        min_vec = _mm_loadu_si128((const __m128i*)vec);\n        max_vec = min_vec;\n        i = 8;\n    } else {\n        int16_t buffer[8];\n        for (size_t j = 0; j < 8; j++) {\n            if (j < length) {\n                buffer[j] = vec[j];\n            } else {\n                buffer[j] = vec[length-1];\n            }\n        }\n        min_vec = _mm_loadu_si128((const __m128i*)buffer);\n        max_vec = min_vec;\n        i = length;\n    }\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, chunk);\n        max_vec = _mm_max_epi16(max_vec, chunk);\n    }\n\n    if (i < length) {\n        int16_t buffer[8];\n        size_t remaining = length - i;\n        for (size_t j = 0; j < 8; j++) {\n            if (j < remaining) {\n                buffer[j] = vec[i+j];\n            } else {\n                buffer[j] = vec[i+remaining-1];\n            }\n        }\n        __m128i chunk = _mm_loadu_si128((const __m128i*)buffer);\n        min_vec = _mm_min_epi16(min_vec, chunk);\n        max_vec = _mm_max_epi16(max_vec, chunk);\n    }\n\n    __m128i perm_min1 = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(1,0,3,2));\n    min_vec = _mm_min_epi16(min_vec, perm_min1);\n    __m128i perm_min2 = _mm_shufflelo_epi16(min_vec, _MM_SHUFFLE(1,0,3,2));\n    min_vec = _mm_min_epi16(min_vec, perm_min2);\n    __m128i perm_min3 = _mm_shufflelo_epi16(min_vec, _MM_SHUFFLE(0,0,0,1));\n    min_vec = _mm_min_epi16(min_vec, perm_min3);\n\n    __m128i perm_max1 = _mm_shuffle_epi32(max_vec, _MM_SHUFFLE(1,0,3,2));\n    max_vec = _mm_max_epi16(max_vec, perm_max1);\n    __m128i perm_max2 = _mm_shufflelo_epi16(max_vec, _MM_SHUFFLE(1,0,3,2));\n    max_vec = _mm_max_epi16(max_vec, perm_max2);\n    __m128i perm_max3 = _mm_shufflelo_epi16(max_vec, _MM_SHUFFLE(0,0,0,1));\n    max_vec = _mm_max_epi16(max_vec, perm_max3);\n\n    MinMaxPair result;\n    result.min_num = _mm_extract_epi16(min_vec, 0);\n    result.max_num = _mm_extract_epi16(max_vec, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    int16_t min_val, max_val;\n    if (length < 8) {\n        min_val = vec[0];\n        max_val = vec[0];\n        for (size_t i = 1; i < length; ++i) {\n            if (vec[i] < min_val) min_val = vec[i];\n            else if (vec[i] > max_val) max_val = vec[i];\n        }\n        return (MinMaxPair){min_val, max_val};\n    }\n\n    __m128i min_reg = _mm_loadu_si128((const __m128i*)vec);\n    __m128i max_reg = min_reg;\n    size_t i = 8;\n\n    while (i <= length - 8) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(vec + i));\n        min_reg = _mm_min_epi16(min_reg, chunk);\n        max_reg = _mm_max_epi16(max_reg, chunk);\n        i += 8;\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)min_arr, min_reg);\n    _mm_store_si128((__m128i*)max_arr, max_reg);\n\n    min_val = min_arr[0];\n    max_val = max_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        else if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    return (MinMaxPair){min_val, max_val};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_reg = _mm_set1_epi16(vec[0]);\n    __m128i max_reg = min_reg;\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(vec + i));\n        min_reg = _mm_min_epi16(min_reg, chunk);\n        max_reg = _mm_max_epi16(max_reg, chunk);\n    }\n\n    if (i < length) {\n        int16_t temp[8];\n        for (size_t j = 0; j < 8; j++) {\n            temp[j] = (i + j < length) ? vec[i + j] : vec[0];\n        }\n        __m128i chunk = _mm_loadu_si128((const __m128i*)temp);\n        min_reg = _mm_min_epi16(min_reg, chunk);\n        max_reg = _mm_max_epi16(max_reg, chunk);\n    }\n\n    int16_t min_arr[8];\n    int16_t max_arr[8];\n    _mm_storeu_si128((__m128i*)min_arr, min_reg);\n    _mm_storeu_si128((__m128i*)max_arr, max_reg);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 8; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length < 8) {\n        int16_t min_val = vec[0];\n        int16_t max_val = vec[0];\n        for (size_t i = 1; i < length; i++) {\n            if (vec[i] < min_val) min_val = vec[i];\n            if (vec[i] > max_val) max_val = vec[i];\n        }\n        return (MinMaxPair){min_val, max_val};\n    }\n\n    __m128i min_vec = _mm_loadu_si128((const __m128i*)vec);\n    __m128i max_vec = min_vec;\n    size_t i = 8;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i data = _mm_loadu_si128((const __m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, data);\n        max_vec = _mm_max_epi16(max_vec, data);\n    }\n\n    int16_t min_arr[8];\n    int16_t max_arr[8];\n    _mm_storeu_si128((__m128i*)min_arr, min_vec);\n    _mm_storeu_si128((__m128i*)max_arr, max_vec);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n    for (int j = 1; j < 8; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < length; i++) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    return (MinMaxPair){min_val, max_val};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length < 8) {\n        int16_t min_val = vec[0];\n        int16_t max_val = vec[0];\n        for (size_t j = 1; j < length; j++) {\n            if (vec[j] < min_val) min_val = vec[j];\n            if (vec[j] > max_val) max_val = vec[j];\n        }\n        return (MinMaxPair){min_val, max_val};\n    } else {\n        __m128i min_vals = _mm_loadu_si128((const __m128i*)vec);\n        __m128i max_vals = min_vals;\n        size_t i = 8;\n        for (; i <= length - 8; i += 8) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(vec + i));\n            min_vals = _mm_min_epi16(min_vals, chunk);\n            max_vals = _mm_max_epi16(max_vals, chunk);\n        }\n\n        int16_t min_arr[8] __attribute__((aligned(16)));\n        int16_t max_arr[8] __attribute__((aligned(16)));\n        _mm_store_si128((__m128i*)min_arr, min_vals);\n        _mm_store_si128((__m128i*)max_arr, max_vals);\n\n        int16_t final_min = min_arr[0];\n        int16_t final_max = max_arr[0];\n        for (int j = 1; j < 8; j++) {\n            if (min_arr[j] < final_min) final_min = min_arr[j];\n            if (max_arr[j] > final_max) final_max = max_arr[j];\n        }\n\n        for (size_t j = i; j < length; j++) {\n            if (vec[j] < final_min) final_min = vec[j];\n            if (vec[j] > final_max) final_max = vec[j];\n        }\n\n        return (MinMaxPair){final_min, final_max};\n    }\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    \n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        if (cols == 1) {\n            continue;\n        }\n        \n        size_t j = 0;\n        while (j <= cols - 5) {\n            __m128i current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j));\n            __m128i next = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n            j += 4;\n        }\n        \n        for (; j < cols - 1; ++j) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        while (j <= cols - 5) {\n            __m128i v1 = _mm_loadu_si128((__m128i const*)(row + j));\n            __m128i v2 = _mm_loadu_si128((__m128i const*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(v1, v2);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n            j += 4;\n        }\n\n        while (j < cols - 1) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n            j++;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    if (cols == 1) {\n        return true;\n    }\n    \n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        size_t n = cols - 1;\n        size_t j = 0;\n        \n        while (j + 4 <= n) {\n            __m128i current = _mm_loadu_si128((const __m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((const __m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n            j += 4;\n        }\n        \n        while (j < n) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n            j++;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n\n        if (cols == 1) {\n            continue;\n        }\n\n        size_t j = 0;\n        for (; j + 4 < cols; j += 4) {\n            __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row + j));\n            __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(v0, v1);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; j++) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n    \n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        \n        if (cols == 1) {\n            continue;\n        }\n        \n        size_t j = 0;\n        while (j <= cols - 5) {\n            __m128i a = _mm_loadu_si128((const __m128i*)(row_ptr + j));\n            __m128i b = _mm_loadu_si128((const __m128i*)(row_ptr + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(a, b);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                return false;\n            }\n            j += 4;\n        }\n        \n        for (; j < cols - 1; ++j) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0) continue;\n\n            for (size_t r = 0; r < block_size; r++) {\n                const int* ref_row = matrix + r * cols;\n                const int* cur_row = matrix + (i + r) * cols + j;\n                size_t k = 0;\n\n                for (; k + 3 < block_size; k += 4) {\n                    __m128i ref_vec = _mm_loadu_si128((const __m128i*)(ref_row + k));\n                    __m128i cur_vec = _mm_loadu_si128((const __m128i*)(cur_row + k));\n                    __m128i cmp = _mm_cmpeq_epi32(ref_vec, cur_vec);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; k < block_size; k++) {\n                    if (ref_row[k] != cur_row[k]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    \n    size_t row_blocks = rows / block_size;\n    size_t col_blocks = cols / block_size;\n    \n    for (size_t by = 0; by < row_blocks; by++) {\n        for (size_t bx = 0; bx < col_blocks; bx++) {\n            if (by == 0 && bx == 0) {\n                continue;\n            }\n            \n            size_t i = by * block_size;\n            size_t j = bx * block_size;\n            \n            for (size_t r = 0; r < block_size; r++) {\n                size_t c = 0;\n                for (; c + 3 < block_size; c += 4) {\n                    const int* ref_ptr = matrix + r * cols + c;\n                    const int* cur_ptr = matrix + (i + r) * cols + j + c;\n                    \n                    __m128i ref_vec = _mm_loadu_si128((__m128i const*)ref_ptr);\n                    __m128i cur_vec = _mm_loadu_si128((__m128i const*)cur_ptr);\n                    __m128i cmp_vec = _mm_cmpeq_epi32(ref_vec, cur_vec);\n                    int mask = _mm_movemask_epi8(cmp_vec);\n                    \n                    if (mask != 0xFFFF) {\n                        return false;\n                    }\n                }\n                \n                for (; c < block_size; c++) {\n                    if (matrix[r * cols + c] != matrix[(i + r) * cols + j + c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            if (i == 0 && j == 0) continue;\n\n            for (size_t r = 0; r < block_size; ++r) {\n                const int* ref_row = matrix + r * cols;\n                const int* cur_row = matrix + (i + r) * cols + j;\n\n                size_t c = 0;\n                for (; c + 3 < block_size; c += 4) {\n                    __m128i ref_vec = _mm_loadu_si128((const __m128i*)(ref_row + c));\n                    __m128i cur_vec = _mm_loadu_si128((const __m128i*)(cur_row + c));\n                    __m128i cmp = _mm_cmpeq_epi32(ref_vec, cur_vec);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; c < block_size; ++c) {\n                    if (ref_row[c] != cur_row[c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_rows = rows / block_size;\n    size_t block_cols = cols / block_size;\n\n    for (size_t i_block = 0; i_block < block_rows; i_block++) {\n        for (size_t j_block = 0; j_block < block_cols; j_block++) {\n            if (i_block == 0 && j_block == 0) {\n                continue;\n            }\n\n            const int* block_start = matrix + (i_block * block_size) * cols + (j_block * block_size);\n\n            for (size_t r = 0; r < block_size; r++) {\n                const int* cur_row = block_start + r * cols;\n                const int* ref_row = matrix + r * cols;\n\n                size_t c = 0;\n                for (; c + 3 < block_size; c += 4) {\n                    __m128i current = _mm_loadu_si128((const __m128i*)(cur_row + c));\n                    __m128i ref = _mm_loadu_si128((const __m128i*)(ref_row + c));\n                    __m128i cmp = _mm_cmpeq_epi32(current, ref);\n                    unsigned int mask = _mm_movemask_epi8(cmp);\n                    if (mask != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; c < block_size; c++) {\n                    if (cur_row[c] != ref_row[c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t total_blocks_vertical = rows / block_size;\n    size_t total_blocks_horizontal = cols / block_size;\n\n    for (size_t i_block = 0; i_block < total_blocks_vertical; i_block++) {\n        for (size_t j_block = 0; j_block < total_blocks_horizontal; j_block++) {\n            if (i_block == 0 && j_block == 0) continue;\n\n            size_t start_row = i_block * block_size;\n            size_t start_col = j_block * block_size;\n\n            for (size_t r = 0; r < block_size; r++) {\n                const int* ref_row = matrix + r * cols;\n                const int* cur_row = matrix + (start_row + r) * cols + start_col;\n                size_t c = 0;\n\n                for (; c + 3 < block_size; c += 4) {\n                    __m128i v_ref = _mm_loadu_si128((const __m128i*)(ref_row + c));\n                    __m128i v_cur = _mm_loadu_si128((const __m128i*)(cur_row + c));\n                    __m128i v_cmp = _mm_cmpeq_epi32(v_ref, v_cur);\n                    if (_mm_movemask_epi8(v_cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                for (; c < block_size; c++) {\n                    if (ref_row[c] != cur_row[c]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || length < block_size || tolerance < 0.0 || tolerance != tolerance) {\n        return false;\n    }\n\n    size_t n_blocks = length / block_size;\n    if (n_blocks == 0) {\n        return false;\n    }\n\n    if (n_blocks == 1) {\n        return true;\n    }\n\n    size_t pairs = block_size / 2;\n    size_t remainder = block_size % 2;\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n    __m128d sign_mask = _mm_set1_pd(-0.0);\n\n    for (size_t block_idx = 1; block_idx < n_blocks; block_idx++) {\n        for (size_t j = 0; j < pairs; j++) {\n            size_t index = j * 2;\n            __m128d first = _mm_loadu_pd(vec + index);\n            __m128d current = _mm_loadu_pd(vec + block_idx * block_size + index);\n            __m128d diff = _mm_sub_pd(first, current);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n\n            __m128d isnan = _mm_cmpunord_pd(abs_diff, abs_diff);\n            if (_mm_movemask_pd(isnan)) {\n                return false;\n            }\n\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n\n        if (remainder) {\n            size_t index = pairs * 2;\n            double a = vec[index];\n            double b = vec[block_idx * block_size + index];\n            double d = a - b;\n            if (d != d || fabs(d) > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n    \n    size_t n_blocks = length / block_size;\n    if (n_blocks < 2) {\n        return true;\n    }\n    \n    const __m128d tol_vec = _mm_set1_pd(tolerance);\n    const __m128d sign_mask = _mm_set1_pd(-0.0);\n    \n    for (size_t block_idx = 1; block_idx < n_blocks; block_idx++) {\n        size_t i = 0;\n        for (; i + 1 < block_size; i += 2) {\n            __m128d a = _mm_loadu_pd(vec + i);\n            __m128d b = _mm_loadu_pd(vec + block_idx * block_size + i);\n            __m128d diff = _mm_sub_pd(a, b);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n        \n        if (i < block_size) {\n            double d1 = vec[i];\n            double d2 = vec[block_idx * block_size + i];\n            double diff = d1 - d2;\n            if (diff < 0) {\n                diff = -diff;\n            }\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        size_t j = 0;\n        __m128d tol_vec = _mm_set1_pd(tolerance);\n        while (j + 1 < block_size) {\n            __m128d first_block = _mm_loadu_pd(vec + j);\n            __m128d current_block = _mm_loadu_pd(vec + block_idx * block_size + j);\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n            if (_mm_movemask_pd(cmp) != 0x3) {\n                return false;\n            }\n            j += 2;\n        }\n        if (j < block_size) {\n            double a = vec[j];\n            double b = vec[block_idx * block_size + j];\n            double d = a - b;\n            if (d < 0) d = -d;\n            if (d > tolerance) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t n_blocks = length / block_size;\n    if (n_blocks < 2) {\n        return true;\n    }\n\n    const size_t simd_elements = 2;\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t block_idx = 1; block_idx < n_blocks; ++block_idx) {\n        const double* block_start = vec + block_idx * block_size;\n        size_t j = 0;\n\n        for (; j + simd_elements <= block_size; j += simd_elements) {\n            __m128d first_block = _mm_loadu_pd(vec + j);\n            __m128d current_block = _mm_loadu_pd(block_start + j);\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j < block_size; ++j) {\n            double diff = vec[j] - block_start[j];\n            if (diff < 0) diff = -diff;\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < 2 * block_size) {\n        return false;\n    }\n    \n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n    \n    __m128d tol_vec = _mm_set1_pd(tolerance);\n    __m128d sign_mask = _mm_set1_pd(-0.0);\n    \n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t i = 0;\n        for (; i <= block_size - 2; i += 2) {\n            __m128d v0 = _mm_loadu_pd(vec + i);\n            __m128d vb = _mm_loadu_pd(vec + b * block_size + i);\n            __m128d diff = _mm_sub_pd(v0, vb);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp) {\n                return false;\n            }\n        }\n        \n        if (i < block_size) {\n            double a = vec[i];\n            double b_val = vec[b * block_size + i];\n            double diff = a - b_val;\n            if (diff < 0) diff = -diff;\n            if (diff > tolerance) {\n                return false;\n            }\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        double th = thresholds[i];\n        if (th < 0.0) {\n            result[i] = true;\n            continue;\n        }\n        \n        __m128d sum_vec = _mm_setzero_pd();\n        size_t j = 0;\n        size_t limit = cols - (cols % 2);\n        for (; j < limit; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            row_vec = _mm_mul_pd(row_vec, row_vec);\n            sum_vec = _mm_add_pd(sum_vec, row_vec);\n        }\n        \n        __m128d high = _mm_unpackhi_pd(sum_vec, sum_vec);\n        double squared_norm = _mm_cvtsd_f64(_mm_add_sd(sum_vec, high));\n        \n        if (cols % 2) {\n            squared_norm += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n        \n        double th_sq = th * th;\n        result[i] = (squared_norm > th_sq);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        double thresh = thresholds[i];\n        if (thresh < 0) {\n            result[i] = true;\n        } else {\n            __m128d sum_vec = _mm_setzero_pd();\n            size_t j = 0;\n            for (; j + 1 < cols; j += 2) {\n                __m128d v = _mm_loadu_pd(&matrix[i * cols + j]);\n                __m128d sq = _mm_mul_pd(v, v);\n                sum_vec = _mm_add_pd(sum_vec, sq);\n            }\n            double tail = 0.0;\n            if (j < cols) {\n                tail = matrix[i * cols + j] * matrix[i * cols + j];\n            }\n            __m128d high = _mm_unpackhi_pd(sum_vec, sum_vec);\n            __m128d sum_scalar = _mm_add_sd(sum_vec, high);\n            double sum_val = _mm_cvtsd_f64(sum_scalar);\n            double total_sq = sum_val + tail;\n            result[i] = (total_sq > thresh * thresh);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        __m128d sum_sq = _mm_setzero_pd();\n        size_t j = 0;\n        while (j + 1 < cols) {\n            __m128d row_vec = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d sq = _mm_mul_pd(row_vec, row_vec);\n            sum_sq = _mm_add_pd(sum_sq, sq);\n            j += 2;\n        }\n        if (j < cols) {\n            double last_val = matrix[i * cols + j];\n            __m128d last_sq = _mm_set_sd(last_val * last_val);\n            sum_sq = _mm_add_sd(sum_sq, last_sq);\n        }\n        __m128d temp = _mm_shuffle_pd(sum_sq, sum_sq, 1);\n        __m128d total_vec = _mm_add_pd(sum_sq, temp);\n        double total_sum = _mm_cvtsd_f64(total_vec);\n        double norm = sqrt(total_sum);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        __m128d sum_sq = _mm_setzero_pd();\n        size_t j = 0;\n\n        // Process two elements at a time\n        for (; j + 1 < cols; j += 2) {\n            __m128d v = _mm_loadu_pd(row_ptr + j);\n            __m128d sq = _mm_mul_pd(v, v);\n            sum_sq = _mm_add_pd(sum_sq, sq);\n        }\n\n        // Handle last element if cols is odd\n        if (j < cols) {\n            __m128d last = _mm_load_sd(row_ptr + j);\n            __m128d sq_last = _mm_mul_pd(last, last);\n            sum_sq = _mm_add_pd(sum_sq, sq_last);\n        }\n\n        // Horizontal sum: [a, b] becomes [a+b, a+b]\n        __m128d shuf = _mm_shuffle_pd(sum_sq, sum_sq, 0x1);\n        __m128d total = _mm_add_pd(sum_sq, shuf);\n        double norm_sq = _mm_cvtsd_f64(total);\n\n        // Compare squared norm to squared threshold\n        double thresh_sq = thresholds[i] * thresholds[i];\n        result[i] = (norm_sq > thresh_sq);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        __m128d sum_vec = _mm_setzero_pd();\n        size_t j = 0;\n        const size_t limit = cols - (cols % 2);\n\n        for (; j < limit; j += 2) {\n            __m128d x = _mm_loadu_pd(row_ptr + j);\n            __m128d x_sq = _mm_mul_pd(x, x);\n            sum_vec = _mm_add_pd(sum_vec, x_sq);\n        }\n\n        __m128d high = _mm_unpackhi_pd(sum_vec, sum_vec);\n        __m128d low_sum = _mm_add_sd(sum_vec, high);\n        double sum = _mm_cvtsd_f64(low_sum);\n\n        if (cols % 2 != 0) {\n            double last_val = row_ptr[cols - 1];\n            sum += last_val * last_val;\n        }\n\n        double thr = thresholds[i];\n        double thr_sq = thr * thr;\n        result[i] = (sum > thr_sq);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    const size_t simd_iters = length / simd_elements;\n\n    for (; i < simd_iters * simd_elements; i += simd_elements) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c_vec = _mm_or_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c_vec = _mm_or_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    const size_t simd_limit = length - (length % simd_elements);\n\n    for (; i < simd_limit; i += simd_elements) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c_vec = _mm_or_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t vectorizable_elements = length - (length % 4);\n    \n    for (; i < vectorizable_elements; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c_vec = _mm_or_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    \n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    const size_t simd_limit = length - (length % simd_elements);\n\n    for (; i < simd_limit; i += simd_elements) {\n        __m128i a_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c_vec = _mm_or_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    size_t i = 0;\n    __m128i sq_acc_low = _mm_setzero_si128();\n    __m128i sq_acc_high = _mm_setzero_si128();\n    __m128i prod_acc_low = _mm_setzero_si128();\n    __m128i prod_acc_high = _mm_setzero_si128();\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(B + i));\n\n        __m128i a_lo = _mm_cvtepi8_epi16(a);\n        __m128i a_hi = _mm_cvtepi8_epi16(_mm_srli_si128(a, 8));\n        __m128i b_lo = _mm_cvtepi8_epi16(b);\n        __m128i b_hi = _mm_cvtepi8_epi16(_mm_srli_si128(b, 8));\n\n        __m128i sq_lo = _mm_madd_epi16(a_lo, a_lo);\n        __m128i sq_hi = _mm_madd_epi16(a_hi, a_hi);\n        __m128i sign_sq_lo = _mm_srai_epi32(sq_lo, 31);\n        __m128i sign_sq_hi = _mm_srai_epi32(sq_hi, 31);\n        __m128i sq_lo_low = _mm_unpacklo_epi32(sq_lo, sign_sq_lo);\n        __m128i sq_lo_high = _mm_unpackhi_epi32(sq_lo, sign_sq_lo);\n        __m128i sq_hi_low = _mm_unpacklo_epi32(sq_hi, sign_sq_hi);\n        __m128i sq_hi_high = _mm_unpackhi_epi32(sq_hi, sign_sq_hi);\n        sq_acc_low = _mm_add_epi64(sq_acc_low, sq_lo_low);\n        sq_acc_high = _mm_add_epi64(sq_acc_high, sq_lo_high);\n        sq_acc_low = _mm_add_epi64(sq_acc_low, sq_hi_low);\n        sq_acc_high = _mm_add_epi64(sq_acc_high, sq_hi_high);\n\n        __m128i prod_lo = _mm_madd_epi16(a_lo, b_lo);\n        __m128i prod_hi = _mm_madd_epi16(a_hi, b_hi);\n        __m128i sign_prod_lo = _mm_srai_epi32(prod_lo, 31);\n        __m128i sign_prod_hi = _mm_srai_epi32(prod_hi, 31);\n        __m128i prod_lo_low = _mm_unpacklo_epi32(prod_lo, sign_prod_lo);\n        __m128i prod_lo_high = _mm_unpackhi_epi32(prod_lo, sign_prod_lo);\n        __m128i prod_hi_low = _mm_unpacklo_epi32(prod_hi, sign_prod_hi);\n        __m128i prod_hi_high = _mm_unpackhi_epi32(prod_hi, sign_prod_hi);\n        prod_acc_low = _mm_add_epi64(prod_acc_low, prod_lo_low);\n        prod_acc_high = _mm_add_epi64(prod_acc_high, prod_lo_high);\n        prod_acc_low = _mm_add_epi64(prod_acc_low, prod_hi_low);\n        prod_acc_high = _mm_add_epi64(prod_acc_high, prod_hi_high);\n    }\n\n    int64_t sq_low_arr[2], sq_high_arr[2];\n    _mm_storeu_si128((__m128i*)sq_low_arr, sq_acc_low);\n    _mm_storeu_si128((__m128i*)sq_high_arr, sq_acc_high);\n    int64_t total_sq = sq_low_arr[0] + sq_low_arr[1] + sq_high_arr[0] + sq_high_arr[1];\n\n    int64_t prod_low_arr[2], prod_high_arr[2];\n    _mm_storeu_si128((__m128i*)prod_low_arr, prod_acc_low);\n    _mm_storeu_si128((__m128i*)prod_high_arr, prod_acc_high);\n    int64_t total_prod = prod_low_arr[0] + prod_low_arr[1] + prod_high_arr[0] + prod_high_arr[1];\n\n    for (; i < length; i++) {\n        total_sq += (int64_t)A[i] * A[i];\n        total_prod += (int64_t)A[i] * B[i];\n    }\n\n    return total_sq - total_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t total_A2 = 0;\n    int64_t total_AB = 0;\n    __m128i v_acc_A2 = _mm_setzero_si128();\n    __m128i v_acc_AB = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i vA = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i vB = _mm_loadu_si128((const __m128i*)(B + i));\n\n        __m128i vA_low = _mm_unpacklo_epi8(vA, vA);\n        vA_low = _mm_srai_epi16(vA_low, 8);\n        __m128i vA_high = _mm_unpackhi_epi8(vA, vA);\n        vA_high = _mm_srai_epi16(vA_high, 8);\n\n        __m128i vB_low = _mm_unpacklo_epi8(vB, vB);\n        vB_low = _mm_srai_epi16(vB_low, 8);\n        __m128i vB_high = _mm_unpackhi_epi8(vB, vB);\n        vB_high = _mm_srai_epi16(vB_high, 8);\n\n        __m128i vAB_low = _mm_madd_epi16(vA_low, vB_low);\n        __m128i vAB_high = _mm_madd_epi16(vA_high, vB_high);\n        __m128i vA2_low = _mm_madd_epi16(vA_low, vA_low);\n        __m128i vA2_high = _mm_madd_epi16(vA_high, vA_high);\n\n        v_acc_AB = _mm_add_epi32(v_acc_AB, vAB_low);\n        v_acc_AB = _mm_add_epi32(v_acc_AB, vAB_high);\n        v_acc_A2 = _mm_add_epi32(v_acc_A2, vA2_low);\n        v_acc_A2 = _mm_add_epi32(v_acc_A2, vA2_high);\n    }\n\n    int32_t tmp_A2[4];\n    int32_t tmp_AB[4];\n    _mm_storeu_si128((__m128i*)tmp_A2, v_acc_A2);\n    _mm_storeu_si128((__m128i*)tmp_AB, v_acc_AB);\n\n    total_A2 = (int64_t)tmp_A2[0] + (int64_t)tmp_A2[1] + (int64_t)tmp_A2[2] + (int64_t)tmp_A2[3];\n    total_AB = (int64_t)tmp_AB[0] + (int64_t)tmp_AB[1] + (int64_t)tmp_AB[2] + (int64_t)tmp_AB[3];\n\n    for (; i < length; i++) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        total_A2 += (int64_t)a * a;\n        total_AB += (int64_t)a * b;\n    }\n\n    return total_A2 - total_AB;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t sA2 = 0, sB2 = 0, sAB = 0;\n    size_t i = 0;\n    __m128i vzero = _mm_setzero_si128();\n\n    for (; i + 15 < length; i += 16) {\n        __m128i vA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i signA = _mm_cmpgt_epi8(vzero, vA);\n        __m128i signB = _mm_cmpgt_epi8(vzero, vB);\n        __m128i vA_low = _mm_unpacklo_epi8(vA, signA);\n        __m128i vA_high = _mm_unpackhi_epi8(vA, signA);\n        __m128i vB_low = _mm_unpacklo_epi8(vB, signB);\n        __m128i vB_high = _mm_unpackhi_epi8(vB, signB);\n\n        __m128i vA2_low = _mm_madd_epi16(vA_low, vA_low);\n        __m128i vA2_high = _mm_madd_epi16(vA_high, vA_high);\n        __m128i vA2_total = _mm_add_epi32(vA2_low, vA2_high);\n        vA2_total = _mm_hadd_epi32(vA2_total, vA2_total);\n        vA2_total = _mm_hadd_epi32(vA2_total, vA2_total);\n        sA2 += (int32_t)_mm_cvtsi128_si32(vA2_total);\n\n        __m128i vB2_low = _mm_madd_epi16(vB_low, vB_low);\n        __m128i vB2_high = _mm_madd_epi16(vB_high, vB_high);\n        __m128i vB2_total = _mm_add_epi32(vB2_low, vB2_high);\n        vB2_total = _mm_hadd_epi32(vB2_total, vB2_total);\n        vB2_total = _mm_hadd_epi32(vB2_total, vB2_total);\n        sB2 += (int32_t)_mm_cvtsi128_si32(vB2_total);\n\n        __m128i vAB_low = _mm_madd_epi16(vA_low, vB_low);\n        __m128i vAB_high = _mm_madd_epi16(vA_high, vB_high);\n        __m128i vAB_total = _mm_add_epi32(vAB_low, vAB_high);\n        vAB_total = _mm_hadd_epi32(vAB_total, vAB_total);\n        vAB_total = _mm_hadd_epi32(vAB_total, vAB_total);\n        sAB += (int32_t)_mm_cvtsi128_si32(vAB_total);\n    }\n\n    for (; i < length; i++) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        sA2 += (int64_t)a * a;\n        sB2 += (int64_t)b * b;\n        sAB += (int64_t)a * b;\n    }\n\n    int64_t result = sA2 + sB2 - 2 * sAB;\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t sum_sqA = 0, sum_sqB = 0, sum_prod = 0;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 15 < length; i += 16) {\n        __m128i va = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i vb = _mm_loadu_si128((const __m128i*)(B + i));\n\n        __m128i sign_va = _mm_cmpgt_epi8(zero, va);\n        __m128i va_low = _mm_unpacklo_epi8(va, sign_va);\n        __m128i va_high = _mm_unpackhi_epi8(va, sign_va);\n\n        __m128i sign_vb = _mm_cmpgt_epi8(zero, vb);\n        __m128i vb_low = _mm_unpacklo_epi8(vb, sign_vb);\n        __m128i vb_high = _mm_unpackhi_epi8(vb, sign_vb);\n\n        __m128i sqA_low = _mm_mullo_epi16(va_low, va_low);\n        __m128i sqA_high = _mm_mullo_epi16(va_high, va_high);\n        __m128i sqB_low = _mm_mullo_epi16(vb_low, vb_low);\n        __m128i sqB_high = _mm_mullo_epi16(vb_high, vb_high);\n\n        __m128i prod_low = _mm_mullo_epi16(va_low, vb_low);\n        __m128i prod_high = _mm_mullo_epi16(va_high, vb_high);\n\n        __m128i sqA_low_lo32 = _mm_unpacklo_epi16(sqA_low, zero);\n        __m128i sqA_low_hi32 = _mm_unpackhi_epi16(sqA_low, zero);\n        __m128i sqA_high_lo32 = _mm_unpacklo_epi16(sqA_high, zero);\n        __m128i sqA_high_hi32 = _mm_unpackhi_epi16(sqA_high, zero);\n\n        __m128i sqB_low_lo32 = _mm_unpacklo_epi16(sqB_low, zero);\n        __m128i sqB_low_hi32 = _mm_unpackhi_epi16(sqB_low, zero);\n        __m128i sqB_high_lo32 = _mm_unpacklo_epi16(sqB_high, zero);\n        __m128i sqB_high_hi32 = _mm_unpackhi_epi16(sqB_high, zero);\n\n        __m128i sign_prod_low = _mm_cmpgt_epi16(zero, prod_low);\n        __m128i prod_low_lo32 = _mm_unpacklo_epi16(prod_low, sign_prod_low);\n        __m128i prod_low_hi32 = _mm_unpackhi_epi16(prod_low, sign_prod_low);\n\n        __m128i sign_prod_high = _mm_cmpgt_epi16(zero, prod_high);\n        __m128i prod_high_lo32 = _mm_unpacklo_epi16(prod_high, sign_prod_high);\n        __m128i prod_high_hi32 = _mm_unpackhi_epi16(prod_high, sign_prod_high);\n\n        __m128i sum_sqA_chunk = _mm_add_epi32(sqA_low_lo32, sqA_low_hi32);\n        sum_sqA_chunk = _mm_add_epi32(sum_sqA_chunk, sqA_high_lo32);\n        sum_sqA_chunk = _mm_add_epi32(sum_sqA_chunk, sqA_high_hi32);\n\n        __m128i sum_sqB_chunk = _mm_add_epi32(sqB_low_lo32, sqB_low_hi32);\n        sum_sqB_chunk = _mm_add_epi32(sum_sqB_chunk, sqB_high_lo32);\n        sum_sqB_chunk = _mm_add_epi32(sum_sqB_chunk, sqB_high_hi32);\n\n        __m128i sum_prod_chunk = _mm_add_epi32(prod_low_lo32, prod_low_hi32);\n        sum_prod_chunk = _mm_add_epi32(sum_prod_chunk, prod_high_lo32);\n        sum_prod_chunk = _mm_add_epi32(sum_prod_chunk, prod_high_hi32);\n\n        __m128i v1 = _mm_srli_si128(sum_sqA_chunk, 8);\n        sum_sqA_chunk = _mm_add_epi32(sum_sqA_chunk, v1);\n        __m128i v2 = _mm_srli_si128(sum_sqA_chunk, 4);\n        sum_sqA_chunk = _mm_add_epi32(sum_sqA_chunk, v2);\n        int32_t chunk_sum_sqA = _mm_cvtsi128_si32(sum_sqA_chunk);\n\n        v1 = _mm_srli_si128(sum_sqB_chunk, 8);\n        sum_sqB_chunk = _mm_add_epi32(sum_sqB_chunk, v1);\n        v2 = _mm_srli_si128(sum_sqB_chunk, 4);\n        sum_sqB_chunk = _mm_add_epi32(sum_sqB_chunk, v2);\n        int32_t chunk_sum_sqB = _mm_cvtsi128_si32(sum_sqB_chunk);\n\n        v1 = _mm_srli_si128(sum_prod_chunk, 8);\n        sum_prod_chunk = _mm_add_epi32(sum_prod_chunk, v1);\n        v2 = _mm_srli_si128(sum_prod_chunk, 4);\n        sum_prod_chunk = _mm_add_epi32(sum_prod_chunk, v2);\n        int32_t chunk_sum_prod = _mm_cvtsi128_si32(sum_prod_chunk);\n\n        sum_sqA += chunk_sum_sqA;\n        sum_sqB += chunk_sum_sqB;\n        sum_prod += chunk_sum_prod;\n    }\n\n    for (; i < length; i++) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        sum_sqA += a * a;\n        sum_sqB += b * b;\n        sum_prod += a * b;\n    }\n\n    return sum_sqA + sum_sqB - 2 * sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    const size_t stride = 16;\n    size_t iters = length / stride;\n    size_t remainder = length % stride;\n    __m128i zero = _mm_setzero_si128();\n    __m128i sq_acc = _mm_setzero_si128();\n    __m128i prod_acc = _mm_setzero_si128();\n\n    for (size_t i = 0; i < iters; i++) {\n        __m128i vA = _mm_loadu_si128((__m128i*)(A + i * stride));\n        __m128i vB = _mm_loadu_si128((__m128i*)(B + i * stride));\n\n        __m128i sign_mask_A = _mm_cmpgt_epi8(zero, vA);\n        __m128i vA_low = _mm_unpacklo_epi8(vA, sign_mask_A);\n        __m128i vA_high = _mm_unpackhi_epi8(vA, sign_mask_A);\n\n        __m128i sign_mask_B = _mm_cmpgt_epi8(zero, vB);\n        __m128i vB_low = _mm_unpacklo_epi8(vB, sign_mask_B);\n        __m128i vB_high = _mm_unpackhi_epi8(vB, sign_mask_B);\n\n        __m128i sqA_low = _mm_mullo_epi16(vA_low, vA_low);\n        __m128i sqA_high = _mm_mullo_epi16(vA_high, vA_high);\n\n        __m128i prod_low = _mm_mullo_epi16(vA_low, vB_low);\n        __m128i prod_high = _mm_mullo_epi16(vA_high, vB_high);\n\n        __m128i sign_mask_sqA_low = _mm_cmpgt_epi16(zero, sqA_low);\n        __m128i sqA_low_low = _mm_unpacklo_epi16(sqA_low, sign_mask_sqA_low);\n        __m128i sqA_low_high = _mm_unpackhi_epi16(sqA_low, sign_mask_sqA_low);\n\n        __m128i sign_mask_sqA_high = _mm_cmpgt_epi16(zero, sqA_high);\n        __m128i sqA_high_low = _mm_unpacklo_epi16(sqA_high, sign_mask_sqA_high);\n        __m128i sqA_high_high = _mm_unpackhi_epi16(sqA_high, sign_mask_sqA_high);\n\n        __m128i sign_mask_prod_low = _mm_cmpgt_epi16(zero, prod_low);\n        __m128i prod_low_low = _mm_unpacklo_epi16(prod_low, sign_mask_prod_low);\n        __m128i prod_low_high = _mm_unpackhi_epi16(prod_low, sign_mask_prod_low);\n\n        __m128i sign_mask_prod_high = _mm_cmpgt_epi16(zero, prod_high);\n        __m128i prod_high_low = _mm_unpacklo_epi16(prod_high, sign_mask_prod_high);\n        __m128i prod_high_high = _mm_unpackhi_epi16(prod_high, sign_mask_prod_high);\n\n        sq_acc = _mm_add_epi32(sq_acc, sqA_low_low);\n        sq_acc = _mm_add_epi32(sq_acc, sqA_low_high);\n        sq_acc = _mm_add_epi32(sq_acc, sqA_high_low);\n        sq_acc = _mm_add_epi32(sq_acc, sqA_high_high);\n\n        prod_acc = _mm_add_epi32(prod_acc, prod_low_low);\n        prod_acc = _mm_add_epi32(prod_acc, prod_low_high);\n        prod_acc = _mm_add_epi32(prod_acc, prod_high_low);\n        prod_acc = _mm_add_epi32(prod_acc, prod_high_high);\n    }\n\n    int32_t sq_array[4] = {0};\n    int32_t prod_array[4] = {0};\n    _mm_storeu_si128((__m128i*)sq_array, sq_acc);\n    _mm_storeu_si128((__m128i*)prod_array, prod_acc);\n    int64_t total_sq = (int64_t)sq_array[0] + sq_array[1] + sq_array[2] + sq_array[3];\n    int64_t total_prod = (int64_t)prod_array[0] + prod_array[1] + prod_array[2] + prod_array[3];\n\n    for (size_t i = iters * stride; i < length; i++) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        total_sq += (int64_t)a * a;\n        total_prod += (int64_t)a * b;\n    }\n\n    return total_sq - total_prod;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i result = _mm_andnot_si128(b_vec, a_vec);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t step = 2;\n    const size_t simd_iters = length / step;\n\n    for (; i < simd_iters * step; i += step) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i result = _mm_andnot_si128(b_vec, a_vec);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 2; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i result = _mm_andnot_si128(b, a);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    const size_t simd_elements = 2;\n    const size_t simd_steps = length / simd_elements;\n    \n    for (; i < simd_steps * simd_elements; i += simd_elements) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i result = _mm_andnot_si128(b_vec, a_vec);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i result = _mm_andnot_si128(b, a);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi8(1);\n    const __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(B + i));\n\n        __m128i a_pos = _mm_cmpgt_epi8(a, zero);\n        __m128i a_neg = _mm_cmpgt_epi8(zero, a);\n        __m128i b_pos = _mm_cmpgt_epi8(b, zero);\n        __m128i b_neg = _mm_cmpgt_epi8(zero, b);\n\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n\n        __m128i res = _mm_or_si128(\n            _mm_and_si128(both_pos, one),\n            _mm_and_si128(both_neg, minus_one)\n        );\n\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i minus_ones = _mm_set1_epi8(-1);\n    \n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, zero);\n        __m128i b_pos = _mm_cmpgt_epi8(b, zero);\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        \n        __m128i a_neg = _mm_cmplt_epi8(a, zero);\n        __m128i b_neg = _mm_cmplt_epi8(b, zero);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i res_pos = _mm_and_si128(both_pos, ones);\n        __m128i res_neg = _mm_and_si128(both_neg, minus_ones);\n        \n        __m128i res = _mm_or_si128(res_pos, res_neg);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    \n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi8(1);\n    const __m128i minus_one = _mm_set1_epi8(-1);\n    \n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a, zero);\n        __m128i b_pos = _mm_cmpgt_epi8(b, zero);\n        __m128i a_neg = _mm_cmplt_epi8(a, zero);\n        __m128i b_neg = _mm_cmplt_epi8(b, zero);\n        \n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i res_pos = _mm_and_si128(both_pos, one);\n        __m128i res_neg = _mm_and_si128(both_neg, minus_one);\n        \n        __m128i result = _mm_or_si128(res_pos, res_neg);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    \n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one_vec = _mm_set1_epi8(1);\n    __m128i minus_one_vec = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_positive = _mm_cmpgt_epi8(a, zero);\n        __m128i b_positive = _mm_cmpgt_epi8(b, zero);\n        __m128i both_positive = _mm_and_si128(a_positive, b_positive);\n        __m128i pos_result = _mm_and_si128(both_positive, one_vec);\n\n        __m128i a_negative = _mm_cmpgt_epi8(zero, a);\n        __m128i b_negative = _mm_cmpgt_epi8(zero, b);\n        __m128i both_negative = _mm_and_si128(a_negative, b_negative);\n        __m128i neg_result = _mm_and_si128(both_negative, minus_one_vec);\n\n        __m128i result = _mm_or_si128(pos_result, neg_result);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i neg_ones = _mm_set1_epi8(-1);\n\n    for (; i + 15 < length; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        \n        __m128i a_pos = _mm_cmpgt_epi8(a_vec, zero);\n        __m128i b_pos = _mm_cmpgt_epi8(b_vec, zero);\n        __m128i both_pos = _mm_and_si128(a_pos, b_pos);\n        \n        __m128i a_neg = _mm_cmplt_epi8(a_vec, zero);\n        __m128i b_neg = _mm_cmplt_epi8(b_vec, zero);\n        __m128i both_neg = _mm_and_si128(a_neg, b_neg);\n        \n        __m128i res_pos = _mm_and_si128(both_pos, ones);\n        __m128i res_neg = _mm_and_si128(both_neg, neg_ones);\n        __m128i result = _mm_or_si128(res_pos, res_neg);\n        \n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) return;\n    \n    size_t mid = length / 2;\n    size_t i = 0;\n    \n    // Process elements before mid (XOR)\n    for (; i + 8 <= mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Set midpoint element\n    C[mid] = A[mid];\n    i = mid + 1;\n    \n    // Process elements after mid (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i res = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid_index = length / 2;\n    bool has_mid = (length % 2 != 0);\n    size_t first_part_end = mid_index;\n    \n    size_t i = 0;\n    for (; i + 8 <= first_part_end; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < first_part_end; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    if (has_mid) {\n        C[mid_index] = A[mid_index];\n    }\n    \n    size_t j = has_mid ? mid_index + 1 : mid_index;\n    for (; j + 8 <= length; j += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + j));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + j));\n        __m128i res = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + j), res);\n    }\n    for (; j < length; j++) {\n        C[j] = A[j] & B[j];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    \n    // Process elements before midpoint (XOR)\n    size_t i = 0;\n    for (; i + 8 <= mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c_vec = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Set midpoint element\n    if (length > 0) {\n        C[mid] = A[mid];\n    }\n    \n    // Process elements after midpoint (AND)\n    i = mid + 1;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i c_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), c_vec);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) {\n        return;\n    }\n    size_t mid = length / 2;\n    size_t i = 0;\n    for (; i + 8 <= mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n    C[mid] = A[mid];\n    i = mid + 1;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i res = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    if (length == 0) return;\n    \n    size_t mid = length / 2;\n    size_t i = 0;\n    \n    // Process elements before midpoint (XOR)\n    for (; i + 8 <= mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i res = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < mid; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n    \n    // Set midpoint element\n    C[mid] = A[mid];\n    i = mid + 1;\n    \n    // Process elements after midpoint (AND)\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)(B + i));\n        __m128i res = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    for (; i + 3 < total; i += 4) {\n        bool c10 = cond1[i];\n        bool c11 = cond1[i+1];\n        bool c12 = cond1[i+2];\n        bool c13 = cond1[i+3];\n        bool c20 = cond2[i];\n        bool c21 = cond2[i+1];\n        bool c22 = cond2[i+2];\n        bool c23 = cond2[i+3];\n        \n        __m128 mask1 = _mm_set_ps(c13 ? -1.0f : 0.0f, c12 ? -1.0f : 0.0f, c11 ? -1.0f : 0.0f, c10 ? -1.0f : 0.0f);\n        __m128 mask2 = _mm_set_ps(c23 ? -1.0f : 0.0f, c22 ? -1.0f : 0.0f, c21 ? -1.0f : 0.0f, c20 ? -1.0f : 0.0f);\n        \n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 c = _mm_loadu_ps(C + i);\n        __m128 d = _mm_loadu_ps(D + i);\n        \n        __m128 tmp1 = _mm_or_ps(_mm_and_ps(mask2, a), _mm_andnot_ps(mask2, b));\n        __m128 tmp2 = _mm_or_ps(_mm_and_ps(mask2, c), _mm_andnot_ps(mask2, d));\n        __m128 result = _mm_or_ps(_mm_and_ps(mask1, tmp1), _mm_andnot_ps(mask1, tmp2));\n        \n        _mm_storeu_ps(out + i, result);\n    }\n    for (; i < total; ++i) {\n        if (cond1[i]) {\n            out[i] = cond2[i] ? A[i] : B[i];\n        } else {\n            out[i] = cond2[i] ? C[i] : D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i cond1_8 = _mm_loadu_si32(cond1 + i);\n        __m128i cond1_16 = _mm_unpacklo_epi8(cond1_8, zero);\n        __m128i cond1_vec = _mm_unpacklo_epi16(cond1_16, zero);\n        __m128i mask1 = _mm_cmpeq_epi32(cond1_vec, _mm_set1_epi32(1));\n\n        __m128i cond2_8 = _mm_loadu_si32(cond2 + i);\n        __m128i cond2_16 = _mm_unpacklo_epi8(cond2_8, zero);\n        __m128i cond2_vec = _mm_unpacklo_epi16(cond2_16, zero);\n        __m128i mask2 = _mm_cmpeq_epi32(cond2_vec, _mm_set1_epi32(1));\n\n        __m128i not_mask1 = _mm_andnot_si128(mask1, _mm_set1_epi32(-1));\n        __m128i not_mask2 = _mm_andnot_si128(mask2, _mm_set1_epi32(-1));\n\n        __m128i term1 = _mm_and_si128(mask1, mask2);\n        __m128i term2 = _mm_and_si128(mask1, not_mask2);\n        __m128i term3 = _mm_and_si128(not_mask1, mask2);\n        __m128i term4 = _mm_and_si128(not_mask1, not_mask2);\n\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 c = _mm_loadu_ps(C + i);\n        __m128 d = _mm_loadu_ps(D + i);\n\n        __m128 t1 = _mm_and_ps(_mm_castsi128_ps(term1), a);\n        __m128 t2 = _mm_and_ps(_mm_castsi128_ps(term2), b);\n        __m128 t3 = _mm_and_ps(_mm_castsi128_ps(term3), c);\n        __m128 t4 = _mm_and_ps(_mm_castsi128_ps(term4), d);\n\n        __m128 temp1 = _mm_or_ps(t1, t2);\n        __m128 temp2 = _mm_or_ps(t3, t4);\n        __m128 result = _mm_or_ps(temp1, temp2);\n\n        _mm_storeu_ps(out + i, result);\n    }\n\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const __m128 all_ones = _mm_castsi128_ps(_mm_set1_epi32(-1));\n\n    for (; i + 4 <= total_elements; i += 4) {\n        uint32_t c1_int, c2_int;\n        memcpy(&c1_int, cond1 + i, sizeof(uint32_t));\n        memcpy(&c2_int, cond2 + i, sizeof(uint32_t));\n\n        uint32_t mask1_0 = (uint32_t)(-(int32_t)(c1_int & 0xFF));\n        uint32_t mask1_1 = (uint32_t)(-(int32_t)((c1_int >> 8) & 0xFF));\n        uint32_t mask1_2 = (uint32_t)(-(int32_t)((c1_int >> 16) & 0xFF));\n        uint32_t mask1_3 = (uint32_t)(-(int32_t)((c1_int >> 24) & 0xFF));\n\n        uint32_t mask2_0 = (uint32_t)(-(int32_t)(c2_int & 0xFF));\n        uint32_t mask2_1 = (uint32_t)(-(int32_t)((c2_int >> 8) & 0xFF));\n        uint32_t mask2_2 = (uint32_t)(-(int32_t)((c2_int >> 16) & 0xFF));\n        uint32_t mask2_3 = (uint32_t)(-(int32_t)((c2_int >> 24) & 0xFF));\n\n        __m128 mask1 = _mm_castsi128_ps(_mm_set_epi32(mask1_3, mask1_2, mask1_1, mask1_0));\n        __m128 mask2 = _mm_castsi128_ps(_mm_set_epi32(mask2_3, mask2_2, mask2_1, mask2_0));\n\n        __m128 a_val = _mm_loadu_ps(A + i);\n        __m128 b_val = _mm_loadu_ps(B + i);\n        __m128 c_val = _mm_loadu_ps(C + i);\n        __m128 d_val = _mm_loadu_ps(D + i);\n\n        __m128 mask_a = _mm_and_ps(mask1, mask2);\n        __m128 mask_b = _mm_andnot_ps(mask2, mask1);\n        __m128 mask_c = _mm_andnot_ps(mask1, mask2);\n        __m128 mask_d = _mm_andnot_ps(_mm_or_ps(mask1, mask2), all_ones);\n\n        __m128 out_val = _mm_or_ps(_mm_and_ps(mask_a, a_val), _mm_and_ps(mask_b, b_val));\n        out_val = _mm_or_ps(out_val, _mm_and_ps(mask_c, c_val));\n        out_val = _mm_or_ps(out_val, _mm_and_ps(mask_d, d_val));\n\n        _mm_storeu_ps(out + i, out_val);\n    }\n\n    for (; i < total_elements; i++) {\n        if (cond1[i]) {\n            out[i] = cond2[i] ? A[i] : B[i];\n        } else {\n            out[i] = cond2[i] ? C[i] : D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        uint32_t cond1_chunk;\n        uint32_t cond2_chunk;\n        memcpy(&cond1_chunk, cond1 + i, 4);\n        memcpy(&cond2_chunk, cond2 + i, 4);\n\n        __m128i mask1_i = _mm_set_epi32(\n            (cond1_chunk >> 24) & 0xFF ? -1 : 0,\n            (cond1_chunk >> 16) & 0xFF ? -1 : 0,\n            (cond1_chunk >> 8) & 0xFF ? -1 : 0,\n            cond1_chunk & 0xFF ? -1 : 0\n        );\n        __m128i mask2_i = _mm_set_epi32(\n            (cond2_chunk >> 24) & 0xFF ? -1 : 0,\n            (cond2_chunk >> 16) & 0xFF ? -1 : 0,\n            (cond2_chunk >> 8) & 0xFF ? -1 : 0,\n            cond2_chunk & 0xFF ? -1 : 0\n        );\n\n        __m128 mask1 = _mm_castsi128_ps(mask1_i);\n        __m128 mask2 = _mm_castsi128_ps(mask2_i);\n\n        __m128 a_vec = _mm_loadu_ps(A + i);\n        __m128 b_vec = _mm_loadu_ps(B + i);\n        __m128 c_vec = _mm_loadu_ps(C + i);\n        __m128 d_vec = _mm_loadu_ps(D + i);\n\n        __m128 part1 = _mm_or_ps(\n            _mm_and_ps(mask2, c_vec),\n            _mm_andnot_ps(mask2, d_vec)\n        );\n        __m128 part2 = _mm_or_ps(\n            _mm_and_ps(mask2, a_vec),\n            _mm_andnot_ps(mask2, b_vec)\n        );\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(mask1, part2),\n            _mm_andnot_ps(mask1, part1)\n        );\n\n        _mm_storeu_ps(out + i, result);\n    }\n\n    for (; i < total; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t rounded_total = total & ~((size_t)3);\n    const float* A_ptr = A;\n    const float* B_ptr = B;\n    const float* C_ptr = C;\n    const float* D_ptr = D;\n    const bool* cond1_ptr = cond1;\n    const bool* cond2_ptr = cond2;\n    float* out_ptr = out;\n\n    for (size_t i = 0; i < rounded_total; i += 4) {\n        __m128i mask1 = _mm_set_epi32(\n            -static_cast<int>(cond1_ptr[3]),\n            -static_cast<int>(cond1_ptr[2]),\n            -static_cast<int>(cond1_ptr[1]),\n            -static_cast<int>(cond1_ptr[0])\n        );\n        __m128i mask2 = _mm_set_epi32(\n            -static_cast<int>(cond2_ptr[3]),\n            -static_cast<int>(cond2_ptr[2]),\n            -static_cast<int>(cond2_ptr[1]),\n            -static_cast<int>(cond2_ptr[0])\n        );\n        __m128 mask1_f = _mm_castsi128_ps(mask1);\n        __m128 mask2_f = _mm_castsi128_ps(mask2);\n        __m128 a_val = _mm_loadu_ps(A_ptr);\n        __m128 b_val = _mm_loadu_ps(B_ptr);\n        __m128 c_val = _mm_loadu_ps(C_ptr);\n        __m128 d_val = _mm_loadu_ps(D_ptr);\n        __m128 true_branch = _mm_or_ps(_mm_and_ps(mask2_f, a_val), _mm_andnot_ps(mask2_f, b_val));\n        __m128 false_branch = _mm_or_ps(_mm_and_ps(mask2_f, c_val), _mm_andnot_ps(mask2_f, d_val));\n        __m128 result = _mm_or_ps(_mm_and_ps(mask1_f, true_branch), _mm_andnot_ps(mask1_f, false_branch));\n        _mm_storeu_ps(out_ptr, result);\n        A_ptr += 4;\n        B_ptr += 4;\n        C_ptr += 4;\n        D_ptr += 4;\n        cond1_ptr += 4;\n        cond2_ptr += 4;\n        out_ptr += 4;\n    }\n\n    for (size_t i = rounded_total; i < total; i++) {\n        bool c1 = *cond1_ptr;\n        bool c2 = *cond2_ptr;\n        if (c1) {\n            if (c2) {\n                *out_ptr = *A_ptr;\n            } else {\n                *out_ptr = *B_ptr;\n            }\n        } else {\n            if (c2) {\n                *out_ptr = *C_ptr;\n            } else {\n                *out_ptr = *D_ptr;\n            }\n        }\n        A_ptr++;\n        B_ptr++;\n        C_ptr++;\n        D_ptr++;\n        cond1_ptr++;\n        cond2_ptr++;\n        out_ptr++;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    \n    for (; i + 3 < total; i += 4) {\n        __m128i x = _mm_loadu_si128((const __m128i*)(A + i));\n        \n        __m128i t = _mm_srli_epi32(x, 1);\n        t = _mm_and_si128(t, mask1);\n        x = _mm_sub_epi32(x, t);\n        \n        __m128i a = _mm_and_si128(x, mask2);\n        __m128i b = _mm_srli_epi32(x, 2);\n        b = _mm_and_si128(b, mask2);\n        x = _mm_add_epi32(a, b);\n        \n        __m128i c = _mm_srli_epi32(x, 4);\n        x = _mm_add_epi32(x, c);\n        x = _mm_and_si128(x, mask3);\n        \n        alignas(16) uint32_t temp[4];\n        _mm_store_si128((__m128i*)temp, x);\n        \n        for (int j = 0; j < 4; j++) {\n            uint32_t v = temp[j];\n            v = (v * 0x01010101) >> 24;\n            out[i + j] = v;\n        }\n    }\n    \n    for (; i < total; i++) {\n        uint32_t v = A[i];\n        v = v - ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v + (v >> 4)) & 0x0F0F0F0F;\n        v = (v * 0x01010101) >> 24;\n        out[i] = v;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    const __m128i m1 = _mm_set1_epi32(0x55555555);\n    const __m128i m2 = _mm_set1_epi32(0x33333333);\n    const __m128i m4 = _mm_set1_epi32(0x0F0F0F0F);\n\n    for (; i + 3 < total_elements; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(A + i));\n        \n        __m128i x = _mm_srli_epi32(v, 1);\n        x = _mm_and_si128(x, m1);\n        v = _mm_sub_epi32(v, x);\n        \n        x = _mm_and_si128(v, m2);\n        v = _mm_srli_epi32(v, 2);\n        v = _mm_and_si128(v, m2);\n        v = _mm_add_epi32(v, x);\n        \n        x = _mm_srli_epi32(v, 4);\n        v = _mm_add_epi32(v, x);\n        v = _mm_and_si128(v, m4);\n        \n        uint32_t temp[4];\n        _mm_storeu_si128((__m128i*)temp, v);\n        \n        for (int j = 0; j < 4; j++) {\n            out[i + j] = (temp[j] * 0x01010101) >> 24;\n        }\n    }\n    \n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        out[i] = (val * 0x01010101) >> 24;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t num_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 3 < num_elements; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        \n        __m128i t = _mm_and_si128(a_vec, mask1);\n        __m128i t_shift = _mm_srli_epi32(a_vec, 1);\n        t_shift = _mm_and_si128(t_shift, mask1);\n        t = _mm_add_epi32(t, t_shift);\n        \n        t_shift = _mm_srli_epi32(t, 2);\n        t = _mm_and_si128(t, mask2);\n        t_shift = _mm_and_si128(t_shift, mask2);\n        t = _mm_add_epi32(t, t_shift);\n        \n        t_shift = _mm_srli_epi32(t, 4);\n        t = _mm_and_si128(t, mask3);\n        t_shift = _mm_and_si128(t_shift, mask3);\n        t = _mm_add_epi32(t, t_shift);\n        \n        __m128i a = _mm_and_si128(t, mask4);\n        __m128i b = _mm_and_si128(_mm_srli_epi32(t, 8), mask4);\n        t = _mm_add_epi32(a, b);\n        \n        a = _mm_and_si128(t, mask5);\n        b = _mm_and_si128(_mm_srli_epi32(t, 16), mask5);\n        t = _mm_add_epi32(a, b);\n        \n        __m128i t16 = _mm_packs_epi32(t, zero);\n        __m128i t8 = _mm_packus_epi16(t16, zero);\n        \n        uint32_t store_val = _mm_cvtsi128_si32(t8);\n        memcpy(out + i, &store_val, 4);\n    }\n\n    for (; i < num_elements; i++) {\n        uint32_t a_val = A[i];\n        a_val = (a_val & 0x55555555) + ((a_val >> 1) & 0x55555555);\n        a_val = (a_val & 0x33333333) + ((a_val >> 2) & 0x33333333);\n        a_val = (a_val & 0x0F0F0F0F) + ((a_val >> 4) & 0x0F0F0F0F);\n        a_val = (a_val & 0x00FF00FF) + ((a_val >> 8) & 0x00FF00FF);\n        a_val = (a_val & 0x0000FFFF) + ((a_val >> 16) & 0x0000FFFF);\n        out[i] = (uint8_t)a_val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    const __m128i m1 = _mm_set1_epi32(0x55555555);\n    const __m128i m2 = _mm_set1_epi32(0x33333333);\n    const __m128i m3 = _mm_set1_epi32(0x0F0F0F0F);\n\n    for (; i + 3 < total; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i t = _mm_srli_epi32(v, 1);\n        t = _mm_and_si128(t, m1);\n        v = _mm_sub_epi32(v, t);\n        t = _mm_srli_epi32(v, 2);\n        t = _mm_and_si128(t, m2);\n        v = _mm_and_si128(v, m2);\n        v = _mm_add_epi32(v, t);\n        t = _mm_srli_epi32(v, 4);\n        v = _mm_add_epi32(v, t);\n        v = _mm_and_si128(v, m3);\n\n        alignas(16) uint32_t temp[4];\n        _mm_store_si128((__m128i*)temp, v);\n        for (int j = 0; j < 4; j++) {\n            uint32_t x = temp[j];\n            x = (x * 0x01010101) >> 24;\n            out[i + j] = (uint8_t)x;\n        }\n    }\n\n    for (; i < total; i++) {\n        uint32_t x = A[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = (x * 0x01010101) >> 24;\n        out[i] = (uint8_t)x;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total = dim1 * dim2 * dim3;\n    size_t i = 0;\n    \n    for (; i + 3 < total; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        \n        __m128i mask1 = _mm_set1_epi32(0x55555555);\n        __m128i b = _mm_srli_epi32(a, 1);\n        b = _mm_and_si128(b, mask1);\n        a = _mm_and_si128(a, mask1);\n        a = _mm_add_epi32(a, b);\n        \n        __m128i mask2 = _mm_set1_epi32(0x33333333);\n        b = _mm_srli_epi32(a, 2);\n        b = _mm_and_si128(b, mask2);\n        a = _mm_and_si128(a, mask2);\n        a = _mm_add_epi32(a, b);\n        \n        __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n        b = _mm_srli_epi32(a, 4);\n        b = _mm_and_si128(b, mask3);\n        a = _mm_and_si128(a, mask3);\n        a = _mm_add_epi32(a, b);\n        \n        __m128i mask = _mm_set1_epi32(0x000000FF);\n        __m128i b0 = _mm_and_si128(a, mask);\n        __m128i b1 = _mm_srli_epi32(a, 8);\n        b1 = _mm_and_si128(b1, mask);\n        __m128i b2 = _mm_srli_epi32(a, 16);\n        b2 = _mm_and_si128(b2, mask);\n        __m128i b3 = _mm_srli_epi32(a, 24);\n        b3 = _mm_and_si128(b3, mask);\n        \n        a = _mm_add_epi32(b0, b1);\n        a = _mm_add_epi32(a, b2);\n        a = _mm_add_epi32(a, b3);\n        \n        __m128i a16 = _mm_packs_epi32(a, _mm_setzero_si128());\n        __m128i a8 = _mm_packus_epi16(a16, _mm_setzero_si128());\n        \n        *((uint32_t*)(out + i)) = _mm_cvtsi128_si32(a8);\n    }\n    \n    for (; i < total; ++i) {\n        uint32_t x = A[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x & 0x0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F);\n        x = (x + (x >> 8)) & 0x00FF00FF;\n        x = (x + (x >> 16)) & 0x000000FF;\n        out[i] = (uint8_t)x;\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    for (; i + 3 < total; i += 4) {\n        __m128i x = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 x_float = _mm_cvtepi32_ps(x);\n        __m128 div = _mm_div_ps(x_float, _mm_set1_ps(3.0f));\n        __m128i q = _mm_cvttps_epi32(div);\n        __m128i q3 = _mm_add_epi32(q, _mm_add_epi32(q, q));\n        __m128i r = _mm_sub_epi32(x, q3);\n        __m128i mask = _mm_cmpeq_epi32(r, _mm_setzero_si128());\n        __m128i sign = _mm_srai_epi32(x, 31);\n        __m128i abs_x = _mm_sub_epi32(_mm_xor_si128(x, sign), sign);\n        __m128i res = _mm_or_si128(_mm_and_si128(mask, abs_x), _mm_andnot_si128(mask, x));\n        _mm_storeu_si128((__m128i*)(B + i), res);\n    }\n\n    for (; i < total; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = (A[i] < 0) ? -A[i] : A[i];\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    \n    __m128i v0xFFFF = _mm_set1_epi32(0xFFFF);\n    __m128i v0xFF = _mm_set1_epi32(0xFF);\n    __m128i v0xF = _mm_set1_epi32(0xF);\n    __m128i v0x3 = _mm_set1_epi32(0x3);\n    __m128i v3 = _mm_set1_epi32(3);\n    __m128i v2 = _mm_set1_epi32(2);\n    __m128i v0 = _mm_setzero_si128();\n    \n    for (; i + 4 <= total; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i sign_mask = _mm_srai_epi32(a, 31);\n        __m128i abs_a = _mm_sub_epi32(_mm_xor_si128(a, sign_mask), sign_mask);\n        \n        __m128i x = abs_a;\n        x = _mm_add_epi32(_mm_and_si128(x, v0xFFFF), _mm_srli_epi32(x, 16));\n        x = _mm_add_epi32(_mm_and_si128(x, v0xFF), _mm_srli_epi32(x, 8));\n        x = _mm_add_epi32(_mm_and_si128(x, v0xF), _mm_srli_epi32(x, 4));\n        x = _mm_add_epi32(_mm_and_si128(x, v0x3), _mm_srli_epi32(x, 2));\n        x = _mm_add_epi32(_mm_and_si128(x, v0x3), _mm_srli_epi32(x, 2));\n        \n        __m128i gt2 = _mm_cmpgt_epi32(x, v2);\n        x = _mm_sub_epi32(x, _mm_and_si128(gt2, v3));\n        gt2 = _mm_cmpgt_epi32(x, v2);\n        x = _mm_sub_epi32(x, _mm_and_si128(gt2, v3));\n        \n        __m128i mask = _mm_cmpeq_epi32(x, v0);\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask, abs_a),\n            _mm_andnot_si128(mask, a)\n        );\n        _mm_storeu_si128((__m128i*)(B + i), result);\n    }\n    \n    for (; i < total; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = (A[i] < 0) ? -A[i] : A[i];\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    for (; i + 3 < total; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        int temp[4];\n        _mm_storeu_si128((__m128i*)temp, a_vec);\n        for (int j = 0; j < 4; j++) {\n            if (temp[j] % 3 == 0) {\n                if (temp[j] < 0) {\n                    temp[j] = -temp[j];\n                }\n            }\n        }\n        __m128i b_vec = _mm_loadu_si128((const __m128i*)temp);\n        _mm_storeu_si128((__m128i*)(B + i), b_vec);\n    }\n    for (; i < total; i++) {\n        if (A[i] % 3 == 0) {\n            B[i] = (A[i] < 0) ? -A[i] : A[i];\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i intmin = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i mask33 = _mm_set1_epi32(0x33333333);\n    __m128i mask0F = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask00FF = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask0000FFFF = _mm_set1_epi32(0x0000FFFF);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (; i + 3 < total; i += 4) {\n        __m128i v_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i mask_sign = _mm_cmplt_epi32(v_vec, zero);\n        __m128i u_candidate = _mm_sub_epi32(_mm_xor_si128(v_vec, mask_sign), mask_sign);\n        __m128i mask_intmin = _mm_cmpeq_epi32(v_vec, intmin);\n        __m128i u_vec = _mm_or_si128(_mm_and_si128(mask_intmin, one), \n                                     _mm_andnot_si128(mask_intmin, u_candidate));\n        __m128i t = u_vec;\n        t = _mm_add_epi32(_mm_and_si128(t, mask33), _mm_and_si128(_mm_srli_epi32(t, 2), mask33));\n        t = _mm_and_si128(_mm_add_epi32(t, _mm_srli_epi32(t, 4)), mask0F);\n        t = _mm_and_si128(_mm_add_epi32(t, _mm_srli_epi32(t, 8)), mask00FF);\n        t = _mm_add_epi32(t, _mm_srli_epi32(t, 16));\n        t = _mm_and_si128(t, mask0000FFFF);\n        t = _mm_add_epi32(_mm_and_si128(t, three), _mm_and_si128(_mm_srli_epi32(t, 2), three));\n        __m128i mask_ge_3 = _mm_cmpgt_epi32(t, two);\n        t = _mm_sub_epi32(t, _mm_and_si128(mask_ge_3, three));\n        __m128i cond = _mm_cmpeq_epi32(t, zero);\n        __m128i res = _mm_or_si128(_mm_and_si128(cond, u_vec), _mm_andnot_si128(cond, v_vec));\n        _mm_storeu_si128((__m128i*)(B + i), res);\n    }\n    for (; i < total; ++i) {\n        int val = A[i];\n        if (val % 3 == 0) {\n            B[i] = (val == 0x80000000) ? val : (val < 0 ? -val : val);\n        } else {\n            B[i] = val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const int* a_row = A + i * cols;\n        int* b_row = B + i * cols;\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m128i n_vec = _mm_loadu_si128((const __m128i*)(a_row + j));\n            __m128i zero = _mm_setzero_si128();\n            __m128i sign_mask = _mm_srai_epi32(n_vec, 31);\n            __m128i abs_vec = _mm_sub_epi32(_mm_xor_si128(n_vec, sign_mask), sign_mask);\n            \n            __m128i low16 = _mm_and_si128(abs_vec, _mm_set1_epi32(0xFFFF));\n            __m128i high16 = _mm_srli_epi32(abs_vec, 16);\n            __m128i sum1 = _mm_add_epi32(low16, high16);\n            \n            low16 = _mm_and_si128(sum1, _mm_set1_epi32(0xFFFF));\n            high16 = _mm_srli_epi32(sum1, 16);\n            __m128i sum2 = _mm_add_epi32(low16, high16);\n            \n            low16 = _mm_and_si128(sum2, _mm_set1_epi32(0xFFFF));\n            high16 = _mm_srli_epi32(sum2, 16);\n            __m128i small = _mm_add_epi32(low16, high16);\n            \n            low16 = _mm_and_si128(small, _mm_set1_epi32(0xFFFF));\n            high16 = _mm_srli_epi32(small, 16);\n            small = _mm_add_epi32(low16, high16);\n            \n            __m128i magic = _mm_set1_epi32(0x5556);\n            __m128i product = _mm_mullo_epi32(small, magic);\n            __m128i q = _mm_srli_epi32(product, 16);\n            __m128i q_times_3 = _mm_mullo_epi32(q, _mm_set1_epi32(3));\n            __m128i rem = _mm_sub_epi32(small, q_times_3);\n            \n            __m128i mask = _mm_cmpeq_epi32(rem, zero);\n            __m128i result = _mm_or_si128(_mm_and_si128(mask, abs_vec), _mm_andnot_si128(mask, n_vec));\n            _mm_storeu_si128((__m128i*)(b_row + j), result);\n        }\n        for (; j < cols; j++) {\n            int val = a_row[j];\n            int abs_val = (val < 0) ? -val : val;\n            if (abs_val % 3 == 0) {\n                b_row[j] = abs_val;\n            } else {\n                b_row[j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    uint8_t r = rotate_bits & 0x1F; // Reduce rotation to 0-31 bits\n    __m128i v_mask = _mm_set1_epi32(0xF);\n    __m128i v_ones = _mm_set1_epi32(-1);\n    __m128i v_rot_const = _mm_set1_epi32(32 - r);\n    size_t i = 0;\n    \n    for (; i + 3 < length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i low4 = _mm_and_si128(v, v_mask);\n        __m128i mask = _mm_cmpeq_epi32(low4, v_mask);\n        __m128i not_v = _mm_xor_si128(v, v_ones);\n        __m128i right = _mm_srli_epi32(v, r);\n        __m128i left = _mm_slli_epi32(v, 32 - r);\n        __m128i rotated = _mm_or_si128(right, left);\n        __m128i part1 = _mm_and_si128(mask, rotated);\n        __m128i part2 = _mm_andnot_si128(mask, not_v);\n        __m128i result = _mm_or_si128(part1, part2);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> r) | (val << (32 - r));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    uint32_t r = rotate_bits & 31;\n    size_t i = 0;\n    __m128i maskF = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n    \n    switch (r) {\n        case 0:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i rotated = src_vec;\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 1:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 1);\n                __m128i part2 = _mm_slli_epi32(src_vec, 31);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 2:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 2);\n                __m128i part2 = _mm_slli_epi32(src_vec, 30);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 3:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 3);\n                __m128i part2 = _mm_slli_epi32(src_vec, 29);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 4:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 4);\n                __m128i part2 = _mm_slli_epi32(src_vec, 28);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 5:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 5);\n                __m128i part2 = _mm_slli_epi32(src_vec, 27);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 6:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 6);\n                __m128i part2 = _mm_slli_epi32(src_vec, 26);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 7:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 7);\n                __m128i part2 = _mm_slli_epi32(src_vec, 25);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 8:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 8);\n                __m128i part2 = _mm_slli_epi32(src_vec, 24);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 9:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 9);\n                __m128i part2 = _mm_slli_epi32(src_vec, 23);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 10:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 10);\n                __m128i part2 = _mm_slli_epi32(src_vec, 22);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 11:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 11);\n                __m128i part2 = _mm_slli_epi32(src_vec, 21);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 12:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 12);\n                __m128i part2 = _mm_slli_epi32(src_vec, 20);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 13:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 13);\n                __m128i part2 = _mm_slli_epi32(src_vec, 19);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 14:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 14);\n                __m128i part2 = _mm_slli_epi32(src_vec, 18);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 15:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 15);\n                __m128i part2 = _mm_slli_epi32(src_vec, 17);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 16:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 16);\n                __m128i part2 = _mm_slli_epi32(src_vec, 16);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 17:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 17);\n                __m128i part2 = _mm_slli_epi32(src_vec, 15);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 18:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 18);\n                __m128i part2 = _mm_slli_epi32(src_vec, 14);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 19:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 19);\n                __m128i part2 = _mm_slli_epi32(src_vec, 13);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 20:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 20);\n                __m128i part2 = _mm_slli_epi32(src_vec, 12);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 21:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 21);\n                __m128i part2 = _mm_slli_epi32(src_vec, 11);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 22:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 22);\n                __m128i part2 = _mm_slli_epi32(src_vec, 10);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 23:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 23);\n                __m128i part2 = _mm_slli_epi32(src_vec, 9);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 24:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 24);\n                __m128i part2 = _mm_slli_epi32(src_vec, 8);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 25:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 25);\n                __m128i part2 = _mm_slli_epi32(src_vec, 7);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 26:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 26);\n                __m128i part2 = _mm_slli_epi32(src_vec, 6);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 27:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 27);\n                __m128i part2 = _mm_slli_epi32(src_vec, 5);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128\u6781 result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 28:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _\u6781 and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 28);\n                __m128i part2 = _mm_slli_epi32(src_vec, 4);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 29:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 29);\n                __m128i part2 = _mm_slli_epi32(src_vec, 3);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 30:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 30);\n                __m128i part2 = _mm_slli_epi32(src_vec, 2);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n        case 31:\n            for (; i + 3 < length; i += 4) {\n                __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n                __m128i low4 = _mm_and_si128(src_vec, maskF);\n                __m128i cond_mask = _mm_cmpeq_epi32(low4, maskF);\n                __m128i part1 = _mm_srli_epi32(src_vec, 31);\n                __m128i part2 = _mm_slli_epi32(src_vec, 1);\n                __m128i rotated = _mm_or_si128(part1, part2);\n                __m128i negated = _mm_xor_si128(src_vec, all_ones);\n                __m128i result = _mm_or_si128(_mm_and_si128(cond_mask, rotated), _mm_andnot_si128(cond_mask, negated));\n                _mm_storeu_si128((__m128i*)(dst + i), result);\n            }\n            break;\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> r) | (val << (32 - r));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    if (length == 0) return;\n\n    uint32_t r_mod = rotate_bits & 31;\n    uint32_t left_shift = (32 - r_mod) & 31;\n    __m128i low4_mask = _mm_set1_epi32(0xF);\n    __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n    __m128i cond_value = _mm_set1_epi32(0xF);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i low4 = _mm_and_si128(v, low4_mask);\n        __m128i cond_mask = _mm_cmpeq_epi32(low4, cond_value);\n\n        uint32_t src_tmp[4];\n        uint32_t rot_tmp[4];\n        _mm_storeu_si128((__m128i*)src_tmp, v);\n        for (int j = 0; j < 4; j++) {\n            rot_tmp[j] = (src_tmp[j] >> r_mod) | (src_tmp[j] << left_shift);\n        }\n        __m128i rotated = _mm_loadu_si128((const __m128i*)rot_tmp);\n        __m128i negated = _mm_xor_si128(v, all_ones);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(cond_mask, rotated),\n            _mm_andnot_si128(cond_mask, negated)\n        );\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        if ((x & 0xF) == 0xF) {\n            dst[i] = (x >> r_mod) | (x << left_shift);\n        } else {\n            dst[i] = ~x;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    uint32_t r = rotate_bits & 31;\n    uint32_t l = (32 - r) & 31;\n    __m128i v_r = _mm_set1_epi32(r);\n    __m128i v_l = _mm_set1_epi32(l);\n    __m128i v_low4_mask = _mm_set1_epi32(0xF);\n    __m128i v_ones = _mm_set1_epi32(0xFFFFFFFF);\n    __m128i v_target = _mm_set1_epi32(0xF);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low4 = _mm_and_si128(v_src, v_low4_mask);\n        __m128i cond_mask = _mm_cmpeq_epi32(low4, v_target);\n        __m128i right_shift = _mm_srl_epi32(v_src, v_r);\n        __m128i left_shift = _mm_sll_epi32(v_src, v_l);\n        __m128i rotated = _mm_or_si128(right_shift, left_shift);\n        __m128i negated = _mm_xor_si128(v_src, v_ones);\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(cond_mask, rotated),\n            _mm_andnot_si128(cond_mask, negated)\n        );\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> r) | (val << l);\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    __m128i v_all_ones = _mm_set1_epi32(-1);\n    __m128i v_low4_mask = _mm_set1_epi32(0xF);\n    __m128i v_rotate_count = _mm_set1_epi32(rotate_bits);\n    __m128i v_shift_left_count = _mm_set1_epi32(32 - rotate_bits);\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i v_low4 = _mm_and_si128(v_src, v_low4_mask);\n        __m128i v_condition = _mm_cmpeq_epi32(v_low4, v_low4_mask);\n        \n        __m128i v_right = _mm_srl_epi32(v_src, v_rotate_count);\n        __m128i v_left = _mm_sll_epi32(v_src, v_shift_left_count);\n        __m128i v_rotated = _mm_or_si128(v_right, v_left);\n        \n        __m128i v_negated = _mm_xor_si128(v_src, v_all_ones);\n        \n        __m128i v_part1 = _mm_and_si128(v_condition, v_rotated);\n        __m128i v_part2 = _mm_andnot_si128(v_condition, v_negated);\n        __m128i v_result = _mm_or_si128(v_part1, v_part2);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n    \n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        if ((x & 0xF) == 0xF) {\n            dst[i] = (x >> rotate_bits) | (x << (32 - rotate_bits));\n        } else {\n            dst[i] = ~x;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_eps = fabsf(epsilon);\n    __m128 abs_eps_vec = _mm_set1_ps(abs_eps);\n    __m128 neg_abs_eps_vec = _mm_set1_ps(-abs_eps);\n    __m128i zero_vec = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 mask_positive = _mm_cmpgt_ps(diff, abs_eps_vec);\n        __m128 mask_negative = _mm_cmplt_ps(diff, neg_abs_eps_vec);\n\n        __m128i mask_neg_int = _mm_castps_si128(mask_negative);\n        __m128i mask_pos_int = _mm_castps_si128(mask_positive);\n        __m128i res_int = _mm_sub_epi32(mask_neg_int, mask_pos_int);\n\n        __m128i packed16 = _mm_packs_epi32(res_int, zero_vec);\n        __m128i packed8 = _mm_packs_epi16(packed16, zero_vec);\n        int result = _mm_cvtsi128_si32(packed8);\n        *(int32_t*)(out + i) = result;\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > abs_eps) {\n            out[i] = 1;\n        } else if (diff < -abs_eps) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float epsilon_abs = (epsilon < 0) ? -epsilon : epsilon;\n    __m128 veps = _mm_set1_ps(epsilon_abs);\n    __m128 vminus_eps = _mm_set1_ps(-epsilon_abs);\n    __m128 vones = _mm_set1_ps(1.0f);\n    __m128 vminus_ones = _mm_set1_ps(-1.0f);\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128 vA = _mm_loadu_ps(A + i);\n        __m128 vB = _mm_loadu_ps(B + i);\n        __m128 vdiff = _mm_sub_ps(vA, vB);\n        \n        __m128 mask1 = _mm_cmpgt_ps(vdiff, veps);\n        __m128 mask2 = _mm_cmplt_ps(vdiff, vminus_eps);\n        \n        __m128 vresult = _mm_or_ps(_mm_and_ps(mask1, vones), _mm_and_ps(mask2, vminus_ones));\n        __m128i vresult_int = _mm_cvtps_epi32(vresult);\n        \n        __m128i packed16 = _mm_packs_epi32(vresult_int, vresult_int);\n        __m128i packed8 = _mm_packs_epi16(packed16, packed16);\n        int32_t result_i32 = _mm_cvtsi128_si32(packed8);\n        \n        out[i] = (int8_t)(result_i32 & 0xFF);\n        out[i+1] = (int8_t)((result_i32 >> 8) & 0xFF);\n        out[i+2] = (int8_t)((result_i32 >> 16) & 0xFF);\n        out[i+3] = (int8_t)((result_i32 >> 24) & 0xFF);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon_abs) {\n            out[i] = 1;\n        } else if (diff < -epsilon_abs) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <cmath>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_eps = fabsf(epsilon);\n    __m128 vEps = _mm_set1_ps(abs_eps);\n    __m128 vNegEps = _mm_set1_ps(-abs_eps);\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m128 vA0 = _mm_loadu_ps(A + i);\n        __m128 vA1 = _mm_loadu_ps(A + i + 4);\n        __m128 vA2 = _mm_loadu_ps(A + i + 8);\n        __m128 vA3 = _mm_loadu_ps(A + i + 12);\n\n        __m128 vB0 = _mm_loadu_ps(B + i);\n        __m128 vB1 = _mm_loadu_ps(B + i + 4);\n        __m128 vB2 = _mm_loadu_ps(B + i + 8);\n        __m128 vB3 = _mm_loadu_ps(B + i + 12);\n\n        __m128 vDiff0 = _mm_sub_ps(vA0, vB0);\n        __m128 vDiff1 = _mm_sub_ps(vA1, vB1);\n        __m128 vDiff2 = _mm_sub_ps(vA2, vB2);\n        __m128 vDiff3 = _mm_sub_ps(vA3, vB3);\n\n        __m128 mask_gt0 = _mm_cmpgt_ps(vDiff0, vEps);\n        __m128 mask_lt0 = _mm_cmplt_ps(vDiff0, vNegEps);\n        __m128 mask_gt1 = _mm_cmpgt_ps(vDiff1, vEps);\n        __m128 mask_lt1 = _mm_cmplt_ps(vDiff1, vNegEps);\n        __m128 mask_gt2 = _mm_cmpgt_ps(vDiff2, vEps);\n        __m128 mask_lt2 = _mm_cmplt_ps(vDiff2, vNegEps);\n        __m128 mask_gt3 = _mm_cmpgt_ps(vDiff3, vEps);\n        __m128 mask_lt3 = _mm_cmplt_ps(vDiff3, vNegEps);\n\n        __m128i res0 = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_gt0), _mm_set1_epi32(1)),\n            _mm_and_si128(_mm_castps_si128(mask_lt0), _mm_set1_epi32(-1))\n        );\n        __m128i res1 = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_gt1), _mm_set1_epi32(1)),\n            _mm_and_si128(_mm_castps_si128(mask_lt1), _mm_set1_epi32(-1))\n        );\n        __m128i res2 = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_gt2), _mm_set1_epi32(1)),\n            _mm_and_si128(_mm_castps_si128(mask_lt2), _mm_set1_epi32(-1))\n        );\n        __m128i res3 = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_gt3), _mm_set1_epi32(1)),\n            _mm_and_si128(_mm_castps_si128(mask_lt3), _mm_set1_epi32(-1))\n        );\n\n        __m128i res01 = _mm_packs_epi32(res0, res1);\n        __m128i res23 = _mm_packs_epi32(res2, res3);\n        __m128i res_bytes = _mm_packs_epi16(res01, res23);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(out + i), res_bytes);\n    }\n\n    for (; i + 3 < length; i += 4) {\n        __m128 vA = _mm_loadu_ps(A + i);\n        __m128 vB = _mm_loadu_ps(B + i);\n        __m128 vDiff = _mm_sub_ps(vA, vB);\n\n        __m128 mask_gt = _mm_cmpgt_ps(vDiff, vEps);\n        __m128 mask_lt = _mm_cmplt_ps(vDiff, vNegEps);\n\n        __m128i res_positive = _mm_and_si128(_mm_castps_si128(mask_gt), _mm_set1_epi32(1));\n        __m128i res_negative = _mm_and_si128(_mm_castps_si128(mask_lt), _mm_set1_epi32(-1));\n        __m128i res = _mm_or_si128(res_positive, res_negative);\n\n        alignas(16) int32_t temp[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), res);\n\n        out[i] = static_cast<int8_t>(temp[0]);\n        out[i+1] = static_cast<int8_t>(temp[1]);\n        out[i+2] = static_cast<int8_t>(temp[2]);\n        out[i+3] = static_cast<int8_t>(temp[3]);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > abs_eps) {\n            out[i] = 1;\n        } else if (diff < -abs_eps) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float eps_abs = fabsf(epsilon);\n    size_t i = 0;\n    __m128 eps_vec = _mm_set1_ps(eps_abs);\n    __m128 neg_eps_vec = _mm_set1_ps(-eps_abs);\n    \n    for (; i + 3 < length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        \n        __m128 mask_pos = _mm_cmpgt_ps(diff, eps_vec);\n        __m128 mask_neg = _mm_cmplt_ps(diff, neg_eps_vec);\n        \n        __m128i mask_pos_i = _mm_castps_si128(mask_pos);\n        __m128i mask_neg_i = _mm_castps_si128(mask_neg);\n        \n        __m128i one_vec = _mm_set1_epi32(1);\n        __m128i minus_one_vec = _mm_set1_epi32(-1);\n        \n        __m128i res_int = _mm_or_si128(\n            _mm_and_si128(mask_pos_i, one_vec),\n            _mm_and_si128(mask_neg_i, minus_one_vec)\n        );\n        \n        __m128i res_short = _mm_packs_epi32(res_int, _mm_setzero_si128());\n        __m128i res_byte = _mm_packs_epi16(res_short, _mm_setzero_si128());\n        \n        int32_t packed = _mm_cvtsi128_si32(res_byte);\n        __builtin_memcpy(out + i, &packed, 4);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > eps_abs) {\n            out[i] = 1;\n        } else if (diff < -eps_abs) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    float abs_epsilon = fabsf(epsilon);\n    size_t i = 0;\n    const size_t aligned_length = length & ~3;\n    \n    __m128 v_abs_eps = _mm_set1_ps(abs_epsilon);\n    __m128 v_neg_abs_eps = _mm_set1_ps(-abs_epsilon);\n    __m128i v_ones = _mm_set1_epi32(1);\n    \n    for (; i < aligned_length; i += 4) {\n        __m128 a_val = _mm_loadu_ps(A + i);\n        __m128 b_val = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a_val, b_val);\n        \n        __m128 mask_gt = _mm_cmpgt_ps(diff, v_abs_eps);\n        __m128 mask_lt = _mm_cmplt_ps(diff, v_neg_abs_eps);\n        \n        __m128i v_gt = _mm_and_si128(_mm_castps_si128(mask_gt), v_ones);\n        __m128i v_lt = _mm_and_si128(_mm_castps_si128(mask_lt), v_ones);\n        __m128i res32 = _mm_sub_epi32(v_gt, v_lt);\n        \n        __m128i res16 = _mm_packs_epi32(res32, _mm_setzero_si128());\n        __m128i res8 = _mm_packs_epi16(res16, _mm_setzero_si128());\n        \n        *((int32_t*)(out + i)) = _mm_cvtsi128_si32(res8);\n    }\n    \n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > abs_epsilon) {\n            out[i] = 1;\n        } else if (diff < -abs_epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 upper_bound = _mm_set1_ps(2147483648.0f);\n    const __m128 lower_bound = _mm_set1_ps(-2147483648.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_val = _mm_loadu_ps(A + i);\n        __m128 overflow_mask = _mm_cmpge_ps(a_val, upper_bound);\n        __m128 underflow_mask = _mm_cmplt_ps(a_val, lower_bound);\n        __m128 in_range_mask = _mm_andnot_ps(_mm_or_ps(overflow_mask, underflow_mask), \n                                            _mm_castsi128_ps(_mm_set1_epi32(-1)));\n        __m128i int_val = _mm_cvttps_epi32(a_val);\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(overflow_mask), int_max),\n            _mm_and_si128(_mm_castps_si128(underflow_mask), int_min)\n        );\n        result = _mm_or_si128(result, \n                             _mm_and_si128(_mm_castps_si128(in_range_mask), int_val));\n        _mm_storeu_si128((__m128i*)(B + i), result);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val >= 2147483648.0f) {\n            B[i] = INT32_MAX;\n        } else if (val < -2147483648.0f) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m128 int32_max = _mm_set1_ps(2147483647.0f);\n    const __m128 int32_min = _mm_set1_ps(-2147483648.0f);\n    const __m128 zero = _mm_setzero_ps();\n    const __m128i int_min_vec = _mm_set1_epi32(0x80000000);\n    const __m128i int_max_val = _mm_set1_epi32(INT32_MAX);\n\n    for (; i + 3 < size; i += 4) {\n        __m128 v = _mm_loadu_ps(&A[i]);\n        __m128 clamped = _mm_min_ps(v, int32_max);\n        clamped = _mm_max_ps(clamped, int32_min);\n        __m128i res = _mm_cvtps_epi32(clamped);\n        \n        __m128 positive = _mm_cmpgt_ps(v, zero);\n        __m128i is_min = _mm_cmpeq_epi32(res, int_min_vec);\n        __m128 fix_mask = _mm_and_ps(positive, _mm_castsi128_ps(is_min));\n        \n        res = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(fix_mask), int_max_val),\n            _mm_andnot_si128(_mm_castps_si128(fix_mask), res)\n        );\n        \n        _mm_storeu_si128((__m128i*)&B[i], res);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m128 max_bound = _mm_set1_ps(2147483648.0f);\n    const __m128 min_bound = _mm_set1_ps(-2147483648.0f);\n    const __m128i int32_max = _mm_set1_epi32(2147483647);\n\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        a = _mm_min_ps(a, max_bound);\n        a = _mm_max_ps(a, min_bound);\n        __m128i b = _mm_cvttps_epi32(a);\n        __m128 mask = _mm_cmpeq_ps(a, max_bound);\n        __m128i fixed_b = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask), int32_max),\n            _mm_andnot_si128(_mm_castps_si128(mask), b)\n        );\n        _mm_storeu_si128((__m128i*)&B[i], fixed_b);\n    }\n\n    for (; i < size; ++i) {\n        float a_val = A[i];\n        if (a_val >= 2147483648.0f) {\n            B[i] = 2147483647;\n        } else if (a_val < -2147483648.0f) {\n            B[i] = -2147483647 - 1;\n        } else {\n            B[i] = (int32_t)a_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const float float_max_threshold = 2147483648.0f;\n    const float float_min_threshold = (float)INT32_MIN;\n    const __m128 v_max_threshold = _mm_set1_ps(float_max_threshold);\n    const __m128 v_min_threshold = _mm_set1_ps(float_min_threshold);\n    const __m128i v_int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i v_int32_min = _mm_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 x = _mm_loadu_ps(A + i);\n        __m128i converted = _mm_cvtps_epi32(x);\n        __m128 mask_overflow = _mm_cmpge_ps(x, v_max_threshold);\n        __m128 mask_underflow = _mm_cmplt_ps(x, v_min_threshold);\n        \n        converted = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_underflow), v_int32_min),\n            _mm_andnot_si128(_mm_castps_si128(mask_underflow), converted)\n        );\n        \n        converted = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_overflow), v_int32_max),\n            _mm_andnot_si128(_mm_castps_si128(mask_overflow), converted)\n        );\n        \n        _mm_storeu_si128((__m128i*)(B + i), converted);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val >= float_max_threshold) {\n            B[i] = INT32_MAX;\n        } else if (val < float_min_threshold) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = _mm_cvtss_si32(_mm_set_ss(val));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 zero = _mm_setzero_ps();\n    const __m128 min_bound = _mm_set1_ps((float)INT32_MIN);\n    const __m128 max_bound = _mm_set1_ps((float)INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        a = _mm_max_ps(a, min_bound);\n        a = _mm_min_ps(a, max_bound);\n        \n        __m128i b = _mm_cvtps_epi32(a);\n        \n        __m128 mask_overflow = _mm_and_ps(\n            _mm_cmpgt_ps(a, zero),\n            _mm_cmpeq_ps(a, max_bound)\n        );\n        __m128i mask_overflow_int = _mm_castps_si128(mask_overflow);\n        b = _mm_or_si128(\n            _mm_and_si128(mask_overflow_int, int_max),\n            _mm_andnot_si128(mask_overflow_int, b)\n        );\n        \n        _mm_storeu_si128((__m128i*)&B[i], b);\n    }\n    \n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_factor = static_cast<float>(scale);\n    __m128 scale_vec = _mm_set1_ps(scale_factor);\n    \n    size_t i = 0;\n    const size_t simd_iters = size / 4;\n    \n    for (; i < simd_iters * 4; i += 4) {\n        __m128i int_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);\n        float_vec = _mm_mul_ps(float_vec, scale_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale_factor;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale);\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec);\n        _mm_storeu_ps(B + i, scaled_vec);\n    }\n    for (; i < size; i++) {\n        B[i] = (float)(A[i]) * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps(static_cast<float>(scale));\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128 b_vec = _mm_cvtepi32_ps(a_vec);\n        b_vec = _mm_mul_ps(b_vec, scale_vec);\n        _mm_storeu_ps(B + i, b_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * static_cast<float>(scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_factor = (float)scale;\n    __m128 scale_vec = _mm_set1_ps(scale_factor);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);\n        __m128 result = _mm_mul_ps(float_vec, scale_vec);\n        _mm_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; i++) {\n        B[i] = (float)(A[i]) * scale_factor;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    float scale_factor = (float)scale;\n    __m128 scale_vec = _mm_set1_ps(scale_factor);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec);\n        _mm_storeu_ps(&B[i], scaled_vec);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = (float)A[i] * scale_factor;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)(C + i), rounded);\n    }\n    for (; i < size; i++) {\n        __m128 a_val = _mm_load_ss(A + i);\n        __m128 b_val = _mm_load_ss(B + i);\n        __m128 div_val = _mm_div_ss(a_val, b_val);\n        int32_t result = _mm_cvtss_si32(div_val);\n        C[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i res = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < size; i++) {\n        __m128 a_val = _mm_load_ss(A + i);\n        __m128 b_val = _mm_load_ss(B + i);\n        __m128 div_val = _mm_div_ss(a_val, b_val);\n        int32_t r = _mm_cvtss_si32(div_val);\n        C[i] = r;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i res = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)(C + i), res);\n    }\n    for (; i < size; i++) {\n        __m128 a = _mm_load_ss(A + i);\n        __m128 b = _mm_load_ss(B + i);\n        __m128 div = _mm_div_ss(a, b);\n        C[i] = _mm_cvtss_si32(div);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n    \n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t rounded_size = size - (size % 4);\n\n    for (; i < rounded_size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i res = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], res);\n    }\n\n    for (; i < size; i++) {\n        C[i] = _mm_cvtss_si32(_mm_set_ss(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_d = _mm_set1_pd(-FLT_MAX);\n    \n    size_t i = 0;\n    for (; i + 1 < size; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n        __m128d clamped = _mm_min_pd(a, max_d);\n        clamped = _mm_max_pd(clamped, min_d);\n        __m128 flt = _mm_cvtpd_ps(clamped);\n        __m128i flt_i = _mm_castps_si128(flt);\n        _mm_storel_epi64((__m128i*)&B[i], flt_i);\n    }\n    \n    if (i < size) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d flt_max_d = _mm_set1_pd((double)FLT_MAX);\n    const __m128d neg_flt_max_d = _mm_set1_pd((double)-FLT_MAX);\n    size_t i = 0;\n\n    for (; i < size - (size % 2); i += 2) {\n        __m128d a_val = _mm_loadu_pd(A + i);\n        a_val = _mm_min_pd(a_val, flt_max_d);\n        a_val = _mm_max_pd(a_val, neg_flt_max_d);\n        __m128 b_val = _mm_cvtpd_ps(a_val);\n        _mm_store_ss(B + i, b_val);\n        __m128 b_second = _mm_shuffle_ps(b_val, b_val, _MM_SHUFFLE(1, 1, 1, 1));\n        _mm_store_ss(B + i + 1, b_second);\n    }\n\n    if (i < size) {\n        double a_val = A[i];\n        float b_val;\n        if (a_val > FLT_MAX) b_val = FLT_MAX;\n        else if (a_val < -FLT_MAX) b_val = -FLT_MAX;\n        else b_val = (float)a_val;\n        B[i] = b_val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    if (size == 0) return;\n\n    const __m128d max_double = _mm_set1_pd(FLT_MAX);\n    const __m128d min_double = _mm_set1_pd(-FLT_MAX);\n    size_t i = 0;\n\n    for (; i + 1 < size; i += 2) {\n        __m128d d = _mm_loadu_pd(A + i);\n        d = _mm_max_pd(d, min_double);\n        d = _mm_min_pd(d, max_double);\n        __m128 f = _mm_cvtpd_ps(d);\n        _mm_storeu_si64((void*)(B + i), _mm_castps_si128(f));\n    }\n\n    if (i < size) {\n        double a = A[i];\n        if (a > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (a < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)a;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <cfloat>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_vec = _mm_set1_pd(FLT_MAX);\n    const __m128d min_vec = _mm_set1_pd(-FLT_MAX);\n    size_t i = 0;\n    for (; i + 1 < size; i += 2) {\n        __m128d d_val = _mm_loadu_pd(A + i);\n        d_val = _mm_max_pd(d_val, min_vec);\n        d_val = _mm_min_pd(d_val, max_vec);\n        __m128 f_val = _mm_cvtpd_ps(d_val);\n        __m128i f_i = _mm_castps_si128(f_val);\n        _mm_storel_epi64((__m128i*)(B + i), f_i);\n    }\n    if (i < size) {\n        double a_val = A[i];\n        float b_val;\n        if (a_val > FLT_MAX) {\n            b_val = FLT_MAX;\n        } else if (a_val < -FLT_MAX) {\n            b_val = -FLT_MAX;\n        } else {\n            b_val = (float)a_val;\n        }\n        B[i] = b_val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <cfloat>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    if (size == 0) {\n        return;\n    }\n\n    const __m128d max_val = _mm_set1_pd(FLT_MAX);\n    const __m128d min_val = _mm_set1_pd(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i < size - 1; i += 2) {\n        __m128d a = _mm_loadu_pd(A + i);\n        __m128d mask_high = _mm_cmpgt_pd(a, max_val);\n        __m128d mask_low = _mm_cmplt_pd(a, min_val);\n\n        __m128d clamped_high = _mm_or_pd(_mm_and_pd(mask_high, max_val), _mm_andnot_pd(mask_high, a));\n        __m128d clamped = _mm_or_pd(_mm_and_pd(mask_low, min_val), _mm_andnot_pd(mask_low, clamped_high));\n\n        __m128 f = _mm_cvtpd_ps(clamped);\n        _mm_storeu_sd((double*)(B + i), _mm_castps_pd(f));\n    }\n\n    if (i < size) {\n        double a_val = A[i];\n        if (a_val > (double)FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (a_val < (double)-FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)a_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const size_t num_simd_elements = 8;\n    \n    for (; i + num_simd_elements <= size; i += num_simd_elements) {\n        __m128i vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i low = _mm_unpacklo_epi16(vec, vec);\n        __m128i high = _mm_unpackhi_epi16(vec, vec);\n        low = _mm_srai_epi32(low, 16);\n        high = _mm_srai_epi32(high, 16);\n        _mm_storeu_si128((__m128i*)(B + i), low);\n        _mm_storeu_si128((__m128i*)(B + i + 4), high);\n    }\n    \n    for (; i < size; i++) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 7 < size; i += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i sign_mask = _mm_cmpgt_epi16(zero, vec);\n        __m128i low = _mm_unpacklo_epi16(vec, sign_mask);\n        __m128i high = _mm_unpackhi_epi16(vec, sign_mask);\n        _mm_storeu_si128((__m128i*)(B + i), low);\n        _mm_storeu_si128((__m128i*)(B + i + 4), high);\n    }\n    for (; i < size; i++) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i sign = _mm_srai_epi16(v, 15);\n        __m128i low = _mm_unpacklo_epi16(v, sign);\n        __m128i high = _mm_unpackhi_epi16(v, sign);\n        _mm_storeu_si128((__m128i*)(B + i), low);\n        _mm_storeu_si128((__m128i*)(B + i + 4), high);\n    }\n    for (; i < size; i++) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i low = _mm_unpacklo_epi16(v, v);\n        __m128i high = _mm_unpackhi_epi16(v, v);\n        low = _mm_srai_epi32(low, 16);\n        high = _mm_srai_epi32(high, 16);\n        _mm_storeu_si128((__m128i*)(B + i), low);\n        _mm_storeu_si128((__m128i*)(B + i + 4), high);\n    }\n    for (; i < size; i++) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i vec16 = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i low = _mm_unpacklo_epi16(vec16, vec16);\n        __m128i high = _mm_unpackhi_epi16(vec16, vec16);\n        __m128i low32 = _mm_srai_epi32(low, 16);\n        __m128i high32 = _mm_srai_epi32(high, 16);\n        _mm_storeu_si128((__m128i*)(B + i), low32);\n        _mm_storeu_si128((__m128i*)(B + i + 4), high32);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float denom = max_val - min_val;\n    float scale = (denom != 0.0f) ? 1.0f / denom : 0.0f;\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 scale_vec = _mm_set1_ps(scale);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128i zero_int = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(A + i);\n        __m128i ctrl_vec = _mm_loadu_si128((__m128i const*)(control + i));\n        __m128i mask_control = _mm_cmpgt_epi32(ctrl_vec, zero_int);\n        __m128 mask_float = _mm_castsi128_ps(mask_control);\n\n        __m128 norm_val = _mm_sub_ps(a_vec, min_vec);\n        norm_val = _mm_mul_ps(norm_val, scale_vec);\n        norm_val = _mm_max_ps(norm_val, zero_vec);\n        norm_val = _mm_min_ps(norm_val, one_vec);\n\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(mask_float, norm_val),\n            _mm_andnot_ps(mask_float, a_vec)\n        );\n        _mm_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) * scale;\n            if (val < 0.0f) val = 0.0f;\n            else if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 denom_vec = _mm_set1_ps(max_val - min_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128i ctrl_vec = _mm_loadu_si128((const __m128i*)&control[i]);\n        __m128i mask_int = _mm_cmpgt_epi32(ctrl_vec, _mm_setzero_si128());\n        __m128 mask_float = _mm_castsi128_ps(mask_int);\n        \n        __m128 normalized = _mm_sub_ps(a_vec, min_vec);\n        normalized = _mm_div_ps(normalized, denom_vec);\n        normalized = _mm_max_ps(normalized, zero_vec);\n        normalized = _mm_min_ps(normalized, one_vec);\n        \n        __m128 result = _mm_blendv_ps(a_vec, normalized, mask_float);\n        _mm_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : (val > 1.0f) ? 1.0f : val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float denom_val = max_val - min_val;\n    if (denom_val == 0.0f) {\n        denom_val = 1.0f;\n    }\n    __m128 v_denom = _mm_set1_ps(denom_val);\n    __m128 v_min = _mm_set1_ps(min_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_val = _mm_loadu_ps(A + i);\n        __m128i c_val = _mm_loadu_si128((const __m128i*)(control + i));\n        __m128i mask_int = _mm_cmpgt_epi32(c_val, _mm_setzero_si128());\n        __m128 mask_float = _mm_castsi128_ps(mask_int);\n        \n        __m128 normalized = _mm_sub_ps(a_val, v_min);\n        normalized = _mm_div_ps(normalized, v_denom);\n        normalized = _mm_max_ps(normalized, v_zero);\n        normalized = _mm_min_ps(normalized, v_one);\n        \n        __m128 result = _mm_blendv_ps(a_val, normalized, mask_float);\n        _mm_storeu_ps(B + i, result);\n    }\n    \n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / denom_val;\n            if (val < 0.0f) val = 0.0f;\n            else if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float denom = max_val - min_val;\n    if (denom == 0.0f) {\n        denom = 1.0f;\n    }\n    \n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 denom_vec = _mm_set1_ps(denom);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    \n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_val = _mm_loadu_ps(&A[i]);\n        __m128i ctrl = _mm_loadu_si128((const __m128i*)&control[i]);\n        __m128i mask = _mm_cmpgt_epi32(ctrl, _mm_setzero_si128());\n        __m128 mask_float = _mm_castsi128_ps(mask);\n        \n        __m128 norm = _mm_sub_ps(a_val, min_vec);\n        norm = _mm_div_ps(norm, denom_vec);\n        norm = _mm_max_ps(norm, zero);\n        norm = _mm_min_ps(norm, one);\n        \n        __m128 result = _mm_blendv_ps(a_val, norm, mask_float);\n        _mm_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / denom;\n            if (val < 0.0f) val = 0.0f;\n            else if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    float range = max_val - min_val;\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128i zero_int = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 a_val = _mm_loadu_ps(A + i);\n        __m128i ctrl_val = _mm_loadu_si128((const __m128i*)(control + i));\n        __m128i mask_int = _mm_cmpgt_epi32(ctrl_val, zero_int);\n        __m128 mask = _mm_castsi128_ps(mask_int);\n\n        __m128 normalized = _mm_sub_ps(a_val, min_vec);\n        normalized = _mm_div_ps(normalized, range_vec);\n        normalized = _mm_max_ps(normalized, zero_vec);\n        normalized = _mm_min_ps(normalized, one_vec);\n\n        __m128 not_mask = _mm_andnot_ps(mask, a_val);\n        __m128 yes_mask = _mm_and_ps(mask, normalized);\n        __m128 result = _mm_or_ps(not_mask, yes_mask);\n\n        _mm_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / range;\n            if (val < 0.0f) val = 0.0f;\n            else if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t aligned_size = size & ~3;\n    \n    for (; i < aligned_size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i shifted = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(&B[i], sqrt_vec);\n    }\n    \n    for (; i < size; ++i) {\n        uint16_t high_bits = (uint16_t)(A[i] >> 16);\n        B[i] = _mm_cvtss_f32(_mm_sqrt_ss(_mm_set_ss((float)high_bits)));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t aligned_size = size - (size % 4);\n    \n    for (; i < aligned_size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i high16 = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(high16);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = _mm_cvtss_f32(_mm_sqrt_ss(_mm_set_ss((float)high16)));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    size_t end = size / simd_elements * simd_elements;\n\n    for (; i < end; i += simd_elements) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i shifted = _mm_srli_epi32(int_vec, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = _mm_cvtss_f32(_mm_sqrt_ss(_mm_set_ss((float)high16)));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t numBlocks = size / 4;\n    \n    for (i = 0; i < numBlocks * 4; i += 4) {\n        __m128i intVec = _mm_loadu_si128((const __m128i*)(A + i));\n        intVec = _mm_srli_epi32(intVec, 16);\n        __m128 floatVec = _mm_cvtepi32_ps(intVec);\n        __m128 sqrtVec = _mm_sqrt_ps(floatVec);\n        _mm_storeu_ps(B + i, sqrtVec);\n    }\n    \n    for (; i < size; ++i) {\n        uint32_t highBits = A[i] >> 16;\n        float val = (float)highBits;\n        B[i] = sqrtf(val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i shifted = _mm_srli_epi32(a, 16);\n        __m128 float_vec = _mm_cvtepi32_ps(shifted);\n        __m128 sqrt_vec = _mm_sqrt_ps(float_vec);\n        _mm_storeu_ps(B + i, sqrt_vec);\n    }\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        B[i] = sqrtf((float)high16);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    size_t simd_iters = size / simd_elements;\n\n    for (; i < simd_iters; ++i) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i * simd_elements));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i * simd_elements, float_vec);\n    }\n\n    for (i = simd_iters * simd_elements; i < size; ++i) {\n        uint32_t int_val = A[i];\n        float float_val;\n        memcpy(&float_val, &int_val, sizeof(float));\n        B[i] = float_val;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t num_sse_elements = 4;\n    size_t sse_iters = size / num_sse_elements;\n\n    for (; i < sse_iters * num_sse_elements; i += num_sse_elements) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        *((uint32_t*)&B[i]) = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elements = 4;\n    const size_t simd_limit = size - (size % simd_elements);\n\n    for (; i < simd_limit; i += simd_elements) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        union {\n            uint32_t u;\n            float f;\n        } converter;\n        converter.u = A[i];\n        B[i] = converter.f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    // Process 4 elements at a time using SSE\n    for (; i + 3 < size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float f;\n        memcpy(&f, &A[i], sizeof(float));\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_elems = 4;\n    const size_t simd_stop = size - (size % simd_elems);\n    \n    for (; i < simd_stop; i += simd_elems) {\n        __m128i int_vec = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        _mm_storeu_ps(B + i, float_vec);\n    }\n    \n    for (; i < size; ++i) {\n        *(B + i) = *(const float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i <= size - 4; i += 4) {\n        __m128i current_A = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i prev_A = _mm_loadu_si128((__m128i*)(A + i - 1));\n        __m128i diff_i32 = _mm_sub_epi32(current_A, prev_A);\n\n        uint8_t cond_bytes[4];\n        cond_bytes[0] = cond[i];\n        cond_bytes[1] = cond[i+1];\n        cond_bytes[2] = cond[i+2];\n        cond_bytes[3] = cond[i+3];\n        __m128i cond_elems = _mm_set_epi32(cond_bytes[3], cond_bytes[2], cond_bytes[1], cond_bytes[0]);\n        __m128i mask = _mm_cmpeq_epi32(cond_elems, _mm_set1_epi32(1));\n        __m128i masked_diff = _mm_and_si128(diff_i32, mask);\n        __m128 diff_float = _mm_cvtepi32_ps(masked_diff);\n        _mm_storeu_ps(diff + i, diff_float);\n    }\n\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    size_t i = 1;\n    for (; i <= size - 4; i += 4) {\n        __m128i current = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i previous = _mm_loadu_si128((const __m128i*)(A + i - 1));\n        __m128i diff_i = _mm_sub_epi32(current, previous);\n        __m128 diff_f = _mm_cvtepi32_ps(diff_i);\n\n        int cond_block;\n        memcpy(&cond_block, cond + i, sizeof(int));\n        float mask0 = (cond_block & 0xFF) ? 1.0f : 0.0f;\n        float mask1 = ((cond_block >> 8) & 0xFF) ? 1.0f : 0.0f;\n        float mask2 = ((cond_block >> 16) & 0xFF) ? 1.0f : 0.0f;\n        float mask3 = ((cond_block >> 24) & 0xFF) ? 1.0f : 0.0f;\n        __m128 mask = _mm_set_ps(mask3, mask2, mask1, mask0);\n\n        diff_f = _mm_mul_ps(diff_f, mask);\n        _mm_storeu_ps(diff + i, diff_f);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) {\n        return;\n    }\n    diff[0] = 0.0f;\n    if (size <= 1) {\n        return;\n    }\n    \n    size_t i = 1;\n    if (size >= 5) {\n        for (; i <= size - 4; i += 4) {\n            __m128i cond_block = _mm_cvtsi32_si128(*(const int32_t*)(cond + i));\n            __m128i zero = _mm_setzero_si128();\n            __m128i cond_16 = _mm_unpacklo_epi8(cond_block, zero);\n            __m128i cond_32 = _mm_unpacklo_epi16(cond_16, zero);\n            __m128i cond_mask = _mm_cmpgt_epi32(cond_32, zero);\n            \n            __m128i current = _mm_loadu_si128((const __m128i*)(A + i));\n            __m128i previous = _mm_loadu_si128((const __m128i*)(A + i - 1));\n            __m128i diff_int = _mm_sub_epi32(current, previous);\n            __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n            \n            __m128 mask = _mm_castsi128_ps(cond_mask);\n            __m128 zeros = _mm_setzero_ps();\n            __m128 result = _mm_or_ps(_mm_and_ps(mask, diff_float), \n                                     _mm_andnot_ps(mask, zeros));\n            _mm_storeu_ps(diff + i, result);\n        }\n    }\n    \n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) {\n        return;\n    }\n    diff[0] = 0.0f;\n    if (size == 1) {\n        return;\n    }\n    \n    size_t i = 1;\n    for (; i <= size - 4; i += 4) {\n        int cond_data;\n        memcpy(&cond_data, cond + i, 4);\n        __m128i cond_vec = _mm_cvtsi32_si128(cond_data);\n        cond_vec = _mm_unpacklo_epi8(cond_vec, _mm_setzero_si128());\n        cond_vec = _mm_unpacklo_epi16(cond_vec, _mm_setzero_si128());\n        __m128i mask_int = _mm_cmpgt_epi32(cond_vec, _mm_setzero_si128());\n        __m128 mask_float = _mm_castsi128_ps(mask_int);\n        \n        __m128i current_A = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i prev_A = _mm_loadu_si128((const __m128i*)(A + i - 1));\n        __m128i diff_int = _mm_sub_epi32(current_A, prev_A);\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n        \n        __m128 res = _mm_and_ps(diff_float, mask_float);\n        _mm_storeu_ps(diff + i, res);\n    }\n    \n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i-1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    \n    diff[0] = 0.0f;\n    if (size <= 1) return;\n    \n    size_t i = 1;\n    for (; i <= size - 4; i += 4) {\n        __m128i current = _mm_loadu_si128((const __m128i*)(A + i));\n        __m128i previous = _mm_loadu_si128((const __m128i*)(A + i - 1));\n        __m128i diff_i = _mm_sub_epi32(current, previous);\n        __m128 diff_f = _mm_cvtepi32_ps(diff_i);\n        \n        uint32_t cond_bits;\n        __builtin_memcpy(&cond_bits, cond + i, sizeof(uint32_t));\n        \n        __m128i cond_vec = _mm_setr_epi32(\n            (int)(cond_bits & 0xFF),\n            (int)((cond_bits >> 8) & 0xFF),\n            (int)((cond_bits >> 16) & 0xFF),\n            (int)((cond_bits >> 24) & 0xFF)\n        );\n        __m128i mask_vec = _mm_cmpgt_epi32(cond_vec, _mm_setzero_si128());\n        __m128 mask_float = _mm_castsi128_ps(mask_vec);\n        \n        __m128 result = _mm_and_ps(diff_f, mask_float);\n        _mm_storeu_ps(diff + i, result);\n    }\n    \n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    if (length == 0) \n        return;\n\n    const __m128i mask = _mm_set1_epi64x(0xFFFFFFFF00000000ULL);\n    const __m128i zero = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i data = _mm_loadl_epi64((const __m128i*)(src + i));\n        __m128i unpacked = _mm_unpacklo_epi32(data, zero);\n        __m128i result = _mm_or_si128(unpacked, mask);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 4) {\n        const __m128i high_bits = _mm_set1_epi64x(0x100000000ULL);\n        for (; i <= length - 4; i += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n            __m128i low_part = _mm_unpacklo_epi32(vec, _mm_setzero_si128());\n            __m128i high_part = _mm_unpackhi_epi32(vec, _mm_setzero_si128());\n            low_part = _mm_or_si128(low_part, high_bits);\n            high_part = _mm_or_si128(high_part, high_bits);\n            _mm_storeu_si128((__m128i*)(dst + i), low_part);\n            _mm_storeu_si128((__m128i*)(dst + i + 2), high_part);\n        }\n    }\n    for (; i < length; i++) {\n        dst[i] = (1ULL << 32) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    if (length == 0) \n        return;\n\n    __m128i ones_high = _mm_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t j = 0;\n\n    for (; j + 3 < length; j += 4) {\n        __m128i x = _mm_loadu_si128((__m128i*)(src + j));\n        __m128i low = _mm_unpacklo_epi32(x, _mm_setzero_si128());\n        __m128i high = _mm_unpackhi_epi32(x, _mm_setzero_si128());\n        low = _mm_or_si128(low, ones_high);\n        high = _mm_or_si128(high, ones_high);\n        _mm_storeu_si128((__m128i*)(dst + j), low);\n        _mm_storeu_si128((__m128i*)(dst + j + 2), high);\n    }\n\n    for (; j < length; j++) {\n        dst[j] = (0xFFFFFFFF00000000ULL) | src[j];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    if (length == 0) {\n        return;\n    }\n\n    __m128i high_mask = _mm_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t i = 0;\n\n    for (; i + 1 < length; i += 2) {\n        __m128i src_vec = _mm_loadl_epi64((const __m128i*)(src + i));\n        __m128i zero = _mm_setzero_si128();\n        __m128i unpacked = _mm_unpacklo_epi32(src_vec, zero);\n        __m128i result = _mm_or_si128(unpacked, high_mask);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m128i mask = _mm_set1_epi64x(0xFFFFFFFF00000000ULL);\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i low = _mm_unpacklo_epi32(src_vec, _mm_setzero_si128());\n        __m128i high = _mm_unpackhi_epi32(src_vec, _mm_setzero_si128());\n        __m128i res_low = _mm_or_si128(low, mask);\n        __m128i res_high = _mm_or_si128(high, mask);\n        _mm_storeu_si128((__m128i*)(dst + i), res_low);\n        _mm_storeu_si128((__m128i*)(dst + i + 2), res_high);\n    }\n    for (; i < length; i++) {\n        dst[i] = (0xFFFFFFFF00000000ULL) | src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d acc0 = _mm_setzero_pd();\n    __m128d acc1 = _mm_setzero_pd();\n    __m128d acc2 = _mm_setzero_pd();\n    __m128d acc3 = _mm_setzero_pd();\n    \n    size_t i = 0;\n    const size_t unroll = 8;\n    for (; i + unroll <= length; i += unroll) {\n        __m128i idx = _mm_loadu_si128((const __m128i*)(index + i));\n        \n        double d0 = vec[_mm_extract_epi16(idx, 0)];\n        double d1 = vec[_mm_extract_epi16(idx, 1)];\n        double d2 = vec[_mm_extract_epi16(idx, 2)];\n        double d3 = vec[_mm_extract_epi16(idx, 3)];\n        double d4 = vec[_mm_extract_epi16(idx, 4)];\n        double d5 = vec[_mm_extract_epi16(idx, 5)];\n        double d6 = vec[_mm_extract_epi16(idx, 6)];\n        double d7 = vec[_mm_extract_epi16(idx, 7)];\n        \n        __m128d t0 = _mm_set_pd(d1, d0);\n        __m128d t1 = _mm_set_pd(d3, d2);\n        __m128d t2 = _mm_set_pd(d5, d4);\n        __m128d t3 = _mm_set_pd(d7, d6);\n        \n        acc0 = _mm_add_pd(acc0, t0);\n        acc1 = _mm_add_pd(acc1, t1);\n        acc2 = _mm_add_pd(acc2, t2);\n        acc3 = _mm_add_pd(acc3, t3);\n    }\n    \n    acc0 = _mm_add_pd(acc0, acc1);\n    acc2 = _mm_add_pd(acc2, acc3);\n    acc0 = _mm_add_pd(acc0, acc2);\n    \n    __m128d shuf = _mm_unpackhi_pd(acc0, acc0);\n    __m128d sum = _mm_add_sd(acc0, shuf);\n    double total = _mm_cvtsd_f64(sum);\n    \n    for (; i < length; i++) {\n        total += vec[index[i]];\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    if (length == 0) {\n        return 0.0;\n    }\n\n    double result = 0.0;\n    __m128d sum0 = _mm_setzero_pd();\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m128i idx_vec = _mm_loadu_si128((const __m128i*)(index + i));\n        int16_t idx[8];\n        _mm_storeu_si128((__m128i*)idx, idx_vec);\n\n        sum0 = _mm_add_pd(sum0, _mm_set_pd(vec[idx[1]], vec[idx[0]]));\n        sum1 = _mm_add_pd(sum1, _mm_set_pd(vec[idx[3]], vec[idx[2]]));\n        sum2 = _mm_add_pd(sum2, _mm_set_pd(vec[idx[5]], vec[idx[4]]));\n        sum3 = _mm_add_pd(sum3, _mm_set_pd(vec[idx[7]], vec[idx[6]]));\n    }\n\n    sum0 = _mm_add_pd(sum0, sum1);\n    sum2 = _mm_add_pd(sum2, sum3);\n    sum0 = _mm_add_pd(sum0, sum2);\n\n    double temp[2];\n    _mm_storeu_pd(temp, sum0);\n    result = temp[0] + temp[1];\n\n    for (; i < length; i++) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    if (length == 0) {\n        return 0.0;\n    }\n\n    double result = 0.0;\n    size_t i = 0;\n\n    if (length >= 8) {\n        __m128d accum = _mm_setzero_pd();\n\n        for (; i <= length - 8; i += 8) {\n            __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n            int16_t indices[8];\n            _mm_storeu_si128((__m128i*)indices, idx);\n\n            __m128d v0 = _mm_set_pd(vec[indices[1]], vec[indices[0]]);\n            __m128d v1 = _mm_set_pd(vec[indices[3]], vec[indices[2]]);\n            __m128d v2 = _mm_set_pd(vec[indices[5]], vec[indices[4]]);\n            __m128d v3 = _mm_set_pd(vec[indices[7]], vec[indices[6]]);\n\n            __m128d s0 = _mm_add_pd(v0, v1);\n            __m128d s1 = _mm_add_pd(v2, v3);\n            __m128d s2 = _mm_add_pd(s0, s1);\n            accum = _mm_add_pd(accum, s2);\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, accum);\n        result = temp[0] + temp[1];\n    }\n\n    for (; i < length; i++) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d global_accum = _mm_setzero_pd();\n    size_t i = 0;\n\n    if (length >= 8) {\n        for (; i <= length - 8; i += 8) {\n            __m128i idx_reg = _mm_loadu_si128((__m128i*)(index + i));\n            uint16_t idx_arr[8];\n            _mm_storeu_si128((__m128i*)idx_arr, idx_reg);\n\n            __m128d v0 = _mm_set_pd(vec[idx_arr[1]], vec[idx_arr[0]]);\n            __m128d v1 = _mm_set_pd(vec[idx_arr[3]], vec[idx_arr[2]]);\n            __m128d v2 = _mm_set_pd(vec[idx_arr[5]], vec[idx_arr[4]]);\n            __m128d v3 = _mm_set_pd(vec[idx_arr[7]], vec[idx_arr[6]]);\n\n            __m128d t0 = _mm_add_pd(v0, v1);\n            __m128d t1 = _mm_add_pd(v2, v3);\n            __m128d chunk_accum = _mm_add_pd(t0, t1);\n            global_accum = _mm_add_pd(global_accum, chunk_accum);\n        }\n    }\n\n    double remainder_sum = 0.0;\n    for (; i < length; i++) {\n        remainder_sum += vec[index[i]];\n    }\n\n    double tmp[2];\n    _mm_storeu_pd(tmp, global_accum);\n    return tmp[0] + tmp[1] + remainder_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    double total = 0.0;\n    if (length == 0) \n        return total;\n\n    __m128d sum0 = _mm_setzero_pd();\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m128i idx_reg = _mm_loadu_si128((__m128i*)(index + i));\n        int16_t idx_arr[8];\n        _mm_storeu_si128((__m128i*)idx_arr, idx_reg);\n\n        __m128d v0 = _mm_setr_pd(vec[idx_arr[0]], vec[idx_arr[1]]);\n        __m128d v1 = _mm_setr_pd(vec[idx_arr[2]], vec[idx_arr[3]]);\n        __m128d v2 = _mm_setr_pd(vec[idx_arr[4]], vec[idx_arr[5]]);\n        __m128d v3 = _mm_setr_pd(vec[idx_arr[6]], vec[idx_arr[7]]);\n\n        sum0 = _mm_add_pd(sum0, v0);\n        sum1 = _mm_add_pd(sum1, v1);\n        sum2 = _mm_add_pd(sum2, v2);\n        sum3 = _mm_add_pd(sum3, v3);\n    }\n\n    sum0 = _mm_add_pd(sum0, sum1);\n    sum2 = _mm_add_pd(sum2, sum3);\n    sum0 = _mm_add_pd(sum0, sum2);\n\n    double temp[2];\n    _mm_storeu_pd(temp, sum0);\n    total = temp[0] + temp[1];\n\n    for (; i < length; i++) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_width = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_width; i++) {\n        size_t j = 0;\n        for (; j <= output_width - 2; j += 2) {\n            __m128d accum = _mm_setzero_pd();\n            for (size_t k = 0; k < kernel_size; k++) {\n                for (size_t l = 0; l < kernel_size; l++) {\n                    __m128d k_val = _mm_load1_pd(kernel + k * kernel_size + l);\n                    __m128d i_val = _mm_loadu_pd(input + (i + k) * input_size + j + l);\n                    accum = _mm_add_pd(accum, _mm_mul_pd(k_val, i_val));\n                }\n            }\n            accum = _mm_max_pd(accum, _mm_setzero_pd());\n            _mm_storeu_pd(output + i * output_width + j, accum);\n        }\n        if (j < output_width) {\n            double total = 0.0;\n            for (size_t k = 0; k < kernel_size; k++) {\n                for (size_t l = 0; l < kernel_size; l++) {\n                    total += kernel[k * kernel_size + l] * input[(i + k) * input_size + j + l];\n                }\n            }\n            if (total < 0.0) total = 0.0;\n            output[i * output_width + j] = total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t out_size = input_size - kernel_size + 1;\n    if (out_size == 0) return;\n\n    for (size_t i = 0; i < out_size; i++) {\n        size_t j = 0;\n        while (j < out_size) {\n            if (j + 1 < out_size) {\n                __m128d accum = _mm_setzero_pd();\n                for (size_t m = 0; m < kernel_size; m++) {\n                    for (size_t n = 0; n < kernel_size; n++) {\n                        __m128d k_vec = _mm_set1_pd(kernel[m * kernel_size + n]);\n                        __m128d in_vec = _mm_loadu_pd(input + (i + m) * input_size + j + n);\n                        accum = _mm_add_pd(accum, _mm_mul_pd(k_vec, in_vec));\n                    }\n                }\n                accum = _mm_max_pd(accum, _mm_setzero_pd());\n                double temp[2];\n                _mm_storeu_pd(temp, accum);\n                output[i * out_size + j] = temp[0];\n                output[i * out_size + j + 1] = temp[1];\n                j += 2;\n            } else {\n                double sum = 0.0;\n                for (size_t m = 0; m < kernel_size; m++) {\n                    __m128d row_accum = _mm_setzero_pd();\n                    size_t n = 0;\n                    for (; n <= kernel_size - 2; n += 2) {\n                        __m128d k_vec = _mm_loadu_pd(kernel + m * kernel_size + n);\n                        __m128d in_vec = _mm_loadu_pd(input + (i + m) * input_size + j + n);\n                        row_accum = _mm_add_pd(row_accum, _mm_mul_pd(k_vec, in_vec));\n                    }\n                    if (n < kernel_size) {\n                        sum += kernel[m * kernel_size + n] * input[(i + m) * input_size + j + n];\n                    }\n                    double temp[2];\n                    _mm_storeu_pd(temp, row_accum);\n                    sum += temp[0] + temp[1];\n                }\n                if (sum < 0.0) sum = 0.0;\n                output[i * out_size + j] = sum;\n                j++;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            double total = 0.0;\n            for (size_t k = 0; k < kernel_size; ++k) {\n                const double* in_row = input + (i + k) * input_size + j;\n                const double* k_row = kernel + k * kernel_size;\n                __m128d sum2 = _mm_setzero_pd();\n                size_t l = 0;\n                for (; l + 1 < kernel_size; l += 2) {\n                    __m128d in_vec = _mm_loadu_pd(in_row + l);\n                    __m128d k_vec = _mm_loadu_pd(k_row + l);\n                    sum2 = _mm_add_pd(sum2, _mm_mul_pd(k_vec, in_vec));\n                }\n                double row_sum = 0.0;\n                if (l < kernel_size) {\n                    row_sum = k_row[l] * in_row[l];\n                }\n                double temp[2];\n                _mm_storeu_pd(temp, sum2);\n                row_sum += temp[0] + temp[1];\n                total += row_sum;\n            }\n            output[i * output_size + j] = (total > 0.0) ? total : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t out_size = input_size - kernel_size + 1;\n    const size_t even_limit = kernel_size - (kernel_size % 2);\n    const bool is_odd = kernel_size % 2;\n\n    for (size_t i = 0; i < out_size; i++) {\n        for (size_t j = 0; j < out_size; j++) {\n            __m128d vsum = _mm_setzero_pd();\n            double sum = 0.0;\n\n            for (size_t k = 0; k < kernel_size; k++) {\n                for (size_t l = 0; l < even_limit; l += 2) {\n                    __m128d vk = _mm_loadu_pd(kernel + k * kernel_size + l);\n                    __m128d vi = _mm_loadu_pd(input + (i + k) * input_size + j + l);\n                    vsum = _mm_add_pd(vsum, _mm_mul_pd(vk, vi));\n                }\n                if (is_odd) {\n                    double k_val = kernel[k * kernel_size + even_limit];\n                    double i_val = input[(i + k) * input_size + j + even_limit];\n                    sum += k_val * i_val;\n                }\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, vsum);\n            double total = temp[0] + temp[1] + sum;\n\n            if (total < 0.0) {\n                total = 0.0;\n            }\n\n            output[i * out_size + j] = total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            double sum_remainder = 0.0;\n            __m128d acc_total = _mm_setzero_pd();\n            for (size_t k = 0; k < kernel_size; k++) {\n                const double* kernel_row = kernel + k * kernel_size;\n                const double* input_row = input + (i + k) * input_size + j;\n                size_t l = 0;\n                for (; l < kernel_size - 1; l += 2) {\n                    __m128d k_vec = _mm_loadu_pd(kernel_row + l);\n                    __m128d in_vec = _mm_loadu_pd(input_row + l);\n                    __m128d prod = _mm_mul_pd(k_vec, in_vec);\n                    acc_total = _mm_add_pd(acc_total, prod);\n                }\n                if (l < kernel_size) {\n                    sum_remainder += kernel_row[l] * input_row[l];\n                }\n            }\n            __m128d sum_vec = _mm_hadd_pd(acc_total, acc_total);\n            double total = _mm_cvtsd_f64(sum_vec) + sum_remainder;\n            if (total < 0.0) {\n                total = 0.0;\n            }\n            output[i * output_size + j] = total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (threshold < 0.0 || n < 2) {\n        return false;\n    }\n\n    const __m128d sign_mask = _mm_set1_pd(-0.0);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128d current_v = _mm_set1_pd(numbers[i]);\n        size_t j = i + 1;\n\n        for (; j <= n - 2; j += 2) {\n            __m128d others = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(current_v, others);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n            __m128d thresh_v = _mm_set1_pd(threshold);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh_v);\n\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        if (j < n) {\n            if (std::fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n    if (n == 2) {\n        double diff = numbers[0] - numbers[1];\n        if (diff < 0) diff = -diff;\n        return diff < threshold;\n    }\n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    __m128d sign_mask = _mm_set1_pd(-0.0);\n    for (size_t i = 0; i <= n-3; i++) {\n        __m128d v0 = _mm_loadu_pd(&numbers[i]);\n        __m128d v1 = _mm_loadu_pd(&numbers[i+1]);\n        __m128d diff = _mm_sub_pd(v0, v1);\n        __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n        __m128d cmp = _mm_cmplt_pd(abs_diff, thresh_vec);\n        int mask = _mm_movemask_pd(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    const __m128d sign_mask = _mm_set1_pd(-0.0);\n\n    for (size_t i = 0; i < n-1; i++) {\n        __m128d v_num = _mm_set1_pd(numbers[i]);\n        size_t j = i+1;\n\n        while (j+1 < n) {\n            __m128d v_others = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(v_num, v_others);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n            __m128d v_thresh = _mm_set1_pd(threshold);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, v_thresh);\n\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n\n            j += 2;\n        }\n\n        if (j < n) {\n            double diff = fabs(numbers[i] - numbers[j]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    size_t i = 0;\n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    __m128d sign_mask = _mm_set1_pd(-0.0);\n\n    for (; i <= n-3; i += 2) {\n        __m128d a = _mm_loadu_pd(&numbers[i]);\n        __m128d b = _mm_loadu_pd(&numbers[i+1]);\n        __m128d diff = _mm_sub_pd(a, b);\n        __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n        __m128d cmp = _mm_cmplt_pd(abs_diff, thresh_vec);\n        if (_mm_movemask_pd(cmp)) \n            return true;\n    }\n\n    if (i < n-1) {\n        double d = std::fabs(numbers[i] - numbers[i+1]);\n        if (d < threshold) \n            return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    size_t n = numbers.size();\n    if (n < 2) \n        return false;\n\n    const __m128d v_threshold = _mm_set1_pd(threshold);\n    const __m128d sign_mask = _mm_set1_pd(-0.0);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128d v_i = _mm_set1_pd(numbers[i]);\n        size_t j = i + 1;\n        size_t remaining = n - i - 1;\n\n        while (remaining >= 2) {\n            __m128d v_j = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(v_i, v_j);\n            __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n            __m128d mask = _mm_cmplt_pd(abs_diff, v_threshold);\n            if (_mm_movemask_pd(mask)) {\n                return true;\n            }\n            j += 2;\n            remaining -= 2;\n        }\n\n        if (remaining == 1) {\n            double diff = numbers[i] - numbers[j];\n            if (diff < 0) \n                diff = -diff;\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int current = 0;\n    int i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&operations[i]);\n        __m128i shifted1 = _mm_slli_si128(chunk, 4);\n        __m128i sum1 = _mm_add_epi32(chunk, shifted1);\n        __m128i shifted2 = _mm_slli_si128(sum1, 8);\n        __m128i prefix = _mm_add_epi32(sum1, shifted2);\n        __m128i cum_vec = _mm_set1_epi32(current);\n        __m128i running_totals = _mm_add_epi32(prefix, cum_vec);\n\n        __m128i zero = _mm_setzero_si128();\n        __m128i negative_mask = _mm_cmplt_epi32(running_totals, zero);\n        if (_mm_movemask_epi8(negative_mask) != 0) {\n            return true;\n        }\n\n        int totals[4] __attribute__((aligned(16)));\n        _mm_store_si128((__m128i*)totals, running_totals);\n        current = totals[3];\n    }\n\n    for (; i < n; ++i) {\n        current += operations[i];\n        if (current < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    int n = operations.size();\n    if (n == 0) \n        return false;\n    \n    int carry = 0;\n    int i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&operations[i]);\n        __m128i shifted1 = _mm_srli_si128(v, 4);\n        __m128i partial1 = _mm_add_epi32(v, shifted1);\n        __m128i shifted2 = _mm_srli_si128(partial1, 8);\n        __m128i partial2 = _mm_add_epi32(partial1, shifted2);\n        __m128i carry_vec = _mm_set1_epi32(carry);\n        __m128i prefix = _mm_add_epi32(carry_vec, partial2);\n        \n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmplt_epi32(prefix, zero);\n        if (_mm_movemask_epi8(mask) != 0) {\n            return true;\n        }\n        \n        carry = _mm_cvtsi128_si32(_mm_srli_si128(prefix, 12));\n    }\n\n    for (; i < n; ++i) {\n        carry += operations[i];\n        if (carry < 0)\n            return true;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    int n = operations.size();\n    if (n == 0) \n        return false;\n\n    int acc = 0;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        __m128i shifted1 = _mm_slli_si128(chunk, 4);\n        __m128i sum1 = _mm_add_epi32(chunk, shifted1);\n        __m128i shifted2 = _mm_slli_si128(sum1, 8);\n        __m128i prefix = _mm_add_epi32(sum1, shifted2);\n        __m128i acc_v = _mm_set1_epi32(acc);\n        __m128i total = _mm_add_epi32(prefix, acc_v);\n        __m128i cmp = _mm_cmplt_epi32(total, _mm_setzero_si128());\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return true;\n        }\n        __m128i last = _mm_srli_si128(total, 12);\n        acc = _mm_cvtsi128_si32(last);\n    }\n\n    for (; i < n; i++) {\n        acc += operations[i];\n        if (acc < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    if (n == 0) \n        return false;\n\n    int current_balance = 0;\n    int i = 0;\n\n    for (; i <= n-4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&operations[i]);\n        __m128i shifted1 = _mm_slli_si128(v, 4);\n        __m128i partial = _mm_add_epi32(v, shifted1);\n        __m128i shifted2 = _mm_slli_si128(partial, 8);\n        partial = _mm_add_epi32(partial, shifted2);\n        __m128i balance_vec = _mm_add_epi32(partial, _mm_set1_epi32(current_balance));\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmplt_epi32(balance_vec, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return true;\n        }\n        current_balance = _mm_cvtsi128_si32(_mm_srli_si128(balance_vec, 12));\n    }\n\n    for (; i < n; i++) {\n        current_balance += operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int base = 0;\n    int i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        \n        __m128i shifted1 = _mm_srli_si128(chunk, 4);\n        __m128i sum1 = _mm_add_epi32(chunk, shifted1);\n        \n        __m128i shifted2 = _mm_srli_si128(sum1, 8);\n        __m128i cumulative = _mm_add_epi32(sum1, shifted2);\n        \n        __m128i base_v = _mm_set1_epi32(base);\n        __m128i balance = _mm_add_epi32(base_v, cumulative);\n        \n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmplt_epi32(balance, zero);\n        int mask_bits = _mm_movemask_epi8(mask);\n        if (mask_bits != 0) {\n            return true;\n        }\n        \n        __m128i last = _mm_shuffle_epi32(cumulative, _MM_SHUFFLE(3, 3, 3, 3));\n        base += _mm_cvtsi128_si32(last);\n    }\n    \n    for (; i < n; i++) {\n        base += operations[i];\n        if (base < 0) {\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n    \n    __m128d vsum = _mm_setzero_pd();\n    size_t i = 0;\n    for (; i < n - (n % 2); i += 2) {\n        __m128d v = _mm_loadu_pd(&numbers[i]);\n        vsum = _mm_add_pd(vsum, v);\n    }\n    double sum = _mm_cvtsd_f64(_mm_add_pd(vsum, _mm_unpackhi_pd(vsum, vsum)));\n    if (n % 2) sum += numbers[i];\n    \n    double mean = sum / n;\n    __m128d vmean = _mm_set1_pd(mean);\n    __m128d vabs_sum = _mm_setzero_pd();\n    __m128d sign_mask = _mm_set1_pd(-0.0);\n    \n    i = 0;\n    for (; i < n - (n % 2); i += 2) {\n        __m128d v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(v, vmean);\n        __m128d abs_diff = _mm_andnot_pd(sign_mask, diff);\n        vabs_sum = _mm_add_pd(vabs_sum, abs_diff);\n    }\n    double abs_sum = _mm_cvtsd_f64(_mm_add_pd(vabs_sum, _mm_unpackhi_pd(vabs_sum, vabs_sum)));\n    if (n % 2) abs_sum += std::fabs(numbers[i] - mean);\n    \n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n    \n    __m128d vsum = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t i = 0;\n    \n    for (; i <= n - 2; i += 2) {\n        __m128d v = _mm_loadu_pd(&numbers[i]);\n        vsum = _mm_add_pd(vsum, v);\n    }\n    \n    double temp[2];\n    _mm_storeu_pd(temp, vsum);\n    sum = temp[0] + temp[1];\n    \n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n    \n    double mean = sum / n;\n    __m128d vmean = _mm_set1_pd(mean);\n    __m128d vabs_sum = _mm_setzero_pd();\n    const __m128d sign_mask = _mm_set1_pd(-0.0);\n    double abs_sum = 0.0;\n    i = 0;\n    \n    for (; i <= n - 2; i += 2) {\n        __m128d v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(v, vmean);\n        __m128d abs_val = _mm_andnot_pd(sign_mask, diff);\n        vabs_sum = _mm_add_pd(vabs_sum, abs_val);\n    }\n    \n    _mm_storeu_pd(temp, vabs_sum);\n    abs_sum = temp[0] + temp[1];\n    \n    for (; i < n; ++i) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n    \n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Compute the mean\n    double sum = 0.0;\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 doubles\n    for (; i <= n - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, data);\n    }\n\n    // Horizontal add: sum_vec = [a0+a1, ?]\n    sum_vec = _mm_hadd_pd(sum_vec, sum_vec);\n    _mm_store_sd(&sum, sum_vec);\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Step 2: Compute absolute deviations\n    double abs_sum = 0.0;\n    __m128d abs_sum_vec = _mm_setzero_pd();\n    __m128d mean_vec = _mm_set1_pd(mean);\n    const __m128d sign_mask = _mm_set1_pd(-0.0); // Mask to clear sign bit\n    i = 0;\n\n    for (; i <= n - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_val = _mm_andnot_pd(sign_mask, diff); // Clear sign bit\n        abs_sum_vec = _mm_add_pd(abs_sum_vec, abs_val);\n    }\n\n    // Horizontal add: abs_sum_vec = [a0+a1, ?]\n    abs_sum_vec = _mm_hadd_pd(abs_sum_vec, abs_sum_vec);\n    _mm_store_sd(&abs_sum, abs_sum_vec);\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d vsum = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t i = 0;\n\n    for (; i <= n - 2; i += 2) {\n        vsum = _mm_add_pd(vsum, _mm_loadu_pd(&numbers[i]));\n    }\n    if (i < n) {\n        sum += numbers[i];\n    }\n\n    double temp[2];\n    _mm_storeu_pd(temp, vsum);\n    sum += temp[0] + temp[1];\n    double mean = sum / n;\n\n    __m128d vmean = _mm_set1_pd(mean);\n    __m128d vsign_mask = _mm_set1_pd(-0.0);\n    __m128d vabs_sum = _mm_setzero_pd();\n    double abs_sum = 0.0;\n    i = 0;\n\n    for (; i <= n - 2; i += 2) {\n        __m128d v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(v, vmean);\n        __m128d abs_diff = _mm_andnot_pd(vsign_mask, diff);\n        vabs_sum = _mm_add_pd(vabs_sum, abs_diff);\n    }\n    if (i < n) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n\n    _mm_storeu_pd(temp, vabs_sum);\n    abs_sum += temp[0] + temp[1];\n    \n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return 0.0;\n    }\n\n    __m128d sum_vec = _mm_setzero_pd();\n    double sum = 0.0;\n    size_t i = 0;\n\n    for (; i <= n - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, data);\n    }\n\n    double temp[2];\n    _mm_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1];\n\n    if (i < n) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d abs_sum_vec = _mm_setzero_pd();\n    __m128d abs_mask = _mm_set1_pd(-0.0);\n    double abs_sum = 0.0;\n    i = 0;\n\n    for (; i <= n - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(abs_mask, diff);\n        abs_sum_vec = _mm_add_pd(abs_sum_vec, abs_diff);\n    }\n\n    _mm_storeu_pd(temp, abs_sum_vec);\n    abs_sum = temp[0] + temp[1];\n\n    if (i < n) {\n        abs_sum += std::fabs(numbers[i] - mean);\n    }\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    int n = numbers.size();\n    if (n == 0) {\n        return std::vector<int>();\n    }\n    std::vector<int> out(2 * n - 1);\n    int j = 0;\n\n    if (n >= 4) {\n        __m128i d_vec = _mm_set1_epi32(delimeter);\n        int i = 0;\n        for (; i <= n - 4; i += 4) {\n            __m128i v = _mm_loadu_si128((__m128i*)&numbers[i]);\n            __m128i v_low = _mm_unpacklo_epi32(v, d_vec);\n            __m128i v_high = _mm_unpackhi_epi32(v, d_vec);\n            _mm_storeu_si128((__m128i*)&out[j], v_low);\n            _mm_storel_epi64((__m128i*)&out[j+4], v_high);\n            __m128i shifted = _mm_srli_si128(v_high, 8);\n            out[j+6] = _mm_cvtsi128_si32(shifted);\n            j += 7;\n            if (i + 4 < n) {\n                out[j] = delimeter;\n                j++;\n            }\n        }\n    }\n\n    int i_start = (n >= 4) ? n - (n % 4) : 0;\n    if (i_start < n) {\n        out[j++] = numbers[i_start];\n        for (int i = i_start + 1; i < n; i++) {\n            out[j++] = delimeter;\n            out[j++] = numbers[i];\n        }\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return std::vector<int>();\n    }\n    std::vector<int> output(2 * n - 1);\n    output[0] = numbers[0];\n    if (n == 1) {\n        return output;\n    }\n    \n    __m128i d_vec = _mm_set1_epi32(delimeter);\n    size_t i = 1;\n    while (i + 3 < n) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i low = _mm_unpacklo_epi32(d_vec, chunk);\n        __m128i high = _mm_unpackhi_epi32(d_vec, chunk);\n        _mm_storeu_si128((__m128i*)&output[2*i-1], low);\n        _mm_storeu_si128((__m128i*)&output[2*i-1+4], high);\n        i += 4;\n    }\n    \n    for (; i < n; i++) {\n        output[2*i-1] = delimeter;\n        output[2*i] = numbers[i];\n    }\n    \n    return output;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n    \n    std::vector<int> result(2 * n - 1);\n    result[2 * n - 2] = numbers[n - 1];\n    \n    if (n == 1) {\n        return result;\n    }\n    \n    size_t part1_size = n - 1;\n    int* output_ptr = result.data();\n    const int* input_ptr = numbers.data();\n    \n    size_t i = 0;\n    for (; i + 3 < part1_size; i += 4) {\n        __m128i v_in = _mm_loadu_si128(reinterpret_cast<const __m128i*>(input_ptr + i));\n        __m128i v_delim = _mm_set1_epi32(delimeter);\n        __m128i low = _mm_unpacklo_epi32(v_in, v_delim);\n        __m128i high = _mm_unpackhi_epi32(v_in, v_delim);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_ptr + 2 * i), low);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_ptr + 2 * i + 4), high);\n    }\n    \n    for (; i < part1_size; i++) {\n        output_ptr[2 * i] = input_ptr[i];\n        output_ptr[2 * i + 1] = delimeter;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n    if (n == 1) {\n        return {numbers[0]};\n    }\n    size_t m = n - 1;\n    std::vector<int> result(2 * n - 1);\n    __m128i d_vec = _mm_set1_epi32(delimeter);\n    size_t i = 0;\n    for (; i + 4 <= m; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(numbers.data() + i));\n        __m128i low = _mm_unpacklo_epi32(a_vec, d_vec);\n        __m128i high = _mm_unpackhi_epi32(a_vec, d_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + 2*i), low);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + 2*i + 4), high);\n    }\n    for (; i < m; i++) {\n        result[2*i] = numbers[i];\n        result[2*i+1] = delimeter;\n    }\n    result[2*n-2] = numbers[n-1];\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n    std::vector<int> out(2 * n - 1, 0);\n    if (n == 1) {\n        out[0] = numbers[0];\n        return out;\n    }\n\n    size_t i = 0;\n    size_t j = 0;\n    __m128i Dvec = _mm_set1_epi32(delimeter);\n\n    for (; i <= n - 3; i += 2) {\n        __m128i in = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        __m128i mixed = _mm_unpacklo_epi32(in, Dvec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[j]), mixed);\n        j += 4;\n    }\n\n    size_t remaining = n - i;\n    if (remaining == 2) {\n        out[j] = numbers[i];\n        out[j + 1] = delimeter;\n        out[j + 2] = numbers[i + 1];\n    } else if (remaining == 1) {\n        out[j] = numbers[i];\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n    \n    int64_t total_sum = 0;\n    int64_t total_product = 1;\n    int i = 0;\n    \n    if (n >= 4) {\n        __m128i sum_vec = _mm_setzero_si128();\n        \n        for (; i <= n-4; i += 4) {\n            __m128i data = _mm_loadl_epi64((__m128i const*)(numbers.data() + i));\n            __m128i data32 = _mm_cvtepi16_epi32(data);\n            \n            __m128i shuffled = _mm_shuffle_epi32(data32, _MM_SHUFFLE(2, 3, 0, 1));\n            __m128i prod64 = _mm_mul_epi32(data32, shuffled);\n            int64_t p0 = _mm_extract_epi64(prod64, 0);\n            int64_t p1 = _mm_extract_epi64(prod64, 1);\n            total_product = total_product * p0 * p1;\n            \n            __m128i low64 = _mm_cvtepi32_epi64(data32);\n            __m128i high64 = _mm_cvtepi32_epi64(_mm_srli_si128(data32, 8));\n            sum_vec = _mm_add_epi64(sum_vec, low64);\n            sum_vec = _mm_add_epi64(sum_vec, high64);\n        }\n        \n        uint64_t sum_arr[2];\n        _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n        total_sum += (sum_arr[0] + sum_arr[1]);\n    }\n    \n    for (; i < n; i++) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n    \n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cstdint>\n#include <cstring>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    __m128i sum_acc0 = _mm_setzero_si128();\n    __m128i sum_acc1 = _mm_setzero_si128();\n    int64_t prod[4] = {1, 1, 1, 1};\n    size_t i = 0;\n\n    for (; i + 7 < n; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(numbers.data() + i));\n        \n        __m128i low = _mm_cvtepi16_epi32(v);\n        __m128i high = _mm_cvtepi16_epi32(_mm_srli_si128(v, 8));\n        \n        __m128i low0 = _mm_cvtepi32_epi64(low);\n        __m128i low1 = _mm_cvtepi32_epi64(_mm_srli_si128(low, 8));\n        __m128i high0 = _mm_cvtepi32_epi64(high);\n        __m128i high1 = _mm_cvtepi32_epi64(_mm_srli_si128(high, 8));\n        \n        __m128i sum_low = _mm_add_epi64(low0, high0);\n        __m128i sum_high = _mm_add_epi64(low1, high1);\n        sum_acc0 = _mm_add_epi64(sum_acc0, sum_low);\n        sum_acc1 = _mm_add_epi64(sum_acc1, sum_high);\n        \n        alignas(16) int32_t temp_low[4];\n        alignas(16) int32_t temp_high[4];\n        _mm_store_si128((__m128i*)temp_low, low);\n        _mm_store_si128((__m128i*)temp_high, high);\n        \n        prod[0] *= static_cast<int64_t>(temp_low[0]) * static_cast<int64_t>(temp_high[0]);\n        prod[1] *= static_cast<int64_t>(temp_low[1]) * static_cast<int64_t>(temp_high[1]);\n        prod[2] *= static_cast<int64_t>(temp_low[2]) * static_cast<int64_t>(temp_high[2]);\n        prod[3] *= static_cast<int64_t>(temp_low[3]) * static_cast<int64_t>(temp_high[3]);\n    }\n\n    int64_t sum_remaining = 0;\n    for (; i < n; i++) {\n        sum_remaining += numbers[i];\n        prod[i % 4] *= numbers[i];\n    }\n\n    alignas(16) uint64_t sum0[2];\n    alignas(16) uint64_t sum1[2];\n    _mm_store_si128((__m128i*)sum0, sum_acc0);\n    _mm_store_si128((__m128i*)sum1, sum_acc1);\n    int64_t total_sum = static_cast<int64_t>(sum0[0]) + static_cast<int64_t>(sum0[1]) + \n                        static_cast<int64_t>(sum1[0]) + static_cast<int64_t>(sum1[1]) + \n                        sum_remaining;\n    \n    int64_t total_prod = prod[0] * prod[1] * prod[2] * prod[3];\n    \n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    __m128i sum_acc1 = _mm_setzero_si128();\n    __m128i sum_acc2 = _mm_setzero_si128();\n    int64_t prod_acc[4] = {1, 1, 1, 1};\n\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i*)(numbers.data() + i));\n        \n        __m128i sign16 = _mm_srai_epi16(v, 15);\n        __m128i low32 = _mm_unpacklo_epi16(v, sign16);\n        __m128i sign32_low = _mm_srai_epi32(low32, 31);\n        __m128i low64_low = _mm_unpacklo_epi32(low32, sign32_low);\n        __m128i low64_high = _mm_unpackhi_epi32(low32, sign32_low);\n        \n        __m128i high32 = _mm_unpackhi_epi16(v, sign16);\n        __m128i sign32_high = _mm_srai_epi32(high32, 31);\n        __m128i high64_low = _mm_unpacklo_epi32(high32, sign32_high);\n        __m128i high64_high = _mm_unpackhi_epi32(high32, sign32_high);\n        \n        sum_acc1 = _mm_add_epi64(sum_acc1, low64_low);\n        sum_acc1 = _mm_add_epi64(sum_acc1, high64_low);\n        sum_acc2 = _mm_add_epi64(sum_acc2, low64_high);\n        sum_acc2 = _mm_add_epi64(sum_acc2, high64_high);\n        \n        for (int j = 0; j < 8; j++) {\n            int16_t x = _mm_extract_epi16(v, j);\n            prod_acc[j % 4] *= static_cast<int64_t>(x);\n        }\n    }\n\n    int64_t sum_arr[2];\n    _mm_storeu_si128((__m128i*)sum_arr, sum_acc1);\n    int64_t total_sum = sum_arr[0] + sum_arr[1];\n    _mm_storeu_si128((__m128i*)sum_arr, sum_acc2);\n    total_sum += sum_arr[0] + sum_arr[1];\n    \n    int64_t total_prod = prod_acc[0] * prod_acc[1] * prod_acc[2] * prod_acc[3];\n    \n    for (; i < n; i++) {\n        total_sum += numbers[i];\n        total_prod *= static_cast<int64_t>(numbers[i]);\n    }\n    \n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return std::vector<int64_t>{0, 1};\n    }\n    \n    int64_t product = 1;\n    for (auto x : numbers) {\n        product *= x;\n    }\n\n    size_t n = numbers.size();\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    for (; i + 8 <= n; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(numbers.data() + i));\n        sum_vec = _mm_add_epi16(sum_vec, v);\n    }\n\n    int32_t sum_remaining = 0;\n    for (; i < n; i++) {\n        sum_remaining += numbers[i];\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_cmpgt_epi16(zero, sum_vec);\n    __m128i low_part = _mm_unpacklo_epi16(sum_vec, sign_mask);\n    __m128i high_part = _mm_unpackhi_epi16(sum_vec, sign_mask);\n    __m128i sum32 = _mm_add_epi32(low_part, high_part);\n\n    __m128i shuf = _mm_shuffle_epi32(sum32, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sums = _mm_add_epi32(sum32, shuf);\n    __m128i shifted = _mm_srli_si128(sums, 4);\n    sums = _mm_add_epi32(sums, shifted);\n    int32_t sum_simd = _mm_cvtsi128_si32(sums);\n    int32_t sum_total = sum_simd + sum_remaining;\n\n    return std::vector<int64_t>{static_cast<int64_t>(sum_total), product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    // SIMD for sum\n    __m128i sum32 = _mm_setzero_si128();\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(numbers.data() + i));\n        __m128i sign_mask = _mm_cmpgt_epi16(_mm_setzero_si128(), chunk);\n        __m128i low = _mm_unpacklo_epi16(chunk, sign_mask);\n        __m128i high = _mm_unpackhi_epi16(chunk, sign_mask);\n        sum32 = _mm_add_epi32(sum32, low);\n        sum32 = _mm_add_epi32(sum32, high);\n    }\n    alignas(16) int32_t sum_arr[4];\n    _mm_store_si128((__m128i*)sum_arr, sum32);\n    sum = (int64_t)sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    // Scalar for remaining sum elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Scalar for product\n    for (i = 0; i < n; ++i) {\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    const int n = numbers.size();\n    std::vector<int> output(n);\n    if (n == 0) {\n        return output;\n    }\n\n    int M_prev = INT_MIN;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(numbers.data() + i));\n        __m128i v_prev = _mm_set1_epi32(M_prev);\n        __m128i candidate0 = _mm_max_epi32(v0, v_prev);\n\n        __m128i candidate0_0 = _mm_shuffle_epi32(candidate0, _MM_SHUFFLE(0, 0, 0, 0));\n        __m128i candidate1 = _mm_max_epi32(v0, candidate0_0);\n\n        __m128i candidate1_1 = _mm_shuffle_epi32(candidate1, _MM_SHUFFLE(1, 1, 1, 1));\n        __m128i candidate2 = _mm_max_epi32(v0, candidate1_1);\n\n        __m128i candidate2_2 = _mm_shuffle_epi32(candidate2, _MM_SHUFFLE(2, 2, 2, 2));\n        __m128i candidate3 = _mm_max_epi32(v0, candidate2_2);\n\n        int c00 = _mm_cvtsi128_si32(candidate0);\n        int c11 = _mm_cvtsi128_si32(_mm_srli_si128(candidate1, 4));\n        int c22 = _mm_cvtsi128_si32(_mm_srli_si128(candidate2, 8));\n        int c33 = _mm_cvtsi128_si32(_mm_srli_si128(candidate3, 12));\n\n        __m128i result = _mm_set_epi32(c33, c22, c11, c00);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output.data() + i), result);\n        M_prev = c33;\n    }\n\n    for (; i < n; i++) {\n        M_prev = std::max(M_prev, numbers[i]);\n        output[i] = M_prev;\n    }\n\n    return output;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) {\n        return result;\n    }\n    \n    int prev_max = std::numeric_limits<int>::min();\n    int i = 0;\n    \n    for (; i <= n-4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i const*)(numbers.data() + i));\n        __m128i v1 = _mm_slli_si128(chunk, 4);\n        __m128i mask = _mm_cmpgt_epi32(chunk, v1);\n        v1 = _mm_or_si128(_mm_and_si128(mask, chunk), _mm_andnot_si128(mask, v1));\n        \n        __m128i v2 = _mm_slli_si128(v1, 8);\n        mask = _mm_cmpgt_epi32(v1, v2);\n        v2 = _mm_or_si128(_mm_and_si128(mask, v1), _mm_andnot_si128(mask, v2));\n        \n        __m128i prev_vec = _mm_set1_epi32(prev_max);\n        mask = _mm_cmpgt_epi32(prev_vec, v2);\n        __m128i res_vec = _mm_or_si128(_mm_and_si128(mask, prev_vec), _mm_andnot_si128(mask, v2));\n        \n        _mm_storeu_si128((__m128i*)(result.data() + i), res_vec);\n        \n        int temp[4];\n        _mm_storeu_si128((__m128i*)temp, res_vec);\n        prev_max = temp[3];\n    }\n    \n    for (; i < n; i++) {\n        if (numbers[i] > prev_max) {\n            prev_max = numbers[i];\n        }\n        result[i] = prev_max;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) \n        return result;\n\n    int i = 0;\n    int current_max = INT_MIN;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(numbers.data() + i));\n        int x0 = (current_max > numbers[i]) ? current_max : numbers[i];\n        __m128i all_ones = _mm_cmpeq_epi32(chunk, chunk);\n        __m128i mask = _mm_srli_si128(all_ones, 4);\n        __m128i x0_vec = _mm_set1_epi32(x0);\n        __m128i w = _mm_or_si128(_mm_and_si128(mask, chunk), _mm_andnot_si128(mask, x0_vec));\n        __m128i scan = w;\n        __m128i shifted = _mm_srli_si128(scan, 4);\n        scan = _mm_max_epi32(scan, shifted);\n        shifted = _mm_srli_si128(scan, 8);\n        scan = _mm_max_epi32(scan, shifted);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), scan);\n        alignas(16) int temp[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), scan);\n        current_max = temp[3];\n    }\n\n    for (; i < n; ++i) {\n        current_max = (current_max > numbers[i]) ? current_max : numbers[i];\n        result[i] = current_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    if (numbers.empty()) \n        return result;\n\n    int prev_max = std::numeric_limits<int>::min();\n    size_t i = 0;\n    size_t n = numbers.size();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&numbers[i]);\n        __m128i v1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(2,1,0,0));\n        __m128i v2 = _mm_max_epi32(v, v1);\n        __m128i v3 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(1,0,0,0));\n        __m128i v4 = _mm_max_epi32(v2, v3);\n        __m128i v_prev = _mm_set1_epi32(prev_max);\n        __m128i v5 = _mm_max_epi32(v4, v_prev);\n        _mm_storeu_si128((__m128i*)&result[i], v5);\n        __m128i v_last = _mm_shuffle_epi32(v5, _MM_SHUFFLE(3,3,3,3));\n        prev_max = _mm_cvtsi128_si32(v_last);\n    }\n\n    for (; i < n; i++) {\n        if (numbers[i] > prev_max) \n            prev_max = numbers[i];\n        result[i] = prev_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    int current_max = numbers[0];\n    result[0] = numbers[0];\n    int i = 1;\n\n    while (i < n) {\n        int remaining = n - i;\n        if (remaining >= 4) {\n            __m128i v_in = _mm_loadu_si128((__m128i*)(numbers.data() + i));\n            int a0 = _mm_cvtsi128_si32(v_in);\n            __m128i v_shift1 = _mm_srli_si128(v_in, 4);\n            int a1 = _mm_cvtsi128_si32(v_shift1);\n            __m128i v_shift2 = _mm_srli_si128(v_in, 8);\n            int a2 = _mm_cvtsi128_si32(v_shift2);\n            __m128i v_shift3 = _mm_srli_si128(v_in, 12);\n            int a3 = _mm_cvtsi128_si32(v_shift3);\n\n            int out0 = std::max(current_max, a0);\n            int out1 = std::max(out0, a1);\n            int out2 = std::max(out1, a2);\n            int out3 = std::max(out2, a3);\n\n            result[i] = out0;\n            result[i+1] = out1;\n            result[i+2] = out2;\n            result[i+3] = out3;\n\n            current_max = out3;\n            i += 4;\n        } else {\n            for (int j = 0; j < remaining; j++) {\n                current_max = std::max(current_max, numbers[i+j]);\n                result[i+j] = current_max;\n            }\n            i += remaining;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t n = a.size();\n    std::string res(n, '0');\n    size_t i = 0;\n\n    for (; i + 15 < n; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n        \n        __m128i mask = _mm_set1_epi8(1);\n        __m128i a_bits = _mm_and_si128(a_vec, mask);\n        __m128i b_bits = _mm_and_si128(b_vec, mask);\n        \n        __m128i xor_result = _mm_xor_si128(a_bits, b_bits);\n        __m128i result_ascii = _mm_add_epi8(xor_result, _mm_set1_epi8('0'));\n        \n        _mm_storeu_si128(reinterpret_cast<__m128i*>(res.data() + i), result_ascii);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = '0' + ((a[i] & 1) ^ (b[i] & 1));\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string res(len, '0');\n    \n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n        __m128i vx = _mm_xor_si128(va, vb);\n        __m128i vzero = _mm_set1_epi8('0');\n        __m128i vres = _mm_add_epi8(vx, vzero);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), vres);\n    }\n    \n    for (; i < len; ++i) {\n        res[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t n = std::min(a.size(), b.size());\n    std::string res(n, '0');\n    size_t i = 0;\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n        __m128i vx = _mm_xor_si128(va, vb);\n        __m128i vres = _mm_add_epi8(vx, _mm_set1_epi8('0'));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), vres);\n    }\n\n    for (; i < n; i++) {\n        res[i] = (a[i] ^ b[i]) + '0';\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <stdexcept>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t n = a.size();\n    if (n != b.size()) {\n        throw std::invalid_argument(\"Strings must be of the same length\");\n    }\n    std::string res(n, '\\0');\n    size_t i = 0;\n    __m128i zero = _mm_set1_epi8('0');\n    \n    for (; i + 15 < n; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        \n        __m128i a_int = _mm_sub_epi8(a_vec, zero);\n        __m128i b_int = _mm_sub_epi8(b_vec, zero);\n        \n        __m128i res_int = _mm_xor_si128(a_int, b_int);\n        \n        __m128i res_char = _mm_add_epi8(res_int, zero);\n        \n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), res_char);\n    }\n    \n    for (; i < n; ++i) {\n        res[i] = (a[i] != b[i]) ? '1' : '0';\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    if (a.length() != b.length()) {\n        return \"\";\n    }\n    size_t len = a.length();\n    std::string result;\n    result.resize(len);\n    \n    const size_t stride = 16;\n    size_t i = 0;\n    for (; i + stride <= len; i += stride) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n        __m128i mask = _mm_set1_epi8(1);\n        __m128i a_bits = _mm_and_si128(a_vec, mask);\n        __m128i b_bits = _mm_and_si128(b_vec, mask);\n        __m128i xor_bits = _mm_xor_si128(a_bits, b_bits);\n        __m128i res_vec = _mm_add_epi8(_mm_set1_epi8('0'), xor_bits);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), res_vec);\n    }\n    \n    for (; i < len; ++i) {\n        result[i] = '0' + ((a[i] & 1) ^ (b[i] & 1));\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::vector<int> lengths;\n    lengths.reserve(strings.size());\n    for (const auto& s : strings) {\n        lengths.push_back(static_cast<int>(s.size()));\n    }\n\n    auto mm_max_epi32 = [](__m128i a, __m128i b) -> __m128i {\n        __m128i mask = _mm_cmpgt_epi32(a, b);\n        return _mm_or_si128(_mm_and_si128(mask, a), _mm_andnot_si128(mask, b));\n    };\n\n    int global_max = -1;\n    int global_index = -1;\n    int n = static_cast<int>(lengths.size());\n\n    int i = 0;\n    for (; i <= n - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lengths[i]));\n        __m128i v1 = _mm_shuffle_epi32(chunk, _MM_SHUFFLE(2,3,0,1));\n        __m128i max1 = mm_max_epi32(chunk, v1);\n        __m128i v2 = _mm_shuffle_epi32(max1, _MM_SHUFFLE(1,0,3,2));\n        __m128i max2 = mm_max_epi32(max1, v2);\n        int chunk_max = _mm_cvtsi128_si32(max2);\n\n        if (chunk_max > global_max) {\n            __m128i candidate_vec = _mm_set1_epi32(chunk_max);\n            __m128i mask = _mm_cmpeq_epi32(chunk, candidate_vec);\n            alignas(16) int mask_arr[4];\n            _mm_store_si128(reinterpret_cast<__m128i*>(mask_arr), mask);\n\n            for (int k = 0; k < 4; ++k) {\n                if (mask_arr[k]) {\n                    global_max = chunk_max;\n                    global_index = i + k;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lengths[i] > global_max) {\n            global_max = lengths[i];\n            global_index = i;\n        }\n    }\n\n    return strings[global_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n    std::vector<int> lengths;\n    lengths.reserve(strings.size());\n    for (const auto& s : strings) {\n        lengths.push_back(static_cast<int>(s.size()));\n    }\n\n    int max_val = lengths[0];\n    int max_index = 0;\n    size_t i = 0;\n    size_t n = strings.size();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lengths[i]));\n        __m128i v1 = _mm_shuffle_epi32(chunk, _MM_SHUFFLE(1,0,3,2));\n        __m128i v2 = _mm_max_epi32(chunk, v1);\n        __m128i v3 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0,1,0,1));\n        __m128i v4 = _mm_max_epi32(v2, v3);\n        int chunk_max = _mm_cvtsi128_si32(v4);\n\n        if (chunk_max > max_val) {\n            max_val = chunk_max;\n            for (size_t j = i; j < i+4; j++) {\n                if (lengths[j] == max_val) {\n                    max_index = j;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (lengths[i] > max_val) {\n            max_val = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return std::string();\n    }\n    int global_max = -1;\n    int global_index = -1;\n    int i = 0;\n    const int n = strings.size();\n\n    auto max_epi32 = [](__m128i a, __m128i b) -> __m128i {\n        __m128i diff = _mm_sub_epi32(a, b);\n        __m128i mask = _mm_srai_epi32(diff, 31);\n        return _mm_sub_epi32(a, _mm_and_si128(diff, mask));\n    };\n\n    for (; i+3 < n; i+=4) {\n        int len0 = strings[i].size();\n        int len1 = strings[i+1].size();\n        int len2 = strings[i+2].size();\n        int len3 = strings[i+3].size();\n\n        __m128i v = _mm_set_epi32(len3, len2, len1, len0);\n\n        __m128i v_perm1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(0,3,2,1));\n        __m128i max1 = max_epi32(v, v_perm1);\n        __m128i v_perm2 = _mm_shuffle_epi32(max1, _MM_SHUFFLE(0,3,2,1));\n        __m128i max2 = max_epi32(max1, v_perm2);\n        __m128i v_perm3 = _mm_shuffle_epi32(max2, _MM_SHUFFLE(1,0,3,2));\n        __m128i max3 = max_epi32(max2, v_perm3);\n\n        int max_in_block;\n        int max3_arr[4];\n        _mm_storeu_si128((__m128i*)max3_arr, max3);\n        max_in_block = max3_arr[0];\n\n        __m128i max_bc = _mm_set1_epi32(max_in_block);\n        __m128i mask = _mm_cmpeq_epi32(v, max_bc);\n\n        int mask_arr[4];\n        _mm_storeu_si128((__m128i*)mask_arr, mask);\n\n        int candidate_index_in_block = -1;\n        if (mask_arr[0]) {\n            candidate_index_in_block = i;\n        } else if (mask_arr[1]) {\n            candidate_index_in_block = i+1;\n        } else if (mask_arr[2]) {\n            candidate_index_in_block = i+2;\n        } else if (mask_arr[3]) {\n            candidate_index_in_block = i+3;\n        }\n\n        if (max_in_block > global_max) {\n            global_max = max_in_block;\n            global_index = candidate_index_in_block;\n        }\n    }\n\n    for (; i < n; i++) {\n        int len = strings[i].size();\n        if (len > global_max) {\n            global_max = len;\n            global_index = i;\n        }\n    }\n\n    return strings[global_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <climits>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n    size_t n = strings.size();\n    if (n == 1) {\n        return strings[0];\n    }\n\n    std::vector<int> lens;\n    lens.reserve(n);\n    for (size_t i = 0; i < n; i++) {\n        size_t len = strings[i].size();\n        if (len > static_cast<size_t>(INT_MAX)) {\n            lens.push_back(INT_MAX);\n        } else {\n            lens.push_back(static_cast<int>(len));\n        }\n    }\n\n    int global_max = lens[0];\n    __m128i max_v = _mm_set1_epi32(global_max);\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lens[i]));\n        __m128i mask = _mm_cmpgt_epi32(chunk, max_v);\n        max_v = _mm_or_si128(_mm_and_si128(mask, chunk), \n                            _mm_andnot_si128(mask, max_v));\n    }\n\n    alignas(16) int max_arr[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(max_arr), max_v);\n    for (int j = 0; j < 4; j++) {\n        if (max_arr[j] > global_max) {\n            global_max = max_arr[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if (lens[i] > global_max) {\n            global_max = lens[i];\n        }\n    }\n\n    for (size_t i = 0; i < n; i++) {\n        if (lens[i] == global_max) {\n            return strings[i];\n        }\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return std::string();\n    }\n\n    size_t global_max = 0;\n    size_t i = 0;\n\n    if (strings.size() >= 4) {\n        for (; i <= strings.size() - 4; i += 4) {\n            alignas(16) uint32_t len_arr[4] = {\n                static_cast<uint32_t>(strings[i].size()),\n                static_cast<uint32_t>(strings[i+1].size()),\n                static_cast<uint32_t>(strings[i+2].size()),\n                static_cast<uint32_t>(strings[i+3].size())\n            };\n            __m128i vec = _mm_load_si128(reinterpret_cast<const __m128i*>(len_arr));\n\n            __m128i v_shuffled = _mm_shuffle_epi32(vec, _MM_SHUFFLE(1,0,3,2));\n            __m128i cmp = _mm_cmpgt_epi32(vec, v_shuffled);\n            __m128i max1 = _mm_or_si128(_mm_and_si128(cmp, vec), _mm_andnot_si128(cmp, v_shuffled));\n            __m128i max1_shuffled = _mm_shuffle_epi32(max1, _MM_SHUFFLE(2,3,0,1));\n            __m128i cmp2 = _mm_cmpgt_epi32(max1, max1_shuffled);\n            __m128i max2 = _mm_or_si128(_mm_and_si128(cmp2, max1), _mm_andnot_si128(cmp2, max1_shuffled));\n            uint32_t max_in_chunk = _mm_cvtsi128_si32(max2);\n\n            if (max_in_chunk > global_max) {\n                global_max = max_in_chunk;\n            }\n        }\n    }\n\n    for (; i < strings.size(); ++i) {\n        size_t len = strings[i].size();\n        if (len > global_max) {\n            global_max = len;\n        }\n    }\n\n    for (i = 0; i < strings.size(); ++i) {\n        if (strings[i].size() == global_max) {\n            return strings[i];\n        }\n    }\n\n    return std::string();\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n    \n    int shift_a = 0;\n    while ((a & 1) == 0) {\n        a >>= 1;\n        shift_a++;\n    }\n    int shift_b = 0;\n    while ((b & 1) == 0) {\n        b >>= 1;\n        shift_b++;\n    }\n    int shift = shift_a < shift_b ? shift_a : shift_b;\n\n    while (b != 0) {\n        while ((b & 1) == 0) {\n            b >>= 1;\n        }\n        if (a > b) {\n            uint64_t temp = a;\n            a = b;\n            b = temp;\n        }\n        __m128i va = _mm_cvtsi64_si128(a);\n        __m128i vb = _mm_cvtsi64_si128(b);\n        __m128i vdiff = _mm_sub_epi64(vb, va);\n        b = _mm_cvtsi128_si64(vdiff);\n    }\n    return a << shift;\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    __m128i v = _mm_setr_epi64x(a, b);\n    __m128i mask_low = _mm_setr_epi64x(~0ULL, 0);\n    __m128i mask_high = _mm_setr_epi64x(0, ~0ULL);\n    __m128i one = _mm_set1_epi64x(1);\n    __m128i zero = _mm_setzero_si128();\n    int shift = 0;\n\n    while (1) {\n        __m128i and_res = _mm_and_si128(v, one);\n        __m128i cmp = _mm_cmpeq_epi64(and_res, zero);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) break;\n        v = _mm_srli_epi64(v, 1);\n        shift++;\n    }\n\n    while (1) {\n        __m128i and_res = _mm_and_si128(v, one);\n        uint64_t a_low = _mm_cvtsi128_si64(and_res);\n        if (a_low) break;\n        __m128i shifted = _mm_srli_epi64(v, 1);\n        v = _mm_or_si128(_mm_and_si128(mask_low, shifted), _mm_and_si128(mask_high, v));\n    }\n\n    while (1) {\n        __m128i and_res = _mm_and_si128(v, one);\n        uint64_t b_high = _mm_extract_epi16(and_res, 4) | ((uint64_t)_mm_extract_epi16(and_res, 5) << 16);\n        b_high |= ((uint64_t)_mm_extract_epi16(and_res, 6) << 32) | ((uint64_t)_mm_extract_epi16(and_res, 7) << 48);\n        if (b_high) break;\n        __m128i shifted = _mm_srli_epi64(v, 1);\n        v = _mm_or_si128(_mm_and_si128(mask_high, shifted), _mm_and_si128(mask_low, v));\n    }\n\n    while (1) {\n        uint64_t b_val = _mm_extract_epi16(v, 4) | ((uint64_t)_mm_extract_epi16(v, 5) << 16);\n        b_val |= ((uint64_t)_mm_extract_epi16(v, 6) << 32) | ((uint64_t)_mm_extract_epi16(v, 7) << 48);\n        if (b_val == 0) break;\n        \n        uint64_t a_val = _mm_cvtsi128_si64(v);\n        if (a_val < b_val) {\n            v = _mm_setr_epi64x(b_val, a_val);\n            a_val = b_val;\n        }\n        \n        __m128i new_low = _mm_set_epi64x(0, a_val - b_val);\n        v = _mm_or_si128(_mm_and_si128(mask_low, new_low), _mm_and_si128(mask_high, v));\n        \n        while (1) {\n            uint64_t a_val = _mm_cvtsi128_si64(v);\n            if (a_val & 1) break;\n            __m128i shifted = _mm_srli_epi64(v, 1);\n            v = _mm_or_si128(_mm_and_si128(mask_low, shifted), _mm_and_si128(mask_high, v));\n        }\n    }\n    \n    uint64_t res = _mm_cvtsi128_si64(v);\n    return res << shift;\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n    \n    uint64_t shift = 0;\n    __m128i v = _mm_set_epi64x(b, a);\n    uint64_t data[2];\n    \n    while (1) {\n        _mm_store_si128((__m128i*)data, v);\n        uint64_t a0 = data[0];\n        uint64_t b0 = data[1];\n        \n        if (a0 == 0) {\n            return b0 << shift;\n        }\n        if (b0 == 0) {\n            return a0 << shift;\n        }\n        \n        if ((a0 & 1) == 0 && (b0 & 1) == 0) {\n            v = _mm_srli_epi64(v, 1);\n            shift++;\n        } else if ((a0 & 1) == 0) {\n            __m128i shifted = _mm_srli_epi64(v, 1);\n            __m128i mask = _mm_set_epi64x((uint64_t)-1, 0);\n            v = _mm_or_si128(\n                _mm_and_si128(mask, v),\n                _mm_andnot_si128(mask, shifted)\n            );\n        } else if ((b0 & 1) == 0) {\n            __m128i shifted = _mm_srli_epi64(v, 1);\n            __m128i mask = _mm_set_epi64x(0, (uint64_t)-1);\n            v = _mm_or_si128(\n                _mm_and_si128(mask, v),\n                _mm_andnot_si128(mask, shifted)\n            );\n        } else {\n            if (a0 >= b0) {\n                a0 = (a0 - b0) >> 1;\n                v = _mm_set_epi64x(b0, a0);\n            } else {\n                b0 = (b0 - a0) >> 1;\n                v = _mm_set_epi64x(b0, a0);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "To solve this problem, we need to compute the greatest common divisor (GCD) of two positive integers using SSE/SSE2 intrinsics to leverage parallelism. The approach involves using the binary GCD algorithm (Stein's algorithm), which is efficient for this purpose. The key steps include:\n\n1. **Handling Common Factors of 2**: Both numbers are divided by 2 as long as they are even, counting the number of times this division occurs.\n2. **Making Numbers Odd**: Ensure each number is odd by right-shifting until the least significant bit is set.\n3. **Subtraction and Shifting**: Repeatedly subtract the smaller number from the larger one and shift the result until it becomes odd. This process continues until the numbers become equal.\n\nThe SIMD intrinsics are used to parallelize the handling of common factors of 2. Specifically, we load both numbers into a 128-bit SSE register and perform simultaneous right shifts while both numbers remain even. This parallel processing enhances efficiency.\n\n```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n\n    int shift = 0;\n    __m128i v = _mm_set_epi64x(b, a);\n    __m128i one = _mm_set1_epi64x(1);\n    while (1) {\n        __m128i lowbits = _mm_and_si128(v, one);\n        alignas(16) uint64_t lanes[2];\n        _mm_store_si128((__m128i*)lanes, lowbits);\n        if (lanes[0] == 0 && lanes[1] == 0) {\n            v = _mm_srli_epi64(v, 1);\n            shift++;\n        } else {\n            break;\n        }\n    }\n\n    alignas(16) uint64_t lanes[2];\n    _mm_store_si128((__m128i*)lanes, v);\n    uint64_t a_val = lanes[0];\n    uint64_t b_val = lanes[1];\n\n    while ((a_val & 1) == 0) a_val >>= 1;\n    while ((b_val & 1) == 0) b_val >>= 1;\n\n    while (a_val != b_val) {\n        if (a_val > b_val) {\n            a_val = a_val - b_val;\n            while ((a_val & 1) == 0) a_val >>= 1;\n        } else {\n            b_val = b_val - a_val;\n            while ((b_val & 1) == 0) b_val >>= 1;\n        }\n    }\n\n    return a_val << shift;\n}\n```\n\nThis implementation efficiently computes the GCD by using SIMD operations to handle common factors of 2 in parallel, followed by scalar operations for the remaining steps. The solution ensures correctness while leveraging parallelism for improved performance."}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    if (a == 0) return b;\n    if (b == 0) return a;\n    int tz_a = __builtin_ctzll(a);\n    int tz_b = __builtin_ctzll(b);\n    int shift = tz_a < tz_b ? tz_a : tz_b;\n    __m128i v = _mm_set_epi64x(a, b);\n    __m128i count = _mm_set_epi64x(shift, shift);\n    __m128i shifted = _mm_srl_epi64(v, count);\n    uint64_t arr[2];\n    _mm_store_si128((__m128i*)arr, shifted);\n    a = arr[1];\n    b = arr[0];\n    a >>= __builtin_ctzll(a);\n    b >>= __builtin_ctzll(b);\n    while (b != 0) {\n        if (a > b) {\n            uint64_t t = a;\n            a = b;\n            b = t;\n        }\n        b = b - a;\n        if (b == 0) break;\n        b >>= __builtin_ctzll(b);\n    }\n    return a << shift;\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <algorithm>\n#include <cfloat>\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n    std::sort(numbers.begin(), numbers.end());\n    float min_diff = FLT_MAX;\n    int min_index = 0;\n    int i = 0;\n    for (; i <= static_cast<int>(numbers.size()) - 5; i += 4) {\n        __m128 a = _mm_loadu_ps(&numbers[i]);\n        __m128 b = _mm_loadu_ps(&numbers[i+1]);\n        __m128 diff = _mm_sub_ps(b, a);\n        float diff_arr[4];\n        _mm_storeu_ps(diff_arr, diff);\n        for (int k = 0; k < 4; ++k) {\n            if (diff_arr[k] < min_diff) {\n                min_diff = diff_arr[k];\n                min_index = i + k;\n            }\n        }\n    }\n    for (; i < static_cast<int>(numbers.size()) - 1; ++i) {\n        float diff = numbers[i+1] - numbers[i];\n        if (diff < min_diff) {\n            min_diff = diff;\n            min_index = i;\n        }\n    }\n    return {numbers[min_index], numbers[min_index+1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <cfloat>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    int n = numbers.size();\n    if (n == 2) {\n        return {numbers[0], numbers[1]};\n    }\n    float min_diff = FLT_MAX;\n    int min_index = -1;\n    int i = 0;\n    for (; i <= n - 5; i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]);\n        __m128 next = _mm_loadu_ps(&numbers[i+1]);\n        __m128 diff = _mm_sub_ps(next, current);\n        float d[4];\n        _mm_storeu_ps(d, diff);\n        for (int j = 0; j < 4; j++) {\n            if (d[j] < min_diff) {\n                min_diff = d[j];\n                min_index = i + j;\n            }\n        }\n    }\n    for (; i < n - 1; i++) {\n        float d = numbers[i+1] - numbers[i];\n        if (d < min_diff) {\n            min_diff = d;\n            min_index = i;\n        }\n    }\n    return {numbers[min_index], numbers[min_index+1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <cfloat>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    size_t n = numbers.size();\n    float global_min_diff = FLT_MAX;\n    size_t global_min_index = 0;\n    size_t i = 0;\n\n    for (; i < n - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&numbers[i]);\n        __m128 b = _mm_loadu_ps(&numbers[i+1]);\n        __m128 diff = _mm_sub_ps(b, a);\n        \n        __m128 global_min_vec = _mm_set1_ps(global_min_diff);\n        __m128 mask = _mm_cmplt_ps(diff, global_min_vec);\n        int mask_int = _mm_movemask_ps(mask);\n        \n        if (mask_int) {\n            float diffs[4];\n            _mm_storeu_ps(diffs, diff);\n            \n            for (int j = 0; j < 4; j++) {\n                if (diffs[j] < global_min_diff) {\n                    global_min_diff = diffs[j];\n                    global_min_index = i + j;\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        float diff = numbers[i+1] - numbers[i];\n        if (diff < global_min_diff) {\n            global_min_diff = diff;\n            global_min_index = i;\n        }\n    }\n\n    return { numbers[global_min_index], numbers[global_min_index+1] };\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    std::sort(numbers.begin(), numbers.end());\n    int n = numbers.size();\n    std::vector<float> diffs(n - 1);\n    float* v = numbers.data();\n    float* d = diffs.data();\n\n    int i = 0;\n    for (; i <= n - 5; i += 4) {\n        __m128 a = _mm_loadu_ps(v + i);\n        __m128 b = _mm_loadu_ps(v + i + 1);\n        __m128 diff = _mm_sub_ps(b, a);\n        _mm_storeu_ps(d + i, diff);\n    }\n\n    for (; i < n - 1; ++i) {\n        diffs[i] = v[i + 1] - v[i];\n    }\n\n    float min_diff = diffs[0];\n    int min_index = 0;\n    for (i = 1; i < n - 1; ++i) {\n        if (diffs[i] < min_diff) {\n            min_diff = diffs[i];\n            min_index = i;\n        }\n    }\n\n    return {v[min_index], v[min_index + 1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <cfloat>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() == 2) {\n        return numbers;\n    }\n    std::sort(numbers.begin(), numbers.end());\n    size_t n = numbers.size();\n    size_t num_diffs = n - 1;\n    std::vector<float> diffs(num_diffs);\n    float* num_ptr = numbers.data();\n    float* diff_ptr = diffs.data();\n    size_t i = 0;\n\n    for (; i <= num_diffs - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(num_ptr + i);\n        __m128 v2 = _mm_loadu_ps(num_ptr + i + 1);\n        __m128 d = _mm_sub_ps(v2, v1);\n        _mm_storeu_ps(diff_ptr + i, d);\n    }\n    for (; i < num_diffs; i++) {\n        diffs[i] = numbers[i+1] - numbers[i];\n    }\n\n    float min_diff = FLT_MAX;\n    __m128 minVec = _mm_set1_ps(FLT_MAX);\n    i = 0;\n    for (; i <= num_diffs - 4; i += 4) {\n        __m128 diffVec = _mm_loadu_ps(diff_ptr + i);\n        minVec = _mm_min_ps(minVec, diffVec);\n    }\n    float minArr[4];\n    _mm_storeu_ps(minArr, minVec);\n    for (int j = 0; j < 4; j++) {\n        if (minArr[j] < min_diff) {\n            min_diff = minArr[j];\n        }\n    }\n    for (; i < num_diffs; i++) {\n        if (diffs[i] < min_diff) {\n            min_diff = diffs[i];\n        }\n    }\n\n    size_t min_index = 0;\n    for (i = 0; i < num_diffs; i++) {\n        if (diffs[i] == min_diff) {\n            min_index = i;\n            break;\n        }\n    }\n\n    return { numbers[min_index], numbers[min_index+1] };\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    std::vector<float> result = numbers;\n    size_t n = result.size();\n    if (n == 0) {\n        return result;\n    }\n\n    float min_val = result[0];\n    float max_val = result[0];\n    size_t i = 0;\n\n    if (n >= 4) {\n        __m128 min_vec = _mm_loadu_ps(&result[0]);\n        __m128 max_vec = min_vec;\n        i = 4;\n\n        for (; i <= n - 4; i += 4) {\n            __m128 chunk = _mm_loadu_ps(&result[i]);\n            min_vec = _mm_min_ps(min_vec, chunk);\n            max_vec = _mm_max_ps(max_vec, chunk);\n        }\n\n        __m128 tmp = _mm_shuffle_ps(min_vec, min_vec, _MM_SHUFFLE(2, 3, 0, 1));\n        min_vec = _mm_min_ps(min_vec, tmp);\n        tmp = _mm_shuffle_ps(min_vec, min_vec, _MM_SHUFFLE(1, 0, 3, 2));\n        min_vec = _mm_min_ps(min_vec, tmp);\n        _mm_store_ss(&min_val, min_vec);\n\n        tmp = _mm_shuffle_ps(max_vec, max_vec, _MM_SHUFFLE(2, 3, 0, 1));\n        max_vec = _mm_max_ps(max_vec, tmp);\n        tmp = _mm_shuffle_ps(max_vec, max_vec, _MM_SHUFFLE(1, 0, 3, 2));\n        max_vec = _mm_max_ps(max_vec, tmp);\n        _mm_store_ss(&max_val, max_vec);\n    }\n\n    for (; i < n; i++) {\n        if (result[i] < min_val) min_val = result[i];\n        if (result[i] > max_val) max_val = result[i];\n    }\n\n    float scale;\n    if (max_val == min_val) {\n        scale = 0.0f;\n    } else {\n        scale = 1.0f / (max_val - min_val);\n    }\n\n    __m128 min_broadcast = _mm_set1_ps(min_val);\n    __m128 scale_broadcast = _mm_set1_ps(scale);\n    i = 0;\n\n    if (n >= 4) {\n        for (; i <= n - 4; i += 4) {\n            __m128 chunk = _mm_loadu_ps(&result[i]);\n            chunk = _mm_sub_ps(chunk, min_broadcast);\n            chunk = _mm_mul_ps(chunk, scale_broadcast);\n            _mm_storeu_ps(&result[i], chunk);\n        }\n    }\n\n    for (; i < n; i++) {\n        result[i] = (result[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    std::vector<float> result = numbers;\n    int n = result.size();\n    if (n == 0) {\n        return result;\n    }\n\n    float min_val = FLT_MAX;\n    float max_val = -FLT_MAX;\n\n    __m128 vmin = _mm_set1_ps(FLT_MAX);\n    __m128 vmax = _mm_set1_ps(-FLT_MAX);\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&result[i]);\n        vmin = _mm_min_ps(vmin, chunk);\n        vmax = _mm_max_ps(vmax, chunk);\n    }\n\n    alignas(16) float min_vals[4];\n    alignas(16) float max_vals[4];\n    _mm_store_ps(min_vals, vmin);\n    _mm_store_ps(max_vals, vmax);\n\n    for (int j = 0; j < 4; j++) {\n        if (min_vals[j] < min_val) min_val = min_vals[j];\n        if (max_vals[j] > max_val) max_val = max_vals[j];\n    }\n\n    for (; i < n; i++) {\n        if (result[i] < min_val) min_val = result[i];\n        if (result[i] > max_val) max_val = result[i];\n    }\n\n    if (min_val == max_val) {\n        for (float &x : result) {\n            x = 0.0f;\n        }\n        return result;\n    }\n\n    float scale = 1.0f / (max_val - min_val);\n    i = 0;\n    __m128 vmin_vec = _mm_set1_ps(min_val);\n    __m128 vscale_vec = _mm_set1_ps(scale);\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&result[i]);\n        vec = _mm_sub_ps(vec, vmin_vec);\n        vec = _mm_mul_ps(vec, vscale_vec);\n        _mm_storeu_ps(&result[i], vec);\n    }\n\n    for (; i < n; i++) {\n        result[i] = (result[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return numbers;\n    }\n    if (n == 1) {\n        numbers[0] = 0.5f;\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    size_t i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&numbers[i]);\n        vmin = _mm_min_ps(vmin, chunk);\n        vmax = _mm_max_ps(vmax, chunk);\n    }\n\n    float min_arr[4] __attribute__((aligned(16)));\n    float max_arr[4] __attribute__((aligned(16)));\n    _mm_store_ps(min_arr, vmin);\n    _mm_store_ps(max_arr, vmax);\n    for (int j = 0; j < 4; j++) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < n; i++) {\n        if (numbers[i] < min_val) min_val = numbers[i];\n        if (numbers[i] > max_val) max_val = numbers[i];\n    }\n\n    if (min_val == max_val) {\n        for (float &num : numbers) {\n            num = 0.5f;\n        }\n        return numbers;\n    }\n\n    float factor = 1.0f / (max_val - min_val);\n    __m128 vmin_bc = _mm_set1_ps(min_val);\n    __m128 vfactor = _mm_set1_ps(factor);\n    i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&numbers[i]);\n        chunk = _mm_sub_ps(chunk, vmin_bc);\n        chunk = _mm_mul_ps(chunk, vfactor);\n        _mm_storeu_ps(&numbers[i], chunk);\n    }\n\n    for (; i < n; i++) {\n        numbers[i] = (numbers[i] - min_val) * factor;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return numbers;\n    \n    float min_val = numbers[0];\n    float max_val = numbers[0];\n    size_t i = 0;\n    \n    if (n >= 4) {\n        __m128 min_reg = _mm_loadu_ps(&numbers[0]);\n        __m128 max_reg = min_reg;\n        \n        for (i = 4; i <= n - 4; i += 4) {\n            __m128 data = _mm_loadu_ps(&numbers[i]);\n            min_reg = _mm_min_ps(min_reg, data);\n            max_reg = _mm_max_ps(max_reg, data);\n        }\n        \n        alignas(16) float min_buf[4], max_buf[4];\n        _mm_store_ps(min_buf, min_reg);\n        _mm_store_ps(max_buf, max_reg);\n        \n        for (int j = 0; j < 4; ++j) {\n            min_val = std::min(min_val, min_buf[j]);\n            max_val = std::max(max_val, max_buf[j]);\n        }\n    }\n    \n    for (; i < n; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n    \n    if (min_val == max_val) {\n        std::fill(numbers.begin(), numbers.end(), 0.5f);\n        return numbers;\n    }\n    \n    float scale = 1.0f / (max_val - min_val);\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 scale_vec = _mm_set1_ps(scale);\n    \n    for (i = 0; i <= n - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&numbers[i]);\n        data = _mm_sub_ps(data, min_vec);\n        data = _mm_mul_ps(data, scale_vec);\n        _mm_storeu_ps(&numbers[i], data);\n    }\n    \n    for (; i < n; ++i) {\n        numbers[i] = (numbers[i] - min_val) * scale;\n    }\n    \n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    if (n < 4) {\n        for (size_t i = 1; i < n; i++) {\n            if (numbers[i] < min_val) min_val = numbers[i];\n            if (numbers[i] > max_val) max_val = numbers[i];\n        }\n    } else {\n        __m128 vmin = _mm_loadu_ps(&numbers[0]);\n        __m128 vmax = vmin;\n        size_t i = 4;\n        for (; i <= n - 4; i += 4) {\n            __m128 chunk = _mm_loadu_ps(&numbers[i]);\n            vmin = _mm_min_ps(vmin, chunk);\n            vmax = _mm_max_ps(vmax, chunk);\n        }\n        float min_temp[4];\n        float max_temp[4];\n        _mm_storeu_ps(min_temp, vmin);\n        _mm_storeu_ps(max_temp, vmax);\n        min_val = min_temp[0];\n        max_val = max_temp[0];\n        for (int j = 1; j < 4; j++) {\n            if (min_temp[j] < min_val) min_val = min_temp[j];\n            if (max_temp[j] > max_val) max_val = max_temp[j];\n        }\n        for (; i < n; i++) {\n            if (numbers[i] < min_val) min_val = numbers[i];\n            if (numbers[i] > max_val) max_val = numbers[i];\n        }\n    }\n\n    if (min_val == max_val) {\n        for (size_t i = 0; i < n; i++) {\n            numbers[i] = 0.5f;\n        }\n        return numbers;\n    }\n\n    float scale = 1.0f / (max_val - min_val);\n    size_t i = 0;\n\n    if (n >= 4) {\n        __m128 vmin_vec = _mm_set1_ps(min_val);\n        __m128 vscale_vec = _mm_set1_ps(scale);\n        for (; i <= n - 4; i += 4) {\n            __m128 chunk = _mm_loadu_ps(&numbers[i]);\n            chunk = _mm_sub_ps(chunk, vmin_vec);\n            chunk = _mm_mul_ps(chunk, vscale_vec);\n            _mm_storeu_ps(&numbers[i], chunk);\n        }\n    }\n\n    for (; i < n; i++) {\n        numbers[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n == 1) {\n        return 1;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n\n    uint64_t sqrt_n = static_cast<uint64_t>(std::sqrt(n));\n    if (n <= 0xFFFFFFFF) {\n        uint32_t n32 = static_cast<uint32_t>(n);\n        uint32_t d = 3;\n        while (d <= sqrt_n) {\n            __m128d n_vec = _mm_set1_pd(static_cast<double>(n32));\n            __m128d divisors1 = _mm_set_pd(static_cast<double>(d+2), static_cast<double>(d));\n            __m128d quot1 = _mm_div_pd(n_vec, divisors1);\n            __m128i quot1_int = _mm_cvttpd_epi32(quot1);\n            __m128d quot1_d = _mm_cvtepi32_pd(quot1_int);\n            __m128d product1 = _mm_mul_pd(quot1_d, divisors1);\n            __m128d rem1 = _mm_sub_pd(n_vec, product1);\n\n            __m128d divisors2 = _mm_set_pd(static_cast<double>(d+6), static_cast<double>(d+4));\n            __m128d quot2 = _mm_div_pd(n_vec, divisors2);\n            __m128i quot2_int = _mm_cvttpd_epi32(quot2);\n            __m128d quot2_d = _mm_cvtepi32_pd(quot2_int);\n            __m128d product2 = _mm_mul_pd(quot2_d, divisors2);\n            __m128d rem2 = _mm_sub_pd(n_vec, product2);\n\n            __m128d zero = _mm_setzero_pd();\n            int mask1 = _mm_movemask_pd(_mm_cmpeq_pd(rem1, zero));\n            int mask2 = _mm_movemask_pd(_mm_cmpeq_pd(rem2, zero));\n\n            if (mask1 & 1) return n / d;\n            if (mask1 & 2) return n / (d+2);\n            if (mask2 & 1) return n / (d+4);\n            if (mask2 & 2) return n / (d+6);\n            d += 8;\n        }\n    } else {\n        uint64_t d = 3;\n        while (d <= sqrt_n) {\n            if (n % d == 0) return n / d;\n            if (n % (d+2) == 0) return n / (d+2);\n            if (n % (d+4) == 0) return n / (d+4);\n            if (n % (d+6) == 0) return n / (d+6);\n            d += 8;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n    if (n == 1) {\n        return 0;\n    }\n    if (n == 3) {\n        return 1;\n    }\n    __m128i zero = _mm_setzero_si128();\n    uint64_t sqrt_n = (uint64_t)sqrt(n);\n    for (uint64_t d = 3; d <= sqrt_n; d += 4) {\n        if (n % d == 0) {\n            return n / d;\n        }\n        if (n % (d + 2) == 0) {\n            return n / (d + 2);\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n\n    uint64_t root = (uint64_t)sqrt((double)n);\n    if (root > 0 && root > n / root) {\n        root--;\n    }\n    if (root + 1 <= n / (root + 1)) {\n        root++;\n    }\n\n    uint32_t n_high = (uint32_t)(n >> 32);\n    uint32_t n_low = (uint32_t)(n);\n\n    for (uint64_t base = 3; base <= root; base += 4) {\n        uint64_t d1 = base;\n        uint64_t d2 = base + 2;\n\n        if (d2 > root) {\n            if (n % d1 == 0) {\n                return n / d1;\n            }\n            break;\n        }\n\n        uint64_t t1 = (0x100000000ULL) % d1;\n        uint64_t t2 = (0x100000000ULL) % d2;\n\n        __m128i a_vec = _mm_set1_epi32(n_high);\n        __m128i t_vec = _mm_set_epi64x(t2, t1);\n        __m128i product = _mm_mul_epu32(a_vec, t_vec);\n        __m128i b_vec = _mm_set1_epi64x(n_low);\n        __m128i sum = _mm_add_epi64(product, b_vec);\n\n        uint64_t temp_arr[2];\n        _mm_storeu_si128((__m128i*)temp_arr, sum);\n        uint64_t temp1 = temp_arr[0];\n        uint64_t temp2 = temp_arr[1];\n\n        if (temp1 % d1 == 0) {\n            return n / d1;\n        }\n        if (temp2 % d2 == 0) {\n            return n / d2;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n == 1) {\n        return 0;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    uint64_t d = 3;\n    while (d <= sqrt_n) {\n        __m128i divs = _mm_set_epi64x(d+2, d);\n        uint64_t d0 = (uint64_t)_mm_cvtsi128_si64(divs);\n        __m128i shifted = _mm_srli_si128(divs, 8);\n        uint64_t d1 = (uint64_t)_mm_cvtsi128_si64(shifted);\n\n        if (d0 <= sqrt_n && n % d0 == 0) {\n            return n / d0;\n        }\n        if (d1 <= sqrt_n && n % d1 == 0) {\n            return n / d1;\n        }\n        d += 4;\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) {\n        return 0;\n    }\n    uint64_t start = n / 2;\n    if (n % start == 0) {\n        return start;\n    }\n    for (uint64_t i = start - 1; i >= 2; i -= 2) {\n        __m128i v_candidate = _mm_set_epi64x(i, i - 1);\n        uint64_t candidates[2];\n        _mm_store_si128((__m128i*)candidates, v_candidate);\n        uint64_t cand_high = candidates[1];\n        uint64_t cand_low = candidates[0];\n        if (n % cand_high == 0) {\n            return cand_high;\n        }\n        if (n % cand_low == 0) {\n            return cand_low;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n    int d = 3;\n    while (n > 1) {\n        if (d > n / d) {\n            factors.push_back(n);\n            break;\n        }\n        double divisors_arr[4] = { (double)d, (double)(d+2), (double)(d+4), (double)(d+6) };\n        double rems_d[4];\n        __m128d n_vec = _mm_set1_pd((double)n);\n        __m128d div1 = _mm_loadu_pd(divisors_arr);\n        __m128d quot1 = _mm_div_pd(n_vec, div1);\n        __m128i quot1_int = _mm_cvttpd_epi32(quot1);\n        __m128d quot1_floor = _mm_cvtepi32_pd(quot1_int);\n        __m128d rem1 = _mm_sub_pd(n_vec, _mm_mul_pd(div1, quot1_floor));\n        _mm_storeu_pd(rems_d, rem1);\n\n        __m128d div2 = _mm_loadu_pd(divisors_arr + 2);\n        __m128d quot2 = _mm_div_pd(n_vec, div2);\n        __m128i quot2_int = _mm_cvttpd_epi32(quot2);\n        __m128d quot2_floor = _mm_cvtepi32_pd(quot2_int);\n        __m128d rem2 = _mm_sub_pd(n_vec, _mm_mul_pd(div2, quot2_floor));\n        _mm_storeu_pd(rems_d + 2, rem2);\n\n        int found_index = -1;\n        for (int i = 0; i < 4; i++) {\n            if (rems_d[i] == 0.0) {\n                found_index = i;\n                break;\n            }\n        }\n        if (found_index != -1) {\n            int factor = d + 2 * found_index;\n            factors.push_back(factor);\n            n /= factor;\n        } else {\n            d += 8;\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        return factors;\n    }\n    int d = 2;\n\n    while (n > 1) {\n        if (static_cast<long long>(d) * d > n) {\n            factors.push_back(n);\n            break;\n        }\n\n        int found = -1;\n        __m128d n_vec = _mm_set1_pd(static_cast<double>(n));\n\n        __m128d divs1 = _mm_set_pd(static_cast<double>(d+1), static_cast<double>(d));\n        __m128d quot1 = _mm_div_pd(n_vec, divs1);\n        __m128i q_int1 = _mm_cvttpd_epi32(quot1);\n        __m128d q_rounded1 = _mm_cvtepi32_pd(q_int1);\n        __m128d product1 = _mm_mul_pd(q_rounded1, divs1);\n        __m128d rem1 = _mm_sub_pd(n_vec, product1);\n        __m128d mask1 = _mm_cmpeq_pd(rem1, _mm_setzero_pd());\n        double mask_arr1[2];\n        _mm_store_pd(mask_arr1, mask1);\n\n        __m128d divs2 = _mm_set_pd(static_cast<double>(d+3), static_cast<double>(d+2));\n        __m128d quot2 = _mm_div_pd(n_vec, divs2);\n        __m128i q_int2 = _mm_cvttpd_epi32(quot2);\n        __m128d q_rounded2 = _mm_cvtepi32_pd(q_int2);\n        __m128d product2 = _mm_mul_pd(q_rounded2, divs2);\n        __m128d rem2 = _mm_sub_pd(n_vec, product2);\n        __m128d mask2 = _mm_cmpeq_pd(rem2, _mm_setzero_pd());\n        double mask_arr2[2];\n        _mm_store_pd(mask_arr2, mask2);\n\n        if (mask_arr1[0] != 0) {\n            found = d;\n        } else if (mask_arr1[1] != 0) {\n            found = d+1;\n        } else if (mask_arr2[0] != 0) {\n            found = d+2;\n        } else if (mask_arr2[1] != 0) {\n            found = d+3;\n        }\n\n        if (found != -1) {\n            factors.push_back(found);\n            n /= found;\n            d = found;\n        } else {\n            d += 4;\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n    if (n == 1) {\n        return factors;\n    }\n    int f = 3;\n    while (n > 1) {\n        if ((long long)f * f > n) {\n            factors.push_back(n);\n            break;\n        }\n        int found = 0;\n        int factor = 0;\n        __m128d n_vec = _mm_set1_pd((double)n);\n        __m128d div1 = _mm_set_pd((double)(f+2), (double)f);\n        __m128d quot1 = _mm_div_pd(n_vec, div1);\n        __m128i quot1_int = _mm_cvttpd_epi32(quot1);\n        __m128d quot1_dbl = _mm_cvtepi32_pd(quot1_int);\n        __m128d product1 = _mm_mul_pd(quot1_dbl, div1);\n        __m128d rem1 = _mm_sub_pd(n_vec, product1);\n        __m128d zero = _mm_setzero_pd();\n        __m128d cmp1 = _mm_cmpeq_pd(rem1, zero);\n        int mask1 = _mm_movemask_pd(cmp1);\n\n        __m128d div2 = _mm_set_pd((double)(f+6), (double)(f+4));\n        __m128d quot2 = _mm_div_pd(n_vec, div2);\n        __m128i quot2_int = _mm_cvttpd_epi32(quot2);\n        __m128d quot2_dbl = _mm_cvtepi32_pd(quot2_int);\n        __m128d product2 = _mm_mul_pd(quot2_dbl, div2);\n        __m128d rem2 = _mm_sub_pd(n_vec, product2);\n        __m128d cmp2 = _mm_cmpeq_pd(rem2, zero);\n        int mask2 = _mm_movemask_pd(cmp2);\n\n        int full_mask = mask1 | (mask2 << 2);\n        if (full_mask) {\n            for (int i = 0; i < 4; i++) {\n                if (full_mask & (1 << i)) {\n                    factor = f + 2 * i;\n                    found = 1;\n                    break;\n                }\n            }\n        }\n        if (found) {\n            factors.push_back(factor);\n            n /= factor;\n            f = factor;\n        } else {\n            f += 8;\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Factor out all 2s\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // If n becomes 1 after factoring 2s\n    if (n == 1) return factors;\n\n    int f = 3;\n    while (f <= n / f) {\n        int candidates[4] = {f, f+2, f+4, f+6};\n\n        // Convert n and divisors to float for SIMD operations\n        __m128 n_vec = _mm_set1_ps(static_cast<float>(n));\n        __m128i div_i = _mm_setr_epi32(candidates[0], candidates[1], candidates[2], candidates[3]);\n        __m128 div_f = _mm_cvtepi32_ps(div_i);\n        \n        // Compute quotient approximations\n        __m128 quot_f = _mm_div_ps(n_vec, div_f);\n        __m128i quot_i = _mm_cvtps_epi32(quot_f);\n        __m128 quot_f_again = _mm_cvtepi32_ps(quot_i);\n        \n        // Compute remainder approximations\n        __m128 product = _mm_mul_ps(quot_f_again, div_f);\n        __m128 rem_f = _mm_sub_ps(n_vec, product);\n        \n        // Extract remainder values\n        float rems[4];\n        _mm_store_ps(rems, rem_f);\n\n        bool maybe_factor = false;\n        for (int i = 0; i < 4; ++i) {\n            if (rems[i] < 1e-5 || rems[i] > candidates[i] - 1e-5) {\n                maybe_factor = true;\n                break;\n            }\n        }\n\n        if (maybe_factor) {\n            for (int i = 0; i < 4; ++i) {\n                int candidate = candidates[i];\n                if (candidate > n / candidate) break; // Avoid overflow and unnecessary checks\n                while (n % candidate == 0) {\n                    factors.push_back(candidate);\n                    n /= candidate;\n                }\n            }\n        }\n\n        f += 8;\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Factor out all 2s\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n    if (n == 1) return factors;\n\n    int d = 3;\n    while (d * d <= n) {\n        if (d > 65535) {\n            if (n % d == 0) {\n                factors.push_back(d);\n                n /= d;\n            } else {\n                d += 2;\n            }\n            continue;\n        }\n\n        if (d + 6 > 65535) {\n            if (n % d == 0) {\n                factors.push_back(d);\n                n /= d;\n            } else {\n                d += 2;\n            }\n            continue;\n        }\n\n        double dn = static_cast<double>(n);\n        __m128d n_vec = _mm_set1_pd(dn);\n\n        // First pack: d and d+2\n        __m128d div1 = _mm_set_pd(static_cast<double>(d+2), static_cast<double>(d));\n        __m128d quot1 = _mm_div_pd(n_vec, div1);\n        __m128i int_trunc1 = _mm_cvttpd_epi32(quot1);\n        __m128d trunc1 = _mm_cvtepi32_pd(int_trunc1);\n        __m128d rem1 = _mm_sub_pd(n_vec, _mm_mul_pd(trunc1, div1));\n\n        // Second pack: d+4 and d+6\n        __m128d div2 = _mm_set_pd(static_cast<double>(d+6), static_cast<double>(d+4));\n        __m128d quot2 = _mm_div_pd(n_vec, div2);\n        __m128i int_trunc2 = _mm_cvttpd_epi32(quot2);\n        __m128d trunc2 = _mm_cvtepi32_pd(int_trunc2);\n        __m128d rem2 = _mm_sub_pd(n_vec, _mm_mul_pd(trunc2, div2));\n\n        // Compare remainders with zero\n        __m128d zero = _mm_setzero_pd();\n        __m128d cmp1 = _mm_cmpeq_pd(rem1, zero);\n        __m128d cmp2 = _mm_cmpeq_pd(rem2, zero);\n        int mask1 = _mm_movemask_pd(cmp1);\n        int mask2 = _mm_movemask_pd(cmp2);\n\n        if (mask1 || mask2) {\n            int found = 0;\n            if (mask1 & 1) {\n                found = d;\n            } else if (mask1 & 2) {\n                found = d + 2;\n            } else if (mask2 & 1) {\n                found = d + 4;\n            } else if (mask2 & 2) {\n                found = d + 6;\n            }\n            factors.push_back(found);\n            n /= found;\n            d = found;\n        } else {\n            d += 8;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> output;\n    if (l.empty()) return output;\n    output.reserve(l.size());\n    \n    size_t i = 0;\n    const size_t n = l.size();\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        int bitmask = _mm_movemask_ps(mask);\n        float temp[4];\n        _mm_storeu_ps(temp, vec);\n        \n        if (bitmask & 1) output.push_back(temp[0]);\n        if (bitmask & 2) output.push_back(temp[1]);\n        if (bitmask & 4) output.push_back(temp[2]);\n        if (bitmask & 8) output.push_back(temp[3]);\n    }\n    \n    for (; i < n; ++i) {\n        if (l[i] > 0.0f) {\n            output.push_back(l[i]);\n        }\n    }\n    return output;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    if (l.empty()) return result;\n    \n    result.reserve(l.size());\n    const size_t num_elements = l.size();\n    const float* data = l.data();\n    size_t i = 0;\n    \n    // Process 4 elements at a time using SSE\n    for (; i + 3 < num_elements; i += 4) {\n        __m128 vec = _mm_loadu_ps(data + i);\n        __m128 cmp = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        int mask = _mm_movemask_ps(cmp);\n        \n        if (mask & 1) result.push_back(l[i]);\n        if (mask & 2) result.push_back(l[i+1]);\n        if (mask & 4) result.push_back(l[i+2]);\n        if (mask & 8) result.push_back(l[i+3]);\n    }\n    \n    // Process remaining elements\n    for (; i < num_elements; ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> res;\n    res.reserve(l.size());\n    size_t i = 0;\n    const size_t simd_size = 4;\n    const size_t n = l.size();\n    \n    for (; i + simd_size <= n; i += simd_size) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 cmp = _mm_cmpgt_ps(v, _mm_setzero_ps());\n        int mask = _mm_movemask_ps(cmp);\n        \n        if (mask & 1) res.push_back(l[i]);\n        if (mask & 2) res.push_back(l[i+1]);\n        if (mask & 4) res.push_back(l[i+2]);\n        if (mask & 8) res.push_back(l[i+3]);\n    }\n    \n    for (; i < n; ++i) {\n        if (l[i] > 0.0f) {\n            res.push_back(l[i]);\n        }\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if (l.empty()) return result;\n\n    result.reserve(l.size());\n    const float* data = l.data();\n    size_t i = 0;\n    size_t n = l.size();\n    const __m128 zero = _mm_setzero_ps();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(data + i);\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        int bits = _mm_movemask_ps(mask);\n\n        if (bits & 1) result.push_back(data[i]);\n        if (bits & 2) result.push_back(data[i + 1]);\n        if (bits & 4) result.push_back(data[i + 2]);\n        if (bits & 8) result.push_back(data[i + 3]);\n    }\n\n    for (; i < n; ++i) {\n        if (data[i] > 0.0f) {\n            result.push_back(data[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> res;\n    if (l.empty()) \n        return res;\n\n    size_t i = 0;\n    const size_t n = l.size();\n    const size_t aligned_end = n & ~3;\n\n    for (; i < aligned_end; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        int bitmask = _mm_movemask_ps(mask);\n        if (bitmask == 0) \n            continue;\n\n        float tmp[4];\n        _mm_storeu_ps(tmp, vec);\n        for (int j = 0; j < 4; j++) {\n            if (bitmask & (1 << j)) {\n                res.push_back(tmp[j]);\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (l[i] > 0) {\n            res.push_back(l[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) \n        return false;\n    if (n == 2) \n        return true;\n    if (n % 2 == 0) \n        return false;\n\n    int64_t d = 3;\n    while (d * d <= n) {\n        __m128i divisor_vec = _mm_set_epi64x(d + 2, d);\n        int64_t d0 = _mm_cvtsi128_si64(divisor_vec);\n        int64_t d1 = _mm_extract_epi64(divisor_vec, 1);\n\n        if (n % d0 == 0 || n % d1 == 0) \n            return false;\n\n        d += 4;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n    if (n % 3 == 0) return n == 3;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t part1_limit = (limit > 0x7FFFFFFF) ? 0x7FFFFFFF : limit;\n\n    for (int64_t i = 5; i <= part1_limit; i += 8) {\n        __m128i div_vec = _mm_setr_epi32(i, i+2, i+4, i+6);\n        __m128i limit_vec = _mm_set1_epi32(part1_limit);\n        __m128i cmp = _mm_cmple_epi32(div_vec, limit_vec);\n\n        uint32_t divisors[4];\n        uint32_t active[4];\n        _mm_store_si128((__m128i*)divisors, div_vec);\n        _mm_store_si128((__m128i*)active, cmp);\n\n        for (int j = 0; j < 4; j++) {\n            if (active[j]) {\n                if (n % divisors[j] == 0) {\n                    return false;\n                }\n            } else {\n                break;\n            }\n        }\n    }\n\n    if (limit > 0x7FFFFFFF) {\n        int64_t start = (0x80000000 % 2 == 0) ? 0x80000001 : 0x80000000;\n        if (start < 5) start = 5;\n        for (int64_t i = start; i <= limit; i += 2) {\n            if (n % i == 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n#include <climits>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n\n    if (n > UINT32_MAX) {\n        uint64_t limit = static_cast<uint64_t>(std::sqrt(static_cast<double>(n))) + 1;\n        for (uint64_t d = 3; d <= limit; d += 2) {\n            if (n % d == 0) return false;\n        }\n        return true;\n    }\n\n    uint32_t n32 = static_cast<uint32_t>(n);\n    uint32_t limit = static_cast<uint32_t>(std::sqrt(static_cast<double>(n32))) + 1;\n\n    for (uint32_t d = 3; d <= limit; d += 8) {\n        __m128i v_d = _mm_setr_epi32(d, d+2, d+4, d+6);\n        __m128i v_n = _mm_set1_epi32(n32);\n        __m128i v_limit_vec = _mm_set1_epi32(limit);\n        __m128i v_mask = _mm_cmplt_epi32(v_d, _mm_set1_epi32(limit+1));\n        v_d = _mm_or_si128(_mm_and_si128(v_mask, v_d), _mm_andnot_si128(v_mask, _mm_set1_epi32(1)));\n\n        __m128d v_d01 = _mm_cvtepi32_pd(v_d);\n        __m128i v_d_high = _mm_srli_si128(v_d, 8);\n        __m128d v_d23 = _mm_cvtepi32_pd(v_d_high);\n        __m128d v_n_d = _mm_set1_pd(static_cast<double>(n32));\n\n        __m128d v_quot01 = _mm_div_pd(v_n_d, v_d01);\n        __m128d v_quot23 = _mm_div_pd(v_n_d, v_d23);\n        __m128i v_quot01_int = _mm_cvttpd_epi32(v_quot01);\n        __m128i v_quot23_int = _mm_cvttpd_epi32(v_quot23);\n\n        int q0 = _mm_cvtsi128_si32(v_quot01_int);\n        int q1 = _mm_cvtsi128_si32(_mm_shuffle_epi32(v_quot01_int, _MM_SHUFFLE(1,1,1,1));\n        int q2 = _mm_cvtsi128_si32(v_quot23_int);\n        int q3 = _mm_cvtsi128_si32(_mm_shuffle_epi32(v_quot23_int, _MM_SHUFFLE(1,1,1,1)));\n        uint32_t divisors[4] = {d, d+2, d+4, d+6};\n        int quotients[4] = {q0, q1, q2, q3};\n\n        for (int i = 0; i < 4; i++) {\n            if (divisors[i] > limit) continue;\n            uint32_t remainder = n32 - divisors[i] * quotients[i];\n            if (remainder == 0) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2) return true;\n    if (n % 2 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt(n);\n    for (int64_t base = 3; base <= limit; base += 4) {\n        __m128i divs = _mm_setr_epi64x(base, base + 2);\n        int64_t candidates[2];\n        _mm_storeu_si128((__m128i*)candidates, divs);\n        for (int i = 0; i < 2; i++) {\n            if (candidates[i] > limit) break;\n            if (n % candidates[i] == 0) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) {\n        return false;\n    }\n    if (n == 2) {\n        return true;\n    }\n    if (n % 2 == 0) {\n        return false;\n    }\n    int64_t limit = (int64_t)sqrt((double)n) + 1;\n    for (int64_t i = 3; i <= limit; i += 8) {\n        uint32_t d0 = (uint32_t)i;\n        uint32_t d1 = (uint32_t)(i+2);\n        uint32_t d2 = (uint32_t)(i+4);\n        uint32_t d3 = (uint32_t)(i+6);\n        __m128i divs = _mm_set_epi32(d3, d2, d1, d0);\n        uint32_t divisors[4];\n        _mm_storeu_si128((__m128i*)divisors, divs);\n        for (int j = 0; j < 4; j++) {\n            if (divisors[j] > limit) {\n                break;\n            }\n            if (n % divisors[j] == 0) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const char* src = s.data();\n    size_t n = s.length();\n    if (n == 0) {\n        return \"\";\n    }\n    bool has_letter = false;\n    size_t i = 0;\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(src + i));\n        __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n        __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n        __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n        __m128i Z_plus1 = _mm_set1_epi8('Z' + 1);\n        __m128i mask_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, a_minus1), _mm_cmplt_epi8(chunk, z_plus1));\n        __m128i mask_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, A_minus1), _mm_cmplt_epi8(chunk, Z_plus1));\n        __m128i mask_letter = _mm_or_si128(mask_lower, mask_upper);\n        if (_mm_movemask_epi8(mask_letter) != 0) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        for (; i < n; ++i) {\n            char c = src[i];\n            if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n\n    if (has_letter) {\n        std::string result = s;\n        char* dest = result.data();\n        i = 0;\n        for (; i + 16 <= n; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(dest + i));\n            __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n            __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n            __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n            __m128i Z_plus1 = _mm_set1_epi8('Z' + 1);\n            __m128i mask_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, a_minus1), _mm_cmplt_epi8(chunk, z_plus1));\n            __m128i mask_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, A_minus1), _mm_cmplt_epi8(chunk, Z_plus1));\n            __m128i mask_letter = _mm_or_si128(mask_lower, mask_upper);\n            __m128i flip = _mm_and_si128(mask_letter, _mm_set1_epi8(0x20));\n            chunk = _mm_xor_si128(chunk, flip);\n            _mm_storeu_si128((__m128i*)(dest + i), chunk);\n        }\n        for (; i < n; ++i) {\n            char c = dest[i];\n            if (c >= 'a' && c <= 'z') {\n                dest[i] = c - 32;\n            } else if (c >= 'A' && c <= 'Z') {\n                dest[i] = c + 32;\n            }\n        }\n        return result;\n    } else {\n        std::string result(n, '\\0');\n        char* dest_ptr = result.data();\n        size_t rounded_down = n & ~(size_t)15;\n        i = 0;\n        for (; i < rounded_down; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(src + i));\n            chunk = _mm_shuffle_epi32(chunk, _MM_SHUFFLE(1, 0, 3, 2));\n            chunk = _mm_shufflelo_epi16(chunk, _MM_SHUFFLE(3, 2, 1, 0));\n            chunk = _mm_shufflehi_epi16(chunk, _MM_SHUFFLE(3, 2, 1, 0));\n            __m128i shift_left = _mm_slli_epi16(chunk, 8);\n            __m128i shift_right = _mm_srli_epi16(chunk, 8);\n            chunk = _mm_or_si128(shift_left, shift_right);\n            chunk = _mm_shuffle_epi32(chunk, _MM_SHUFFLE(1, 0, 3, 2));\n            _mm_storeu_si128((__m128i*)(dest_ptr + n - i - 16), chunk);\n        }\n        for (; i < n; ++i) {\n            dest_ptr[n - 1 - i] = src[i];\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    if (len == 0) return s;\n    \n    bool has_letter = false;\n    __m128i zero = _mm_setzero_si128();\n    __m128i global_letter_mask = zero;\n    size_t i = 0;\n    \n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i gt_A = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1));\n        __m128i lt_Z = _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1));\n        __m128i upper_mask = _mm_and_si128(gt_A, lt_Z);\n        \n        __m128i gt_a = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1));\n        __m128i lt_z = _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1));\n        __m128i lower_mask = _mm_and_si128(gt_a, lt_z);\n        \n        __m128i letter_mask = _mm_or_si128(upper_mask, lower_mask);\n        global_letter_mask = _mm_or_si128(global_letter_mask, letter_mask);\n    }\n    \n    if (_mm_movemask_epi8(global_letter_mask) != 0) {\n        has_letter = true;\n    } else {\n        for (; i < len; ++i) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n    \n    if (has_letter) {\n        std::string result = s;\n        i = 0;\n        __m128i flip = _mm_set1_epi8(0x20);\n        \n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(result.data() + i));\n            __m128i gt_A = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1));\n            __m128i lt_Z = _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1));\n            __m128i upper_mask = _mm_and_si128(gt_A, lt_Z);\n            \n            __m128i gt_a = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1));\n            __m128i lt_z = _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1));\n            __m128i lower_mask = _mm_and_si128(gt_a, lt_z);\n            \n            __m128i letter_mask = _mm_or_si128(upper_mask, lower_mask);\n            __m128i to_flip = _mm_and_si128(letter_mask, flip);\n            chunk = _mm_xor_si128(chunk, to_flip);\n            _mm_storeu_si128((__m128i*)(result.data() + i), chunk);\n        }\n        \n        for (; i < len; ++i) {\n            char c = result[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 32;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 32;\n            }\n        }\n        return result;\n    } else {\n        std::string result = s;\n        for (size_t left = 0, right = len - 1; left < right; ++left, --right) {\n            std::swap(result[left], result[right]);\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n == 0) return s;\n\n    bool has_letter = false;\n    size_t i = 0;\n\n    for (; i + 15 < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i A = _mm_set1_epi8('A' - 1);\n        __m128i Z = _mm_set1_epi8('Z' + 1);\n        __m128i a = _mm_set1_epi8('a' - 1);\n        __m128i z = _mm_set1_epi8('z' + 1);\n\n        __m128i in_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, A), _mm_cmpgt_epi8(Z, chunk));\n        __m128i in_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, a), _mm_cmpgt_epi8(z, chunk));\n        __m128i any_letter = _mm_or_si128(in_upper, in_lower);\n\n        if (_mm_movemask_epi8(any_letter) != 0) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        for (; i < n; ++i) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n\n    if (has_letter) {\n        std::string result = s;\n        i = 0;\n        for (; i + 15 < n; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n            __m128i A = _mm_set1_epi8('A' - 1);\n            __m128i Z = _mm_set1_epi8('Z' + 1);\n            __m128i a = _mm_set1_epi8('a' - 1);\n            __m128i z = _mm_set1_epi8('z' + 1);\n\n            __m128i in_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, A), _mm_cmpgt_epi8(Z, chunk));\n            __m128i in_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, a), _mm_cmpgt_epi8(z, chunk));\n            __m128i is_letter = _mm_or_si128(in_upper, in_lower);\n\n            __m128i flip = _mm_set1_epi8(0x20);\n            __m128i flipped = _mm_xor_si128(chunk, flip);\n            __m128i res = _mm_or_si128(\n                _mm_andnot_si128(is_letter, chunk),\n                _mm_and_si128(is_letter, flipped)\n            );\n\n            _mm_storeu_si128((__m128i*)(result.data() + i), res);\n        }\n\n        for (; i < n; ++i) {\n            char c = result[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 32;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 32;\n            }\n        }\n        return result;\n    } else {\n        std::string result(n, '\\0');\n        size_t r = n;\n        size_t i_write = 0;\n        while (r > 0) {\n            size_t to_read = (r >= 16) ? 16 : r;\n            r -= to_read;\n            if (to_read == 16) {\n                __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + r));\n                alignas(16) uint8_t temp[16];\n                _mm_store_si128((__m128i*)temp, chunk);\n                for (int k = 0; k < 8; ++k) {\n                    std::swap(temp[k], temp[15 - k]);\n                }\n                _mm_storeu_si128((__m128i*)(result.data() + i_write), _mm_load_si128((const __m128i*)temp));\n            } else {\n                for (size_t k = 0; k < to_read; ++k) {\n                    result[i_write + k] = s[r + to_read - 1 - k];\n                }\n            }\n            i_write += to_read;\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n == 0) return s;\n\n    bool has_letter = false;\n    size_t i = 0;\n\n    for (; i + 15 < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i upper_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)),\n            _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1))\n        );\n        __m128i lower_mask = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)),\n            _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1))\n        );\n        __m128i mask = _mm_or_si128(upper_mask, lower_mask);\n        if (_mm_movemask_epi8(mask)) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        for (; i < n; ++i) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n\n    if (has_letter) {\n        std::string result = s;\n        i = 0;\n        for (; i + 15 < n; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n            __m128i upper_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)),\n                _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1))\n            );\n            __m128i lower_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)),\n                _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1))\n            );\n            __m128i mask = _mm_or_si128(upper_mask, lower_mask);\n            __m128i flip = _mm_and_si128(mask, _mm_set1_epi8(0x20));\n            __m128i flipped = _mm_xor_si128(chunk, flip);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), flipped);\n        }\n        for (; i < n; ++i) {\n            char c = s[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 0x20;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 0x20;\n            }\n        }\n        return result;\n    } else {\n        std::string result = s;\n        for (size_t left = 0, right = n - 1; left < right; ++left, --right) {\n            std::swap(result[left], result[right]);\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    bool has_letter = false;\n    size_t len = s.size();\n    size_t i = 0;\n    \n    // Check for any letter in the string\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i gt_A = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1));\n        __m128i lt_Z = _mm_cmpgt_epi8(_mm_set1_epi8('Z' + 1), chunk);\n        __m128i upper = _mm_and_si128(gt_A, lt_Z);\n        __m128i gt_a = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1));\n        __m128i lt_z = _mm_cmpgt_epi8(_mm_set1_epi8('z' + 1), chunk);\n        __m128i lower = _mm_and_si128(gt_a, lt_z);\n        __m128i letter = _mm_or_si128(upper, lower);\n        if (_mm_movemask_epi8(letter) != 0) {\n            has_letter = true;\n            break;\n        }\n    }\n    if (!has_letter) {\n        for (; i < len; ++i) {\n            char c = s[i];\n            if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n                has_letter = true;\n                break;\n            }\n        }\n    }\n    \n    std::string result = s;\n    if (has_letter) {\n        // Flip case of letters\n        i = 0;\n        for (; i + 15 < len; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(result.data() + i));\n            __m128i gt_A = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1));\n            __m128i lt_Z = _mm_cmpgt_epi8(_mm_set1_epi8('Z' + 1), chunk);\n            __m128i upper = _mm_and_si128(gt_A, lt_Z);\n            __m128i gt_a = _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1));\n            __m128i lt_z = _mm_cmpgt_epi8(_mm_set1_epi8('z' + 1), chunk);\n            __m128i lower = _mm_and_si128(gt_a, lt_z);\n            __m128i letter = _mm_or_si128(upper, lower);\n            __m128i flip = _mm_and_si128(letter, _mm_set1_epi8(0x20));\n            chunk = _mm_xor_si128(chunk, flip);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), chunk);\n        }\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            char c = result[i];\n            if (c >= 'A' && c <= 'Z') {\n                result[i] = c + 0x20;\n            } else if (c >= 'a' && c <= 'z') {\n                result[i] = c - 0x20;\n            }\n        }\n    } else {\n        // Reverse the entire string\n        std::reverse(result.begin(), result.end());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) {\n        return l;\n    }\n\n    std::vector<int> result = l;\n    size_t n3 = (n + 2) / 3;\n    std::vector<int> to_sort(n3);\n\n    size_t i_block = 0;\n    size_t i_output = 0;\n    for (; i_block + 11 < n; i_block += 12, i_output += 4) {\n        __m128i v0 = _mm_loadu_si128((__m128i*)(l.data() + i_block));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(l.data() + i_block + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(l.data() + i_block + 8));\n\n        __m128i t0 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(0, 0, 0, 0));\n        __m128i t1 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(3, 3, 3, 3));\n        __m128i t2 = _mm_shuffle_epi32(v1, _MM_SHUFFLE(2, 2, 2, 2));\n        __m128i t3 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(1, 1, 1, 1));\n\n        __m128i mask0 = _mm_set_epi32(0, 0, 0, -1);\n        __m128i mask1 = _mm_set_epi32(0, 0, -1, 0);\n        __m128i mask2 = _mm_set_epi32(0, -1, 0, 0);\n        __m128i mask3 = _mm_set_epi32(-1, 0, 0, 0);\n\n        __m128i combined = _mm_or_si128(\n            _mm_or_si128(\n                _mm_and_si128(t0, mask0),\n                _mm_and_si128(t1, mask1)\n            ),\n            _mm_or_si128(\n                _mm_and_si128(t2, mask2),\n                _mm_and_si128(t3, mask3)\n            )\n        );\n        _mm_storeu_si128((__m128i*)(to_sort.data() + i_output), combined);\n    }\n\n    for (size_t i = i_block; i < n; ++i) {\n        if (i % 3 == 0) {\n            to_sort[i_output++] = l[i];\n        }\n    }\n\n    std::sort(to_sort.begin(), to_sort.end());\n\n    i_output = 0;\n    i_block = 0;\n    for (; i_block + 11 < n; i_block += 12, i_output += 4) {\n        __m128i sorted = _mm_loadu_si128((__m128i*)(to_sort.data() + i_output));\n        __m128i v0 = _mm_loadu_si128((__m128i*)(result.data() + i_block));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(result.data() + i_block + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(result.data() + i_block + 8));\n\n        __m128i s0 = _mm_shuffle_epi32(sorted, _MM_SHUFFLE(0, 0, 0, 0));\n        __m128i mask0 = _mm_set_epi32(0, 0, 0, -1);\n        v0 = _mm_or_si128(_mm_andnot_si128(mask0, v0), _mm_and_si128(s0, mask0));\n\n        __m128i s1 = _mm_shuffle_epi32(sorted, _MM_SHUFFLE(1, 1, 1, 1));\n        __m128i mask1 = _mm_set_epi32(-1, 0, 0, 0);\n        v0 = _mm_or_si128(_mm_andnot_si128(mask1, v0), _mm_and_si128(s1, mask1));\n\n        __m128i s2 = _mm_shuffle_epi32(sorted, _MM_SHUFFLE(2, 2, 2, 2));\n        __m128i mask2 = _mm_set_epi32(0, -1, 0, 0);\n        v1 = _mm_or_si128(_mm_andnot_si128(mask2, v1), _mm_and_si128(s2, mask2));\n\n        __m128i s3 = _mm_shuffle_epi32(sorted, _MM_SHUFFLE(3, 3, 3, 3));\n        __m128i mask3 = _mm_set_epi32(0, 0, -1, 0);\n        v2 = _mm_or_si128(_mm_andnot_si128(mask3, v2), _mm_and_si128(s3, mask3));\n\n        _mm_storeu_si128((__m128i*)(result.data() + i_block), v0);\n        _mm_storeu_si128((__m128i*)(result.data() + i_block + 4), v1);\n        _mm_storeu_si128((__m128i*)(result.data() + i_block + 8), v2);\n    }\n\n    for (size_t i = i_block; i < n; ++i) {\n        if (i % 3 == 0) {\n            result[i] = to_sort[i_output++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) {\n        return l;\n    }\n    int m = (n + 2) / 3;\n    std::vector<int> temp(m);\n    int i = 0, j = 0;\n\n    for (; i + 11 < n; i += 12, j += 4) {\n        __m128i chunk0 = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i chunk1 = _mm_loadu_si128((__m128i*)&l[i+4]);\n        __m128i chunk2 = _mm_loadu_si128((__m128i*)&l[i+8]);\n\n        temp[j]   = _mm_cvtsi128_si32(chunk0);\n        temp[j+1] = _mm_cvtsi128_si32(_mm_srli_si128(chunk0, 12));\n        temp[j+2] = _mm_cvtsi128_si32(_mm_srli_si128(chunk1, 8));\n        temp[j+3] = _mm_cvtsi128_si32(_mm_srli_si128(chunk2, 4));\n    }\n\n    for (; i < n; i += 3) {\n        temp[j++] = l[i];\n    }\n\n    std::sort(temp.begin(), temp.end());\n\n    i = 0; j = 0;\n    static const __m128i mask0 = _mm_setr_epi32(0, -1, -1, 0);\n    static const __m128i mask1 = _mm_setr_epi32(-1, -1, 0, -1);\n    static const __m128i mask2 = _mm_setr_epi32(-1, 0, -1, -1);\n\n    for (; i + 11 < n; i += 12, j += 4) {\n        __m128i chunk0 = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i chunk1 = _mm_loadu_si128((__m128i*)&l[i+4]);\n        __m128i chunk2 = _mm_loadu_si128((__m128i*)&l[i+8]);\n\n        __m128i new_vals0 = _mm_setr_epi32(temp[j], 0, 0, temp[j+1]);\n        __m128i new_vals1 = _mm_setr_epi32(0, 0, temp[j+2], 0);\n        __m128i new_vals2 = _mm_setr_epi32(0, temp[j+3], 0, 0);\n\n        __m128i res0 = _mm_or_si128(_mm_and_si128(chunk0, mask0), new_vals0);\n        __m128i res1 = _mm_or_si128(_mm_and_si128(chunk1, mask1), new_vals1);\n        __m128i res2 = _mm_or_si128(_mm_and_si128(chunk2, mask2), new_vals2);\n\n        _mm_storeu_si128((__m128i*)&l[i], res0);\n        _mm_storeu_si128((__m128i*)&l[i+4], res1);\n        _mm_storeu_si128((__m128i*)&l[i+8], res2);\n    }\n\n    for (; i < n; i += 3) {\n        l[i] = temp[j++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    if (l.empty()) {\n        return l;\n    }\n    std::vector<int> result(l.size());\n    size_t n = l.size();\n    size_t i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), chunk);\n    }\n    for (; i < n; ++i) {\n        result[i] = l[i];\n    }\n    std::vector<int> temp;\n    for (size_t j = 0; j < n; j += 3) {\n        temp.push_back(result[j]);\n    }\n    std::sort(temp.begin(), temp.end());\n    size_t k = 0;\n    for (size_t j = 0; j < n; j += 3) {\n        result[j] = temp[k++];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n == 0) return l;\n    int m = (n + 2) / 3;\n    std::vector<int> temp(m);\n    \n    int i = 0;\n    for (; i + 12 <= n; i += 12) {\n        __m128i v0 = _mm_loadu_si128((__m128i*)(l.data() + i));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(l.data() + i + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(l.data() + i + 8));\n        \n        int block[12];\n        _mm_storeu_si128((__m128i*)block, v0);\n        _mm_storeu_si128((__m128i*)(block + 4), v1);\n        _mm_storeu_si128((__m128i*)(block + 8), v2);\n        \n        int j = i / 3;\n        temp[j]     = block[0];\n        temp[j + 1] = block[3];\n        temp[j + 2] = block[6];\n        temp[j + 3] = block[9];\n    }\n    \n    int j = i / 3;\n    for (; i < n; i++) {\n        if (i % 3 == 0) {\n            temp[j++] = l[i];\n        }\n    }\n    \n    std::sort(temp.begin(), temp.end());\n    \n    i = 0;\n    for (; i + 12 <= n; i += 12) {\n        int j = i / 3;\n        int e0 = temp[j];\n        int e1 = temp[j + 1];\n        int e2 = temp[j + 2];\n        int e3 = temp[j + 3];\n        \n        __m128i v0 = _mm_loadu_si128((__m128i*)(l.data() + i));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(l.data() + i + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(l.data() + i + 8));\n        \n        int block[12];\n        _mm_storeu_si128((__m128i*)block, v0);\n        _mm_storeu_si128((__m128i*)(block + 4), v1);\n        _mm_storeu_si128((__m128i*)(block + 8), v2);\n        \n        block[0] = e0;\n        block[3] = e1;\n        block[6] = e2;\n        block[9] = e3;\n        \n        v0 = _mm_loadu_si128((__m128i*)block);\n        v1 = _mm_loadu_si128((__m128i*)(block + 4));\n        v2 = _mm_loadu_si128((__m128i*)(block + 8));\n        \n        _mm_storeu_si128((__m128i*)(l.data() + i), v0);\n        _mm_storeu_si128((__m128i*)(l.data() + i + 4), v1);\n        _mm_storeu_si128((__m128i*)(l.data() + i + 8), v2);\n    }\n    \n    j = i / 3;\n    for (; i < n; i++) {\n        if (i % 3 == 0) {\n            l[i] = temp[j++];\n        }\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n == 0) return l;\n\n    std::vector<int> extracted;\n    extracted.reserve((n + 2) / 3);\n\n    int i = 0;\n    for (; i + 12 <= n; i += 12) {\n        __m128i v0 = _mm_loadu_si128((__m128i*)(l.data() + i));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(l.data() + i + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(l.data() + i + 8));\n\n        int temp0[4], temp1[4], temp2[4];\n        _mm_storeu_si128((__m128i*)temp0, v0);\n        _mm_storeu_si128((__m128i*)temp1, v1);\n        _mm_storeu_si128((__m128i*)temp2, v2);\n\n        extracted.push_back(temp0[0]);\n        extracted.push_back(temp0[3]);\n        extracted.push_back(temp1[2]);\n        extracted.push_back(temp2[1]);\n    }\n\n    for (int j = i; j < n; j++) {\n        if (j % 3 == 0) {\n            extracted.push_back(l[j]);\n        }\n    }\n\n    std::sort(extracted.begin(), extracted.end());\n\n    int idx = 0;\n    i = 0;\n    for (; i + 12 <= n; i += 12) {\n        __m128i v0 = _mm_loadu_si128((__m128i*)(l.data() + i));\n        __m128i v1 = _mm_loadu_si128((__m128i*)(l.data() + i + 4));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(l.data() + i + 8));\n\n        int temp0[4], temp1[4], temp2[4];\n        _mm_storeu_si128((__m128i*)temp0, v0);\n        _mm_storeu_si128((__m128i*)temp1, v1);\n        _mm_storeu_si128((__m128i*)temp2, v2);\n\n        temp0[0] = extracted[idx++];\n        temp0[3] = extracted[idx++];\n        temp1[2] = extracted[idx++];\n        temp2[1] = extracted[idx++];\n\n        v0 = _mm_loadu_si128((__m128i*)temp0);\n        v1 = _mm_loadu_si128((__m128i*)temp1);\n        v2 = _mm_loadu_si128((__m128i*)temp2);\n\n        _mm_storeu_si128((__m128i*)(l.data() + i), v0);\n        _mm_storeu_si128((__m128i*)(l.data() + i + 4), v1);\n        _mm_storeu_si128((__m128i*)(l.data() + i + 8), v2);\n    }\n\n    for (int j = i; j < n; j++) {\n        if (j % 3 == 0) {\n            l[j] = extracted[idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    const size_t n = l.size();\n    const float* data = l.data();\n\n    if (n == 0) {\n        return 0.0f;\n    }\n\n    if (n < 4) {\n        float max_val = data[0];\n        for (size_t i = 1; i < n; i++) {\n            if (data[i] > max_val) {\n                max_val = data[i];\n            }\n        }\n        return max_val;\n    }\n\n    __m128 max_vector = _mm_loadu_ps(data);\n    size_t i = 4;\n\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(data + i);\n        max_vector = _mm_max_ps(max_vector, chunk);\n    }\n\n    float max_arr[4];\n    _mm_storeu_ps(max_arr, max_vector);\n    float max_val = max_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (max_arr[j] > max_val) {\n            max_val = max_arr[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if (data[i] > max_val) {\n            max_val = data[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return 0.0f;\n    if (n < 4) {\n        float max_val = l[0];\n        for (size_t i = 1; i < n; ++i) {\n            if (l[i] > max_val) max_val = l[i];\n        }\n        return max_val;\n    }\n    __m128 max_vec = _mm_loadu_ps(&l[0]);\n    size_t i = 4;\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, chunk);\n    }\n    __m128 max1 = _mm_shuffle_ps(max_vec, max_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    max1 = _mm_max_ps(max_vec, max1);\n    __m128 max2 = _mm_movehl_ps(max1, max1);\n    max1 = _mm_max_ps(max1, max2);\n    float max_val = _mm_cvtss_f32(max1);\n    for (; i < n; ++i) {\n        if (l[i] > max_val) max_val = l[i];\n    }\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return 0.0f;\n    }\n    __m128 max_vec = _mm_set1_ps(-std::numeric_limits<float>::infinity());\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, chunk);\n    }\n\n    float tmp[4];\n    _mm_storeu_ps(tmp, max_vec);\n    float max_val = tmp[0];\n    if (tmp[1] > max_val) max_val = tmp[1];\n    if (tmp[2] > max_val) max_val = tmp[2];\n    if (tmp[3] > max_val) max_val = tmp[3];\n\n    for (; i < n; i++) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n < 4) {\n        float max_val = l[0];\n        for (int i = 1; i < n; i++) {\n            if (l[i] > max_val) max_val = l[i];\n        }\n        return max_val;\n    }\n\n    __m128 max_v = _mm_loadu_ps(l.data());\n    int i = 4;\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        max_v = _mm_max_ps(max_v, chunk);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(max_v, max_v, _MM_SHUFFLE(2,3,0,1));\n    __m128 max_temp = _mm_max_ps(max_v, shuf);\n    __m128 max_high = _mm_movehl_ps(max_temp, max_temp);\n    __m128 max_reduced = _mm_max_ps(max_temp, max_high);\n    float max_val = _mm_cvtss_f32(max_reduced);\n\n    for (; i < n; i++) {\n        if (l[i] > max_val) max_val = l[i];\n    }\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cfloat>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return 0.0f;\n    }\n    __m128 max_vector;\n    size_t i = 0;\n    if (n >= 4) {\n        max_vector = _mm_loadu_ps(&l[0]);\n        i = 4;\n    } else {\n        float temp[4] = { -FLT_MAX, -FLT_MAX, -FLT_MAX, -FLT_MAX };\n        for (size_t j = 0; j < n; j++) {\n            temp[j] = l[j];\n        }\n        max_vector = _mm_loadu_ps(temp);\n        i = n;\n    }\n\n    for (; i <= n - 4; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        max_vector = _mm_max_ps(max_vector, chunk);\n    }\n\n    if (i < n) {\n        float temp[4] = { -FLT_MAX, -FLT_MAX, -FLT_MAX, -FLT_MAX };\n        size_t num_tail = n - i;\n        for (size_t j = 0; j < num_tail; j++) {\n            temp[j] = l[i + j];\n        }\n        __m128 tail = _mm_loadu_ps(temp);\n        max_vector = _mm_max_ps(max_vector, tail);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(max_vector, max_vector, _MM_SHUFFLE(2,3,0,1));\n    __m128 maxs = _mm_max_ps(max_vector, shuf);\n    __m128 high = _mm_movehl_ps(maxs, maxs);\n    __m128 max_final = _mm_max_ss(maxs, high);\n    float result = _mm_cvtss_f32(max_final);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    for (int i = 0; i < n; i += 4) {\n        __m128i v = _mm_set_epi32(i+3, i+2, i+1, i);\n        for (int j = 0; j < 4; j++) {\n            int num = _mm_extract_epi32(v, j);\n            if (num >= n) \n                continue;\n            if (num % 11 == 0 || num % 13 == 0) {\n                int t = num;\n                do {\n                    if (t % 10 == 7) {\n                        count++;\n                    }\n                    t /= 10;\n                } while (t);\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstatic inline int horizontal_sum(__m128i v) {\n    __m128i shft = _mm_srli_si128(v, 8);\n    __m128i sum1 = _mm_add_epi32(v, shft);\n    __m128i shft2 = _mm_srli_si128(sum1, 4);\n    __m128i sum2 = _mm_add_epi32(sum1, shft2);\n    return _mm_cvtsi128_si32(sum2);\n}\n\nstatic int count_digit7_scalar(int i) {\n    if (i == 0) return 0;\n    int count = 0;\n    while (i) {\n        if (i % 10 == 7) count++;\n        i /= 10;\n    }\n    return count;\n}\n\nstatic __m128i count_digit7_in_vector(__m128i v) {\n    alignas(16) uint32_t vals[4];\n    _mm_store_si128((__m128i*)vals, v);\n    uint32_t counts[4] = {0};\n    for (int j = 0; j < 4; j++) {\n        int num = vals[j];\n        while (num) {\n            if (num % 10 == 7) counts[j]++;\n            num /= 10;\n        }\n    }\n    return _mm_set_epi32(counts[3], counts[2], counts[1], counts[0]);\n}\n\nstatic uint32_t process_multiples(int n, int step) {\n    if (step <= 0 || n <= 0) return 0;\n    uint32_t total_count = 0;\n    int i = 0;\n    for (; i + 3 * step < n; i += 4 * step) {\n        int i0 = i;\n        int i1 = i0 + step;\n        int i2 = i1 + step;\n        int i3 = i2 + step;\n        __m128i vec = _mm_set_epi32(i3, i2, i1, i0);\n        __m128i counts = count_digit7_in_vector(vec);\n        total_count += horizontal_sum(counts);\n    }\n    for (; i < n; i += step) {\n        total_count += count_digit7_scalar(i);\n    }\n    return total_count;\n}\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    uint32_t count = process_multiples(n, 11);\n    count += process_multiples(n, 13);\n    count -= process_multiples(n, 143);\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    int count = 0;\n    for (int i = 0; i < n; i += 4) {\n        __m128i v_int = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_n = _mm_set1_epi32(n);\n        __m128i mask_valid = _mm_cmplt_epi32(v_int, v_n);\n        v_int = _mm_and_si128(v_int, mask_valid);\n\n        __m128d v0 = _mm_cvtepi32_pd(v_int);\n        __m128i v_high = _mm_srli_si128(v_int, 8);\n        __m128d v1 = _mm_cvtepi32_pd(v_high);\n\n        __m128d divisor11 = _mm_set1_pd(11.0);\n        __m128d divisor13 = _mm_set1_pd(13.0);\n\n        __m128d div11_0 = _mm_div_pd(v0, divisor11);\n        __m128d quotient11_0 = _mm_cvtepi32_pd(_mm_cvttpd_epi32(div11_0));\n        __m128d rem11_0 = _mm_sub_pd(v0, _mm_mul_pd(quotient11_0, divisor11));\n        __m128d mask11_0 = _mm_cmpeq_pd(rem11_0, _mm_setzero_pd());\n\n        __m128d div13_0 = _mm_div_pd(v0, divisor13);\n        __m128d quotient13_0 = _mm_cvtepi32_pd(_mm_cvttpd_epi32(div13_0));\n        __m128d rem13_0 = _mm_sub_pd(v0, _mm_mul_pd(quotient13_0, divisor13));\n        __m128d mask13_0 = _mm_cmpeq_pd(rem13_0, _mm_setzero_pd());\n        __m128d mask_div0 = _mm_or_pd(mask11_0, mask13_0);\n\n        __m128d div11_1 = _mm_div_pd(v1, divisor11);\n        __m128d quotient11_1 = _mm_cvtepi32_pd(_mm_cvttpd_epi32(div11_1));\n        __m128d rem11_1 = _mm_sub_pd(v1, _mm_mul_pd(quotient11_1, divisor11));\n        __m128d mask11_1 = _mm_cmpeq_pd(rem11_1, _mm_setzero_pd());\n\n        __m128d div13_1 = _mm_div_pd(v1, divisor13);\n        __m128d quotient13_1 = _mm_cvtepi32_pd(_mm_cvttpd_epi32(div13_1));\n        __m128d rem13_1 = _mm_sub_pd(v1, _mm_mul_pd(quotient13_1, divisor13));\n        __m128d mask13_1 = _mm_cmpeq_pd(rem13_1, _mm_setzero_pd());\n        __m128d mask_div1 = _mm_or_pd(mask11_1, mask13_1);\n\n        int mask0 = _mm_movemask_pd(mask_div0);\n        int mask1 = _mm_movemask_pd(mask_div1);\n        int mask_div = mask0 | (mask1 << 2);\n\n        if (mask_div) {\n            int values[4];\n            _mm_storeu_si128((__m128i*)values, v_int);\n            for (int j = 0; j < 4; j++) {\n                if (mask_div & (1 << j)) {\n                    int num = values[j];\n                    int c = 0;\n                    if (num > 0) {\n                        while (num) {\n                            if (num % 10 == 7) {\n                                c++;\n                            }\n                            num /= 10;\n                        }\n                    }\n                    count += c;\n                }\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    int total_count = 0;\n    for (int i = 0; i < n; i += 4) {\n        __m128i v = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_setzero_si128();\n        for (int j = 0; j < 4; j++) {\n            int num = i + j;\n            if (num >= n) continue;\n            if (num % 11 == 0 || num % 13 == 0) {\n                mask = _mm_insert_epi32(mask, -1, j);\n            }\n        }\n        if (_mm_movemask_epi8(mask) {\n            __m128i counts = _mm_setzero_si128();\n            for (int j = 0; j < 4; j++) {\n                int num = i + j;\n                if (num >= n) continue;\n                int digit_count = 0;\n                int value = num;\n                do {\n                    if (value % 10 == 7) digit_count++;\n                    value /= 10;\n                } while (value);\n                counts = _mm_insert_epi32(counts, digit_count, j);\n            }\n            counts = _mm_and_si128(counts, mask);\n            int arr[4];\n            _mm_storeu_si128((__m128i*)arr, counts);\n            for (int j = 0; j < 4; j++) {\n                if (i + j < n) {\n                    total_count += arr[j];\n                }\n            }\n        }\n    }\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstatic int count_digit7(int num) {\n    if (num == 0) return 0;\n    int count = 0;\n    while (num) {\n        if (num % 10 == 7) \n            count++;\n        num /= 10;\n    }\n    return count;\n}\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n    int total = 0;\n    int i;\n    for (i = 0; i < n - 1; i += 2) {\n        __m128i int_vec = _mm_set_epi32(0, 0, i+1, i);\n        __m128d dbl_vec = _mm_cvtepi32_pd(int_vec);\n        \n        __m128d div11 = _mm_set1_pd(11.0);\n        __m128i quot11_int = _mm_cvttpd_epi32(_mm_div_pd(dbl_vec, div11));\n        __m128d quot11 = _mm_cvtepi32_pd(quot11_int);\n        __m128d remd11 = _mm_sub_pd(dbl_vec, _mm_mul_pd(quot11, div11));\n        __m128i remi11 = _mm_cvtpd_epi32(remd11);\n        \n        __m128d div13 = _mm_set1_pd(13.0);\n        __m128i quot13_int = _mm_cvttpd_epi32(_mm_div_pd(dbl_vec, div13));\n        __m128d quot13 = _mm_cvtepi32_pd(quot13_int);\n        __m128d remd13 = _mm_sub_pd(dbl_vec, _mm_mul_pd(quot13, div13));\n        __m128i remi13 = _mm_cvtpd_epi32(remd13);\n        \n        __m128i zero = _mm_setzero_si128();\n        __m128i mask11 = _mm_cmpeq_epi32(remi11, zero);\n        __m128i mask13 = _mm_cmpeq_epi32(remi13, zero);\n        __m128i mask = _mm_or_si128(mask11, mask13);\n        \n        int mask0 = _mm_extract_epi32(mask, 0);\n        int mask1 = _mm_extract_epi32(mask, 1);\n        \n        if (mask0) total += count_digit7(i);\n        if (mask1) total += count_digit7(i+1);\n    }\n    if (i < n) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            total += count_digit7(i);\n        }\n    }\n    return total;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <algorithm>\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n == 0) {\n        return l;\n    }\n\n    int even_size = (n + 1) / 2;\n    std::vector<float> evens(even_size);\n\n    for (int i = 0; i < n; i += 4) {\n        if (i + 3 < n) {\n            __m128 v = _mm_loadu_ps(&l[i]);\n            __m128 t = _mm_shuffle_ps(v, v, _MM_SHUFFLE(0, 0, 2, 0));\n            _mm_storel_pi(reinterpret_cast<__m64*>(&evens[i/2]), t);\n        } else {\n            for (int j = i; j < n; j++) {\n                if (j % 2 == 0) {\n                    evens[j/2] = l[j];\n                }\n            }\n        }\n    }\n\n    std::sort(evens.begin(), evens.end());\n\n    std::vector<float> result = l;\n\n    for (int i = 0; i < n; i += 4) {\n        if (i + 3 < n) {\n            __m128 se = _mm_loadl_pi(_mm_setzero_ps(), reinterpret_cast<const __m64*>(&evens[i/2]));\n            __m128 se_expanded = _mm_unpacklo_ps(se, _mm_setzero_ps());\n            __m128i mask_i = _mm_setr_epi32(-1, 0, -1, 0);\n            __m128 mask = _mm_castsi128_ps(mask_i);\n            __m128 v = _mm_loadu_ps(&result[i]);\n            __m128 v_even = _mm_and_ps(mask, se_expanded);\n            __m128 v_odd = _mm_andnot_ps(mask, v);\n            __m128 new_v = _mm_or_ps(v_even, v_odd);\n            _mm_storeu_ps(&result[i], new_v);\n        } else {\n            for (int j = i; j < n; j++) {\n                if (j % 2 == 0) {\n                    result[j] = evens[j/2];\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n == 0) return l;\n    \n    int even_size = (n + 1) / 2;\n    int odd_size = n / 2;\n    std::vector<float> even_arr(even_size);\n    std::vector<float> odd_arr(odd_size);\n    \n    int i = 0;\n    int even_index = 0, odd_index = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 permuted = _mm_shuffle_ps(v, v, _MM_SHUFFLE(3, 1, 2, 0));\n        float tmp[4];\n        _mm_storeu_ps(tmp, permuted);\n        even_arr[even_index++] = tmp[0];\n        even_arr[even_index++] = tmp[1];\n        odd_arr[odd_index++] = tmp[2];\n        odd_arr[odd_index++] = tmp[3];\n    }\n    for (; i < n; i++) {\n        if (i % 2 == 0) {\n            even_arr[even_index++] = l[i];\n        } else {\n            odd_arr[odd_index++] = l[i];\n        }\n    }\n    \n    std::sort(even_arr.begin(), even_arr.end());\n    \n    std::vector<float> result(n);\n    i = 0;\n    even_index = 0;\n    odd_index = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 even_chunk = _mm_loadl_pi(_mm_setzero_ps(), (const __m64*)&even_arr[even_index]);\n        __m128 odd_chunk = _mm_loadl_pi(_mm_setzero_ps(), (const __m64*)&odd_arr[odd_index]);\n        __m128 interleaved = _mm_unpacklo_ps(even_chunk, odd_chunk);\n        _mm_storeu_ps(&result[i], interleaved);\n        even_index += 2;\n        odd_index += 2;\n    }\n    while (even_index < even_arr.size() || odd_index < odd_arr.size()) {\n        if (even_index < even_arr.size()) {\n            result[i++] = even_arr[even_index++];\n        }\n        if (odd_index < odd_arr.size()) {\n            result[i++] = odd_arr[odd_index++];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) {\n        return l;\n    }\n\n    size_t even_count = (n + 1) / 2;\n    size_t odd_count = n / 2;\n    std::vector<float> even(even_count);\n    std::vector<float> odd(odd_count);\n\n    for (size_t i = 0; i < n; i++) {\n        if (i % 2 == 0) {\n            even[i / 2] = l[i];\n        } else {\n            odd[i / 2] = l[i];\n        }\n    }\n\n    std::sort(even.begin(), even.end());\n\n    std::vector<float> result(n);\n    size_t i_vec = 0;\n\n    for (; i_vec + 1 < even_count && i_vec + 1 < odd_count; i_vec += 2) {\n        __m128d even_d = _mm_load_sd((const double*)&even[i_vec]);\n        __m128 even_chunk = _mm_castpd_ps(even_d);\n        __m128d odd_d = _mm_load_sd((const double*)&odd[i_vec]);\n        __m128 odd_chunk = _mm_castpd_ps(odd_d);\n\n        __m128 result_chunk = _mm_unpacklo_ps(even_chunk, odd_chunk);\n        _mm_storeu_ps(&result[2 * i_vec], result_chunk);\n    }\n\n    for (size_t j = i_vec; j < even_count; j++) {\n        result[2 * j] = even[j];\n        if (j < odd_count) {\n            result[2 * j + 1] = odd[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n == 0) \n        return l;\n\n    int even_size = (n + 1) / 2;\n    std::vector<float> even(even_size);\n\n    for (int i = 0; i < n; i += 4) {\n        if (i + 3 < n) {\n            __m128 v = _mm_loadu_ps(&l[i]);\n            __m128 evens = _mm_shuffle_ps(v, v, _MM_SHUFFLE(3, 1, 2, 0));\n            _mm_store_sd(reinterpret_cast<double*>(&even[i/2]), _mm_castps_pd(evens));\n        } else {\n            for (int j = i; j < n; j += 2) {\n                even[j/2] = l[j];\n            }\n        }\n    }\n\n    std::sort(even.begin(), even.end());\n\n    for (int i = 0; i < n; i += 4) {\n        if (i + 3 < n) {\n            __m128 sorted_even = _mm_castpd_ps(_mm_load_sd(reinterpret_cast<double*>(&even[i/2])));\n            __m128 v = _mm_loadu_ps(&l[i]);\n            __m128 shuffled = _mm_shuffle_ps(v, v, _MM_SHUFFLE(3, 1, 2, 0));\n            __m128 odd_low = _mm_movehl_ps(shuffled, shuffled);\n            __m128 result = _mm_unpacklo_ps(sorted_even, odd_low);\n            _mm_storeu_ps(&l[i], result);\n        } else {\n            for (int j = i; j < n; j += 2) {\n                l[j] = even[j/2];\n            }\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    std::vector<float> result = l;\n    int n = l.size();\n    if (n == 0) {\n        return result;\n    }\n    int even_size = (n + 1) / 2;\n    std::vector<float> even(even_size);\n    const __m128 mask = _mm_setr_ps(-1.0f, 0.0f, -1.0f, 0.0f);\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        __m128 evens = _mm_shuffle_ps(chunk, chunk, _MM_SHUFFLE(2, 0, 2, 0));\n        _mm_storel_pd(reinterpret_cast<double*>(&even[i/2]), _mm_castps_pd(evens));\n    }\n    for (; i < n; i += 2) {\n        even[i/2] = l[i];\n    }\n    std::sort(even.begin(), even.end());\n    int j = 0;\n    for (; j + 3 < n; j += 4) {\n        __m128 chunk = _mm_loadu_ps(&result[j]);\n        float val0 = even[j/2];\n        float val1 = even[j/2+1];\n        __m128 sorted_vals = _mm_setr_ps(val0, 0.0f, val1, 0.0f);\n        __m128 zeroed_chunk = _mm_andnot_ps(mask, chunk);\n        sorted_vals = _mm_and_ps(mask, sorted_vals);\n        chunk = _mm_or_ps(zeroed_chunk, sorted_vals);\n        _mm_storeu_ps(&result[j], chunk);\n    }\n    for (; j < n; j += 2) {\n        result[j] = even[j/2];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if (n < 3) \n        return false;\n    std::sort(l.begin(), l.end());\n\n    for (int i = 0; i < n - 2; i++) {\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n\n        while (j < k) {\n            bool updated = false;\n\n            if (k - j >= 4) {\n                __m128i j_vec = _mm_loadu_si128((__m128i*)&l[j]);\n                __m128i k_val = _mm_set1_epi32(l[k]);\n                __m128i sum_vec = _mm_add_epi32(j_vec, k_val);\n                __m128i target_vec = _mm_set1_epi32(target);\n\n                __m128i eq_mask = _mm_cmpeq_epi32(sum_vec, target_vec);\n                if (_mm_movemask_epi8(eq_mask) != 0) {\n                    return true;\n                }\n\n                __m128i lt_mask = _mm_cmplt_epi32(sum_vec, target_vec);\n                if (_mm_movemask_epi8(lt_mask) == 0xFFFF) {\n                    j += 4;\n                    updated = true;\n                }\n            }\n\n            if (updated) \n                continue;\n\n            if (k - j >= 4) {\n                __m128i k_vec = _mm_loadu_si128((__m128i*)&l[k-3]);\n                __m128i j_val = _mm_set1_epi32(l[j]);\n                __m128i sum_vec = _mm_add_epi32(j_val, k_vec);\n                __m128i target_vec = _mm_set1_epi32(target);\n\n                __m128i eq_mask = _mm_cmpeq_epi32(sum_vec, target_vec);\n                if (_mm_movemask_epi8(eq_mask) != 0) {\n                    return true;\n                }\n\n                __m128i gt_mask = _mm_cmpgt_epi32(sum_vec, target_vec);\n                if (_mm_movemask_epi8(gt_mask) == 0xFFFF) {\n                    k -= 4;\n                    updated = true;\n                }\n            }\n\n            if (updated) \n                continue;\n\n            int sum = l[j] + l[k];\n            if (sum < target) {\n                j++;\n            } else if (sum > target) {\n                k--;\n            } else {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    std::sort(l.begin(), l.end());\n\n    for (int i = 0; i < n - 2; i++) {\n        if (i > 0 && l[i] == l[i-1]) continue;\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n\n        while (j < k) {\n            if (j + 3 < k) {\n                __m128i j_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[j]));\n                __m128i k_val = _mm_set1_epi32(l[k]);\n                __m128i sum = _mm_add_epi32(j_vec, k_val);\n                __m128i target_vec = _mm_set1_epi32(target);\n                __m128i cmp_eq = _mm_cmpeq_epi32(sum, target_vec);\n                if (_mm_movemask_epi8(cmp_eq) != 0) {\n                    return true;\n                }\n\n                __m128i cmp_lt = _mm_cmplt_epi32(sum, target_vec);\n                __m128i cmp_gt = _mm_cmpgt_epi32(sum, target_vec);\n                int mask_lt = _mm_movemask_epi8(cmp_lt);\n                int mask_gt = _mm_movemask_epi8(cmp_gt);\n\n                if (mask_lt == 0xFFFF) {\n                    j += 4;\n                } else if (mask_gt) {\n                    k--;\n                } else {\n                    break;\n                }\n            } else {\n                int sum_val = l[j] + l[k];\n                if (sum_val == target) {\n                    return true;\n                } else if (sum_val < target) {\n                    j++;\n                } else {\n                    k--;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n    std::sort(l.begin(), l.end());\n    \n    for (int i = 0; i < n - 2; ++i) {\n        int target = -l[i];\n        int j = i + 1;\n        int k = n - 1;\n        \n        while (j < k) {\n            if (k - j >= 4) {\n                __m128i j_vec = _mm_loadu_si128((__m128i*)&l[j]);\n                __m128i k_val = _mm_set1_epi32(l[k]);\n                __m128i sum_vec = _mm_add_epi32(j_vec, k_val);\n                __m128i target_vec = _mm_set1_epi32(target);\n                \n                __m128i cmp_eq = _mm_cmpeq_epi32(sum_vec, target_vec);\n                if (_mm_movemask_epi8(cmp_eq) != 0) {\n                    return true;\n                }\n                \n                if (l[j+3] + l[k] < target) {\n                    j += 4;\n                    continue;\n                }\n                \n                if (l[j] + l[k] > target) {\n                    --k;\n                    continue;\n                }\n            }\n            \n            int sum = l[j] + l[k];\n            if (sum == target) {\n                return true;\n            }\n            if (sum < target) {\n                ++j;\n            } else {\n                --k;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) {\n        return false;\n    }\n    std::sort(l.begin(), l.end());\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int needed = - (l[i] + l[j]);\n            if (needed < l[j + 1]) {\n                break;\n            }\n            if (needed > l[n - 1]) {\n                continue;\n            }\n            int start = j + 1;\n            int len = n - start;\n            __m128i tgt_vec = _mm_set1_epi32(needed);\n            int idx = 0;\n            for (; idx <= len - 4; idx += 4) {\n                __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[start + idx]));\n                __m128i cmp = _mm_cmpeq_epi32(chunk, tgt_vec);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return true;\n                }\n            }\n            for (; idx < len; ++idx) {\n                if (l[start + idx] == needed) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) {\n        return false;\n    }\n    std::sort(l.begin(), l.end());\n    for (int i = 0; i < n - 2; i++) {\n        int j = i + 1;\n        int k = n - 1;\n        while (j < k) {\n            if (j + 3 < k) {\n                __m128i j_vec = _mm_loadu_si128((__m128i*)&l[j]);\n                __m128i k_val = _mm_set1_epi32(l[k]);\n                __m128i i_val = _mm_set1_epi32(l[i]);\n                __m128i sum = _mm_add_epi32(i_val, _mm_add_epi32(j_vec, k_val));\n                \n                __m128i zero = _mm_setzero_si128();\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n                \n                __m128i sign_neg = _mm_cmplt_epi32(sum, zero);\n                __m128i sign_pos = _mm_cmpgt_epi32(sum, zero);\n                int mask_neg = _mm_movemask_epi8(sign_neg);\n                int mask_pos = _mm_movemask_epi8(sign_pos);\n                \n                if (mask_neg == 0xFFFF) {\n                    j += 4;\n                } else if (mask_pos == 0xFFFF) {\n                    k--;\n                } else {\n                    break;\n                }\n            } else {\n                int s = l[i] + l[j] + l[k];\n                if (s == 0) {\n                    return true;\n                } else if (s < 0) {\n                    j++;\n                } else {\n                    k--;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    if (n >= 4) {\n        __m128i one = _mm_set1_epi32(1);\n        for (; i <= n - 4; i += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i const*)&l[i]);\n            __m128i res_vec = _mm_add_epi32(vec, one);\n            _mm_storeu_si128((__m128i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n    const size_t aligned_end = n - (n % 4);\n    __m128i one = _mm_set1_epi32(1);\n\n    int* input_ptr = l.data();\n    int* output_ptr = result.data();\n\n    for (; i < aligned_end; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(input_ptr + i));\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)(output_ptr + i), vec);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n    \n    if (n >= 4) {\n        __m128i one = _mm_set1_epi32(1);\n        int* src = l.data();\n        int* dst = result.data();\n        \n        for (; i <= n - 4; i += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n            __m128i res = _mm_add_epi32(vec, one);\n            _mm_storeu_si128((__m128i*)(dst + i), res);\n        }\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t i = 0;\n    std::vector<int> result(n);\n    \n    __m128i one = _mm_set1_epi32(1);\n    int* data_in = l.data();\n    int* data_out = result.data();\n\n    for (; i + 3 < n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(data_in + i));\n        __m128i res = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m128i*)(data_out + i), res);\n    }\n\n    for (; i < n; ++i) {\n        data_out[i] = data_in[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t i = 0;\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i result = _mm_add_epi32(data, one);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&l[i]), result);\n    }\n\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) \n        return false;\n    \n    std::sort(l.begin(), l.end());\n    \n    for (int i = 0; i < n; i++) {\n        if (i > 0 && l[i] == l[i-1])\n            continue;\n            \n        int x = l[i];\n        __m128i negx = _mm_set1_epi32(-x);\n        \n        for (int j = 0; j < n; j += 4) {\n            int remaining = n - j;\n            if (remaining < 4) {\n                for (int k = 0; k < remaining; k++) {\n                    if (j + k != i && l[j + k] == -x) {\n                        return true;\n                    }\n                }\n            } else {\n                __m128i chunk = _mm_loadu_si128((__m128i*)(l.data() + j));\n                __m128i cmp = _mm_cmpeq_epi32(negx, chunk);\n                int mask = _mm_movemask_epi8(cmp);\n                \n                if (mask) {\n                    for (int k = 0; k < 4; k++) {\n                        int kth_mask = (mask >> (4 * k)) & 0xF;\n                        if (kth_mask == 0xF && j + k != i) {\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) \n        return false;\n\n    for (int i = 0; i < n-1; i++) {\n        int target_val = -l[i];\n        __m128i target_vec = _mm_set1_epi32(target_val);\n\n        int j = i+1;\n        while (j <= n-4) {\n            __m128i data = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i cmp = _mm_cmpeq_epi32(data, target_vec);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n            j += 4;\n        }\n\n        for (; j < n; j++) {\n            if (l[j] == target_val) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) {\n        return false;\n    }\n    \n    for (int i = 0; i < n; i++) {\n        int a = l[i];\n        int j = i + 1;\n        __m128i va = _mm_set1_epi32(a);\n        \n        for (; j <= n - 4; j += 4) {\n            __m128i vb = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(va, vb);\n            __m128i cmp = _mm_cmpeq_epi32(sum, _mm_setzero_si128());\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n        \n        for (; j < n; j++) {\n            if (a + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) \n        return false;\n\n    for (size_t i = 0; i < n; i++) {\n        __m128i v_i = _mm_set1_epi32(l[i]);\n        size_t j = i + 1;\n\n        for (; j + 3 < n; j += 4) {\n            __m128i v_j = _mm_loadu_si128((__m128i const*)&l[j]);\n            __m128i sum = _mm_add_epi32(v_i, v_j);\n            __m128i mask = _mm_cmpeq_epi32(sum, _mm_setzero_si128());\n            if (_mm_movemask_epi8(mask) != 0) {\n                return true;\n            }\n        }\n\n        for (; j < n; j++) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) \n        return false;\n\n    for (int i = 0; i < n - 1; i++) {\n        int a = l[i];\n        __m128i a_vec = _mm_set1_epi32(a);\n        int j = i + 1;\n        for (; j <= n - 4; j += 4) {\n            __m128i b_vec = _mm_loadu_si128((__m128i*)(&l[j]));\n            __m128i sum = _mm_add_epi32(a_vec, b_vec);\n            __m128i cmp = _mm_cmpeq_epi32(sum, _mm_setzero_si128());\n            if (_mm_movemask_epi8(cmp) != 0) {\n                return true;\n            }\n        }\n        for (; j < n; j++) {\n            if (a + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n\n    uint64_t base2 = static_cast<uint64_t>(base) * base;\n    uint64_t base3 = base2 * base;\n    uint64_t base4 = base3 * base;\n\n    uint8_t rev_buffer[66];\n    int rev_idx = 0;\n\n    uint64_t x_val = static_cast<uint64_t>(x);\n    while (x_val) {\n        uint64_t r;\n        if (x_val < base4) {\n            r = x_val;\n            x_val = 0;\n        } else {\n            r = x_val % base4;\n            x_val = x_val / base4;\n        }\n\n        for (int i = 0; i < 4; i++) {\n            if (r == 0 && x_val == 0) break;\n            rev_buffer[rev_idx++] = r % base;\n            r = r / base;\n        }\n    }\n\n    if (rev_idx == 0) {\n        return \"0\";\n    }\n\n    for (int i = 0; i < rev_idx / 2; i++) {\n        std::swap(rev_buffer[i], rev_buffer[rev_idx - 1 - i]);\n    }\n\n    int start = 0;\n    while (start < rev_idx && rev_buffer[start] == 0) {\n        start++;\n    }\n    if (start == rev_idx) {\n        return \"0\";\n    }\n    int length = rev_idx - start;\n\n    std::string result;\n    result.resize(length);\n\n    __m128i zeros = _mm_set1_epi8('0');\n    int i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i digits = _mm_loadu_si128(reinterpret_cast<__m128i*>(&rev_buffer[start + i]));\n        __m128i ascii = _mm_add_epi8(digits, zeros);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), ascii);\n    }\n\n    for (; i < length; i++) {\n        result[i] = rev_buffer[start + i] + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n\n    char digits_rev[64];\n    int index = 0;\n    int64_t n = x;\n    while (n) {\n        digits_rev[index++] = static_cast<char>(n % base);\n        n = n / base;\n    }\n\n    char digits_forward[64];\n    for (int i = 0; i < index; i++) {\n        digits_forward[i] = digits_rev[index - 1 - i];\n    }\n\n    std::string result;\n    result.resize(index);\n    const int chunk_size = 16;\n    __m128i zero_vec = _mm_set1_epi8('0');\n    int i = 0;\n    for (; i + chunk_size <= index; i += chunk_size) {\n        __m128i digit_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(digits_forward + i));\n        __m128i char_vec = _mm_add_epi8(digit_vec, zero_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), char_vec);\n    }\n    for (; i < index; i++) {\n        result[i] = digits_forward[i] + '0';\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <cstdint>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    \n    uint8_t temp_digits[64];\n    int L = 0;\n    int64_t temp = x;\n    while (temp) {\n        temp_digits[L++] = temp % base;\n        temp /= base;\n    }\n    \n    uint8_t rev_digits[64];\n    for (int i = 0; i < L; i++) {\n        rev_digits[i] = temp_digits[L - 1 - i];\n    }\n    \n    char output_buffer[64];\n    const int chunk_size = 16;\n    __m128i ascii_zero = _mm_set1_epi8('0');\n    int i = 0;\n    for (; i <= L - chunk_size; i += chunk_size) {\n        __m128i digits_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(rev_digits + i));\n        __m128i ascii_vec = _mm_add_epi8(digits_vec, ascii_zero);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output_buffer + i), ascii_vec);\n    }\n    for (; i < L; i++) {\n        output_buffer[i] = '0' + rev_digits[i];\n    }\n    \n    return std::string(output_buffer, L);\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <cstdint>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    if (base < 2 || base > 9) {\n        return \"\";\n    }\n    if (x < 0) {\n        return \"\";\n    }\n\n    std::vector<uint8_t> buffer;\n    int64_t num = x;\n    while (num) {\n        buffer.push_back(static_cast<uint8_t>(num % base));\n        num = num / base;\n    }\n    std::reverse(buffer.begin(), buffer.end());\n\n    std::string res;\n    res.resize(buffer.size());\n    size_t i = 0;\n    size_t n = buffer.size();\n    const uint8_t* buf_data = buffer.data();\n    char* res_data = &res[0];\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i v_digits = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buf_data + i));\n        __m128i v_ascii_zero = _mm_set1_epi8('0');\n        __m128i v_chars = _mm_add_epi8(v_digits, v_ascii_zero);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(res_data + i), v_chars);\n    }\n\n    for (; i < n; ++i) {\n        res_data[i] = buf_data[i] + '0';\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <string>\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) {\n        return \"0\";\n    }\n    if (base < 2 || base > 9) {\n        base = 10;\n    }\n    uint64_t divisor = (uint64_t)base * base * base * base;\n    uint64_t x_unsigned = x;\n    std::vector<char> buffer;\n\n    while (x_unsigned) {\n        uint64_t chunk = x_unsigned % divisor;\n        x_unsigned = x_unsigned / divisor;\n        uint8_t digits[4];\n        digits[0] = chunk % base;\n        chunk /= base;\n        digits[1] = chunk % base;\n        chunk /= base;\n        digits[2] = chunk % base;\n        chunk /= base;\n        digits[3] = chunk;\n\n        __m128i v_digits = _mm_setr_epi8(\n            digits[0], digits[1], digits[2], digits[3],\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n        );\n        __m128i v_zero = _mm_set1_epi8('0');\n        __m128i v_chars = _mm_add_epi8(v_digits, v_zero);\n        char chars_array[16];\n        _mm_storeu_si128((__m128i*)chars_array, v_chars);\n        for (int i = 0; i < 4; i++) {\n            buffer.push_back(chars_array[i]);\n        }\n    }\n\n    std::reverse(buffer.begin(), buffer.end());\n    size_t start = 0;\n    while (start < buffer.size() && buffer[start] == '0') {\n        start++;\n    }\n    if (start == buffer.size()) {\n        return \"0\";\n    }\n    return std::string(buffer.begin() + start, buffer.end());\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    const char* s = text.c_str();\n    size_t n = text.size();\n    size_t left = 0;\n    size_t right = n - 1;\n\n    if (n == 0) {\n        return true;\n    }\n\n    while (left + 30 < right) {\n        __m128i left_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s + left));\n\n        __m128i right_chunk = _mm_set_epi8(\n            s[right - 15], s[right - 14], s[right - 13], s[right - 12],\n            s[right - 11], s[right - 10], s[right - 9], s[right - 8],\n            s[right - 7], s[right - 6], s[right - 5], s[right - 4],\n            s[right - 3], s[right - 2], s[right - 1], s[right]\n        );\n\n        __m128i cmp = _mm_cmpeq_epi8(left_chunk, right_chunk);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n\n        left += 16;\n        right -= 16;\n    }\n\n    while (left < right) {\n        if (s[left] != s[right]) {\n            return false;\n        }\n        left++;\n        right--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.length();\n    if (n == 0) {\n        return true;\n    }\n    size_t count = n / 2;\n    const char* str = text.data();\n    size_t i = 0;\n\n    while (i + 16 <= count) {\n        __m128i left = _mm_loadu_si128((const __m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((const __m128i*)(str + n - 16 - i));\n\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(1, 0, 3, 2));\n        right = _mm_shufflelo_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shufflehi_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i hi_shift = _mm_slli_epi16(right, 8);\n        __m128i lo_shift = _mm_srli_epi16(right, 8);\n        right = _mm_or_si128(hi_shift, lo_shift);\n\n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        unsigned int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n        i += 16;\n    }\n\n    for (; i < count; i++) {\n        if (str[i] != str[n - 1 - i]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.size();\n    if (n == 0) \n        return true;\n\n    const __m128i reverse_mask = _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n    size_t i = 0;\n\n    for (; i + 16 <= n / 2; i += 16) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.data() + i));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.data() + n - i - 16));\n        __m128i reversed_right = _mm_shuffle_epi8(right, reverse_mask);\n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n        unsigned int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) \n            return false;\n    }\n\n    for (; i < n / 2; ++i) {\n        if (text[i] != text[n - 1 - i]) \n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) {\n        return true;\n    }\n\n    size_t half_len = len / 2;\n    size_t num_simd_chunks = half_len / 16;\n    size_t remaining = half_len % 16;\n\n    const __m128i reverse_mask = _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n\n    if (num_simd_chunks > 0) {\n        const char* left_ptr = text.data();\n        const char* right_ptr = text.data() + len - 16;\n\n        for (size_t i = 0; i < num_simd_chunks; i++) {\n            __m128i left_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(left_ptr));\n            __m128i right_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(right_ptr));\n\n            __m128i reversed_right = _mm_shuffle_epi8(right_chunk, reverse_mask);\n            __m128i cmp = _mm_cmpeq_epi8(left_chunk, reversed_right);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFF) {\n                return false;\n            }\n\n            left_ptr += 16;\n            right_ptr -= 16;\n        }\n    }\n\n    const char* left_remaining = text.data() + num_simd_chunks * 16;\n    const char* second_half_remaining = text.data() + half_len;\n\n    for (size_t i = 0; i < remaining; i++) {\n        if (left_remaining[i] != second_half_remaining[remaining - 1 - i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t n = text.size();\n    if (n == 0) {\n        return true;\n    }\n\n    size_t i = 0;\n    size_t end_index = n / 2;\n\n    while (i + 16 <= end_index) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.data() + i));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.data() + n - i - 16));\n\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(1, 0, 3, 2));\n        right = _mm_shufflelo_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shufflehi_epi16(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_or_si128(_mm_slli_epi16(right, 8), _mm_srli_epi16(right, 8));\n\n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        unsigned int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n        i += 16;\n    }\n\n    while (i < end_index) {\n        if (text[i] != text[n - 1 - i]) {\n            return false;\n        }\n        i++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    if (s.empty()) \n        return s;\n    \n    size_t n = s.length();\n    std::string out;\n    out.resize(n);\n    const char* src = s.data();\n    char* dst = &out[0];\n    \n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i add_val = _mm_set1_epi8(21);\n    const __m128i twenty_six = _mm_set1_epi8(26);\n    const __m128i twenty_five = _mm_set1_epi8(25);\n    \n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i shifted = _mm_sub_epi8(vec, base);\n        __m128i y = _mm_add_epi8(shifted, add_val);\n        __m128i mask = _mm_cmpgt_epi8(y, twenty_five);\n        __m128i subtractor = _mm_and_si128(mask, twenty_six);\n        __m128i adjusted = _mm_sub_epi8(y, subtractor);\n        __m128i result = _mm_add_epi8(adjusted, base);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), result);\n    }\n    \n    for (; i < n; ++i) {\n        int temp = (src[i] - 'a' + 21);\n        if (temp >= 26) \n            temp -= 26;\n        dst[i] = static_cast<char>(temp + 'a');\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    size_t len = s.length();\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i a_vec = _mm_set1_epi8('a');\n        __m128i shifted = _mm_sub_epi8(vec, a_vec);\n        __m128i add_val = _mm_set1_epi8(21);\n        __m128i temp = _mm_add_epi8(shifted, add_val);\n        __m128i twentyfive = _mm_set1_epi8(25);\n        __m128i mask = _mm_cmpgt_epi8(temp, twentyfive);\n        __m128i twenty_six = _mm_set1_epi8(26);\n        __m128i adjust = _mm_and_si128(mask, twenty_six);\n        temp = _mm_sub_epi8(temp, adjust);\n        temp = _mm_add_epi8(temp, a_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(out.data() + i), temp);\n    }\n\n    for (; i < len; ++i) {\n        int x = (s[i] - 'a' + 21);\n        if (x >= 26) {\n            x -= 26;\n        }\n        out[i] = static_cast<char>(x + 'a');\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    std::string out(s.length(), '\\0');\n    const size_t len = s.length();\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        vec = _mm_sub_epi8(vec, _mm_set1_epi8(5));\n        __m128i cmp = _mm_cmplt_epi8(vec, _mm_set1_epi8('a'));\n        __m128i adjust = _mm_and_si128(cmp, _mm_set1_epi8(26));\n        vec = _mm_add_epi8(vec, adjust);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(out.data() + i), vec);\n    }\n\n    for (; i < len; ++i) {\n        char c = s[i];\n        c -= 5;\n        if (c < 'a') {\n            c += 26;\n        }\n        out[i] = c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out;\n    out.resize(s.length());\n    size_t n = s.length();\n    size_t i = 0;\n\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i add_val = _mm_set1_epi8(21);\n    const __m128i cmp_threshold = _mm_set1_epi8(25);\n    const __m128i mask_val = _mm_set1_epi8(26);\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i offset = _mm_sub_epi8(vec, base);\n        __m128i x = _mm_add_epi8(offset, add_val);\n        __m128i cmp = _mm_cmpgt_epi8(x, cmp_threshold);\n        __m128i mask = _mm_and_si128(cmp, mask_val);\n        x = _mm_sub_epi8(x, mask);\n        __m128i result = _mm_add_epi8(x, base);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    for (; i < n; ++i) {\n        out[i] = ((s[i] - 'a' + 21) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out;\n    out.resize(len);\n    \n    const __m128i a_vec = _mm_set1_epi8('a');\n    const __m128i twenty_one = _mm_set1_epi8(21);\n    const __m128i twenty_six = _mm_set1_epi8(26);\n    const __m128i twenty_five = _mm_set1_epi8(25);\n    \n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i shifted = _mm_sub_epi8(chunk, a_vec);\n        shifted = _mm_add_epi8(shifted, twenty_one);\n        __m128i mask = _mm_cmpgt_epi8(shifted, twenty_five);\n        __m128i adjust = _mm_and_si128(mask, twenty_six);\n        shifted = _mm_sub_epi8(shifted, adjust);\n        __m128i decoded = _mm_add_epi8(shifted, a_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(out.data() + i), decoded);\n    }\n    \n    for (; i < len; ++i) {\n        int t = (s[i] - 'a' + 21);\n        if (t > 25) t -= 26;\n        out[i] = static_cast<char>(t + 'a');\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    if (l.empty()) {\n        return true;\n    }\n\n    size_t n = l.size();\n    size_t i = 0;\n    __m128i t_vec = _mm_set1_epi32(t);\n\n    for (; i + 3 < n; i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(data, t_vec);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int n = l.size();\n    int i = 0;\n    \n    if (n == 0) {\n        return true;\n    }\n    \n    __m128i thresh_vec = _mm_set1_epi32(t);\n    \n    for (; i <= n - 4; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp_gt = _mm_cmpgt_epi32(data, thresh_vec);\n        __m128i cmp_eq = _mm_cmpeq_epi32(data, thresh_vec);\n        __m128i cmp_ge = _mm_or_si128(cmp_gt, cmp_eq);\n        \n        if (_mm_movemask_ps(_mm_castsi128_ps(cmp_ge)) != 0) {\n            return false;\n        }\n    }\n    \n    for (; i < n; i++) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```c\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    int n = l.size();\n    int i = 0;\n    __m128i thresh = _mm_set1_epi32(t);\n\n    for (; i <= n - 4; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&l[i]);\n        __m128i mask = _mm_cmplt_epi32(data, thresh);\n        int bitmask = _mm_movemask_epi8(mask);\n        if (bitmask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < n; i++) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    if (l.empty()) \n        return true;\n\n    size_t i = 0;\n    const size_t size = l.size();\n    const __m128i threshold = _mm_set1_epi32(t);\n\n    for (; i + 3 < size; i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i mask = _mm_cmpgt_epi32(threshold, data);\n        if (_mm_movemask_epi8(mask) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    if (l.empty())\n        return true;\n    \n    size_t i = 0;\n    const size_t n = l.size();\n    const size_t simd_width = 4;\n    __m128i threshold = _mm_set1_epi32(t);\n    \n    for (; i <= n - simd_width; i += simd_width) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmpgt_epi32(data, threshold);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        if (mask != 0) {\n            return false;\n        }\n    }\n    \n    for (; i < n; ++i) {\n        if (l[i] > t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.size();\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(brackets.data() + i));\n        __m128i v_open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i v_close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n\n        v_open = _mm_and_si128(v_open, _mm_set1_epi8(1));\n        v_close = _mm_and_si128(v_close, _mm_set1_epi8(1));\n        __m128i v_sign = _mm_sub_epi8(v_open, v_close);\n\n        alignas(16) int8_t changes[16];\n        _mm_store_si128((__m128i*)changes, v_sign);\n\n        int min_in_chunk = 0;\n        int current = 0;\n        int total = 0;\n        for (int j = 0; j < 16; j++) {\n            total += changes[j];\n            current += changes[j];\n            if (current < min_in_chunk) {\n                min_in_chunk = current;\n            }\n        }\n\n        if (balance + min_in_chunk < 0) {\n            return false;\n        }\n\n        balance += total;\n    }\n\n    for (; i < len; i++) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n        }\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.length();\n    if (len == 0) {\n        return true;\n    }\n\n    int global_balance = 0;\n    int global_min = 0;\n    size_t i = 0;\n\n    while (i < len) {\n        size_t block_size = len - i;\n        if (block_size > 16) {\n            block_size = 16;\n        }\n\n        char temp[16] = {0};\n        memcpy(temp, brackets.data() + i, block_size);\n        __m128i chunk = _mm_loadu_si128((__m128i*)temp);\n        __m128i open = _mm_set1_epi8('<');\n        __m128i close = _mm_set1_epi8('>');\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i neg_ones = _mm_set1_epi8(-1);\n\n        __m128i mask_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i mask_close = _mm_cmpeq_epi8(chunk, close);\n        __m128i changes8 = _mm_or_si128(_mm_and_si128(mask_open, ones), _mm_and_si128(mask_close, neg_ones));\n\n        char changes[16];\n        _mm_storeu_si128((__m128i*)changes, changes8);\n\n        int block_total = 0;\n        int block_min_val = 0;\n        int current = 0;\n        for (size_t j = 0; j < 16; j++) {\n            current += changes[j];\n            if (current < block_min_val) {\n                block_min_val = current;\n            }\n            block_total += changes[j];\n        }\n\n        if (global_balance + block_min_val < global_min) {\n            global_min = global_balance + block_min_val;\n        }\n        global_balance += block_total;\n\n        if (global_min < 0) {\n            return false;\n        }\n\n        i += block_size;\n    }\n\n    return global_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    size_t i = 0;\n    const size_t n = brackets.size();\n    const size_t simd_size = 16;\n\n    for (; i + simd_size <= n; i += simd_size) {\n        __m128i v = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        __m128i mask_less = _mm_cmpeq_epi8(v, _mm_set1_epi8('<'));\n        __m128i deltas = _mm_add_epi8(\n            _mm_and_si128(mask_less, _mm_set1_epi8(1)),\n            _mm_andnot_si128(mask_less, _mm_set1_epi8(-1))\n        );\n\n        __m128i v0 = deltas;\n        __m128i v1 = _mm_add_epi8(v0, _mm_srli_si128(v0, 1));\n        __m128i v2 = _mm_add_epi8(v1, _mm_srli_si128(v1, 2));\n        __m128i v3 = _mm_add_epi8(v2, _mm_srli_si128(v2, 4));\n        __m128i v4 = _mm_add_epi8(v3, _mm_srli_si128(v3, 8));\n\n        __m128i v4_0 = _mm_cvtepi8_epi32(v4);\n        __m128i v4_1 = _mm_cvtepi8_epi32(_mm_srli_si128(v4, 4));\n        __m128i v4_2 = _mm_cvtepi8_epi32(_mm_srli_si128(v4, 8));\n        __m128i v4_3 = _mm_cvtepi8_epi32(_mm_srli_si128(v4, 12));\n\n        __m128i balance_vec = _mm_set1_epi32(balance);\n        __m128i running0 = _mm_add_epi32(balance_vec, v4_0);\n        __m128i running1 = _mm_add_epi32(balance_vec, v4_1);\n        __m128i running2 = _mm_add_epi32(balance_vec, v4_2);\n        __m128i running3 = _mm_add_epi32(balance_vec, v4_3);\n\n        __m128i min01 = _mm_min_epi32(running0, running1);\n        __m128i min23 = _mm_min_epi32(running2, running3);\n        __m128i min0123 = _mm_min_epi32(min01, min23);\n        __m128i min_shifted = _mm_shuffle_epi32(min0123, _MM_SHUFFLE(2,3,0,1));\n        __m128i min1 = _mm_min_epi32(min0123, min_shifted);\n        min_shifted = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1,0,3,2));\n        __m128i min2 = _mm_min_epi32(min1, min_shifted);\n        int min_val = _mm_extract_epi32(min2, 0);\n\n        if (min_val < 0) {\n            return false;\n        }\n\n        balance = _mm_extract_epi32(running3, 3);\n    }\n\n    for (; i < n; i++) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n        }\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int count = 0;\n    size_t n = brackets.size();\n    const char* data = brackets.data();\n    size_t i = 0;\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i less_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i greater_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n        __m128i v_delta = _mm_sub_epi8(\n            _mm_and_si128(less_mask, _mm_set1_epi8(1)),\n            _mm_and_si128(greater_mask, _mm_set1_epi8(1))\n        );\n\n        int8_t delta_arr[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(delta_arr), v_delta);\n\n        __m128i group0 = _mm_set_epi32(delta_arr[3], delta_arr[2], delta_arr[1], delta_arr[0]);\n        __m128i group1 = _mm_set_epi32(delta_arr[7], delta_arr[6], delta_arr[5], delta_arr[4]);\n        __m128i group2 = _mm_set_epi32(delta_arr[11], delta_arr[10], delta_arr[9], delta_arr[8]);\n        __m128i group3 = _mm_set_epi32(delta_arr[15], delta_arr[14], delta_arr[13], delta_arr[12]);\n\n        __m128i groups[4] = {group0, group1, group2, group3};\n        __m128i v_prev = _mm_set1_epi32(count);\n        __m128i min_vec = _mm_set1_epi32(INT_MAX);\n\n        for (int j = 0; j < 4; j++) {\n            __m128i g = groups[j];\n            __m128i shifted = _mm_slli_si128(g, 4);\n            g = _mm_add_epi32(g, shifted);\n            shifted = _mm_slli_si128(g, 8);\n            g = _mm_add_epi32(g, shifted);\n            g = _mm_add_epi32(g, v_prev);\n            v_prev = _mm_shuffle_epi32(g, _MM_SHUFFLE(3, 3, 3, 3));\n            min_vec = _mm_min_epi32(min_vec, g);\n        }\n\n        __m128i min1 = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1));\n        min_vec = _mm_min_epi32(min_vec, min1);\n        min1 = _mm_shuffle_epi32(min_vec, _MM_SHUFFLE(1, 0, 3, 2));\n        min_vec = _mm_min_epi32(min_vec, min1);\n        int min_in_chunk = _mm_cvtsi128_si32(min_vec);\n\n        if (min_in_chunk < 0) {\n            return false;\n        }\n\n        count = _mm_cvtsi128_si32(v_prev);\n    }\n\n    for (; i < n; i++) {\n        if (data[i] == '<') {\n            count++;\n        } else if (data[i] == '>') {\n            count--;\n        }\n        if (count < 0) {\n            return false;\n        }\n    }\n\n    return count == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int current_balance = 0;\n    size_t i = 0;\n    size_t len = brackets.size();\n    const char* str = brackets.c_str();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i v_open = _mm_cmpeq_epi8(v, _mm_set1_epi8('<'));\n        __m128i v_close = _mm_cmpeq_epi8(v, _mm_set1_epi8('>'));\n\n        v_open = _mm_and_si128(v_open, _mm_set1_epi8(1));\n        v_close = _mm_and_si128(v_close, _mm_set1_epi8(1));\n        __m128i v_delta = _mm_sub_epi8(v_open, v_close);\n\n        int8_t delta_arr[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(delta_arr), v_delta);\n\n        for (int j = 0; j < 16; j++) {\n            current_balance += delta_arr[j];\n            if (current_balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    for (; i < len; i++) {\n        if (str[i] == '<') {\n            current_balance++;\n        } else if (str[i] == '>') {\n            current_balance--;\n        }\n        if (current_balance < 0) {\n            return false;\n        }\n    }\n\n    return current_balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    size_t len = brackets.length();\n    size_t i = 0;\n    while (i < len) {\n        size_t chunk_size = std::min<size_t>(16, len - i);\n        alignas(16) char buffer[16] = {0};\n        std::memcpy(buffer, brackets.data() + i, chunk_size);\n\n        __m128i chunk = _mm_load_si128(reinterpret_cast<const __m128i*>(buffer));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(')'));\n        __m128i change8 = _mm_sub_epi8(close_mask, open_mask);\n\n        alignas(16) signed char changes[16];\n        _mm_store_si128(reinterpret_cast<__m128i*>(changes), change8);\n\n        for (size_t j = 0; j < chunk_size; ++j) {\n            balance += changes[j];\n            if (balance < 0) {\n                return false;\n            }\n        }\n        i += 16;\n    }\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const char* ptr = brackets.c_str();\n    int len = brackets.length();\n    int balance = 0;\n\n    for (int i = 0; i < len; i += 16) {\n        int chunk_len = std::min(16, len - i);\n        __m128i chunk;\n        if (i + 16 <= len) {\n            chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(ptr + i));\n        } else {\n            char temp[16] = {0};\n            for (int j = 0; j < chunk_len; j++) {\n                temp[j] = ptr[i + j];\n            }\n            chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        }\n\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(')'));\n        __m128i values = _mm_sub_epi8(ones, _mm_add_epi8(mask, mask));\n\n        char values_arr[16] __attribute__((aligned(16)));\n        _mm_store_si128(reinterpret_cast<__m128i*>(values_arr), values);\n\n        for (int j = 0; j < chunk_len; j++) {\n            balance += values_arr[j];\n            if (balance < 0) {\n                return false;\n            }\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.length();\n    if (len == 0) return true;\n    int balance = 0;\n    size_t i = 0;\n    const __m128i indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (; i < len; i += 16) {\n        size_t n = len - i;\n        if (n > 16) n = 16;\n\n        const char* ptr = brackets.c_str() + i;\n        __m128i chunk = _mm_loadu_si128((const __m128i*)ptr);\n\n        __m128i mask = _mm_cmplt_epi8(indices, _mm_set1_epi8(n));\n\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(')'));\n        __m128i step8 = _mm_sub_epi8(\n            _mm_and_si128(open_mask, _mm_set1_epi8(1)),\n            _mm_and_si128(close_mask, _mm_set1_epi8(1))\n        );\n        step8 = _mm_and_si128(step8, mask);\n\n        __m128i step16_low = _mm_cvtepi8_epi16(step8);\n        __m128i step16_high = _mm_cvtepi8_epi16(_mm_srli_si128(step8, 8));\n\n        __m128i scan_low = step16_low;\n        scan_low = _mm_add_epi16(scan_low, _mm_slli_si128(scan_low, 2));\n        scan_low = _mm_add_epi16(scan_low, _mm_slli_si128(scan_low, 4));\n        scan_low = _mm_add_epi16(scan_low, _mm_slli_si128(scan_low, 8));\n\n        __m128i scan_high = step16_high;\n        scan_high = _mm_add_epi16(scan_high, _mm_slli_si128(scan_high, 2));\n        scan_high = _mm_add_epi16(scan_high, _mm_slli_si128(scan_high, 4));\n        scan_high = _mm_add_epi16(scan_high, _mm_slli_si128(scan_high, 8));\n\n        int total_low = _mm_extract_epi16(scan_low, 7);\n        scan_high = _mm_add_epi16(scan_high, _mm_set1_epi16(total_low));\n\n        __m128i v1 = _mm_min_epi16(scan_low, _mm_srli_si128(scan_low, 8));\n        v1 = _mm_min_epi16(v1, _mm_srli_si128(v1, 4));\n        v1 = _mm_min_epi16(v1, _mm_srli_si128(v1, 2));\n        int min_low_val = _mm_extract_epi16(v1, 0);\n\n        __m128i v2 = _mm_min_epi16(scan_high, _mm_srli_si128(scan_high, 8));\n        v2 = _mm_min_epi16(v2, _mm_srli_si128(v2, 4));\n        v2 = _mm_min_epi16(v2, _mm_srli_si128(v2, 2));\n        int min_high_val = _mm_extract_epi16(v2, 0);\n\n        int min_prefix = (min_low_val < min_high_val) ? min_low_val : min_high_val;\n\n        if (balance + min_prefix < 0) {\n            return false;\n        }\n\n        int total_chunk = _mm_extract_epi16(scan_high, 7);\n        balance += total_chunk;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.length();\n    if (len == 0) {\n        return true;\n    }\n\n    int balance = 0;\n    size_t i = 0;\n\n    while (i + 16 <= len) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i mask_open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('('));\n        __m128i mask_close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(')'));\n        __m128i change = _mm_sub_epi8(\n            _mm_and_si128(mask_open, _mm_set1_epi8(1)),\n            _mm_and_si128(mask_close, _mm_set1_epi8(1))\n        );\n\n        __m128i low = _mm_cvtepi8_epi16(change);\n        __m128i high = _mm_cvtepi8_epi16(_mm_srli_si128(change, 8));\n\n        __m128i low1 = _mm_add_epi16(low, _mm_slli_si128(low, 2));\n        __m128i low2 = _mm_add_epi16(low1, _mm_slli_si128(low1, 4));\n        __m128i low3 = _mm_add_epi16(low2, _mm_slli_si128(low2, 8));\n        int total_low = (short)_mm_extract_epi16(low3, 7);\n\n        __m128i min1_low = _mm_min_epi16(low3, _mm_srli_si128(low3, 8));\n        __m128i min2_low = _mm_min_epi16(min1_low, _mm_srli_si128(min1_low, 4));\n        __m128i min3_low = _mm_min_epi16(min2_low, _mm_srli_si128(min2_low, 2));\n        int min_val_low = (short)_mm_extract_epi16(min3_low, 0);\n        int min_low = std::min(0, min_val_low);\n\n        __m128i high1 = _mm_add_epi16(high, _mm_slli_si128(high, 2));\n        __m128i high2 = _mm_add_epi16(high1, _mm_slli_si128(high1, 4));\n        __m128i high3 = _mm_add_epi16(high2, _mm_slli_si128(high2, 8));\n        int total_high = (short)_mm_extract_epi16(high3, 7);\n\n        __m128i min1_high = _mm_min_epi16(high3, _mm_srli_si128(high3, 8));\n        __m128i min2_high = _mm_min_epi16(min1_high, _mm_srli_si128(min1_high, 4));\n        __m128i min3_high = _mm_min_epi16(min2_high, _mm_srli_si128(min2_high, 2));\n        int min_val_high = (short)_mm_extract_epi16(min3_high, 0);\n        int min_high = std::min(0, min_val_high);\n\n        int min_chunk = std::min(min_low, total_low + min_high);\n        if (balance + min_chunk < 0) {\n            return false;\n        }\n        balance += total_low + total_high;\n        i += 16;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\ninline __m128i inclusive_scan_epi16(__m128i a) {\n    __m128i shifted1 = _mm_slli_si128(a, 2);\n    __m128i sum1 = _mm_add_epi16(a, shifted1);\n    __m128i shifted2 = _mm_slli_si128(sum1, 4);\n    __m128i sum2 = _mm_add_epi16(sum1, shifted2);\n    __m128i shifted3 = _mm_slli_si128(sum2, 8);\n    __m128i sum3 = _mm_add_epi16(sum2, shifted3);\n    return sum3;\n}\n\nbool correct_bracketing_simd(std::string brackets) {\n    const char* p = brackets.data();\n    size_t len = brackets.size();\n    int balance = 0;\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('('));\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i neg_ones = _mm_set1_epi8(-1);\n        __m128i values = _mm_or_si128(_mm_and_si128(mask, ones), \n                                 _mm_andnot_si128(mask, neg_ones));\n\n        __m128i v_low = _mm_cvtepi8_epi16(values);\n        __m128i v_high = _mm_cvtepi8_epi16(_mm_srli_si128(values, 8));\n\n        __m128i scan_low = inclusive_scan_epi16(v_low);\n        scan_low = _mm_add_epi16(scan_low, _mm_set1_epi16(balance));\n        if (_mm_movemask_epi8(_mm_cmplt_epi16(scan_low, _mm_setzero_si128()))) {\n            return false;\n        }\n\n        int next_balance = _mm_extract_epi16(scan_low, 7);\n\n        __m128i scan_high = inclusive_scan_epi16(v_high);\n        scan_high = _mm_add_epi16(scan_high, _mm_set1_epi16(next_balance));\n        if (_mm_movemask_epi8(_mm_cmplt_epi16(scan_high, _mm_setzero_si128()))) {\n            return false;\n        }\n\n        balance = _mm_extract_epi16(scan_high, 7);\n    }\n\n    for (; i < len; ++i) {\n        if (p[i] == '(') {\n            ++balance;\n        } else if (p[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    \n    size_t n = xs.size();\n    size_t result_size = n - 1;\n    std::vector<float> result(result_size);\n    const float* xs_data = xs.data() + 1; // Skip first element\n    float* result_data = result.data();\n    \n    size_t i = 0;\n    for (; i + 4 <= result_size; i += 4) {\n        __m128 base = _mm_set1_ps(static_cast<float>(i + 1));\n        __m128 inc = _mm_setr_ps(0.0f, 1.0f, 2.0f, 3.0f);\n        __m128 factors = _mm_add_ps(base, inc);\n        \n        __m128 data_vec = _mm_loadu_ps(xs_data + i);\n        __m128 product = _mm_mul_ps(factors, data_vec);\n        _mm_storeu_ps(result_data + i, product);\n    }\n    \n    for (; i < result_size; ++i) {\n        result_data[i] = static_cast<float>(i + 1) * xs_data[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    size_t n = xs.size() - 1;\n    std::vector<float> res(n);\n    __m128 inc_vec = _mm_setr_ps(0.0f, 1.0f, 2.0f, 3.0f);\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 base = _mm_set1_ps((float)(i + 1));\n        __m128 indices = _mm_add_ps(base, inc_vec);\n        __m128 product = _mm_mul_ps(coeffs, indices);\n        _mm_storeu_ps(&res[i], product);\n    }\n    for (; i < n; i++) {\n        res[i] = (i + 1) * xs[i + 1];\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    if (xs.size() <= 1) {\n        return std::vector<float>();\n    }\n    size_t n = xs.size();\n    size_t n_result = n - 1;\n    std::vector<float> result(n_result);\n    \n    __m128 mult = _mm_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n    __m128 step = _mm_set1_ps(4.0f);\n    size_t i = 0;\n    \n    for (; i + 4 <= n_result; i += 4) {\n        __m128 data = _mm_loadu_ps(xs.data() + i + 1);\n        __m128 res = _mm_mul_ps(data, mult);\n        _mm_storeu_ps(result.data() + i, res);\n        mult = _mm_add_ps(mult, step);\n    }\n    \n    for (; i < n_result; i++) {\n        result[i] = (i+1) * xs[i+1];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) {\n        return std::vector<float>();\n    }\n    std::vector<float> res(n - 1);\n\n    static const __m128 inc = _mm_setr_ps(0.0f, 1.0f, 2.0f, 3.0f);\n    size_t i = 0;\n    for (; i + 4 <= n - 1; i += 4) {\n        __m128 base = _mm_set1_ps(static_cast<float>(i + 1));\n        __m128 multipliers = _mm_add_ps(base, inc);\n        __m128 coeffs = _mm_loadu_ps(&xs[1 + i]);\n        __m128 result = _mm_mul_ps(multipliers, coeffs);\n        _mm_storeu_ps(&res[i], result);\n    }\n\n    for (; i < n - 1; ++i) {\n        res[i] = static_cast<float>(i + 1) * xs[1 + i];\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) {\n        return std::vector<float>();\n    }\n    std::vector<float> res(n-1);\n    size_t i = 1;\n    for (; i + 3 < n; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i]);\n        __m128 idx = _mm_setr_ps(static_cast<float>(i), \n                                 static_cast<float>(i+1), \n                                 static_cast<float>(i+2), \n                                 static_cast<float>(i+3));\n        __m128 product = _mm_mul_ps(coeffs, idx);\n        _mm_storeu_ps(&res[i-1], product);\n    }\n    for (; i < n; i++) {\n        res[i-1] = static_cast<float>(i) * xs[i];\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    size_t len = s.length();\n    if (len == 0) \n        return 0;\n\n    __m128i to_lower = _mm_set1_epi8(0x20);\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i lower_chunk = _mm_or_si128(chunk, to_lower);\n        \n        __m128i v_a = _mm_set1_epi8('a');\n        __m128i v_e = _mm_set1_epi8('e');\n        __m128i v_i = _mm_set1_epi8('i');\n        __m128i v_o = _mm_set1_epi8('o');\n        __m128i v_u = _mm_set1_epi8('u');\n        \n        __m128i cmp_a = _mm_cmpeq_epi8(lower_chunk, v_a);\n        __m128i cmp_e = _mm_cmpeq_epi8(lower_chunk, v_e);\n        __m128i cmp_i = _mm_cmpeq_epi8(lower_chunk, v_i);\n        __m128i cmp_o = _mm_cmpeq_epi8(lower_chunk, v_o);\n        __m128i cmp_u = _mm_cmpeq_epi8(lower_chunk, v_u);\n        \n        __m128i any = _mm_or_si128(cmp_a, cmp_e);\n        any = _mm_or_si128(any, cmp_i);\n        any = _mm_or_si128(any, cmp_o);\n        any = _mm_or_si128(any, cmp_u);\n        \n        unsigned int mask = _mm_movemask_epi8(any);\n        count += __builtin_popcount(mask);\n    }\n    \n    for (; i < len; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n    \n    char last_char = s[len - 1];\n    if (last_char == 'y' || last_char == 'Y') {\n        count++;\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    size_t len = s.length();\n    if (len == 0) return 0;\n\n    const __m128i v_0x20 = _mm_set1_epi8(0x20);\n    const __m128i v_a = _mm_set1_epi8('a');\n    const __m128i v_e = _mm_set1_epi8('e');\n    const __m128i v_i = _mm_set1_epi8('i');\n    const __m128i v_o = _mm_set1_epi8('o');\n    const __m128i v_u = _mm_set1_epi8('u');\n\n    int count = 0;\n    size_t i = 0;\n    for (; i < len; i += 16) {\n        size_t remaining = len - i;\n        __m128i chunk;\n        if (remaining >= 16) {\n            chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        } else {\n            char buffer[16] = {0};\n            std::memcpy(buffer, s.data() + i, remaining);\n            chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer));\n        }\n\n        __m128i lower = _mm_or_si128(chunk, v_0x20);\n        __m128i eq_a = _mm_cmpeq_epi8(lower, v_a);\n        __m128i eq_e = _mm_cmpeq_epi8(lower, v_e);\n        __m128i eq_i = _mm_cmpeq_epi8(lower, v_i);\n        __m128i eq_o = _mm_cmpeq_epi8(lower, v_o);\n        __m128i eq_u = _mm_cmpeq_epi8(lower, v_u);\n\n        __m128i any = _mm_or_si128(eq_a, eq_e);\n        any = _mm_or_si128(any, eq_i);\n        any = _mm_or_si128(any, eq_o);\n        any = _mm_or_si128(any, eq_u);\n\n        int mask = _mm_movemask_epi8(any);\n        uint16_t m = static_cast<uint16_t>(mask);\n        m = (m & 0x5555) + ((m >> 1) & 0x5555);\n        m = (m & 0x3333) + ((m >> 2) & 0x3333);\n        m = (m & 0x0F0F) + ((m >> 4) & 0x0F0F);\n        m = (m & 0x00FF) + (m >> 8);\n        count += m;\n    }\n\n    char last_char = s[len - 1];\n    if (last_char == 'y' || last_char == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    size_t n = s.length();\n    if (n == 0) return 0;\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i lower_block = _mm_or_si128(block, _mm_set1_epi8(0x20));\n\n        __m128i a = _mm_set1_epi8('a');\n        __m128i e = _mm_set1_epi8('e');\n        __m128i i_vow = _mm_set1_epi8('i');\n        __m128i o = _mm_set1_epi8('o');\n        __m128i u = _mm_set1_epi8('u');\n\n        __m128i eq_a = _mm_cmpeq_epi8(lower_block, a);\n        __m128i eq_e = _mm_cmpeq_epi8(lower_block, e);\n        __m128i eq_i = _mm_cmpeq_epi8(lower_block, i_vow);\n        __m128i eq_o = _mm_cmpeq_epi8(lower_block, o);\n        __m128i eq_u = _mm_cmpeq_epi8(lower_block, u);\n\n        __m128i any = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(eq_a, eq_e), eq_i), eq_o), eq_u);\n        int mask = _mm_movemask_epi8(any);\n\n        int bit_count = 0;\n        for (int bit = 0; bit < 16; bit++) {\n            bit_count += (mask >> bit) & 1;\n        }\n        count += bit_count;\n    }\n\n    for (; i < n; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'A' || c == 'e' || c == 'E' || \n            c == 'i' || c == 'I' || c == 'o' || c == 'O' || \n            c == 'u' || c == 'U') {\n            count++;\n        }\n    }\n\n    char last_char = s[n-1];\n    if (last_char == 'y' || last_char == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    size_t len = s.length();\n    if (len == 0) {\n        return 0;\n    }\n    int count = 0;\n    size_t i = 0;\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i lower = _mm_or_si128(chunk, _mm_set1_epi8(0x20));\n        __m128i v_a = _mm_cmpeq_epi8(lower, _mm_set1_epi8('a'));\n        __m128i v_e = _mm_cmpeq_epi8(lower, _mm_set1_epi8('e'));\n        __m128i v_i = _mm_cmpeq_epi8(lower, _mm_set1_epi8('i'));\n        __m128i v_o = _mm_cmpeq_epi8(lower, _mm_set1_epi8('o'));\n        __m128i v_u = _mm_cmpeq_epi8(lower, _mm_set1_epi8('u'));\n        __m128i tmp1 = _mm_or_si128(v_a, v_e);\n        __m128i tmp2 = _mm_or_si128(v_i, v_o);\n        __m128i tmp3 = _mm_or_si128(tmp1, tmp2);\n        __m128i vowel_match = _mm_or_si128(tmp3, v_u);\n        int mask = _mm_movemask_epi8(vowel_match);\n        unsigned int m = (unsigned int)mask;\n        int cnt = 0;\n        for (int j = 0; j < 16; j++) {\n            cnt += (m >> j) & 1;\n        }\n        count += cnt;\n    }\n\n    for (; i < len; i++) {\n        char c = s[i];\n        char lower_c = c | 0x20;\n        if (lower_c == 'a' || lower_c == 'e' || lower_c == 'i' || lower_c == 'o' || lower_c == 'u') {\n            count++;\n        }\n    }\n\n    char last_char = s[len - 1];\n    if (last_char == 'y' || last_char == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    size_t len = s.length();\n    if (len == 0) return 0;\n\n    const __m128i a_minus1 = _mm_set1_epi8('A' - 1);\n    const __m128i z_plus1 = _mm_set1_epi8('Z' + 1);\n    const __m128i offset = _mm_set1_epi8(32);\n\n    size_t j = 0;\n    for (; j + 16 <= len; j += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + j));\n        __m128i gt_A = _mm_cmpgt_epi8(chunk, a_minus1);\n        __m128i lt_Z = _mm_cmplt_epi8(chunk, z_plus1);\n        __m128i mask = _mm_and_si128(gt_A, lt_Z);\n        __m128i to_add = _mm_and_si128(mask, offset);\n        __m128i lower_chunk = _mm_add_epi8(chunk, to_add);\n        _mm_storeu_si128((__m128i*)(s.data() + j), lower_chunk);\n    }\n\n    for (; j < len; j++) {\n        char c = s[j];\n        if (c >= 'A' && c <= 'Z') {\n            s[j] = c + 32;\n        }\n    }\n\n    int count = 0;\n    size_t i = 0;\n    const __m128i v_a = _mm_set1_epi8('a');\n    const __m128i v_e = _mm_set1_epi8('e');\n    const __m128i v_i = _mm_set1_epi8('i');\n    const __m128i v_o = _mm_set1_epi8('o');\n    const __m128i v_u = _mm_set1_epi8('u');\n    const __m128i ones = _mm_set1_epi8(1);\n    const __m128i zero = _mm_setzero_si128();\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i eq_a = _mm_cmpeq_epi8(chunk, v_a);\n        __m128i eq_e = _mm_cmpeq_epi8(chunk, v_e);\n        __m128i eq_i = _mm_cmpeq_epi8(chunk, v_i);\n        __m128i eq_o = _mm_cmpeq_epi8(chunk, v_o);\n        __m128i eq_u = _mm_cmpeq_epi8(chunk, v_u);\n\n        __m128i any = _mm_or_si128(eq_a, eq_e);\n        any = _mm_or_si128(any, eq_i);\n        any = _mm_or_si128(any, eq_o);\n        any = _mm_or_si128(any, eq_u);\n\n        __m128i masked = _mm_and_si128(any, ones);\n        __m128i sum = _mm_sad_epu8(masked, zero);\n        uint64_t sum_arr[2];\n        _mm_storeu_si128((__m128i*)sum_arr, sum);\n        count += static_cast<int>(sum_arr[0] + sum_arr[1]);\n    }\n\n    for (; i < len; i++) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    if (len > 0 && s[len - 1] == 'y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* data = s.data();\n    size_t len = s.length();\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n\n    for (; i + 15 < len; i += 16) {\n        __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i maskA = _mm_cmpgt_epi8(block, _mm_set1_epi8('A' - 1));\n        __m128i maskZ = _mm_cmplt_epi8(block, _mm_set1_epi8('Z' + 1));\n        __m128i mask = _mm_and_si128(maskA, maskZ);\n        __m128i upper_chars = _mm_and_si128(block, mask);\n        __m128i partial = _mm_sad_epu8(upper_chars, _mm_setzero_si128());\n        sum_vec = _mm_add_epi64(sum_vec, partial);\n    }\n\n    unsigned long long sum_arr[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_arr), sum_vec);\n    unsigned long long total = sum_arr[0] + sum_arr[1];\n\n    for (; i < len; ++i) {\n        char c = data[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += static_cast<unsigned char>(c);\n        }\n    }\n\n    return static_cast<int>(total);\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    size_t len = s.length();\n    if (len == 0) return 0;\n    const char* p = s.c_str();\n    uint64_t total = 0;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n    __m128i Z_vec = _mm_set1_epi8('Z');\n\n    for (; i + 15 < len; i += 16) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i));\n        __m128i mask1 = _mm_cmpgt_epi8(v, A_minus1);\n        __m128i mask2 = _mm_cmpgt_epi8(v, Z_vec);\n        __m128i mask = _mm_andnot_si128(mask2, mask1);\n        __m128i masked = _mm_and_si128(v, mask);\n        __m128i sad = _mm_sad_epu8(masked, zero);\n        uint64_t tmp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(tmp), sad);\n        total += tmp[0] + tmp[1];\n    }\n\n    for (; i < len; i++) {\n        char c = p[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += static_cast<unsigned char>(c);\n        }\n    }\n\n    return static_cast<int>(total);\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nint digitSum_simd(std::string s) {\n    int len = s.length();\n    const char* str = s.c_str();\n    uint64_t total_sum = 0;\n    int i = 0;\n\n    if (len >= 16) {\n        __m128i A_minus1 = _mm_set1_epi8(64);\n        __m128i Z_plus1 = _mm_set1_epi8(91);\n\n        for (; i <= len - 16; i += 16) {\n            __m128i v = _mm_loadu_si128((const __m128i*)(str + i));\n            __m128i cmp_ge = _mm_cmpgt_epi8(v, A_minus1);\n            __m128i cmp_le = _mm_cmplt_epi8(v, Z_plus1);\n            __m128i mask = _mm_and_si128(cmp_ge, cmp_le);\n            __m128i upper_chars = _mm_and_si128(v, mask);\n            __m128i sumv = _mm_sad_epu8(upper_chars, _mm_setzero_si128());\n            alignas(16) uint64_t tmp[2];\n            _mm_store_si128((__m128i*)tmp, sumv);\n            total_sum += tmp[0] + tmp[1];\n        }\n    }\n\n    for (; i < len; i++) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            total_sum += (unsigned char)c;\n        }\n    }\n\n    return (int)total_sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    const size_t len = s.length();\n    const char* str = s.c_str();\n\n    const __m128i v_zero = _mm_setzero_si128();\n    const __m128i v_A = _mm_set1_epi8('A' - 1);\n    const __m128i v_Z = _mm_set1_epi8('Z' + 1);\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i cmp_ge = _mm_cmpgt_epi8(chunk, v_A);\n        __m128i cmp_le = _mm_cmplt_epi8(chunk, v_Z);\n        __m128i mask = _mm_and_si128(cmp_ge, cmp_le);\n        __m128i filtered = _mm_and_si128(chunk, mask);\n        __m128i sums = _mm_sad_epu8(filtered, v_zero);\n        sum += _mm_extract_epi16(sums, 0) + _mm_extract_epi16(sums, 4);\n    }\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c >= 'A' && c <= 'Z') {\n            sum += static_cast<unsigned char>(c);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int total = 0;\n    size_t i = 0;\n    size_t len = s.length();\n    const char* data = s.c_str();\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i mask1 = _mm_cmpgt_epi8(chunk, _mm_set1_epi8(64));\n        __m128i mask2 = _mm_cmplt_epi8(chunk, _mm_set1_epi8(91));\n        __m128i mask = _mm_and_si128(mask1, mask2);\n        __m128i selected = _mm_and_si128(chunk, mask);\n        \n        __m128i zeros = _mm_setzero_si128();\n        __m128i sad = _mm_sad_epu8(selected, zeros);\n        __m128i high64 = _mm_srli_si128(sad, 8);\n        __m128i sum64 = _mm_add_epi64(sad, high64);\n        total += _mm_cvtsi128_si32(sum64);\n    }\n\n    for (; i < len; i++) {\n        char c = data[i];\n        if (c >= 'A' && c <= 'Z') {\n            total += c;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <climits>\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ninline __m128i min4(__m128i a) {\n    __m128i a_perm1 = _mm_shuffle_epi32(a, _MM_SHUFFLE(2,3,0,1)); \n    __m128i mask1 = _mm_cmplt_epi32(a, a_perm1);\n    __m128i min1 = _mm_or_si128(_mm_and_si128(mask1, a), _mm_andnot_si128(mask1, a_perm1));\n    __m128i a_perm2 = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1,0,3,2)); \n    __m128i mask2 = _mm_cmplt_epi32(min1, a_perm2);\n    __m128i min2 = _mm_or_si128(_mm_and_si128(mask2, min1), _mm_andnot_si128(mask2, a_perm2));\n    return min2;\n}\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return {};\n    }\n\n    int global_min_value = INT_MAX;\n    int global_min_index = INT_MAX;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i i_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i one = _mm_set1_epi32(1);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(v, one), _mm_setzero_si128());\n        __m128i v_adj = _mm_or_si128(\n            _mm_and_si128(even_mask, v),\n            _mm_andnot_si128(even_mask, _mm_set1_epi32(INT_MAX))\n        );\n        __m128i i_adj = _mm_or_si128(\n            _mm_and_si128(even_mask, i_vec),\n            _mm_andnot_si128(even_mask, _mm_set1_epi32(INT_MAX))\n        );\n        __m128i v_min = min4(v_adj);\n        int min_val_chunk = _mm_cvtsi128_si32(v_min);\n        __m128i min_val_vec = _mm_set1_epi32(min_val_chunk);\n        __m128i mask_eq = _mm_cmpeq_epi32(v_adj, min_val_vec);\n        __m128i candidate_indices = _mm_or_si128(\n            _mm_and_si128(mask_eq, i_adj),\n            _mm_andnot_si128(mask_eq, _mm_set1_epi32(INT_MAX))\n        );\n        __m128i i_min = min4(candidate_indices);\n        int min_index_chunk = _mm_cvtsi128_si32(i_min);\n\n        if (min_val_chunk < global_min_value) {\n            global_min_value = min_val_chunk;\n            global_min_index = min_index_chunk;\n        } else if (min_val_chunk == global_min_value && min_index_chunk < global_min_index) {\n            global_min_index = min_index_chunk;\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < global_min_value) {\n                global_min_value = arr[i];\n                global_min_index = i;\n            } else if (arr[i] == global_min_value && i < global_min_index) {\n                global_min_index = i;\n            }\n        }\n    }\n\n    if (global_min_index == INT_MAX) {\n        return {};\n    } else {\n        return {global_min_value, global_min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return {};\n    }\n\n    int global_min_even = INT_MAX;\n    int global_min_index = -1;\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i and_res = _mm_and_si128(data, ones);\n        __m128i mask = _mm_cmpeq_epi32(and_res, _mm_setzero_si128());\n        \n        __m128i intmax_vec = _mm_set1_epi32(INT_MAX);\n        __m128i candidate = _mm_or_si128(\n            _mm_and_si128(mask, data),\n            _mm_andnot_si128(mask, intmax_vec)\n        );\n        \n        __m128i min_temp = candidate;\n        __m128i shuf1 = _mm_shuffle_epi32(min_temp, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i min1 = _mm_min_epi32(min_temp, shuf1);\n        __m128i shuf2 = _mm_shuffle_epi32(min1, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i min2 = _mm_min_epi32(min1, shuf2);\n        int chunk_min_value = _mm_extract_epi32(min2, 0);\n        \n        if (chunk_min_value < global_min_even) {\n            global_min_even = chunk_min_value;\n            for (int j = 0; j < 4; j++) {\n                if ((arr[i + j] & 1) == 0 && arr[i + j] == chunk_min_value) {\n                    global_min_index = i + j;\n                    break;\n                }\n            }\n        } else if (global_min_even == INT_MAX && chunk_min_value == INT_MAX) {\n            if (_mm_movemask_epi8(mask) != 0) {\n                for (int j = 0; j < 4; j++) {\n                    if ((arr[i + j] & 1) == 0 && arr[i + j] == INT_MAX) {\n                        global_min_index = i + j;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    for (int j = i; j < n; j++) {\n        if ((arr[j] & 1) == 0) {\n            if (arr[j] < global_min_even) {\n                global_min_even = arr[j];\n                global_min_index = j;\n            } else if (global_min_even == INT_MAX && arr[j] == INT_MAX) {\n                global_min_index = j;\n            }\n        }\n    }\n\n    if (global_min_index == -1) {\n        return {};\n    } else {\n        return {global_min_even, global_min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int n = arr.size();\n    int current_min_index = -1;\n    int current_min_even;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(&arr[i]));\n        __m128i one = _mm_set1_epi32(1);\n        __m128i and_result = _mm_and_si128(chunk, one);\n        __m128i mask = _mm_cmpeq_epi32(and_result, _mm_setzero_si128());\n        int mask_bits = _mm_movemask_ps(_mm_castsi128_ps(mask));\n\n        int vals[4];\n        _mm_storeu_si128((__m128i*)vals, chunk);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_bits & (1 << j)) {\n                int value = vals[j];\n                int index = i + j;\n                if (current_min_index == -1) {\n                    current_min_even = value;\n                    current_min_index = index;\n                } else {\n                    if (value < current_min_even || (value == current_min_even && index < current_min_index)) {\n                        current_min_even = value;\n                        current_min_index = index;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        int value = arr[i];\n        if ((value & 1) == 0) {\n            if (current_min_index == -1) {\n                current_min_even = value;\n                current_min_index = i;\n            } else {\n                if (value < current_min_even || (value == current_min_even && i < current_min_index)) {\n                    current_min_even = value;\n                    current_min_index = i;\n                }\n            }\n        }\n    }\n\n    if (current_min_index == -1) {\n        return {};\n    } else {\n        return {current_min_even, current_min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return {};\n    }\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    int i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i lowbits = _mm_and_si128(vec, ones);\n        __m128i mask = _mm_cmpeq_epi32(lowbits, _mm_setzero_si128());\n        \n        int mask_arr[4];\n        int values[4];\n        _mm_storeu_si128((__m128i*)mask_arr, mask);\n        _mm_storeu_si128((__m128i*)values, vec);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                if (values[j] < min_even) {\n                    min_even = values[j];\n                    min_index = i + j;\n                }\n            }\n        }\n    }\n\n    if (i < n) {\n        int count = n - i;\n        int temp[4];\n        for (int j = 0; j < 4; j++) {\n            if (j < count) {\n                temp[j] = arr[i + j];\n            } else {\n                temp[j] = 1;\n            }\n        }\n        __m128i vec = _mm_loadu_si128((__m128i*)temp);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i lowbits = _mm_and_si128(vec, ones);\n        __m128i mask = _mm_cmpeq_epi32(lowbits, _mm_setzero_si128());\n        \n        int mask_arr[4];\n        int values[4];\n        _mm_storeu_si128((__m128i*)mask_arr, mask);\n        _mm_storeu_si128((__m128i*)values, vec);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                if (values[j] < min_even) {\n                    min_even = values[j];\n                    min_index = i + j;\n                }\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) \n        return {};\n\n    int global_min_value = INT_MAX;\n    int global_min_index = -1;\n\n    for (int i = 0; i < n; i += 4) {\n        int remaining = n - i;\n        __m128i v;\n        if (remaining < 4) {\n            int temp[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n            for (int j = 0; j < remaining; j++) {\n                temp[j] = arr[i + j];\n            }\n            v = _mm_loadu_si128((__m128i*)temp);\n        } else {\n            v = _mm_loadu_si128((__m128i*)&arr[i]);\n        }\n\n        __m128i one = _mm_set1_epi32(1);\n        __m128i mod = _mm_and_si128(v, one);\n        __m128i mask_even = _mm_cmpeq_epi32(mod, _mm_setzero_si128());\n        __m128i intmax_vec = _mm_set1_epi32(INT_MAX);\n        __m128i candidate = _mm_or_si128(v, _mm_andnot_si128(mask_even, intmax_vec));\n\n        __m128i min_vertical = _mm_min_epi32(candidate, _mm_shuffle_epi32(candidate, _MM_SHUFFLE(2, 3, 0, 1)));\n        __m128i min_final = _mm_min_epi32(min_vertical, _mm_shuffle_epi32(min_vertical, _MM_SHUFFLE(1, 0, 3, 2)));\n        int min_val_in_block = _mm_cvtsi128_si32(min_final);\n\n        if (min_val_in_block == INT_MAX) \n            continue;\n\n        if (min_val_in_block < global_min_value) {\n            global_min_value = min_val_in_block;\n            __m128i min_broadcast = _mm_set1_epi32(min_val_in_block);\n            __m128i eq_mask = _mm_cmpeq_epi32(v, min_broadcast);\n            int mask_bits = _mm_movemask_ps(_mm_castsi128_ps(eq_mask));\n            int j = 0;\n            for (; j < 4; j++) {\n                if (mask_bits & (1 << j))\n                    break;\n            }\n            global_min_index = i + j;\n        }\n    }\n\n    if (global_min_index == -1)\n        return {};\n    else\n        return {global_min_value, global_min_index};\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) {\n        return -1;\n    }\n    int n = lst.size();\n    __m128i max_vec = _mm_set1_epi32(0);\n    size_t i = 0;\n    if (n >= 4) {\n        for (; i <= n - 4; i += 4) {\n            __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n            __m128i mask = _mm_cmpgt_epi32(max_vec, data);\n            max_vec = _mm_or_si128(\n                _mm_and_si128(mask, max_vec),\n                _mm_andnot_si128(mask, data)\n            );\n        }\n    }\n    int max_val = 0;\n    if (n >= 4) {\n        int max_arr[4] __attribute__((aligned(16)));\n        _mm_store_si128((__m128i*)max_arr, max_vec);\n        max_val = max_arr[0];\n        for (int k = 1; k < 4; k++) {\n            if (max_arr[k] > max_val) {\n                max_val = max_arr[k];\n            }\n        }\n    }\n    for (; i < n; i++) {\n        if (lst[i] > max_val) {\n            max_val = lst[i];\n        }\n    }\n    std::vector<int> freq(max_val + 1, 0);\n    for (int x : lst) {\n        if (x <= max_val) {\n            freq[x]++;\n        }\n    }\n    int candidate = -1;\n    int j;\n    for (j = max_val; j - 3 >= 1; j -= 4) {\n        __m128i f = _mm_loadu_si128((__m128i const*)&freq[j - 3]);\n        __m128i idx = _mm_set_epi32(j, j - 1, j - 2, j - 3);\n        __m128i cmp_gt = _mm_cmpgt_epi32(f, idx);\n        __m128i cmp_eq = _mm_cmpeq_epi32(f, idx);\n        __m128i cmp = _mm_or_si128(cmp_gt, cmp_eq);\n        int mask[4];\n        _mm_storeu_si128((__m128i*)mask, cmp);\n        if (mask[3]) {\n            candidate = j;\n            break;\n        } else if (mask[2]) {\n            candidate = j - 1;\n            break;\n        } else if (mask[1]) {\n            candidate = j - 2;\n            break;\n        } else if (mask[0]) {\n            candidate = j - 3;\n            break;\n        }\n    }\n    if (candidate == -1) {\n        for (int k = j; k >= 1; k--) {\n            if (freq[k] >= k) {\n                candidate = k;\n                break;\n            }\n        }\n    }\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) \n        return -1;\n\n    int max_val = 0;\n    int i = 0;\n    __m128i max_vec = _mm_setzero_si128();\n    for (; i <= n - 4; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i cmp = _mm_cmpgt_epi32(max_vec, data);\n        max_vec = _mm_or_si128(_mm_and_si128(cmp, max_vec), _mm_andnot_si128(cmp, data));\n    }\n    int temp[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)temp, max_vec);\n    int max_temp = 0;\n    for (int j = 0; j < 4; j++) {\n        if (temp[j] > max_temp) \n            max_temp = temp[j];\n    }\n    for (; i < n; i++) {\n        if (lst[i] > max_temp) \n            max_temp = lst[i];\n    }\n    max_val = max_temp;\n\n    if (max_val <= 0) \n        return -1;\n\n    std::vector<int> freq(max_val + 1, 0);\n    for (i = 0; i < n; i++) {\n        if (lst[i] <= max_val) \n            freq[lst[i]]++;\n    }\n\n    int result = -1;\n    i = max_val;\n    for (; i >= 4; i -= 4) {\n        __m128i freq_vec = _mm_loadu_si128((__m128i*)&freq[i - 3]);\n        __m128i indices = _mm_set_epi32(i, i - 1, i - 2, i - 3);\n        __m128i cmp_lt = _mm_cmplt_epi32(freq_vec, indices);\n        __m128i cmp_ge = _mm_andnot_si128(cmp_lt, _mm_set1_epi32(-1));\n        int mask = _mm_movemask_epi8(cmp_ge);\n\n        if (mask & (1 << 15)) {\n            result = i;\n            break;\n        } else if (mask & (1 << 11)) {\n            result = i - 1;\n            break;\n        } else if (mask & (1 << 7)) {\n            result = i - 2;\n            break;\n        } else if (mask & (1 << 3)) {\n            result = i - 3;\n            break;\n        }\n    }\n    if (result == -1) {\n        for (; i >= 1; i--) {\n            if (freq[i] >= i) {\n                result = i;\n                break;\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) {\n        return -1;\n    }\n\n    int n = lst.size();\n    __m128i max_v = _mm_set1_epi32(0);\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i mask = _mm_cmpgt_epi32(max_v, chunk);\n        max_v = _mm_or_si128(_mm_and_si128(mask, max_v), _mm_andnot_si128(mask, chunk));\n    }\n\n    alignas(16) int max_arr[4];\n    _mm_store_si128((__m128i*)max_arr, max_v);\n    int max_val = max_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    for (; i < n; i++) {\n        if (lst[i] > max_val) max_val = lst[i];\n    }\n\n    if (max_val < 1) {\n        return -1;\n    }\n\n    std::vector<int> freq0(max_val + 1, 0);\n    std::vector<int> freq1(max_val + 1, 0);\n    std::vector<int> freq2(max_val + 1, 0);\n    std::vector<int> freq3(max_val + 1, 0);\n\n    i = 0;\n    for (; i <= n - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i const*)&lst[i]);\n        int a0 = _mm_cvtsi128_si32(chunk);\n        int a1 = _mm_extract_epi16(chunk, 2);\n        int a2 = _mm_extract_epi16(chunk, 4);\n        int a3 = _mm_extract_epi16(chunk, 6);\n        if (a0 <= max_val) freq0[a0]++;\n        if (a1 <= max_val) freq1[a1]++;\n        if (a2 <= max_val) freq2[a2]++;\n        if (a3 <= max_val) freq3[a3]++;\n    }\n\n    for (; i < n; i++) {\n        int a = lst[i];\n        if (a <= max_val) freq0[a]++;\n    }\n\n    for (int j = 0; j <= max_val; j++) {\n        freq0[j] += freq1[j] + freq2[j] + freq3[j];\n    }\n\n    int candidate = -1;\n    i = max_val;\n    for (; i >= 4; i -= 4) {\n        __m128i freqs = _mm_loadu_si128((__m128i const*)&freq0[i-3]);\n        __m128i indices = _mm_setr_epi32(i-3, i-2, i-1, i);\n        __m128i cmp_gt = _mm_cmpgt_epi32(freqs, indices);\n        __m128i cmp_eq = _mm_cmpeq_epi32(freqs, indices);\n        __m128i mask = _mm_or_si128(cmp_gt, cmp_eq);\n        int bitmask = _mm_movemask_ps(_mm_castsi128_ps(mask));\n\n        if (bitmask & 8) {\n            candidate = i;\n            break;\n        } else if (bitmask & 4) {\n            candidate = i-1;\n            break;\n        } else if (bitmask & 2) {\n            candidate = i-2;\n            break;\n        } else if (bitmask & 1) {\n            candidate = i-3;\n            break;\n        }\n    }\n\n    if (candidate != -1) {\n        return candidate;\n    }\n\n    for (; i >= 1; i--) {\n        if (freq0[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) \n        return -1;\n    \n    int n = lst.size();\n    std::vector<int> freq(n + 1, 0);\n\n    for (int i = 0; i < n; i++) {\n        int num = lst[i];\n        if (num >= 1 && num <= n) {\n            freq[num]++;\n        }\n    }\n\n    int i = n;\n    for (; i >= 4; i -= 4) {\n        __m128i freq_vec = _mm_loadu_si128(reinterpret_cast<__m128i const*>(freq.data() + i - 3));\n        __m128i cand = _mm_set_epi32(i, i - 1, i - 2, i - 3);\n        __m128i cmp = _mm_cmple_epi32(cand, freq_vec);\n        \n        alignas(16) int res[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(res), cmp);\n        \n        if (res[3]) return i;\n        if (res[2]) return i - 1;\n        if (res[1]) return i - 2;\n        if (res[0]) return i - 3;\n    }\n\n    for (; i >= 1; i--) {\n        if (freq[i] >= i) {\n            return i;\n        }\n    }\n    \n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    int max_val = 0;\n    int i = 0;\n    __m128i max_vec = _mm_set1_epi32(0);\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i mask = _mm_cmpgt_epi32(max_vec, data);\n        max_vec = _mm_or_si128(_mm_and_si128(mask, max_vec), _mm_andnot_si128(mask, data));\n    }\n    alignas(16) int max_arr[4];\n    _mm_store_si128((__m128i*)max_arr, max_vec);\n    max_val = max_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n    for (; i < n; i++) {\n        if (lst[i] > max_val) max_val = lst[i];\n    }\n\n    int max_candidate = (max_val < n) ? max_val : n;\n    std::vector<int> freq(max_candidate + 1, 0);\n\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i high = _mm_set1_epi32(max_candidate);\n        __m128i mask_gt = _mm_cmpgt_epi32(high, data);\n        __m128i mask_eq = _mm_cmpeq_epi32(data, high);\n        __m128i mask = _mm_or_si128(mask_gt, mask_eq);\n        alignas(16) int mask_arr[4];\n        alignas(16) int data_arr[4];\n        _mm_store_si128((__m128i*)mask_arr, mask);\n        _mm_store_si128((__m128i*)data_arr, data);\n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                freq[data_arr[j]]++;\n            }\n        }\n    }\n    for (; i < n; i++) {\n        int val = lst[i];\n        if (val >= 1 && val <= max_candidate) {\n            freq[val]++;\n        }\n    }\n\n    for (int x = max_candidate; x >= 1; x--) {\n        if (freq[x] >= x) {\n            return x;\n        }\n    }\n    return -1;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int half = n / 2;\n    int i = 0;\n\n    for (; i <= half - 4; i += 4) {\n        __m128i left_vec = _mm_loadu_si128((__m128i*)(q.data() + i));\n        __m128i right_vec = _mm_loadu_si128((__m128i*)(q.data() + n - 4 - i));\n        __m128i reversed_right = _mm_shuffle_epi32(right_vec, _MM_SHUFFLE(0,1,2,3));\n        __m128i cmp = _mm_cmpeq_epi32(left_vec, reversed_right);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < half; i++) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    int j = 0;\n    for (; j <= n - 4; j += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)(q.data() + j));\n        sum_vec = _mm_add_epi32(sum_vec, v);\n    }\n\n    __m128i sum64 = _mm_add_epi32(sum_vec, _mm_srli_si128(sum_vec, 8));\n    __m128i sum32 = _mm_add_epi32(sum64, _mm_srli_si128(sum64, 4));\n    int total = _mm_cvtsi128_si32(sum32);\n\n    for (; j < n; j++) {\n        total += q[j];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    size_t i = 0, j = n - 1;\n\n    while (j - i >= 7) {\n        __m128i a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        __m128i b = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[j-3]));\n        __m128i reversed_b = _mm_shuffle_epi32(b, _MM_SHUFFLE(0,1,2,3));\n        __m128i cmp = _mm_cmpeq_epi32(a, reversed_b);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n        i += 4;\n        j -= 4;\n    }\n\n    while (i < j) {\n        if (q[i] != q[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t k = 0;\n    for (; k + 4 <= n; k += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[k]));\n        sum_vec = _mm_add_epi32(sum_vec, v);\n    }\n\n    int sum_array[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_array), sum_vec);\n    int total = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    for (; k < n; k++) {\n        total += q[k];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int i = 0;\n    int j = n - 1;\n\n    for (; i <= j - 3; i += 4, j -= 4) {\n        __m128i left = _mm_loadu_si128((const __m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((const __m128i*)&q[j-3]);\n        __m128i rev_right = _mm_shuffle_epi32(right, _MM_SHUFFLE(3,2,1,0));\n        __m128i cmp = _mm_cmpeq_epi32(left, rev_right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < j; i++, j--) {\n        if (q[i] != q[j]) {\n            return false;\n        }\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    int k = 0;\n    for (; k <= n-4; k += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&q[k]);\n        sum_vec = _mm_add_epi32(sum_vec, v);\n    }\n\n    __m128i sum1 = _mm_add_epi32(sum_vec, _mm_srli_si128(sum_vec, 8));\n    __m128i shuf = _mm_shuffle_epi32(sum1, _MM_SHUFFLE(1,0,3,2));\n    __m128i sum2 = _mm_add_epi32(sum1, shuf);\n    int total = _mm_cvtsi128_si32(sum2);\n\n    for (; k < n; k++) {\n        total += q[k];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int half = n / 2;\n    int i = 0;\n    \n    for (; i <= half - 4; i += 4) {\n        __m128i front = _mm_loadu_si128((__m128i*)(q.data() + i));\n        __m128i back = _mm_loadu_si128((__m128i*)(q.data() + n - i - 4));\n        __m128i reversed_back = _mm_shuffle_epi32(back, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(front, reversed_back);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n    \n    for (; i < half; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n    \n    __m128i sum_vec = _mm_setzero_si128();\n    int j = 0;\n    for (; j <= n - 4; j += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)(q.data() + j));\n        sum_vec = _mm_add_epi32(sum_vec, v);\n    }\n    \n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    int total = temp[0] + temp[1] + temp[2] + temp[3];\n    for (; j < n; ++j) {\n        total += q[j];\n    }\n    \n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int i = 0;\n    int j = n - 1;\n\n    while (i + 3 <= j) {\n        __m128i front = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i rear = _mm_loadu_si128((__m128i*)&q[j - 3]);\n        __m128i reversed_rear = _mm_shuffle_epi32(rear, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(front, reversed_rear);\n        unsigned mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n        i += 4;\n        j -= 4;\n    }\n\n    while (i < j) {\n        if (q[i] != q[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    int k = 0;\n    for (; k + 4 <= n; k += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&q[k]);\n        sum_vec = _mm_add_epi32(sum_vec, data);\n    }\n    alignas(16) int sum_array[4];\n    _mm_store_si128((__m128i*)sum_array, sum_vec);\n    int total = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    for (; k < n; k++) {\n        total += q[k];\n    }\n\n    return total <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) \n        return 0;\n    int i = 0;\n    int j = n - 1;\n    int changes = 0;\n\n    for (; i + 3 <= j - 3; i += 4, j -= 4) {\n        __m128i front = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i back = _mm_loadu_si128((__m128i*)(arr.data() + j - 3));\n        __m128i back_reversed = _mm_shuffle_epi32(back, _MM_SHUFFLE(0,1,2,3));\n        __m128i cmp = _mm_cmpeq_epi32(front, back_reversed);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        int matches = (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n        changes += (4 - matches);\n    }\n\n    while (i < j) {\n        if (arr[i] != arr[j])\n            changes++;\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int half = n / 2;\n    if (n == 0) return 0;\n    const int* left = arr.data();\n    const int* right = arr.data() + n - 1;\n    int mismatches = 0;\n    int i = 0;\n\n    for (; i <= half - 4; i += 4) {\n        __m128i left_vec = _mm_loadu_si128((const __m128i*)left);\n        __m128i right_vec = _mm_loadu_si128((const __m128i*)(right - 3));\n        __m128i reversed_right = _mm_shuffle_epi32(right_vec, _MM_SHUFFLE(0,1,2,3));\n        __m128i cmp = _mm_cmpeq_epi32(left_vec, reversed_right);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i not_equal = _mm_andnot_si128(cmp, ones);\n        __m128i sum1 = _mm_add_epi32(not_equal, _mm_srli_si128(not_equal, 8));\n        __m128i sum2 = _mm_add_epi32(sum1, _mm_srli_si128(sum1, 4));\n        int count = _mm_cvtsi128_si32(sum2);\n        mismatches += count;\n        left += 4;\n        right -= 4;\n    }\n\n    for (; i < half; i++) {\n        if (*left != *right) {\n            mismatches++;\n        }\n        left++;\n        right--;\n    }\n\n    return mismatches;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) \n        return 0;\n\n    int total_mismatches = 0;\n    int i = 0;\n    int j = n - 1;\n\n    for (; i + 7 <= j; i += 4, j -= 4) {\n        __m128i front = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i back = _mm_loadu_si128((__m128i*)(arr.data() + j - 3));\n        __m128i reversed_back = _mm_shuffle_epi32(back, _MM_SHUFFLE(3,2,1,0));\n        __m128i cmp = _mm_cmpeq_epi32(front, reversed_back);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i neq = _mm_andnot_si128(cmp, ones);\n        __m128i sum = neq;\n        sum = _mm_add_epi32(sum, _mm_srli_si128(neq, 4));\n        sum = _mm_add_epi32(sum, _mm_srli_si128(neq, 8));\n        sum = _mm_add_epi32(sum, _mm_srli_si128(neq, 12));\n        total_mismatches += _mm_cvtsi128_si32(sum);\n    }\n\n    while (i < j) {\n        if (arr[i] != arr[j]) \n            total_mismatches++;\n        i++;\n        j--;\n    }\n\n    return total_mismatches;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) \n        return 0;\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    while (j - i + 1 >= 8) {\n        __m128i front = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i back = _mm_loadu_si128((__m128i*)(arr.data() + j - 3));\n        __m128i reversed_back = _mm_shuffle_epi32(back, _MM_SHUFFLE(0, 1, 2, 3));\n        __m128i cmp = _mm_cmpeq_epi32(front, reversed_back);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i neq = _mm_andnot_si128(cmp, ones);\n        __m128i sum1 = _mm_add_epi32(neq, _mm_srli_si128(neq, 8));\n        __m128i sum2 = _mm_add_epi32(sum1, _mm_srli_si128(sum1, 4));\n        changes += _mm_extract_epi32(sum2, 0);\n        i += 4;\n        j -= 4;\n    }\n\n    while (i < j) {\n        if (arr[i] != arr[j])\n            changes++;\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int half = n / 2;\n    int count = 0;\n    int i = 0;\n\n    for (; i <= half - 4; i += 4) {\n        __m128i front = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i back = _mm_loadu_si128((__m128i*)(arr.data() + n - 1 - i - 3));\n        back = _mm_shuffle_epi32(back, _MM_SHUFFLE(3,2,1,0));\n        __m128i cmp = _mm_cmpeq_epi32(front, back);\n        int mask = _mm_movemask_epi8(cmp);\n        int cnt_eq = 0;\n        if ((mask & 0x000F) == 0x000F) cnt_eq++;\n        if ((mask & 0x00F0) == 0x00F0) cnt_eq++;\n        if ((mask & 0x0F00) == 0x0F00) cnt_eq++;\n        if ((mask & 0xF000) == 0xF000) cnt_eq++;\n        count += (4 - cnt_eq);\n    }\n\n    for (; i < half; i++) {\n        if (arr[i] != arr[n - 1 - i]) \n            count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0;\n    size_t n1 = lst1.size();\n    size_t i = 0;\n\n    if (n1 >= 2) {\n        __m128i sum_vec = _mm_setzero_si128();\n        for (; i <= n1 - 2; i += 2) {\n            int64_t len0 = static_cast<int64_t>(lst1[i].length());\n            int64_t len1 = static_cast<int64_t>(lst1[i+1].length());\n            __m128i v = _mm_set_epi64x(len1, len0);\n            sum_vec = _mm_add_epi64(sum_vec, v);\n        }\n        uint64_t t0 = _mm_cvtsi128_si64(sum_vec);\n        uint64_t t1 = _mm_cvtsi128_si64(_mm_srli_si128(sum_vec, 8));\n        total1 = t0 + t1;\n    }\n    if (i < n1) {\n        total1 += lst1[i].length();\n    }\n\n    size_t total2 = 0;\n    size_t n2 = lst2.size();\n    i = 0;\n\n    if (n2 >= 2) {\n        __m128i sum_vec = _mm_setzero_si128();\n        for (; i <= n2 - 2; i += 2) {\n            int64_t len0 = static_cast<int64_t>(lst2[i].length());\n            int64_t len1 = static_cast<int64_t>(lst2[i+1].length());\n            __m128i v = _mm_set_epi64x(len1, len0);\n            sum_vec = _mm_add_epi64(sum_vec, v);\n        }\n        uint64_t t0 = _mm_cvtsi128_si64(sum_vec);\n        uint64_t t1 = _mm_cvtsi128_si64(_mm_srli_si128(sum_vec, 8));\n        total2 = t0 + t1;\n    }\n    if (i < n2) {\n        total2 += lst2[i].length();\n    }\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0;\n    size_t total2 = 0;\n    size_t n1 = lst1.size();\n    size_t n2 = lst2.size();\n\n    if (n1 > 0) {\n        __m128i sum1 = _mm_setzero_si128();\n        size_t i = 0;\n        for (; i < n1 - 1; i += 2) {\n            size_t len0 = lst1[i].size();\n            size_t len1 = lst1[i+1].size();\n            __m128i v = _mm_set_epi64x(static_cast<int64_t>(len1), static_cast<int64_t>(len0));\n            sum1 = _mm_add_epi64(sum1, v);\n        }\n        uint64_t lo1 = _mm_cvtsi128_si64(sum1);\n        __m128i hi64_1 = _mm_unpackhi_epi64(sum1, sum1);\n        uint64_t hi1 = _mm_cvtsi128_si64(hi64_1);\n        total1 = lo1 + hi1;\n        if (i < n1) {\n            total1 += lst1[i].size();\n        }\n    }\n\n    if (n2 > 0) {\n        __m128i sum2 = _mm_setzero_si128();\n        size_t i = 0;\n        for (; i < n2 - 1; i += 2) {\n            size_t len0 = lst2[i].size();\n            size_t len1 = lst2[i+1].size();\n            __m128i v = _mm_set_epi64x(static_cast<int64_t>(len1), static_cast<int64_t>(len0));\n            sum2 = _mm_add_epi64(sum2, v);\n        }\n        uint64_t lo2 = _mm_cvtsi128_si64(sum2);\n        __m128i hi64_2 = _mm_unpackhi_epi64(sum2, sum2);\n        uint64_t hi2 = _mm_cvtsi128_si64(hi64_2);\n        total2 = lo2 + hi2;\n        if (i < n2) {\n            total2 += lst2[i].size();\n        }\n    }\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto total_length = [](const std::vector<std::string>& lst) -> size_t {\n        size_t total = 0;\n        size_t n = lst.size();\n        if (n == 0) return 0;\n        \n        __m128i vsum = _mm_setzero_si128();\n        size_t i = 0;\n        for (; i + 1 < n; i += 2) {\n            uint64_t len0 = lst[i].length();\n            uint64_t len1 = lst[i+1].length();\n            __m128i v = _mm_set_epi64x(static_cast<long long>(len1), static_cast<long long>(len0));\n            vsum = _mm_add_epi64(vsum, v);\n        }\n        \n        uint64_t temp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n        total = temp[0] + temp[1];\n        \n        if (i < n) {\n            total += lst[i].length();\n        }\n        return total;\n    };\n\n    size_t total1 = total_length(lst1);\n    size_t total2 = total_length(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto compute_total = [](const std::vector<std::string>& vec) -> uint64_t {\n        uint64_t total = 0;\n        size_t n = vec.size();\n        if (n == 0) \n            return 0;\n        size_t i = 0;\n        __m128i sum128 = _mm_setzero_si128();\n        for (; i + 1 < n; i += 2) {\n            uint64_t len0 = vec[i].size();\n            uint64_t len1 = vec[i+1].size();\n            __m128i v = _mm_set_epi64x(static_cast<int64_t>(len1), static_cast<int64_t>(len0));\n            sum128 = _mm_add_epi64(sum128, v);\n        }\n        uint64_t tmp[2];\n        _mm_storeu_si128((__m128i*)tmp, sum128);\n        total = tmp[0] + tmp[1];\n        if (i < n) {\n            total += vec[i].size();\n        }\n        return total;\n    };\n\n    uint64_t total1 = compute_total(lst1);\n    uint64_t total2 = compute_total(lst2);\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto total_length = [](const std::vector<std::string>& vec) -> uint64_t {\n        size_t n = vec.size();\n        if (n == 0) {\n            return 0;\n        }\n        __m128i total_vec = _mm_setzero_si128();\n        size_t i = 0;\n        for (; i + 1 < n; i += 2) {\n            uint64_t len0 = vec[i].size();\n            uint64_t len1 = vec[i+1].size();\n            __m128i lens = _mm_set_epi64x(static_cast<long long>(len1), static_cast<long long>(len0));\n            total_vec = _mm_add_epi64(total_vec, lens);\n        }\n        uint64_t temp[2];\n        _mm_storeu_si128((__m128i*)temp, total_vec);\n        uint64_t total = temp[0] + temp[1];\n        if (i < n) {\n            total += vec[i].size();\n        }\n        return total;\n    };\n\n    uint64_t total1 = total_length(lst1);\n    uint64_t total2 = total_length(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return (x == 0 || x == 1);\n    }\n    if (n == 1) {\n        return (x == 1);\n    }\n    if (x == 0) {\n        return false;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t base0 = 1;\n    uint64_t base1 = n;\n\n    while (true) {\n        if (base0 == x || base1 == x) {\n            return true;\n        }\n        if (base0 > x && base1 > x) {\n            break;\n        }\n\n        if (base0 == 0 || base1 == 0) {\n            break;\n        }\n\n        if (base0 > UINT64_MAX / n || base1 > UINT64_MAX / n) {\n            base0 = 0;\n            base1 = 0;\n        } else {\n            uint64_t next_base0 = base0 * n;\n            uint64_t next_base1 = base1 * n;\n\n            if (next_base0 > UINT64_MAX / n || next_base1 > UINT64_MAX / n) {\n                base0 = 0;\n                base1 = 0;\n            } else {\n                base0 = next_base0 * n;\n                base1 = next_base1 * n;\n            }\n        }\n\n        if (base0 == 0 && base1 == 0) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return x == 0;\n    }\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t powers[66];\n    int count = 0;\n    powers[count++] = 1;\n\n    while (count < 66) {\n        if (powers[count-1] > x / n) {\n            break;\n        }\n        powers[count] = powers[count-1] * n;\n        count++;\n    }\n\n    int i;\n    for (i = 0; i + 1 < count; i += 2) {\n        __m128i p_vec = _mm_loadu_si128((__m128i*)(powers + i));\n        uint64_t temp[2];\n        _mm_storeu_si128((__m128i*)temp, p_vec);\n        if (temp[0] == x || temp[1] == x) {\n            return true;\n        }\n    }\n    for (; i < count; i++) {\n        if (powers[i] == x) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 0) {\n        if (n == 0) return true;\n        else return false;\n    }\n    if (x == 1) {\n        if (n == 0) return false;\n        else return true;\n    }\n    if (n == 0) {\n        return false;\n    }\n    if (n == 1) {\n        return false;\n    }\n\n    uint64_t current_power = n;\n    while (current_power <= x) {\n        uint64_t power1 = current_power;\n        uint64_t power2 = current_power * n;\n\n        __m128i vec_p = _mm_set_epi64x(power2, power1);\n        __m128i vec_x = _mm_set1_epi64x(x);\n        __m128i neq = _mm_xor_si128(vec_p, vec_x);\n\n        uint64_t neq0 = _mm_cvtsi128_si64(neq);\n        __m128i neq_high128 = _mm_unpackhi_epi64(neq, neq);\n        uint64_t neq1 = _mm_cvtsi128_si64(neq_high128);\n\n        if (neq0 == 0 || neq1 == 0) {\n            return true;\n        }\n\n        if (power2 > UINT64_MAX / n) {\n            break;\n        }\n        current_power = power2 * n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) {\n        return true;\n    }\n    if (n == 1) {\n        return false;\n    }\n    if (n == x) {\n        return true;\n    }\n    if (n > x) {\n        return false;\n    }\n    if (n > UINT64_MAX / n) {\n        return false;\n    }\n    uint64_t base = n * n;\n    if (base > UINT64_MAX / n) {\n        return (base == x);\n    }\n    uint64_t p0 = base;\n    uint64_t p1 = n * base;\n\n    while (1) {\n        __m128i vp = _mm_set_epi64x(p1, p0);\n        __m128i vx = _mm_set1_epi64x(x);\n        __m128i neq = _mm_xor_si128(vp, vx);\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi8(neq, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if ((mask & 0xFF) == 0xFF) {\n            return true;\n        }\n        if ((mask & 0xFF00) == 0xFF00) {\n            return true;\n        }\n        if (p0 > x) {\n            break;\n        }\n        if (p0 > UINT64_MAX / base) {\n            break;\n        }\n        p0 = p0 * base;\n        p1 = p1 * base;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 0) {\n        return (x == 0);\n    }\n    if (x == 0) {\n        return false;\n    }\n    if (n == 1) {\n        return (x == 1);\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    uint64_t current = n;\n    for (uint64_t k = 1; ; k += 2) {\n        uint64_t next = current * n;\n        if (next < current) {\n            if (current == x) {\n                return true;\n            }\n            break;\n        }\n\n        __m128i v_p = _mm_set_epi64x(next, current);\n        __m128i v_x = _mm_set1_epi64x(x);\n        __m128i mask = _mm_set1_epi64x(0xFFFFFFFF);\n        __m128i v_p_low = _mm_and_si128(v_p, mask);\n        __m128i v_x_low = _mm_and_si128(v_x, mask);\n        __m128i v_p_high = _mm_srli_epi64(v_p, 32);\n        __m128i v_x_high = _mm_srli_epi64(v_x, 32);\n        __m128i cmp_low = _mm_cmpeq_epi32(v_p_low, v_x_low);\n        __m128i cmp_high = _mm_cmpeq_epi32(v_p_high, v_x_high);\n        __m128i cmp = _mm_and_si128(cmp_low, cmp_high);\n\n        uint32_t cmp_arr[4];\n        _mm_storeu_si128((__m128i*)cmp_arr, cmp);\n        if ((cmp_arr[0] && cmp_arr[1]) || (cmp_arr[2] && cmp_arr[3])) {\n            return true;\n        }\n\n        if (next > x) {\n            break;\n        }\n\n        uint64_t next2 = next * n;\n        if (next2 < next) {\n            break;\n        }\n        current = next2;\n        if (current > x) {\n            break;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    const char* p = num.c_str();\n    size_t len = num.length();\n    if (len == 0) return 0;\n\n    size_t i = 0;\n    int count = 0;\n\n    const __m128i v2 = _mm_set1_epi8('2');\n    const __m128i v3 = _mm_set1_epi8('3');\n    const __m128i v5 = _mm_set1_epi8('5');\n    const __m128i v7 = _mm_set1_epi8('7');\n    const __m128i vB = _mm_set1_epi8('B');\n    const __m128i vD = _mm_set1_epi8('D');\n    const __m128i one = _mm_set1_epi8(1);\n    const __m128i zero = _mm_setzero_si128();\n\n    for (; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i));\n        __m128i mask = _mm_cmpeq_epi8(data, v2);\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(data, v3));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(data, v5));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(data, v7));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(data, vB));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(data, vD));\n\n        __m128i prime_bytes = _mm_and_si128(mask, one);\n        __m128i sum1 = _mm_sad_epu8(prime_bytes, zero);\n        uint64_t low = static_cast<uint64_t>(_mm_cvtsi128_si64(sum1));\n        __m128i shifted = _mm_srli_si128(sum1, 8);\n        uint64_t high = static_cast<uint64_t>(_mm_cvtsi128_si64(shifted));\n        count += static_cast<int>(low + high);\n    }\n\n    for (; i < len; i++) {\n        char c = p[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    const char* str = num.c_str();\n    size_t len = num.size();\n    if (len == 0) return 0;\n\n    int count = 0;\n    size_t i = 0;\n\n    const __m128i prime2 = _mm_set1_epi8('2');\n    const __m128i prime3 = _mm_set1_epi8('3');\n    const __m128i prime5 = _mm_set1_epi8('5');\n    const __m128i prime7 = _mm_set1_epi8('7');\n    const __m128i primeB = _mm_set1_epi8('B');\n    const __m128i primeD = _mm_set1_epi8('D');\n\n    for (; i + 15 < len; i += 16) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(str + i));\n\n        __m128i cmp2 = _mm_cmpeq_epi8(v, prime2);\n        __m128i cmp3 = _mm_cmpeq_epi8(v, prime3);\n        __m128i cmp5 = _mm_cmpeq_epi8(v, prime5);\n        __m128i cmp7 = _mm_cmpeq_epi8(v, prime7);\n        __m128i cmpB = _mm_cmpeq_epi8(v, primeB);\n        __m128i cmpD = _mm_cmpeq_epi8(v, primeD);\n\n        __m128i any = _mm_or_si128(cmp2, cmp3);\n        any = _mm_or_si128(any, cmp5);\n        any = _mm_or_si128(any, cmp7);\n        any = _mm_or_si128(any, cmpB);\n        any = _mm_or_si128(any, cmpD);\n\n        int mask = _mm_movemask_epi8(any) & 0xFFFF;\n        mask = (mask & 0x5555) + ((mask >> 1) & 0x5555);\n        mask = (mask & 0x3333) + ((mask >> 2) & 0x3333);\n        mask = (mask & 0x0F0F) + ((mask >> 4) & 0x0F0F);\n        mask = (mask & 0x00FF) + ((mask >> 8) & 0x00FF);\n        count += mask;\n    }\n\n    for (; i < len; i++) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    size_t len = num.length();\n    if (len == 0) return 0;\n    \n    int count = 0;\n    size_t i = 0;\n    const char* str = num.c_str();\n    \n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i v2 = _mm_set1_epi8('2');\n        __m128i v3 = _mm_set1_epi8('3');\n        __m128i v5 = _mm_set1_epi8('5');\n        __m128i v7 = _mm_set1_epi8('7');\n        __m128i vB = _mm_set1_epi8('B');\n        __m128i vD = _mm_set1_epi8('D');\n        \n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, v2);\n        __m128i cmp3 = _mm_cmpeq_epi8(chunk, v3);\n        __m128i cmp5 = _mm_cmpeq_epi8(chunk, v5);\n        __m128i cmp7 = _mm_cmpeq_epi8(chunk, v7);\n        __m128i cmpB = _mm_cmpeq_epi8(chunk, vB);\n        __m128i cmpD = _mm_cmpeq_epi8(chunk, vD);\n        \n        __m128i any = _mm_or_si128(cmp2, cmp3);\n        any = _mm_or_si128(any, cmp5);\n        any = _mm_or_si128(any, cmp7);\n        any = _mm_or_si128(any, cmpB);\n        any = _mm_or_si128(any, cmpD);\n        \n        int mask = _mm_movemask_epi8(any);\n        count += _mm_popcnt_u32(mask);\n    }\n    \n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    size_t len = num.length();\n    if (len == 0) \n        return 0;\n    \n    size_t i = 0;\n    int count = 0;\n    const __m128i zero_char = _mm_set1_epi8('0');\n    const __m128i nine = _mm_set1_epi8(9);\n    const __m128i seven = _mm_set1_epi8(7);\n    const __m128i prime2 = _mm_set1_epi8(2);\n    const __m128i prime3 = _mm_set1_epi8(3);\n    const __m128i prime5 = _mm_set1_epi8(5);\n    const __m128i prime7 = _mm_set1_epi8(7);\n    const __m128i prime11 = _mm_set1_epi8(11);\n    const __m128i prime13 = _mm_set1_epi8(13);\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(num.data() + i));\n        __m128i v = _mm_sub_epi8(chunk, zero_char);\n        __m128i mask_gt9 = _mm_cmpgt_epi8(v, nine);\n        __m128i adjust = _mm_and_si128(mask_gt9, seven);\n        v = _mm_sub_epi8(v, adjust);\n        \n        __m128i eq2 = _mm_cmpeq_epi8(v, prime2);\n        __m128i eq3 = _mm_cmpeq_epi8(v, prime3);\n        __m128i eq5 = _mm_cmpeq_epi8(v, prime5);\n        __m128i eq7 = _mm_cmpeq_epi8(v, prime7);\n        __m128i eq11 = _mm_cmpeq_epi8(v, prime11);\n        __m128i eq13 = _mm_cmpeq_epi8(v, prime13);\n        \n        __m128i or1 = _mm_or_si128(eq2, eq3);\n        __m128i or2 = _mm_or_si128(eq5, eq7);\n        __m128i or3 = _mm_or_si128(eq11, eq13);\n        __m128i or12 = _mm_or_si128(or1, or2);\n        __m128i or_all = _mm_or_si128(or12, or3);\n        \n        int mask = _mm_movemask_epi8(or_all);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        int val = c - '0';\n        if (val > 9) \n            val -= 7;\n        if (val == 2 || val == 3 || val == 5 || val == 7 || val == 11 || val == 13) \n            count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) \n        return 0;\n\n    const char* str = num.c_str();\n    size_t len = num.size();\n    size_t i = 0;\n    int total = 0;\n\n    const __m128i prime2 = _mm_set1_epi8('2');\n    const __m128i prime3 = _mm_set1_epi8('3');\n    const __m128i prime5 = _mm_set1_epi8('5');\n    const __m128i prime7 = _mm_set1_epi8('7');\n    const __m128i primeB = _mm_set1_epi8('B');\n    const __m128i primeD = _mm_set1_epi8('D');\n    const __m128i ones_vector = _mm_set1_epi8(1);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        \n        __m128i mask = _mm_cmpeq_epi8(v, prime2);\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(v, prime3));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(v, prime5));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(v, prime7));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(v, primeB));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(v, primeD));\n        \n        __m128i t = _mm_and_si128(mask, ones_vector);\n        __m128i sum = _mm_sad_epu8(t, _mm_setzero_si128());\n        \n        uint64_t tmp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(tmp), sum);\n        total += static_cast<int>(tmp[0] + tmp[1]);\n    }\n\n    for (; i < len; ++i) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            total++;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s){\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n\n    size_t i;\n    for (i = 0; i <= n - 17; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(s.data() + i + 1));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n    for (; i < n - 1; i++) {\n        if (s[i] == s[i+1]) {\n            return false;\n        }\n    }\n\n    size_t j;\n    for (j = 0; j <= n - 18; j += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(s.data() + j));\n        __m128i b = _mm_loadu_si128((const __m128i*)(s.data() + j + 2));\n        __m128i cmp = _mm_cmpeq_epi8(a, b);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n    for (; j < n - 2; j++) {\n        if (s[j] == s[j+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n    const char* data = s.data();\n    size_t i = 0;\n\n    for (; i <= n - 18; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(data + i));\n        __m128i b = _mm_loadu_si128((const __m128i*)(data + i + 1));\n        __m128i c = _mm_loadu_si128((const __m128i*)(data + i + 2));\n\n        __m128i eq_ab = _mm_cmpeq_epi8(a, b);\n        __m128i eq_ac = _mm_cmpeq_epi8(a, c);\n        __m128i eq_bc = _mm_cmpeq_epi8(b, c);\n\n        __m128i any_eq = _mm_or_si128(eq_ab, eq_ac);\n        any_eq = _mm_or_si128(any_eq, eq_bc);\n\n        if (_mm_movemask_epi8(any_eq) != 0) {\n            return false;\n        }\n    }\n\n    for (; i <= n - 3; i++) {\n        if (data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.length();\n    if (n < 3) {\n        return false;\n    }\n    const char* p = s.data();\n    size_t i = 0;\n\n    if (n >= 18) {\n        for (; i <= n - 18; i += 16) {\n            __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i));\n            __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i + 1));\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i + 2));\n\n            __m128i cmp0 = _mm_cmpeq_epi8(v0, v1);\n            __m128i cmp1 = _mm_cmpeq_epi8(v0, v2);\n            __m128i cmp2 = _mm_cmpeq_epi8(v1, v2);\n\n            __m128i any_eq = _mm_or_si128(cmp0, _mm_or_si128(cmp1, cmp2));\n            if (_mm_movemask_epi8(any_eq)) {\n                return false;\n            }\n        }\n    }\n\n    for (; i <= n - 3; ++i) {\n        if (p[i] == p[i+1] || p[i] == p[i+2] || p[i+1] == p[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n    const char* data = s.data();\n    size_t i = 0;\n\n    while (i <= n - 18) {\n        __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i + 1));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i + 2));\n\n        __m128i cmp01 = _mm_cmpeq_epi8(v0, v1);\n        __m128i cmp02 = _mm_cmpeq_epi8(v0, v2);\n        __m128i cmp12 = _mm_cmpeq_epi8(v1, v2);\n\n        __m128i any_eq = _mm_or_si128(_mm_or_si128(cmp01, cmp02), cmp12);\n\n        if (_mm_movemask_epi8(any_eq) != 0) {\n            return false;\n        }\n\n        i += 16;\n    }\n\n    for (; i <= n - 3; ++i) {\n        if (data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s){\n    size_t n = s.size();\n    if (n < 3) {\n        return false;\n    }\n\n    const char* data = s.data();\n    size_t i = 0;\n\n    for (; i < n - 17; i += 16) {\n        __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i + 1));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i + 2));\n\n        __m128i eq01 = _mm_cmpeq_epi8(v0, v1);\n        __m128i eq02 = _mm_cmpeq_epi8(v0, v2);\n        __m128i eq12 = _mm_cmpeq_epi8(v1, v2);\n\n        __m128i any_eq = _mm_or_si128(_mm_or_si128(eq01, eq02), eq12);\n\n        if (_mm_movemask_epi8(any_eq) != 0) {\n            return false;\n        }\n    }\n\n    for (; i <= n - 3; ++i) {\n        if (data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.size();\n    if (len < 2) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    int n = static_cast<int>(len);\n    int limit = static_cast<int>(std::sqrt(n)) + 1;\n\n    for (int base = 3; base <= limit; base += 8) {\n        int d0 = base;\n        int d1 = base + 2;\n        int d2 = base + 4;\n        int d3 = base + 6;\n\n        int r0 = (d0 <= limit) ? (n % d0) : 1;\n        int r1 = (d1 <= limit) ? (n % d1) : 1;\n        int r2 = (d2 <= limit) ? (n % d2) : 1;\n        int r3 = (d3 <= limit) ? (n % d3) : 1;\n\n        __m128i rems = _mm_setr_epi32(r0, r1, r2, r3);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpeq_epi32(rems, zero);\n        if (_mm_movemask_epi8(mask) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    size_t n = str.length();\n    if (n <= 1) \n        return false;\n    if (n == 2)\n        return true;\n    if (n % 2 == 0)\n        return false;\n\n    if (n >= (1 << 24)) {\n        for (size_t i = 3; i * i <= n; i += 2) {\n            if (n % i == 0)\n                return false;\n        }\n        return true;\n    }\n\n    size_t limit = static_cast<size_t>(std::sqrt(n)) + 1;\n    for (size_t i = 3; i <= limit; i += 8) {\n        __m128i divisors = _mm_setr_epi32(i, i+2, i+4, i+6);\n        __m128i limit_plus_one = _mm_set1_epi32(limit + 1);\n        __m128i mask_valid = _mm_cmplt_epi32(divisors, limit_plus_one);\n        __m128 div_vec_f = _mm_cvtepi32_ps(divisors);\n        __m128 n_vec_f = _mm_set1_ps(static_cast<float>(n));\n        __m128 quot_f = _mm_div_ps(n_vec_f, div_vec_f);\n        __m128i quot_i = _mm_cvttps_epi32(quot_f);\n        __m128 quot_f2 = _mm_cvtepi32_ps(quot_i);\n        __m128 product_f = _mm_mul_ps(div_vec_f, quot_f2);\n        __m128i product = _mm_cvttps_epi32(product_f);\n        __m128i rem = _mm_sub_epi32(_mm_set1_epi32(static_cast<int>(n)), product);\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i rem_valid = _mm_or_si128(\n            _mm_and_si128(mask_valid, rem),\n            _mm_andnot_si128(mask_valid, ones)\n        );\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(rem_valid, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <cmath>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) {\n        return false;\n    }\n    if (len == 2) {\n        return true;\n    }\n    if (len % 2 == 0) {\n        return false;\n    }\n\n    size_t limit = (size_t)(std::sqrt((double)len)) + 1;\n    for (size_t d = 3; d <= limit; d += 8) {\n        size_t divisors[4] = {0};\n        int count = 0;\n        if (d <= limit) {\n            divisors[count++] = d;\n        }\n        if (d + 2 <= limit) {\n            divisors[count++] = d + 2;\n        }\n        if (d + 4 <= limit) {\n            divisors[count++] = d + 4;\n        }\n        if (d + 6 <= limit) {\n            divisors[count++] = d + 6;\n        }\n\n        if (count == 0) {\n            break;\n        }\n\n        unsigned int r0 = (count > 0) ? (unsigned int)(len % divisors[0]) : 1;\n        unsigned int r1 = (count > 1) ? (unsigned int)(len % divisors[1]) : 1;\n        unsigned int r2 = (count > 2) ? (unsigned int)(len % divisors[2]) : 1;\n        unsigned int r3 = (count > 3) ? (unsigned int)(len % divisors[3]) : 1;\n\n        __m128i rem_vec = _mm_set_epi32(r3, r2, r1, r0);\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(rem_vec, zero);\n\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int n = str.length();\n    if (n <= 1) {\n        return false;\n    }\n    if (n == 2) {\n        return true;\n    }\n    if (n % 2 == 0) {\n        return false;\n    }\n    if (n < 9) {\n        return true;\n    }\n    int root = static_cast<int>(std::sqrt(n));\n    int d = 3;\n    for (; d <= root - 2; d += 4) {\n        double divisors[2] = {static_cast<double>(d), static_cast<double>(d+2)};\n        __m128d div_vec = _mm_loadu_pd(divisors);\n        __m128d n_vec = _mm_set1_pd(static_cast<double>(n));\n        __m128d q_vec = _mm_div_pd(n_vec, div_vec);\n        __m128i q_int = _mm_cvttpd_epi32(q_vec);\n        __m128d q_dbl = _mm_cvtepi32_pd(q_int);\n        __m128d rem_dbl = _mm_sub_pd(n_vec, _mm_mul_pd(div_vec, q_dbl));\n        __m128i rem_int = _mm_cvttpd_epi32(rem_dbl);\n        int rem0 = _mm_cvtsi128_si32(rem_int);\n        int rem1 = _mm_cvtsi128_si32(_mm_srli_si128(rem_int, 4));\n        if (rem0 == 0 || rem1 == 0) {\n            return false;\n        }\n    }\n    for (; d <= root; d += 2) {\n        if (n % d == 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int n = str.length();\n    if (n <= 1) \n        return false;\n    if (n == 2) \n        return true;\n    if (n % 2 == 0) \n        return false;\n    \n    int limit = static_cast<int>(std::sqrt(n)) + 1;\n    \n    for (int d = 3; d <= limit; d += 8) {\n        __m128i divisors = _mm_set_epi32(d+6, d+4, d+2, d);\n        __m128i limitv = _mm_set1_epi32(limit);\n        __m128i mask_invalid = _mm_cmpgt_epi32(divisors, limitv);\n        __m128i mask_valid = _mm_xor_si128(mask_invalid, _mm_set1_epi32(-1));\n        \n        __m128 div_f = _mm_cvtepi32_ps(divisors);\n        __m128 nv_f = _mm_set1_ps(static_cast<float>(n));\n        __m128 quot_f = _mm_div_ps(nv_f, div_f);\n        __m128i quot_i = _mm_cvttps_epi32(quot_f);\n        __m128 quot_i_f = _mm_cvtepi32_ps(quot_i);\n        __m128 product_f = _mm_mul_ps(div_f, quot_i_f);\n        __m128i product_i = _mm_cvttps_epi32(product_f);\n        \n        __m128i nv = _mm_set1_epi32(n);\n        __m128i rem = _mm_sub_epi32(nv, product_i);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask_zero = _mm_cmpeq_epi32(rem, zero);\n        __m128i mask_found = _mm_and_si128(mask_zero, mask_valid);\n        \n        if (_mm_movemask_epi8(mask_found) != 0) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <string>\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N) {\n    char buffer[32];\n    for (int i = 0; i < 32; ++i) {\n        buffer[i] = '0';\n    }\n\n    uint64_t temp = N;\n    int len = 0;\n    while (temp) {\n        buffer[len++] = '0' + (temp % 10);\n        temp /= 10;\n    }\n\n    __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(buffer));\n    __m128i v2 = _mm_loadu_si128(reinterpret_cast<__m128i*>(buffer + 16));\n\n    __m128i zeros_char = _mm_set1_epi8('0');\n    v1 = _mm_sub_epi8(v1, zeros_char);\n    v2 = _mm_sub_epi8(v2, zeros_char);\n\n    __m128i v1_sum = _mm_sad_epu8(v1, _mm_setzero_si128());\n    __m128i v2_sum = _mm_sad_epu8(v2, _mm_setzero_si128());\n    __m128i v_sum = _mm_add_epi64(v1_sum, v2_sum);\n\n    uint64_t sum_arr[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_arr), v_sum);\n    unsigned int total_sum = static_cast<unsigned int>(sum_arr[0] + sum_arr[1]);\n\n    if (total_sum == 0) {\n        return \"0\";\n    }\n\n    char bin_buf[32] = {0};\n    int idx = 0;\n    unsigned int num = total_sum;\n    while (num) {\n        bin_buf[idx++] = '0' + (num & 1);\n        num >>= 1;\n    }\n    for (int i = 0; i < idx / 2; ++i) {\n        char t = bin_buf[i];\n        bin_buf[i] = bin_buf[idx - 1 - i];\n        bin_buf[idx - 1 - i] = t;\n    }\n\n    return std::string(bin_buf, idx);\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    if (N == 0) {\n        return \"0\";\n    }\n    std::string str = std::to_string(N);\n    int len = str.length();\n    char buffer[32];\n    std::memset(buffer, '0', 32);\n    std::memcpy(buffer, str.c_str(), len);\n    __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer));\n    __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer + 16));\n    __m128i zero_char = _mm_set1_epi8('0');\n    v1 = _mm_sub_epi8(v1, zero_char);\n    v2 = _mm_sub_epi8(v2, zero_char);\n    __m128i zero = _mm_setzero_si128();\n    __m128i s1 = _mm_sad_epu8(v1, zero);\n    __m128i s2 = _mm_sad_epu8(v2, zero);\n    __m128i s = _mm_add_epi64(s1, s2);\n    uint64_t tval[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(tval), s);\n    uint64_t digit_sum = tval[0] + tval[1];\n    if (digit_sum == 0) {\n        return \"0\";\n    }\n    std::string binary;\n    while (digit_sum) {\n        binary = static_cast<char>('0' + (digit_sum & 1)) + binary;\n        digit_sum >>= 1;\n    }\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <cstdio>\n#include <cstring>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N) {\n    char digits[21];\n    snprintf(digits, sizeof(digits), \"%020llu\", static_cast<unsigned long long>(N));\n\n    __m128i zero_char = _mm_set1_epi8('0');\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i mask_low4 = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1);\n\n    __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(digits));\n    v1 = _mm_sub_epi8(v1, zero_char);\n    __m128i s1 = _mm_sad_epu8(v1, zero_vec);\n\n    uint32_t last4_digits;\n    memcpy(&last4_digits, digits + 16, 4);\n    __m128i v2 = _mm_cvtsi32_si128(last4_digits);\n    v2 = _mm_sub_epi8(v2, zero_char);\n    v2 = _mm_and_si128(v2, mask_low4);\n    __m128i s2 = _mm_sad_epu8(v2, zero_vec);\n\n    uint64_t s1_arr[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(s1_arr), s1);\n    uint64_t s2_arr[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(s2_arr), s2);\n    uint64_t total_sum = s1_arr[0] + s1_arr[1] + s2_arr[0];\n\n    if (total_sum == 0) {\n        return \"0\";\n    }\n    std::string bin;\n    uint64_t num = total_sum;\n    while (num) {\n        bin = static_cast<char>('0' + (num & 1)) + bin;\n        num >>= 1;\n    }\n    return bin;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <string>\n#include <cstring>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N) {\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n    \n    char buffer[32];\n    memset(buffer, '0', sizeof(buffer));\n    memcpy(buffer, num_str.c_str(), len);\n    \n    int num_chunks = (len + 15) / 16;\n    __m128i total = _mm_setzero_si128();\n    \n    for (int i = 0; i < num_chunks; i++) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer + 16*i));\n        __m128i digits = _mm_sub_epi8(chunk, _mm_set1_epi8('0'));\n        __m128i sum_part = _mm_sad_epu8(digits, _mm_setzero_si128());\n        total = _mm_add_epi64(total, sum_part);\n    }\n    \n    uint64_t t[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(t), total);\n    uint64_t total_sum = t[0] + t[1];\n    \n    if (total_sum == 0) {\n        return \"0\";\n    }\n    \n    std::string bin_str;\n    while (total_sum) {\n        bin_str = (char)((total_sum & 1) + '0') + bin_str;\n        total_sum >>= 1;\n    }\n    return bin_str;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N) {\n    if (N == 0) {\n        return \"0\";\n    }\n\n    char buffer_digits[32];\n    for (int i = 0; i < 32; i++) {\n        buffer_digits[i] = '0';\n    }\n    char temp[32];\n    int len = 0;\n    uint64_t n = N;\n    while (n) {\n        temp[len++] = '0' + (n % 10);\n        n /= 10;\n    }\n    for (int i = 0; i < len; i++) {\n        buffer_digits[i] = temp[len - 1 - i];\n    }\n\n    __m128i v1 = _mm_loadu_si128((__m128i*)buffer_digits);\n    __m128i v2 = _mm_loadu_si128((__m128i*)(buffer_digits + 16));\n    __m128i zeros_char = _mm_set1_epi8('0');\n    v1 = _mm_sub_epi8(v1, zeros_char);\n    v2 = _mm_sub_epi8(v2, zeros_char);\n\n    __m128i s1 = _mm_sad_epu8(v1, _mm_setzero_si128());\n    __m128i s2 = _mm_sad_epu8(v2, _mm_setzero_si128());\n\n    uint64_t sum_arr[2];\n    _mm_storeu_si128((__m128i*)sum_arr, s1);\n    uint64_t part1 = sum_arr[0] + sum_arr[1];\n    _mm_storeu_si128((__m128i*)sum_arr, s2);\n    uint64_t part2 = sum_arr[0] + sum_arr[1];\n    uint64_t total_sum = part1 + part2;\n\n    if (total_sum == 0) {\n        return \"0\";\n    }\n\n    __m128i shift_counts = _mm_setr_epi8(7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m128i zero_si128 = _mm_setzero_si128();\n    __m128i shift_counts_16 = _mm_unpacklo_epi8(shift_counts, zero_si128);\n    __m128i x_vec = _mm_set1_epi16(total_sum);\n    __m128i shifted = _mm_srl_epi16(x_vec, shift_counts_16);\n    __m128i bits = _mm_and_si128(shifted, _mm_set1_epi16(1));\n    __m128i bits_packed = _mm_packus_epi16(bits, zero_si128);\n    __m128i bin_chars = _mm_add_epi8(bits_packed, _mm_set1_epi8('0'));\n\n    char bin_buffer[16] = {0};\n    _mm_storeu_si128((__m128i*)bin_buffer, bin_chars);\n\n    int start = 0;\n    while (start < 8 && bin_buffer[start] == '0') {\n        start++;\n    }\n    if (start == 8) {\n        return \"0\";\n    }\n    return std::string(bin_buffer + start, 8 - start);\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    int i = 0;\n    __m128i total_vec = _mm_setzero_si128();\n    const __m128i ones = _mm_set1_epi32(1);\n    __m128i mask_odd = _mm_set_epi32(1, 0, 1, 0);\n    __m128i odd_index_mask = _mm_cmpeq_epi32(mask_odd, ones);\n\n    for (; i <= n-4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i lsb = _mm_and_si128(chunk, ones);\n        __m128i even_mask = _mm_cmpeq_epi32(lsb, _mm_setzero_si128());\n        __m128i condition_mask = _mm_and_si128(odd_index_mask, even_mask);\n        __m128i selected = _mm_and_si128(chunk, condition_mask);\n        total_vec = _mm_add_epi32(total_vec, selected);\n    }\n\n    __m128i tmp = _mm_add_epi32(total_vec, _mm_srli_si128(total_vec, 8));\n    int sum0 = _mm_cvtsi128_si32(tmp);\n    int sum1 = _mm_cvtsi128_si32(_mm_srli_si128(tmp, 4));\n    int total = sum0 + sum1;\n\n    for (; i < n; i++) {\n        if (i % 2 == 1 && (lst[i] % 2) == 0) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) \n        return 0;\n\n    __m128i index_mask = _mm_set_epi32(-1, 0, -1, 0);\n    __m128i vsum = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i lsb = _mm_and_si128(data, ones);\n        __m128i even_mask = _mm_cmpeq_epi32(lsb, _mm_setzero_si128());\n        __m128i mask = _mm_and_si128(even_mask, index_mask);\n        __m128i selected = _mm_and_si128(data, mask);\n        vsum = _mm_add_epi32(vsum, selected);\n    }\n\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n    int total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; i++) {\n        if (i % 2 == 1 && (lst[i] % 2 == 0)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst){\n    int n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n    __m128i accum = _mm_setzero_si128();\n    int i = 0;\n    __m128i base_indices = _mm_setr_epi32(0,1,2,3);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i <= n-4; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)&lst[i]);\n        __m128i base = _mm_set1_epi32(i);\n        __m128i indices_vec = _mm_add_epi32(base, base_indices);\n        __m128i index_lsb = _mm_and_si128(indices_vec, one);\n        __m128i index_mask = _mm_cmpeq_epi32(index_lsb, one);\n        __m128i element_lsb = _mm_and_si128(data, one);\n        __m128i element_mask = _mm_cmpeq_epi32(element_lsb, zero);\n        __m128i mask = _mm_and_si128(index_mask, element_mask);\n        __m128i selected = _mm_and_si128(data, mask);\n        accum = _mm_add_epi32(accum, selected);\n    }\n\n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, accum);\n    int total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    for (; i < n; i++) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i accumulator = _mm_setzero_si128();\n    __m128i pos_mask = _mm_setr_epi32(0, -1, 0, -1);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i in = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i lowbit = _mm_and_si128(in, one);\n        __m128i even_mask = _mm_cmpeq_epi32(lowbit, zero);\n        __m128i mask = _mm_and_si128(even_mask, pos_mask);\n        __m128i selected = _mm_and_si128(in, mask);\n        accumulator = _mm_add_epi32(accumulator, selected);\n    }\n\n    __m128i shift1 = _mm_srli_si128(accumulator, 4);\n    __m128i shift3 = _mm_srli_si128(accumulator, 12);\n    int total = _mm_cvtsi128_si32(shift1) + _mm_cvtsi128_si32(shift3);\n\n    for (; i < n; ++i) {\n        if ((i & 1) && !(lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i total_vec = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i index_mask = _mm_set_epi32(1, 0, 1, 0);\n    __m128i index_flag = _mm_cmpeq_epi32(index_mask, ones);\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i lsb = _mm_and_si128(data, ones);\n        __m128i even_flag = _mm_cmpeq_epi32(lsb, _mm_setzero_si128());\n        __m128i condition = _mm_and_si128(index_flag, even_flag);\n        __m128i selected = _mm_and_si128(data, condition);\n        total_vec = _mm_add_epi32(total_vec, selected);\n    }\n\n    int tail_sum = 0;\n    for (; i < n; i++) {\n        if (i % 2 == 1 && (lst[i] % 2) == 0) {\n            tail_sum += lst[i];\n        }\n    }\n\n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, total_vec);\n    int sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + tail_sum;\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) return s;\n    \n    std::string result(s.size(), '\\0');\n    const size_t len = s.size();\n    const size_t simd_size = len - (len % 16);\n    const char* src = s.data();\n    char* dst = &result[0];\n    \n    const __m128i add_val = _mm_set1_epi8(4);\n    const __m128i z_val = _mm_set1_epi8('z');\n    const __m128i wrap_val = _mm_set1_epi8(26);\n    \n    for (size_t i = 0; i < simd_size; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i added = _mm_add_epi8(vec, add_val);\n        __m128i mask = _mm_cmpgt_epi8(added, z_val);\n        __m128i to_sub = _mm_and_si128(mask, wrap_val);\n        __m128i final_vec = _mm_sub_epi8(added, to_sub);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), final_vec);\n    }\n    \n    for (size_t i = simd_size; i < len; ++i) {\n        char c = src[i] + 4;\n        if (c > 'z') {\n            c -= 26;\n        }\n        dst[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) return s;\n    size_t len = s.size();\n    std::string out;\n    out.resize(len);\n    size_t i = 0;\n    const __m128i a_minus = _mm_set1_epi8('a' - 1);\n    const __m128i z_plus = _mm_set1_epi8('z' + 1);\n    const __m128i four = _mm_set1_epi8(4);\n    const __m128i z_char = _mm_set1_epi8('z');\n    const __m128i twenty_six = _mm_set1_epi8(26);\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i ge_a = _mm_cmpgt_epi8(chunk, a_minus);\n        __m128i le_z = _mm_cmplt_epi8(chunk, z_plus);\n        __m128i mask_letter = _mm_and_si128(ge_a, le_z);\n        __m128i to_add = _mm_and_si128(mask_letter, four);\n        __m128i shifted = _mm_add_epi8(chunk, to_add);\n        __m128i mask_wrap = _mm_cmpgt_epi8(shifted, z_char);\n        mask_wrap = _mm_and_si128(mask_wrap, mask_letter);\n        __m128i to_sub = _mm_and_si128(mask_wrap, twenty_six);\n        __m128i result = _mm_sub_epi8(shifted, to_sub);\n        _mm_storeu_si128((__m128i*)(out.data() + i), result);\n    }\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            c += 4;\n            if (c > 'z') {\n                c -= 26;\n            }\n        }\n        out[i] = c;\n    }\n    return out;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) return \"\";\n    std::string res(s.size(), ' ');\n    const char* src = s.data();\n    char* dst = &res[0];\n    size_t i = 0;\n    const size_t len = s.size();\n    \n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i base = _mm_set1_epi8('a');\n        __m128i relative = _mm_sub_epi8(chunk, base);\n        __m128i shift_val = _mm_set1_epi8(4);\n        __m128i shifted = _mm_add_epi8(relative, shift_val);\n        __m128i twenty_five = _mm_set1_epi8(25);\n        __m128i mask = _mm_cmpgt_epi8(shifted, twenty_five);\n        __m128i twenty_six = _mm_set1_epi8(26);\n        __m128i corrected = _mm_sub_epi8(shifted, twenty_six);\n        __m128i mod_result = _mm_or_si128(\n            _mm_and_si128(mask, corrected),\n            _mm_andnot_si128(mask, shifted)\n        );\n        __m128i encrypted_chunk = _mm_add_epi8(mod_result, base);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), encrypted_chunk);\n    }\n\n    for (; i < len; ++i) {\n        char c = src[i];\n        int shifted = (c - 'a' + 4) % 26;\n        dst[i] = 'a' + shifted;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) return s;\n    std::string res(s.length(), 0);\n    size_t i = 0;\n\n    for (; i + 16 <= s.length(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n        __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n        __m128i ge_a = _mm_cmpgt_epi8(data, a_minus1);\n        __m128i le_z = _mm_cmplt_epi8(data, z_plus1);\n        __m128i is_letter = _mm_and_si128(ge_a, le_z);\n\n        __m128i base = _mm_sub_epi8(data, _mm_set1_epi8('a'));\n        __m128i base_plus4 = _mm_add_epi8(base, _mm_set1_epi8(4));\n        __m128i cmp = _mm_cmpgt_epi8(base_plus4, _mm_set1_epi8(25));\n        __m128i mask = _mm_and_si128(cmp, _mm_set1_epi8(26));\n        __m128i adjusted = _mm_sub_epi8(base_plus4, mask);\n        __m128i transformed = _mm_add_epi8(adjusted, _mm_set1_epi8('a'));\n\n        __m128i res_vec = _mm_or_si128(\n            _mm_and_si128(is_letter, transformed),\n            _mm_andnot_si128(is_letter, data)\n        );\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), res_vec);\n    }\n\n    for (; i < s.length(); ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            int x = c - 'a';\n            x += 4;\n            if (x >= 26) {\n                x -= 26;\n            }\n            c = 'a' + x;\n        }\n        res[i] = c;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    if (s.empty()) return \"\";\n    std::string out(s.size(), '\\0');\n    size_t i = 0;\n    const size_t len = s.size();\n    const size_t simd_limit = len - 15;\n\n    for (; i < simd_limit; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i shifted = _mm_add_epi8(chunk, _mm_set1_epi8(4));\n        __m128i cmp = _mm_cmpgt_epi8(shifted, _mm_set1_epi8('z'));\n        __m128i adjust = _mm_and_si128(cmp, _mm_set1_epi8(26));\n        __m128i result = _mm_sub_epi8(shifted, adjust);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    for (; i < len; ++i) {\n        char c = s[i] + 4;\n        if (c > 'z') c -= 26;\n        out[i] = c;\n    }\n    return out;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    if (len == 0) {\n        return \"\";\n    }\n    std::string result(len, '\\0');\n    const char* src = message.data();\n    char* dst = &result[0];\n    size_t i = 0;\n\n    const __m128i upper_bound_low = _mm_set1_epi8('A' - 1);\n    const __m128i upper_bound_high = _mm_set1_epi8('Z' + 1);\n    const __m128i lower_bound_low = _mm_set1_epi8('a' - 1);\n    const __m128i lower_bound_high = _mm_set1_epi8('z' + 1);\n    const __m128i flip_mask = _mm_set1_epi8(0x20);\n    const char vowels_str[] = \"aeiouAEIOU\";\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(src + i));\n        \n        __m128i in_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, upper_bound_low),\n            _mm_cmpgt_epi8(upper_bound_high, chunk)\n        );\n        __m128i in_lower = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, lower_bound_low),\n            _mm_cmpgt_epi8(lower_bound_high, chunk)\n        );\n        __m128i is_letter = _mm_or_si128(in_upper, in_lower);\n        \n        __m128i swap_val = _mm_and_si128(is_letter, flip_mask);\n        __m128i swapped = _mm_xor_si128(chunk, swap_val);\n        \n        __m128i is_vowel = _mm_setzero_si128();\n        for (int j = 0; j < 10; j++) {\n            __m128i vowel = _mm_set1_epi8(vowels_str[j]);\n            __m128i mask = _mm_cmpeq_epi8(swapped, vowel);\n            is_vowel = _mm_or_si128(is_vowel, mask);\n        }\n        is_vowel = _mm_and_si128(is_vowel, is_letter);\n        \n        __m128i add_val = _mm_and_si128(is_vowel, _mm_set1_epi8(2));\n        __m128i res = _mm_add_epi8(swapped, add_val);\n        \n        _mm_storeu_si128((__m128i*)(dst + i), res);\n    }\n\n    for (; i < len; i++) {\n        char c = src[i];\n        if (('A' <= c && c <= 'Z') || ('a' <= c && c <= 'z')) {\n            c ^= 0x20;\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c += 2;\n            }\n        }\n        dst[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result;\n    if (message.empty()) return result;\n    result.resize(message.size());\n\n    size_t i = 0;\n    const __m128i A_bound_low = _mm_set1_epi8('A' - 1);\n    const __m128i A_bound_high = _mm_set1_epi8('Z' + 1);\n    const __m128i a_bound_low = _mm_set1_epi8('a' - 1);\n    const __m128i a_bound_high = _mm_set1_epi8('z' + 1);\n    const __m128i mask_0x20 = _mm_set1_epi8(0x20);\n    const __m128i mask_0xDF = _mm_set1_epi8(0xDF);\n    const __m128i two = _mm_set1_epi8(2);\n    const __m128i vowel_A = _mm_set1_epi8('A');\n    const __m128i vowel_E = _mm_set1_epi8('E');\n    const __m128i vowel_I = _mm_set1_epi8('I');\n    const __m128i vowel_O = _mm_set1_epi8('O');\n    const __m128i vowel_U = _mm_set1_epi8('U');\n\n    for (; i + 16 <= message.size(); i += 16) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(message.data() + i));\n\n        __m128i is_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(v, A_bound_low),\n            _mm_cmplt_epi8(v, A_bound_high)\n        );\n        __m128i is_lower = _mm_and_si128(\n            _mm_cmpgt_epi8(v, a_bound_low),\n            _mm_cmplt_epi8(v, a_bound_high)\n        );\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n        __m128i swap_val = _mm_and_si128(is_letter, mask_0x20);\n        __m128i swapped = _mm_xor_si128(v, swap_val);\n\n        __m128i swapped_upper = _mm_and_si128(swapped, mask_0xDF);\n        __m128i cmpA = _mm_cmpeq_epi8(swapped_upper, vowel_A);\n        __m128i cmpE = _mm_cmpeq_epi8(swapped_upper, vowel_E);\n        __m128i cmpI = _mm_cmpeq_epi8(swapped_upper, vowel_I);\n        __m128i cmpO = _mm_cmpeq_epi8(swapped_upper, vowel_O);\n        __m128i cmpU = _mm_cmpeq_epi8(swapped_upper, vowel_U);\n\n        __m128i vowel_cmp = _mm_or_si128(cmpA, cmpE);\n        vowel_cmp = _mm_or_si128(vowel_cmp, cmpI);\n        vowel_cmp = _mm_or_si128(vowel_cmp, cmpO);\n        vowel_cmp = _mm_or_si128(vowel_cmp, cmpU);\n\n        __m128i replaced = _mm_add_epi8(swapped, two);\n        replaced = _mm_or_si128(\n            _mm_and_si128(vowel_cmp, replaced),\n            _mm_andnot_si128(vowel_cmp, swapped)\n        );\n\n        _mm_storeu_si128((__m128i*)(result.data() + i), replaced);\n    }\n\n    for (; i < message.size(); i++) {\n        char c = message[i];\n        if (('A' <= c && c <= 'Z') || ('a' <= c && c <= 'z')) {\n            c ^= 0x20;\n            char upper_c = c & 0xDF;\n            if (upper_c == 'A' || upper_c == 'E' || upper_c == 'I' || upper_c == 'O' || upper_c == 'U') {\n                c += 2;\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string result(message.size(), ' ');\n    size_t i = 0;\n    if (message.size() >= 16) {\n        __m128i a_minus1 = _mm_set1_epi8('A' - 1);\n        __m128i z_plus1 = _mm_set1_epi8('Z' + 1);\n        __m128i a_lower_minus1 = _mm_set1_epi8('a' - 1);\n        __m128i z_lower_plus1 = _mm_set1_epi8('z' + 1);\n        __m128i flip_val = _mm_set1_epi8(0x20);\n        const char vowels_str[11] = \"aeiouAEIOU\";\n\n        for (; i <= message.size() - 16; i += 16) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(message.data() + i));\n            __m128i mask_upper = _mm_and_si128(_mm_cmpgt_epi8(vec, a_minus1), _mm_cmpgt_epi8(z_plus1, vec));\n            __m128i mask_lower = _mm_and_si128(_mm_cmpgt_epi8(vec, a_lower_minus1), _mm_cmpgt_epi8(z_lower_plus1, vec));\n            __m128i mask_letter = _mm_or_si128(mask_upper, mask_lower);\n            __m128i flip_mask = _mm_and_si128(mask_letter, flip_val);\n            __m128i swapped = _mm_xor_si128(vec, flip_mask);\n\n            __m128i vowel_mask = _mm_setzero_si128();\n            for (int j = 0; j < 10; ++j) {\n                __m128i vowel = _mm_set1_epi8(vowels_str[j]);\n                vowel_mask = _mm_or_si128(vowel_mask, _mm_cmpeq_epi8(swapped, vowel));\n            }\n\n            __m128i add2 = _mm_and_si128(vowel_mask, _mm_set1_epi8(2));\n            __m128i result_vec = _mm_add_epi8(swapped, add2);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), result_vec);\n        }\n    }\n    for (; i < message.size(); ++i) {\n        char c = message[i];\n        if (('A' <= c && c <= 'Z') || ('a' <= c && c <= 'z')) {\n            c ^= 0x20;\n        }\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2;\n        }\n        result[i] = c;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t n = message.size();\n    std::string result;\n    result.resize(n);\n    size_t i = 0;\n\n    const __m128i case_mask = _mm_set1_epi8(0x20);\n    const __m128i upper_mask = _mm_set1_epi8(0xDF);\n    const __m128i two_vec = _mm_set1_epi8(2);\n    const __m128i vowel_A = _mm_set1_epi8('A');\n    const __m128i vowel_E = _mm_set1_epi8('E');\n    const __m128i vowel_I = _mm_set1_epi8('I');\n    const __m128i vowel_O = _mm_set1_epi8('O');\n    const __m128i vowel_U = _mm_set1_epi8('U');\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(message.data() + i));\n        __m128i swapped = _mm_xor_si128(vec, case_mask);\n        __m128i temp = _mm_and_si128(swapped, upper_mask);\n        \n        __m128i cmpA = _mm_cmpeq_epi8(temp, vowel_A);\n        __m128i cmpE = _mm_cmpeq_epi8(temp, vowel_E);\n        __m128i cmpI = _mm_cmpeq_epi8(temp, vowel_I);\n        __m128i cmpO = _mm_cmpeq_epi8(temp, vowel_O);\n        __m128i cmpU = _mm_cmpeq_epi8(temp, vowel_U);\n        \n        __m128i vowel_mask = _mm_or_si128(cmpA, cmpE);\n        vowel_mask = _mm_or_si128(vowel_mask, cmpI);\n        vowel_mask = _mm_or_si128(vowel_mask, cmpO);\n        vowel_mask = _mm_or_si128(vowel_mask, cmpU);\n        \n        __m128i add_val = _mm_and_si128(vowel_mask, two_vec);\n        __m128i res = _mm_add_epi8(swapped, add_val);\n        \n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), res);\n    }\n\n    for (; i < n; ++i) {\n        char c = message[i];\n        c ^= 0x20;\n        char upper_c = c & 0xDF;\n        if (upper_c == 'A' || upper_c == 'E' || upper_c == 'I' || upper_c == 'O' || upper_c == 'U') {\n            c += 2;\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    std::string result;\n    result.resize(len);\n    size_t i = 0;\n\n    const __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n    const __m128i Z_plus1 = _mm_set1_epi8('Z' + 1);\n    const __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n    const __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n    const __m128i bit20 = _mm_set1_epi8(0x20);\n    const __m128i vowel_a = _mm_set1_epi8('a');\n    const __m128i vowel_e = _mm_set1_epi8('e');\n    const __m128i vowel_i = _mm_set1_epi8('i');\n    const __m128i vowel_o = _mm_set1_epi8('o');\n    const __m128i vowel_u = _mm_set1_epi8('u');\n    const __m128i two = _mm_set1_epi8(2);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(message.data() + i));\n\n        __m128i gt_A_minus1 = _mm_cmpgt_epi8(v, A_minus1);\n        __m128i lt_Z_plus1 = _mm_cmplt_epi8(v, Z_plus1);\n        __m128i is_upper = _mm_and_si128(gt_A_minus1, lt_Z_plus1);\n\n        __m128i gt_a_minus1 = _mm_cmpgt_epi8(v, a_minus1);\n        __m128i lt_z_plus1 = _mm_cmplt_epi8(v, z_plus1);\n        __m128i is_lower = _mm_and_si128(gt_a_minus1, lt_z_plus1);\n\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n        __m128i swap_val = _mm_and_si128(is_letter, bit20);\n        __m128i swapped = _mm_xor_si128(v, swap_val);\n\n        __m128i v_low = _mm_or_si128(swapped, bit20);\n\n        __m128i mask_a = _mm_cmpeq_epi8(v_low, vowel_a);\n        __m128i mask_e = _mm_cmpeq_epi8(v_low, vowel_e);\n        __m128i mask_i = _mm_cmpeq_epi8(v_low, vowel_i);\n        __m128i mask_o = _mm_cmpeq_epi8(v_low, vowel_o);\n        __m128i mask_u = _mm_cmpeq_epi8(v_low, vowel_u);\n\n        __m128i vowel_mask = _mm_or_si128(mask_a, mask_e);\n        vowel_mask = _mm_or_si128(vowel_mask, mask_i);\n        vowel_mask = _mm_or_si128(vowel_mask, mask_o);\n        vowel_mask = _mm_or_si128(vowel_mask, mask_u);\n\n        __m128i addend = _mm_and_si128(vowel_mask, two);\n        __m128i encoded = _mm_add_epi8(swapped, addend);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), encoded);\n    }\n\n    for (; i < len; ++i) {\n        char c = message[i];\n        bool is_letter = (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z');\n        char swapped = c;\n        if (is_letter) {\n            swapped = c ^ 0x20;\n        }\n        char low = swapped | 0x20;\n        if (low == 'a' || low == 'e' || low == 'i' || low == 'o' || low == 'u') {\n            swapped += 2;\n        }\n        result[i] = swapped;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    auto it = dict.begin();\n    const std::string& first_key = it->first;\n    bool has_lower = false;\n    bool has_upper = false;\n    bool has_non_alpha = false;\n    size_t len = first_key.length();\n    size_t i = 0;\n\n    __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n    __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n    __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n    __m128i Z_plus1 = _mm_set1_epi8('Z' + 1);\n\n    for (; i + 15 < len; i += 16) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(first_key.data() + i));\n        __m128i in_lower = _mm_and_si128(\n            _mm_cmpgt_epi8(vec, a_minus1),\n            _mm_cmpgt_epi8(z_plus1, vec)\n        );\n        __m128i in_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(vec, A_minus1),\n            _mm_cmpgt_epi8(Z_plus1, vec)\n        );\n        __m128i in_letter = _mm_or_si128(in_lower, in_upper);\n        int mask_letter = _mm_movemask_epi8(in_letter);\n        if (mask_letter != 0xFFFF) {\n            has_non_alpha = true;\n        }\n        int mask_lower = _mm_movemask_epi8(in_lower);\n        int mask_upper = _mm_movemask_epi8(in_upper);\n        if (mask_lower) {\n            has_lower = true;\n        }\n        if (mask_upper) {\n            has_upper = true;\n        }\n    }\n\n    for (; i < len; ++i) {\n        char c = first_key[i];\n        if (c >= 'a' && c <= 'z') {\n            has_lower = true;\n        } else if (c >= 'A' && c <= 'Z') {\n            has_upper = true;\n        } else {\n            has_non_alpha = true;\n        }\n    }\n\n    int expected_case = 0;\n    if (has_non_alpha || (has_lower && has_upper)) {\n        return false;\n    } else if (has_lower) {\n        expected_case = 1;\n    } else if (has_upper) {\n        expected_case = 2;\n    } else {\n        return false;\n    }\n\n    ++it;\n    for (; it != dict.end(); ++it) {\n        const std::string& key = it->first;\n        has_lower = false;\n        has_upper = false;\n        has_non_alpha = false;\n        len = key.length();\n        i = 0;\n\n        for (; i + 15 < len; i += 16) {\n            __m128i vec = _mm_loadu_si128((__m128i const*)(key.data() + i));\n            __m128i in_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(vec, a_minus1),\n                _mm_cmpgt_epi8(z_plus1, vec)\n            );\n            __m128i in_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(vec, A_minus1),\n                _mm_cmpgt_epi8(Z_plus1, vec)\n            );\n            __m128i in_letter = _mm_or_si128(in_lower, in_upper);\n            int mask_letter = _mm_movemask_epi8(in_letter);\n            if (mask_letter != 0xFFFF) {\n                has_non_alpha = true;\n            }\n            int mask_lower = _mm_movemask_epi8(in_lower);\n            int mask_upper = _mm_movemask_epi8(in_upper);\n            if (mask_lower) {\n                has_lower = true;\n            }\n            if (mask_upper) {\n                has_upper = true;\n            }\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (c >= 'a' && c <= 'z') {\n                has_lower = true;\n            } else if (c >= 'A' && c <= 'Z') {\n                has_upper = true;\n            } else {\n                has_non_alpha = true;\n            }\n        }\n\n        if (expected_case == 1) {\n            if (has_non_alpha || has_upper) {\n                return false;\n            }\n        } else if (expected_case == 2) {\n            if (has_non_alpha || has_lower) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    const __m128i A_minus1 = _mm_set1_epi8('A' - 1);\n    const __m128i Z_plus1 = _mm_set1_epi8('Z' + 1);\n    const __m128i a_minus1 = _mm_set1_epi8('a' - 1);\n    const __m128i z_plus1 = _mm_set1_epi8('z' + 1);\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        size_t len = key.size();\n        const char* p = key.c_str();\n        bool has_letter = false;\n        bool has_upper = false;\n        bool has_lower = false;\n\n        size_t i = 0;\n\n        for (; i + 15 < len; i += 16) {\n            __m128i chunk = _mm_loadu_si128((const __m128i*)(p + i));\n\n            __m128i gt_A = _mm_cmpgt_epi8(chunk, A_minus1);\n            __m128i lt_Z = _mm_cmplt_epi8(chunk, Z_plus1);\n            __m128i is_upper = _mm_and_si128(gt_A, lt_Z);\n\n            __m128i gt_a = _mm_cmpgt_epi8(chunk, a_minus1);\n            __m128i lt_z = _mm_cmplt_epi8(chunk, z_plus1);\n            __m128i is_lower = _mm_and_si128(gt_a, lt_z);\n\n            if (!has_upper && _mm_movemask_epi8(is_upper)) {\n                has_upper = true;\n            }\n            if (!has_lower && _mm_movemask_epi8(is_lower)) {\n                has_lower = true;\n            }\n            __m128i is_alpha = _mm_or_si128(is_upper, is_lower);\n            if (!has_letter && _mm_movemask_epi8(is_alpha)) {\n                has_letter = true;\n            }\n\n            if (has_upper && has_lower) {\n                break;\n            }\n        }\n\n        if (!(has_upper && has_lower)) {\n            for (; i < len; i++) {\n                char c = p[i];\n                if (c >= 'A' && c <= 'Z') {\n                    has_letter = true;\n                    has_upper = true;\n                } else if (c >= 'a' && c <= 'z') {\n                    has_letter = true;\n                    has_lower = true;\n                }\n                if (has_upper && has_lower) {\n                    break;\n                }\n            }\n        }\n\n        if (has_letter) {\n            if (has_upper && has_lower) {\n                all_lower = false;\n                all_upper = false;\n            } else if (has_upper) {\n                all_lower = false;\n            } else if (has_lower) {\n                all_upper = false;\n            }\n        } else {\n            all_lower = false;\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            break;\n        }\n    }\n\n    return (all_lower || all_upper);\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) \n        return false;\n    \n    int overall_case = 0;\n    __m128i v_A = _mm_set1_epi8('A' - 1);\n    __m128i v_Z = _mm_set1_epi8('Z' + 1);\n    __m128i v_a = _mm_set1_epi8('a' - 1);\n    __m128i v_z = _mm_set1_epi8('z' + 1);\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        size_t len = key.size();\n        if (len == 0) \n            return false;\n        \n        bool has_lower = false;\n        bool has_upper = false;\n        bool non_letter = false;\n        size_t i = 0;\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            __m128i upper_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, v_A),\n                _mm_cmpgt_epi8(v_Z, chunk)\n            );\n            __m128i lower_mask = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, v_a),\n                _mm_cmpgt_epi8(v_z, chunk)\n            );\n            __m128i letter_mask = _mm_or_si128(upper_mask, lower_mask);\n\n            if (_mm_movemask_epi8(letter_mask) != 0xFFFF) {\n                non_letter = true;\n                break;\n            }\n\n            if (_mm_movemask_epi8(lower_mask) != 0) \n                has_lower = true;\n            if (_mm_movemask_epi8(upper_mask) != 0) \n                has_upper = true;\n        }\n\n        if (!non_letter) {\n            for (; i < len; ++i) {\n                char c = key[i];\n                if (c >= 'A' && c <= 'Z') {\n                    has_upper = true;\n                } else if (c >= 'a' && c <= 'z') {\n                    has_lower = true;\n                } else {\n                    non_letter = true;\n                    break;\n                }\n            }\n        }\n\n        if (non_letter || (has_lower && has_upper) || (!has_lower && !has_upper))\n            return false;\n        \n        int key_case = has_lower ? 1 : 2;\n        if (overall_case == 0) {\n            overall_case = key_case;\n        } else if (overall_case != key_case) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    const __m128i sse_index = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n    bool global_all_upper = true;\n    bool global_all_lower = true;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        bool is_upper = true;\n        bool is_lower = true;\n\n        if (!key.empty()) {\n            size_t len = key.length();\n            size_t i = 0;\n\n            while (i < len && (is_upper || is_lower)) {\n                size_t remaining = len - i;\n                size_t chunk_size = (remaining >= 16) ? 16 : remaining;\n                __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n\n                __m128i v_gt_z = _mm_cmpgt_epi8(v, _mm_set1_epi8('z'));\n                __m128i v_lt_A = _mm_cmplt_epi8(v, _mm_set1_epi8('A'));\n                __m128i v_gt_Z = _mm_cmpgt_epi8(v, _mm_set1_epi8('Z'));\n                __m128i v_lt_a = _mm_cmplt_epi8(v, _mm_set1_epi8('a'));\n                __m128i v_in_gap = _mm_and_si128(v_gt_Z, v_lt_a);\n                __m128i non_letter = _mm_or_si128(_mm_or_si128(v_gt_z, v_lt_A), v_in_gap);\n\n                __m128i thresh = _mm_set1_epi8(static<char>(chunk_size));\n                __m128i mask = _mm_cmplt_epi8(sse_index, thresh);\n                non_letter = _mm_and_si128(non_letter, mask);\n\n                if (_mm_movemask_epi8(non_letter) != 0) {\n                    is_upper = false;\n                    is_lower = false;\n                    break;\n                }\n\n                __m128i v_ge_a = _mm_cmpgt_epi8(v, _mm_set1_epi8('a' - 1));\n                __m128i v_le_z = _mm_cmplt_epi8(v, _mm_set1_epi8('z' + 1));\n                __m128i is_lowercase = _mm_and_si128(v_ge_a, v_le_z);\n                is_lowercase = _mm_and_si128(is_lowercase, mask);\n                if (_mm_movemask_epi8(is_lowercase)) {\n                    is_upper = false;\n                }\n\n                __m128i v_ge_A = _mm_cmpgt_epi8(v, _mm_set1_epi8('A' - 1));\n                __m128i v_le_Z = _mm_cmplt_epi8(v, _mm_set1_epi8('Z' + 1));\n                __m128i is_uppercase = _mm_and_si128(v_ge_A, v_le_Z);\n                is_uppercase = _mm_and_si128(is_uppercase, mask);\n                if (_mm_movemask_epi8(is_uppercase)) {\n                    is_lower = false;\n                }\n\n                i += chunk_size;\n            }\n        }\n\n        global_all_upper = global_all_upper && is_upper;\n        global_all_lower = global_all_lower && is_lower;\n\n        if (!global_all_upper && !global_all_lower) {\n            break;\n        }\n    }\n\n    return global_all_upper || global_all_lower;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) \n        return false;\n\n    bool has_lower = false;\n    bool has_upper = false;\n\n    for (const auto& kv : dict) {\n        const std::string& key = kv.first;\n        size_t len = key.size();\n        const char* ptr = key.c_str();\n        size_t i = 0;\n\n        for (; i + 15 < len; i += 16) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(ptr + i));\n\n            __m128i v_ge_A = _mm_cmpgt_epi8(v, _mm_set1_epi8('A' - 1));\n            __m128i v_le_Z = _mm_cmplt_epi8(v, _mm_set1_epi8('Z' + 1));\n            __m128i in_upper = _mm_and_si128(v_ge_A, v_le_Z);\n\n            __m128i v_ge_a = _mm_cmpgt_epi8(v, _mm_set1_epi8('a' - 1));\n            __m128i v_le_z = _mm_cmplt_epi8(v, _mm_set1_epi8('z' + 1));\n            __m128i in_lower = _mm_and_si128(v_ge_a, v_le_z);\n\n            __m128i in_any = _mm_or_si128(in_upper, in_lower);\n            unsigned int mask_any = _mm_movemask_epi8(in_any);\n            if (mask_any != 0xFFFF) \n                return false;\n\n            if (has_lower && has_upper) \n                continue;\n\n            unsigned int mask_lower = _mm_movemask_epi8(in_lower);\n            if (mask_lower) \n                has_lower = true;\n\n            unsigned int mask_upper = _mm_movemask_epi8(in_upper);\n            if (mask_upper) \n                has_upper = true;\n        }\n\n        for (; i < len; ++i) {\n            unsigned char c = ptr[i];\n            if (!std::isalpha(c)) \n                return false;\n\n            if (has_lower && has_upper) \n                continue;\n\n            if (std::islower(c)) \n                has_lower = true;\n            else \n                has_upper = true;\n        }\n    }\n\n    return !(has_lower && has_upper);\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return std::vector<int>();\n    }\n\n    long long nll = n;\n    int block_count = (n + 127) / 128;\n    __m128i* sieve = (__m128i*)_mm_malloc(block_count * sizeof(__m128i), 16);\n    __m128i ones = _mm_set1_epi8(0xFF);\n    for (int i = 0; i < block_count; ++i) {\n        sieve[i] = ones;\n    }\n\n    unsigned char* bytes = (unsigned char*)sieve;\n    bytes[0] &= ~0x01;\n    if (n > 1) {\n        bytes[0] &= ~0x02;\n    }\n\n    for (int p = 2; (long long)p * p < nll; ++p) {\n        int block_idx = p / 128;\n        int bitpos = p % 128;\n        int byte_idx = bitpos / 8;\n        int bit_in_byte = bitpos % 8;\n        unsigned char* block_ptr = (unsigned char*)(sieve + block_idx);\n        if (block_ptr[byte_idx] & (1 << bit_in_byte)) {\n            long long start = (long long)p * p;\n            if (start >= nll) continue;\n            int start_block = start / 128;\n            int last_block = (n - 1) / 128;\n            for (int blk = start_block; blk <= last_block; ++blk) {\n                long long base = (long long)blk * 128;\n                long long rem = base % p;\n                long long first_offset = rem ? base + p - rem : base;\n                if (first_offset < start) {\n                    long long k = (start - first_offset + p - 1) / p;\n                    first_offset += k * p;\n                }\n                if (first_offset >= base + 128) continue;\n                __m128i mask = _mm_setzero_si128();\n                unsigned char* mask_bytes = (unsigned char*)&mask;\n                for (long long j = first_offset; j < base + 128; j += p) {\n                    int pos_in_block = j - base;\n                    int byte_index = pos_in_block / 8;\n                    int bit_index = pos_in_block % 8;\n                    mask_bytes[byte_index] |= (1 << bit_index);\n                }\n                sieve[blk] = _mm_andnot_si128(mask, sieve[blk]);\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; ++i) {\n        int block_idx = i / 128;\n        int bitpos = i % 128;\n        int byte_idx = bitpos / 8;\n        int bit_in_byte = bitpos % 8;\n        unsigned char* block_ptr = (unsigned char*)(sieve + block_idx);\n        if (block_ptr[byte_idx] & (1 << bit_in_byte)) {\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(sieve);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return {};\n    }\n\n    std::vector<unsigned char> is_prime(n, 1);\n    is_prime[0] = 0;\n    is_prime[1] = 0;\n\n    int sqrt_n = static_cast<int>(std::sqrt(n)) + 1;\n\n    for (int i = 2; i * i < sqrt_n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < sqrt_n; j += i) {\n                is_prime[j] = 0;\n            }\n        }\n    }\n\n    const int block_size = 16 * 1024;\n    for (int low = sqrt_n; low < n; low += block_size) {\n        int high = std::min(low + block_size, n);\n        for (int p = 2; p < sqrt_n; ++p) {\n            if (!is_prime[p]) continue;\n            long long start_index = static_cast<long long>(p) * p;\n            if (start_index >= high) continue;\n            if (start_index < low) {\n                int remainder = low % p;\n                start_index = low + (remainder == 0 ? 0 : p - remainder);\n            }\n            unsigned char* block_ptr = &is_prime[low];\n            int i = 0;\n            for (; i <= (high - low) - 16; i += 16) {\n                long long chunk_start = low + i;\n                if (start_index >= chunk_start + 16) continue;\n                long long first_in_chunk = std::max(start_index, chunk_start);\n                int offset = static_cast<int>(first_in_chunk - chunk_start);\n                unsigned char temp[16];\n                for (int k = 0; k < 16; ++k) temp[k] = 0xFF;\n                while (offset < 16) {\n                    temp[offset] = 0;\n                    offset += p;\n                }\n                __m128i mask = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n                __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(block_ptr + i));\n                data = _mm_and_si128(data, mask);\n                _mm_storeu_si128(reinterpret_cast<__m128i*>(block_ptr + i), data);\n            }\n            for (long long j = start_index; j < high; j += p) {\n                if (j >= low + i) {\n                    is_prime[j] = 0;\n                }\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n < 2) {\n        return std::vector<int>();\n    }\n\n    std::vector<char> sieve(n, 1);\n    sieve[0] = 0;\n    sieve[1] = 0;\n\n    for (long long p = 2; static_cast<long long>(p) * static_cast<long long>(p) < n; ++p) {\n        if (sieve[p]) {\n            if (p <= 16) {\n                __m128i masks[16];\n                for (int r = 0; r < p; ++r) {\n                    unsigned char mask_arr[16];\n                    for (int j = 0; j < 16; ++j) {\n                        if ((j + r) % p == 0) {\n                            mask_arr[j] = 0x00;\n                        } else {\n                            mask_arr[j] = 0xFF;\n                        }\n                    }\n                    masks[r] = _mm_setr_epi8(\n                        mask_arr[0], mask_arr[1], mask_arr[2], mask_arr[3],\n                        mask_arr[4], mask_arr[5], mask_arr[6], mask_arr[7],\n                        mask_arr[8], mask_arr[9], mask_arr[10], mask_arr[11],\n                        mask_arr[12], mask_arr[13], mask_arr[14], mask_arr[15]\n                    );\n                }\n\n                long long start = static_cast<long long>(p) * static_cast<long long>(p);\n                int r = 0;\n                int step = 16 % p;\n                char* sieve_data = sieve.data();\n\n                for (long long i = start; i < n; ) {\n                    if (i + 16 <= n) {\n                        __m128i data = _mm_loadu_si128(reinterpret_cast<__m128i*>(sieve_data + i));\n                        __m128i mask = masks[r];\n                        data = _mm_and_si128(data, mask);\n                        _mm_storeu_si128(reinterpret_cast<__m128i*>(sieve_data + i), data);\n\n                        r += step;\n                        if (r >= p) r -= p;\n                        i += 16;\n                    } else {\n                        for (long long j = i; j < n; ++j) {\n                            if (j % p == 0) {\n                                sieve[j] = 0;\n                            }\n                        }\n                        break;\n                    }\n                }\n            } else {\n                for (long long j = static_cast<long long>(p) * static_cast<long long>(p); j < n; j += p) {\n                    sieve[j] = 0;\n                }\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    for (int i = 2; i < n; ++i) {\n        if (sieve[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n < 2) {\n        return {};\n    }\n\n    std::vector<unsigned char> sieve(n, 1);\n    sieve[0] = 0;\n    sieve[1] = 0;\n\n    int sqrt_n = std::sqrt(n);\n    for (int p = 2; p <= sqrt_n; ++p) {\n        if (sieve[p]) {\n            if (p <= 15) {\n                int start = p * p;\n                for (int chunk_start = start; chunk_start < n; chunk_start += 16) {\n                    char mask_arr[16];\n                    for (int i = 0; i < 16; ++i) {\n                        mask_arr[i] = -1;\n                    }\n                    int offset = (p - (chunk_start % p)) % p;\n                    for (int pos = offset; pos < 16 && chunk_start + pos < n; pos += p) {\n                        mask_arr[pos] = 0;\n                    }\n                    __m128i mask = _mm_loadu_si128(reinterpret_cast<const __m128i*>(mask_arr));\n                    __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(sieve.data() + chunk_start));\n                    data = _mm_and_si128(data, mask);\n                    _mm_storeu_si128(reinterpret_cast<__m128i*>(sieve.data() + chunk_start), data);\n                }\n            } else {\n                for (int j = p * p; j < n; j += p) {\n                    sieve[j] = 0;\n                }\n            }\n        }\n    }\n\n    std::vector<int> primes;\n    int i = 0;\n    for (; i <= n - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(sieve.data() + i));\n        __m128i zeros = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpgt_epi8(chunk, zeros);\n        int mask = _mm_movemask_epi8(cmp);\n        for (int j = 0; j < 16; ++j) {\n            if (mask & (1 << j)) {\n                int num = i + j;\n                if (num >= 2) {\n                    primes.push_back(num);\n                }\n            }\n        }\n    }\n    for (; i < n; ++i) {\n        if (sieve[i] && i >= 2) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) {\n        return {};\n    }\n    int sqrt_n = static_cast<int>(std::sqrt(n)) + 1;\n    std::vector<char> is_prime_small(sqrt_n + 1, 1);\n    is_prime_small[0] = is_prime_small[1] = 0;\n    for (int i = 2; i * i <= sqrt_n; ++i) {\n        if (is_prime_small[i]) {\n            for (int j = i * i; j <= sqrt_n; j += i) {\n                is_prime_small[j] = 0;\n            }\n        }\n    }\n    std::vector<int> small_primes;\n    for (int i = 2; i <= sqrt_n; ++i) {\n        if (is_prime_small[i]) {\n            small_primes.push_back(i);\n        }\n    }\n    const int L = 1 << 16;\n    std::vector<int> primes;\n    for (int low = 0; low < n; low += L) {\n        int high = std::min(low + L, n);\n        uint8_t* chunk = new uint8_t[high - low];\n        std::fill(chunk, chunk + (high - low), 1);\n        if (low == 0) {\n            if (high > 0) chunk[0] = 0;\n            if (high > 1) chunk[1] = 0;\n        }\n        for (int p : small_primes) {\n            long long start_index = (static_cast<long long>(low) + p - 1) / p * static_cast<long long>(p);\n            if (start_index < static_cast<long long>(p) * p) {\n                start_index = static_cast<long long>(p) * p;\n            }\n            if (start_index >= high) {\n                continue;\n            }\n            for (int block_start = low; block_start < high; block_start += 16) {\n                int block_end = std::min(block_start + 16, high);\n                long long first = (static_cast<long long>(block_start) + p - 1) / p * static_cast<long long>(p);\n                if (first < start_index) {\n                    first = start_index;\n                }\n                if (first < block_end) {\n                    uint8_t mask_arr[16];\n                    for (int i = 0; i < 16; ++i) {\n                        mask_arr[i] = 0xFF;\n                    }\n                    int offset = static_cast<int>(first - block_start);\n                    while (offset < 16 && block_start + offset < high) {\n                        mask_arr[offset] = 0;\n                        offset += p;\n                    }\n                    __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(chunk + (block_start - low)));\n                    __m128i mask = _mm_loadu_si128(reinterpret_cast<const __m128i*>(mask_arr));\n                    data = _mm_and_si128(data, mask);\n                    _mm_storeu_si128(reinterpret_cast<__m128i*>(chunk + (block_start - low)), data);\n                }\n            }\n        }\n        for (int i = (low < 2 ? 2 : low); i < high; ++i) {\n            if (chunk[i - low]) {\n                primes.push_back(i);\n            }\n        }\n        delete[] chunk;\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return 0;\n\n    int count = 0;\n    size_t i = 0;\n    \n    const __m128i vowelA = _mm_set1_epi8('A');\n    const __m128i vowelE = _mm_set1_epi8('E');\n    const __m128i vowelI = _mm_set1_epi8('I');\n    const __m128i vowelO = _mm_set1_epi8('O');\n    const __m128i vowelU = _mm_set1_epi8('U');\n    const __m128i even_mask = _mm_set_epi8(0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, \n                                          0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF);\n\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i eqA = _mm_cmpeq_epi8(chunk, vowelA);\n        __m128i eqE = _mm_cmpeq_epi8(chunk, vowelE);\n        __m128i eqI = _mm_cmpeq_epi8(chunk, vowelI);\n        __m128i eqO = _mm_cmpeq_epi8(chunk, vowelO);\n        __m128i eqU = _mm_cmpeq_epi8(chunk, vowelU);\n        \n        __m128i anyVowel = _mm_or_si128(_mm_or_si128(_mm_or_si128(eqA, eqE), \n                                       _mm_or_si128(_mm_or_si128(eqI, eqO), eqU)));\n        __m128i masked = _mm_and_si128(anyVowel, even_mask);\n        int bitmask = _mm_movemask_epi8(masked);\n        count += __builtin_popcount(bitmask);\n    }\n\n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = s[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s){\n    size_t len = s.length();\n    if (len == 0) return 0;\n    int count = 0;\n    size_t i = 0;\n    const __m128i mask_even = _mm_set_epi8(0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, \n                                          0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF);\n    const __m128i vowelA = _mm_set1_epi8('A');\n    const __m128i vowelE = _mm_set1_epi8('E');\n    const __m128i vowelI = _mm_set1_epi8('I');\n    const __m128i vowelO = _mm_set1_epi8('O');\n    const __m128i vowelU = _mm_set1_epi8('U');\n    \n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i even_chars = _mm_and_si128(chunk, mask_even);\n        __m128i eqA = _mm_cmpeq_epi8(even_chars, vowelA);\n        __m128i eqE = _mm_cmpeq_epi8(even_chars, vowelE);\n        __m128i eqI = _mm_cmpeq_epi8(even_chars, vowelI);\n        __m128i eqO = _mm_cmpeq_epi8(even_chars, vowelO);\n        __m128i eqU = _mm_cmpeq_epi8(even_chars, vowelU);\n        __m128i any = _mm_or_si128(eqA, eqE);\n        any = _mm_or_si128(any, eqI);\n        any = _mm_or_si128(any, eqO);\n        any = _mm_or_si128(any, eqU);\n        int mask = _mm_movemask_epi8(any);\n        count += __builtin_popcount(mask);\n    }\n    \n    for (size_t j = i; j < len; j += 2) {\n        char c = s[j];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return 0;\n\n    const __m128i even_mask = _mm_set_epi8(0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF);\n    const __m128i A = _mm_set1_epi8('A');\n    const __m128i E = _mm_set1_epi8('E');\n    const __m128i I = _mm_set1_epi8('I');\n    const __m128i O = _mm_set1_epi8('O');\n    const __m128i U = _mm_set1_epi8('U');\n\n    size_t i = 0;\n    int count = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(s.data() + i));\n        __m128i eqA = _mm_cmpeq_epi8(chunk, A);\n        __m128i eqE = _mm_cmpeq_epi8(chunk, E);\n        __m128i eqI = _mm_cmpeq_epi8(chunk, I);\n        __m128i eqO = _mm_cmpeq_epi8(chunk, O);\n        __m128i eqU = _mm_cmpeq_epi8(chunk, U);\n\n        __m128i vowels = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(eqA, eqE), _mm_or_si128(eqI, eqO)), eqU);\n        __m128i vowels_even = _mm_and_si128(vowels, even_mask);\n\n        __m128i zeros = _mm_setzero_si128();\n        __m128i sum_bytes = _mm_sad_epu8(vowels_even, zeros);\n        uint64_t sums[2];\n        _mm_storeu_si128((__m128i*)sums, sum_bytes);\n        count += (sums[0] + sums[1]) / 255;\n    }\n\n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = s[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return 0;\n\n    size_t i = 0;\n    int count = 0;\n\n    const __m128i even_mask = _mm_set_epi8(0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF);\n    const __m128i vowelA = _mm_set1_epi8('A');\n    const __m128i vowelE = _mm_set1_epi8('E');\n    const __m128i vowelI = _mm_set1_epi8('I');\n    const __m128i vowelO = _mm_set1_epi8('O');\n    const __m128i vowelU = _mm_set1_epi8('U');\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        \n        __m128i cmpA = _mm_cmpeq_epi8(chunk, vowelA);\n        __m128i cmpE = _mm_cmpeq_epi8(chunk, vowelE);\n        __m128i cmpI = _mm_cmpeq_epi8(chunk, vowelI);\n        __m128i cmpO = _mm_cmpeq_epi8(chunk, vowelO);\n        __m128i cmpU = _mm_cmpeq_epi8(chunk, vowelU);\n        \n        __m128i any = _mm_or_si128(cmpA, cmpE);\n        any = _mm_or_si128(any, cmpI);\n        any = _mm_or_si128(any, cmpO);\n        any = _mm_or_si128(any, cmpU);\n        \n        __m128i masked = _mm_and_si128(any, even_mask);\n        int m = _mm_movemask_epi8(masked);\n        m &= 0xFFFF;\n        \n        while (m) {\n            count++;\n            m &= m - 1;\n        }\n    }\n    \n    for (size_t j = i; j < len; j += 2) {\n        char c = s[j];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    size_t len = s.length();\n    int count = 0;\n    size_t i = 0;\n    \n    const __m128i even_mask = _mm_setr_epi8(\n        -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0\n    );\n    \n    const __m128i vowel_A = _mm_set1_epi8('A');\n    const __m128i vowel_E = _mm_set1_epi8('E');\n    const __m128i vowel_I = _mm_set1_epi8('I');\n    const __m128i vowel_O = _mm_set1_epi8('O');\n    const __m128i vowel_U = _mm_set1_epi8('U');\n    \n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        \n        __m128i eqA = _mm_cmpeq_epi8(chunk, vowel_A);\n        __m128i eqE = _mm_cmpeq_epi8(chunk, vowel_E);\n        __m128i eqI = _mm_cmpeq_epi8(chunk, vowel_I);\n        __m128i eqO = _mm_cmpeq_epi8(chunk, vowel_O);\n        __m128i eqU = _mm_cmpeq_epi8(chunk, vowel_U);\n        \n        __m128i any = _mm_or_si128(eqA, eqE);\n        any = _mm_or_si128(any, eqI);\n        any = _mm_or_si128(any, eqO);\n        any = _mm_or_si128(any, eqU);\n        \n        __m128i result = _mm_and_si128(any, even_mask);\n        int bitmask = _mm_movemask_epi8(result);\n        count += __builtin_popcount(bitmask);\n    }\n    \n    for (; i < len; i++) {\n        if (i % 2 == 0) {\n            char c = s[i];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) {\n        return res;\n    }\n    int i = 0;\n    __m128i v_base_index = _mm_setr_epi32(0, 1, 2, 3);\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v_j = _mm_set1_epi32(i);\n        __m128i v_global_index = _mm_add_epi32(v_j, v_base_index);\n        __m128i v_double_index = _mm_slli_epi32(v_global_index, 1);\n        __m128i v_n = _mm_set1_epi32(n);\n        __m128i v_value = _mm_add_epi32(v_n, v_double_index);\n        _mm_storeu_si128((__m128i*)&res[i], v_value);\n    }\n\n    for (; i < n; i++) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> vec(n);\n    if (n <= 0) {\n        return vec;\n    }\n\n    const __m128i increments = _mm_setr_epi32(0, 2, 4, 6);\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        int base_val = n + 2 * i;\n        __m128i base = _mm_set1_epi32(base_val);\n        __m128i result = _mm_add_epi32(base, increments);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&vec[i]), result);\n    }\n\n    for (; i < n; ++i) {\n        vec[i] = n + 2 * i;\n    }\n\n    return vec;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) {\n        return res;\n    }\n\n    __m128i increment = _mm_set_epi32(6, 4, 2, 0);\n    int i = 0;\n    for (; i <= n - 4; i += 4) {\n        int base_val = n + 2 * i;\n        __m128i base_vec = _mm_set1_epi32(base_val);\n        __m128i result_vec = _mm_add_epi32(base_vec, increment);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(res.data() + i), result_vec);\n    }\n\n    for (; i < n; ++i) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) {\n        return res;\n    }\n    \n    int i = 0;\n    for (; i <= n - 4; i += 4) {\n        int base_val = n + 2 * i;\n        __m128i base = _mm_set1_epi32(base_val);\n        __m128i increments = _mm_setr_epi32(0, 2, 4, 6);\n        __m128i values = _mm_add_epi32(base, increments);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), values);\n    }\n    \n    for (; i < n; i++) {\n        res[i] = n + 2 * i;\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) {\n        return res;\n    }\n\n    __m128i v_step = _mm_setr_epi32(0, 2, 4, 6);\n    int i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        int base_val = n + 2 * i;\n        __m128i v_base = _mm_set1_epi32(base_val);\n        __m128i v_values = _mm_add_epi32(v_base, v_step);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(res.data() + i), v_values);\n    }\n\n    for (; i < n; i++) {\n        res[i] = n + 2 * i;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <string>\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v = _mm_set_epi64x(m, n);\n    __m128i mask = _mm_cvtsi64_si128(-1);\n    __m128i low_part = _mm_and_si128(v, mask);\n    __m128i high_part = _mm_srli_si128(v, 8);\n    __m128i sum_vec = _mm_add_epi64(low_part, high_part);\n    __m128i avg_vec = _mm_srli_epi64(sum_vec, 1);\n    int64_t avg = _mm_cvtsi128_si64(avg_vec);\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    std::string res;\n    while (avg) {\n        res = char('0' + (avg & 1)) + res;\n        avg >>= 1;\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i n_vec = _mm_cvtsi64_si128(n);\n    __m128i m_vec = _mm_cvtsi64_si128(m);\n    __m128i sum_vec = _mm_add_epi64(n_vec, m_vec);\n    __m128i avg_vec = _mm_srli_epi64(sum_vec, 1);\n    int64_t avg = _mm_cvtsi128_si64(avg_vec);\n\n    if (avg == 0) {\n        return \"0\";\n    }\n\n    int clz = __builtin_clzll(avg);\n    int num_bits = 64 - clz;\n    std::string binary;\n    binary.reserve(num_bits);\n    unsigned long long u_avg = avg;\n    unsigned long long bit_mask = 1ULL << (num_bits - 1);\n\n    for (int i = 0; i < num_bits; ++i) {\n        binary += (u_avg & bit_mask) ? '1' : '0';\n        bit_mask >>= 1;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i vn = _mm_cvtsi64_si128(n);\n    __m128i vm = _mm_cvtsi64_si128(m);\n    __m128i vsum = _mm_add_epi64(vn, vm);\n    __m128i vavg = _mm_srli_epi64(vsum, 1);\n    uint64_t avg = (uint64_t)_mm_cvtsi128_si64(vavg);\n\n    if (avg == 0) {\n        return \"0\";\n    }\n    std::string binary;\n    while (avg) {\n        binary = char('0' + (avg & 1)) + binary;\n        avg >>= 1;\n    }\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i n_vec = _mm_cvtsi64_si128(n);\n    __m128i m_vec = _mm_cvtsi64_si128(m);\n    __m128i sum = _mm_add_epi64(n_vec, m_vec);\n    __m128i avgv = _mm_srli_epi64(sum, 1);\n    int64_t avg = _mm_cvtsi128_si64(avgv);\n    int num_bits = 64 - __builtin_clzll(avg);\n    std::string res(num_bits, '0');\n    for (int i = 0; i < num_bits; i++) {\n        if (avg & (1LL << (num_bits - 1 - i))) {\n            res[i] = '1';\n        }\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i m_vec = _mm_set1_epi64x(m);\n    __m128i sum = _mm_add_epi64(n_vec, m_vec);\n    __m128i avg_vec = _mm_srli_epi64(sum, 1);\n    int64_t avg = _mm_extract_epi64(avg_vec, 0);\n    \n    if (avg == 0) {\n        return \"0\";\n    }\n    \n    int num_bits = 0;\n    uint64_t temp = static_cast<uint64_t>(avg);\n    while (temp) {\n        num_bits++;\n        temp >>= 1;\n    }\n    \n    std::string res(num_bits, '0');\n    uint64_t u_avg = static_cast<uint64_t>(avg);\n    for (int i = 0; i < num_bits; i++) {\n        if (u_avg & (1ULL << (num_bits - 1 - i))) {\n            res[i] = '1';\n        }\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n, 0);\n    if (n <= 0) \n        return res;\n\n    for (int j = 0; j < n; j += 8) {\n        int base_j = j;\n        __m128i ivals = _mm_setr_epi32(\n            (base_j < n) ? (base_j + 1) : 0,\n            (base_j + 2 < n) ? (base_j + 3) : 0,\n            (base_j + 4 < n) ? (base_j + 5) : 0,\n            (base_j + 6 < n) ? (base_j + 7) : 0\n        );\n        __m128i ivals_plus1 = _mm_add_epi32(ivals, _mm_set1_epi32(1));\n        __m128i prod = _mm_mullo_epi32(ivals, ivals_plus1);\n        __m128i T = _mm_srli_epi32(prod, 1);\n\n        if (base_j < n) \n            res[base_j] = _mm_extract_epi32(T, 0);\n        if (base_j + 2 < n) \n            res[base_j + 2] = _mm_extract_epi32(T, 1);\n        if (base_j + 4 < n) \n            res[base_j + 4] = _mm_extract_epi32(T, 2);\n        if (base_j + 6 < n) \n            res[base_j + 6] = _mm_extract_epi32(T, 3);\n    }\n\n    int m = n / 2;\n    if (m > 0) {\n        int prev = 1;\n        int k = 1;\n        while (k <= m) {\n            int k0 = k;\n            int count = (m - k + 1) < 4 ? (m - k + 1) : 4;\n            __m128i base = _mm_set1_epi32(2 * k0);\n            __m128i increments = _mm_setr_epi32(0, 2, 4, 6);\n            __m128i second = _mm_add_epi32(base, increments);\n            __m128i first = _mm_add_epi32(_mm_sub_epi32(base, _mm_set1_epi32(1)), increments);\n            __m128i P = _mm_mullo_epi32(first, second);\n            \n            int p[4];\n            p[0] = _mm_extract_epi32(P, 0);\n            p[1] = _mm_extract_epi32(P, 1);\n            p[2] = _mm_extract_epi32(P, 2);\n            p[3] = _mm_extract_epi32(P, 3);\n            \n            int F[4];\n            for (int i = 0; i < count; i++) {\n                prev = (prev * p[i]) % 10000;\n                F[i] = prev;\n            }\n            \n            for (int i = 0; i < count; i++) {\n                int j_index = 2 * (k0 + i) - 1;\n                if (j_index < n) \n                    res[j_index] = F[i];\n            }\n            k += count;\n        }\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n == 0) \n        return res;\n\n    int last_i = 0;\n    long long last_fact = 1;\n\n    int base;\n    for (base = 0; base <= n - 4; base += 4) {\n        __m128i i_vals = _mm_setr_epi32(base + 1, 0, base + 3, 0);\n        __m128i i_next = _mm_setr_epi32(base + 2, 0, base + 4, 0);\n        __m128i prod = _mm_mul_epu32(i_vals, i_next);\n        __m128i half = _mm_srli_epi64(prod, 1);\n\n        alignas(16) uint64_t temp[2];\n        _mm_store_si128((__m128i*)temp, half);\n\n        int odd_val0 = temp[0] % 10000;\n        int odd_val1 = temp[1] % 10000;\n\n        for (int k = last_i + 1; k <= base + 2; ++k) {\n            last_fact = (last_fact * k) % 10000;\n        }\n        last_i = base + 2;\n        int even_val0 = last_fact;\n\n        for (int k = last_i + 1; k <= base + 4; ++k) {\n            last_fact = (last_fact * k) % 10000;\n        }\n        last_i = base + 4;\n        int even_val1 = last_fact;\n\n        res[base] = odd_val0;\n        res[base + 1] = even_val0;\n        res[base + 2] = odd_val1;\n        res[base + 3] = even_val1;\n    }\n\n    for (int j = base; j < n; ++j) {\n        int i_val = j + 1;\n        if (i_val % 2 == 0) {\n            for (int k = last_i + 1; k <= i_val; ++k) {\n                last_fact = (last_fact * k) % 10000;\n            }\n            last_i = i_val;\n            res[j] = last_fact;\n        } else {\n            long long prod = (long long)i_val * (i_val + 1);\n            res[j] = (prod / 2) % 10000;\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<int> func_simd(int n) {\n    if (n <= 0) {\n        return std::vector<int>();\n    }\n    std::vector<int> result(n, 0);\n    \n    // Vectorized loop for even indices\n    int base = 0;\n    __m128i indices = _mm_setr_epi32(0, 2, 4, 6);\n    for (; base + 6 < n; base += 8) {\n        __m128i base_vec = _mm_set1_epi32(base);\n        __m128i j_vec = _mm_add_epi32(base_vec, indices);\n        __m128i k_vec = _mm_srli_epi32(j_vec, 1);\n        __m128i term1 = _mm_add_epi32(_mm_slli_epi32(k_vec, 1), _mm_set1_epi32(1));\n        __m128i term2 = _mm_add_epi32(k_vec, _mm_set1_epi32(1));\n        __m128i T_vec = _mm_mullo_epi32(term1, term2);\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), T_vec);\n        if (base < n) result[base] = temp[0];\n        if (base + 2 < n) result[base + 2] = temp[1];\n        if (base + 4 < n) result[base + 4] = temp[2];\n        if (base + 6 < n) result[base + 6] = temp[3];\n    }\n    \n    // Scalar loop for remaining even indices\n    for (int j = base; j < n; j++) {\n        if (j % 2 == 0) {\n            int k = j / 2;\n            result[j] = (2 * k + 1) * (k + 1);\n        }\n    }\n    \n    // Sequential loop for odd indices\n    long long current_fact = 1;\n    for (int j = 1; j < n; j += 2) {\n        int k = (j + 1) / 2;\n        current_fact = (current_fact * (2LL * k - 1) * (2LL * k) % 10000;\n        result[j] = static_cast<int>(current_fact);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n <= 0) {\n        return res;\n    }\n\n    int i = 0;\n    for (; i <= n - 4; i += 4) {\n        __m128i base = _mm_setr_epi32(i + 1, i + 2, i + 3, i + 4);\n        __m128i base_plus_one = _mm_add_epi32(base, _mm_set1_epi32(1));\n        __m128i mask = _mm_cmpeq_epi32(\n            _mm_and_si128(base, _mm_set1_epi32(1)),\n            _mm_setzero_si128()\n        );\n        __m128i base_shifted = _mm_srli_epi32(base, 1);\n        __m128i base_plus_one_shifted = _mm_srli_epi32(base_plus_one, 1);\n        __m128i term1 = _mm_mullo_epi32(base_shifted, base_plus_one);\n        __m128i term2 = _mm_mullo_epi32(base, base_plus_one_shifted);\n        __m128i tri = _mm_or_si128(\n            _mm_and_si128(mask, term1),\n            _mm_andnot_si128(mask, term2)\n        );\n        _mm_storeu_si128((__m128i*)(res.data() + i), tri);\n    }\n\n    for (; i < n; i++) {\n        int idx_val = i + 1;\n        res[i] = (idx_val * (idx_val + 1)) / 2;\n    }\n\n    long long fact = 1;\n    for (int j = 2; j <= n; j += 2) {\n        fact = (fact * (j - 1)) % 10000;\n        fact = (fact * j) % 10000;\n        res[j - 1] = static_cast<int>(fact);\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> res(n);\n    if (n >= 1) {\n        res[0] = 1;\n    }\n    if (n >= 2) {\n        res[1] = 2;\n    }\n    if (n >= 3) {\n        res[2] = (3 * 4) / 2 % 10000;\n    }\n    if (n >= 4) {\n        res[3] = 24;\n    }\n\n    int last_factorial = 24;\n    for (int idx = 4; idx < n; idx++) {\n        int i = idx + 1;\n        if (i % 2 == 1) {\n            long long product = static_cast<long long>(i) * (i + 1);\n            res[idx] = (product / 2) % 10000;\n        } else {\n            if (idx + 2 < n) {\n                __m128i v_i = _mm_setr_epi32(i, i + 2, 0, 0);\n                __m128i v_one = _mm_set1_epi32(1);\n                __m128i v_i_minus = _mm_sub_epi32(v_i, v_one);\n                __m128i v_term = _mm_mullo_epi32(v_i, v_i_minus);\n\n                int term1 = _mm_extract_epi32(v_term, 0);\n                int term2 = _mm_extract_epi32(v_term, 1);\n\n                int fact1 = (last_factorial * term1) % 10000;\n                int fact2 = (fact1 * term2) % 10000;\n\n                res[idx] = fact1;\n                res[idx + 2] = fact2;\n                last_factorial = fact2;\n\n                if (idx + 1 < n) {\n                    int i_next = i + 1;\n                    long long product_next = static_cast<long long>(i_next) * (i_next + 1);\n                    res[idx + 1] = (product_next / 2) % 10000;\n                }\n\n                idx += 2;\n            } else {\n                int term = (i - 1) * i;\n                last_factorial = (last_factorial * term) % 10000;\n                res[idx] = last_factorial;\n            }\n        }\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    if (n < 1) \n        return {0,0};\n    \n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int base = 1; base <= n; base += 4) {\n        __m128i v_nums = _mm_set_epi32(base+3, base+2, base+1, base);\n        __m128i n_plus1 = _mm_set1_epi32(n+1);\n        __m128i v_valid = _mm_cmplt_epi32(v_nums, n_plus1);\n\n        int nums[4];\n        _mm_storeu_si128((__m128i*)nums, v_nums);\n\n        int d0[4], d1[4], d2[4], d3[4];\n        for (int j = 0; j < 4; j++) {\n            int x = nums[j];\n            d0[j] = x % 10;\n            x /= 10;\n            d1[j] = x % 10;\n            x /= 10;\n            d2[j] = x % 10;\n            x /= 10;\n            d3[j] = x % 10;\n        }\n\n        __m128i v_d0 = _mm_set_epi32(d0[3], d0[2], d0[1], d0[0]);\n        __m128i v_d1 = _mm_set_epi32(d1[3], d1[2], d1[1], d1[0]);\n        __m128i v_d2 = _mm_set_epi32(d2[3], d2[2], d2[1], d2[0]);\n        __m128i v_d3 = _mm_set_epi32(d3[3], d3[2], d3[1], d3[0]);\n\n        __m128i v_mask4 = _mm_cmpgt_epi32(v_d3, zero);\n        __m128i v_mask3 = _mm_andnot_si128(v_mask4, _mm_cmpgt_epi32(v_d2, zero));\n        __m128i v_mask2_temp = _mm_and_si128(_mm_cmpeq_epi32(v_d3, zero), _mm_cmpeq_epi32(v_d2, zero));\n        __m128i v_mask2 = _mm_and_si128(v_mask2_temp, _mm_cmpgt_epi32(v_d1, zero));\n        __m128i v_mask1 = _mm_and_si128(v_mask2_temp, _mm_cmpeq_epi32(v_d1, zero));\n\n        __m128i cond4 = _mm_and_si128(_mm_and_si128(_mm_cmpeq_epi32(v_d0, v_d3), _mm_cmpeq_epi32(v_d1, v_d2));\n        cond4 = _mm_and_si128(cond4, v_mask4);\n        __m128i cond3 = _mm_and_si128(_mm_cmpeq_epi32(v_d0, v_d2), v_mask3);\n        __m128i cond2 = _mm_and_si128(_mm_cmpeq_epi32(v_d0, v_d1), v_mask2);\n        __m128i cond1 = v_mask1;\n\n        __m128i cond = _mm_or_si128(_mm_or_si128(cond4, cond3), _mm_or_si128(cond2, cond1));\n        cond = _mm_and_si128(cond, v_valid);\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(v_nums, one), zero);\n        __m128i even_inc = _mm_and_si128(cond, even_mask);\n        __m128i odd_inc = _mm_andnot_si128(even_mask, cond);\n\n        even_count = _mm_sub_epi32(even_count, even_inc);\n        odd_count = _mm_sub_epi32(odd_count, odd_inc);\n    }\n\n    alignas(16) int even_arr[4], odd_arr[4];\n    _mm_storeu_si128((__m128i*)even_arr, even_count);\n    _mm_storeu_si128((__m128i*)odd_arr, odd_count);\n\n    int total_even = even_arr[0] + even_arr[1] + even_arr[2] + even_arr[3];\n    int total_odd = odd_arr[0] + odd_arr[1] + odd_arr[2] + odd_arr[3];\n\n    return {total_even, total_odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even = 0;\n    int odd = 0;\n    int i = 1;\n    while (i <= n) {\n        int nums[4] = {0};\n        for (int j = 0; j < 4; j++) {\n            if (i + j <= n) {\n                nums[j] = i + j;\n            }\n        }\n        i += 4;\n\n        __m128i v = _mm_set_epi32(nums[3], nums[2], nums[1], nums[0]);\n        __m128i v_orig = v;\n\n        __m128i ten = _mm_set1_epi32(10);\n        __m128i d0, d1, d2, d3;\n\n        // Extract d0\n        __m128 vf = _mm_cvtepi32_ps(v);\n        __m128 qf = _mm_mul_ps(vf, _mm_set1_ps(0.1f));\n        __m128i q = _mm_cvttps_epi32(qf);\n        __m128i q10 = _mm_add_epi32(_mm_slli_epi32(q, 3), _mm_slli_epi32(q, 1));\n        d0 = _mm_sub_epi32(v, q10);\n        v = q;\n\n        // Extract d1\n        vf = _mm_cvtepi32_ps(v);\n        qf = _mm_mul_ps(vf, _mm_set1_ps(0.1f));\n        q = _mm_cvttps_epi32(qf);\n        q10 = _mm_add_epi32(_mm_slli_epi32(q, 3), _mm_slli_epi32(q, 1));\n        d1 = _mm_sub_epi32(v, q10);\n        v = q;\n\n        // Extract d2\n        vf = _mm_cvtepi32_ps(v);\n        qf = _mm_mul_ps(vf, _mm_set1_ps(0.1f));\n        q = _mm_cvttps_epi32(qf);\n        q10 = _mm_add_epi32(_mm_slli_epi32(q, 3), _mm_slli_epi32(q, 1));\n        d2 = _mm_sub_epi32(v, q10);\n        v = q;\n\n        // Extract d3\n        vf = _mm_cvtepi32_ps(v);\n        qf = _mm_mul_ps(vf, _mm_set1_ps(0.1f));\n        q = _mm_cvttps_epi32(qf);\n        q10 = _mm_add_epi32(_mm_slli_epi32(q, 3), _mm_slli_epi32(q, 1));\n        d3 = _mm_sub_epi32(v, q10);\n\n        // Compute reversed_full in scalar\n        int v_arr[4], d0_arr[4], d1_arr[4], d2_arr[4], d3_arr[4];\n        _mm_storeu_si128((__m128i*)v_arr, v_orig);\n        _mm_storeu_si128((__m128i*)d0_arr, d0);\n        _mm_storeu_si128((__m128i*)d1_arr, d1);\n        _mm_storeu_si128((__m128i*)d2_arr, d2);\n        _mm_storeu_si128((__m128i*)d3_arr, d3);\n\n        for (int j = 0; j < 4; j++) {\n            if (v_arr[j] == 0) continue;\n\n            int reversed_full = d0_arr[j] * 1000 + d1_arr[j] * 100 + d2_arr[j] * 10 + d3_arr[j];\n            int num = v_arr[j];\n            int k = 1;\n            if (num >= 10) k++;\n            if (num >= 100) k++;\n            if (num >= 1000) k++;\n\n            int divisor = 1;\n            if (k == 1) divisor = 1000;\n            else if (k == 2) divisor = 100;\n            else if (k == 3) divisor = 10;\n\n            int reversed_actual = reversed_full / divisor;\n            if (num == reversed_actual) {\n                if (num % 2 == 0) even++;\n                else odd++;\n            }\n        }\n    }\n    return {even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i v = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i n_vec = _mm_set1_epi32(n);\n        __m128i valid_mask = _mm_cmple_epi32(v, n_vec);\n\n        __m128 v_float = _mm_cvtepi32_ps(v);\n\n        __m128 quotient1_float = _mm_div_ps(v_float, _mm_set1_ps(10.0f));\n        __m128i quotient1 = _mm_cvttps_epi32(quotient1_float);\n        __m128i units = _mm_sub_epi32(v, _mm_mullo_epi32(quotient1, _mm_set1_epi32(10)));\n\n        __m128 quotient1_float2 = _mm_cvtepi32_ps(quotient1);\n        __m128 quotient2_float = _mm_div_ps(quotient1_float2, _mm_set1_ps(10.0f));\n        __m128i quotient2 = _mm_cvttps_epi32(quotient2_float);\n        __m128i tens = _mm_sub_epi32(quotient1, _mm_mullo_epi32(quotient2, _mm_set1_epi32(10)));\n\n        __m128 quotient2_float2 = _mm_cvtepi32_ps(quotient2);\n        __m128 quotient3_float = _mm_div_ps(quotient2_float2, _mm_set1_ps(10.0f));\n        __m128i quotient3 = _mm_cvttps_epi32(quotient3_float);\n        __m128i hundreds = _mm_sub_epi32(quotient2, _mm_mullo_epi32(quotient3, _mm_set1_epi32(10)));\n\n        __m128 quotient3_float2 = _mm_cvtepi32_ps(quotient3);\n        __m128 quotient4_float = _mm_div_ps(quotient3_float2, _mm_set1_ps(10.0f));\n        __m128i quotient4 = _mm_cvttps_epi32(quotient4_float);\n        __m128i thousands = _mm_sub_epi32(quotient3, _mm_mullo_epi32(quotient4, _mm_set1_epi32(10)));\n\n        __m128i mask_ge1000 = _mm_cmpge_epi32(v, _mm_set1_epi32(1000));\n        __m128i mask_ge100 = _mm_cmpge_epi32(v, _mm_set1_epi32(100));\n        __m128i mask_ge10 = _mm_cmpge_epi32(v, _mm_set1_epi32(10));\n\n        __m128i mask_4digits = mask_ge1000;\n        __m128i mask_3digits = _mm_and_si128(mask_ge100, _mm_andnot_si128(mask_ge1000, _mm_set1_epi32(-1)));\n        __m128i mask_2digits = _mm_and_si128(mask_ge10, _mm_andnot_si128(mask_ge100, _mm_set1_epi32(-1)));\n        __m128i mask_1digit = _mm_andnot_si128(mask_ge10, _mm_set1_epi32(-1));\n\n        __m128i cond_4 = _mm_and_si128(\n            _mm_cmpeq_epi32(units, thousands),\n            _mm_cmpeq_epi32(tens, hundreds)\n        );\n        __m128i cond_3 = _mm_cmpeq_epi32(units, hundreds);\n        __m128i cond_2 = _mm_cmpeq_epi32(units, tens);\n        __m128i cond_1 = _mm_set1_epi32(-1);\n\n        __m128i pal_4 = _mm_and_si128(mask_4digits, cond_4);\n        __m128i pal_3 = _mm_and_si128(mask_3digits, cond_3);\n        __m128i pal_2 = _mm_and_si128(mask_2digits, cond_2);\n        __m128i pal_1 = _mm_and_si128(mask_1digit, cond_1);\n\n        __m128i is_palindrome = _mm_or_si128(pal_4, _mm_or_si128(pal_3, _mm_or_si128(pal_2, pal_1)));\n\n        __m128i lsb = _mm_and_si128(v, _mm_set1_epi32(1));\n        __m128i even_mask = _mm_cmpeq_epi32(lsb, _mm_setzero_si128());\n        __m128i odd_mask = _mm_cmpeq_epi32(lsb, _mm_set1_epi32(1));\n\n        __m128i even_pal = _mm_and_si128(is_palindrome, even_mask);\n        __m128i odd_pal = _mm_and_si128(is_palindrome, odd_mask);\n\n        even_pal = _mm_and_si128(even_pal, valid_mask);\n        odd_pal = _mm_and_si128(odd_pal, valid_mask);\n\n        alignas(16) int even_arr[4], odd_arr[4];\n        _mm_store_si128((__m128i*)even_arr, even_pal);\n        _mm_store_si128((__m128i*)odd_arr, odd_pal);\n\n        for (int j = 0; j < 4; j++) {\n            if (even_arr[j] != 0) {\n                even_count++;\n            }\n            if (odd_arr[j] != 0) {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_count_vec = _mm_setzero_si128();\n    __m128i odd_count_vec = _mm_setzero_si128();\n    __m128i ten = _mm_set1_epi32(10);\n    __m128i hundred = _mm_set1_epi32(100);\n    __m128i thousand = _mm_set1_epi32(1000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i ninety_nine = _mm_set1_epi32(99);\n    __m128i nine_ninety_nine = _mm_set1_epi32(999);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; i += 4) {\n        int indices[4];\n        for (int j = 0; j < 4; ++j) {\n            if (i + j <= n) {\n                indices[j] = i + j;\n            } else {\n                indices[j] = 10;\n            }\n        }\n\n        __m128i v_x = _mm_loadu_si128((__m128i*)indices);\n\n        int d0_arr[4], d1_arr[4], d2_arr[4], d3_arr[4];\n        for (int j = 0; j < 4; ++j) {\n            int x = indices[j];\n            d0_arr[j] = x % 10;\n            int t = x / 10;\n            d1_arr[j] = t % 10;\n            t /= 10;\n            d2_arr[j] = t % 10;\n            t /= 10;\n            d3_arr[j] = t % 10;\n        }\n\n        __m128i v_d0 = _mm_loadu_si128((__m128i*)d0_arr);\n        __m128i v_d1 = _mm_loadu_si128((__m128i*)d1_arr);\n        __m128i v_d2 = _mm_loadu_si128((__m128i*)d2_arr);\n        __m128i v_d3 = _mm_loadu_si128((__m128i*)d3_arr);\n\n        __m128i cond1 = _mm_cmplt_epi32(v_x, ten);\n        __m128i cond2a = _mm_cmplt_epi32(v_x, hundred);\n        __m128i cond2b = _mm_cmpeq_epi32(v_d0, v_d1);\n        __m128i cond2 = _mm_and_si128(cond2a, cond2b);\n        __m128i cond3a = _mm_cmplt_epi32(v_x, thousand);\n        __m128i cond3b = _mm_cmpgt_epi32(v_x, ninety_nine);\n        __m128i cond3c = _mm_and_si128(cond3a, cond3b);\n        __m128i cond3d = _mm_cmpeq_epi32(v_d0, v_d2);\n        __m128i cond3 = _mm_and_si128(cond3c, cond3d);\n        __m128i cond4a = _mm_cmpgt_epi32(v_x, nine_ninety_nine);\n        __m128i cond4b = _mm_cmpeq_epi32(v_d0, v_d3);\n        __m128i cond4c = _mm_cmpeq_epi32(v_d1, v_d2);\n        __m128i cond4d = _mm_and_si128(cond4b, cond4c);\n        __m128i cond4 = _mm_and_si128(cond4a, cond4d);\n        __m128i is_pal = _mm_or_si128(cond1, _mm_or_si128(cond2, _mm_or_si128(cond3, cond4)));\n\n        __m128i mask_even = _mm_cmpeq_epi32(_mm_and_si128(v_x, one), zero);\n        __m128i mask_odd = _mm_cmpeq_epi32(_mm_and_si128(v_x, one), one);\n        __m128i mask_even_pal = _mm_and_si128(is_pal, mask_even);\n        __m128i mask_odd_pal = _mm_and_si128(is_pal, mask_odd);\n\n        __m128i even_inc = _mm_and_si128(mask_even_pal, one);\n        __m128i odd_inc = _mm_and_si128(mask_odd_pal, one);\n        even_count_vec = _mm_add_epi32(even_count_vec, even_inc);\n        odd_count_vec = _mm_add_epi32(odd_count_vec, odd_inc);\n    }\n\n    alignas(16) int even_tmp[4], odd_tmp[4];\n    _mm_store_si128((__m128i*)even_tmp, even_count_vec);\n    _mm_store_si128((__m128i*)odd_tmp, odd_count_vec);\n    int even_count = 0, odd_count = 0;\n    for (int i = 0; i < 4; ++i) {\n        even_count += even_tmp[i];\n        odd_count += odd_tmp[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_count_vec = _mm_setzero_si128();\n    __m128i odd_count_vec = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i v = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask_valid = _mm_cmplt_epi32(v, _mm_set1_epi32(n+1));\n\n        __m128 v_f = _mm_cvtepi32_ps(v);\n        __m128 d3_f = _mm_div_ps(v_f, _mm_set1_ps(1000.0f));\n        __m128i d3 = _mm_cvttps_epi32(d3_f);\n        __m128i d3_16 = _mm_packs_epi32(d3, d3);\n        __m128i d3_1000_16 = _mm_mullo_epi16(d3_16, _mm_set1_epi16(1000));\n        __m128i d3_1000_32 = _mm_unpacklo_epi16(d3_1000_16, zero);\n        __m128i v1 = _mm_sub_epi32(v, d3_1000_32);\n\n        __m128 v1_f = _mm_cvtepi32_ps(v1);\n        __m128 d2_f = _mm_div_ps(v1_f, _mm_set1_ps(100.0f));\n        __m128i d2 = _mm_cvttps_epi32(d2_f);\n        __m128i d2_16 = _mm_packs_epi32(d2, d2);\n        __m128i d2_100_16 = _mm_mullo_epi16(d2_16, _mm_set1_epi16(100));\n        __m128i d2_100_32 = _mm_unpacklo_epi16(d2_100_16, zero);\n        __m128i v2 = _mm_sub_epi32(v1, d2_100_32);\n\n        __m128 v2_f = _mm_cvtepi32_ps(v2);\n        __m128 d1_f = _mm_div_ps(v2_f, _mm_set1_ps(10.0f));\n        __m128i d1 = _mm_cvttps_epi32(d1_f);\n        __m128i d1_16 = _mm_packs_epi32(d1, d1);\n        __m128i d1_10_16 = _mm_mullo_epi16(d1_16, _mm_set1_epi16(10));\n        __m128i d1_10_32 = _mm_unpacklo_epi16(d1_10_16, zero);\n        __m128i d0 = _mm_sub_epi32(v2, d1_10_32);\n\n        __m128i mask1 = _mm_cmplt_epi32(v, _mm_set1_epi32(10));\n        __m128i mask2_cond = _mm_and_si128(\n            _mm_cmpgt_epi32(v, _mm_set1_epi32(9)),\n            _mm_cmplt_epi32(v, _mm_set1_epi32(100))\n        );\n        __m128i mask2 = _mm_and_si128(mask2_cond, _mm_cmpeq_epi32(d0, d1));\n        __m128i mask3_cond = _mm_and_si128(\n            _mm_cmpgt_epi32(v, _mm_set1_epi32(99)),\n            _mm_cmplt_epi32(v, _mm_set1_epi32(1000))\n        );\n        __m128i mask3 = _mm_and_si128(mask3_cond, _mm_cmpeq_epi32(d0, d2));\n        __m128i mask4_cond = _mm_cmpgt_epi32(v, _mm_set1_epi32(999));\n        __m128i mask4_inner = _mm_and_si128(\n            _mm_cmpeq_epi32(d0, d3),\n            _mm_cmpeq_epi32(d1, d2)\n        );\n        __m128i mask4 = _mm_and_si128(mask4_cond, mask4_inner);\n\n        __m128i mask_palindrome = _mm_or_si128(\n            mask1,\n            _mm_or_si128(\n                mask2,\n                _mm_or_si128(mask3, mask4)\n            )\n        );\n        mask_palindrome = _mm_and_si128(mask_palindrome, mask_valid);\n\n        __m128i lsb = _mm_and_si128(v, one);\n        __m128i even_cond = _mm_cmpeq_epi32(lsb, zero);\n        __m128i even_mask = _mm_and_si128(mask_palindrome, even_cond);\n        __m128i odd_cond = _mm_cmpeq_epi32(lsb, one);\n        __m128i odd_mask = _mm_and_si128(mask_palindrome, odd_cond);\n\n        even_count_vec = _mm_add_epi32(even_count_vec, _mm_and_si128(even_mask, one));\n        odd_count_vec = _mm_add_epi32(odd_count_vec, _mm_and_si128(odd_mask, one));\n    }\n\n    alignas(16) int even_buf[4], odd_buf[4];\n    _mm_store_si128((__m128i*)even_buf, even_count_vec);\n    _mm_store_si128((__m128i*)odd_buf, odd_count_vec);\n    int even_count = 0, odd_count = 0;\n    for (int i = 0; i < 4; i++) {\n        even_count += even_buf[i];\n        odd_count += odd_buf[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int i = 0;\n    int size = n.size();\n    \n    for (; i + 4 <= size; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i zero = _mm_setzero_si128();\n        __m128i sign_mask = _mm_cmplt_epi32(v, zero);\n        __m128i neg_v = _mm_sub_epi32(zero, v);\n        __m128i abs_v = _mm_or_si128(_mm_and_si128(sign_mask, neg_v), \n                _mm_andnot_si128(sign_mask, v));\n        \n        int abs_arr[4];\n        int orig_arr[4];\n        _mm_storeu_si128((__m128i*)abs_arr, abs_v);\n        _mm_storeu_si128((__m128i*)orig_arr, v);\n        \n        for (int j = 0; j < 4; j++) {\n            if (abs_arr[j] == 0) continue;\n            int temp = abs_arr[j];\n            int msd = 0;\n            int ds = 0;\n            while (temp) {\n                int digit = temp % 10;\n                ds += digit;\n                temp /= 10;\n                if (temp == 0) {\n                    msd = digit;\n                }\n            }\n            if (orig_arr[j] < 0) {\n                ds = ds - 2 * msd;\n            }\n            if (ds > 0) {\n                count++;\n            }\n        }\n    }\n    \n    for (; i < size; i++) {\n        int num = n[i];\n        int x_abs = (num < 0) ? -num : num;\n        if (x_abs == 0) continue;\n        int temp = x_abs;\n        int msd = 0;\n        int ds = 0;\n        while (temp) {\n            int digit = temp % 10;\n            ds += digit;\n            temp /= 10;\n            if (temp == 0) {\n                msd = digit;\n            }\n        }\n        if (num < 0) {\n            ds = ds - 2 * msd;\n        }\n        if (ds > 0) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n\n    for (; i + 3 < size; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&n[i]);\n        int values[4];\n        _mm_storeu_si128((__m128i*)values, v);\n\n        int counts[4] = {0};\n        int abs_values[4];\n        for (int j = 0; j < 4; j++) {\n            if (values[j] > 0) {\n                counts[j] = 1;\n                abs_values[j] = 0;\n            } else if (values[j] < 0) {\n                abs_values[j] = -values[j];\n            } else {\n                abs_values[j] = 0;\n            }\n        }\n\n        int digit_sum[4] = {0};\n        int first_digit[4] = {0};\n        int current[4];\n        for (int j = 0; j < 4; j++) {\n            current[j] = abs_values[j];\n        }\n\n        for (int d = 0; d < 10; d++) {\n            for (int j = 0; j < 4; j++) {\n                if (current[j] == 0) continue;\n                int digit = current[j] % 10;\n                digit_sum[j] += digit;\n                current[j] /= 10;\n                if (current[j] == 0) {\n                    first_digit[j] = digit;\n                }\n            }\n        }\n\n        for (int j = 0; j < 4; j++) {\n            if (values[j] < 0) {\n                int adjusted_sum = digit_sum[j] - 2 * first_digit[j];\n                if (adjusted_sum > 0) {\n                    counts[j] = 1;\n                } else {\n                    counts[j] = 0;\n                }\n            }\n        }\n\n        count += counts[0] + counts[1] + counts[2] + counts[3];\n    }\n\n    for (; i < size; i++) {\n        int val = n[i];\n        if (val > 0) {\n            count++;\n        } else if (val < 0) {\n            int abs_val = -val;\n            int digit_sum = 0;\n            int first_digit = 0;\n            int current = abs_val;\n            while (current) {\n                int digit = current % 10;\n                digit_sum += digit;\n                current /= 10;\n                if (current == 0) {\n                    first_digit = digit;\n                }\n            }\n            int adjusted_sum = digit_sum - 2 * first_digit;\n            if (adjusted_sum > 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n#include <cmath>\n\nint count_nums_simd(std::vector<int> n) {\n    int total_count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n    if (size == 0) return 0;\n    \n    for (; i + 3 < size; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i zero = _mm_setzero_si128();\n        __m128i sign_mask = _mm_cmplt_epi32(v, zero);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, sign_mask), sign_mask);\n        \n        alignas(16) int abs_arr[4];\n        alignas(16) int sign_arr[4];\n        _mm_store_si128((__m128i*)abs_arr, abs_v);\n        _mm_store_si128((__m128i*)sign_arr, sign_mask);\n        \n        int sums[4] = {0, 0, 0, 0};\n        for (int j = 0; j < 4; j++) {\n            if (abs_arr[j] == 0) {\n                sums[j] = 0;\n                continue;\n            }\n            int temp = abs_arr[j];\n            int total_digit = 0;\n            int last_digit = 0;\n            while (temp != 0) {\n                int digit = temp % 10;\n                total_digit += digit;\n                if (temp < 10) {\n                    last_digit = temp;\n                }\n                temp /= 10;\n            }\n            if (sign_arr[j] != 0) {\n                total_digit = total_digit - 2 * last_digit;\n            }\n            sums[j] = total_digit;\n        }\n        \n        __m128i sum_vec = _mm_load_si128((__m128i*)sums);\n        __m128i cmp = _mm_cmpgt_epi32(sum_vec, zero);\n        alignas(16) int cmp_arr[4];\n        _mm_store_si128((__m128i*)cmp_arr, cmp);\n        for (int j = 0; j < 4; j++) {\n            if (cmp_arr[j] != 0) {\n                total_count++;\n            }\n        }\n    }\n    \n    for (; i < size; i++) {\n        int num = n[i];\n        if (num == 0) {\n            continue;\n        }\n        int sign = (num < 0) ? -1 : 1;\n        int abs_num = (num == INT_MIN) ? 2147483648 : std::abs(num);\n        int temp = abs_num;\n        int total_digit = 0;\n        int last_digit = 0;\n        while (temp != 0) {\n            int digit = temp % 10;\n            total_digit += digit;\n            if (temp < 10) {\n                last_digit = temp;\n            }\n            temp /= 10;\n        }\n        if (sign == -1) {\n            total_digit = total_digit - 2 * last_digit;\n        }\n        if (total_digit > 0) {\n            total_count++;\n        }\n    }\n    \n    return total_count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int len = n.size();\n    int i = 0;\n    \n    auto signed_digit_sum = [](int num) -> int {\n        if (num == 0) return 0;\n        int sign = (num >= 0) ? 1 : -1;\n        unsigned int abs_val;\n        if (num == -2147483647 - 1) { // INT_MIN\n            abs_val = 2147483648u;\n        } else {\n            abs_val = (num >= 0) ? num : -num;\n        }\n        int digit_sum = 0;\n        int last_digit = 0;\n        while (abs_val) {\n            last_digit = abs_val % 10;\n            digit_sum += last_digit;\n            abs_val /= 10;\n        }\n        return (sign > 0) ? digit_sum : (digit_sum - 2 * last_digit);\n    };\n\n    for (; i + 4 <= len; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&n[i]);\n        int arr[4];\n        _mm_storeu_si128((__m128i*)arr, v);\n        \n        int s0 = signed_digit_sum(arr[0]);\n        int s1 = signed_digit_sum(arr[1]);\n        int s2 = signed_digit_sum(arr[2]);\n        int s3 = signed_digit_sum(arr[3]);\n        \n        __m128i sum_vec = _mm_set_epi32(s3, s2, s1, s0);\n        __m128i zeros = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpgt_epi32(sum_vec, zeros);\n        __m128i ones = _mm_srli_epi32(cmp, 31);\n        \n        int ones_arr[4];\n        _mm_storeu_si128((__m128i*)ones_arr, ones);\n        count += ones_arr[0] + ones_arr[1] + ones_arr[2] + ones_arr[3];\n    }\n    \n    for (; i < len; ++i) {\n        if (signed_digit_sum(n[i]) > 0) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstatic int digit_sum(int num) {\n    if (num == 0) {\n        return 0;\n    }\n    int total = 0;\n    if (num < 0) {\n        int n = std::abs(num);\n        int divisor = 1;\n        while (n >= 10) {\n            divisor *= 10;\n            n /= 10;\n        }\n        total = -n;\n        num = std::abs(num) - n * divisor;\n    }\n    while (num != 0) {\n        total += num % 10;\n        num /= 10;\n    }\n    return total;\n}\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) {\n        return 0;\n    }\n    std::vector<int> sums(n.size());\n    for (size_t i = 0; i < n.size(); ++i) {\n        sums[i] = digit_sum(n[i]);\n    }\n    const unsigned int popcount_lut[16] = {\n        0, 1, 1, 2, 1, 2, 2, 3,\n        1, 2, 2, 3, 2, 3, 3, 4\n    };\n    int count = 0;\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 3 < sums.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&sums[i]));\n        __m128i mask = _mm_cmpgt_epi32(v, zero);\n        int bits = _mm_movemask_ps(_mm_castsi128_ps(mask));\n        bits &= 0x0F;\n        count += popcount_lut[bits];\n    }\n    for (; i < sums.size(); ++i) {\n        if (sums[i] > 0) {\n            ++count;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n    int drops = 0;\n    int i = 0;\n    __m128i count_vec = _mm_setzero_si128();\n    int limit = n - 5;\n    for (; i <= limit; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)(arr.data() + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(arr.data() + i + 1));\n        __m128i cmp = _mm_cmpgt_epi32(a, b);\n        count_vec = _mm_sub_epi32(count_vec, cmp);\n    }\n    alignas(16) int tmp[4];\n    _mm_store_si128((__m128i*)tmp, count_vec);\n    int vector_drops = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n    drops += vector_drops;\n    for (; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            drops++;\n        }\n    }\n    if (arr[n - 1] > arr[0]) {\n        drops++;\n    }\n    return drops <= 1;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) \n        return true;\n\n    int count = 0;\n    int i = 0;\n\n    if (n > 4) {\n        for (; i <= n - 5; i += 4) {\n            __m128i v1 = _mm_loadu_si128((const __m128i*)(arr.data() + i));\n            __m128i v2 = _mm_loadu_si128((const __m128i*)(arr.data() + i + 1));\n            __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n            int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n            count += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n        }\n    }\n\n    for (; i < n - 1; i++) {\n        if (arr[i] > arr[i + 1])\n            count++;\n    }\n\n    if (arr[n - 1] > arr[0])\n        count++;\n\n    return (count <= 1);\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) {\n        return true;\n    }\n    std::vector<int> arr2 = arr;\n    arr2.push_back(arr[0]);\n\n    int count = 0;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr2[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr2[i+1]);\n        __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n    }\n\n    for (; i < n; i++) {\n        if (arr2[i] > arr2[i+1]) {\n            count++;\n        }\n    }\n\n    return count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) \n        return true;\n\n    int count = 0;\n    int* base = arr.data();\n\n    int i = 0;\n    int contig_n = n - 1;\n\n    for (; i <= contig_n - 4; i += 4) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(base + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(base + i + 1));\n        __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        count += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n    }\n\n    for (; i < contig_n; i++) {\n        if (base[i] > base[i+1])\n            count++;\n    }\n\n    if (base[n-1] > base[0])\n        count++;\n\n    return count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) \n        return true;\n    const int* data = arr.data();\n    int linear_count = 0;\n    if (n > 1) {\n        int i = 0;\n        for (; i <= n - 5; i += 4) {\n            __m128i vec1 = _mm_loadu_si128((const __m128i*)(data + i));\n            __m128i vec2 = _mm_loadu_si128((const __m128i*)(data + i + 1));\n            __m128i cmp = _mm_cmpgt_epi32(vec1, vec2);\n            int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n            linear_count += __builtin_popcount(mask);\n        }\n        for (; i < n - 1; i++) {\n            if (data[i] > data[i + 1])\n                linear_count++;\n        }\n    }\n    int circular_count = (n > 1 && data[n - 1] > data[0]) ? 1 : 0;\n    int total_count = linear_count + circular_count;\n    return total_count <= 1;\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int total_evens = 0;\n    size_t i = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i zeros = _mm_setzero_si128();\n\n    size_t n1 = lst1.size();\n    for (; i + 4 <= n1; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&lst1[i]);\n        __m128i low = _mm_and_si128(v, ones);\n        __m128i cmp = _mm_cmpeq_epi32(low, zeros);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        total_evens += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n    }\n    for (; i < n1; ++i) {\n        if (lst1[i] % 2 == 0) {\n            total_evens++;\n        }\n    }\n\n    i = 0;\n    size_t n2 = lst2.size();\n    for (; i + 4 <= n2; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&lst2[i]);\n        __m128i low = _mm_and_si128(v, ones);\n        __m128i cmp = _mm_cmpeq_epi32(low, zeros);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        total_evens += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n    }\n    for (; i < n2; ++i) {\n        if (lst2[i] % 2 == 0) {\n            total_evens++;\n        }\n    }\n\n    if (total_evens >= lst1.size()) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstatic int count_odds(const std::vector<int>& vec) {\n    size_t n = vec.size();\n    if (n == 0) \n        return 0;\n    size_t i = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i total = _mm_setzero_si128();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&vec[i]);\n        __m128i mask = _mm_and_si128(v, ones);\n        total = _mm_add_epi32(total, mask);\n    }\n\n    unsigned int partial[4];\n    _mm_storeu_si128((__m128i*)partial, total);\n    int count = partial[0] + partial[1] + partial[2] + partial[3];\n\n    for (; i < n; i++) {\n        count += (vec[i] & 1);\n    }\n\n    return count;\n}\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n2 = lst2.size();\n    int odd1 = count_odds(lst1);\n    int odd2 = count_odds(lst2);\n    if (odd1 + odd2 <= n2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    auto count_even = [](const std::vector<int>& vec) -> size_t {\n        size_t cnt = 0;\n        size_t i = 0;\n        size_t n = vec.size();\n        if (n >= 4) {\n            for (; i <= n - 4; i += 4) {\n                __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&vec[i]));\n                __m128i ones = _mm_set1_epi32(1);\n                __m128i lsbs = _mm_and_si128(v, ones);\n                __m128i cmp = _mm_cmpeq_epi32(lsbs, _mm_setzero_si128());\n                int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n                cnt += (mask & 1) + ((mask >> 1) & 1) + ((mask >> 2) & 1) + ((mask >> 3) & 1);\n            }\n        }\n        for (; i < n; ++i) {\n            if ((vec[i] & 1) == 0) {\n                ++cnt;\n            }\n        }\n        return cnt;\n    };\n\n    size_t total_even = count_even(lst1) + count_even(lst2);\n    if (total_even >= lst1.size()) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i zeros = _mm_setzero_si128();\n    __m128i total1 = _mm_setzero_si128();\n    __m128i total2 = _mm_setzero_si128();\n    size_t i;\n\n    size_t n1 = lst1.size();\n    for (i = 0; i + 4 <= n1; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)(lst1.data() + i));\n        __m128i lsb = _mm_and_si128(data, ones);\n        __m128i mask = _mm_cmpeq_epi32(lsb, zeros);\n        __m128i even_vec = _mm_and_si128(mask, ones);\n        total1 = _mm_add_epi32(total1, even_vec);\n    }\n    int tail_count1 = 0;\n    for (; i < n1; i++) {\n        if ((lst1[i] & 1) == 0) {\n            tail_count1++;\n        }\n    }\n    int count_arr1[4];\n    _mm_storeu_si128((__m128i*)count_arr1, total1);\n    int even1 = count_arr1[0] + count_arr1[1] + count_arr1[2] + count_arr1[3] + tail_count1;\n\n    size_t n2 = lst2.size();\n    for (i = 0; i + 4 <= n2; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)(lst2.data() + i));\n        __m128i lsb = _mm_and_si128(data, ones);\n        __m128i mask = _mm_cmpeq_epi32(lsb, zeros);\n        __m128i even_vec = _mm_and_si128(mask, ones);\n        total2 = _mm_add_epi32(total2, even_vec);\n    }\n    int tail_count2 = 0;\n    for (; i < n2; i++) {\n        if ((lst2[i] & 1) == 0) {\n            tail_count2++;\n        }\n    }\n    int count_arr2[4];\n    _mm_storeu_si128((__m128i*)count_arr2, total2);\n    int even2 = count_arr2[0] + count_arr2[1] + count_arr2[2] + count_arr2[3] + tail_count2;\n\n    if (even1 + even2 >= n1) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    auto count_even = [](const std::vector<int>& vec) -> unsigned long long {\n        if (vec.empty()) \n            return 0;\n        const size_t n = vec.size();\n        const int* data = vec.data();\n        size_t i = 0;\n        __m128i total_vec = _mm_setzero_si128();\n        \n        for (; i + 4 <= n; i += 4) {\n            __m128i v = _mm_loadu_si128((const __m128i*)(data + i));\n            __m128i ones = _mm_set1_epi32(1);\n            __m128i and_result = _mm_and_si128(v, ones);\n            __m128i mask = _mm_cmpeq_epi32(and_result, _mm_setzero_si128());\n            __m128i count_vec = _mm_srli_epi32(mask, 31);\n            total_vec = _mm_add_epi32(total_vec, count_vec);\n        }\n        \n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i*)temp, total_vec);\n        unsigned long long count = temp[0] + temp[1] + temp[2] + temp[3];\n        \n        for (; i < n; ++i) {\n            if ((data[i] & 1) == 0) {\n                ++count;\n            }\n        }\n        return count;\n    };\n    \n    unsigned long long total_evens = count_even(lst1) + count_even(lst2);\n    if (total_evens >= lst1.size()) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    const std::string base = \"the number of odd elements in the string i of the input.\";\n    std::vector<std::string> result;\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        size_t len = s.length();\n        const char* str = s.c_str();\n        size_t i = 0;\n        \n        if (len >= 16) {\n            __m128i zero_char = _mm_set1_epi8('0');\n            __m128i ones = _mm_set1_epi8(1);\n            for (; i <= len - 16; i += 16) {\n                __m128i chunk = _mm_loadu_si128((const __m128i*)(str + i));\n                __m128i digits = _mm_sub_epi8(chunk, zero_char);\n                __m128i odd_mask = _mm_and_si128(digits, ones);\n                __m128i sum_part = _mm_sad_epu8(odd_mask, _mm_setzero_si128());\n                \n                unsigned long long tmp0 = (unsigned long long)_mm_cvtsi128_si64(sum_part);\n                unsigned long long tmp1 = (unsigned long long)_mm_cvtsi128_si64(_mm_srli_si128(sum_part, 8));\n                count += (int)(tmp0 + tmp1);\n            }\n        }\n        \n        for (; i < len; ++i) {\n            int digit = str[i] - '0';\n            count += (digit & 1);\n        }\n        \n        std::string count_str = std::to_string(count);\n        std::string message = base;\n        size_t pos = 0;\n        while ((pos = message.find('i', pos)) != std::string::npos) {\n            message.replace(pos, 1, count_str);\n            pos += count_str.length();\n        }\n        result.push_back(message);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> res;\n    const std::string base = \"the number of odd elements in the string i of the input.\";\n\n    for (const auto& s : lst) {\n        size_t count = 0;\n        size_t len = s.size();\n        size_t i = 0;\n\n        if (len >= 16) {\n            __m128i ones = _mm_set1_epi8(1);\n            for (; i <= len - 16; i += 16) {\n                __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n                __m128i lsb = _mm_and_si128(chunk, ones);\n                __m128i zeros = _mm_setzero_si128();\n                __m128i sum_part = _mm_sad_epu8(lsb, zeros);\n                uint64_t sum_arr[2];\n                _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_arr), sum_part);\n                count += sum_arr[0] + sum_arr[1];\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (s[i] & 1) {\n                count++;\n            }\n        }\n\n        std::string count_str = std::to_string(count);\n        std::string new_str;\n        for (char c : base) {\n            if (c == 'i') {\n                new_str += count_str;\n            } else {\n                new_str.push_back(c);\n            }\n        }\n        res.push_back(new_str);\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    auto count_odd_digits = [](const std::string& s) -> unsigned long long {\n        size_t len = s.size();\n        if (len == 0) \n            return 0;\n        \n        const char* p = s.data();\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i total_vec = _mm_setzero_si128();\n        size_t i = 0;\n        \n        for (; i + 15 < len; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(p + i));\n            __m128i odd_flags = _mm_and_si128(chunk, ones);\n            __m128i sum_chunk = _mm_sad_epu8(odd_flags, _mm_setzero_si128());\n            total_vec = _mm_add_epi64(total_vec, sum_chunk);\n        }\n        \n        unsigned long long temp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), total_vec);\n        unsigned long long count = temp[0] + temp[1];\n        \n        for (; i < len; ++i) {\n            count += (p[i] & 1);\n        }\n        \n        return count;\n    };\n\n    std::vector<std::string> result;\n    const std::string template_str = \"the number of odd elements in the string i of the input.\";\n    \n    for (const std::string& s : lst) {\n        unsigned long long count_val = count_odd_digits(s);\n        std::string count_str = std::to_string(count_val);\n        std::string output_str;\n        for (char c : template_str) {\n            if (c == 'i') {\n                output_str += count_str;\n            } else {\n                output_str += c;\n            }\n        }\n        result.push_back(output_str);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <cstring>\n#include <cstdint>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    auto count_odd_digits = [](const std::string& s) -> int {\n        int len = s.length();\n        if (len == 0) return 0;\n        int total_count = 0;\n        int i = 0;\n        const __m128i zero = _mm_set1_epi8('0');\n        const __m128i one = _mm_set1_epi8(1);\n        const __m128i three = _mm_set1_epi8(3);\n        const __m128i five = _mm_set1_epi8(5);\n        const __m128i seven = _mm_set1_epi8(7);\n        const __m128i nine = _mm_set1_epi8(9);\n        for (; i + 15 < len; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n            __m128i digits = _mm_sub_epi8(chunk, zero);\n            __m128i cmp1 = _mm_cmpeq_epi8(digits, one);\n            __m128i cmp3 = _mm_cmpeq_epi8(digits, three);\n            __m128i cmp5 = _mm_cmpeq_epi8(digits, five);\n            __m128i cmp7 = _mm_cmpeq_epi8(digits, seven);\n            __m128i cmp9 = _mm_cmpeq_epi8(digits, nine);\n            __m128i any = _mm_or_si128(cmp1, cmp3);\n            any = _mm_or_si128(any, cmp5);\n            any = _mm_or_si128(any, cmp7);\n            any = _mm_or_si128(any, cmp9);\n            int mask = _mm_movemask_epi8(any);\n            total_count += __builtin_popcount(mask);\n        }\n        for (; i < len; ++i) {\n            if (s[i] == '1' || s[i] == '3' || s[i] == '5' || s[i] == '7' || s[i] == '9') {\n                total_count++;\n            }\n        }\n        return total_count;\n    };\n    const std::string templ = \"the number of odd elements in the string i of the input.\";\n    std::vector<std::string> res;\n    for (const auto& str : lst) {\n        int cnt = count_odd_digits(str);\n        std::string cnt_str = std::to_string(cnt);\n        std::string new_string;\n        for (char c : templ) {\n            if (c == 'i') {\n                new_string += cnt_str;\n            } else {\n                new_string.push_back(c);\n            }\n        }\n        res.push_back(new_string);\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> res;\n    res.reserve(lst.size());\n    const std::string base = \"the number of odd elements in the string i of the input.\";\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        size_t len = s.size();\n        size_t i = 0;\n        \n        if (len >= 16) {\n            const __m128i zero_char = _mm_set1_epi8('0');\n            const __m128i one = _mm_set1_epi8(1);\n            for (; i <= len - 16; i += 16) {\n                __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n                block = _mm_sub_epi8(block, zero_char);\n                __m128i mask = _mm_and_si128(block, one);\n                __m128i sum16 = _mm_sad_epu8(mask, _mm_setzero_si128());\n                uint64_t tmp[2];\n                _mm_storeu_si128(reinterpret_cast<__m128i*>(tmp), sum16);\n                count += tmp[0] + tmp[1];\n            }\n        }\n        \n        for (; i < len; ++i) {\n            int digit = s[i] - '0';\n            if (digit & 1) {\n                ++count;\n            }\n        }\n        \n        std::string count_str = std::to_string(count);\n        std::string new_str;\n        for (char c : base) {\n            if (c == 'i') {\n                new_str += count_str;\n            } else {\n                new_str += c;\n            }\n        }\n        res.push_back(new_str);\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    int n = nums.size();\n    if (n == 0) {\n        return 0;\n    }\n    int64_t global_min = nums[0];\n    int64_t current = nums[0];\n    int i = 1;\n    while (i < n) {\n        if (i + 1 < n) {\n            __m128i a = _mm_loadu_si128((__m128i*)&nums[i]);\n            int64_t a_arr[2];\n            _mm_storeu_si128((__m128i*)a_arr, a);\n            __m128i current_vec = _mm_set1_epi64x(current);\n            __m128i temp1 = _mm_add_epi64(current_vec, a);\n            int64_t temp1_arr[2];\n            _mm_storeu_si128((__m128i*)temp1_arr, temp1);\n            int64_t a0 = a_arr[0];\n            int64_t a1 = a_arr[1];\n            int64_t t0 = temp1_arr[0];\n            int64_t t1 = temp1_arr[1];\n            int64_t min1 = (a0 < t0) ? a0 : t0;\n            int64_t min2 = (a1 < (min1 + a1)) ? a1 : (min1 + a1);\n            if (min1 < global_min) {\n                global_min = min1;\n            }\n            if (min2 < global_min) {\n                global_min = min2;\n            }\n            current = min2;\n            i += 2;\n        } else {\n            current = (nums[i] < current + nums[i]) ? nums[i] : current + nums[i];\n            if (current < global_min) {\n                global_min = current;\n            }\n            i++;\n        }\n    }\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n    if (n == 1) return nums[0];\n    \n    std::vector<int64_t> P(n + 1);\n    P[0] = 0;\n    int64_t base = 0;\n    int i = 0;\n    \n    for (; i < n; i += 2) {\n        if (i + 1 < n) {\n            __m128i a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n            __m128i zero = _mm_setzero_si128();\n            __m128i a0_vec = _mm_unpacklo_epi64(zero, a);\n            __m128i partial = _mm_add_epi64(a, a0_vec);\n            __m128i base_vec = _mm_set1_epi64x(base);\n            __m128i p_chunk = _mm_add_epi64(base_vec, partial);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&P[i + 1]), p_chunk);\n            __m128i temp = _mm_srli_si128(p_chunk, 8);\n            base = _mm_cvtsi128_si64(temp);\n        } else {\n            P[i + 1] = base + nums[i];\n            base = P[i + 1];\n        }\n    }\n    \n    int64_t min_sum = P[1];\n    int64_t max_prefix = P[0];\n    for (int j = 1; j <= n; j++) {\n        if (P[j] - max_prefix < min_sum) {\n            min_sum = P[j] - max_prefix;\n        }\n        if (P[j] > max_prefix) {\n            max_prefix = P[j];\n        }\n    }\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) {\n        return 0;\n    }\n    int64_t prefix = 0;\n    int64_t max_prefix = 0;\n    int64_t min_sub = LLONG_MAX;\n    size_t i = 0;\n\n    for (; i <= n-2; i += 2) {\n        __m128i v_in = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i v_base = _mm_set1_epi64x(prefix);\n        __m128i v_shifted = _mm_slli_si128(v_in, 8);\n        __m128i v_in_partial = _mm_add_epi64(v_in, v_shifted);\n        __m128i v_prefix = _mm_add_epi64(v_base, v_in_partial);\n        int64_t temp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), v_prefix);\n        int64_t prefix1 = temp[0];\n        int64_t prefix2 = temp[1];\n        int64_t candidate1 = prefix1 - max_prefix;\n        int64_t max_prefix1 = (max_prefix > prefix1) ? max_prefix : prefix1;\n        int64_t candidate2 = prefix2 - max_prefix1;\n        if (candidate1 < min_sub) min_sub = candidate1;\n        if (candidate2 < min_sub) min_sub = candidate2;\n        max_prefix = (max_prefix1 > prefix2) ? max_prefix1 : prefix2;\n        prefix = prefix2;\n    }\n\n    for (; i < n; i++) {\n        prefix += nums[i];\n        int64_t candidate = prefix - max_prefix;\n        if (candidate < min_sub) min_sub = candidate;\n        if (prefix > max_prefix) max_prefix = prefix;\n    }\n\n    return min_sub;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <vector>\n#include <climits>\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n    \n    std::vector<int64_t> prefix(n + 1);\n    prefix[0] = 0;\n    for (int i = 0; i < n; ++i) {\n        prefix[i + 1] = prefix[i] + nums[i];\n    }\n    \n    int64_t max_prefix = prefix[0];\n    int64_t min_sum = LLONG_MAX;\n    int i = 1;\n    \n    for (; i <= n - 1; i += 2) {\n        int64_t a = prefix[i];\n        int64_t b = prefix[i + 1];\n        int64_t s1 = (max_prefix > a) ? max_prefix : a;\n        \n        __m128i vec_ab = _mm_set_epi64x(b, a);\n        __m128i vec_s = _mm_set_epi64x(s1, max_prefix);\n        __m128i vec_candidate = _mm_sub_epi64(vec_ab, vec_s);\n        \n        alignas(16) int64_t temp[2];\n        _mm_store_si128((__m128i*)temp, vec_candidate);\n        int64_t cand0 = temp[0];\n        int64_t cand1 = temp[1];\n        \n        if (cand0 < min_sum) min_sum = cand0;\n        if (cand1 < min_sum) min_sum = cand1;\n        \n        max_prefix = (s1 > b) ? s1 : b;\n    }\n    \n    if (i <= n) {\n        int64_t candidate = prefix[i] - max_prefix;\n        if (candidate < min_sum) min_sum = candidate;\n        if (prefix[i] > max_prefix) max_prefix = prefix[i];\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n    \n    int64_t* S = new int64_t[n+1];\n    S[0] = 0;\n    int64_t carry_scalar = 0;\n    size_t i;\n    for (i = 0; i + 1 < n; i += 2) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(&nums[i]));\n        __m128i v_carry = _mm_set_epi64x(0, carry_scalar);\n        __m128i v1 = _mm_add_epi64(v, v_carry);\n        int64_t first = _mm_extract_epi64(v1, 0);\n        __m128i v_add = _mm_set_epi64x(first, 0);\n        __m128i v2 = _mm_add_epi64(v1, v_add);\n        _mm_storeu_si128((__m128i*)(S + i + 1), v2);\n        carry_scalar = _mm_extract_epi64(v2, 1);\n    }\n    if (i < n) {\n        S[i+1] = carry_scalar + nums[i];\n    }\n\n    int64_t global_min = S[1];\n    int64_t min_prefix = (S[0] < S[1]) ? S[0] : S[1];\n    size_t j;\n    for (j = 2; j + 1 <= n; j += 2) {\n        __m128i s_vec = _mm_loadu_si128((__m128i const*)(S + j));\n        int64_t s0 = _mm_extract_epi64(s_vec, 0);\n        int64_t s1 = _mm_extract_epi64(s_vec, 1);\n        int64_t candidate0 = s0 - min_prefix;\n        int64_t min0 = (min_prefix < s0) ? min_prefix : s0;\n        int64_t candidate1 = s1 - min0;\n        int64_t min1 = (min0 < s1) ? min0 : s1;\n        if (candidate0 < global_min) global_min = candidate0;\n        if (candidate1 < global_min) global_min = candidate1;\n        min_prefix = min1;\n    }\n    if (j <= n) {\n        int64_t candidate = S[j] - min_prefix;\n        if (candidate < global_min) global_min = candidate;\n    }\n    delete[] S;\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    if (grid.empty()) \n        return 0;\n    int num_rows = grid.size();\n    int num_cols = grid[0].size();\n    long long total_ops = 0;\n\n    for (int i = 0; i < num_rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int row_sum = 0;\n        int j = 0;\n        __m128i vsum = _mm_setzero_si128();\n\n        for (; j <= num_cols - 4; j += 4) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[j]));\n            vsum = _mm_add_epi32(vsum, v);\n        }\n\n        alignas(16) int sum_arr[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(sum_arr), vsum);\n        row_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n        for (; j < num_cols; ++j) {\n            row_sum += row[j];\n        }\n\n        if (row_sum > 0) {\n            total_ops += (row_sum + capacity - 1) / capacity;\n        }\n    }\n\n    return total_ops;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    if (rows == 0) \n        return 0;\n    int cols = grid[0].size();\n    int total_lowerings = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const std::vector<int>& row = grid[i];\n        int row_sum = 0;\n        int j = 0;\n        __m128i vsum = _mm_setzero_si128();\n\n        for (; j <= cols - 4; j += 4) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[j]));\n            vsum = _mm_add_epi32(vsum, v);\n        }\n\n        int tmp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(tmp), vsum);\n        row_sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n\n        for (; j < cols; ++j) {\n            row_sum += row[j];\n        }\n\n        total_lowerings += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total_lowerings;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int total_steps = 0;\n    int n = grid.size();\n    if (n == 0) return 0;\n    int m = grid[0].size();\n\n    for (int i = 0; i < n; i++) {\n        const int* row = grid[i].data();\n        int j = 0;\n        __m128i sum4 = _mm_setzero_si128();\n        for (; j + 4 <= m; j += 4) {\n            __m128i v = _mm_loadu_si128((const __m128i*)(row + j));\n            sum4 = _mm_add_epi32(sum4, v);\n        }\n        int count = 0;\n        for (; j < m; j++) {\n            count += row[j];\n        }\n        alignas(16) int tmp[4];\n        _mm_store_si128((__m128i*)tmp, sum4);\n        count += tmp[0] + tmp[1] + tmp[2] + tmp[3];\n        total_steps += (count + capacity - 1) / capacity;\n    }\n    return total_steps;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    long long total_ops = 0;\n    if (grid.empty()) return 0;\n    size_t n = grid[0].size();\n    for (size_t i = 0; i < grid.size(); ++i) {\n        const std::vector<int>& row = grid[i];\n        size_t j = 0;\n        __m128i vsum0 = _mm_setzero_si128();\n        __m128i vsum1 = _mm_setzero_si128();\n        for (; j + 7 < n; j += 8) {\n            __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[j]));\n            __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[j+4]));\n            vsum0 = _mm_add_epi32(vsum0, v0);\n            vsum1 = _mm_add_epi32(vsum1, v1);\n        }\n        vsum0 = _mm_add_epi32(vsum0, vsum1);\n        if (j + 3 < n) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[j]));\n            vsum0 = _mm_add_epi32(vsum0, v);\n            j += 4;\n        }\n        int temp[4] __attribute__((aligned(16)));\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), vsum0);\n        int row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n        for (; j < n; ++j) {\n            row_sum += row[j];\n        }\n        if (row_sum > 0) {\n            total_ops += (row_sum + capacity - 1) / capacity;\n        }\n    }\n    return total_ops;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n    int total_operations = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        const int* row_ptr = grid[i].data();\n        int j = 0;\n        __m128i vsum1 = _mm_setzero_si128();\n        __m128i vsum2 = _mm_setzero_si128();\n        __m128i vsum3 = _mm_setzero_si128();\n        __m128i vsum4 = _mm_setzero_si128();\n\n        for (; j + 15 < cols; j += 16) {\n            __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j));\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j + 4));\n            __m128i v3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j + 8));\n            __m128i v4 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j + 12));\n\n            vsum1 = _mm_add_epi32(vsum1, v1);\n            vsum2 = _mm_add_epi32(vsum2, v2);\n            vsum3 = _mm_add_epi32(vsum3, v3);\n            vsum4 = _mm_add_epi32(vsum4, v4);\n        }\n\n        vsum1 = _mm_add_epi32(vsum1, vsum2);\n        vsum1 = _mm_add_epi32(vsum1, vsum3);\n        vsum1 = _mm_add_epi32(vsum1, vsum4);\n\n        alignas(16) int tmp[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(tmp), vsum1);\n        int row_sum = tmp[0] + tmp[1] + tmp[2] + tmp[3];\n\n        for (; j + 3 < cols; j += 4) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(row_ptr + j));\n            _mm_store_si128(reinterpret_cast<__m128i*>(tmp), v);\n            row_sum += tmp[0] + tmp[1] + tmp[2] + tmp[3];\n        }\n\n        for (; j < cols; ++j) {\n            row_sum += row_ptr[j];\n        }\n\n        if (row_sum > 0) {\n            total_operations += (row_sum + capacity - 1) / capacity;\n        }\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    int64_t total = 0;\n    size_t i = 0;\n\n    __m128i mask_even_start = _mm_set_epi16(0, -1, 0, -1, 0, -1, 0, -1);\n    __m128i mask_odd_start = _mm_set_epi16(-1, 0, -1, 0, -1, 0, -1, 0);\n\n    for (; i + 8 <= n; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i mask_pos = (i % 2 == 0) ? mask_even_start : mask_odd_start;\n\n        __m128i ones = _mm_set1_epi16(1);\n        __m128i and_result = _mm_and_si128(chunk, ones);\n        __m128i odd_mask = _mm_cmpeq_epi16(and_result, ones);\n        __m128i combined_mask = _mm_and_si128(odd_mask, mask_pos);\n        __m128i selected = _mm_and_si128(chunk, combined_mask);\n\n        __m128i sign = _mm_srai_epi16(selected, 15);\n        __m128i low_part = _mm_unpacklo_epi16(selected, sign);\n        __m128i high_part = _mm_unpackhi_epi16(selected, sign);\n\n        int32_t low_arr[4];\n        int32_t high_arr[4];\n        _mm_storeu_si128((__m128i*)low_arr, low_part);\n        _mm_storeu_si128((__m128i*)high_arr, high_part);\n\n        for (int j = 0; j < 4; j++) {\n            total += (int64_t)low_arr[j];\n            total += (int64_t)high_arr[j];\n        }\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t total = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    if (size >= 8) {\n        __m128i index_mask = _mm_set_epi16(0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF);\n        for (; i <= size - 8; i += 8) {\n            __m128i data = _mm_loadu_si128((__m128i const*)(lst.data() + i));\n            __m128i ones = _mm_set1_epi16(1);\n            __m128i odd_val = _mm_and_si128(data, ones);\n            __m128i odd_mask = _mm_cmpeq_epi16(odd_val, ones);\n            __m128i mask = _mm_and_si128(index_mask, odd_mask);\n            __m128i selected = _mm_and_si128(data, mask);\n            \n            total += static_cast<int64_t>(static_cast<int16_t>(_mm_extract_epi16(selected, 0));\n            total += static_cast<int64_t>(static_cast<int16_t>(_mm_extract_epi16(selected, 2));\n            total += static_cast<int64_t>(static_cast<int16_t>(_mm_extract_epi16(selected, 4));\n            total += static_cast<int64_t>(static_cast<int16_t>(_mm_extract_epi16(selected, 6));\n        }\n    }\n    \n    for (; i < size; i += 2) {\n        if (lst[i] & 1) {\n            total += lst[i];\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    if (lst.empty()) \n        return 0;\n\n    size_t n = lst.size();\n    int64_t total = 0;\n    __m128i acc = _mm_setzero_si128();\n    const __m128i even_pos_mask = _mm_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n    const __m128i one = _mm_set1_epi16(1);\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i odd_flag = _mm_and_si128(data, one);\n        __m128i condition = _mm_and_si128(even_pos_mask, odd_flag);\n        __m128i selected = _mm_mullo_epi16(data, condition);\n        \n        __m128i low_part = _mm_cvtepi16_epi32(selected);\n        __m128i high_part = _mm_cvtepi16_epi32(_mm_srli_si128(selected, 8));\n        \n        acc = _mm_add_epi32(acc, low_part);\n        acc = _mm_add_epi32(acc, high_part);\n    }\n\n    alignas(16) int32_t temp[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), acc);\n    total = static_cast<int64_t>(temp[0]) + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n    int64_t total = 0;\n    int i = 0;\n    const __m128i index_mask = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    const __m128i ones = _mm_set1_epi16(1);\n    __m128i acc = _mm_setzero_si128();\n\n    for (; i + 8 <= n; i += 8) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i odd_bits = _mm_and_si128(v, ones);\n        __m128i odd_mask = _mm_cmpeq_epi16(odd_bits, ones);\n        __m128i condition = _mm_and_si128(index_mask, odd_mask);\n        __m128i masked = _mm_and_si128(v, condition);\n        __m128i product = _mm_madd_epi16(masked, ones);\n        acc = _mm_add_epi32(acc, product);\n    }\n\n    alignas(16) int32_t temp[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), acc);\n    total = static_cast<int64_t>(temp[0]) + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t total = 0;\n    size_t i = 0;\n    const size_t n = lst.size();\n    const size_t chunk_size = 8;\n\n    if (n == 0) {\n        return 0;\n    }\n\n    __m128i mask_even_start = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m128i mask_odd_start = _mm_set_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n    __m128i one_vec = _mm_set1_epi16(1);\n\n    for (; i + chunk_size <= n; i += chunk_size) {\n        __m128i even_mask = (i % 2 == 0) ? mask_even_start : mask_odd_start;\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i odd_mask = _mm_and_si128(data, one_vec);\n        __m128i both_mask = _mm_and_si128(even_mask, odd_mask);\n        __m128i condition = _mm_cmpeq_epi16(both_mask, one_vec);\n        __m128i selected = _mm_and_si128(data, condition);\n        __m128i products = _mm_madd_epi16(selected, one_vec);\n        int32_t sum_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_arr), products);\n        total += static_cast<int64_t>(sum_arr[0]) + static_cast<int64_t>(sum_arr[1]) + \n                 static_cast<int64_t>(sum_arr[2]) + static_cast<int64_t>(sum_arr[3]);\n    }\n\n    for (; i < n; i++) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int total = 0;\n    int i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n\n    for (; i <= k - 4; i += 4) {\n        __m128i data = _mm_loadu_si128((const __m128i*)&arr[i]);\n        __m128i low_bound = _mm_set1_epi32(-100);\n        __m128i high_bound = _mm_set1_epi32(100);\n        __m128i mask_low = _mm_cmpgt_epi32(data, low_bound);\n        __m128i mask_high = _mm_cmpgt_epi32(high_bound, data);\n        __m128i mask_combined = _mm_and_si128(mask_low, mask_high);\n        __m128i selected = _mm_and_si128(data, mask_combined);\n        sum_vec = _mm_add_epi32(sum_vec, selected);\n    }\n\n    int sum_buffer[4];\n    _mm_storeu_si128((__m128i*)sum_buffer, sum_vec);\n    total = sum_buffer[0] + sum_buffer[1] + sum_buffer[2] + sum_buffer[3];\n\n    for (; i < k; ++i) {\n        int x = arr[i];\n        if (x >= -99 && x <= 99) {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n    if (k >= 4) {\n        __m128i accum = _mm_setzero_si128();\n        for (; i <= k - 4; i += 4) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n            __m128i low_bound = _mm_set1_epi32(-99);\n            __m128i high_bound = _mm_set1_epi32(99);\n            __m128i mask_low = _mm_cmplt_epi32(vec, low_bound);\n            __m128i mask_high = _mm_cmpgt_epi32(vec, high_bound);\n            __m128i mask_combined = _mm_or_si128(mask_low, mask_high);\n            vec = _mm_andnot_si128(mask_combined, vec);\n            accum = _mm_add_epi32(accum, vec);\n        }\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), accum);\n        sum = temp[0] + temp[1] + temp[2] + temp[3];\n    }\n    for (; i < k; i++) {\n        int n = arr[i];\n        if (n >= -99 && n <= 99) {\n            sum += n;\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int total = 0;\n    int i = 0;\n    __m128i v_sum = _mm_setzero_si128();\n    __m128i v_neg_100 = _mm_set1_epi32(-100);\n    __m128i v_100 = _mm_set1_epi32(100);\n\n    if (k >= 4) {\n        for (; i <= k - 4; i += 4) {\n            __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n            __m128i cond1 = _mm_cmpgt_epi32(v, v_neg_100);\n            __m128i cond2 = _mm_cmplt_epi32(v, v_100);\n            __m128i mask = _mm_and_si128(cond1, cond2);\n            __m128i selected = _mm_and_si128(v, mask);\n            v_sum = _mm_add_epi32(v_sum, selected);\n        }\n    }\n\n    alignas(16) int sum_arr[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(sum_arr), v_sum);\n    total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    for (; i < k; ++i) {\n        int x = arr[i];\n        if (x > -100 && x < 100) {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vector = _mm_setzero_si128();\n    int i = 0;\n\n    for (; i + 3 < k; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask1 = _mm_cmpgt_epi32(chunk, _mm_set1_epi32(-100));\n        __m128i mask2 = _mm_cmplt_epi32(chunk, _mm_set1_epi32(100));\n        __m128i mask = _mm_and_si128(mask1, mask2);\n        __m128i selected = _mm_and_si128(chunk, mask);\n        sum_vector = _mm_add_epi32(sum_vector, selected);\n    }\n\n    int total = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vector);\n    total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < k; i++) {\n        int x = arr[i];\n        if (x > -100 && x < 100) {\n            total += x;\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    __m128i v_sum = _mm_setzero_si128();\n    int i = 0;\n\n    for (; i + 3 < k; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i low_bound = _mm_set1_epi32(-100);\n        __m128i high_bound = _mm_set1_epi32(100);\n        __m128i cmp_gt = _mm_cmpgt_epi32(v, low_bound);\n        __m128i cmp_lt = _mm_cmplt_epi32(v, high_bound);\n        __m128i mask = _mm_and_si128(cmp_gt, cmp_lt);\n        __m128i selected = _mm_and_si128(v, mask);\n        v_sum = _mm_add_epi32(v_sum, selected);\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, v_sum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < k; i++) {\n        int x = arr[i];\n        if (x > -100 && x < 100) {\n            sum += x;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <unordered_map>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) {\n        return true;\n    }\n    bool sorted = true;\n    size_t i = 0;\n    if (n >= 5) {\n        for (; i <= n - 5; i += 4) {\n            __m128i v0 = _mm_loadu_si128((__m128i const*)(&lst[i]));\n            __m128i v1 = _mm_loadu_si128((__m128i const*)(&lst[i+1]));\n            __m128i cmp = _mm_cmpgt_epi32(v0, v1);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n        }\n    }\n    if (sorted) {\n        size_t start = (n >= 5) ? i + 4 : 0;\n        for (size_t j = start; j < n - 1; j++) {\n            if (lst[j] > lst[j+1]) {\n                sorted = false;\n                break;\n            }\n        }\n    }\n    std::unordered_map<int, int> freq;\n    bool hasTriplicate = false;\n    for (int x : lst) {\n        freq[x]++;\n        if (freq[x] >= 3) {\n            hasTriplicate = true;\n            break;\n        }\n    }\n    return sorted && !hasTriplicate;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) \n        return true;\n\n    const int* data = lst.data();\n    size_t i = 0;\n\n    for (; i <= n-5; i += 4) {\n        __m128i curr = _mm_loadu_si128((__m128i const*)(data + i));\n        __m128i next = _mm_loadu_si128((__m128i const*)(data + i + 1));\n        __m128i cmp = _mm_cmpgt_epi32(curr, next);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    for (; i < n-1; i++) {\n        if (data[i] > data[i+1])\n            return false;\n    }\n\n    if (n < 3)\n        return true;\n\n    i = 0;\n    for (; i <= n-6; i += 4) {\n        __m128i v0 = _mm_loadu_si128((__m128i const*)(data + i));\n        __m128i v1 = _mm_loadu_si128((__m128i const*)(data + i + 1));\n        __m128i v2 = _mm_loadu_si128((__m128i const*)(data + i + 2));\n        __m128i cmp01 = _mm_cmpeq_epi32(v0, v1);\n        __m128i cmp02 = _mm_cmpeq_epi32(v0, v2);\n        __m128i both = _mm_and_si128(cmp01, cmp02);\n        if (_mm_movemask_epi8(both) != 0) {\n            return false;\n        }\n    }\n\n    for (; i < n-2; i++) {\n        if (data[i] == data[i+1] && data[i] == data[i+2])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    // Check non-decreasing order\n    size_t i = 0;\n    if (n >= 5) {\n        for (; i <= n - 5; i += 4) {\n            __m128i current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n            __m128i next = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i+1]));\n            __m128i cmp = _mm_cmple_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n    }\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i+1]) {\n            return false;\n        }\n    }\n\n    // Check for duplicates (appearing more than twice)\n    if (n < 3) {\n        return true;\n    }\n    i = 0;\n    if (n >= 6) {\n        for (; i <= n - 6; i += 4) {\n            __m128i v0 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i+2]));\n            __m128i eq = _mm_cmpeq_epi32(v0, v2);\n            if (_mm_movemask_epi8(eq) != 0) {\n                return false;\n            }\n        }\n    }\n    for (; i <= n - 3; ++i) {\n        if (lst[i] == lst[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n < 2) \n        return true;\n    if (n == 2)\n        return lst[0] <= lst[1];\n\n    size_t i = 0;\n    for (; i <= n-5; i += 4) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(&lst[i]));\n        __m128i b = _mm_loadu_si128((const __m128i*)(&lst[i+1]));\n        __m128i cmp = _mm_cmpgt_epi32(a, b);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n    for (; i < n-1; ++i) {\n        if (lst[i] > lst[i+1])\n            return false;\n    }\n\n    i = 0;\n    for (; i <= n-6; i += 4) {\n        __m128i a = _mm_loadu_si128((const __m128i*)(&lst[i]));\n        __m128i b = _mm_loadu_si128((const __m128i*)(&lst[i+1]));\n        __m128i c = _mm_loadu_si128((const __m128i*)(&lst[i+2]));\n        __m128i cmp1 = _mm_cmpeq_epi32(a, b);\n        __m128i cmp2 = _mm_cmpeq_epi32(b, c);\n        __m128i res = _mm_and_si128(cmp1, cmp2);\n        if (_mm_movemask_epi8(res) != 0) {\n            return false;\n        }\n    }\n    for (; i < n-2; ++i) {\n        if (lst[i] == lst[i+1] && lst[i] == lst[i+2])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n == 0) {\n        return true;\n    }\n    if (n == 1) {\n        return true;\n    }\n\n    size_t i = 0;\n    if (n >= 2) {\n        for (; i <= n - 5; i += 4) {\n            __m128i current = _mm_loadu_si128((__m128i const*)(&lst[i]));\n            __m128i next = _mm_loadu_si128((__m128i const*)(&lst[i+1]));\n            __m128i cmp = _mm_cmpgt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n        for (; i < n - 1; i++) {\n            if (lst[i] > lst[i+1]) {\n                return false;\n            }\n        }\n    }\n\n    if (n >= 3) {\n        i = 0;\n        for (; i <= n - 6; i += 4) {\n            __m128i v0 = _mm_loadu_si128((__m128i const*)(&lst[i]));\n            __m128i v1 = _mm_loadu_si128((__m128i const*)(&lst[i+1]));\n            __m128i v2 = _mm_loadu_si128((__m128i const*)(&lst[i+2]));\n            __m128i eq1 = _mm_cmpeq_epi32(v0, v1);\n            __m128i eq2 = _mm_cmpeq_epi32(v0, v2);\n            __m128i res = _mm_and_si128(eq1, eq2);\n            if (_mm_movemask_epi8(res) != 0) {\n                return false;\n            }\n        }\n        for (; i < n - 2; i++) {\n            if (lst[i] == lst[i+1] && lst[i] == lst[i+2]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n    \n    bool found_zero = false;\n    __m128i sum_abs = _mm_setzero_si128();\n    __m128i count_neg = _mm_setzero_si128();\n    size_t n = arr.size();\n    size_t i = 0;\n    \n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i zero = _mm_setzero_si128();\n        __m128i is_zero = _mm_cmpeq_epi32(v, zero);\n        if (_mm_movemask_epi8(is_zero) != 0) {\n            found_zero = true;\n            break;\n        }\n        \n        __m128i mask = _mm_cmplt_epi32(v, zero);\n        __m128i v_abs = _mm_sub_epi32(_mm_xor_si128(v, mask), mask);\n        __m128i neg_flags = _mm_srli_epi32(mask, 31);\n        \n        sum_abs = _mm_add_epi32(sum_abs, v_abs);\n        count_neg = _mm_add_epi32(count_neg, neg_flags);\n    }\n    \n    if (found_zero) {\n        return 0;\n    }\n    \n    int total_abs_tail = 0;\n    int total_neg_tail = 0;\n    for (; i < n; ++i) {\n        int x = arr[i];\n        if (x == 0) {\n            found_zero = true;\n            break;\n        }\n        if (x < 0) {\n            total_abs_tail += -x;\n            total_neg_tail++;\n        } else {\n            total_abs_tail += x;\n        }\n    }\n    \n    if (found_zero) {\n        return 0;\n    }\n    \n    __m128i shuf_abs = _mm_shuffle_epi32(sum_abs, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sum_abs1 = _mm_add_epi32(sum_abs, shuf_abs);\n    __m128i shuf_abs2 = _mm_shuffle_epi32(sum_abs1, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128i total_abs_vec = _mm_add_epi32(sum_abs1, shuf_abs2);\n    int total_abs = _mm_cvtsi128_si32(total_abs_vec) + total_abs_tail;\n    \n    __m128i shuf_neg = _mm_shuffle_epi32(count_neg, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i count_neg1 = _mm_add_epi32(count_neg, shuf_neg);\n    __m128i shuf_neg2 = _mm_shuffle_epi32(count_neg1, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128i total_neg_vec = _mm_add_epi32(count_neg1, shuf_neg2);\n    int total_neg = _mm_cvtsi128_si32(total_neg_vec) + total_neg_tail;\n    \n    int sign_product = (total_neg & 1) ? -1 : 1;\n    return sign_product * total_abs;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    const int* data = arr.data();\n    __m128i zero_v = _mm_setzero_si128();\n    __m128i abs_sum_vec = zero_v;\n    __m128i neg_count_vec = zero_v;\n    size_t i = 0;\n    int found_zero = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(data + i));\n        __m128i cmp_zero = _mm_cmpeq_epi32(v, zero_v);\n        int mask = _mm_movemask_epi8(cmp_zero);\n        if (mask != 0) {\n            found_zero = 1;\n            break;\n        }\n\n        __m128i sign_mask = _mm_srai_epi32(v, 31);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, sign_mask), sign_mask);\n        abs_sum_vec = _mm_add_epi32(abs_sum_vec, abs_v);\n\n        __m128i is_neg = _mm_cmplt_epi32(v, zero_v);\n        __m128i neg_counts = _mm_srli_epi32(is_neg, 31);\n        neg_count_vec = _mm_add_epi32(neg_count_vec, neg_counts);\n    }\n\n    if (found_zero) {\n        return 0;\n    }\n\n    int scalar_abs_sum = 0;\n    int scalar_neg_count = 0;\n    for (; i < n; i++) {\n        int x = data[i];\n        if (x == 0) {\n            return 0;\n        }\n        scalar_abs_sum += (x < 0) ? -x : x;\n        if (x < 0) {\n            scalar_neg_count++;\n        }\n    }\n\n    alignas(16) int abs_sum_arr[4];\n    alignas(16) int neg_count_arr[4];\n    _mm_store_si128((__m128i*)abs_sum_arr, abs_sum_vec);\n    _mm_store_si128((__m128i*)neg_count_arr, neg_count_vec);\n\n    int total_abs_sum = abs_sum_arr[0] + abs_sum_arr[1] + abs_sum_arr[2] + abs_sum_arr[3] + scalar_abs_sum;\n    int total_neg_count = neg_count_arr[0] + neg_count_arr[1] + neg_count_arr[2] + neg_count_arr[3] + scalar_neg_count;\n    int sign_product = (total_neg_count % 2 == 0) ? 1 : -1;\n\n    return sign_product * total_abs_sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n    int global_sign = 1;\n    int total_neg_count = 0;\n    int abs_sum = 0;\n    int i = 0;\n    const int n = arr.size();\n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i zero = _mm_setzero_si128();\n        __m128i is_zero = _mm_cmpeq_epi32(v, zero);\n        if (_mm_movemask_epi8(is_zero) != 0) {\n            global_sign = 0;\n        }\n        if (global_sign != 0) {\n            __m128i mask_neg = _mm_cmplt_epi32(v, zero);\n            __m128i neg_counts = _mm_srli_epi32(mask_neg, 31);\n            __m128i sum1 = _mm_add_epi32(neg_counts, _mm_srli_si128(neg_counts, 8));\n            __m128i sum2 = _mm_add_epi32(sum1, _mm_srli_si128(sum1, 4));\n            total_neg_count += _mm_cvtsi128_si32(sum2);\n        }\n        __m128i sign_bits = _mm_srai_epi32(v, 31);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, sign_bits), sign_bits);\n        __m128i sum_abs1 = _mm_add_epi32(abs_v, _mm_srli_si128(abs_v, 8));\n        __m128i sum_abs2 = _mm_add_epi32(sum_abs1, _mm_srli_si128(sum_abs1, 4));\n        abs_sum += _mm_cvtsi128_si32(sum_abs2);\n    }\n    for (; i < n; i++) {\n        int x = arr[i];\n        if (x == 0) {\n            global_sign = 0;\n        } else if (global_sign != 0) {\n            if (x < 0) {\n                total_neg_count++;\n            }\n        }\n        abs_sum += (x < 0) ? -x : x;\n    }\n    if (global_sign != 0) {\n        global_sign = (total_neg_count % 2 == 0) ? 1 : -1;\n    }\n    return global_sign * abs_sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    const int* data = arr.data();\n    size_t n = arr.size();\n\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 3 < n; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(data + i));\n        __m128i mask = _mm_cmpeq_epi32(v, zero);\n        int m = _mm_movemask_epi8(mask);\n        if (m != 0) {\n            return 0;\n        }\n    }\n    for (; i < n; i++) {\n        if (data[i] == 0) {\n            return 0;\n        }\n    }\n\n    __m128i v_sum_abs = _mm_setzero_si128();\n    __m128i v_parity = _mm_setzero_si128();\n    i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(data + i));\n        __m128i v_sign = _mm_srai_epi32(v, 31);\n        __m128i v_abs = _mm_sub_epi32(_mm_xor_si128(v, v_sign), v_sign);\n        v_sum_abs = _mm_add_epi32(v_sum_abs, v_abs);\n\n        __m128i v_sign_flag = _mm_and_si128(v_sign, _mm_set1_epi32(1));\n        v_parity = _mm_xor_si128(v_parity, v_sign_flag);\n    }\n\n    alignas(16) int sum_abs_arr[4];\n    alignas(16) int parity_arr[4];\n    _mm_store_si128((__m128i*)sum_abs_arr, v_sum_abs);\n    _mm_store_si128((__m128i*)parity_arr, v_parity);\n\n    int sum_abs = sum_abs_arr[0] + sum_abs_arr[1] + sum_abs_arr[2] + sum_abs_arr[3];\n    int total_parity = parity_arr[0] ^ parity_arr[1] ^ parity_arr[2] ^ parity_arr[3];\n\n    for (; i < n; i++) {\n        int val = data[i];\n        int abs_val = (val < 0) ? -val : val;\n        sum_abs += abs_val;\n        total_parity ^= (val < 0) ? 1 : 0;\n    }\n\n    int sign_product = total_parity ? -1 : 1;\n    return sign_product * sum_abs;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n    int n = arr.size();\n    int i = 0;\n    bool has_zero = false;\n    __m128i abs_sum_vec = _mm_setzero_si128();\n    int global_parity = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(arr.data() + i));\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(v, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_zero = true;\n            break;\n        }\n        __m128i mask = _mm_cmpgt_epi32(zero, v);\n        __m128i abs_val = _mm_add_epi32(\n            _mm_xor_si128(v, mask),\n            _mm_srli_epi32(mask, 31)\n        );\n        abs_sum_vec = _mm_add_epi32(abs_sum_vec, abs_val);\n        __m128i sign_bits = _mm_and_si128(mask, _mm_set1_epi32(1));\n        alignas(16) int sign_arr[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(sign_arr), sign_bits);\n        int chunk_parity = sign_arr[0] ^ sign_arr[1] ^ sign_arr[2] ^ sign_arr[3];\n        global_parity ^= chunk_parity;\n    }\n\n    if (has_zero) {\n        return 0;\n    }\n\n    __m128i shuf1 = _mm_shuffle_epi32(abs_sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sum1 = _mm_add_epi32(abs_sum_vec, shuf1);\n    __m128i shuf2 = _mm_shuffle_epi32(sum1, _MM_SHUFFLE(1, 0, 3, 2));\n    __m128i sum2 = _mm_add_epi32(sum1, shuf2);\n    int abs_sum = _mm_cvtsi128_si32(sum2);\n\n    for (; i < n; i++) {\n        int x = arr[i];\n        if (x == 0) {\n            return 0;\n        }\n        if (x < 0) {\n            global_parity ^= 1;\n            abs_sum += -x;\n        } else {\n            abs_sum += x;\n        }\n    }\n\n    int sign_factor = (global_parity & 1) ? -1 : 1;\n    return sign_factor * abs_sum;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\nusing namespace std;\n\nvector<int> minPath_simd(const vector<vector<int> > & grid, int k) {\n    int n = grid.size();\n    int i0 = -1, j0 = -1;\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            if (grid[i][j] == 1) {\n                i0 = i;\n                j0 = j;\n                break;\n            }\n        }\n        if (i0 != -1) break;\n    }\n\n    vector<int> path;\n    path.push_back(1);\n    if (k == 1) {\n        return path;\n    }\n\n    int i = i0, j = j0;\n    int dirs[4][2] = {{-1,0}, {1,0}, {0,-1}, {0,1}};\n\n    for (int step = 1; step < k; step++) {\n        int values[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n        for (int d = 0; d < 4; d++) {\n            int ni = i + dirs[d][0];\n            int nj = j + dirs[d][1];\n            if (ni >= 0 && ni < n && nj >= 0 && nj < n) {\n                values[d] = grid[ni][nj];\n            }\n        }\n\n        __m128i v0 = _mm_loadu_si128((__m128i*)values);\n        __m128i v1 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(2,3,0,1));\n        __m128i m0 = _mm_min_epi32(v0, v1);\n        __m128i m1 = _mm_shuffle_epi32(m0, _MM_SHUFFLE(2,3,0,1));\n        __m128i m2 = _mm_min_epi32(m0, m1);\n        int min_val = _mm_cvtsi128_si32(m2);\n\n        __m128i minv = _mm_set1_epi32(min_val);\n        __m128i mask = _mm_cmpeq_epi32(v0, minv);\n        int mask_low = _mm_movemask_epi8(mask);\n\n        int idx = -1;\n        if ((mask_low & 0x000F) == 0x000F) {\n            idx = 0;\n        } else if ((mask_low & 0x00F0) == 0x00F0) {\n            idx = 1;\n        } else if ((mask_low & 0x0F00) == 0x0F00) {\n            idx = 2;\n        } else if ((mask_low & 0xF000) == 0xF000) {\n            idx = 3;\n        } else {\n            for (int d = 0; d < 4; d++) {\n                if (values[d] == min_val) {\n                    idx = d;\n                    break;\n                }\n            }\n        }\n\n        i += dirs[idx][0];\n        j += dirs[idx][1];\n        path.push_back(min_val);\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int n = grid.size();\n    int min_val = INT_MAX;\n    int start_i = -1, start_j = -1;\n\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            if (grid[i][j] < min_val) {\n                min_val = grid[i][j];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    std::vector<int> path;\n    path.push_back(min_val);\n    if (k == 1) {\n        return path;\n    }\n\n    int cur_i = start_i;\n    int cur_j = start_j;\n\n    for (int step = 1; step < k; step++) {\n        int i_arr[4] = { cur_i - 1, cur_i + 1, cur_i,     cur_i };\n        int j_arr[4] = { cur_j,     cur_j,     cur_j - 1, cur_j + 1 };\n\n        __m128i i_vec = _mm_loadu_si128((__m128i*)i_arr);\n        __m128i j_vec = _mm_loadu_si128((__m128i*)j_arr);\n\n        __m128i neg_one = _mm_set1_epi32(-1);\n        __m128i n_vector = _mm_set1_epi32(n);\n        __m128i mask_i_ge0 = _mm_cmpgt_epi32(i_vec, neg_one);\n        __m128i mask_i_lt_n = _mm_cmplt_epi32(i_vec, n_vector);\n        __m128i mask_j_ge0 = _mm_cmpgt_epi32(j_vec, neg_one);\n        __m128i mask_j_lt_n = _mm_cmplt_epi32(j_vec, n_vector);\n\n        __m128i mask = _mm_and_si128(mask_i_ge0, mask_i_lt_n);\n        mask = _mm_and_si128(mask, mask_j_ge0);\n        mask = _mm_and_si128(mask, mask_j_lt_n);\n\n        int mask_arr[4];\n        _mm_storeu_si128((__m128i*)mask_arr, mask);\n\n        int candidate_vals[4];\n        for (int d = 0; d < 4; d++) {\n            if (mask_arr[d]) {\n                candidate_vals[d] = grid[i_arr[d]][j_arr[d]];\n            } else {\n                candidate_vals[d] = INT_MAX;\n            }\n        }\n\n        __m128i v = _mm_loadu_si128((__m128i*)candidate_vals);\n        __m128i v_swapped1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i min1 = _mm_min_epi32(v, v_swapped1);\n        __m128i min1_swapped = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i min2 = _mm_min_epi32(min1, min1_swapped);\n        int min_val_neighbor = _mm_cvtsi128_si32(min2);\n\n        int next_d = -1;\n        for (int d = 0; d < 4; d++) {\n            if (mask_arr[d] && candidate_vals[d] == min_val_neighbor) {\n                next_d = d;\n                break;\n            }\n        }\n\n        cur_i = i_arr[next_d];\n        cur_j = j_arr[next_d];\n        path.push_back(min_val_neighbor);\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "To solve this problem, we need to find the lexicographically smallest path of length `k` in a grid where each cell contains a unique integer from 1 to N*N. The path can start at any cell, and each subsequent step must move to a neighboring cell (up, down, left, or right). The solution involves dynamic programming combined with SIMD (SSE/SSE2) intrinsics to efficiently compare paths lexicographically.\n\n### Approach\n1. **Dynamic Programming (DP) Setup**: We use a 3D DP array `dp[len][i][j]` to store the lexicographically smallest path of length `len` starting at cell `(i, j)`. \n2. **Initialization**: For paths of length 1, each path is simply the value of the starting cell.\n3. **Building Paths**: For paths of length `len` from 2 to `k`, for each cell `(i, j)`, we consider all valid neighboring cells. The path for `(i, j)` is constructed by appending the lexicographically smallest path from the neighbors to the current cell's value.\n4. **Lexicographical Comparison**: To efficiently compare paths, we use SSE/SSE2 intrinsics. This allows us to compare four integers at a time, significantly speeding up the comparison process.\n5. **Result Extraction**: After building all paths of length `k`, we find the lexicographically smallest path among all starting cells.\n\n### Solution Code\n```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstatic bool isLess(const std::vector<int>& a, const std::vector<int>& b) {\n    int n = a.size();\n    int i = 0;\n    for (; i <= n - 4; i += 4) {\n        __m128i va = _mm_loadu_si128((__m128i const*)&a[i]);\n        __m128i vb = _mm_loadu_si128((__m128i const*)&b[i]);\n        __m128i vcmp = _mm_cmpeq_epi32(va, vb);\n        unsigned int mask = _mm_movemask_epi8(vcmp);\n        if (mask != 0xFFFF) {\n            for (int j = 0; j < 4; j++) {\n                if (a[i+j] != b[i+j]) {\n                    return a[i+j] < b[i+j];\n                }\n            }\n        }\n    }\n    for (; i < n; i++) {\n        if (a[i] != b[i]) {\n            return a[i] < b[i];\n        }\n    }\n    return false;\n}\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<std::vector<std::vector<std::vector<int>>>> dp(k+1, \n        std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(N)));\n    \n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            dp[1][i][j] = { grid[i][j] };\n        }\n    }\n    \n    int dx[] = {0, 0, 1, -1};\n    int dy[] = {1, -1, 0, 0};\n    \n    for (int len = 2; len <= k; len++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                std::vector<const std::vector<int>*> candidates;\n                for (int d = 0; d < 4; d++) {\n                    int ni = i + dx[d];\n                    int nj = j + dy[d];\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        candidates.push_back(&dp[len-1][ni][nj]);\n                    }\n                }\n                \n                if (candidates.empty()) {\n                    dp[len][i][j] = std::vector<int>(len, 1000000000);\n                } else {\n                    const std::vector<int>* best = candidates[0];\n                    for (size_t idx = 1; idx < candidates.size(); idx++) {\n                        if (isLess(*candidates[idx], *best)) {\n                            best = candidates[idx];\n                        }\n                    }\n                    dp[len][i][j] = { grid[i][j] };\n                    dp[len][i][j].insert(dp[len][i][j].end(), best->begin(), best->end());\n                }\n            }\n        }\n    }\n    \n    std::vector<int> ans = dp[k][0][0];\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (isLess(dp[k][i][j], ans)) {\n                ans = dp[k][i][j];\n            }\n        }\n    }\n    \n    return ans;\n}\n```\n\n### Explanation\n1. **Initialization**: The DP array for paths of length 1 is initialized with each cell's value.\n2. **Path Construction**: For each path length from 2 to `k`, and for each cell, the algorithm checks all neighboring cells. The path starting at the current cell is formed by appending the lexicographically smallest path from the neighbors.\n3. **Lexicographical Comparison**: The `isLess` function uses SSE intrinsics to compare four integers at a time, making the comparison process efficient. If any of the four integers differ, it breaks down to scalar comparison for the exact position.\n4. **Result Extraction**: After constructing all paths of length `k`, the algorithm scans all starting cells to find the lexicographically smallest path, which is then returned as the result.\n\nThis approach efficiently leverages dynamic programming to build paths step-by-step and uses SIMD instructions to optimize the lexicographical comparisons, ensuring optimal performance even for larger grids."}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int n = grid.size();\n    int r0 = -1, c0 = -1;\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            if (grid[i][j] == 1) {\n                r0 = i;\n                c0 = j;\n                break;\n            }\n        }\n        if (r0 != -1) break;\n    }\n\n    std::vector<int> path;\n    path.push_back(1);\n    if (k == 1) {\n        return path;\n    }\n\n    int r = r0, c = c0;\n\n    for (int step = 1; step < k; step++) {\n        int next_r_arr[4] = { r - 1, r + 1, r, r };\n        int next_c_arr[4] = { c, c, c - 1, c + 1 };\n        int valid_arr[4] = {\n            (r - 1 >= 0) ? 1 : 0,\n            (r + 1 < n) ? 1 : 0,\n            (c - 1 >= 0) ? 1 : 0,\n            (c + 1 < n) ? 1 : 0\n        };\n        int values_arr[4] = {\n            valid_arr[0] ? grid[r - 1][c] : INT_MAX,\n            valid_arr[1] ? grid[r + 1][c] : INT_MAX,\n            valid_arr[2] ? grid[r][c - 1] : INT_MAX,\n            valid_arr[3] ? grid[r][c + 1] : INT_MAX\n        };\n\n        __m128i v = _mm_loadu_si128((__m128i*)values_arr);\n        __m128i v_perm1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i min1 = _mm_min_epi32(v, v_perm1);\n        __m128i min1_perm = _mm_shuffle_epi32(min1, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i min2 = _mm_min_epi32(min1, min1_perm);\n        int min_val = _mm_extract_epi32(min2, 0);\n\n        __m128i min_bc = _mm_set1_epi32(min_val);\n        __m128i mask = _mm_cmpeq_epi32(v, min_bc);\n        int bits = _mm_movemask_epi8(mask);\n\n        int idx = 0;\n        if (bits & 0x000F) {\n            idx = 0;\n        } else if (bits & 0x00F0) {\n            idx = 1;\n        } else if (bits & 0x0F00) {\n            idx = 2;\n        } else if (bits & 0xF000) {\n            idx = 3;\n        } else {\n            idx = 0;\n        }\n\n        r = next_r_arr[idx];\n        c = next_c_arr[idx];\n        path.push_back(min_val);\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\n__m128i my_min_epi32(__m128i a, __m128i b) {\n    __m128i cmp = _mm_cmplt_epi32(a, b);\n    return _mm_or_si128(_mm_and_si128(cmp, a), _mm_andnot_si128(cmp, b));\n}\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    int min_val = INT_MAX;\n    int start_i = -1, start_j = -1;\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (grid[i][j] < min_val) {\n                min_val = grid[i][j];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    std::vector<int> path;\n    path.push_back(min_val);\n    if (k == 1) {\n        return path;\n    }\n\n    int cur_i = start_i, cur_j = start_j;\n    int dx[4] = {-1, 1, 0, 0};\n    int dy[4] = {0, 0, -1, 1};\n\n    for (int step = 1; step < k; step++) {\n        int neighbors[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n        int next_i[4], next_j[4];\n\n        for (int d = 0; d < 4; d++) {\n            int ni = cur_i + dx[d];\n            int nj = cur_j + dy[d];\n            next_i[d] = ni;\n            next_j[d] = nj;\n            if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                neighbors[d] = grid[ni][nj];\n            }\n        }\n\n        __m128i v = _mm_loadu_si128((__m128i*)neighbors);\n        __m128i v_perm1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i min1 = my_min_epi32(v, v_perm1);\n        __m128i min1_perm2 = _mm_shuffle_epi32(min1, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i min2 = my_min_epi32(min1, min1_perm2);\n        int global_min = _mm_cvtsi128_si32(min2);\n\n        __m128i min_vec = _mm_set1_epi32(global_min);\n        __m128i cmp = _mm_cmpeq_epi32(v, min_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        int index = 0;\n        if (mask & 0x000F) {\n            index = 0;\n        } else if (mask & 0x00F0) {\n            index = 1;\n        } else if (mask & 0x0F00) {\n            index = 2;\n        } else if (mask & 0xF000) {\n            index = 3;\n        }\n\n        cur_i = next_i[index];\n        cur_j = next_j[index];\n        path.push_back(global_min);\n    }\n\n    return path;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t arr[32];\n    for (int i = 0; i < 32; i++) {\n        arr[i] = 1;\n    }\n    int index = 0;\n    uint64_t num = n;\n    while (num) {\n        arr[index] = num % 10;\n        num /= 10;\n        index++;\n    }\n    __m128i ones_v = _mm_set1_epi8(1);\n    __m128i zero_v = _mm_setzero_si128();\n    __m128i v1 = _mm_loadu_si128((const __m128i*)arr);\n    __m128i mask_bits1 = _mm_and_si128(v1, ones_v);\n    __m128i even_mask1 = _mm_cmpeq_epi8(mask_bits1, zero_v);\n    __m128i odd_mask1 = _mm_cmpeq_epi8(mask_bits1, ones_v);\n    v1 = _mm_or_si128(_mm_andnot_si128(even_mask1, v1), _mm_and_si128(even_mask1, ones_v));\n    _mm_storeu_si128((__m128i*)arr, v1);\n    __m128i v2 = _mm_loadu_si128((const __m128i*)(arr+16));\n    __m128i mask_bits2 = _mm_and_si128(v2, ones_v);\n    __m128i even_mask2 = _mm_cmpeq_epi8(mask_bits2, zero_v);\n    __m128i odd_mask2 = _mm_cmpeq_epi8(mask_bits2, ones_v);\n    v2 = _mm_or_si128(_mm_andnot_si128(even_mask2, v2), _mm_and_si128(even_mask2, ones_v));\n    _mm_storeu_si128((__m128i*)(arr+16), v2);\n    __m128i any_odd = _mm_or_si128(odd_mask1, odd_mask2);\n    if (!_mm_movemask_epi8(any_odd)) {\n        return 0;\n    }\n    uint64_t product = 1;\n    for (int i = 0; i < 20; i++) {\n        product *= arr[i];\n    }\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t digits[20] = {0};\n    int count = 0;\n    do {\n        digits[count++] = n % 10;\n        n /= 10;\n    } while (n);\n\n    __m128i ones_byte = _mm_set1_epi8(1);\n    __m128i zero_byte = _mm_setzero_si128();\n    uint64_t product = 1;\n    int any_odd = 0;\n\n    __m128i d1 = _mm_loadu_si128((__m128i*)digits);\n    __m128i mask1 = _mm_and_si128(d1, ones_byte);\n    __m128i is_even1 = _mm_cmpeq_epi8(mask1, zero_byte);\n    __m128i factors1 = _mm_or_si128(_mm_and_si128(is_even1, ones_byte), _mm_andnot_si128(is_even1, d1));\n    any_odd |= _mm_movemask_epi8(_mm_cmpgt_epi8(mask1, zero_byte));\n\n    uint8_t farr1[16];\n    _mm_storeu_si128((__m128i*)farr1, factors1);\n    for (int i = 0; i < 16; i++) {\n        product *= farr1[i];\n    }\n\n    uint8_t temp[16] = {0};\n    int remaining = count - 16;\n    if (remaining > 0) {\n        for (int i = 0; i < remaining; i++) {\n            temp[i] = digits[16 + i];\n        }\n    }\n    __m128i d2 = _mm_loadu_si128((__m128i*)temp);\n    __m128i mask2 = _mm_and_si128(d2, ones_byte);\n    __m128i is_even2 = _mm_cmpeq_epi8(mask2, zero_byte);\n    __m128i factors2 = _mm_or_si128(_mm_and_si128(is_even2, ones_byte), _mm_andnot_si128(is_even2, d2));\n    any_odd |= _mm_movemask_epi8(_mm_cmpgt_epi8(mask2, zero_byte));\n\n    uint8_t farr2[16];\n    _mm_storeu_si128((__m128i*)farr2, factors2);\n    for (int i = 0; i < 16; i++) {\n        product *= farr2[i];\n    }\n\n    return any_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    uint8_t digits[20] = {0};\n    int count = 0;\n    uint64_t temp = n;\n    while (temp) {\n        digits[count++] = temp % 10;\n        temp /= 10;\n    }\n\n    uint8_t arr1[16] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1};\n    uint8_t arr2[16] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1};\n    uint8_t mask_arr1[16] = {0};\n    uint8_t mask_arr2[16] = {0};\n\n    for (int i = 0; i < 16; i++) {\n        if (i < count) {\n            arr1[i] = digits[i];\n            mask_arr1[i] = digits[i] & 1;\n        }\n    }\n    for (int i = 0; i < 16; i++) {\n        if (i+16 < count) {\n            arr2[i] = digits[i+16];\n            mask_arr2[i] = digits[i+16] & 1;\n        }\n    }\n\n    __m128i v_mask1 = _mm_loadu_si128((__m128i*)mask_arr1);\n    __m128i v_mask2 = _mm_loadu_si128((__m128i*)mask_arr2);\n    __m128i v_mask_or = _mm_or_si128(v_mask1, v_mask2);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_cmp = _mm_cmpeq_epi8(v_mask_or, v_zero);\n    int mask = _mm_movemask_epi8(v_cmp);\n    if (mask == 0xFFFF) {\n        return 0;\n    }\n\n    __m128i v_ones = _mm_set1_epi8(1);\n    __m128i v1 = _mm_loadu_si128((__m128i*)arr1);\n    __m128i v2 = _mm_loadu_si128((__m128i*)arr2);\n\n    __m128i v1_mask = _mm_and_si128(v1, v_ones);\n    __m128i v1_replaced = _mm_or_si128(\n        _mm_and_si128(v1_mask, v1),\n        _mm_andnot_si128(v1_mask, v_ones)\n    );\n    __m128i v2_mask = _mm_and_si128(v2, v_ones);\n    __m128i v2_replaced = _mm_or_si128(\n        _mm_and_si128(v2_mask, v2),\n        _mm_andnot_si128(v2_mask, v_ones)\n    );\n\n    uint8_t temp1[16], temp2[16];\n    _mm_storeu_si128((__m128i*)temp1, v1_replaced);\n    _mm_storeu_si128((__m128i*)temp2, v2_replaced);\n\n    uint64_t prod = 1;\n    for (int i = 0; i < 16; i++) {\n        prod *= temp1[i];\n    }\n    for (int i = 0; i < 16; i++) {\n        prod *= temp2[i];\n    }\n    return prod;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    uint8_t d[20] = {0};\n    int len = 0;\n    uint64_t num = n;\n    while (num) {\n        d[len++] = num % 10;\n        num /= 10;\n    }\n\n    if (len == 0) {\n        return 0;\n    }\n\n    int any_odd = 0;\n    for (int i = 0; i < len; i++) {\n        if (d[i] % 2 == 1) {\n            any_odd = 1;\n            break;\n        }\n    }\n    if (!any_odd) {\n        return 0;\n    }\n\n    __m128i ones = _mm_set1_epi8(1);\n    uint64_t product = 1;\n\n    for (int offset = 0; offset < len; offset += 16) {\n        int remaining = len - offset;\n        int chunk_size = (remaining < 16) ? remaining : 16;\n        uint8_t temp[16] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1};\n        for (int i = 0; i < chunk_size; i++) {\n            temp[i] = d[offset + i];\n        }\n\n        __m128i v = _mm_loadu_si128((__m128i*)temp);\n        __m128i v_odd_mask = _mm_and_si128(v, ones);\n        __m128i factors = _mm_or_si128(_mm_and_si128(v_odd_mask, v), \n                                   _mm_andnot_si128(v_odd_mask, ones));\n\n        uint8_t fact_arr[16];\n        _mm_storeu_si128((__m128i*)fact_arr, factors);\n\n        for (int i = 0; i < chunk_size; i++) {\n            product *= fact_arr[i];\n        }\n    }\n\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    uint8_t digits[32] = {0};\n    int len = 0;\n    uint64_t m = n;\n    while (m) {\n        digits[len++] = m % 10;\n        m /= 10;\n    }\n    for (int i = len; i < 32; i++) {\n        digits[i] = 0;\n    }\n\n    __m128i v1 = _mm_loadu_si128((__m128i*)digits);\n    __m128i v2 = _mm_loadu_si128((__m128i*)(digits + 16));\n    __m128i ones = _mm_set1_epi8(1);\n    \n    __m128i v1_mask = _mm_cmpeq_epi8(_mm_and_si128(v1, ones), ones);\n    __m128i v2_mask = _mm_cmpeq_epi8(_mm_and_si128(v2, ones), ones);\n    \n    __m128i v1_replaced = _mm_or_si128(_mm_and_si128(v1_mask, v1), \n             _mm_andnot_si128(v1_mask, ones));\n    __m128i v2_replaced = _mm_or_si128(_mm_and_si128(v2_mask, v2), \n             _mm_andnot_si128(v2_mask, ones));\n    \n    __m128i any_odd = _mm_or_si128(v1_mask, v2_mask);\n    if (_mm_movemask_epi8(any_odd) == 0) {\n        return 0;\n    }\n\n    uint8_t replaced[32];\n    _mm_storeu_si128((__m128i*)replaced, v1_replaced);\n    _mm_storeu_si128((__m128i*)(replaced + 16), v2_replaced);\n\n    uint64_t product = 1;\n    for (int i = 0; i < 32; i++) {\n        product *= replaced[i];\n    }\n    return product;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t total = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    if (size >= 4) {\n        __m128i zero = _mm_setzero_si128();\n        __m128 one = _mm_set1_ps(1.0f);\n        for (; i <= size - 4; i += 4) {\n            __m128 x = _mm_loadu_ps(&lst[i]);\n            __m128i trunci = _mm_cvttps_epi32(x);\n            __m128 t = _mm_cvtepi32_ps(trunci);\n            __m128 mask = _mm_cmpgt_ps(x, t);\n            __m128i adjust = _mm_srli_epi32(_mm_castps_si128(mask), 31);\n            __m128i ints = _mm_add_epi32(trunci, adjust);\n\n            __m128i low = _mm_unpacklo_epi32(ints, zero);\n            __m128i high = _mm_unpackhi_epi32(ints, zero);\n            __m128i squares_low = _mm_mul_epu32(low, low);\n            __m128i squares_high = _mm_mul_epu32(high, high);\n            __m128i squares = _mm_add_epi64(squares_low, squares_high);\n\n            __m128i shifted = _mm_srli_si128(squares, 8);\n            __m128i sum_vec = _mm_add_epi64(squares, shifted);\n            uint64_t chunk_sum = _mm_cvtsi128_si64(sum_vec);\n            total += chunk_sum;\n        }\n    }\n\n    for (; i < size; i++) {\n        float val = lst[i];\n        int32_t n = (int32_t)ceil(val);\n        total += (uint64_t)n * n;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) \n        return 0;\n\n    __m128d acc_low = _mm_setzero_pd();\n    __m128d acc_high = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128 rounded = _mm_ceil_ps(data);\n        __m128d low = _mm_cvtps_pd(rounded);\n        __m128 rounded_high = _mm_movehl_ps(rounded, rounded);\n        __m128d high = _mm_cvtps_pd(rounded_high);\n\n        acc_low = _mm_add_pd(acc_low, _mm_mul_pd(low, low));\n        acc_high = _mm_add_pd(acc_high, _mm_mul_pd(high, high));\n    }\n\n    acc_low = _mm_add_pd(acc_low, acc_high);\n    __m128d high_part = _mm_unpackhi_pd(acc_low, acc_low);\n    acc_low = _mm_add_sd(acc_low, high_part);\n    double total = _mm_cvtsd_f64(acc_low);\n\n    for (; i < n; i++) {\n        __m128 data = _mm_load_ss(&lst[i]);\n        __m128 rounded = _mm_ceil_ps(data);\n        float r;\n        _mm_store_ss(&r, rounded);\n        total += static_cast<double>(r) * static_cast<double>(r);\n    }\n\n    return static_cast<uint64_t>(total);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t total = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n    \n    if (size >= 4) {\n        for (; i <= size - 4; i += 4) {\n            __m128 x = _mm_loadu_ps(&lst[i]);\n            __m128 neg_x = _mm_sub_ps(_mm_setzero_ps(), x);\n            __m128i t_int = _mm_cvttps_epi32(neg_x);\n            __m128 t = _mm_cvtepi32_ps(t_int);\n            __m128 mask = _mm_cmplt_ps(neg_x, t);\n            __m128 adjust = _mm_and_ps(mask, _mm_set1_ps(1.0f));\n            __m128 floor_negx = _mm_sub_ps(t, adjust);\n            __m128 ceil_x = _mm_sub_ps(_mm_setzero_ps(), floor_negx);\n            \n            __m128d lo = _mm_cvtps_pd(ceil_x);\n            __m128 hi_ps = _mm_movehl_ps(ceil_x, ceil_x);\n            __m128d hi = _mm_cvtps_pd(hi_ps);\n            \n            double d_lo[2], d_hi[2];\n            _mm_storeu_pd(d_lo, lo);\n            _mm_storeu_pd(d_hi, hi);\n            \n            for (int j = 0; j < 2; j++) {\n                int64_t val = (int64_t)d_lo[j];\n                total += (uint64_t)(val * val);\n            }\n            for (int j = 0; j < 2; j++) {\n                int64_t val = (int64_t)d_hi[j];\n                total += (uint64_t)(val * val);\n            }\n        }\n    }\n    \n    for (; i < size; i++) {\n        float x = lst[i];\n        int64_t val;\n        if (x >= 0) {\n            val = (int64_t)x;\n            if ((float)val < x) {\n                val++;\n            }\n        } else {\n            val = (int64_t)x;\n        }\n        total += (uint64_t)(val * val);\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t total_sum = 0;\n    const float* data = lst.data();\n    size_t len = lst.size();\n    size_t i = 0;\n    alignas(16) int32_t ceil_ints[4];\n\n    for (; i + 4 <= len; i += 4) {\n        __m128 vec = _mm_loadu_ps(data + i);\n        __m128i int_val = _mm_cvttps_epi32(vec);\n        __m128 trunc_vec = _mm_cvtepi32_ps(int_val);\n        __m128 mask = _mm_cmpgt_ps(vec, trunc_vec);\n        __m128i adjustment = _mm_and_si128(_mm_castps_si128(mask), _mm_set1_epi32(1));\n        int_val = _mm_add_epi32(int_val, adjustment);\n        _mm_store_si128(reinterpret_cast<__m128i*>(ceil_ints), int_val);\n\n        for (int j = 0; j < 4; j++) {\n            int64_t val = ceil_ints[j];\n            total_sum += static_cast<uint64_t>(val * val);\n        }\n    }\n\n    for (; i < len; i++) {\n        float x = data[i];\n        int32_t t = static_cast<int32_t>(x);\n        if (x > static_cast<float>(t)) {\n            t++;\n        }\n        int64_t val = t;\n        total_sum += static_cast<uint64_t>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    if (lst.empty()) \n        return 0;\n\n    __m128i acc = _mm_setzero_si128();\n    size_t i = 0;\n    const size_t n = lst.size();\n    __m128i zero_i128 = _mm_setzero_si128();\n    __m128 zero_ps = _mm_setzero_ps();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128i trunci = _mm_cvttps_epi32(v);\n        __m128 truncf = _mm_cvtepi32_ps(trunci);\n        __m128 diff = _mm_sub_ps(v, truncf);\n        __m128 mask = _mm_cmpgt_ps(diff, zero_ps);\n        __m128i adjust_i = _mm_srli_epi32(_mm_castps_si128(mask), 31);\n        __m128i ceil_i = _mm_add_epi32(trunci, adjust_i);\n\n        __m128i mask_abs = _mm_srai_epi32(ceil_i, 31);\n        __m128i abs_32 = _mm_sub_epi32(_mm_xor_si128(ceil_i, mask_abs), mask_abs);\n\n        __m128i low_part = _mm_unpacklo_epi32(abs_32, zero_i128);\n        __m128i high_part = _mm_unpackhi_epi32(abs_32, zero_i128);\n\n        __m128i low_sq = _mm_mul_epu32(low_part, low_part);\n        __m128i high_sq = _mm_mul_epu32(high_part, high_part);\n\n        acc = _mm_add_epi64(acc, low_sq);\n        acc = _mm_add_epi64(acc, high_sq);\n    }\n\n    if (i < n) {\n        float temp_arr[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n        for (size_t j = 0; j < n - i; j++) {\n            temp_arr[j] = lst[i + j];\n        }\n        __m128 v = _mm_loadu_ps(temp_arr);\n        __m128i trunci = _mm_cvttps_epi32(v);\n        __m128 truncf = _mm_cvtepi32_ps(trunci);\n        __m128 diff = _mm_sub_ps(v, truncf);\n        __m128 mask = _mm_cmpgt_ps(diff, zero_ps);\n        __m128i adjust_i = _mm_srli_epi32(_mm_castps_si128(mask), 31);\n        __m128i ceil_i = _mm_add_epi32(trunci, adjust_i);\n\n        __m128i mask_abs = _mm_srai_epi32(ceil_i, 31);\n        __m128i abs_32 = _mm_sub_epi32(_mm_xor_si128(ceil_i, mask_abs), mask_abs);\n\n        __m128i low_part = _mm_unpacklo_epi32(abs_32, zero_i128);\n        __m128i high_part = _mm_unpackhi_epi32(abs_32, zero_i128);\n\n        __m128i low_sq = _mm_mul_epu32(low_part, low_part);\n        __m128i high_sq = _mm_mul_epu32(high_part, high_part);\n\n        acc = _mm_add_epi64(acc, low_sq);\n        acc = _mm_add_epi64(acc, high_sq);\n    }\n\n    uint64_t result[2];\n    _mm_storeu_si128((__m128i*)result, acc);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int candidate = -1;\n    int i = 1;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i-1]));\n        __m128i B = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i cmp = _mm_cmplt_epi32(B, A);\n        int bitmask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        if (bitmask & 1) candidate = i;\n        if (bitmask & 2) candidate = i+1;\n        if (bitmask & 4) candidate = i+2;\n        if (bitmask & 8) candidate = i+3;\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] < arr[i-1]) {\n            candidate = i;\n        }\n    }\n\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    size_t n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    const int* data = arr.data();\n    int last_index = -1;\n    size_t j = 0;\n\n    for (; j + 4 <= n-1; j += 4) {\n        __m128i prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + j));\n        __m128i next = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + j + 1));\n        __m128i cmp = _mm_cmplt_epi32(next, prev);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n\n        if (mask) {\n            for (int k = 3; k >= 0; k--) {\n                if (mask & (1 << k)) {\n                    last_index = static_cast<int>(j) + k + 1;\n                    break;\n                }\n            }\n        }\n    }\n\n    for (size_t i = j + 1; i < n; i++) {\n        if (data[i] < data[i - 1]) {\n            last_index = static_cast<int>(i);\n        }\n    }\n\n    return last_index;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int last_drop_index = -1;\n    int i = 1;\n    for (; i <= n - 4; i += 4) {\n        __m128i current_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i - 1]));\n        __m128i next_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i cmp = _mm_cmplt_epi32(next_vec, current_vec);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        if (mask) {\n            if (mask & 8) {\n                last_drop_index = i + 3;\n            } else if (mask & 4) {\n                last_drop_index = i + 2;\n            } else if (mask & 2) {\n                last_drop_index = i + 1;\n            } else if (mask & 1) {\n                last_drop_index = i;\n            }\n        }\n    }\n    for (; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            last_drop_index = i;\n        }\n    }\n    return last_drop_index;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int result = -1;\n    int i = 1;\n\n    for (; i <= n-4; i += 4) {\n        __m128i v_prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(arr.data() + i - 1));\n        __m128i v_curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(arr.data() + i));\n        __m128i mask = _mm_cmplt_epi32(v_curr, v_prev);\n        int m = _mm_movemask_ps(_mm_castsi128_ps(mask));\n        for (int j = 0; j < 4; j++) {\n            if (m & (1 << j)) {\n                result = i + j;\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        if (arr[i] < arr[i-1]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) {\n        return -1;\n    }\n    int num_pairs = n - 1;\n    int candidate = -1;\n    const int* data = arr.data();\n\n    for (int i = 0; i <= num_pairs - 4; i += 4) {\n        __m128i prev = _mm_loadu_si128((__m128i const*)(data + i));\n        __m128i curr = _mm_loadu_si128((__m128i const*)(data + i + 1));\n        __m128i mask = _mm_cmplt_epi32(curr, prev);\n        int mask4 = _mm_movemask_ps(_mm_castsi128_ps(mask));\n\n        if (mask4) {\n            for (int j = 3; j >= 0; j--) {\n                if (mask4 & (1 << j)) {\n                    candidate = i + j + 1;\n                    break;\n                }\n            }\n        }\n    }\n\n    int tail_start = num_pairs - (num_pairs % 4);\n    for (int i = tail_start; i < num_pairs; i++) {\n        if (data[i + 1] < data[i]) {\n            candidate = i + 1;\n        }\n    }\n\n    return candidate;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <climits>\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg_v = _mm_set1_epi32(INT_MIN);\n    __m128i min_pos_v = _mm_set1_epi32(INT_MAX);\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    size_t full_chunks = lst.size() / 4;\n    for (; i < full_chunks * 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n\n        __m128i mask_neg = _mm_cmplt_epi32(vec, zero);\n        __m128i candidate_neg = _mm_or_si128(_mm_and_si128(mask_neg, vec), \n                _mm_andnot_si128(mask_neg, _mm_set1_epi32(INT_MIN)));\n        __m128i cmp_neg = _mm_cmpgt_epi32(max_neg_v, candidate_neg);\n        max_neg_v = _mm_or_si128(_mm_and_si128(cmp_neg, max_neg_v), \n                _mm_andnot_si128(cmp_neg, candidate_neg));\n\n        __m128i mask_pos = _mm_cmpgt_epi32(vec, zero);\n        __m128i candidate_pos = _mm_or_si128(_mm_and_si128(mask_pos, vec),\n                _mm_andnot_si128(mask_pos, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp_pos = _mm_cmplt_epi32(candidate_pos, min_pos_v);\n        min_pos_v = _mm_or_si128(_mm_and_si128(cmp_pos, candidate_pos),\n                _mm_andnot_si128(cmp_pos, min_pos_v));\n    }\n\n    int max_neg_scalar = INT_MIN;\n    int min_pos_scalar = INT_MAX;\n    for (; i < lst.size(); i++) {\n        int x = lst[i];\n        if (x < 0) {\n            if (x > max_neg_scalar) max_neg_scalar = x;\n        } else if (x > 0) {\n            if (x < min_pos_scalar) min_pos_scalar = x;\n        }\n    }\n\n    int max_neg_arr[4];\n    int min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg_v);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos_v);\n\n    int max_neg_final = INT_MIN;\n    for (int j = 0; j < 4; j++) {\n        if (max_neg_arr[j] > max_neg_final) {\n            max_neg_final = max_neg_arr[j];\n        }\n    }\n    if (max_neg_scalar > max_neg_final) {\n        max_neg_final = max_neg_scalar;\n    }\n    if (max_neg_final == INT_MIN) {\n        max_neg_final = 0;\n    }\n\n    int min_pos_final = INT_MAX;\n    for (int j = 0; j < 4; j++) {\n        if (min_pos_arr[j] < min_pos_final) {\n            min_pos_final = min_pos_arr[j];\n        }\n    }\n    if (min_pos_scalar < min_pos_final) {\n        min_pos_final = min_pos_scalar;\n    }\n    if (min_pos_final == INT_MAX) {\n        min_pos_final = 0;\n    }\n\n    return {max_neg_final, min_pos_final};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <vector>\n#include <climits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) {\n        return {0, 0};\n    }\n    \n    const int int_min = INT_MIN;\n    const int int_max = INT_MAX;\n    int n = lst.size();\n    int i = 0;\n    \n    __m128i v_max_neg = _mm_set1_epi32(int_min);\n    __m128i v_min_pos = _mm_set1_epi32(int_max);\n    __m128i v_any_neg = _mm_setzero_si128();\n    __m128i v_any_pos = _mm_setzero_si128();\n    \n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i mask_neg = _mm_cmplt_epi32(v, _mm_setzero_si128());\n        __m128i mask_pos = _mm_cmpgt_epi32(v, _mm_setzero_si128());\n        \n        v_any_neg = _mm_or_si128(v_any_neg, mask_neg);\n        v_any_pos = _mm_or_si128(v_any_pos, mask_pos);\n        \n        __m128i v_neg = _mm_or_si128(\n            _mm_and_si128(mask_neg, v),\n            _mm_andnot_si128(mask_neg, _mm_set1_epi32(int_min))\n        );\n        v_max_neg = _mm_max_epi32(v_max_neg, v_neg);\n        \n        __m128i v_pos = _mm_or_si128(\n            _mm_and_si128(mask_pos, v),\n            _mm_andnot_si128(mask_pos, _mm_set1_epi32(int_max))\n        );\n        v_min_pos = _mm_min_epi32(v_min_pos, v_pos);\n    }\n    \n    int max_neg_arr[4] __attribute__((aligned(16)));\n    int min_pos_arr[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)max_neg_arr, v_max_neg);\n    _mm_store_si128((__m128i*)min_pos_arr, v_min_pos);\n    \n    int max_neg_val = max_neg_arr[0];\n    int min_pos_val = min_pos_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (max_neg_arr[j] > max_neg_val) max_neg_val = max_neg_arr[j];\n        if (min_pos_arr[j] < min_pos_val) min_pos_val = min_pos_arr[j];\n    }\n    \n    bool any_neg = _mm_movemask_epi8(v_any_neg) != 0;\n    bool any_pos = _mm_movemask_epi8(v_any_pos) != 0;\n    \n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            any_neg = true;\n            if (x > max_neg_val) max_neg_val = x;\n        } else if (x > 0) {\n            any_pos = true;\n            if (x < min_pos_val) min_pos_val = x;\n        }\n    }\n    \n    if (!any_neg) max_neg_val = 0;\n    if (!any_pos) min_pos_val = 0;\n    \n    return { max_neg_val, min_pos_val };\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    int n = lst.size();\n    int i = 0;\n    \n    __m128i v_max_neg = _mm_set1_epi32(0x80000000);\n    __m128i v_min_pos = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i v_neg_exist = _mm_setzero_si128();\n    __m128i v_pos_exist = _mm_setzero_si128();\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i v_data = _mm_loadu_si128((const __m128i*)&lst[i]);\n\n        __m128i v_neg_mask = _mm_cmplt_epi32(v_data, v_zero);\n        v_neg_exist = _mm_or_si128(v_neg_exist, v_neg_mask);\n        __m128i v_neg_candidate = _mm_or_si128(\n            _mm_and_si128(v_neg_mask, v_data),\n            _mm_andnot_si128(v_neg_mask, _mm_set1_epi32(0x80000000))\n        );\n        __m128i v_cmp_neg = _mm_cmpgt_epi32(v_neg_candidate, v_max_neg);\n        v_max_neg = _mm_or_si128(\n            _mm_and_si128(v_cmp_neg, v_neg_candidate),\n            _mm_andnot_si128(v_cmp_neg, v_max_neg)\n        );\n\n        __m128i v_pos_mask = _mm_cmpgt_epi32(v_data, v_zero);\n        v_pos_exist = _mm_or_si128(v_pos_exist, v_pos_mask);\n        __m128i v_pos_candidate = _mm_or_si128(\n            _mm_and_si128(v_pos_mask, v_data),\n            _mm_andnot_si128(v_pos_mask, _mm_set1_epi32(0x7FFFFFFF))\n        );\n        __m128i v_cmp_pos = _mm_cmplt_epi32(v_pos_candidate, v_min_pos);\n        v_min_pos = _mm_or_si128(\n            _mm_and_si128(v_cmp_pos, v_pos_candidate),\n            _mm_andnot_si128(v_cmp_pos, v_min_pos)\n        );\n    }\n\n    int max_neg_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, v_max_neg);\n    int max_neg_candidate = max_neg_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (max_neg_arr[j] > max_neg_candidate) {\n            max_neg_candidate = max_neg_arr[j];\n        }\n    }\n\n    int min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)min_pos_arr, v_min_pos);\n    int min_pos_candidate = min_pos_arr[0];\n    for (int j = 1; j < 4; j++) {\n        if (min_pos_arr[j] < min_pos_candidate) {\n            min_pos_candidate = min_pos_arr[j];\n        }\n    }\n\n    int any_neg = _mm_movemask_epi8(v_neg_exist) != 0;\n    int any_pos = _mm_movemask_epi8(v_pos_exist) != 0;\n\n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            any_neg = 1;\n            if (x > max_neg_candidate) {\n                max_neg_candidate = x;\n            }\n        } else if (x > 0) {\n            any_pos = 1;\n            if (x < min_pos_candidate) {\n                min_pos_candidate = x;\n            }\n        }\n    }\n\n    int a = any_neg ? max_neg_candidate : 0;\n    int b = any_pos ? min_pos_candidate : 0;\n\n    return {a, b};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstatic inline __m128i max_epi32(__m128i a, __m128i b) {\n    __m128i mask = _mm_cmpgt_epi32(a, b);\n    return _mm_or_si128(_mm_and_si128(mask, a), _mm_andnot_si128(mask, b));\n}\n\nstatic inline __m128i min_epi32(__m128i a, __m128i b) {\n    __m128i mask = _mm_cmplt_epi32(a, b);\n    return _mm_or_si128(_mm_and_si128(mask, a), _mm_andnot_si128(mask, b));\n}\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int max_neg = INT_MIN;\n    int min_pos = INT_MAX;\n    bool any_negative = false;\n    bool any_positive = false;\n    \n    if (n == 0) {\n        return {0, 0};\n    }\n\n    __m128i v_neg_max = _mm_set1_epi32(INT_MIN);\n    __m128i v_pos_min = _mm_set1_epi32(INT_MAX);\n    __m128i v_neg_presence = _mm_setzero_si128();\n    __m128i v_pos_presence = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i chunk = _mm_loadu_si128((const __m128i*)(lst.data() + i));\n        __m128i neg_mask = _mm_cmplt_epi32(chunk, zero);\n        __m128i neg_chunk = _mm_or_si128(_mm_and_si128(neg_mask, chunk), _mm_andnot_si128(neg_mask, _mm_set1_epi32(INT_MIN)));\n        v_neg_max = max_epi32(v_neg_max, neg_chunk);\n        v_neg_presence = _mm_or_si128(v_neg_presence, neg_mask);\n        \n        __m128i pos_mask = _mm_cmpgt_epi32(chunk, zero);\n        __m128i pos_chunk = _mm_or_si128(_mm_and_si128(pos_mask, chunk), _mm_andnot_si128(pos_mask, _mm_set1_epi32(INT_MAX)));\n        v_pos_min = min_epi32(v_pos_min, pos_chunk);\n        v_pos_presence = _mm_or_si128(v_pos_presence, pos_mask);\n    }\n\n    alignas(16) int temp_neg_max[4];\n    alignas(16) int temp_neg_pres[4];\n    alignas(16) int temp_pos_min[4];\n    alignas(16) int temp_pos_pres[4];\n    _mm_store_si128((__m128i*)temp_neg_max, v_neg_max);\n    _mm_store_si128((__m128i*)temp_neg_pres, v_neg_presence);\n    _mm_store_si128((__m128i*)temp_pos_min, v_pos_min);\n    _mm_store_si128((__m128i*)temp_pos_pres, v_pos_presence);\n\n    for (int j = 0; j < 4; j++) {\n        if (temp_neg_pres[j]) {\n            if (temp_neg_max[j] > max_neg) {\n                max_neg = temp_neg_max[j];\n            }\n            any_negative = true;\n        }\n        if (temp_pos_pres[j]) {\n            if (temp_pos_min[j] < min_pos) {\n                min_pos = temp_pos_min[j];\n            }\n            any_positive = true;\n        }\n    }\n\n    for (; i < n; i++) {\n        int x = lst[i];\n        if (x < 0) {\n            if (x > max_neg) {\n                max_neg = x;\n            }\n            any_negative = true;\n        } else if (x > 0) {\n            if (x < min_pos) {\n                min_pos = x;\n            }\n            any_positive = true;\n        }\n    }\n\n    if (!any_negative) max_neg = 0;\n    if (!any_positive) min_pos = 0;\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i neg_max = _mm_set1_epi32(INT_MIN);\n    __m128i pos_min = _mm_set1_epi32(INT_MAX);\n    __m128i neg_found = _mm_setzero_si128();\n    __m128i pos_found = _mm_setzero_si128();\n    __m128i zeros = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        \n        __m128i mask_neg = _mm_cmplt_epi32(chunk, zeros);\n        neg_found = _mm_or_si128(neg_found, mask_neg);\n        __m128i candidate_neg = _mm_or_si128(_mm_and_si128(mask_neg, chunk), _mm_andnot_si128(mask_neg, _mm_set1_epi32(INT_MIN)));\n        __m128i cmp_neg = _mm_cmpgt_epi32(neg_max, candidate_neg);\n        neg_max = _mm_or_si128(_mm_and_si128(cmp_neg, neg_max), _mm_andnot_si128(cmp_neg, candidate_neg));\n        \n        __m128i mask_pos = _mm_cmpgt_epi32(chunk, zeros);\n        pos_found = _mm_or_si128(pos_found, mask_pos);\n        __m128i candidate_pos = _mm_or_si128(_mm_and_si128(mask_pos, chunk), _mm_andnot_si128(mask_pos, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp_pos = _mm_cmplt_epi32(pos_min, candidate_pos);\n        pos_min = _mm_or_si128(_mm_and_si128(cmp_pos, pos_min), _mm_andnot_si128(cmp_pos, candidate_pos));\n    }\n\n    if (i < lst.size()) {\n        int temp[4] = {0};\n        size_t num_left = lst.size() - i;\n        for (size_t j = 0; j < num_left; ++j) {\n            temp[j] = lst[i + j];\n        }\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        \n        __m128i mask_neg = _mm_cmplt_epi32(chunk, zeros);\n        neg_found = _mm_or_si128(neg_found, mask_neg);\n        __m128i candidate_neg = _mm_or_si128(_mm_and_si128(mask_neg, chunk), _mm_andnot_si128(mask_neg, _mm_set1_epi32(INT_MIN)));\n        __m128i cmp_neg = _mm_cmpgt_epi32(neg_max, candidate_neg);\n        neg_max = _mm_or_si128(_mm_and_si128(cmp_neg, neg_max), _mm_andnot_si128(cmp_neg, candidate_neg));\n        \n        __m128i mask_pos = _mm_cmpgt_epi32(chunk, zeros);\n        pos_found = _mm_or_si128(pos_found, mask_pos);\n        __m128i candidate_pos = _mm_or_si128(_mm_and_si128(mask_pos, chunk), _mm_andnot_si128(mask_pos, _mm_set1_epi32(INT_MAX)));\n        __m128i cmp_pos = _mm_cmplt_epi32(pos_min, candidate_pos);\n        pos_min = _mm_or_si128(_mm_and_si128(cmp_pos, pos_min), _mm_andnot_si128(cmp_pos, candidate_pos));\n    }\n\n    int neg_found_arr[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(neg_found_arr), neg_found);\n    int any_negative = neg_found_arr[0] | neg_found_arr[1] | neg_found_arr[2] | neg_found_arr[3];\n\n    int pos_found_arr[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(pos_found_arr), pos_found);\n    int any_positive = pos_found_arr[0] | pos_found_arr[1] | pos_found_arr[2] | pos_found_arr[3];\n\n    int neg_arr[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(neg_arr), neg_max);\n    int neg_val = neg_arr[0];\n    for (int j = 1; j < 4; ++j) {\n        if (neg_arr[j] > neg_val) {\n            neg_val = neg_arr[j];\n        }\n    }\n    if (!any_negative) {\n        neg_val = 0;\n    }\n\n    int pos_arr[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(pos_arr), pos_min);\n    int pos_val = pos_arr[0];\n    for (int j = 1; j < 4; ++j) {\n        if (pos_arr[j] < pos_val) {\n            pos_val = pos_arr[j];\n        }\n    }\n    if (!any_positive) {\n        pos_val = 0;\n    }\n\n    return {neg_val, pos_val};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdlib.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) {\n        return 1;\n    }\n\n    uint64_t *fact = (uint64_t*)malloc(n * sizeof(uint64_t));\n    uint64_t current = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        current *= i;\n        fact[i - 1] = current;\n    }\n\n    uint64_t *buf = fact;\n    uint64_t length = n;\n    while (length > 1) {\n        uint64_t new_length = (length + 1) / 2;\n        uint64_t *temp = (uint64_t*)malloc(new_length * sizeof(uint64_t));\n        uint64_t pairs = length / 2;\n        for (uint64_t i = 0; i < pairs; i++) {\n            __m128i vec = _mm_loadu_si128((__m128i const*)&buf[2 * i]);\n            uint64_t a = _mm_cvtsi128_si64(vec);\n            __m128i hi64 = _mm_unpackhi_epi64(vec, vec);\n            uint64_t b = _mm_cvtsi128_si64(hi64);\n            temp[i] = a * b;\n        }\n        if (length % 2) {\n            temp[new_length - 1] = buf[length - 1];\n        }\n        if (buf != fact) {\n            free(buf);\n        }\n        buf = temp;\n        length = new_length;\n    }\n\n    uint64_t result = buf[0];\n    if (buf != fact) {\n        free(buf);\n    }\n    free(fact);\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i += 2) {\n        uint64_t exp1 = n - i + 1;\n        uint64_t exp2 = (i + 1 <= n) ? n - i : 0;\n        uint64_t base1 = i;\n        uint64_t base2 = (i + 1 <= n) ? i + 1 : 1;\n\n        uint64_t term1 = 1;\n        uint64_t term2 = 1;\n        while (exp1 || exp2) {\n            if (exp1 & 1) {\n                term1 *= base1;\n            }\n            base1 *= base1;\n            exp1 >>= 1;\n\n            if (exp2) {\n                if (exp2 & 1) {\n                    term2 *= base2;\n                }\n                base2 *= base2;\n                exp2 >>= 1;\n            }\n        }\n\n        __m128i v_term = _mm_set_epi64x(term2, term1);\n        __m128i v_prod = _mm_set1_epi64x(result);\n        uint64_t terms_product = _mm_extract_epi64(v_term, 0) * _mm_extract_epi64(v_term, 1);\n        result = _mm_extract_epi64(v_prod, 0) * terms_product;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) {\n        return 0;\n    }\n    if (n >= 16) {\n        return 0;\n    }\n    uint64_t F[16];\n    F[0] = 1;\n    for (uint64_t i = 2; i <= n; i++) {\n        F[i-1] = F[i-2] * i;\n    }\n    uint64_t len = n;\n    while (len > 1) {\n        uint64_t out_index = 0;\n        for (uint64_t i = 0; i < len; i += 2) {\n            if (i + 1 < len) {\n                __m128i v = _mm_loadu_si128((__m128i*)(F + i));\n                uint64_t temp[2];\n                _mm_storeu_si128((__m128i*)temp, v);\n                F[out_index++] = temp[0] * temp[1];\n            } else {\n                F[out_index++] = F[i];\n            }\n        }\n        len = out_index;\n    }\n    return F[0];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) {\n        return 1;\n    }\n    if (n > 12) {\n        uint64_t prod = 1;\n        uint64_t fact = 1;\n        for (uint64_t k = 1; k <= n; k++) {\n            fact *= k;\n            prod *= fact;\n        }\n        return prod;\n    }\n\n    uint32_t facts[12];\n    uint32_t current = 1;\n    for (uint32_t k = 1; k <= n; k++) {\n        current *= k;\n        facts[k-1] = current;\n    }\n\n    __m128i acc = _mm_set1_epi64x(1);\n    for (uint32_t i = 0; i < n; i += 2) {\n        uint32_t a = facts[i];\n        uint32_t b = (i+1 < n) ? facts[i+1] : 1;\n        __m128i v = _mm_setr_epi32(a, 0, b, 0);\n        acc = _mm_mul_epu32(acc, v);\n    }\n\n    uint64_t prod_even = _mm_cvtsi128_si64(acc);\n    uint64_t prod_odd = _mm_cvtsi128_si64(_mm_srli_si128(acc, 8));\n    return prod_even * prod_odd;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t F[8];\n    uint64_t fact = 1;\n    for (uint64_t k = 1; k <= n; k++) {\n        fact *= k;\n        F[k - 1] = fact;\n    }\n\n    uint64_t arr[8];\n    for (int i = 0; i < n; i++) {\n        arr[i] = F[i];\n    }\n    int m = n;\n    uint64_t temp[8];\n\n    while (m > 1) {\n        int index = 0;\n        int i = 0;\n\n        for (; i + 3 < m; i += 4) {\n            __m128i a = _mm_set_epi64x(arr[i + 2], arr[i]);\n            __m128i b = _mm_set_epi64x(arr[i + 3], arr[i + 1]);\n\n            __m128i a_lo = _mm_and_si128(a, _mm_set1_epi64x(0xFFFFFFFF));\n            __m128i a_hi = _mm_srli_epi64(a, 32);\n            __m128i b_lo = _mm_and_si128(b, _mm_set1_epi64x(0xFFFFFFFF));\n            __m128i b_hi = _mm_srli_epi64(b, 32);\n\n            __m128i term1 = _mm_mul_epu32(a_lo, b_lo);\n            __m128i term2 = _mm_mul_epu32(a_lo, b_hi);\n            __m128i term3 = _mm_mul_epu32(a_hi, b_lo);\n\n            term2 = _mm_slli_epi64(term2, 32);\n            term3 = _mm_slli_epi64(term3, 32);\n\n            __m128i res = _mm_add_epi64(term1, term2);\n            res = _mm_add_epi64(res, term3);\n\n            _mm_storeu_si128((__m128i*)&temp[index], res);\n            index += 2;\n        }\n\n        if (i + 1 < m) {\n            temp[index] = arr[i] * arr[i + 1];\n            index++;\n            i += 2;\n        }\n        if (i < m) {\n            temp[index] = arr[i];\n            index++;\n        }\n\n        m = index;\n        for (int j = 0; j < m; j++) {\n            arr[j] = temp[j];\n        }\n    }\n\n    return arr[0];\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int n = lst.size();\n    if (n == 0) \n        return 0;\n\n    __m128i vsum = _mm_setzero_si128();\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i data = _mm_loadu_si128((const __m128i*)&lst[i]);\n\n        int mod0 = i % 12;\n        int mod1 = (i+1) % 12;\n        int mod2 = (i+2) % 12;\n        int mod3 = (i+3) % 12;\n\n        __m128i mod_vec = _mm_set_epi32(mod3, mod2, mod1, mod0);\n\n        __m128i mask3_0 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(0));\n        __m128i mask3_3 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(3));\n        __m128i mask3_6 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(6));\n        __m128i mask3_9 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(9));\n        __m128i mask3 = _mm_or_si128(_mm_or_si128(mask3_0, mask3_3), _mm_or_si128(mask3_6, mask3_9));\n\n        __m128i mask4_4 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(4));\n        __m128i mask4_8 = _mm_cmpeq_epi32(mod_vec, _mm_set1_epi32(8));\n        __m128i mask4 = _mm_or_si128(mask4_4, mask4_8);\n\n        __m128i square = _mm_mullo_epi32(data, data);\n        __m128i cube = _mm_mullo_epi32(square, data);\n\n        __m128i temp = _mm_or_si128(_mm_and_si128(mask4, cube), _mm_andnot_si128(mask4, data));\n        __m128i res = _mm_or_si128(_mm_and_si128(mask3, square), _mm_andnot_si128(mask3, temp));\n\n        vsum = _mm_add_epi32(vsum, res);\n    }\n\n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, vsum);\n    int total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n    for (; i < n; i++) {\n        int x = lst[i];\n        if (i % 3 == 0) {\n            x = x * x;\n        } else if (i % 4 == 0) {\n            x = x * x * x;\n        }\n        total += x;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n    \n    __m128i v_sum = _mm_setzero_si128();\n    int i = 0;\n    \n    for (; i <= n - 4; i += 4) {\n        int base = i;\n        int mod3_0 = base % 3;\n        int mod3_1 = (base + 1) % 3;\n        int mod3_2 = (base + 2) % 3;\n        int mod3_3 = (base + 3) % 3;\n        __m128i v_mod3 = _mm_set_epi32(mod3_3, mod3_2, mod3_1, mod3_0);\n        __m128i mask3 = _mm_cmpeq_epi32(v_mod3, _mm_setzero_si128());\n        \n        __m128i v_idx = _mm_set_epi32(base + 3, base + 2, base + 1, base);\n        __m128i v_mod4 = _mm_and_si128(v_idx, _mm_set1_epi32(3));\n        __m128i mask4_temp = _mm_cmpeq_epi32(v_mod4, _mm_setzero_si128());\n        __m128i mask4 = _mm_andnot_si128(mask3, mask4_temp);\n        \n        __m128i data = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i squared = _mm_mullo_epi32(data, data);\n        __m128i cubed = _mm_mullo_epi32(squared, data);\n        \n        __m128i res = _mm_or_si128(_mm_and_si128(mask3, squared), \n                         _mm_andnot_si128(mask3, data));\n        res = _mm_or_si128(_mm_and_si128(mask4, cubed), \n                         _mm_andnot_si128(mask4, res));\n        \n        v_sum = _mm_add_epi32(v_sum, res);\n    }\n    \n    alignas(16) int sum_arr[4];\n    _mm_store_si128((__m128i*)sum_arr, v_sum);\n    int total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    \n    for (; i < n; i++) {\n        int x = lst[i];\n        if (i % 3 == 0) {\n            total += x * x;\n        } else if (i % 4 == 0) {\n            total += x * x * x;\n        } else {\n            total += x;\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    if (n == 0) \n        return 0;\n    \n    __m128i vsum = _mm_setzero_si128();\n    \n    for (; i + 4 <= n; i += 4) {\n        int base_mod3 = i % 3;\n        int m0 = base_mod3;\n        int m1 = (base_mod3 + 1) % 3;\n        int m2 = (base_mod3 + 2) % 3;\n        int m3 = base_mod3;\n        \n        __m128i mod3_vec = _mm_setr_epi32(m0, m1, m2, m3);\n        __m128i mask3 = _mm_cmpeq_epi32(mod3_vec, _mm_setzero_si128());\n        \n        __m128i mod4_vec = _mm_setr_epi32(0, 1, 2, 3);\n        __m128i cond1 = _mm_cmpeq_epi32(mod4_vec, _mm_setzero_si128());\n        __m128i cond2 = _mm_andnot_si128(mask3, _mm_set1_epi32(-1));\n        __m128i mask4 = _mm_and_si128(cond1, cond2);\n        \n        __m128i data = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i square = _mm_mullo_epi32(data, data);\n        __m128i cube = _mm_mullo_epi32(square, data);\n        \n        __m128i part1 = _mm_and_si128(mask3, square);\n        __m128i part2 = _mm_and_si128(mask4, cube);\n        __m128i mask_both = _mm_or_si128(mask3, mask4);\n        __m128i part3 = _mm_andnot_si128(mask_both, data);\n        \n        __m128i result = _mm_or_si128(_mm_or_si128(part1, part2), part3);\n        vsum = _mm_add_epi32(vsum, result);\n    }\n    \n    if (i > 0) {\n        __m128i shuf = _mm_shuffle_epi32(vsum, _MM_SHUFFLE(1,0,3,2));\n        __m128i sums = _mm_add_epi32(vsum, shuf);\n        alignas(16) int tmp[4];\n        _mm_store_si128((__m128i*)tmp, sums);\n        sum += tmp[0] + tmp[1];\n    }\n    \n    for (; i < n; i++) {\n        int index = i;\n        int value = lst[i];\n        if (index % 3 == 0) {\n            sum += value * value;\n        } else if (index % 4 == 0) {\n            sum += value * value * value;\n        } else {\n            sum += value;\n        }\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) \n        return 0;\n    \n    __m128i v_sum = _mm_setzero_si128();\n    int i = 0;\n    \n    for (; i <= n - 4; i += 4) {\n        int base = i % 12;\n        __m128i v_base = _mm_set1_epi32(base);\n        __m128i v_inc = _mm_setr_epi32(0, 1, 2, 3);\n        __m128i v_idx = _mm_add_epi32(v_base, v_inc);\n        \n        __m128i v_ge12 = _mm_cmpgt_epi32(v_idx, _mm_set1_epi32(11));\n        v_idx = _mm_sub_epi32(v_idx, _mm_and_si128(v_ge12, _mm_set1_epi32(12)));\n        \n        __m128i mask3 = _mm_setzero_si128();\n        mask3 = _mm_or_si128(mask3, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(0)));\n        mask3 = _mm_or_si128(mask3, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(3)));\n        mask3 = _mm_or_si128(mask3, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(6)));\n        mask3 = _mm_or_si128(mask3, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(9)));\n        \n        __m128i mask4 = _mm_setzero_si128();\n        mask4 = _mm_or_si128(mask4, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(4)));\n        mask4 = _mm_or_si128(mask4, _mm_cmpeq_epi32(v_idx, _mm_set1_epi32(8)));\n        \n        __m128i data = _mm_loadu_si128((const __m128i*)&lst[i]);\n        \n        int data_arr[4];\n        _mm_storeu_si128((__m128i*)data_arr, data);\n        \n        int square_arr[4], cube_arr[4];\n        for (int k = 0; k < 4; ++k) {\n            square_arr[k] = data_arr[k] * data_arr[k];\n            cube_arr[k] = square_arr[k] * data_arr[k];\n        }\n        __m128i square = _mm_loadu_si128((const __m128i*)square_arr);\n        __m128i cube = _mm_loadu_si128((const __m128i*)cube_arr);\n        \n        __m128i not_mask3 = _mm_andnot_si128(mask3, _mm_set1_epi32(-1));\n        __m128i not_mask4 = _mm_andnot_si128(mask4, _mm_set1_epi32(-1));\n        __m128i neither = _mm_and_si128(not_mask3, not_mask4);\n        __m128i part_data = _mm_and_si128(neither, data);\n        __m128i part_square = _mm_and_si128(mask3, square);\n        __m128i part_cube = _mm_and_si128(mask4, cube);\n        \n        __m128i result = _mm_or_si128(part_data, _mm_or_si128(part_square, part_cube));\n        v_sum = _mm_add_epi32(v_sum, result);\n    }\n    \n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, v_sum);\n    int total = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    \n    for (int j = i; j < n; ++j) {\n        int x = lst[j];\n        if (j % 3 == 0) {\n            total += x * x;\n        } else if (j % 4 == 0) {\n            total += x * x * x;\n        } else {\n            total += x;\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    if (lst.empty()) \n        return 0;\n    \n    const int pattern_size = 12;\n    const int square_mask_pattern[pattern_size] = {1,0,0,1,0,0,1,0,0,1,0,0};\n    const int cube_mask_pattern[pattern_size] = {0,0,0,0,1,0,0,0,1,0,0,0};\n    int n = lst.size();\n    int total = 0;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        int start_index = i % pattern_size;\n        \n        __m128i mask_square = _mm_setr_epi32(\n            square_mask_pattern[start_index],\n            square_mask_pattern[(start_index + 1) % pattern_size],\n            square_mask_pattern[(start_index + 2) % pattern_size],\n            square_mask_pattern[(start_index + 3) % pattern_size]\n        );\n        \n        __m128i mask_cube = _mm_setr_epi32(\n            cube_mask_pattern[start_index],\n            cube_mask_pattern[(start_index + 1) % pattern_size],\n            cube_mask_pattern[(start_index + 2) % pattern_size],\n            cube_mask_pattern[(start_index + 3) % pattern_size]\n        );\n        \n        __m128i v_sq = _mm_mullo_epi32(v, v);\n        __m128i v_cube = _mm_mullo_epi32(v, v_sq);\n        __m128i term_square = _mm_mullo_epi32(mask_square, v_sq);\n        __m128i term_cube = _mm_mullo_epi32(mask_cube, v_cube);\n        \n        __m128i one = _mm_set1_epi32(1);\n        __m128i mask_other = _mm_sub_epi32(one, mask_square);\n        mask_other = _mm_sub_epi32(mask_other, mask_cube);\n        __m128i term_other = _mm_mullo_epi32(mask_other, v);\n        \n        __m128i sum_terms = _mm_add_epi32(term_square, term_cube);\n        sum_terms = _mm_add_epi32(sum_terms, term_other);\n        \n        __m128i shuf = _mm_shuffle_epi32(sum_terms, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i sum1 = _mm_add_epi32(sum_terms, shuf);\n        __m128i shuf2 = _mm_shuffle_epi32(sum1, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i sum2 = _mm_add_epi32(sum1, shuf2);\n        total += _mm_cvtsi128_si32(sum2);\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            total += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            total += lst[i] * lst[i] * lst[i];\n        } else {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int n = nums.size();\n    int count = 0;\n    int i = 0;\n\n    if (n == 0) {\n        return 0;\n    }\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)(&nums[i]));\n        __m128i ten = _mm_set1_epi32(10);\n        __m128i cmp_gt10 = _mm_cmpgt_epi32(v, ten);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask_neg = _mm_cmplt_epi32(v, zero);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, mask_neg), mask_neg);\n        \n        int abs_arr[4];\n        int mask_arr[4];\n        _mm_storeu_si128((__m128i*)abs_arr, abs_v);\n        _mm_storeu_si128((__m128i*)mask_arr, cmp_gt10);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                int num_abs = abs_arr[j];\n                int last_digit = num_abs % 10;\n                if (last_digit % 2 == 1) {\n                    int temp = num_abs;\n                    while (temp >= 10) {\n                        temp /= 10;\n                    }\n                    int first_digit = temp;\n                    if (first_digit % 2 == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10) {\n            int num_abs = num < 0 ? -num : num;\n            int last_digit = num_abs % 10;\n            if (last_digit % 2 == 1) {\n                int temp = num_abs;\n                while (temp >= 10) {\n                    temp /= 10;\n                }\n                int first_digit = temp;\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int n = nums.size();\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&nums[i]);\n        __m128i cmp = _mm_cmpgt_epi32(v, _mm_set1_epi32(10));\n        alignas(16) int cmp_res[4];\n        _mm_store_si128((__m128i*)cmp_res, cmp);\n\n        for (int j = 0; j < 4; ++j) {\n            if (cmp_res[j]) {\n                int num = nums[i + j];\n                int last_digit = num % 10;\n                if (last_digit == 1 || last_digit == 3 || last_digit == 5 || last_digit == 7 || last_digit == 9) {\n                    int first_digit = num;\n                    while (first_digit >= 10) {\n                        first_digit /= 10;\n                    }\n                    if (first_digit % 2 == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int last_digit = num % 10;\n            if (last_digit == 1 || last_digit == 3 || last_digit == 5 || last_digit == 7 || last_digit == 9) {\n                int first_digit = num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int n = nums.size();\n    for (int i = 0; i < n; i += 4) {\n        __m128i v;\n        if (i + 4 <= n) {\n            v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        } else {\n            int temp[4] = {0};\n            for (int j = 0; j < n - i; j++) {\n                temp[j] = nums[i + j];\n            }\n            v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        }\n\n        __m128i mask1 = _mm_cmpgt_epi32(v, _mm_set1_epi32(10));\n        if (_mm_movemask_epi8(mask1) == 0) {\n            continue;\n        }\n\n        __m128 v_float = _mm_cvtepi32_ps(v);\n        __m128 quotient_float = _mm_div_ps(v_float, _mm_set1_ps(10.0f));\n        __m128i quotient = _mm_cvttps_epi32(quotient_float);\n        __m128 product_float = _mm_mul_ps(_mm_cvtepi32_ps(quotient), _mm_set1_ps(10.0f));\n        __m128i product = _mm_cvttps_epi32(product_float);\n        __m128i last_digit = _mm_sub_epi32(v, product);\n        __m128i last_odd_mask = _mm_and_si128(last_digit, _mm_set1_epi32(1));\n        __m128i mask2 = _mm_cmpeq_epi32(last_odd_mask, _mm_set1_epi32(1));\n        __m128i mask_temp = _mm_and_si128(mask1, mask2);\n\n        if (_mm_movemask_epi8(mask_temp) == 0) {\n            continue;\n        }\n\n        alignas(16) int values[4];\n        alignas(16) int mask_arr[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(values), v);\n        _mm_store_si128(reinterpret_cast<__m128i*>(mask_arr), mask_temp);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                int num = values[j];\n                int temp = num;\n                while (temp >= 10) {\n                    temp /= 10;\n                }\n                if (temp & 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int n = nums.size();\n    for (int i = 0; i < n; i += 4) {\n        int num_remaining = n - i;\n        __m128i v;\n        if (num_remaining >= 4) {\n            v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        } else {\n            int tmp[4] = {0};\n            for (int j = 0; j < num_remaining; j++) {\n                tmp[j] = nums[i+j];\n            }\n            v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(tmp));\n        }\n\n        __m128i gt10 = _mm_cmpgt_epi32(v, _mm_set1_epi32(10));\n        alignas(16) int mask_arr[4];\n        alignas(16) int values_arr[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(mask_arr), gt10);\n        _mm_store_si128(reinterpret_cast<__m128i*>(values_arr), v);\n\n        for (int j = 0; j < 4; j++) {\n            if (i + j >= n) break;\n            if (mask_arr[j]) {\n                int num = values_arr[j];\n                int abs_val = (num < 0) ? -num : num;\n                int last_digit = abs_val % 10;\n                if (last_digit % 2 == 1) {\n                    int first_digit = abs_val;\n                    while (first_digit >= 10) {\n                        first_digit /= 10;\n                    }\n                    if (first_digit % 2 == 1) {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint getFirstDigit(int n) {\n    if (n == 0) return 0;\n    while (n >= 10) {\n        n /= 10;\n    }\n    return n;\n}\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int n = nums.size();\n    int i = 0;\n    \n    for (; i <= n - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        __m128i cond1 = _mm_cmpgt_epi32(v, _mm_set1_epi32(10));\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask_neg = _mm_cmpgt_epi32(zero, v);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, mask_neg), mask_neg);\n        \n        __m128i magic = _mm_set1_epi32(0xCCCCCCCD);\n        __m128i ten = _mm_set1_epi32(10);\n        \n        __m128i even_abs = abs_v;\n        __m128i even_product = _mm_mul_epu32(even_abs, magic);\n        __m128i even_quotient = _mm_srli_epi64(even_product, 35);\n        \n        __m128i odd_abs = _mm_srli_epi64(abs_v, 32);\n        __m128i odd_product = _mm_mul_epu32(odd_abs, magic);\n        __m128i odd_quotient = _mm_srli_epi64(odd_product, 35);\n        \n        uint64_t even_quotient_arr[2];\n        _mm_store_si128((__m128i*)even_quotient_arr, even_quotient);\n        uint64_t odd_quotient_arr[2];\n        _mm_store_si128((__m128i*)odd_quotient_arr, odd_quotient);\n        \n        uint32_t quotients_arr[4] = {\n            (uint32_t)(even_quotient_arr[0] & 0xFFFFFFFF),\n            (uint32_t)(odd_quotient_arr[0] & 0xFFFFFFFF),\n            (uint32_t)(even_quotient_arr[1] & 0xFFFFFFFF),\n            (uint32_t)(odd_quotient_arr[1] & 0xFFFFFFFF)\n        };\n        __m128i quotient_vec = _mm_setr_epi32(quotients_arr[0], quotients_arr[1], quotients_arr[2], quotients_arr[3]);\n        __m128i product = _mm_add_epi32(_mm_slli_epi32(quotient_vec, 3), _mm_slli_epi32(quotient_vec, 1));\n        __m128i last_digit_vec = _mm_sub_epi32(abs_v, product);\n        __m128i cond2 = _mm_cmpeq_epi32(_mm_and_si128(last_digit_vec, _mm_set1_epi32(1)), _mm_set1_epi32(1));\n        __m128i candidate_mask = _mm_and_si128(cond1, cond2);\n        \n        int mask_arr[4];\n        _mm_store_si128((__m128i*)mask_arr, candidate_mask);\n        int abs_arr[4];\n        _mm_store_si128((__m128i*)abs_arr, abs_v);\n        \n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j] != 0) {\n                int first_digit = getFirstDigit(abs_arr[j]);\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    \n    for (; i < n; i++) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = abs(num);\n            int last_digit = abs_num % 10;\n            if (last_digit % 2 == 1) {\n                int first_digit = getFirstDigit(abs_num);\n                if (first_digit % 2 == 1) {\n                    count++;\n                }\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count0 = n / 3;\n    uint64_t count1 = (n + 2) / 3;\n    uint64_t count2 = (n + 1) / 3;\n\n    uint64_t group0 = count2;\n    uint64_t group1 = count0 + count1;\n\n    auto comb = [](uint64_t x) -> uint64_t {\n        if (x < 3) return 0;\n        __uint128_t a = x;\n        __uint128_t b = x - 1;\n        __uint128_t c = x - 2;\n        __uint128_t res = a * b * c;\n        res /= 6;\n        return (uint64_t)res;\n    };\n\n    return comb(group0) + comb(group1);\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count0 = (n + 1) / 3;\n    uint64_t count1 = n - count0;\n    uint64_t ans = 0;\n    if (count0 >= 3) {\n        uint64_t term0 = count0 * (count0 - 1) / 2;\n        term0 = term0 * (count0 - 2) / 3;\n        ans = term0;\n    }\n    if (count1 >= 3) {\n        uint64_t term1 = count1 * (count1 - 1) / 2;\n        term1 = term1 * (count1 - 2) / 3;\n        ans += term1;\n    }\n    return ans;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) {\n        return 0;\n    }\n    uint64_t r0 = (n + 1) / 3;\n    uint64_t r1 = n - r0;\n    __m128i v_r = _mm_set_epi64x(r1, r0);\n    uint64_t v_r_array[2];\n    _mm_store_si128((__m128i*)v_r_array, v_r);\n    uint64_t term0 = 0, term1 = 0;\n    if (v_r_array[0] >= 3) {\n        term0 = v_r_array[0] * (v_r_array[0] - 1) * (v_r_array[0] - 2) / 6;\n    }\n    if (v_r_array[1] >= 3) {\n        term1 = v_r_array[1] * (v_r_array[1] - 1) * (v_r_array[1] - 2) / 6;\n    }\n    return term0 + term1;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count0 = (n + 1) / 3;\n    uint64_t count1 = n - count0;\n    \n    auto C3 = [](uint64_t x) -> uint64_t {\n        if (x < 3) return 0;\n        __uint128_t a = x;\n        __uint128_t b = x - 1;\n        __uint128_t c = x - 2;\n        __uint128_t product = a;\n        product = product * b;\n        product = product * c;\n        product = product / 6;\n        return (uint64_t)product;\n    };\n\n    // Use SSE2 to add the two results in a vectorized manner\n    uint64_t result0 = C3(count0);\n    uint64_t result1 = C3(count1);\n    __m128i v_results = _mm_set_epi64x(0, result1);\n    __m128i v_result0 = _mm_set_epi64x(0, result0);\n    __m128i v_sum = _mm_add_epi64(v_results, v_result0);\n    return _mm_cvtsi128_si64(v_sum);\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) {\n        return 0;\n    }\n    uint64_t r2 = (n + 1) / 3;\n    uint64_t rest = n - r2;\n    \n    unsigned __int128 term1 = 0;\n    unsigned __int128 term2 = 0;\n    \n    if (r2 >= 3) {\n        term1 = (unsigned __int128)r2 * (r2 - 1) * (r2 - 2) / 6;\n    }\n    if (rest >= 3) {\n        term2 = (unsigned __int128)rest * (rest - 1) * (rest - 2) / 6;\n    }\n    uint64_t total = (uint64_t)term1 + (uint64_t)term2;\n    \n    __m128i dummy = _mm_setzero_si128();\n    (void)dummy;\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": ""}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n\n    int i = 3;\n    while (i * i <= n) {\n        int d0 = i;\n        int d1 = ((i+2) * (i+2) <= n) ? i+2 : 0;\n        int d2 = ((i+4) * (i+4) <= n) ? i+4 : 0;\n        int d3 = ((i+6) * (i+6) <= n) ? i+6 : 0;\n\n        int r0 = d0 ? (n % d0) : 1;\n        int r1 = d1 ? (n % d1) : 1;\n        int r2 = d2 ? (n % d2) : 1;\n        int r3 = d3 ? (n % d3) : 1;\n\n        __m128i rem_vec = _mm_set_epi32(r3, r2, r1, r0);\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(rem_vec, zero_vec);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return y;\n        }\n        i += 8;\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n    if (n == 2) return x;\n    if (n % 2 == 0) return y;\n    \n    int limit = (int)__builtin_sqrt(n) + 1;\n    for (int i = 3; i <= limit; i += 8) {\n        int d0 = i;\n        int d1 = i + 2;\n        int d2 = i + 4;\n        int d3 = i + 6;\n        \n        if (d0 > limit) break;\n        if (d1 > limit) d1 = n + 1;\n        if (d2 > limit) d2 = n + 1;\n        if (d3 > limit) d3 = n + 1;\n        \n        int r0 = n % d0;\n        int r1 = n % d1;\n        int r2 = n % d2;\n        int r3 = n % d3;\n        \n        __m128i rem_vec = _mm_set_epi32(r3, r2, r1, r0);\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(rem_vec, zero_vec);\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        if (mask != 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) \n        return y;\n    if (n == 2) \n        return x;\n    if (n % 2 == 0) \n        return y;\n\n    int limit = (int)sqrt((double)n) + 1;\n    int i = 3;\n    for (; i + 6 <= limit; i += 4) {\n        __m128i d_vec = _mm_setr_epi32(i, i+2, i+4, i+6);\n        __m128 d_vec_float = _mm_cvtepi32_ps(d_vec);\n        __m128 n_vec_float = _mm_set1_ps((float)n);\n        __m128 quot_float = _mm_div_ps(n_vec_float, d_vec_float);\n        __m128i quot_int = _mm_cvttps_epi32(quot_float);\n        __m128 quot_int_float = _mm_cvtepi32_ps(quot_int);\n        __m128 product_float = _mm_mul_ps(quot_int_float, d_vec_float);\n        __m128i product = _mm_cvttps_epi32(product_float);\n        __m128i rem = _mm_sub_epi32(_mm_set1_epi32(n), product);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpeq_epi32(rem, zero);\n        int lanes[4];\n        _mm_storeu_si128((__m128i*)lanes, mask);\n        if (lanes[0] || lanes[1] || lanes[2] || lanes[3]) {\n            return y;\n        }\n    }\n    for (; i <= limit; i += 2) {\n        if (n % i == 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) {\n        return y;\n    }\n    if (n == 2) {\n        return x;\n    }\n    if (n % 2 == 0) {\n        return y;\n    }\n    int low = 1, high = 46341;\n    int limit = 0;\n    while (low <= high) {\n        int mid = (low + high) / 2;\n        long long sq = (long long)mid * mid;\n        if (sq <= n) {\n            limit = mid;\n            low = mid + 1;\n        } else {\n            high = mid - 1;\n        }\n    }\n    for (int d = 3; d <= limit; d += 8) {\n        int divisors_arr[4] = {d, d+2, d+4, d+6};\n        for (int i = 0; i < 4; ++i) {\n            if (divisors_arr[i] > limit) {\n                divisors_arr[i] = 0;\n            }\n        }\n        int quotients_arr[4];\n        for (int i = 0; i < 4; ++i) {\n            if (divisors_arr[i] == 0) {\n                quotients_arr[i] = 0;\n            } else {\n                quotients_arr[i] = n / divisors_arr[i];\n            }\n        }\n        int products_arr[4];\n        for (int i = 0; i < 4; ++i) {\n            products_arr[i] = divisors_arr[i] * quotients_arr[i];\n        }\n        __m128i n_vec = _mm_set1_epi32(n);\n        __m128i products = _mm_loadu_si128((__m128i*)products_arr);\n        __m128i rems = _mm_sub_epi32(n_vec, products);\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi32(rems, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n    \n    uint64_t total = 0;\n    size_t i = 0;\n    const size_t n = lst.size();\n    const __m128 zero = _mm_setzero_ps();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        \n        __m128 mask_non_neg = _mm_cmpge_ps(v, zero);\n        __m128i vi = _mm_cvttps_epi32(v);\n        __m128 vf = _mm_cvtepi32_ps(vi);\n        __m128 mask_int = _mm_cmpeq_ps(v, vf);\n        \n        __m128 mask_condition_ps = _mm_and_ps(mask_non_neg, mask_int);\n        __m128i mask_condition = _mm_castps_si128(mask_condition_ps);\n        \n        __m128i mask_odd = _mm_and_si128(vi, _mm_set1_epi32(1));\n        mask_odd = _mm_cmpeq_epi32(mask_odd, _mm_set1_epi32(1));\n        \n        mask_condition = _mm_and_si128(mask_condition, mask_odd);\n        __m128i vi_masked = _mm_and_si128(vi, mask_condition);\n        \n        __m128i squares0 = _mm_mul_epu32(vi_masked, vi_masked);\n        __m128i vi_shifted = _mm_srli_si128(vi_masked, 4);\n        __m128i squares1 = _mm_mul_epu32(vi_shifted, vi_shifted);\n        \n        __m128i sum2 = _mm_add_epi64(squares0, squares1);\n        alignas(16) uint64_t tmp[2];\n        _mm_store_si128((__m128i*)tmp, sum2);\n        total += tmp[0] + tmp[1];\n    }\n    \n    for (; i < n; ++i) {\n        float x = lst[i];\n        if (x >= 0 && x == static_cast<float>(static_cast<int32_t>(x))) {\n            int32_t n_val = static_cast<int32_t>(x);\n            if (n_val % 2 != 0) {\n                total += static_cast<uint64_t>(n_val) * n_val;\n            }\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <cstdint>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    size_t n = lst.size();\n    if (n == 0) {\n        return 0;\n    }\n\n    uint64_t sum = 0;\n    size_t i = 0;\n\n    __m128i acc_low = _mm_setzero_si128();\n    __m128i acc_high = _mm_setzero_si128();\n\n    __m128 zero = _mm_setzero_ps();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i + 3 < n; i += 4) {\n        __m128 v = _mm_loadu_ps(&lst[i]);\n\n        __m128 mask_non_neg = _mm_cmpge_ps(v, zero);\n\n        __m128i v_int = _mm_cvtps_epi32(v);\n        __m128 v_float = _mm_cvtepi32_ps(v_int);\n        __m128 mask_integer = _mm_cmpeq_ps(v, v_float);\n\n        __m128i v_odd = _mm_and_si128(v_int, one);\n        __m128i mask_odd_i = _mm_cmpeq_epi32(v_odd, one);\n        __m128 mask_odd = _mm_castsi128_ps(mask_odd_i);\n\n        __m128 mask_combined = _mm_and_ps(mask_non_neg, _mm_and_ps(mask_integer, mask_odd));\n        __m128i mask_combined_i = _mm_castps_si128(mask_combined);\n\n        __m128i v_int_low = _mm_unpacklo_epi32(v_int, _mm_setzero_si128());\n        __m128i v_int_high = _mm_unpackhi_epi32(v_int, _mm_setzero_si128());\n\n        __m128i v_sq_low = _mm_mul_epu32(v_int_low, v_int_low);\n        __m128i v_sq_high = _mm_mul_epu32(v_int_high, v_int_high);\n\n        __m128i mask_low = _mm_unpacklo_epi32(mask_combined_i, mask_combined_i);\n        __m128i mask_high = _mm_unpackhi_epi32(mask_combined_i, mask_combined_i);\n\n        v_sq_low = _mm_and_si128(v_sq_low, mask_low);\n        v_sq_high = _mm_and_si128(v_sq_high, mask_high);\n\n        acc_low = _mm_add_epi64(acc_low, v_sq_low);\n        acc_high = _mm_add_epi64(acc_high, v_sq_high);\n    }\n\n    acc_low = _mm_add_epi64(acc_low, acc_high);\n    alignas(16) uint64_t temp[2];\n    _mm_store_si128((__m128i*)temp, acc_low);\n    sum = temp[0] + temp[1];\n\n    for (; i < n; ++i) {\n        float x = lst[i];\n        if (x >= 0) {\n            int32_t xi = static_cast<int32_t>(x);\n            if (x == static_cast<float>(xi)) {\n                if (xi & 1) {\n                    sum += static_cast<uint64_t>(xi) * static_cast<uint64_t>(xi);\n                }\n            }\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t total = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    if (n == 0) \n        return 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        \n        __m128 zero = _mm_setzero_ps();\n        __m128 mask1 = _mm_cmpge_ps(v, zero);\n        \n        __m128i vi = _mm_cvtps_epi32(v);\n        __m128 v_int = _mm_cvtepi32_ps(vi);\n        __m128 mask2 = _mm_cmpeq_ps(v, v_int);\n        \n        __m128i one = _mm_set1_epi32(1);\n        __m128i vi_and = _mm_and_si128(vi, one);\n        __m128i mask_odd_i = _mm_cmpeq_epi32(vi_and, one);\n        __m128 mask_odd = _mm_castsi128_ps(mask_odd_i);\n        \n        __m128 mask_total = _mm_and_ps(mask1, _mm_and_ps(mask2, mask_odd));\n        __m128i mask_total_i = _mm_castps_si128(mask_total);\n        \n        alignas(16) uint32_t mask_arr[4];\n        alignas(16) int32_t vi_arr[4];\n        _mm_store_si128((__m128i*)mask_arr, mask_total_i);\n        _mm_store_si128((__m128i*)vi_arr, vi);\n        \n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                int32_t val = vi_arr[j];\n                total += (uint64_t)val * (uint64_t)val;\n            }\n        }\n    }\n    \n    for (; i < n; i++) {\n        float x = lst[i];\n        if (x >= 0) {\n            int32_t val = (int32_t)x;\n            if ((float)val == x) {\n                if (val % 2 != 0) {\n                    total += (uint64_t)val * (uint64_t)val;\n                }\n            }\n        }\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t total = 0;\n    size_t i = 0;\n    size_t len = lst.size();\n\n    for (; i + 4 <= len; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask_pos = _mm_cmpgt_ps(vec, zero);\n        __m128i vi = _mm_cvtps_epi32(vec);\n        __m128 vec_round = _mm_cvtepi32_ps(vi);\n        __m128 mask_int = _mm_cmpeq_ps(vec, vec_round);\n        __m128 mask_valid = _mm_and_ps(mask_pos, mask_int);\n        __m128i mask_odd = _mm_cmpeq_epi32(_mm_and_si128(vi, _mm_set1_epi32(1)), _mm_set1_epi32(1));\n        __m128i mask_valid_int = _mm_castps_si128(mask_valid);\n        __m128i mask_final_int = _mm_and_si128(mask_valid_int, mask_odd);\n        \n        uint32_t vi_arr[4];\n        uint32_t mask_arr[4];\n        _mm_storeu_si128((__m128i*)vi_arr, vi);\n        _mm_storeu_si128((__m128i*)mask_arr, mask_final_int);\n        \n        for (int j = 0; j < 4; j++) {\n            if (mask_arr[j]) {\n                uint32_t val = vi_arr[j];\n                total += (uint64_t)val * (uint64_t)val;\n            }\n        }\n    }\n\n    for (; i < len; i++) {\n        float x = lst[i];\n        if (x > 0.0f && x == static_cast<float>(static_cast<int>(x))) {\n            int val = static_cast<int>(x);\n            if (val % 2 != 0) {\n                total += (uint64_t)val * (uint64_t)val;\n            }\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <cmath>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    if (lst.empty()) {\n        return 0;\n    }\n\n    size_t i = 0;\n    const size_t n = lst.size();\n    const size_t limit = n - 3;\n\n    __m128d sum_d = _mm_setzero_pd();\n\n    for (; i < limit; i += 4) {\n        __m128 x = _mm_loadu_ps(&lst[i]);\n\n        __m128 mask_pos = _mm_cmpgt_ps(x, _mm_setzero_ps());\n        __m128i x_int = _mm_cvtps_epi32(x);\n        __m128 x_floor = _mm_cvtepi32_ps(x_int);\n        __m128 mask_int = _mm_cmpeq_ps(x, x_floor);\n        __m128i and_res = _mm_and_si128(x_int, _mm_set1_epi32(1));\n        __m128i mask_odd_i = _mm_cmpeq_epi32(and_res, _mm_set1_epi32(1));\n        __m128 mask_odd = _mm_castsi128_ps(mask_odd_i);\n\n        __m128 mask_combined = _mm_and_ps(mask_pos, mask_int);\n        mask_combined = _mm_and_ps(mask_combined, mask_odd);\n\n        __m128 squares = _mm_mul_ps(x, x);\n        __m128 selected_squares = _mm_and_ps(squares, mask_combined);\n\n        __m128d low_d = _mm_cvtps_pd(selected_squares);\n        __m128 high_lane = _mm_movehl_ps(selected_squares, selected_squares);\n        __m128d high_d = _mm_cvtps_pd(high_lane);\n\n        sum_d = _mm_add_pd(sum_d, low_d);\n        sum_d = _mm_add_pd(sum_d, high_d);\n    }\n\n    double scalar_sum = 0.0;\n    for (; i < n; i++) {\n        float x = lst[i];\n        if (x > 0.0f) {\n            float int_part;\n            float frac = std::modff(x, &int_part);\n            if (frac == 0.0f) {\n                uint64_t val = static_cast<uint64_t>(int_part);\n                if (val & 1) {\n                    scalar_sum += static_cast<double>(val) * static_cast<double>(val);\n                }\n            }\n        }\n    }\n\n    __m128d temp = _mm_unpackhi_pd(sum_d, sum_d);\n    sum_d = _mm_add_sd(sum_d, temp);\n    double total = _mm_cvtsd_f64(sum_d) + scalar_sum;\n\n    return static_cast<uint64_t>(total);\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    if (n == 0) \n        return result;\n\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 3 < n; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i const*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i const*)&guess[i]);\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i mask = _mm_cmpgt_epi32(zero, diff);\n        __m128i abs_diff = _mm_sub_epi32(_mm_xor_si128(diff, mask), mask);\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = diff < 0 ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 3 < n; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n        \n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i mask = _mm_srai_epi32(diff, 31);\n        __m128i abs_diff = _mm_sub_epi32(_mm_xor_si128(diff, mask), mask);\n        \n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = diff < 0 ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE2\n    for (; i + 3 < n; i += 4) {\n        __m128i v_game = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i v_guess = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n        __m128i diff = _mm_sub_epi32(v_game, v_guess);\n        __m128i mask = _mm_srai_epi32(diff, 31);\n        __m128i abs_diff = _mm_sub_epi32(_mm_xor_si128(diff, mask), mask);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff < 0) ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    size_t i = 0;\n    const size_t simd_width = 4;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m128i game_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i guess_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i mask = _mm_srai_epi32(diff, 31);\n        __m128i abs_diff = _mm_sub_epi32(_mm_xor_si128(diff, mask), mask);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff < 0) ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i game_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i guess_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i mask = _mm_srai_epi32(diff, 31);\n        __m128i abs_diff = _mm_sub_epi32(_mm_xor_si128(diff, mask), mask);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = diff < 0 ? -diff : diff;\n    }\n\n    return result;\n}\n```"}

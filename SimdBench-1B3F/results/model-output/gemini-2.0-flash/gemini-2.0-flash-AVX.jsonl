{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to:\n- A non-empty source array of 64-bit integers (src);\n- A non-empty destination array of 64-bit integers (dst);\n- A non-empty mask array of boolean values (mask);\n- Their common length (length).\n\nYour goal is to conditionally copy elements from src to dst where the corresponding mask value is true.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((const __m256i*)(dst + i));\n\n        int mask_int = (mask[i] << 0) | (mask[i + 1] << 1) | (mask[i + 2] << 2) | (mask[i + 3] << 3);\n        __m128i mask_vec = _mm_cvtsi32_si128(mask_int);\n        __m256i mask_256 = _mm256_cvtepi32_epi64(_mm_cvtsi32_si128(mask_int));\n\n        __m256i blend_mask = _mm256_cmpgt_epi64(mask_256, _mm256_setzero_si256());\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, blend_mask);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to:\n- A non-empty source array of 64-bit integers (src);\n- A non-empty destination array of 64-bit integers (dst);\n- A non-empty mask array of boolean values (mask);\n- Their common length (length).\n\nYour goal is to conditionally copy elements from src to dst where the corresponding mask value is true.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((const __m256i*)(dst + i));\n\n        int mask_int[8];\n        for (int j = 0; j < 8; ++j) {\n            mask_int[j] = -((mask + i)[j] != 0);\n        }\n\n        __m256i mask_vec = _mm256_loadu_si256((const __m256i*)mask_int);\n        \n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to:\n- A non-empty source array of 64-bit integers (src);\n- A non-empty destination array of 64-bit integers (dst);\n- A non-empty mask array of boolean values (mask);\n- Their common length (length).\n\nYour goal is to conditionally copy elements from src to dst where the corresponding mask value is true.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((const __m256i*)(dst + i));\n\n        int mask_int = (mask[i] << 0) | (mask[i + 1] << 1) | (mask[i + 2] << 2) | (mask[i + 3] << 3);\n        __m128i mask_vec = _mm_cvtsi32_si128(mask_int);\n        __m256i blend_mask = _mm256_cvtepi32_epi64(_mm_cvtepi8_epi32(_mm_slli_epi64(mask_vec, 7)));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to:\n- A non-empty source array of 64-bit integers (src);\n- A non-empty destination array of 64-bit integers (dst);\n- A non-empty mask array of boolean values (mask);\n- Their common length (length).\n\nYour goal is to conditionally copy elements from src to dst where the corresponding mask value is true.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((const __m256i*)(dst + i));\n\n        int mask_int = (mask[i] << 0) | (mask[i + 1] << 1) | (mask[i + 2] << 2) | (mask[i + 3] << 3);\n        __m128i mask_vec = _mm_cvtsi32_si128(mask_int);\n        __m256i mask_256 = _mm256_cvtepi32_epi64(_mm_cvtsi128_si32(mask_vec));\n\n        __m256i blend_mask = _mm256_set_epi64x(\n            (mask[i+3] ? -1 : 0),\n            (mask[i+2] ? -1 : 0),\n            (mask[i+1] ? -1 : 0),\n            (mask[i] ? -1 : 0)\n        );\n\n        dst_vec = _mm256_blendv_epi8(dst_vec, src_vec, blend_mask);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to:\n- A non-empty source array of 64-bit integers (src);\n- A non-empty destination array of 64-bit integers (dst);\n- A non-empty mask array of boolean values (mask);\n- Their common length (length).\n\nYour goal is to conditionally copy elements from src to dst where the corresponding mask value is true.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((const __m128i*)&mask[i]);\n        \n        int mask_int = _mm_movemask_epi8(mask_vec);\n\n        if (mask_int == 0xFFFF) {\n            _mm256_storeu_si256((__m256i*)&dst[i], src_vec);\n        } else if (mask_int != 0) {\n            if (mask[i]) dst[i] = src[i];\n            if (mask[i+1]) dst[i+1] = src[i+1];\n            if (mask[i+2]) dst[i+2] = src[i+2];\n            if (mask[i+3]) dst[i+3] = src[i+3];\n        }\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 32-bit signed integer arrays (src & dst),\na scaling factor (scale), and the arrays' common length (length).\nYour goal is to perform the following operations on each element of src and store results in dst:\n1. Calculate tmp1 as (element * scale) right-shifted by 3 bits;\n2. Calculate tmp2 as (element * scale) left-shifted by 3 bits;\n3. Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7;\n4. Finally store the result in dst.\n\nOur input guarantee that no overflow will occur during the calculations.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    int scale_arr[8];\n    for (int j = 0; j < 8; j++) {\n        scale_arr[j] = scale;\n    }\n    __m256i scale_vec = _mm256_loadu_si256((__m256i*)scale_arr);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i prod_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1_vec = _mm256_srai_epi32(prod_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(prod_vec, 3);\n        __m256i tmp1_tmp2_vec = _mm256_mullo_epi32(tmp1_vec, tmp2_vec);\n        __m256i sum_vec = _mm256_add_epi32(prod_vec, tmp1_tmp2_vec);\n\n        int temp_arr[8];\n        _mm256_storeu_si256((__m256i*)temp_arr, sum_vec);\n        for (int j = 0; j < 8; j++) {\n            dst[i + j] = temp_arr[j] / 7;\n        }\n    }\n\n    for (; i < length; i++) {\n        int prod = src[i] * scale;\n        int tmp1 = prod >> 3;\n        int tmp2 = prod << 3;\n        dst[i] = (prod + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 32-bit signed integer arrays (src & dst),\na scaling factor (scale), and the arrays' common length (length).\nYour goal is to perform the following operations on each element of src and store results in dst:\n1. Calculate tmp1 as (element * scale) right-shifted by 3 bits;\n2. Calculate tmp2 as (element * scale) left-shifted by 3 bits;\n3. Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7;\n4. Finally store the result in dst.\n\nOur input guarantee that no overflow will occur during the calculations.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    int scale_arr[8];\n    for (int j = 0; j < 8; ++j) {\n        scale_arr[j] = scale;\n    }\n    __m256i scale_vec = _mm256_loadu_si256((const __m256i*)scale_arr);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i prod_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n\n        __m256i tmp1_vec = _mm256_srai_epi32(prod_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(prod_vec, 3);\n\n        __m256i tmp1_tmp2_vec = _mm256_mullo_epi32(tmp1_vec, tmp2_vec);\n        __m256i sum_vec = _mm256_add_epi32(prod_vec, tmp1_tmp2_vec);\n\n        __m256i seven_vec = _mm256_set1_epi32(7);\n        __m256i result_vec = _mm256_div_epi32(sum_vec, seven_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        int prod = src[i] * scale;\n        int tmp1 = prod >> 3;\n        int tmp2 = prod << 3;\n        dst[i] = (prod + tmp1 * tmp2) / 7;\n    }\n}\n\nstatic __m256i _mm256_div_epi32(__m256i a, __m256i b) {\n    int a_arr[8];\n    int b_arr[8];\n    int result_arr[8];\n    _mm256_storeu_si256((__m256i*)a_arr, a);\n    _mm256_storeu_si256((__m256i*)b_arr, b);\n    for (int i = 0; i < 8; ++i) {\n        result_arr[i] = a_arr[i] / b_arr[i];\n    }\n    return _mm256_loadu_si256((const __m256i*)result_arr);\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 32-bit signed integer arrays (src & dst),\na scaling factor (scale), and the arrays' common length (length).\nYour goal is to perform the following operations on each element of src and store results in dst:\n1. Calculate tmp1 as (element * scale) right-shifted by 3 bits;\n2. Calculate tmp2 as (element * scale) left-shifted by 3 bits;\n3. Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7;\n4. Finally store the result in dst.\n\nOur input guarantee that no overflow will occur during the calculations.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    int scale_arr[8];\n    for (int j = 0; j < 8; ++j) {\n        scale_arr[j] = scale;\n    }\n    __m256i scale_vec = _mm256_loadu_si256((const __m256i*)scale_arr);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i prod_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n\n        __m256i tmp1_vec = _mm256_srai_epi32(prod_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(prod_vec, 3);\n\n        __m256i tmp1_mul_tmp2 = _mm256_mullo_epi32(tmp1_vec, tmp2_vec);\n        __m256i sum_vec = _mm256_add_epi32(prod_vec, tmp1_mul_tmp2);\n\n        __m256i seven_vec = _mm256_set1_epi32(7);\n        __m256i result_vec = _mm256_div_epi32(sum_vec, seven_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        int prod = src[i] * scale;\n        int tmp1 = prod >> 3;\n        int tmp2 = prod << 3;\n        dst[i] = (prod + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 32-bit signed integer arrays (src & dst),\na scaling factor (scale), and the arrays' common length (length).\nYour goal is to perform the following operations on each element of src and store results in dst:\n1. Calculate tmp1 as (element * scale) right-shifted by 3 bits;\n2. Calculate tmp2 as (element * scale) left-shifted by 3 bits;\n3. Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7;\n4. Finally store the result in dst.\n\nOur input guarantee that no overflow will occur during the calculations.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    int scale_arr[8];\n    for (int j = 0; j < 8; ++j) {\n        scale_arr[j] = scale;\n    }\n    __m256i scale_vec = _mm256_loadu_si256((__m256i*)scale_arr);\n\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i prod_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1 = _mm256_srai_epi32(prod_vec, 3);\n        __m256i tmp2 = _mm256_slli_epi32(prod_vec, 3);\n        __m256i tmp1_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum_vec = _mm256_add_epi32(prod_vec, tmp1_tmp2);\n\n        int* sum_arr = (int*)&sum_vec;\n        int res_arr[8];\n        for(int j = 0; j < 8; ++j){\n            res_arr[j] = sum_arr[j] / 7;\n        }\n\n        __m256i res_vec = _mm256_loadu_si256((__m256i*)res_arr);\n        _mm256_storeu_si256((__m256i*)(dst + i), res_vec);\n    }\n\n    for (; i < length; ++i) {\n        int prod = src[i] * scale;\n        int tmp1 = prod >> 3;\n        int tmp2 = prod << 3;\n        dst[i] = (prod + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 32-bit signed integer arrays (src & dst),\na scaling factor (scale), and the arrays' common length (length).\nYour goal is to perform the following operations on each element of src and store results in dst:\n1. Calculate tmp1 as (element * scale) right-shifted by 3 bits;\n2. Calculate tmp2 as (element * scale) left-shifted by 3 bits;\n3. Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7;\n4. Finally store the result in dst.\n\nOur input guarantee that no overflow will occur during the calculations.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    int scale_vec[8] = {scale, scale, scale, scale, scale, scale, scale, scale};\n    __m256i scale_256 = _mm256_loadu_si256((__m256i*)scale_vec);\n\n    for (; i + 7 < length; i += 8) {\n        __m256i src_256 = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i element_scale = _mm256_mullo_epi32(src_256, scale_256);\n        \n        __m256i tmp1 = _mm256_srai_epi32(element_scale, 3);\n        __m256i tmp2 = _mm256_slli_epi32(element_scale, 3);\n        \n        __m256i tmp1_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i element_scale_tmp1_tmp2 = _mm256_add_epi32(element_scale, tmp1_tmp2);\n\n        __m256i result = _mm256_div_epi32_int(element_scale_tmp1_tmp2, _mm256_set1_epi32(7));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int element_scale = element * scale;\n        int tmp1 = element_scale >> 3;\n        int tmp2 = element_scale << 3;\n        dst[i] = (element_scale + tmp1 * tmp2) / 7;\n    }\n}\n\n__m256i _mm256_div_epi32_int(__m256i a, __m256i b) {\n    int quotient[8];\n    int divisor[8];\n    _mm256_storeu_si256((__m256i*)divisor, b);\n    int* dividend = (int*)&a;\n\n    for (int i = 0; i < 8; i++) {\n        quotient[i] = dividend[i] / divisor[i];\n    }\n    return _mm256_loadu_si256((__m256i*)quotient);\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    if (cols >= 4 && stride < cols) {\n        for (; i + 3 < rows; i += 4) {\n            __m256d v1 = _mm256_loadu_pd(&src[(i + 0) * cols + stride]);\n            __m256d v2 = _mm256_loadu_pd(&src[(i + 1) * cols + stride]);\n            __m256d v3 = _mm256_loadu_pd(&src[(i + 2) * cols + stride]);\n            __m256d v4 = _mm256_loadu_pd(&src[(i + 3) * cols + stride]);\n\n            double temp[4];\n            temp[0] = ((double*)&v1)[0];\n            temp[1] = ((double*)&v2)[0];\n            temp[2] = ((double*)&v3)[0];\n            temp[3] = ((double*)&v4)[0];\n\n            _mm256_storeu_pd((double*)&dst[i], _mm256_loadu_pd(temp));\n        }\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 64-bit floating-point arrays (src & dst),\nthe number of rows (rows), the number of columns (cols), and a stride value (stride).\nEach array is a 1D representation of a 2D matrix with rows * cols elements.\nYour goal is to perform the following operations:\nFor each row r from 0 to rows-1:\nLoad the element at position (r * cols + stride) from src array, \nStore it in position r of dst array\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        __m256d src_indices = _mm256_set_pd(\n            (double)((i + 3) * cols + stride),\n            (double)((i + 2) * cols + stride),\n            (double)((i + 1) * cols + stride),\n            (double)((i + 0) * cols + stride)\n        );\n\n        double indices_array[4];\n        _mm256_storeu_pd(indices_array, src_indices);\n\n        __m256d values = _mm256_set_pd(\n            src[(size_t)indices_array[3]],\n            src[(size_t)indices_array[2]],\n            src[(size_t)indices_array[1]],\n            src[(size_t)indices_array[0]]\n        );\n\n        _mm256_storeu_pd(&dst[i], values);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 64-bit floating-point arrays (src & dst),\nthe number of rows (rows), the number of columns (cols), and a stride value (stride).\nEach array is a 1D representation of a 2D matrix with rows * cols elements.\nYour goal is to perform the following operations:\nFor each row r from 0 to rows-1:\nLoad the element at position (r * cols + stride) from src array, \nStore it in position r of dst array\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        __m256d src_indices = _mm256_set_pd(\n            (double)((i + 3) * cols + stride),\n            (double)((i + 2) * cols + stride),\n            (double)((i + 1) * cols + stride),\n            (double)((i + 0) * cols + stride)\n        );\n\n        double indices_array[4];\n        _mm256_storeu_pd(indices_array, src_indices);\n\n        __m256d values = _mm256_set_pd(\n            src[(size_t)indices_array[3]],\n            src[(size_t)indices_array[2]],\n            src[(size_t)indices_array[1]],\n            src[(size_t)indices_array[0]]\n        );\n\n        _mm256_storeu_pd(&dst[i], values);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 64-bit floating-point arrays (src & dst),\nthe number of rows (rows), the number of columns (cols), and a stride value (stride).\nEach array is a 1D representation of a 2D matrix with rows * cols elements.\nYour goal is to perform the following operations:\nFor each row r from 0 to rows-1:\nLoad the element at position (r * cols + stride) from src array, \nStore it in position r of dst array\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        __m256d src_indices = _mm256_set_pd(\n            (double)((i + 3) * cols + stride),\n            (double)((i + 2) * cols + stride),\n            (double)((i + 1) * cols + stride),\n            (double)((i + 0) * cols + stride)\n        );\n\n        double indices_array[4];\n        _mm256_storeu_pd(indices_array, src_indices);\n\n        __m256d values = _mm256_set_pd(\n            src[(size_t)indices_array[3]],\n            src[(size_t)indices_array[2]],\n            src[(size_t)indices_array[1]],\n            src[(size_t)indices_array[0]]\n        );\n\n        _mm256_storeu_pd(&dst[i], values);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are: pointers to two non-empty 64-bit floating-point arrays (src & dst),\nthe number of rows (rows), the number of columns (cols), and a stride value (stride).\nEach array is a 1D representation of a 2D matrix with rows * cols elements.\nYour goal is to perform the following operations:\nFor each row r from 0 to rows-1:\nLoad the element at position (r * cols + stride) from src array, \nStore it in position r of dst array\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i + 3 < rows; i += 4) {\n        __m256d src_indices = _mm256_set_pd(\n            (double)((i + 3) * cols + stride),\n            (double)((i + 2) * cols + stride),\n            (double)((i + 1) * cols + stride),\n            (double)((i + 0) * cols + stride)\n        );\n\n        double indices_array[4];\n        _mm256_storeu_pd(indices_array, src_indices);\n\n        __m256d values = _mm256_set_pd(\n            src[(size_t)indices_array[3]],\n            src[(size_t)indices_array[2]],\n            src[(size_t)indices_array[1]],\n            src[(size_t)indices_array[0]]\n        );\n\n        _mm256_storeu_pd(&dst[i], values);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a 32-bit non-empty floating-point source array, a 32-bit integer array of indices, and a 32-bit floating-point\ndestination array, all of the same length. \n\nThe function performs two operations:\n- First, it copies values from the source array to the destination array using the indices \nas lookups (gather operation);\n- Then it copies values from the source array to the destination \narray using the indices as write locations (scatter operation).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 gathered_values = _mm256_i32gather_ps(src, index_vec, 4);\n        _mm256_storeu_ps(&dst[i], gathered_values);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 values_to_scatter = _mm256_loadu_ps(&src[i]);\n        \n        int idx[8];\n        _mm256_storeu_si256((__m256i*)idx, index_vec);\n\n        dst[idx[0]] = src[i + 0];\n        dst[idx[1]] = src[i + 1];\n        dst[idx[2]] = src[i + 2];\n        dst[idx[3]] = src[i + 3];\n        dst[idx[4]] = src[i + 4];\n        dst[idx[5]] = src[i + 5];\n        dst[idx[6]] = src[i + 6];\n        dst[idx[7]] = src[i + 7];\n    }\n\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a 32-bit non-empty floating-point source array, a 32-bit integer array of indices, and a 32-bit floating-point\ndestination array, all of the same length. \n\nThe function performs two operations:\n- First, it copies values from the source array to the destination array using the indices \nas lookups (gather operation);\n- Then it copies values from the source array to the destination \narray using the indices as write locations (scatter operation).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 values = _mm256_loadu_ps(&src[i]);\n        \n        float temp_values[8];\n        _mm256_storeu_ps(temp_values, values);\n\n        int temp_indices[8];\n        _mm256_storeu_si256((__m256i*)temp_indices, idx);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp_indices[j] >= 0 && temp_indices[j] < length) {\n                dst[temp_indices[j]] = temp_values[j];\n            }\n        }\n    }\n    for (; i < length; ++i) {\n        if (indices[i] >= 0 && indices[i] < length) {\n            dst[indices[i]] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a 32-bit non-empty floating-point source array, a 32-bit integer array of indices, and a 32-bit floating-point\ndestination array, all of the same length. \n\nThe function performs two operations:\n- First, it copies values from the source array to the destination array using the indices \nas lookups (gather operation);\n- Then it copies values from the source array to the destination \narray using the indices as write locations (scatter operation).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 gathered_values = _mm256_i32gather_ps(src, index_vec, 4);\n        _mm256_storeu_ps(&dst[i], gathered_values);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 values_to_scatter = _mm256_loadu_ps(&src[i]);\n        \n        int idx[8];\n        _mm256_storeu_si256((__m256i*)idx, index_vec);\n        \n        dst[idx[0]] = src[i + 0];\n        dst[idx[1]] = src[i + 1];\n        dst[idx[2]] = src[i + 2];\n        dst[idx[3]] = src[i + 3];\n        dst[idx[4]] = src[i + 4];\n        dst[idx[5]] = src[i + 5];\n        dst[idx[6]] = src[i + 6];\n        dst[idx[7]] = src[i + 7];\n    }\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a 32-bit non-empty floating-point source array, a 32-bit integer array of indices, and a 32-bit floating-point\ndestination array, all of the same length. \n\nThe function performs two operations:\n- First, it copies values from the source array to the destination array using the indices \nas lookups (gather operation);\n- Then it copies values from the source array to the destination \narray using the indices as write locations (scatter operation).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 gathered_values = _mm256_i32gather_ps(src, index_vec, 4);\n        _mm256_storeu_ps(&dst[i], gathered_values);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i index_vec = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 values_to_scatter = _mm256_loadu_ps(&src[i]);\n        \n        int idx[8];\n        _mm256_storeu_si256((__m256i*)idx, index_vec);\n        \n        dst[idx[0]] = src[i + 0];\n        dst[idx[1]] = src[i + 1];\n        dst[idx[2]] = src[i + 2];\n        dst[idx[3]] = src[i + 3];\n        dst[idx[4]] = src[i + 4];\n        dst[idx[5]] = src[i + 5];\n        dst[idx[6]] = src[i + 6];\n        dst[idx[7]] = src[i + 7];\n    }\n\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a 32-bit non-empty floating-point source array, a 32-bit integer array of indices, and a 32-bit floating-point\ndestination array, all of the same length. \n\nThe function performs two operations:\n- First, it copies values from the source array to the destination array using the indices \nas lookups (gather operation);\n- Then it copies values from the source array to the destination \narray using the indices as write locations (scatter operation).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)&indices[i]);\n        __m256 values = _mm256_loadu_ps(&src[i]);\n        \n        int idx_arr[8];\n        float val_arr[8];\n        _mm256_storeu_si256((__m256i*)idx_arr, idx);\n        _mm256_storeu_ps(val_arr, values);\n\n        for(int j = 0; j < 8; ++j) {\n            if(idx_arr[j] >= 0 && idx_arr[j] < length)\n                dst[idx_arr[j]] = val_arr[j];\n        }\n    }\n    for (; i < length; ++i) {\n        if(indices[i] >= 0 && indices[i] < length)\n            dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 16-bit non-empty integer array (arr);\n- The 64-bit starting index (start);\n- The 64-bit ending index (end).\n\nYour goal is to reverse the elements in the array between the start and end indices (inclusive), the remaining elements should be unchanged.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t len = end - start + 1;\n    uint64_t num_iters = len / 8;\n\n    for (uint64_t i = 0; i < num_iters; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[start + i * 8]);\n        __m128i reversed_v = _mm_shuffle_epi8(v, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1], reversed_v);\n\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1]);\n        __m128i reversed_v2 = _mm_shuffle_epi8(v2, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[start + i * 8], reversed_v2);\n    }\n\n    for (uint64_t i = num_iters * 8; i < len - num_iters * 8; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i + start];\n        arr[end - i + start] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 16-bit non-empty integer array (arr);\n- The 64-bit starting index (start);\n- The 64-bit ending index (end).\n\nYour goal is to reverse the elements in the array between the start and end indices (inclusive), the remaining elements should be unchanged.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t num_elements = end - start + 1;\n    uint64_t num_vector_reversals = num_elements / 16;\n    uint64_t remaining_elements = num_elements % 16;\n\n    for (uint64_t i = 0; i < num_vector_reversals; ++i) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[start + i * 16]);\n\n        // Reverse the 16 elements within the vector\n        vec = _mm256_shuffle_epi8(vec, _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                                        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Store the reversed vector back into the array, swapping with the element at the other end\n        _mm256_storeu_si256((__m256i*)&arr[end - (i + 1) * 16 + 1], vec);\n    }\n\n    // Handle remaining elements (less than 16) using scalar operations\n    uint64_t left = start + num_vector_reversals * 16;\n    uint64_t right = end - num_vector_reversals * 16;\n\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 16-bit non-empty integer array (arr);\n- The 64-bit starting index (start);\n- The 64-bit ending index (end).\n\nYour goal is to reverse the elements in the array between the start and end indices (inclusive), the remaining elements should be unchanged.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t len = end - start + 1;\n    uint64_t num_iters = len / 8;\n\n    for (uint64_t i = 0; i < num_iters; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[start + i * 8]);\n        __m128i reversed_v = _mm_shuffle_epi8(v, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1], reversed_v);\n\n        v = _mm_loadu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1]);\n        reversed_v = _mm_shuffle_epi8(v, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[start + i * 8], reversed_v);\n    }\n\n    for (uint64_t i = num_iters * 8; i < len - num_iters * 8; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i + start];\n        arr[end - i + start] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 16-bit non-empty integer array (arr);\n- The 64-bit starting index (start);\n- The 64-bit ending index (end).\n\nYour goal is to reverse the elements in the array between the start and end indices (inclusive), the remaining elements should be unchanged.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t len = end - start + 1;\n    uint64_t num_iters = len / 8;\n\n    for (uint64_t i = 0; i < num_iters; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i*)&arr[start + i * 8]);\n        __m128i reversed_v = _mm_shuffle_epi8(v, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1], reversed_v);\n\n        v = _mm_loadu_si128((__m128i*)&arr[end - (i + 1) * 8 + 1]);\n        reversed_v = _mm_shuffle_epi8(v, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        _mm_storeu_si128((__m128i*)&arr[start + i * 8], reversed_v);\n    }\n\n    for (uint64_t i = num_iters * 8; i < len - num_iters * 8; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i + start];\n        arr[end - i + start] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 16-bit non-empty integer array (arr);\n- The 64-bit starting index (start);\n- The 64-bit ending index (end).\n\nYour goal is to reverse the elements in the array between the start and end indices (inclusive), the remaining elements should be unchanged.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t num_elements = end - start + 1;\n    uint64_t num_vector_reversals = num_elements / 16;\n    uint64_t remaining_elements = num_elements % 16;\n\n    for (uint64_t i = 0; i < num_vector_reversals; ++i) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[start + i * 16]);\n\n        // Reverse the 16 elements within the vector\n        vec = _mm256_shuffle_epi8(vec, _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                                        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Store the reversed vector back into the array at the reversed position\n        _mm256_storeu_si256((__m256i*)&arr[end - (i + 1) * 16 + 1], vec);\n    }\n\n    // Handle remaining elements (less than 16)\n    uint64_t remaining_start = start + num_vector_reversals * 16;\n    uint64_t remaining_end = end - num_vector_reversals * 16;\n\n    for (uint64_t i = 0; i < remaining_elements / 2; ++i) {\n        int16_t temp = arr[remaining_start + i];\n        arr[remaining_start + i] = arr[remaining_end - i];\n        arr[remaining_end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 3D tensor stored in an non-empty 1D array (tensor);\n- The dimensions of the tensor (dim1, dim2, dim3);\n- The index of the slice to extract (slice_idx).\n\nYour goal is to extract a 2D slice from the 3D tensor (represented as a flattened 1D array in memory) \nalong the third dimension (dim3) at the given index (slice_idx).\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_start = slice_idx;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            size_t tensor_idx = i * dim2 * dim3 + j * dim3 + slice_idx;\n            *slice++ = tensor[tensor_idx];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 3D tensor stored in an non-empty 1D array (tensor);\n- The dimensions of the tensor (dim1, dim2, dim3);\n- The index of the slice to extract (slice_idx).\n\nYour goal is to extract a 2D slice from the 3D tensor (represented as a flattened 1D array in memory) \nalong the third dimension (dim3) at the given index (slice_idx).\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_start = slice_idx;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            size_t tensor_idx = i * dim2 * dim3 + j * dim3 + slice_idx;\n            *slice++ = tensor[tensor_idx];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i + 16]);\n        _mm256_storeu_si256((__m256i*)&slice[i], vec1);\n        _mm256_storeu_si256((__m256i*)&slice[i + 16], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i + 16]);\n        _mm256_storeu_si256((__m256i*)&slice[i], vec1);\n        _mm256_storeu_si256((__m256i*)&slice[i + 16], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&tensor[slice_offset + i + 16]);\n        _mm256_storeu_si256((__m256i*)&slice[i], vec1);\n        _mm256_storeu_si256((__m256i*)&slice[i + 16], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t block_row = 0; block_row < rows; block_row += block_size) {\n        for (size_t block_col = 0; block_col < cols; block_col += block_size) {\n            for (size_t i = block_row; i < block_row + block_size && i < rows; ++i) {\n                for (size_t j = block_col; j < block_col + block_size && j < cols; j += 8) {\n                    __m128i row_data = _mm_loadu_si128((__m128i*)&src[i * cols + j]);\n                    __m128i transposed_data = _mm_unpacklo_epi16(row_data, _mm_setzero_si128());\n                    _mm_storeu_si128((__m128i*)&dst[j * rows + i], transposed_data);\n                    transposed_data = _mm_unpackhi_epi16(row_data, _mm_setzero_si128());\n                    _mm_storeu_si128((__m128i*)&dst[(j + 4) * rows + i], transposed_data);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t bi = 0; bi < block_size && (i + bi) < rows; ++bi) {\n                for (size_t bj = 0; bj < block_size && (j + bj) < cols; bj += 8) {\n                    __m128i row = _mm_loadu_si128((__m128i*)&src[(i + bi) * cols + (j + bj)]);\n                    _mm_storeu_si128((__m128i*)&dst[(j + bj) * rows + (i + bi)], row);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t block_row = 0; block_row < rows; block_row += block_size) {\n        for (size_t block_col = 0; block_col < cols; block_col += block_size) {\n            for (size_t i = block_row; i < block_row + block_size && i < rows; ++i) {\n                for (size_t j = block_col; j < block_col + block_size && j < cols; j += 8) {\n                    __m128i row_data = _mm_loadu_si128((__m128i*)&src[i * cols + j]);\n                    __m128i transposed_data = _mm_unpacklo_epi16(row_data, _mm_setzero_si128());\n                    _mm_storeu_si128((__m128i*)&dst[j * rows + i], transposed_data);\n                    if (j + 4 < cols) {\n                        transposed_data = _mm_unpackhi_epi16(row_data, _mm_setzero_si128());\n                        _mm_storeu_si128((__m128i*)&dst[(j + 4) * rows + i], transposed_data);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- The block size (block_size).\n\nThe matrix src and dst are represented as 1D arrays in memory.\nYour goal is to transpose the matrix in a blocked manner.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t x = i; x < std::min(i + block_size, rows); ++x) {\n                for (size_t y = j; y < std::min(j + block_size, cols); ++y) {\n                    dst[y * rows + x] = src[x * cols + y];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- The block size (block_size).\n\nThe matrix src and dst are represented as 1D arrays in memory.\nYour goal is to transpose the matrix in a blocked manner.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t x = i; x < std::min(i + block_size, rows); ++x) {\n                for (size_t y = j; y < std::min(j + block_size, cols); ++y) {\n                    dst[y * rows + x] = src[x * cols + y];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 32-bit float array (array), representing a 3D array of dimensions dim*dim*dim, flattened into a 1D array;\n- The size of each dimension (dim).\n\nYour goal is to calculate the sum of the diagonal elements (i.e., the ones where the three indices are equal) in a 3D array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // AVX2 implementation\n    if (dim >= 8) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        for (; i < dim - 7; i += 8) {\n            size_t index = i * dim * dim + i * dim + i;\n            __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n\n            float temp_array[8];\n            for(int j=0; j<8; ++j){\n                temp_array[j] = array[(i+j) * dim * dim + (i+j) * dim + (i+j)];\n            }\n            diagonal_elements = _mm256_loadu_ps(temp_array);\n\n            sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Scalar implementation for remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 32-bit float array (array), representing a 3D array of dimensions dim*dim*dim, flattened into a 1D array;\n- The size of each dimension (dim).\n\nYour goal is to calculate the sum of the diagonal elements (i.e., the ones where the three indices are equal) in a 3D array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // AVX2 implementation\n    if (dim >= 8) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            size_t index = i * dim * dim + i * dim + i;\n            __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n\n            // Extract the diagonal elements using a shuffle operation\n            __m256 indices = _mm256_set_ps(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n            __m256i mask = _mm256_cvttps_epi32(indices);\n            __m256 diagonal_values = _mm256_i32gather_ps(array, mask, dim * dim + dim + 1);\n\n            sum_vec = _mm256_add_ps(sum_vec, diagonal_values);\n        }\n\n        // Horizontal sum of the vector\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Scalar implementation for remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 32-bit float array (array), representing a 3D array of dimensions dim*dim*dim, flattened into a 1D array;\n- The size of each dimension (dim).\n\nYour goal is to calculate the sum of the diagonal elements (i.e., the ones where the three indices are equal) in a 3D array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // AVX2 implementation\n    if (dim >= 8) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            size_t index = i * dim * dim + i * dim + i;\n            __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n\n            float temp_array[8];\n            for(int j = 0; j < 8; ++j){\n                temp_array[j] = array[(i+j) * dim * dim + (i+j) * dim + (i+j)];\n            }\n            diagonal_elements = _mm256_loadu_ps(temp_array);\n            sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Scalar implementation for remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 32-bit float array (array), representing a 3D array of dimensions dim*dim*dim, flattened into a 1D array;\n- The size of each dimension (dim).\n\nYour goal is to calculate the sum of the diagonal elements (i.e., the ones where the three indices are equal) in a 3D array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (i = 0; i + 7 < dim; i += 8) {\n        float temp_arr[8];\n        for (int j = 0; j < 8; ++j) {\n            size_t index = (i + j) * dim * dim + (i + j) * dim + (i + j);\n            temp_arr[j] = array[index];\n        }\n        __m256 data_vec = _mm256_loadu_ps(temp_arr);\n        sum_vec = _mm256_add_ps(sum_vec, data_vec);\n    }\n\n    float temp_sum[8];\n    _mm256_storeu_ps(temp_sum, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp_sum[j];\n    }\n\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* The inputs to this function are::\n- A pointer to a non-empty 32-bit float array (array), representing a 3D array of dimensions dim*dim*dim, flattened into a 1D array;\n- The size of each dimension (dim).\n\nYour goal is to calculate the sum of the diagonal elements (i.e., the ones where the three indices are equal) in a 3D array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // AVX2 implementation\n    if (dim >= 8) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            size_t index = i * dim * dim + i * dim + i;\n            __m256 diagonal_elements = _mm256_loadu_ps(&array[index]);\n\n            float temp_array[8];\n            for(int j = 0; j < 8; ++j){\n                temp_array[j] = array[(i+j) * dim * dim + (i+j) * dim + (i+j)];\n            }\n            diagonal_elements = _mm256_loadu_ps(temp_array);\n            sum_vec = _mm256_add_ps(sum_vec, diagonal_elements);\n        }\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Scalar implementation for remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A threshold value (threshold);\n- A scale factor (scale).\n\nYour goal is to apply a conditional scaling operation on the source array.\nIf the value is greater than the threshold, scale it by the scale factor; otherwise, keep it unchanged.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    size_t vector_size = sizeof(__m256d) / sizeof(double);\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += vector_size) {\n            if (cols - j >= vector_size) {\n                __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n                __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n                __m256d result_vec = _mm256_blendv_pd(src_vec, _mm256_mul_pd(src_vec, scale_vec), mask);\n                _mm256_storeu_pd(&dst[i * cols + j], result_vec);\n            } else {\n                for (size_t k = j; k < cols; ++k) {\n                    if (src[i * cols + k] > threshold) {\n                        dst[i * cols + k] = src[i * cols + k] * scale;\n                    } else {\n                        dst[i * cols + k] = src[i * cols + k];\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A threshold value (threshold);\n- A scale factor (scale).\n\nYour goal is to apply a conditional scaling operation on the source array.\nIf the value is greater than the threshold, scale it by the scale factor; otherwise, keep it unchanged.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    size_t vector_size = sizeof(__m256d) / sizeof(double);\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += vector_size) {\n            if (cols - j >= vector_size) {\n                __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n                __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n                __m256d result_vec = _mm256_blendv_pd(src_vec, _mm256_mul_pd(src_vec, scale_vec), mask);\n                _mm256_storeu_pd(&dst[i * cols + j], result_vec);\n            } else {\n                for (size_t k = j; k < cols; ++k) {\n                    if (src[i * cols + k] > threshold) {\n                        dst[i * cols + k] = src[i * cols + k] * scale;\n                    } else {\n                        dst[i * cols + k] = src[i * cols + k];\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A threshold value (threshold);\n- A scale factor (scale).\n\nYour goal is to apply a conditional scaling operation on the source array.\nIf the value is greater than the threshold, scale it by the scale factor; otherwise, keep it unchanged.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    size_t vector_size = sizeof(__m256d) / sizeof(double);\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += vector_size) {\n            if (cols - j >= vector_size) {\n                __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n                __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OS);\n                __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n                __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n                _mm256_storeu_pd(&dst[i * cols + j], result_vec);\n            } else {\n                for (size_t k = j; k < cols; ++k) {\n                    dst[i * cols + k] = (src[i * cols + k] > threshold) ? (src[i * cols + k] * scale) : src[i * cols + k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A threshold value (threshold);\n- A scale factor (scale).\n\nYour goal is to apply a conditional scaling operation on the source array.\nIf the value is greater than the threshold, scale it by the scale factor; otherwise, keep it unchanged.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    size_t cols_simd = cols / 4 * 4;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n            __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n            __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n            _mm256_storeu_pd(&dst[i * cols + j], result_vec);\n        }\n\n        for (; j < cols; j++) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A threshold value (threshold);\n- A scale factor (scale).\n\nYour goal is to apply a conditional scaling operation on the source array.\nIf the value is greater than the threshold, scale it by the scale factor; otherwise, keep it unchanged.\nThe result should be stored in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    size_t vector_length = 4;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += vector_length) {\n            if (j + vector_length <= cols) {\n                __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n                __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n                __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n                __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n                _mm256_storeu_pd(&dst[i * cols + j], result_vec);\n            } else {\n                for (size_t k = j; k < cols; k++) {\n                    if (src[i * cols + k] > threshold) {\n                        dst[i * cols + k] = src[i * cols + k] * scale;\n                    } else {\n                        dst[i * cols + k] = src[i * cols + k];\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix stored in an 1D array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix stored in an 1D array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A pointer to an array of indices (indices).\n\nThe matrix src and dst are matrix in shape (rows, cols) represented as 1D arrays in memory, and `indices` is a 1D array.\nYour goal is to reorder the rows of the source matrix based on the given indices.\nThe result should be stored in the destination matrix.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_by_2 = cols / 2;\n    size_t remainder = cols % 2;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t index = indices[i];\n        const double *src_row = src + index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < cols_by_2; ++j) {\n            __m128d v = _mm_loadu_pd(src_row + j * 2);\n            _mm_storeu_pd(dst_row + j * 2, v);\n        }\n\n        if (remainder) {\n            dst_row[cols - 1] = src_row[cols - 1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix stored in an 1D array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix stored in an 1D array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A pointer to an array of indices (indices).\n\nThe matrix src and dst are matrix in shape (rows, cols) represented as 1D arrays in memory, and `indices` is a 1D array.\nYour goal is to reorder the rows of the source matrix based on the given indices.\nThe result should be stored in the destination matrix.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_per_simd = sizeof(__m256d) / sizeof(double);\n    size_t cols_simd = cols / cols_per_simd;\n    size_t cols_rem = cols % cols_per_simd;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t index = indices[i];\n        const double *src_row = src + index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < cols_simd; ++j) {\n            __m256d v = _mm256_loadu_pd(src_row + j * cols_per_simd);\n            _mm256_storeu_pd(dst_row + j * cols_per_simd, v);\n        }\n\n        for (j = cols_simd * cols_per_simd; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix stored in an 1D array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix stored in an 1D array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A pointer to an array of indices (indices).\n\nThe matrix src and dst are matrix in shape (rows, cols) represented as 1D arrays in memory, and `indices` is a 1D array.\nYour goal is to reorder the rows of the source matrix based on the given indices.\nThe result should be stored in the destination matrix.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_per_simd = sizeof(__m256d) / sizeof(double);\n    size_t cols_simd = cols / cols_per_simd;\n    size_t cols_remainder = cols % cols_per_simd;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t index = indices[i];\n        const double *src_row = src + index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < cols_simd; ++j) {\n            __m256d src_vec = _mm256_loadu_pd(src_row + j * cols_per_simd);\n            _mm256_storeu_pd(dst_row + j * cols_per_simd, src_vec);\n        }\n\n        for (j = cols_simd * cols_per_simd; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix stored in an 1D array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix stored in an 1D array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A pointer to an array of indices (indices).\n\nThe matrix src and dst are matrix in shape (rows, cols) represented as 1D arrays in memory, and `indices` is a 1D array.\nYour goal is to reorder the rows of the source matrix based on the given indices.\nThe result should be stored in the destination matrix.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_per_simd = sizeof(__m256d) / sizeof(double);\n    size_t cols_simd = cols / cols_per_simd;\n    size_t cols_rem = cols % cols_per_simd;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t index = indices[i];\n        const double *src_row = src + index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < cols_simd; ++j) {\n            __m256d v = _mm256_loadu_pd(src_row + j * cols_per_simd);\n            _mm256_storeu_pd(dst_row + j * cols_per_simd, v);\n        }\n\n        for (j = cols_simd * cols_per_simd; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix stored in an 1D array (src);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix stored in an 1D array (dst);\n- The number of rows (rows);\n- The number of columns (cols);\n- A pointer to an array of indices (indices).\n\nThe matrix src and dst are matrix in shape (rows, cols) represented as 1D arrays in memory, and `indices` is a 1D array.\nYour goal is to reorder the rows of the source matrix based on the given indices.\nThe result should be stored in the destination matrix.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_per_simd = sizeof(__m256d) / sizeof(double);\n    size_t cols_simd = cols / cols_per_simd;\n    size_t cols_rem = cols % cols_per_simd;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t index = indices[i];\n        const double *src_row = src + index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < cols_simd; ++j) {\n            __m256d v = _mm256_loadu_pd(src_row + j * cols_per_simd);\n            _mm256_storeu_pd(dst_row + j * cols_per_simd, v);\n        }\n\n        for (j = cols_simd * cols_per_simd; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty source array of 64-bit integers (src1);\n- A pointer to a non-empty source array of 64-bit integers (src2);\n- A pointer to a non-empty destination array of 64-bit integers (dst);\n- A scalar integer value (scalar);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2, \nand then add the scalar value to each element of the result. \nFinally, divide the result by 2 (round-toward-zero).\nThe final result should be stored in the dst array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    const size_t avx_bound = length - length % 4;\n\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    for (i = 0; i < avx_bound; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i sum = _mm256_add_epi64(a, b);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        sum = _mm256_srai_epi64(sum, 1);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty source array of 64-bit integers (src1);\n- A pointer to a non-empty source array of 64-bit integers (src2);\n- A pointer to a non-empty destination array of 64-bit integers (dst);\n- A scalar integer value (scalar);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2, \nand then add the scalar value to each element of the result. \nFinally, divide the result by 2 (round-toward-zero).\nThe final result should be stored in the dst array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    const size_t avx_bound = length - length % 4;\n\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    for (i = 0; i < avx_bound; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i *)(src1 + i));\n        __m256i b = _mm256_loadu_si256((const __m256i *)(src2 + i));\n\n        __m256i sum = _mm256_add_epi64(a, b);\n        __m256i result = _mm256_add_epi64(sum, scalar_vec);\n        __m256i shifted = _mm256_srai_epi64(result, 1);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), shifted);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty source array of 64-bit integers (src1);\n- A pointer to a non-empty source array of 64-bit integers (src2);\n- A pointer to a non-empty destination array of 64-bit integers (dst);\n- A scalar integer value (scalar);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2, \nand then add the scalar value to each element of the result. \nFinally, divide the result by 2 (round-toward-zero).\nThe final result should be stored in the dst array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    const size_t avx_bound = length - length % 4;\n\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    for (i = 0; i < avx_bound; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i *)(src1 + i));\n        __m256i b = _mm256_loadu_si256((const __m256i *)(src2 + i));\n\n        __m256i sum = _mm256_add_epi64(a, b);\n        __m256i added_scalar = _mm256_add_epi64(sum, scalar_vec);\n        __m256i result = _mm256_srai_epi64(added_scalar, 1);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty source array of 64-bit integers (src1);\n- A pointer to a non-empty source array of 64-bit integers (src2);\n- A pointer to a non-empty destination array of 64-bit integers (dst);\n- A scalar integer value (scalar);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2, \nand then add the scalar value to each element of the result. \nFinally, divide the result by 2 (round-toward-zero).\nThe final result should be stored in the dst array.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    const size_t avx_bound = length - length % 4;\n\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    for (i = 0; i < avx_bound; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i *)(src1 + i));\n        __m256i b = _mm256_loadu_si256((const __m256i *)(src2 + i));\n\n        __m256i sum = _mm256_add_epi64(a, b);\n        __m256i added_scalar = _mm256_add_epi64(sum, scalar_vec);\n        __m256i result = _mm256_srai_epi64(added_scalar, 1);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i;\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    size_t vec_len = length / 4;\n\n    for (i = 0; i < vec_len * 4; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        __m256i result = _mm256_add_epi64(sum, scalar_vec);\n        result = _mm256_srli_epi64(result, 1);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find and return the minimum value among the even-numbered elements in the array..\nIf the array is empty, return -1.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    int min_val = src[0]; \n\n    if (length <= 8) {\n        for (size_t i = 2; i < length; i += 2) {\n            if (src[i] < min_val) {\n                min_val = src[i];\n            }\n        }\n        return min_val;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(min_val);\n\n    size_t i = 2;\n    for (; i + 8 <= length; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i vec2 = _mm256_loadu_si256((const __m256i*)(src + i + 8));\n\n        __m256i mask1 = _mm256_set_epi32(0,-1,0,-1,0,-1,0,-1);\n        __m256i mask2 = _mm256_set_epi32(0,-1,0,-1,0,-1,0,-1);\n\n        vec1 = _mm256_and_si256(vec1, mask1);\n        vec2 = _mm256_and_si256(vec2, mask2);\n\n        __m256i min_temp1 = _mm256_min_epi32(min_vec, vec1);\n        __m256i min_temp2 = _mm256_min_epi32(min_vec, vec2);\n        min_vec = _mm256_min_epi32(min_temp1, min_temp2);\n    }\n\n    int temp_arr[8];\n    _mm256_storeu_si256((__m256i*)temp_arr, min_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp_arr[j] < min_val) {\n            min_val = temp_arr[j];\n        }\n    }\n\n    for (; i < length; i += 2) {\n        if (src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1)); // Mask for even indices\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi32(INT_MAX), mask); // Set odd elements to INT_MAX\n        min_vec = _mm256_min_epi32(min_vec, data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0) {\n            min_vec = _mm256_min_epi32(min_vec, _mm256_set1_epi32(src[i]));\n        }\n    }\n\n    // Horizontal reduction to find the minimum value\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    \n    int min_values[8];\n    _mm256_storeu_si256((__m256i*)min_values, min_vec);\n    \n    int min_result = INT_MAX;\n    for (int j = 0; j < 4; j++) {\n        if (min_values[j] < min_result) {\n            min_result = min_values[j];\n        }\n    }\n\n    return min_result == INT_MAX ? -1 : min_result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1)); // Mask for even numbers\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi32(INT_MAX), mask); // Replace odd numbers with INT_MAX\n        min_vec = _mm256_min_epi32(min_vec, data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (src[i] % 2 == 0) {\n            min_vec = _mm256_min_epi32(min_vec, _mm256_set1_epi32(src[i]));\n        }\n    }\n\n    // Horizontal reduction to find the minimum\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    \n    int min_values[8];\n    _mm256_storeu_si256((__m256i*)min_values, min_vec);\n    \n    int final_min = INT_MAX;\n    for (int j = 0; j < 8; j++) {\n        if (min_values[j] < final_min) {\n            final_min = min_values[j];\n        }\n    }\n\n    return final_min == INT_MAX ? -1 : final_min;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT32_MAX);\n    size_t i;\n\n    for (i = 0; i < length / 8 * 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i even_mask = _mm256_and_si256(data, _mm256_set1_epi32(1));\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi32(INT32_MAX), even_mask);\n\n        min_vec = _mm256_min_epi32(min_vec, data);\n    }\n\n    int min_array[8];\n    _mm256_storeu_si256((__m256i*)min_array, min_vec);\n\n    int min_value = INT32_MAX;\n    for (int j = 0; j < 8; j++) {\n        if (min_array[j] < min_value) {\n            min_value = min_array[j];\n        }\n    }\n\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value == INT32_MAX ? -1 : min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1)); // mask for even numbers\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi32(INT_MAX), mask); // set odd numbers to INT_MAX\n        min_vec = _mm256_min_epi32(min_vec, data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0) {\n            min_vec = _mm256_min_epi32(min_vec, _mm256_set1_epi32(src[i]));\n        }\n    }\n\n    // Horizontal reduction to find the minimum\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    min_vec = _mm256_min_epi32(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    \n    int min_values[8];\n    _mm256_storeu_si256((__m256i*)min_values, min_vec);\n    \n    int min_result = INT_MAX;\n    for (int j = 0; j < 4; j++) {\n        if (min_values[j] < min_result) {\n            min_result = min_values[j];\n        }\n    }\n\n    return (min_result == INT_MAX) ? -1 : min_result;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    \n    for (i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_srai_epi32(src_vec, shift);\n        \n        // Logical right shift\n        __m256i unsigned_src_vec = _mm256_cvtepi32_epi64(src_vec);\n        __m256i logical_shifted = _mm256_srli_epi64(unsigned_src_vec, shift);\n        \n        // Cast back to int\n        logical_shifted = _mm256_cvtepi64_epi32(logical_shifted);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shifted, logical_shifted);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    for (i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_srai_epi32(src_vec, shift);\n        \n        // Logical right shift\n        __m256i unsigned_src_vec = _mm256_cvtepi32_epi64(src_vec);\n        __m256i logical_shifted = _mm256_srli_epi64(unsigned_src_vec, shift);\n        logical_shifted = _mm256_cvtepi64_epi32(logical_shifted);\n        \n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shifted, logical_shifted);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer source array (src);\n- A pointer to a 32-bit integer destination array (dst);\n- the number of each element to be shifted (shift);\n- The length of the array (length).\n\nYour goal:\n    First, calculate the arithmetic right shift of each element in the array by the given number of bits;\n    Then, calculate the logical right shift of each element in the array by the same number of bits,\n    converting each int to uint before shifting, and then casting the result back to int;\n    Finally, add the results of the arithmetic and logical shifts together and store them in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        for (; i <= length - 8; i += 8) {\n            __m256i src_vec = _mm256_loadu_si256((const __m256i *)&src[i]);\n            \n            // Arithmetic right shift\n            __m256i arith_shift_vec = _mm256_srai_epi32(src_vec, shift);\n            \n            // Logical right shift\n            __m256i uint_vec = _mm256_cvtepi32_epi32(_mm256_loadu_si256((const __m256i *)&src[i]));\n            __m256i logical_shift_vec = _mm256_srli_epi32(uint_vec, shift);\n            \n            // Add the results\n            __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n            \n            _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n        }\n    }\n    \n    // Process remaining elements\n    for (; i < length; ++i) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (int)((unsigned int)src[i] >> shift);\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer source array (src);\n- A pointer to a 32-bit integer destination array (dst);\n- the number of each element to be shifted (shift);\n- The length of the array (length).\n\nYour goal:\n    First, calculate the arithmetic right shift of each element in the array by the given number of bits;\n    Then, calculate the logical right shift of each element in the array by the same number of bits,\n    converting each int to uint before shifting, and then casting the result back to int;\n    Finally, add the results of the arithmetic and logical shifts together and store them in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        for (; i <= length - 8; i += 8) {\n            __m256i src_vec = _mm256_loadu_si256((const __m256i *)&src[i]);\n            \n            // Arithmetic right shift\n            __m256i arith_shift_vec = _mm256_srai_epi32(src_vec, shift);\n            \n            // Logical right shift\n            __m256i uint_vec = _mm256_cvtepi32_epi32(_mm256_loadu_si256((const __m256i *)&src[i]));\n            __m256i logical_shift_vec = _mm256_srli_epi32(uint_vec, shift);\n            \n            // Add the results\n            __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n            \n            _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n        }\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (int)((unsigned int)src[i] >> shift);\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer source array (src);\n- A pointer to a 32-bit integer destination array (dst);\n- the number of each element to be shifted (shift);\n- The length of the array (length).\n\nYour goal:\n    First, calculate the arithmetic right shift of each element in the array by the given number of bits;\n    Then, calculate the logical right shift of each element in the array by the same number of bits,\n    converting each int to uint before shifting, and then casting the result back to int;\n    Finally, add the results of the arithmetic and logical shifts together and store them in the destination array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    if (length >= 8) {\n        for (; i <= length - 8; i += 8) {\n            __m256i src_vec = _mm256_loadu_si256((const __m256i *)&src[i]);\n            \n            // Arithmetic right shift\n            __m256i arith_shift_vec = _mm256_srai_epi32(src_vec, shift);\n            \n            // Logical right shift\n            __m256i uint_vec = _mm256_cvtepi32_epi32(_mm256_loadu_si256((const __m256i *)&src[i]));\n            __m256i logical_shift_vec = _mm256_srli_epi32(uint_vec, shift);\n            \n            // Add the results\n            __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n            \n            _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n        }\n    }\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (int)(((unsigned int)src[i]) >> shift);\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a matrix stored in a non-empty 32-bit integer array (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to calculate the sum of all elements in the matrix.\nThe sum should be returned as a 64-bit integer value (the input is guaranteed not to cause an overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t sum = 0;\n\n    if (total_elements >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; i <= total_elements - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((const __m256i*)&matrix[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        int32_t temp_arr[8];\n        _mm256_storeu_si256((__m256i*)temp_arr, sum_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_arr[j];\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a matrix stored in a non-empty 32-bit integer array (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to calculate the sum of all elements in the matrix.\nThe sum should be returned as a 64-bit integer value (the input is guaranteed not to cause an overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((const __m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, data_vec);\n    }\n\n    int32_t sum_array[8];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n\n    int64_t total_sum = (int64_t)sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                          sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    for (; i < total_elements; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a matrix stored in a non-empty 32-bit integer array (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to calculate the sum of all elements in the matrix.\nThe sum should be returned as a 64-bit integer value (the input is guaranteed not to cause an overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((const __m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, data_vec);\n    }\n\n    int32_t sum_array[8];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n\n    int64_t total_sum = (int64_t)sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                          sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    for (; i < total_elements; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a matrix stored in a non-empty 32-bit integer array (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to calculate the sum of all elements in the matrix.\nThe sum should be returned as a 64-bit integer value (the input is guaranteed not to cause an overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((const __m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, data_vec);\n    }\n\n    int32_t sum_array[8];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n\n    int64_t total_sum = (int64_t)sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                          sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    for (; i < total_elements; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a matrix stored in a non-empty 32-bit integer array (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to calculate the sum of all elements in the matrix.\nThe sum should be returned as a 64-bit integer value (the input is guaranteed not to cause an overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n    int64_t sum = 0;\n\n    if (total_elements >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; i <= total_elements - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((const __m256i*)&matrix[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        int32_t temp_arr[8];\n        _mm256_storeu_si256((__m256i*)temp_arr, sum_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_arr[j];\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 8-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find the index of the maximum value in the array and return it.\nIf the input array is empty, return 0.\nThe index should be returned as a size_t value.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    if (length < 32) {\n        int8_t max_val = src[0];\n        size_t max_idx = 0;\n        for (size_t i = 1; i < length; ++i) {\n            if (src[i] > max_val) {\n                max_val = src[i];\n                max_idx = i;\n            }\n        }\n        return max_idx;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    __m256i idx_val = _mm256_set_epi32(31, 30, 29, 28, 27, 26, 25, 24);\n    idx_val = _mm256_add_epi32(idx_val, _mm256_set1_epi32(0));\n    __m256i max_idx = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(data, max_val);\n        __m256i new_max_val = _mm256_blendv_epi8(max_val, data, cmp);\n        __m256i new_max_idx = _mm256_blendv_epi8((__m256i)max_idx, (__m256i)_mm256_add_epi32(idx_val, _mm256_set1_epi32((int)i)), cmp);\n        max_val = new_max_val;\n        max_idx = new_max_idx;\n    }\n\n    int8_t max_arr[32];\n    int32_t idx_arr[8];\n    _mm256_storeu_si256((__m256i*)max_arr, max_val);\n    _mm256_storeu_si256((__m256i*)idx_arr, max_idx);\n\n    int8_t final_max_val = max_arr[0];\n    size_t final_max_idx = (size_t)idx_arr[0];\n    for (int j = 1; j < 32; ++j) {\n        if (max_arr[j] > final_max_val) {\n            final_max_val = max_arr[j];\n            final_max_idx = (size_t)idx_arr[j/4];\n            final_max_idx += (j%4)*8;\n        }\n    }\n    \n    for (; i < length; ++i) {\n        if (src[i] > final_max_val) {\n            final_max_val = src[i];\n            final_max_idx = i;\n        }\n    }\n\n    return final_max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 8-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find the index of the maximum value in the array and return it.\nIf the input array is empty, return 0.\nThe index should be returned as a size_t value.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    if (length < 32) {\n        int8_t max_val = src[0];\n        size_t max_index = 0;\n        for (size_t i = 1; i < length; ++i) {\n            if (src[i] > max_val) {\n                max_val = src[i];\n                max_index = i;\n            }\n        }\n        return max_index;\n    }\n\n    __m256i max_val_vec = _mm256_set1_epi8(src[0]);\n    __m256i max_index_vec = _mm256_set1_epi64x(0);\n    int8_t max_val = src[0];\n    size_t max_index = 0;\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp_vec = _mm256_cmpgt_epi8(data_vec, max_val_vec);\n        if (!_mm256_testz_si256(cmp_vec, cmp_vec)) {\n            unsigned int mask = _mm256_movemask_epi8(cmp_vec);\n            for (int j = 0; j < 32; ++j) {\n                if ((mask >> j) & 1) {\n                    if (src[i + j] > max_val) {\n                        max_val = src[i + j];\n                        max_index = i + j;\n                        max_val_vec = _mm256_set1_epi8(max_val);\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 8-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find the index of the maximum value in the array and return it.\nIf the input array is empty, return 0.\nThe index should be returned as a size_t value.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    if (length < 32) {\n        int8_t max_val = src[0];\n        size_t max_idx = 0;\n        for (size_t i = 1; i < length; ++i) {\n            if (src[i] > max_val) {\n                max_val = src[i];\n                max_idx = i;\n            }\n        }\n        return max_idx;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    __m256i idx_val = _mm256_set_epi32(31, 30, 29, 28, 27, 26, 25, 24);\n    idx_val = _mm256_add_epi32(idx_val, _mm256_set1_epi32(0));\n    __m256i max_idx = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(data, max_val);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            __m256i new_max_idx = _mm256_add_epi32(_mm256_set1_epi32(i), idx_val);\n            max_idx = _mm256_blendv_epi8(max_idx, new_max_idx, cmp);\n            max_val = _mm256_max_epi8(max_val, data);\n        }\n        idx_val = _mm256_add_epi32(idx_val, _mm256_set1_epi32(32));\n    }\n\n    int8_t max_arr[32];\n    int32_t idx_arr[8];\n    _mm256_storeu_si256((__m256i*)max_arr, max_val);\n    _mm256_storeu_si256((__m256i*)idx_arr, max_idx);\n\n    size_t final_max_idx = 0;\n    int8_t final_max_val = max_arr[0];\n    for (int j = 1; j < 32; ++j) {\n        if (max_arr[j] > final_max_val) {\n            final_max_val = max_arr[j];\n            final_max_idx = idx_arr[j/4];\n            if(j%4 == 1) final_max_idx += 1;\n            if(j%4 == 2) final_max_idx += 2;\n            if(j%4 == 3) final_max_idx += 3;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > final_max_val) {\n            final_max_val = src[i];\n            final_max_idx = i;\n        }\n    }\n\n    return final_max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 8-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find the index of the maximum value in the array and return it.\nIf the input array is empty, return 0.\nThe index should be returned as a size_t value.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    if (length < 32) {\n        int8_t max_val = src[0];\n        size_t max_idx = 0;\n        for (size_t i = 1; i < length; ++i) {\n            if (src[i] > max_val) {\n                max_val = src[i];\n                max_idx = i;\n            }\n        }\n        return max_idx;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    __m256i max_idx = _mm256_set_epi32(31, 30, 29, 28, 27, 26, 25, 24);\n    max_idx = _mm256_add_epi32(max_idx, _mm256_set1_epi32(0));\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(data, max_val);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            __m256i idx = _mm256_add_epi32(max_idx, _mm256_set1_epi32((int)i));\n            __m256i blend = _mm256_blendv_epi8(max_val, data, cmp);\n            max_val = blend;\n\n            __m256i idx_blend = _mm256_blendv_epi8((__m256i)max_idx, (__m256i)idx, cmp);\n            max_idx = idx_blend;\n        }\n    }\n\n    int8_t max_arr[32];\n    int32_t idx_arr[8];\n    _mm256_storeu_si256((__m256i*)max_arr, max_val);\n    _mm256_storeu_si256((__m256i*)idx_arr, max_idx);\n\n    size_t final_max_idx = idx_arr[0];\n    int8_t final_max_val = max_arr[0];\n    for (int j = 1; j < 32; ++j) {\n        if (max_arr[j] > final_max_val) {\n            final_max_val = max_arr[j];\n            final_max_idx = idx_arr[j / 4] + (j % 4);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > final_max_val) {\n            final_max_val = src[i];\n            final_max_idx = i;\n        }\n    }\n\n    return final_max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 8-bit integer source array (src);\n- The length of the array (length).\n\nYour goal is to find the index of the maximum value in the array and return it.\nIf the input array is empty, return 0.\nThe index should be returned as a size_t value.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    if (length < 32) {\n        int8_t max_val = src[0];\n        size_t max_idx = 0;\n        for (size_t i = 1; i < length; ++i) {\n            if (src[i] > max_val) {\n                max_val = src[i];\n                max_idx = i;\n            }\n        }\n        return max_idx;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    __m256i idx_val = _mm256_set_epi32(31, 30, 29, 28, 27, 26, 25, 24);\n    idx_val = _mm256_add_epi32(idx_val, _mm256_set1_epi32(0));\n    __m256i max_idx = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(data, max_val);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            __m256i new_max_idx = _mm256_add_epi32(_mm256_set1_epi32(i), idx_val);\n            max_idx = _mm256_blendv_epi8(max_idx, new_max_idx, cmp);\n            max_val = _mm256_max_epi8(max_val, data);\n        }\n        idx_val = _mm256_add_epi32(idx_val, _mm256_set1_epi32(32));\n    }\n\n    int8_t max_arr[32];\n    int32_t idx_arr[8];\n    _mm256_storeu_si256((__m256i*)max_arr, max_val);\n    _mm256_storeu_si256((__m256i*)idx_arr, max_idx);\n\n    size_t final_max_idx = 0;\n    int8_t final_max_val = max_arr[0];\n    for (int j = 1; j < 32; ++j) {\n        if (max_arr[j] > final_max_val) {\n            final_max_val = max_arr[j];\n            final_max_idx = idx_arr[j/4];\n            if (j % 4 == 1) final_max_idx += 1;\n            if (j % 4 == 2) final_max_idx += 2;\n            if (j % 4 == 3) final_max_idx += 3;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > final_max_val) {\n            final_max_val = src[i];\n            final_max_idx = i;\n        }\n    }\n\n    return final_max_idx;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint16_t source array (src);\n- A pointer to a non-empty uint16_t destination array (dst);\n- A uint32_t mask value (mask);\n- The length of the array (length).\n\nYour goal is to blend two arrays based on the following condition:\n    If the bitwise AND of the element's index and the mask is non-zero, take the value from src1; \n    otherwise, take the value from src2.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i idx = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8);\n            __m256i idx2 = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n\n            __m256i and_result = _mm256_and_si256(idx, mask_vec);\n            __m256i and_result2 = _mm256_and_si256(idx2, mask_vec);\n\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp_result = _mm256_cmpeq_epi32(and_result, zero);\n            __m256i cmp_result2 = _mm256_cmpeq_epi32(and_result2, zero);\n\n            uint32_t cmp_mask = _mm256_movemask_epi8(cmp_result);\n            uint32_t cmp_mask2 = _mm256_movemask_epi8(cmp_result2);\n\n            const uint16_t* s1 = src1 + i;\n            const uint16_t* s2 = src2 + i;\n            uint16_t* d = dst + i;\n\n            __m256i v_src1 = _mm256_loadu_si256((__m256i*)s1);\n            __m256i v_src2 = _mm256_loadu_si256((__m256i*)s2);\n\n            __m256i v_src1_2 = _mm256_loadu_si256((__m256i*)(s1 + 8));\n            __m256i v_src2_2 = _mm256_loadu_si256((__m256i*)(s2 + 8));\n\n            __m256i blended = _mm256_blendv_epi8(v_src2, v_src1, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask)));\n            __m256i blended2 = _mm256_blendv_epi8(v_src2_2, v_src1_2, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask2)));\n\n            _mm256_storeu_si256((__m256i*)d, blended);\n            _mm256_storeu_si256((__m256i*)(d + 8), blended2);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint16_t source array (src);\n- A pointer to a non-empty uint16_t destination array (dst);\n- A uint32_t mask value (mask);\n- The length of the array (length).\n\nYour goal is to blend two arrays based on the following condition:\n    If the bitwise AND of the element's index and the mask is non-zero, take the value from src1; \n    otherwise, take the value from src2.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i idx = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8);\n            __m256i idx2 = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n\n            __m256i and_result = _mm256_and_si256(idx, mask_vec);\n            __m256i and_result2 = _mm256_and_si256(idx2, mask_vec);\n\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp_result = _mm256_cmpeq_epi32(and_result, zero);\n            __m256i cmp_result2 = _mm256_cmpeq_epi32(and_result2, zero);\n\n            uint32_t cmp_mask = _mm256_movemask_epi8(cmp_result);\n            uint32_t cmp_mask2 = _mm256_movemask_epi8(cmp_result2);\n\n            const uint16_t* s1 = src1 + i;\n            const uint16_t* s2 = src2 + i;\n            uint16_t* d = dst + i;\n\n            __m256i v_src1 = _mm256_loadu_si256((__m256i*)s1);\n            __m256i v_src2 = _mm256_loadu_si256((__m256i*)s2);\n\n            __m256i blended = _mm256_blendv_epi8(v_src1, v_src2, cmp_result);\n            _mm256_storeu_si256((__m256i*)d, blended);\n\n            __m256i v_src1_2 = _mm256_loadu_si256((__m256i*)(s1 + 8));\n            __m256i v_src2_2 = _mm256_loadu_si256((__m256i*)(s2 + 8));\n\n            __m256i blended2 = _mm256_blendv_epi8(v_src1_2, v_src2_2, cmp_result2);\n            _mm256_storeu_si256((__m256i*)(d + 8), blended2);\n        }\n    }\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint16_t source array (src);\n- A pointer to a non-empty uint16_t destination array (dst);\n- A uint32_t mask value (mask);\n- The length of the array (length).\n\nYour goal is to blend two arrays based on the following condition:\n    If the bitwise AND of the element's index and the mask is non-zero, take the value from src1; \n    otherwise, take the value from src2.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i idx = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8);\n            __m256i idx2 = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n\n            __m256i and_result = _mm256_and_si256(idx, mask_vec);\n            __m256i and_result2 = _mm256_and_si256(idx2, mask_vec);\n\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp_result = _mm256_cmpeq_epi32(and_result, zero);\n            __m256i cmp_result2 = _mm256_cmpeq_epi32(and_result2, zero);\n\n            uint32_t cmp_mask = _mm256_movemask_epi8(cmp_result);\n            uint32_t cmp_mask2 = _mm256_movemask_epi8(cmp_result2);\n\n            const uint16_t* s1 = src1 + i;\n            const uint16_t* s2 = src2 + i;\n            uint16_t* d = dst + i;\n\n            __m256i v_src1 = _mm256_loadu_si256((__m256i*)s1);\n            __m256i v_src2 = _mm256_loadu_si256((__m256i*)s2);\n\n            __m256i v_src1_2 = _mm256_loadu_si256((__m256i*)(s1 + 8));\n            __m256i v_src2_2 = _mm256_loadu_si256((__m256i*)(s2 + 8));\n\n            __m256i blended = _mm256_blendv_epi8(v_src2, v_src1, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask)));\n            __m256i blended2 = _mm256_blendv_epi8(v_src2_2, v_src1_2, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask2)));\n\n            _mm256_storeu_si256((__m256i*)d, blended);\n            _mm256_storeu_si256((__m256i*)(d + 8), blended2);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint16_t source array (src);\n- A pointer to a non-empty uint16_t destination array (dst);\n- A uint32_t mask value (mask);\n- The length of the array (length).\n\nYour goal is to blend two arrays based on the following condition:\n    If the bitwise AND of the element's index and the mask is non-zero, take the value from src1; \n    otherwise, take the value from src2.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i idx = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8);\n            __m256i idx2 = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n\n            __m256i and_result = _mm256_and_si256(mask_vec, idx);\n            __m256i and_result2 = _mm256_and_si256(mask_vec, idx2);\n\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp_result = _mm256_cmpeq_epi32(and_result, zero);\n            __m256i cmp_result2 = _mm256_cmpeq_epi32(and_result2, zero);\n\n            uint32_t cmp_mask = _mm256_movemask_epi8(cmp_result);\n            uint32_t cmp_mask2 = _mm256_movemask_epi8(cmp_result2);\n\n            const uint16_t* s1 = src1 + i;\n            const uint16_t* s2 = src2 + i;\n            uint16_t* d = dst + i;\n\n            __m256i v_src1 = _mm256_loadu_si256((const __m256i*)s1);\n            __m256i v_src2 = _mm256_loadu_si256((const __m256i*)s2);\n\n            __m256i v_src1_2 = _mm256_loadu_si256((const __m256i*)(s1 + 8));\n            __m256i v_src2_2 = _mm256_loadu_si256((const __m256i*)(s2 + 8));\n\n            __m256i blended = _mm256_blendv_epi8(v_src2, v_src1, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask)));\n            __m256i blended2 = _mm256_blendv_epi8(v_src2_2, v_src1_2, _mm256_cmpeq_epi8(_mm256_setzero_si256(), _mm256_set1_epi16(0xFFFF & cmp_mask2)));\n\n            _mm256_storeu_si256((__m256i*)d, blended);\n            _mm256_storeu_si256((__m256i*)(d + 8), blended2);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint16_t source array (src);\n- A pointer to a non-empty uint16_t destination array (dst);\n- A uint32_t mask value (mask);\n- The length of the array (length).\n\nYour goal is to blend two arrays based on the following condition:\n    If the bitwise AND of the element's index and the mask is non-zero, take the value from src1; \n    otherwise, take the value from src2.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    if (length >= 16) {\n        for (; i <= length - 16; i += 16) {\n            __m256i idx = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8);\n            __m256i idx2 = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n            __m256i mask_vec = _mm256_set1_epi32(mask);\n\n            __m256i and_result = _mm256_and_si256(idx, mask_vec);\n            __m256i and_result2 = _mm256_and_si256(idx2, mask_vec);\n\n            __m256i zero = _mm256_setzero_si256();\n\n            __m256i cmp_result = _mm256_cmpeq_epi32(and_result, zero);\n            __m256i cmp_result2 = _mm256_cmpeq_epi32(and_result2, zero);\n\n            uint32_t cmp_mask = _mm256_movemask_epi8(cmp_result);\n            uint32_t cmp_mask2 = _mm256_movemask_epi8(cmp_result2);\n\n            cmp_mask = ~cmp_mask;\n            cmp_mask2 = ~cmp_mask2;\n\n            __m256i src1_vec = _mm256_loadu_si256((const __m256i*)(src1 + i));\n            __m256i src2_vec = _mm256_loadu_si256((const __m256i*)(src2 + i));\n\n            uint16_t temp_src1[16];\n            uint16_t temp_src2[16];\n            _mm256_storeu_si256((__m256i*)temp_src1, src1_vec);\n            _mm256_storeu_si256((__m256i*)temp_src2, src2_vec);\n\n            for (int j = 0; j < 16; ++j) {\n                if ((j < 8 && (cmp_mask2 & (1 << (j * 2)))) || (j >= 8 && (cmp_mask & (1 << ((j - 8) * 2))))) {\n                    dst[i + j] = temp_src1[j];\n                } else {\n                    dst[i + j] = temp_src2[j];\n                }\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint32_t source array (src);\n- A pointer to a non-empty uint32_t destination array (dst);\n- The length of the array (length).\n\nYour goal is to calculate the population count (the number of 1 bits in the binary representation of an integer) \nfor each element in the array.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        __m256i lookup_table = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n\n        __m256i low_nibble = _mm256_and_si256(vec, _mm256_set1_epi32(0x0F0F0F0F));\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 4), _mm256_set1_epi32(0x0F0F0F0F));\n\n        __m256i popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepu8_epi32(_mm256_extracti128_si256(low_nibble, 0)));\n        __m256i popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepu8_epi32(_mm256_extracti128_si256(high_nibble, 0)));\n        __m256i sum1 = _mm256_add_epi32(popcount_low, popcount_high);\n\n        popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepu8_epi32(_mm256_extracti128_si256(low_nibble, 1)));\n        popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepu8_epi32(_mm256_extracti128_si256(high_nibble, 1)));\n        __m256i sum2 = _mm256_add_epi32(popcount_low, popcount_high);\n\n        __m256i total_popcount = _mm256_add_epi32(sum1, sum2);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), total_popcount);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        uint32_t count = 0;\n        while (val) {\n            count += val & 1;\n            val >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint32_t source array (src);\n- A pointer to a non-empty uint32_t destination array (dst);\n- The length of the array (length).\n\nYour goal is to calculate the population count (the number of 1 bits in the binary representation of an integer) \nfor each element in the array.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        __m256i lookup_table = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n\n        __m256i low_nibble = _mm256_and_si256(vec, _mm256_set1_epi32(0x0F0F0F0F));\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 4), _mm256_set1_epi32(0x0F0F0F0F));\n\n        __m256i popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(low_nibble));\n        __m256i popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(high_nibble));\n\n        __m256i sum1 = _mm256_add_epi8(popcount_low, popcount_high);\n\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 8), _mm256_set1_epi32(0x0F0F0F0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 12), _mm256_set1_epi32(0x0F0F0F0F));\n\n        popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(low_nibble));\n        popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(high_nibble));\n\n        __m256i sum2 = _mm256_add_epi8(popcount_low, popcount_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum2);\n\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 16), _mm256_set1_epi32(0x0F0F0F0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 20), _mm256_set1_epi32(0x0F0F0F0F));\n\n        popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(low_nibble));\n        popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(high_nibble));\n\n        sum2 = _mm256_add_epi8(popcount_low, popcount_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum2);\n\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 24), _mm256_set1_epi32(0x0F0F0F0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 28), _mm256_set1_epi32(0x0F0F0F0F));\n\n        popcount_low = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(low_nibble));\n        popcount_high = _mm256_shuffle_epi8(lookup_table, _mm256_castsi256_si128(high_nibble));\n\n        sum2 = _mm256_add_epi8(popcount_low, popcount_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum2);\n\n        __m256i result = _mm256_sad_epu8(sum1, _mm256_setzero_si256());\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint32_t source array (src);\n- A pointer to a non-empty uint32_t destination array (dst);\n- The length of the array (length).\n\nYour goal is to calculate the population count (the number of 1 bits in the binary representation of an integer) \nfor each element in the array.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        \n        __m256i lookup_table = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n\n        __m256i low_mask = _mm256_set1_epi8(0x0F);\n\n        __m256i low_nibbles = _mm256_and_si256(vec, low_mask);\n        __m256i high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 4), low_mask);\n\n        __m256i popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        __m256i popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n\n        __m256i sum1 = _mm256_add_epi8(popcnt_low, popcnt_high);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 8), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 12), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 16), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 20), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 24), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 28), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        __m256i result = _mm256_sad_epu8(sum1, _mm256_setzero_si256());\n        \n        uint32_t temp[8];\n        _mm256_storeu_si256((__m256i*)temp, result);\n        \n        dst[i] = temp[0] + temp[4];\n        dst[i+1] = temp[1] + temp[5];\n        dst[i+2] = temp[2] + temp[6];\n        dst[i+3] = temp[3] + temp[7];\n\n        vec = _mm256_loadu_si256((const __m256i*)(src + i + 4));\n        \n        low_nibbles = _mm256_and_si256(vec, low_mask);\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 4), low_mask);\n\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n\n        sum1 = _mm256_add_epi8(popcnt_low, popcnt_high);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 8), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 12), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 16), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 20), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        high_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 24), low_mask);\n        popcnt_high = _mm256_shuffle_epi8(lookup_table, high_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_high);\n\n        low_nibbles = _mm256_and_si256(_mm256_srli_epi32(vec, 28), low_mask);\n        popcnt_low = _mm256_shuffle_epi8(lookup_table, low_nibbles);\n        sum1 = _mm256_add_epi8(sum1, popcnt_low);\n\n        result = _mm256_sad_epu8(sum1, _mm256_setzero_si256());\n        \n        _mm256_storeu_si256((__m256i*)temp, result);\n        \n        dst[i+4] = temp[0] + temp[4];\n        dst[i+5] = temp[1] + temp[5];\n        dst[i+6] = temp[2] + temp[6];\n        dst[i+7] = temp[3] + temp[7];\n    }\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty uint32_t source array (src);\n- A pointer to a non-empty uint32_t destination array (dst);\n- The length of the array (length).\n\nYour goal is to calculate the population count (the number of 1 bits in the binary representation of an integer) \nfor each element in the array.\nThe result should be stored in the destination array (dst).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i popcounts = _mm256_setzero_si256();\n\n        // Use AVX2 for efficient popcount\n        popcounts = _mm256_add_epi32(popcounts, _mm256_and_si256(vec, _mm256_set1_epi32(0x55555555)));\n        vec = _mm256_srli_epi32(vec, 1);\n        popcounts = _mm256_add_epi32(popcounts, _mm256_and_si256(vec, _mm256_set1_epi32(0x55555555)));\n        vec = _mm256_srli_epi32(vec, 1);\n        popcounts = _mm256_add_epi32(popcounts, _mm256_and_si256(vec, _mm256_set1_epi32(0x55555555)));\n        vec = _mm256_srli_epi32(vec, 1);\n        popcounts = _mm256_add_epi32(popcounts, _mm256_and_si256(vec, _mm256_set1_epi32(0x55555555)));\n\n        popcounts = _mm256_add_epi32(popcounts, _mm256_srli_epi32(popcounts, 2));\n        popcounts = _mm256_and_si256(popcounts, _mm256_set1_epi32(0x33333333));\n        popcounts = _mm256_add_epi32(popcounts, _mm256_srli_epi32(popcounts, 4));\n        popcounts = _mm256_and_si256(popcounts, _mm256_set1_epi32(0x0F0F0F0F));\n        popcounts = _mm256_add_epi32(popcounts, _mm256_srli_epi32(popcounts, 8));\n        popcounts = _mm256_add_epi32(popcounts, _mm256_srli_epi32(popcounts, 16));\n        popcounts = _mm256_and_si256(popcounts, _mm256_set1_epi32(0x0000003F));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), popcounts);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        dst[i] = x & 0x0000003F;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n\n        _mm256_storeu_si256((__m256i*)&dst[i], count);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (i = 0; i < length / 16 * 16; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(vec1, vec2);\n        __m256i result = _mm256_min_epu16(sum, max_val);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; i++) {\n        uint32_t temp = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (temp > UINT16_MAX) ? UINT16_MAX : (uint16_t)temp;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (i = 0; i < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi16(a, b);\n        __m256i result = _mm256_min_epi16(sum, max_val);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(a, b);\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n    for (; i < length; i++) {\n        uint32_t temp = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (temp > UINT16_MAX) ? UINT16_MAX : (uint16_t)temp;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(a, b);\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n    for (; i < length; i++) {\n        uint32_t temp = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (temp > UINT16_MAX) ? UINT16_MAX : (uint16_t)temp;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (i = 0; i < length; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi16(vec1, vec2);\n        __m256i result = _mm256_min_epi16(sum, max_val);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (A);\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (B);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix (C);\n- The number of rows in matrix A (m);\n- The number of columns in matrix A and rows in matrix B (n);\n- The number of columns in matrix B (p).\n\nA, B, and C are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of A and B for elements in the range [-100, 100]\n(i.e., skipping summing elements when A[i] or B[i] outside the range),\nand store the result in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n\n    __m256d lower = _mm256_set1_pd(lower_bound);\n    __m256d upper = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n\n                if (a_val >= lower_bound && a_val <= upper_bound &&\n                    b_val >= lower_bound && b_val <= upper_bound) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (A);\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (B);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix (C);\n- The number of rows in matrix A (m);\n- The number of columns in matrix A and rows in matrix B (n);\n- The number of columns in matrix B (p).\n\nA, B, and C are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of A and B for elements in the range [-100, 100]\n(i.e., skipping summing elements when A[i] or B[i] outside the range),\nand store the result in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n\n    __m256d lower = _mm256_set1_pd(lower_bound);\n    __m256d upper = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n\n                if (a_val >= lower_bound && a_val <= upper_bound &&\n                    b_val >= lower_bound && b_val <= upper_bound) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (A);\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (B);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix (C);\n- The number of rows in matrix A (m);\n- The number of columns in matrix A and rows in matrix B (n);\n- The number of columns in matrix B (p).\n\nA, B, and C are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of A and B for elements in the range [-100, 100]\n(i.e., skipping summing elements when A[i] or B[i] outside the range),\nand store the result in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n\n    __m256d lower = _mm256_set1_pd(lower_bound);\n    __m256d upper = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n\n                if (a_val >= lower_bound && a_val <= upper_bound &&\n                    b_val >= lower_bound && b_val <= upper_bound) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (A);\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (B);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix (C);\n- The number of rows in matrix A (m);\n- The number of columns in matrix A and rows in matrix B (n);\n- The number of columns in matrix B (p).\n\nA, B, and C are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of A and B for elements in the range [-100, 100]\n(i.e., skipping summing elements when A[i] or B[i] outside the range),\nand store the result in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n\n    __m256d lower = _mm256_set1_pd(lower_bound);\n    __m256d upper = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n\n                if (a_val >= lower_bound && a_val <= upper_bound &&\n                    b_val >= lower_bound && b_val <= upper_bound) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (A);\n- A pointer to a non-empty 64-bit floating-point (double) source matrix (B);\n- A pointer to a non-empty 64-bit floating-point (double) destination matrix (C);\n- The number of rows in matrix A (m);\n- The number of columns in matrix A and rows in matrix B (n);\n- The number of columns in matrix B (p).\n\nA, B, and C are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of A and B for elements in the range [-100, 100]\n(i.e., skipping summing elements when A[i] or B[i] outside the range),\nand store the result in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const double lower_bound = -100.0;\n    const double upper_bound = 100.0;\n\n    __m256d lower = _mm256_set1_pd(lower_bound);\n    __m256d upper = _mm256_set1_pd(upper_bound);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n\n                if (a_val >= lower_bound && a_val <= upper_bound &&\n                    b_val >= lower_bound && b_val <= upper_bound) {\n                    sum += a_val * b_val;\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit integer source tensor (A);\n- A pointer to a non-empty 64-bit integer source tensor (B);\n- A pointer to a non-empty 64-bit integer destination tensor (C);\n- The dimensions of the tensor (dim1, dim2, dim3).\n\nA, B, and C are 3D tensors stored in non-empty 1D arrays.\nYour goal is to perform element-wise addition of two 3D tensors (A and B),\nand store the result in C (no overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i sum = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), sum);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit integer source tensor (A);\n- A pointer to a non-empty 64-bit integer source tensor (B);\n- A pointer to a non-empty 64-bit integer destination tensor (C);\n- The dimensions of the tensor (dim1, dim2, dim3).\n\nA, B, and C are 3D tensors stored in non-empty 1D arrays.\nYour goal is to perform element-wise addition of two 3D tensors (A and B),\nand store the result in C (no overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit integer source tensor (A);\n- A pointer to a non-empty 64-bit integer source tensor (B);\n- A pointer to a non-empty 64-bit integer destination tensor (C);\n- The dimensions of the tensor (dim1, dim2, dim3).\n\nA, B, and C are 3D tensors stored in non-empty 1D arrays.\nYour goal is to perform element-wise addition of two 3D tensors (A and B),\nand store the result in C (no overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i sum = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), sum);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit integer source tensor (A);\n- A pointer to a non-empty 64-bit integer source tensor (B);\n- A pointer to a non-empty 64-bit integer destination tensor (C);\n- The dimensions of the tensor (dim1, dim2, dim3).\n\nA, B, and C are 3D tensors stored in non-empty 1D arrays.\nYour goal is to perform element-wise addition of two 3D tensors (A and B),\nand store the result in C (no overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit integer source tensor (A);\n- A pointer to a non-empty 64-bit integer source tensor (B);\n- A pointer to a non-empty 64-bit integer destination tensor (C);\n- The dimensions of the tensor (dim1, dim2, dim3).\n\nA, B, and C are 3D tensors stored in non-empty 1D arrays.\nYour goal is to perform element-wise addition of two 3D tensors (A and B),\nand store the result in C (no overflow).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)(C + i), c_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2,\nand then round the result to the nearest integer.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = std::round(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2,\nand then round the result to the nearest integer.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = std::round(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2,\nand then round the result to the nearest integer.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = std::round(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2,\nand then round the result to the nearest integer.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = std::round(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise addition of src1 and src2,\nand then round the result to the nearest integer.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(a, b);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = std::round(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise multiplication of src1 and src2\nfor elements that meet the following conditions:\n    1: the absolute value of the difference is within 50;\n    2: index is is a multiple of 2.\nand then round the result up toward positive infinity.\nFor elements that do not meet the above conditions, the results should be -1.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    float negative_one[8] = {-1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f};\n    __m256 negative_ones = _mm256_loadu_ps(negative_one);\n    __m256 threshold = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        // Create a mask for even indices\n        __m256 even_mask = _mm256_set_ps(1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f);\n        __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        __m256 even_indices = _mm256_cvtepi32_ps(indices);\n        __m256 even_test = _mm256_and_ps(even_mask, even_indices);\n        \n        float even_values[8];\n        _mm256_storeu_ps(even_values, even_test);\n\n        float even_check[8];\n        for(int j = 0; j < 8; j++){\n            if((i + j) % 2 == 0){\n                even_check[j] = 1.0f;\n            } else {\n                even_check[j] = 0.0f;\n            }\n        }\n        __m256 even_check_vec = _mm256_loadu_ps(even_check);\n        __m256 even_mask_final = _mm256_cmp_ps(even_check_vec, _mm256_setzero_ps(), _CMP_NEQ_OQ);\n\n        __m256 combined_mask = _mm256_and_ps(mask, even_mask_final);\n\n        __m256 mul_result = _mm256_mul_ps(a, b);\n\n        float temp_results[8];\n        _mm256_storeu_ps(temp_results, mul_result);\n\n        float rounded_results[8];\n        for(int j = 0; j < 8; j++){\n            rounded_results[j] = std::ceil(temp_results[j]);\n        }\n        __m256 rounded_result = _mm256_loadu_ps(rounded_results);\n\n        __m256 final_result = _mm256_blendv_ps(negative_ones, rounded_result, combined_mask);\n\n        _mm256_storeu_ps(dst + i, final_result);\n    }\n\n    for (; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise multiplication of src1 and src2\nfor elements that meet the following conditions:\n    1: the absolute value of the difference is within 50;\n    2: index is is a multiple of 2.\nand then round the result up toward positive infinity.\nFor elements that do not meet the above conditions, the results should be -1.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const int vector_size = 8;\n    int i = 0;\n\n    __m256 const_50 = _mm256_set1_ps(50.0f);\n    __m256 const_neg_1 = _mm256_set1_ps(-1.0f);\n\n    for (; i + vector_size <= length; i += vector_size) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), v_diff);\n        __m256 v_cmp = _mm256_cmp_ps(v_abs_diff, const_50, _CMP_LE_OQ);\n\n        __m128i indices = _mm_set_epi32(i + 7, i + 6, i + 5, i + 4);\n        __m128i indices2 = _mm_set_epi32(i + 3, i + 2, i + 1, i + 0);\n        \n        int mask1 = _mm_movemask_epi8(_mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        int mask2 = _mm_movemask_epi8(_mm_cmpeq_epi32(_mm_and_si128(indices2, _mm_set1_epi32(1)), _mm_setzero_si128()));\n\n        int combined_mask = (mask2 << 16) | mask1;\n\n        __m256 index_mask = _mm256_set_epi32((combined_mask >> 28) & 1 ? -1 : 0,\n                                            (combined_mask >> 24) & 1 ? -1 : 0,\n                                            (combined_mask >> 20) & 1 ? -1 : 0,\n                                            (combined_mask >> 16) & 1 ? -1 : 0,\n                                            (combined_mask >> 12) & 1 ? -1 : 0,\n                                            (combined_mask >> 8) & 1 ? -1 : 0,\n                                            (combined_mask >> 4) & 1 ? -1 : 0,\n                                            (combined_mask >> 0) & 1 ? -1 : 0);\n\n        v_cmp = _mm256_and_ps(v_cmp, _mm256_castsi256_ps(index_mask));\n\n        __m256 v_mul = _mm256_mul_ps(v_src1, v_src2);\n        \n        _mm256_storeu_ps(&dst[i], _mm256_blendv_ps(const_neg_1, _mm256_round_ps(v_mul, _MM_FROUND_TO_POSINF | _MM_FROUND_NO_EXC), v_cmp));\n    }\n\n    for (; i < length; ++i) {\n        if (abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise multiplication of src1 and src2\nfor elements that meet the following conditions:\n    1: the absolute value of the difference is within 50;\n    2: index is is a multiple of 2.\nand then round the result up toward positive infinity.\nFor elements that do not meet the above conditions, the results should be -1.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    float threshold = 50.0f;\n    __m256 threshold_vec = _mm256_set1_ps(threshold);\n    __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(src1_vec, src2_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n\n        __m256 mask_vec = _mm256_cmp_ps(abs_diff_vec, threshold_vec, _CMP_LE_OS);\n\n        // Check if the index is a multiple of 2\n        float mask_arr[8];\n        _mm256_storeu_ps(mask_arr, mask_vec);\n        \n        for(int j = 0; j < 8; j++){\n            if((i+j) % 2 != 0){\n                mask_arr[j] = 0.0f;\n            }\n        }\n\n        mask_vec = _mm256_loadu_ps(mask_arr);\n\n        __m256 mul_vec = _mm256_mul_ps(src1_vec, src2_vec);\n\n        float mul_arr[8];\n        _mm256_storeu_ps(mul_arr, mul_vec);\n\n        float res_arr[8];\n\n        for(int j = 0; j < 8; j++){\n            if(mask_arr[j] != 0.0f){\n                res_arr[j] = std::ceil(mul_arr[j]);\n            } else {\n                res_arr[j] = -1.0f;\n            }\n        }\n\n        __m256 res_vec = _mm256_loadu_ps(res_arr);\n        _mm256_storeu_ps(&dst[i], res_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50.0f && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise multiplication of src1 and src2\nfor elements that meet the following conditions:\n    1: the absolute value of the difference is within 50;\n    2: index is is a multiple of 2.\nand then round the result up toward positive infinity.\nFor elements that do not meet the above conditions, the results should be -1.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    float negative_one[8] = {-1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f};\n    __m256 negative_ones = _mm256_loadu_ps(negative_one);\n    __m256 threshold = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Create a mask for even indices\n        __m256 index_mask = _mm256_set_ps(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256 even_mask = _mm256_andnot_ps(_mm256_and_ps(index_mask, _mm256_set1_ps(1.0f)), _mm256_set1_ps(1.0f));\n\n        // Combine the masks\n        mask = _mm256_and_ps(mask, even_mask);\n\n        __m256 mul_result = _mm256_mul_ps(a, b);\n\n        // Round towards positive infinity\n        float temp[8];\n        _mm256_storeu_ps(temp, mul_result);\n        for (int j = 0; j < 8; ++j) {\n            temp[j] = std::ceil(temp[j]);\n        }\n        mul_result = _mm256_loadu_ps(temp);\n\n        __m256 result = _mm256_blendv_ps(negative_ones, mul_result, mask);\n\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point source array (src1);\n- A pointer to a non-empty 32-bit floating-point source array (src2);\n- A pointer to a non-empty 32-bit floating-point destination array (dst);\n- The length of the arrays (length).\n\nYour goal is to calculate the result of the element-wise multiplication of src1 and src2\nfor elements that meet the following conditions:\n    1: the absolute value of the difference is within 50;\n    2: index is is a multiple of 2.\nand then round the result up toward positive infinity.\nFor elements that do not meet the above conditions, the results should be -1.\nFinally, the result should be stored in the dst array.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    float negative_one[8] = {-1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f, -1.0f};\n    __m256 negative_ones = _mm256_loadu_ps(negative_one);\n    __m256 threshold = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(src1 + i);\n        __m256 b = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Create a mask for even indices\n        __m256 index_mask = _mm256_set_ps(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256 even_mask = _mm256_andnot_ps(_mm256_and_ps(index_mask, _mm256_set1_ps(1.0f)), _mm256_set1_ps(1.0f));\n\n        // Combine the two masks\n        mask = _mm256_and_ps(mask, even_mask);\n\n        __m256 mul_result = _mm256_mul_ps(a, b);\n\n        // Round towards positive infinity\n        float temp[8];\n        _mm256_storeu_ps(temp, mul_result);\n        for (int j = 0; j < 8; j++) {\n            temp[j] = ceilf(temp[j]);\n        }\n        mul_result = _mm256_loadu_ps(temp);\n\n        __m256 result = _mm256_blendv_ps(negative_ones, mul_result, mask);\n\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point source matrix (mat);\n- A scalar value in 64-bit floating-point (scalar);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (m);\n- The number of columns in the matrix (n).\n\nmat and dst are 2D matrices flattened into 1D arrays.\nYour goal is to calculate the result of the element-wise multiplication of mat and scalar,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t cols_per_iter = 4;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n; j += cols_per_iter) {\n            if (j + cols_per_iter <= n) {\n                __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n                __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n                _mm256_storeu_pd(&dst[i * n + j], result_vec);\n            } else {\n                for (size_t k = j; k < n; ++k) {\n                    dst[i * n + k] = mat[i * n + k] * scalar;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point source matrix (mat);\n- A scalar value in 64-bit floating-point (scalar);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (m);\n- The number of columns in the matrix (n).\n\nmat and dst are 2D matrices flattened into 1D arrays.\nYour goal is to calculate the result of the element-wise multiplication of mat and scalar,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t aligned_cols = n / 4 * 4;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < aligned_cols; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result_vec);\n        }\n\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n\n    size_t i;\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n\n    size_t i;\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t total_elements = m * n;\n\n    size_t i;\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n        \n        float norm[8];\n        _mm256_storeu_ps(norm, sum_squares);\n        float l2_norm = sqrt(norm[0] + norm[1] + norm[2] + norm[3] + norm[4] + norm[5] + norm[6] + norm[7]);\n\n        for (size_t j = 0; j < n; ++j) {\n            mat[i * n + j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n        \n        float norm[8];\n        _mm256_storeu_ps(norm, sum_squares);\n        float l2_norm = sqrt(norm[0] + norm[1] + norm[2] + norm[3] + norm[4] + norm[5] + norm[6] + norm[7]);\n        \n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(vec, _mm256_set1_ps(l2_norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            sum_squares = _mm256_add_ps(sum_squares, _mm256_mul_ps(vec, vec));\n        }\n        \n        float norm[8];\n        _mm256_storeu_ps(norm, sum_squares);\n        float l2_norm = sqrt(norm[0] + norm[1] + norm[2] + norm[3] + \n                             norm[4] + norm[5] + norm[6] + norm[7]);\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(vec, _mm256_set1_ps(l2_norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n        \n        float norm[8];\n        _mm256_storeu_ps(norm, sum_squares);\n        float l2_norm = sqrt(norm[0] + norm[1] + norm[2] + norm[3] + norm[4] + norm[5] + norm[6] + norm[7]);\n        \n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(vec, _mm256_set1_ps(l2_norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares for the current row\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            sum_squares = _mm256_fmadd_ps(_mm256_set1_ps(val), _mm256_set1_ps(val), sum_squares);\n        }\n\n        // Calculate L2 norm\n        float norm[8];\n        _mm256_storeu_ps(norm, sum_squares);\n        float l2_norm = sqrt(norm[0] + norm[1] + norm[2] + norm[3] + norm[4] + norm[5] + norm[6] + norm[7]);\n\n        // Normalize the row\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(vec, _mm256_set1_ps(l2_norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the first matrix (m);\n- The number of columns in the first matrix and rows in the second matrix (n);\n- The number of columns in the second matrix (p).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of mat1 and mat2,\nand then round the result to the nearest integer.\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b_vec;\n                if (k + 3 < n) {\n                    b_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n                } else {\n                    double temp_b[4] = {0.0, 0.0, 0.0, 0.0};\n                    for (size_t l = 0; l < 4 && k + l < n; ++l) {\n                        temp_b[l] = mat2[(k + l) * p + j];\n                    }\n                    b_vec = _mm256_loadu_pd(temp_b);\n                }\n\n                __m256d mul_vec = _mm256_mul_pd(a_vec, b_vec);\n                sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n            }\n\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n            dst[i * p + j] = round(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the first matrix (m);\n- The number of columns in the first matrix and rows in the second matrix (n);\n- The number of columns in the second matrix (p).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of mat1 and mat2,\nand then round the result to the nearest integer.\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec, b_vec;\n\n                if (k + 3 < n) {\n                    a_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                    double b_vals[4] = {mat2[k * p + j], mat2[(k + 1) * p + j], mat2[(k + 2) * p + j], mat2[(k + 3) * p + j]};\n                    b_vec = _mm256_loadu_pd(b_vals);\n\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));\n                } else {\n                    double temp_sum = 0.0;\n                    for (size_t l = k; l < n; ++l) {\n                        temp_sum += mat1[i * n + l] * mat2[l * p + j];\n                    }\n                    double sum_arr[4];\n                    _mm256_storeu_pd(sum_arr, sum_vec);\n                    dst[i * p + j] = (double)round(sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] + temp_sum);\n                    goto next_j;\n                }\n            }\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            dst[i * p + j] = (double)round(sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3]);\n            next_j:;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the first matrix (m);\n- The number of columns in the first matrix and rows in the second matrix (n);\n- The number of columns in the second matrix (p).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of mat1 and mat2,\nand then round the result to the nearest integer.\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec, b_vec;\n\n                if (k + 3 < n) {\n                    a_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                    double b_vals[4] = {mat2[k * p + j], mat2[(k + 1) * p + j], mat2[(k + 2) * p + j], mat2[(k + 3) * p + j]};\n                    b_vec = _mm256_loadu_pd(b_vals);\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));\n                } else {\n                    double temp_sum[4] = {0.0, 0.0, 0.0, 0.0};\n                    for(size_t l = k; l < n; ++l) {\n                        temp_sum[l-k] = mat1[i * n + l] * mat2[l * p + j];\n                    }\n                    __m256d temp_vec = _mm256_loadu_pd(temp_sum);\n                    sum_vec = _mm256_add_pd(sum_vec, temp_vec);\n                    break;\n                }\n            }\n\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double final_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n            dst[i * p + j] = round(final_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the first matrix (m);\n- The number of columns in the first matrix and rows in the second matrix (n);\n- The number of columns in the second matrix (p).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of mat1 and mat2,\nand then round the result to the nearest integer.\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b_vec;\n                if (k + 3 < n) {\n                    b_vec = _mm256_set_pd(mat2[(k + 3) * p + j], mat2[(k + 2) * p + j], mat2[(k + 1) * p + j], mat2[k * p + j]);\n                } else {\n                    double temp_arr[4] = {0.0, 0.0, 0.0, 0.0};\n                    for (size_t l = 0; l < 4 && k + l < n; ++l) {\n                        temp_arr[l] = mat2[(k + l) * p + j];\n                    }\n                    b_vec = _mm256_loadu_pd(temp_arr);\n                    b_vec = _mm256_permute4x64_pd(b_vec, _MM_SHUFFLE(0, 1, 2, 3));\n                }\n                sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec);\n            }\n\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n            dst[i * p + j] = round(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the first matrix (m);\n- The number of columns in the first matrix and rows in the second matrix (n);\n- The number of columns in the second matrix (p).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform matrix multiplication of mat1 and mat2,\nand then round the result to the nearest integer.\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b_vec;\n                if (k + 3 < n) {\n                    b_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n                } else {\n                    double temp_b[4] = {0.0, 0.0, 0.0, 0.0};\n                    for (size_t l = 0; l < 4 && k + l < n; ++l) {\n                        temp_b[l] = mat2[(k + l) * p + j];\n                    }\n                    b_vec = _mm256_loadu_pd(temp_b);\n                }\n\n                __m256d mul_vec = _mm256_mul_pd(a_vec, b_vec);\n                sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n            }\n\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n            dst[i * p + j] = round(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (src);\n- A pointer to a non-empty 32-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to transpose the matrix src and round each element to the nearest quarter (0.25).\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(vec * 4.0f, _MM_FROUND_TO_NEAREST_INT) / 4.0f;\n            \n            float temp[8];\n            _mm256_storeu_ps(temp, rounded);\n\n            for (int k = 0; k < 8; ++k) {\n                if (j + k < cols) {\n                    dst[(j + k) * rows + i] = temp[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (src);\n- A pointer to a non-empty 32-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to transpose the matrix src and round each element to the nearest quarter (0.25).\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 8) {\n            if (cols - j >= 8) {\n                __m256 src_vec = _mm256_loadu_ps(&src[i * cols + j]);\n                __m256 rounded_vec = _mm256_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT);\n                __m256 quarter_val = _mm256_set1_ps(0.25f);\n                rounded_vec = _mm256_round_ps(_mm256_mul_ps(rounded_vec, _mm256_set1_ps(4.0f)), _MM_FROUND_TO_NEAREST_INT);\n                rounded_vec = _mm256_mul_ps(rounded_vec, quarter_val);\n\n                float temp[8];\n                _mm256_storeu_ps(temp, rounded_vec);\n\n                for (int k = 0; k < 8; k++) {\n                    dst[(j + k) * rows + i] = temp[k];\n                }\n            } else {\n                for (size_t k = j; k < cols; k++) {\n                    float val = src[i * cols + k];\n                    float rounded_val = roundf(val * 4.0f) / 4.0f;\n                    dst[k * rows + i] = rounded_val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (src);\n- A pointer to a non-empty 32-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to transpose the matrix src and round each element to the nearest quarter (0.25).\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 8) {\n            if (cols - j >= 8) {\n                __m256 src_vec = _mm256_loadu_ps(&src[i * cols + j]);\n\n                // Round to nearest quarter\n                __m256 quarter = _mm256_set1_ps(0.25f);\n                src_vec = _mm256_round_ps(src_vec / quarter, _MM_FROUND_TO_NEAREST_INT) * quarter;\n\n                // Transpose 8 floats to 8 different locations in dst\n                float temp[8];\n                _mm256_storeu_ps(temp, src_vec);\n                for (int k = 0; k < 8; k++) {\n                    dst[(j + k) * rows + i] = temp[k];\n                }\n            } else {\n                for (size_t k = j; k < cols; k++) {\n                    float val = src[i * cols + k];\n                    val = roundf(val / 0.25f) * 0.25f;\n                    dst[k * rows + i] = val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (src);\n- A pointer to a non-empty 32-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to transpose the matrix src and round each element to the nearest quarter (0.25).\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 8) {\n            if (cols - j >= 8) {\n                __m256 src_vec = _mm256_loadu_ps(&src[i * cols + j]);\n\n                // Round to nearest quarter\n                __m256 quarter = _mm256_set1_ps(0.25f);\n                src_vec = _mm256_round_ps(src_vec / quarter, _MM_FROUND_TO_NEAREST_INT) * quarter;\n\n                // Transpose 8 floats from src row to dst column\n                float temp[8];\n                _mm256_storeu_ps(temp, src_vec);\n                for (int k = 0; k < 8; k++) {\n                    dst[(j + k) * rows + i] = temp[k];\n                }\n            } else {\n                for (size_t k = j; k < cols; k++) {\n                    float val = src[i * cols + k];\n                    val = roundf(val / 0.25f) * 0.25f;\n                    dst[k * rows + i] = val;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (src);\n- A pointer to a non-empty 32-bit floating-point destination matrix (dst);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to transpose the matrix src and round each element to the nearest quarter (0.25).\nThe result should be stored in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 8) {\n            __m256 vec = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(vec * 4.0f, _MM_FROUND_TO_NEAREST_INT) / 4.0f;\n            \n            float temp[8];\n            _mm256_storeu_ps(temp, rounded);\n\n            for (int k = 0; k < 8; ++k) {\n                if (j + k < cols) {\n                    dst[(j + k) * rows + i] = temp[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (tensor1);\n- A pointer to a non-empty 32-bit floating-point tensor (tensor2);\n- A pointer to a non-empty 32-bit floating-point destination tensor (dst);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\ntensor1, tensor2, and dst are 3D tensors in the shape (dim1, dim2, dim3) flattened into 1D arrays.\nYour goal is to perform element-wise subtraction of tensor1 and tensor2,\nand then round the result towards zero.\nThe result should be stored in the dst tensor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&tensor1[i]);\n        __m256 b = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub_result = _mm256_sub_ps(a, b);\n\n        // Round towards zero using AVX (truncate)\n        __m256 rounded_result = _mm256_roundscale_ps(sub_result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n\n        _mm256_storeu_ps(&dst[i], rounded_result);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = truncf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (tensor1);\n- A pointer to a non-empty 32-bit floating-point tensor (tensor2);\n- A pointer to a non-empty 32-bit floating-point destination tensor (dst);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\ntensor1, tensor2, and dst are 3D tensors in the shape (dim1, dim2, dim3) flattened into 1D arrays.\nYour goal is to perform element-wise subtraction of tensor1 and tensor2,\nand then round the result towards zero.\nThe result should be stored in the dst tensor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process the array in chunks of 8 floats using AVX intrinsics\n    for (; i + 8 <= total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2 into AVX registers\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n\n        // Subtract the two vectors\n        __m256 sub_result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero using AVX2 intrinsics\n        __m256 rounded_result = _mm256_roundscale_ps(sub_result, _MM_FROUND_TOWARDZERO);\n\n        // Store the rounded result in the destination array\n        _mm256_storeu_ps(&dst[i], rounded_result);\n    }\n\n    // Process the remaining elements (less than 8) using scalar operations\n    for (; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (sub_result > 0) ? floorf(sub_result) : ceilf(sub_result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (tensor1);\n- A pointer to a non-empty 32-bit floating-point tensor (tensor2);\n- A pointer to a non-empty 32-bit floating-point destination tensor (dst);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\ntensor1, tensor2, and dst are 3D tensors in the shape (dim1, dim2, dim3) flattened into 1D arrays.\nYour goal is to perform element-wise subtraction of tensor1 and tensor2,\nand then round the result towards zero.\nThe result should be stored in the dst tensor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process the array in chunks of 8 floats using AVX intrinsics\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub_result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero using AVX intrinsics\n        __m256 abs_val = _mm256_abs_ps(sub_result);\n        __m256 rounded = _mm256_floor_ps(abs_val);\n        __m256 sign_mask = _mm256_cmp_ps(sub_result, _mm256_setzero_ps(), _CMP_LT_OS);\n        __m256 result = _mm256_blendv_ps(rounded, _mm256_sub_ps(_mm256_setzero_ps(), rounded), sign_mask);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Process the remaining elements\n    for (; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (sub_result > 0) ? floorf(sub_result) : ceilf(sub_result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (tensor1);\n- A pointer to a non-empty 32-bit floating-point tensor (tensor2);\n- A pointer to a non-empty 32-bit floating-point destination tensor (dst);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\ntensor1, tensor2, and dst are 3D tensors in the shape (dim1, dim2, dim3) flattened into 1D arrays.\nYour goal is to perform element-wise subtraction of tensor1 and tensor2,\nand then round the result towards zero.\nThe result should be stored in the dst tensor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process the array in chunks of 8 floats using AVX intrinsics\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256 a = _mm256_loadu_ps(&tensor1[i]);\n        __m256 b = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(a, b);\n\n        // Round towards zero using AVX intrinsics\n        __m256 trunc = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n\n        _mm256_storeu_ps(&dst[i], trunc);\n    }\n\n    // Process the remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = truncf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (tensor1);\n- A pointer to a non-empty 32-bit floating-point tensor (tensor2);\n- A pointer to a non-empty 32-bit floating-point destination tensor (dst);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\ntensor1, tensor2, and dst are 3D tensors in the shape (dim1, dim2, dim3) flattened into 1D arrays.\nYour goal is to perform element-wise subtraction of tensor1 and tensor2,\nand then round the result towards zero.\nThe result should be stored in the dst tensor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process the array in chunks of 8 floats using AVX intrinsics\n    for (; i + 8 <= total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2 into AVX registers\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n\n        // Subtract the two vectors\n        __m256 sub_result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero using AVX2 intrinsics\n        __m256 rounded_result = _mm256_roundscale_ps(sub_result, _MM_FROUND_TOWARDZERO);\n\n        // Store the rounded result in the destination array\n        _mm256_storeu_ps(&dst[i], rounded_result);\n    }\n\n    // Process the remaining elements (less than 8) using scalar operations\n    for (; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (sub_result > 0) ? floorf(sub_result) : ceilf(sub_result);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrices (m);\n- The number of columns in the matrices (n).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform the Hadamard product (element-wise multiplication) of mat1 and mat2,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t aligned_n = n / 4 * 4;\n\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < aligned_n; j += 4) {\n            __m256d a = _mm256_loadu_pd(mat1 + i * n + j);\n            __m256d b = _mm256_loadu_pd(mat2 + i * n + j);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(dst + i * n + j, c);\n        }\n\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrices (m);\n- The number of columns in the matrices (n).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform the Hadamard product (element-wise multiplication) of mat1 and mat2,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t aligned_n = n - (n % 4);\n\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < aligned_n; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrices (m);\n- The number of columns in the matrices (n).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform the Hadamard product (element-wise multiplication) of mat1 and mat2,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    size_t size = m * n;\n\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&mat1[i]);\n        __m256d b = _mm256_loadu_pd(&mat2[i]);\n        __m256d c = _mm256_mul_pd(a, b);\n        _mm256_storeu_pd(&dst[i], c);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrices (m);\n- The number of columns in the matrices (n).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform the Hadamard product (element-wise multiplication) of mat1 and mat2,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t aligned_n = n / 4 * 4;\n\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < aligned_n; j += 4) {\n            __m256d a = _mm256_loadu_pd(mat1 + i * n + j);\n            __m256d b = _mm256_loadu_pd(mat2 + i * n + j);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(dst + i * n + j, c);\n        }\n\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat1);\n- A pointer to a non-empty 64-bit floating-point matrix (mat2);\n- A pointer to a non-empty 64-bit floating-point destination matrix (dst);\n- The number of rows in the matrices (m);\n- The number of columns in the matrices (n).\n\nmat1, mat2, and dst are 2D matrices flattened into 1D arrays.\nYour goal is to perform the Hadamard product (element-wise multiplication) of mat1 and mat2,\nand then store the result in the dst matrix.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    size_t size = m * n;\n\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&mat1[i]);\n        __m256d b = _mm256_loadu_pd(&mat2[i]);\n        __m256d c = _mm256_mul_pd(a, b);\n        _mm256_storeu_pd(&dst[i], c);\n    }\n\n    for (; i < size; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (mat);\n- A pointer to a non-empty 32-bit floating-point vector (vec);\n- A pointer to a non-empty 32-bit floating-point destination vector (dst);\n- The number of rows in the matrix (m);\n- The number of columns in the matrix (n).\n\nThe matrix is flattened into a 1D array.\nmat has the shape (m, n), and vec has the shape (n).\nYour goal is to perform matrix-vector multiplication of mat and vec,\nand then store the result in the dst vector in the shape (m).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n\n*/\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j_simd = 0;\n        for (j = 0; j + 8 <= n; j += 8) {\n            __m256 mat_row_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum_vec = _mm256_add_ps(sum_vec, _mm256_mul_ps(mat_row_vec, vec_vec));\n            j_simd = j + 8;\n        }\n        float sum_arr[8];\n        _mm256_storeu_ps(sum_arr, sum_vec);\n        float sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] +\n                    sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n        for (; j < n; ++j) {\n            sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point matrix (mat);\n- A pointer to a non-empty 32-bit floating-point vector (vec);\n- A pointer to a non-empty 32-bit floating-point destination vector (dst);\n- The number of rows in the matrix (m);\n- The number of columns in the matrix (n).\n\nThe matrix is flattened into a 1D array.\nmat has the shape (m, n), and vec has the shape (n).\nYour goal is to perform matrix-vector multiplication of mat and vec,\nand then store the result in the dst vector in the shape (m).\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n\n*/\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j_simd = 0;\n        for (j = 0; j + 8 <= n; j += 8) {\n            __m256 mat_row_vec = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vec_vec = _mm256_loadu_ps(vec + j);\n            sum_vec = _mm256_add_ps(sum_vec, _mm256_mul_ps(mat_row_vec, vec_vec));\n            j_simd = j + 8;\n        }\n        float sum_arr[8];\n        _mm256_storeu_ps(sum_arr, sum_vec);\n        float row_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3] +\n                        sum_arr[4] + sum_arr[5] + sum_arr[6] + sum_arr[7];\n\n        for (j = j_simd; j < n; ++j) {\n            row_sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = row_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vec = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(mat_vec, vec_vec, sum);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        for (; j < n; j++) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vec = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(mat_vec, vec_vec, sum);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        for (; j < n; j++) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 mat_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vec = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(mat_vec, vec_vec, sum);\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d data_vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data_vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n\n    double total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (mat);\n- The number of rows in the matrix (m);\n- The number of columns in the matrix (n).\n\nYour goal is to calculate and return the average of elements in the matrix mat.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    double sum = 0.0;\n\n    size_t i = 0;\n    if (total_elements >= 4) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        for (; i + 3 < total_elements; i += 4) {\n            __m256d data_vec = _mm256_loadu_pd(&mat[i]);\n            sum_vec = _mm256_add_pd(sum_vec, data_vec);\n        }\n\n        double temp_sum[4];\n        _mm256_storeu_pd(temp_sum, sum_vec);\n        sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n    }\n\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        if (cols < 8) {\n            for (size_t j = 1; j < cols; ++j) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n            continue;\n        }\n\n        for (size_t j = 1; j < cols - 7; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j - 1));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j));\n\n            __m256i result;\n            if (ascending) {\n                result = _mm256_cmpgt_epi32(current, next);\n            } else {\n                result = _mm256_cmpgt_epi32(next, current);\n            }\n\n            if (!_mm256_testz_si256(result, result)) {\n                return false;\n            }\n        }\n\n        for (size_t j = cols - 7; j < cols; ++j) {\n            if (ascending) {\n                if (row[j - 1] > row[j]) return false;\n            } else {\n                if (row[j - 1] < row[j]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        if (cols < 8) {\n            for (size_t j = 1; j < cols; ++j) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n            continue;\n        }\n\n        for (size_t j = 1; j < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j - 1));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j));\n\n            __m256i result;\n            if (ascending) {\n                result = _mm256_cmpgt_epi32(current, next);\n            } else {\n                result = _mm256_cmpgt_epi32(next, current);\n            }\n\n            if (!_mm256_testz_si256(result, result)) {\n                // At least one element is not sorted correctly in this 8-element chunk.\n                // Need to check element by element to find the exact location.\n                for (size_t k = j; k < cols && k < j + 8; ++k) {\n                    if (ascending) {\n                        if (row[k - 1] > row[k]) return false;\n                    } else {\n                        if (row[k - 1] < row[k]) return false;\n                    }\n                }\n                break; // No need to check further in this row\n            }\n            if (j + 8 >= cols && j < cols) {\n                for (size_t k = j + 8 - (j + 8 - cols); k < cols; ++k) {\n                    if (ascending) {\n                        if (row[k - 1] > row[k]) return false;\n                    } else {\n                        if (row[k - 1] < row[k]) return false;\n                    }\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        if (cols < 8) {\n            for (size_t j = 1; j < cols; ++j) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n            continue;\n        }\n\n        for (size_t j = 1; j < cols - 7; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j - 1));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j));\n\n            __m256i result;\n            if (ascending) {\n                result = _mm256_cmpgt_epi32(current, next);\n            } else {\n                result = _mm256_cmpgt_epi32(next, current);\n            }\n\n            if (!_mm256_testz_si256(result, result)) {\n                return false;\n            }\n        }\n\n        for (size_t j = cols - (cols % 8 == 0 ? 8 : cols % 8); j < cols; ++j) {\n            if (ascending) {\n                if (row[j - 1] > row[j]) return false;\n            } else {\n                if (row[j - 1] < row[j]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        if (cols < 8) {\n            for (size_t j = 1; j < cols; ++j) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n            continue;\n        }\n\n        for (size_t j = 1; j < cols - 7; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j - 1));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j));\n\n            __m256i result;\n            if (ascending) {\n                result = _mm256_cmpgt_epi32(current, next);\n            } else {\n                result = _mm256_cmpgt_epi32(next, current);\n            }\n\n            if (!_mm256_testz_si256(result, result)) {\n                return false;\n            }\n        }\n\n        for (size_t j = cols - (cols % 8 == 0 ? 8 : cols % 8); j < cols; ++j) {\n            if (j > 0) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        if (cols < 8) {\n            for (size_t j = 1; j < cols; ++j) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n            continue;\n        }\n\n        for (size_t j = 1; j < cols - 7; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j - 1));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j));\n\n            __m256i result;\n            if (ascending) {\n                result = _mm256_cmpgt_epi32(current, next);\n            } else {\n                result = _mm256_cmpgt_epi32(next, current);\n            }\n\n            if (!_mm256_testz_si256(result, result)) {\n                return false;\n            }\n        }\n\n        for (size_t j = cols - (cols % 8 == 0 ? 8 : cols % 8); j < cols; ++j) {\n            if (j > 0) {\n                if (ascending) {\n                    if (row[j - 1] > row[j]) return false;\n                } else {\n                    if (row[j - 1] < row[j]) return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- A 8-bit unsigned integer (base) that is greater than 0;\n- The common length of src and dst (length).\n\nYour goal is, for each element in src, to store in dst the nearest integer that \n1) is a multiple of base, and \n2) has a value no more than the value of the element in src.\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i temp_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i res_vec = _mm256_mullo_epi16(temp_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), res_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- A 8-bit unsigned integer (base) that is greater than 0;\n- The common length of src and dst (length).\n\nYour goal is, for each element in src, to store in dst the nearest integer that \n1) is a multiple of base, and \n2) has a value no more than the value of the element in src.\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i temp_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i res_vec = _mm256_mullo_epi16(temp_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), res_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- A 8-bit unsigned integer (base) that is greater than 0;\n- The common length of src and dst (length).\n\nYour goal is, for each element in src, to store in dst the nearest integer that \n1) is a multiple of base, and \n2) has a value no more than the value of the element in src.\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i temp_vec = _mm256_divhi_epi16(src_vec, base_vec);\n        __m256i result_vec = _mm256_mullo_epi16(temp_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- A 8-bit unsigned integer (base) that is greater than 0;\n- The common length of src and dst (length).\n\nYour goal is, for each element in src, to store in dst the nearest integer that \n1) is a multiple of base, and \n2) has a value no more than the value of the element in src.\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i temp_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i res_vec = _mm256_mullo_epi16(temp_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), res_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer source array (src);\n- A pointer to a non-empty 16-bit integer destination array (dst);\n- A 8-bit unsigned integer (base) that is greater than 0;\n- The common length of src and dst (length).\n\nYour goal is, for each element in src, to store in dst the nearest integer that \n1) is a multiple of base, and \n2) has a value no more than the value of the element in src.\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i temp_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i result_vec = _mm256_mullo_epi16(temp_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (vector);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if any row in the matrix matches the given vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d vec_vec = _mm256_loadu_pd(&vector[j]);\n            __m256d cmp_vec = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_vec);\n            if (mask != 0xF) {\n                break;\n            }\n        }\n\n        bool row_match = true;\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (vector);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if any row in the matrix matches the given vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d vec_vec = _mm256_loadu_pd(&vector[j]);\n            __m256d cmp_vec = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_vec);\n            if (mask != 0xF) {\n                break;\n            }\n        }\n\n        bool row_match = true;\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (vector);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if any row in the matrix matches the given vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp_vec = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_vec);\n            if (mask != 0xF) {\n                break;\n            }\n        }\n\n        bool row_match = true;\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (vector);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if any row in the matrix matches the given vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp_vec = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_vec);\n            if (mask != 0xF) {\n                break;\n            }\n        }\n\n        bool row_match = true;\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (vector);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if any row in the matrix matches the given vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d vec_vec = _mm256_loadu_pd(vector + j);\n            __m256d cmp_vec = _mm256_cmp_pd(row_vec, vec_vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_vec);\n            if (mask != 0xF) {\n                break;\n            }\n        }\n\n        bool row_match = true;\n        for (; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (B);\n- An array of 4 size_t integers representing the dimensions of the tensors (dims).\n\nA and B are flattened into 1D arrays.\nYour goal is to count the number of equal elements between the two tensors.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 32) {\n        for (; i <= total_elements - 32; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n            __m256i eq = _mm256_cmpeq_epi8(a, b);\n            unsigned int mask = _mm256_movemask_epi8(eq);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (B);\n- An array of 4 size_t integers representing the dimensions of the tensors (dims).\n\nA and B are flattened into 1D arrays.\nYour goal is to count the number of equal elements between the two tensors.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 32) {\n        for (; i <= total_elements - 32; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n            __m256i eq = _mm256_cmpeq_epi8(a, b);\n            unsigned int mask = _mm256_movemask_epi8(eq);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (B);\n- An array of 4 size_t integers representing the dimensions of the tensors (dims).\n\nA and B are flattened into 1D arrays.\nYour goal is to count the number of equal elements between the two tensors.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i + 32 <= total_elements; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        unsigned int mask = _mm256_movemask_epi8(eq);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (B);\n- An array of 4 size_t integers representing the dimensions of the tensors (dims).\n\nA and B are flattened into 1D arrays.\nYour goal is to count the number of equal elements between the two tensors.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 32) {\n        for (; i <= total_elements - 32; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n            __m256i eq = _mm256_cmpeq_epi8(a, b);\n            unsigned int mask = _mm256_movemask_epi8(eq);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (B);\n- An array of 4 size_t integers representing the dimensions of the tensors (dims).\n\nA and B are flattened into 1D arrays.\nYour goal is to count the number of equal elements between the two tensors.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    if (total_elements >= 32) {\n        for (; i <= total_elements - 32; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n            __m256i eq = _mm256_cmpeq_epi8(a, b);\n            unsigned int mask = _mm256_movemask_epi8(eq);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- The common length of the given vectors;\n- A 64-bit integer `a`;\n- Two pointers to 64-bit integer arrays.\n\nYour goal is to do the element-wise vector operation involving:\n1. scalar-vector multiplication a and x;\n2. absolute value of vector: y;\n3. element-wise subtraction.\nStore the results to `y`.\n\nNo overflow in the input.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    const size_t avx_bound = length - length % 4;\n\n    __m256i vec_a = _mm256_set1_epi64x(a);\n\n    for (i = 0; i < avx_bound; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((const __m256i*)(x + i));\n        __m256i vec_y = _mm256_loadu_si256((const __m256i*)(y + i));\n\n        __m256i vec_ax = _mm256_mullo_epi64(vec_a, vec_x);\n        __m256i vec_abs_y = _mm256_abs_epi64(vec_y);\n        __m256i result = _mm256_sub_epi64(vec_ax, vec_abs_y);\n\n        _mm256_storeu_si256((__m256i*)(y + i), result);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i scalar = _mm256_set1_epi64x(a);\n    size_t i;\n\n    for (i = 0; i < length / 4; i++) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i * 4]);\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n        __m256i abs_result = _mm256_abs_epi64(mul_result);\n        _mm256_storeu_si256((__m256i*)&y[i * 4], abs_result);\n    }\n\n    for (; i < length; i++) {\n        y[i] = llabs(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i scalar = _mm256_set1_epi64x(a);\n    size_t i;\n\n    for (i = 0; i < length / 4; i++) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i * 4]);\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n        __m256i abs_result = _mm256_abs_epi64(mul_result);\n        _mm256_storeu_si256((__m256i*)&y[i * 4], abs_result);\n    }\n\n    for (; i < length; i++) {\n        y[i] = llabs(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i scalar = _mm256_set1_epi64x(a);\n    size_t i;\n\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n        __m256i abs_result = _mm256_sign_epi64(mul_result, mul_result);\n        _mm256_storeu_si256((__m256i*)&y[i], abs_result);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i scalar = _mm256_set1_epi64x(a);\n    size_t i;\n\n    for (i = 0; i < length / 4; i++) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i * 4]);\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n        __m256i abs_result = _mm256_abs_epi64(mul_result);\n        _mm256_storeu_si256((__m256i*)&y[i * 4], abs_result);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = {INT16_MAX, INT16_MIN};\n    size_t i;\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (i = 0; i < length / 16 * 16; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int16_t value = vec[i];\n        if (value < result.min_num) result.min_num = value;\n        if (value > result.max_num) result.max_num = value;\n    }\n\n    // Reduce min_vec and max_vec to find final min and max\n    int16_t min_array[16], max_array[16];\n    _mm256_storeu_si256((__m256i*)min_array, min_vec);\n    _mm256_storeu_si256((__m256i*)max_array, max_vec);\n\n    for (i = 0; i < 16; i++) {\n        if (min_array[i] < result.min_num) result.min_num = min_array[i];\n        if (max_array[i] > result.max_num) result.max_num = max_array[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = {INT16_MAX, INT16_MIN};\n    size_t i;\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (i = 0; i < length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    // Horizontal reduction to find the minimum\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shufflehi_epi16(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shufflelo_epi16(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n\n    // Horizontal reduction to find the maximum\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shuffle_epi32(max_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shufflehi_epi16(max_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shufflelo_epi16(max_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n\n    result.min_num = _mm256_extract_epi16(min_vec, 0);\n    result.max_num = _mm256_extract_epi16(max_vec, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = {INT16_MAX, INT16_MIN};\n    size_t i;\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    for (i = 0; i < length / 16 * 16; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int16_t value = vec[i];\n        if (value < result.min_num) result.min_num = value;\n        if (value > result.max_num) result.max_num = value;\n    }\n\n    // Horizontal reduction to find min and max\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shuffle_epi32(min_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shufflehi_epi16(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    min_vec = _mm256_min_epi16(min_vec, _mm256_shufflelo_epi16(min_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shuffle_epi32(max_vec, _MM_SHUFFLE(2, 3, 0, 1)));\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shufflehi_epi16(max_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n    max_vec = _mm256_max_epi16(max_vec, _mm256_shufflelo_epi16(max_vec, _MM_SHUFFLE(1, 0, 3, 2)));\n\n    result.min_num = _mm256_extract_epi16(min_vec, 0);\n    result.max_num = _mm256_extract_epi16(max_vec, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer vector (vec);\n- The length of the given vector.\n\nYour goal is to return the the maximum and minimum values in the given vector.\nThe function should return a value of the defined `MinMaxPair` type:\n```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n```\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    int16_t min_val = vec[0];\n    int16_t max_val = vec[0];\n\n    if (length <= 0) {\n        result.min_num = min_val;\n        result.max_num = max_val;\n        return result;\n    }\n\n    size_t i = 0;\n    if (length >= 16) {\n        __m256i min_vec = _mm256_set1_epi16(min_val);\n        __m256i max_vec = _mm256_set1_epi16(max_val);\n\n        for (i = 0; i <= length - 16; i += 16) {\n            __m256i data = _mm256_loadu_si256((const __m256i*)&vec[i]);\n            min_vec = _mm256_min_epi16(min_vec, data);\n            max_vec = _mm256_max_epi16(max_vec, data);\n        }\n\n        int16_t temp_min[16];\n        int16_t temp_max[16];\n        _mm256_storeu_si256((__m256i*)temp_min, min_vec);\n        _mm256_storeu_si256((__m256i*)temp_max, max_vec);\n\n        for (int j = 0; j < 16; ++j) {\n            min_val = (temp_min[j] < min_val) ? temp_min[j] : min_val;\n            max_val = (temp_max[j] > max_val) ? temp_max[j] : max_val;\n        }\n    }\n\n    for (; i < length; ++i) {\n        min_val = (vec[i] < min_val) ? vec[i] : min_val;\n        max_val = (vec[i] > max_val) ? vec[i] : max_val;\n    }\n\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit integer vector (vec);\n- The length of the given vector.\n\nYour goal is to return the the maximum and minimum values in the given vector.\nThe function should return a value of the defined `MinMaxPair` type:\n```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n```\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    int16_t min_val = vec[0];\n    int16_t max_val = vec[0];\n\n    size_t i = 0;\n    if (length >= 16) {\n        __m256i min_vec = _mm256_set1_epi16(vec[0]);\n        __m256i max_vec = _mm256_set1_epi16(vec[0]);\n\n        for (i = 0; i <= length - 16; i += 16) {\n            __m256i data = _mm256_loadu_si256((const __m256i*)&vec[i]);\n            min_vec = _mm256_min_epi16(min_vec, data);\n            max_vec = _mm256_max_epi16(max_vec, data);\n        }\n\n        int16_t temp_min[16];\n        int16_t temp_max[16];\n        _mm256_storeu_si256((__m256i*)temp_min, min_vec);\n        _mm256_storeu_si256((__m256i*)temp_max, max_vec);\n\n        for (int j = 0; j < 16; ++j) {\n            min_val = (temp_min[j] < min_val) ? temp_min[j] : min_val;\n            max_val = (temp_max[j] > max_val) ? temp_max[j] : max_val;\n        }\n    }\n\n    for (; i < length; ++i) {\n        min_val = (vec[i] < min_val) ? vec[i] : min_val;\n        max_val = (vec[i] > max_val) ? vec[i] : max_val;\n    }\n\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer matrix (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if all rows in the matrix are strictly increasing.\nIf the input matrix is empty, return false.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer matrix (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if all rows in the matrix are strictly increasing.\nIf the input matrix is empty, return false.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 1; ) {\n            if (cols - j >= 8) {\n                __m256i current = _mm256_loadu_si256((const __m256i*)&matrix[i * cols + j]);\n                __m256i next = _mm256_loadu_si256((const __m256i*)&matrix[i * cols + j + 1]);\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n                j += 8;\n            } else {\n                if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                    return false;\n                }\n                ++j;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer matrix (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if all rows in the matrix are strictly increasing.\nIf the input matrix is empty, return false.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer matrix (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if all rows in the matrix are strictly increasing.\nIf the input matrix is empty, return false.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 32-bit integer matrix (matrix);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nYour goal is to check if all rows in the matrix are strictly increasing.\nIf the input matrix is empty, return false.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 1; ) {\n            if (cols - j >= 8) {\n                __m256i current = _mm256_loadu_si256((const __m256i*)&matrix[i * cols + j]);\n                __m256i next = _mm256_loadu_si256((const __m256i*)&matrix[i * cols + j + 1]);\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n                j += 8;\n            } else {\n                if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                    return false;\n                }\n                ++j;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < num_blocks_rows; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_cols; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue;\n\n            const int* current_block = matrix + block_row * block_size * cols + block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                size_t j = 0;\n                for (; j + 7 < block_size; j += 8) {\n                    __m256i first_vec = _mm256_loadu_si256((const __m256i*)(first_block + i * cols + j));\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_block + i * cols + j));\n                    __m256i result = _mm256_cmpeq_epi32(first_vec, current_vec);\n                    int mask = _mm256_movemask_epi8(result);\n                    if (mask != 0xFFFFFFFF) return false;\n                }\n                for (; j < block_size; ++j) {\n                    if (first_block[i * cols + j] != current_block[i * cols + j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_rows = rows / block_size;\n    size_t num_blocks_cols = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            if (i == 0 && j == 0) continue;\n\n            const int* current_block = matrix + i * block_size * cols + j * block_size;\n\n            for (size_t row = 0; row < block_size; ++row) {\n                size_t col = 0;\n                for (; col + 8 <= block_size; col += 8) {\n                    __m256i first_vec = _mm256_loadu_si256((const __m256i*)(first_block + row * cols + col));\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_block + row * cols + col));\n                    __m256i result = _mm256_cmpeq_epi32(first_vec, current_vec);\n                    int mask = _mm256_movemask_epi8(result);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n                for (; col < block_size; ++col) {\n                    if (first_block[row * cols + col] != current_block[row * cols + col]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue;\n\n            const int* current_block = matrix + block_row * block_size * cols + block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                size_t j = 0;\n                for (; j + 7 < block_size; j += 8) {\n                    __m256i first_vec = _mm256_loadu_si256((const __m256i*)(first_block + i * cols + j));\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_block + i * cols + j));\n                    __m256i cmp_result = _mm256_cmpeq_epi32(first_vec, current_vec);\n                    int mask = _mm256_movemask_epi8(cmp_result);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n                for (; j < block_size; ++j) {\n                    if (first_block[i * cols + j] != current_block[i * cols + j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue;\n\n            const int* current_block = matrix + block_row * block_size * cols + block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                size_t j = 0;\n                for (; j + 7 < block_size; j += 8) {\n                    __m256i first_vec = _mm256_loadu_si256((const __m256i*)(first_block + i * cols + j));\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_block + i * cols + j));\n                    __m256i result = _mm256_cmpeq_epi32(first_vec, current_vec);\n                    int mask = _mm256_movemask_epi8(result);\n                    if (mask != 0xFFFFFFFF) return false;\n                }\n                for (; j < block_size; ++j) {\n                    if (first_block[i * cols + j] != current_block[i * cols + j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue;\n\n            const int* current_block = matrix + block_row * block_size * cols + block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                size_t j = 0;\n                for (; j + 7 < block_size; j += 8) {\n                    __m256i first_vec = _mm256_loadu_si256((const __m256i*)(first_block + i * cols + j));\n                    __m256i current_vec = _mm256_loadu_si256((const __m256i*)(current_block + i * cols + j));\n                    __m256i result = _mm256_cmpeq_epi32(first_vec, current_vec);\n                    int mask = _mm256_movemask_epi8(result);\n                    if (mask != 0xFFFFFFFF) {\n                        return false;\n                    }\n                }\n                for (; j < block_size; ++j) {\n                    if (first_block[i * cols + j] != current_block[i * cols + j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 64-bit floating-point vector (vec);\n- A 64-bit floating-point tolerance;\n- The length of the given vector (length);\n- The size of each block (block_size).\n\nYour goal is to check whether all blocks (excluding tail elements that cannot form a complete block)\nof the vector are element-wise equal to the first block, within the specified tolerance.\nReturn false is the input is illegal or no complete blocks in block_size.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || tolerance < 0.0 || block_size > length) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    size_t num_simd_blocks = block_size / 4;\n    size_t remaining_elements = block_size % 4;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n        const double* first_block = vec;\n\n        for (size_t j = 0; j < num_simd_blocks; ++j) {\n            __m256d first_vec = _mm256_loadu_pd(first_block + j * 4);\n            __m256d current_vec = _mm256_loadu_pd(current_block + j * 4);\n\n            __m256d diff = _mm256_sub_pd(first_vec, current_vec);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n\n            __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n            __m256d comparison = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OS);\n\n            if (_mm256_movemask_pd(comparison) != 0) {\n                return false;\n            }\n        }\n\n        for (size_t j = num_simd_blocks * 4; j < block_size; ++j) {\n            if (fabs(first_block[j] - current_block[j]) > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 64-bit floating-point vector (vec);\n- A 64-bit floating-point tolerance;\n- The length of the given vector (length);\n- The size of each block (block_size).\n\nYour goal is to check whether all blocks (excluding tail elements that cannot form a complete block)\nof the vector are element-wise equal to the first block, within the specified tolerance.\nReturn false is the input is illegal or no complete blocks in block_size.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || tolerance < 0.0 || block_size > length) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n    const double* first_block = vec;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block_vec = _mm256_loadu_pd(first_block + j);\n            __m256d current_block_vec = _mm256_loadu_pd(current_block + j);\n            __m256d diff_vec = _mm256_sub_pd(first_block_vec, current_block_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, tolerance_vec, _CMP_GT_OQ);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || tolerance < 0) {\n        return false;\n    }\n\n    if (length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    __m256d first_block_vec[block_size / 4];\n    for (size_t i = 0; i < block_size / 4; ++i) {\n        first_block_vec[i] = _mm256_loadu_pd(&vec[i * 4]);\n    }\n\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        for (size_t i = 0; i < block_size / 4; ++i) {\n            __m256d current_block_vec = _mm256_loadu_pd(&vec[block_idx * block_size + i * 4]);\n            __m256d diff_vec = _mm256_sub_pd(first_block_vec[i], current_block_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, tolerance_vec, _CMP_GT_OS);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 64-bit floating-point vector (vec);\n- A 64-bit floating-point tolerance;\n- The length of the given vector (length);\n- The size of each block (block_size).\n\nYour goal is to check whether all blocks (excluding tail elements that cannot form a complete block)\nof the vector are element-wise equal to the first block, within the specified tolerance.\nReturn false is the input is illegal or no complete blocks in block_size.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || tolerance < 0.0 || block_size > length) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    size_t num_simd_blocks = block_size / 4;\n    size_t remaining_elements = block_size % 4;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n        const double* first_block = vec;\n\n        for (size_t j = 0; j < num_simd_blocks; ++j) {\n            __m256d first_vec = _mm256_loadu_pd(first_block + j * 4);\n            __m256d current_vec = _mm256_loadu_pd(current_block + j * 4);\n\n            __m256d diff = _mm256_sub_pd(first_vec, current_vec);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n\n            __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n            __m256d comparison = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OS);\n\n            if (_mm256_movemask_pd(comparison) != 0) {\n                return false;\n            }\n        }\n\n        for (size_t j = num_simd_blocks * 4; j < block_size; ++j) {\n            if (fabs(first_block[j] - current_block[j]) > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a 64-bit floating-point vector (vec);\n- A 64-bit floating-point tolerance;\n- The length of the given vector (length);\n- The size of each block (block_size).\n\nYour goal is to check whether all blocks (excluding tail elements that cannot form a complete block)\nof the vector are element-wise equal to the first block, within the specified tolerance.\nReturn false is the input is illegal or no complete blocks in block_size.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || tolerance < 0.0 || block_size > length) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks == 0) {\n        return false;\n    }\n\n    size_t num_simd_blocks = block_size / 4;\n    size_t remaining_elements = block_size % 4;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        const double* current_block = vec + i * block_size;\n        const double* first_block = vec;\n\n        for (size_t j = 0; j < num_simd_blocks; ++j) {\n            __m256d first_vec = _mm256_loadu_pd(first_block + j * 4);\n            __m256d current_vec = _mm256_loadu_pd(current_block + j * 4);\n\n            __m256d diff = _mm256_sub_pd(first_vec, current_vec);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n\n            __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n            __m256d cmp_result = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OS);\n\n            if (_mm256_movemask_pd(cmp_result) != 0) {\n                return false;\n            }\n        }\n\n        for (size_t j = num_simd_blocks * 4; j < block_size; ++j) {\n            if (fabs(first_block[j] - current_block[j]) > tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (thresholds);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n- A pointer to a boolean result array (result).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to check if the Euclidean norm of each row in the matrix exceeds the corresponding threshold in the vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_sq_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            sum_sq_vec = _mm256_add_pd(sum_sq_vec, _mm256_mul_pd(row_vec, row_vec));\n        }\n\n        double sum_sq = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_sq_vec);\n        sum_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            sum_sq += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        result[i] = (sqrt(sum_sq) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (thresholds);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n- A pointer to a boolean result array (result).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to check if the Euclidean norm of each row in the matrix exceeds the corresponding threshold in the vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_sq_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_sq_vec = _mm256_add_pd(sum_sq_vec, _mm256_mul_pd(row_vec, row_vec));\n        }\n\n        double sum_sq = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_sq_vec);\n        sum_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            sum_sq += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        result[i] = (sqrt(sum_sq) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (thresholds);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n- A pointer to a boolean result array (result).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to check if the Euclidean norm of each row in the matrix exceeds the corresponding threshold in the vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_sq_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            sum_sq_vec = _mm256_add_pd(sum_sq_vec, _mm256_mul_pd(row_vec, row_vec));\n        }\n\n        double sum_sq = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_sq_vec);\n        sum_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            sum_sq += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        result[i] = (sqrt(sum_sq) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (thresholds);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n- A pointer to a boolean result array (result).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to check if the Euclidean norm of each row in the matrix exceeds the corresponding threshold in the vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_sq_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_sq_vec = _mm256_add_pd(sum_sq_vec, _mm256_mul_pd(row_vec, row_vec));\n        }\n\n        double sum_sq = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_sq_vec);\n        sum_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            sum_sq += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        result[i] = (sqrt(sum_sq) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit floating-point matrix (matrix);\n- A pointer to a non-empty 64-bit floating-point vector (thresholds);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n- A pointer to a boolean result array (result).\n\nThe input matrix is flattened into a 1D array.\nYour goal is to check if the Euclidean norm of each row in the matrix exceeds the corresponding threshold in the vector.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_sq_vec = _mm256_setzero_pd();\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(matrix + i * cols + j);\n            sum_sq_vec = _mm256_add_pd(sum_sq_vec, _mm256_mul_pd(row_vec, row_vec));\n        }\n\n        double sum_sq = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_sq_vec);\n        sum_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            sum_sq += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        result[i] = (sqrt(sum_sq) > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    // Process 8 elements at a time using AVX2\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i < length / 8 * 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    // Process 8 elements at a time\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    size_t vec_size = length / 8 * 8; // Process in chunks of 8 for AVX2\n\n    for (i = 0; i < vec_size; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    // Process 8 elements at a time\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i + 32 <= length; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_madd_epi16(a_vec, a_vec));\n        sum_products = _mm256_add_epi64(sum_products, _mm256_madd_epi16(a_vec, b_vec));\n    }\n\n    int64_t squares[4], products[4];\n    _mm256_storeu_si256((__m256i*)squares, sum_squares);\n    _mm256_storeu_si256((__m256i*)products, sum_products);\n\n    int64_t total_squares = squares[0] + squares[1] + squares[2] + squares[3];\n    int64_t total_products = products[0] + products[1] + products[2] + products[3];\n\n    for (; i < length; i++) {\n        total_squares += (int64_t)A[i] * (int64_t)A[i];\n        total_products += (int64_t)A[i] * (int64_t)B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a1 = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i a2 = _mm256_loadu_si256((__m256i*)&A[i + 16]);\n        __m256i b1 = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i b2 = _mm256_loadu_si256((__m256i*)&B[i + 16]);\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_add_epi64(_mm256_madd_epi16(a1, a1), _mm256_madd_epi16(a2, a2)));\n        sum_products = _mm256_add_epi64(sum_products, _mm256_add_epi64(_mm256_madd_epi16(a1, b1), _mm256_madd_epi16(a2, b2)));\n    }\n\n    int64_t squares = _mm256_reduce_add_epi64(sum_squares);\n    int64_t products = _mm256_reduce_add_epi64(sum_products);\n\n    for (; i < length; i++) {\n        squares += A[i] * A[i];\n        products += A[i] * B[i];\n    }\n\n    return squares - products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_madd_epi16(a, a));\n        sum_products = _mm256_add_epi64(sum_products, _mm256_madd_epi16(a, b));\n    }\n\n    int64_t result_squares[4], result_products[4];\n    _mm256_storeu_si256((__m256i*)result_squares, sum_squares);\n    _mm256_storeu_si256((__m256i*)result_products, sum_products);\n\n    int64_t total_squares = result_squares[0] + result_squares[1] + result_squares[2] + result_squares[3];\n    int64_t total_products = result_products[0] + result_products[1] + result_products[2] + result_products[3];\n\n    for (; i < length; i++) {\n        total_squares += A[i] * A[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_madd_epi16(a_vec, a_vec));\n        sum_products = _mm256_add_epi64(sum_products, _mm256_madd_epi16(a_vec, b_vec));\n    }\n\n    int64_t result_squares[4], result_products[4];\n    _mm256_storeu_si256((__m256i*)result_squares, sum_squares);\n    _mm256_storeu_si256((__m256i*)result_products, sum_products);\n\n    int64_t total_squares = result_squares[0] + result_squares[1] + result_squares[2] + result_squares[3];\n    int64_t total_products = result_products[0] + result_products[1] + result_products[2] + result_products[3];\n\n    for (; i < length; i++) {\n        total_squares += A[i] * A[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nThe inputs to this function are::\n- Pointers to two non-empty 8-bit integer vector (A, B);\n- The common length of A and B\n\nYour goal is to return the difference between the sum of squares and the sum of products.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    size_t i = 0;\n    int64_t sum_sq = 0;\n    int64_t sum_prod = 0;\n\n    if (length >= 32) {\n        __m256i sum_sq_vec = _mm256_setzero_si256();\n        __m256i sum_prod_vec = _mm256_setzero_si256();\n\n        for (; i + 32 <= length; i += 32) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n            __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n            __m256i a_squared = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(a), _mm256_cvtepi8_epi16(a));\n            __m256i b_squared = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(b), _mm256_cvtepi8_epi16(b));\n            __m256i prod = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(a), _mm256_cvtepi8_epi16(b));\n\n            __m256i sum_sq_intermediate = _mm256_add_epi32(_mm256_cvtepi16_epi32(a_squared), _mm256_cvtepi16_epi32(b_squared));\n            __m256i sum_prod_intermediate = _mm256_cvtepi16_epi32(prod);\n\n            sum_sq_vec = _mm256_add_epi64(sum_sq_vec, _mm256_cvtepi32_epi64(sum_sq_intermediate));\n            sum_prod_vec = _mm256_add_epi64(sum_prod_vec, _mm256_cvtepi32_epi64(sum_prod_intermediate));\n        }\n\n        int64_t temp_sum_sq[4] = {0};\n        int64_t temp_sum_prod[4] = {0};\n        _mm256_storeu_si256((__m256i*)temp_sum_sq, sum_sq_vec);\n        _mm256_storeu_si256((__m256i*)temp_sum_prod, sum_prod_vec);\n\n        sum_sq = temp_sum_sq[0] + temp_sum_sq[1] + temp_sum_sq[2] + temp_sum_sq[3];\n        sum_prod = temp_sum_prod[0] + temp_sum_prod[1] + temp_sum_prod[2] + temp_sum_prod[3];\n    }\n\n    for (; i < length; ++i) {\n        sum_sq += (int64_t)A[i] * A[i] + (int64_t)B[i] * B[i];\n        sum_prod += (int64_t)A[i] * B[i];\n    }\n\n    return sum_sq - sum_prod;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit unsigned integer tensor (A);\n- A pointer to a non-empty 64-bit unsigned integer tensor (B);\n- A pointer to a non-empty 64-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise bitwise AND operation between tensor A and the bitwise negation of tensor B,\nand then store the result in tensor C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit unsigned integer tensor (A);\n- A pointer to a non-empty 64-bit unsigned integer tensor (B);\n- A pointer to a non-empty 64-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise bitwise AND operation between tensor A and the bitwise negation of tensor B,\nand then store the result in tensor C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit unsigned integer tensor (A);\n- A pointer to a non-empty 64-bit unsigned integer tensor (B);\n- A pointer to a non-empty 64-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise bitwise AND operation between tensor A and the bitwise negation of tensor B,\nand then store the result in tensor C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit unsigned integer tensor (A);\n- A pointer to a non-empty 64-bit unsigned integer tensor (B);\n- A pointer to a non-empty 64-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise bitwise AND operation between tensor A and the bitwise negation of tensor B,\nand then store the result in tensor C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 64-bit unsigned integer tensor (A);\n- A pointer to a non-empty 64-bit unsigned integer tensor (B);\n- A pointer to a non-empty 64-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise bitwise AND operation between tensor A and the bitwise negation of tensor B,\nand then store the result in tensor C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit integer tensor (A);\n- A pointer to a non-empty 8-bit integer tensor (B);\n- A pointer to a non-empty 8-bit integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise three-valued logic operation on tensors A and B,\nif A[i][j][k] and B[i][j][k] are both positive, C[i][j][k] should be 1;\nif A[i][j][k] and B[i][j][k] are both negative, C[i][j][k] should be -1;\nin other cases, C[i][j][k] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i pos_mask_a = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i pos_mask_b = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n        __m256i neg_mask_a = _mm256_cmplt_epi8(a, _mm256_setzero_si256());\n        __m256i neg_mask_b = _mm256_cmplt_epi8(b, _mm256_setzero_si256());\n\n        __m256i both_pos = _mm256_and_si256(pos_mask_a, pos_mask_b);\n        __m256i both_neg = _mm256_and_si256(neg_mask_a, neg_mask_b);\n\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i neg_ones = _mm256_set1_epi8(-1);\n        __m256i zeros = _mm256_setzero_si256();\n\n        __m256i result = _mm256_blendv_epi8(zeros, ones, both_pos);\n        result = _mm256_blendv_epi8(result, neg_ones, both_neg);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit integer tensor (A);\n- A pointer to a non-empty 8-bit integer tensor (B);\n- A pointer to a non-empty 8-bit integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise three-valued logic operation on tensors A and B,\nif A[i][j][k] and B[i][j][k] are both positive, C[i][j][k] should be 1;\nif A[i][j][k] and B[i][j][k] are both negative, C[i][j][k] should be -1;\nin other cases, C[i][j][k] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i a_neg = _mm256_cmplt_epi8(a, zero);\n        __m256i b_neg = _mm256_cmplt_epi8(b, zero);\n\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i pos_result = _mm256_and_si256(both_pos, _mm256_set1_epi8(1));\n        __m256i neg_result = _mm256_and_si256(both_neg, _mm256_set1_epi8(-1));\n\n        __m256i result = _mm256_or_si256(pos_result, neg_result);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit integer tensor (A);\n- A pointer to a non-empty 8-bit integer tensor (B);\n- A pointer to a non-empty 8-bit integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise three-valued logic operation on tensors A and B,\nif A[i][j][k] and B[i][j][k] are both positive, C[i][j][k] should be 1;\nif A[i][j][k] and B[i][j][k] are both negative, C[i][j][k] should be -1;\nin other cases, C[i][j][k] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i a_neg = _mm256_cmplt_epi8(a, zero);\n        __m256i b_neg = _mm256_cmplt_epi8(b, zero);\n\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i pos_result = _mm256_and_si256(both_pos, _mm256_set1_epi8(1));\n        __m256i neg_result = _mm256_and_si256(both_neg, _mm256_set1_epi8(-1));\n\n        __m256i result = _mm256_or_si256(pos_result, neg_result);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit integer tensor (A);\n- A pointer to a non-empty 8-bit integer tensor (B);\n- A pointer to a non-empty 8-bit integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise three-valued logic operation on tensors A and B,\nif A[i][j][k] and B[i][j][k] are both positive, C[i][j][k] should be 1;\nif A[i][j][k] and B[i][j][k] are both negative, C[i][j][k] should be -1;\nin other cases, C[i][j][k] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i a_pos = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_pos = _mm256_cmpgt_epi8(b, zero);\n        __m256i a_neg = _mm256_cmplt_epi8(a, zero);\n        __m256i b_neg = _mm256_cmplt_epi8(b, zero);\n\n        __m256i both_pos = _mm256_and_si256(a_pos, b_pos);\n        __m256i both_neg = _mm256_and_si256(a_neg, b_neg);\n\n        __m256i pos_result = _mm256_and_si256(both_pos, _mm256_set1_epi8(1));\n        __m256i neg_result = _mm256_and_si256(both_neg, _mm256_set1_epi8(-1));\n\n        __m256i result = _mm256_or_si256(pos_result, neg_result);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 8-bit integer tensor (A);\n- A pointer to a non-empty 8-bit integer tensor (B);\n- A pointer to a non-empty 8-bit integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform element-wise three-valued logic operation on tensors A and B,\nif A[i][j][k] and B[i][j][k] are both positive, C[i][j][k] should be 1;\nif A[i][j][k] and B[i][j][k] are both negative, C[i][j][k] should be -1;\nin other cases, C[i][j][k] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i pos_mask_a = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i pos_mask_b = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n        __m256i neg_mask_a = _mm256_cmplt_epi8(a, _mm256_setzero_si256());\n        __m256i neg_mask_b = _mm256_cmplt_epi8(b, _mm256_setzero_si256());\n        __m256i both_pos = _mm256_and_si256(pos_mask_a, pos_mask_b);\n        __m256i both_neg = _mm256_and_si256(neg_mask_a, neg_mask_b);\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i neg_ones = _mm256_set1_epi8(-1);\n        __m256i result = _mm256_blendv_epi8(_mm256_blendv_epi8(_mm256_setzero_si256(), ones, both_pos), neg_ones, both_neg);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit unsigned integer tensor (A);\n- A pointer to a non-empty 16-bit unsigned integer tensor (B);\n- A pointer to a non-empty 16-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform the following operation to A and B,\nand then store the result in tensor C:\nfor the element **exactly** at the midpoint: keep the value in A;\nfor elements before the midpoint: values are element-wise bitwise XOR operation between A and B;\nfor elements before the midpoint: values are element-wise bitwise AND operation between A and B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 15 < midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length > 0) {\n        C[midpoint] = A[midpoint];\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (i = midpoint + 1; i + 15 < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit unsigned integer tensor (A);\n- A pointer to a non-empty 16-bit unsigned integer tensor (B);\n- A pointer to a non-empty 16-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform the following operation to A and B,\nand then store the result in tensor C:\nfor the element **exactly** at the midpoint: keep the value in A;\nfor elements before the midpoint: values are element-wise bitwise XOR operation between A and B;\nfor elements before the midpoint: values are element-wise bitwise AND operation between A and B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2 intrinsics\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Process the element at the midpoint\n    if (length > 0) {\n        C[midpoint] = A[midpoint];\n    }\n\n    // Process elements after the midpoint using AVX2 intrinsics\n    i = midpoint + 1;\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit unsigned integer tensor (A);\n- A pointer to a non-empty 16-bit unsigned integer tensor (B);\n- A pointer to a non-empty 16-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform the following operation to A and B,\nand then store the result in tensor C:\nfor the element **exactly** at the midpoint: keep the value in A;\nfor elements before the midpoint: values are element-wise bitwise XOR operation between A and B;\nfor elements before the midpoint: values are element-wise bitwise AND operation between A and B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2 intrinsics\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length > 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        return;\n    }\n    \n\n    // Process elements after the midpoint using AVX2 intrinsics\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit unsigned integer tensor (A);\n- A pointer to a non-empty 16-bit unsigned integer tensor (B);\n- A pointer to a non-empty 16-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform the following operation to A and B,\nand then store the result in tensor C:\nfor the element **exactly** at the midpoint: keep the value in A;\nfor elements before the midpoint: values are element-wise bitwise XOR operation between A and B;\nfor elements before the midpoint: values are element-wise bitwise AND operation between A and B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n\n    size_t i = 0;\n    for (; i + 15 < midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    if (length % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = A[midpoint];\n    }\n\n    i = midpoint + 1;\n    for (; i + 15 < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 16-bit unsigned integer tensor (A);\n- A pointer to a non-empty 16-bit unsigned integer tensor (B);\n- A pointer to a non-empty 16-bit unsigned integer tensor (C);\n- The common length of A, B, and C.\n\nYour goal is to perform the following operation to A and B,\nand then store the result in tensor C:\nfor the element **exactly** at the midpoint: keep the value in A;\nfor elements before the midpoint: values are element-wise bitwise XOR operation between A and B;\nfor elements before the midpoint: values are element-wise bitwise AND operation between A and B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2 intrinsics\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length > 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        return;\n    }\n    \n\n    // Process elements after the midpoint using AVX2 intrinsics\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (C);\n- A pointer to a non-empty 32-bit floating-point tensor (D);\n- A pointer to a boolean condition tensor (cond1);\n- A pointer to a boolean condition tensor (cond2);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA, B, C, and D are 3D tensors that are flattened into 1D arrays.\nYour goal is to perform element-wise selection based on the conditions in cond1 and cond2.\nIf cond1[i][j][k] is true and cond2[i][j][k] is true, out[i][j][k] should be A[i][j][k];\nif cond1[i][j][k] is true and cond2[i][j][k] is false, out[i][j][k] should be B[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is true, out[i][j][k] should be C[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is false, out[i][j][k] should be D[i][j][k].\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 cond1_vec = _mm256_set_epi32(cond1[i+7], cond1[i+6], cond1[i+5], cond1[i+4], cond1[i+3], cond1[i+2], cond1[i+1], cond1[i]);\n        __m256 cond2_vec = _mm256_set_epi32(cond2[i+7], cond2[i+6], cond2[i+5], cond2[i+4], cond2[i+3], cond2[i+2], cond2[i+1], cond2[i]);\n\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 result_vec;\n\n        __m256 cond1_float = _mm256_cmp_ps(cond1_vec, _mm256_setzero_ps(), _CMP_NEQ_OQ);\n        __m256 cond2_float = _mm256_cmp_ps(cond2_vec, _mm256_setzero_ps(), _CMP_NEQ_OQ);\n\n        __m256 both_true = _mm256_and_ps(cond1_float, cond2_float);\n        __m256 cond1_true_cond2_false = _mm256_and_ps(cond1_float, _mm256_andnot_ps(cond2_float, _mm256_set1_ps(1.0f)));\n        __m256 cond1_false_cond2_true = _mm256_and_ps(_mm256_andnot_ps(cond1_float, _mm256_set1_ps(1.0f)), cond2_float);\n        __m256 both_false = _mm256_and_ps(_mm256_andnot_ps(cond1_float, _mm256_set1_ps(1.0f)), _mm256_andnot_ps(cond2_float, _mm256_set1_ps(1.0f)));\n\n        result_vec = _mm256_blendv_ps(d_vec, a_vec, both_true);\n        result_vec = _mm256_blendv_ps(result_vec, b_vec, cond1_true_cond2_false);\n        result_vec = _mm256_blendv_ps(result_vec, c_vec, cond1_false_cond2_true);\n\n        _mm256_storeu_ps(&out[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (C);\n- A pointer to a non-empty 32-bit floating-point tensor (D);\n- A pointer to a boolean condition tensor (cond1);\n- A pointer to a boolean condition tensor (cond2);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA, B, C, and D are 3D tensors that are flattened into 1D arrays.\nYour goal is to perform element-wise selection based on the conditions in cond1 and cond2.\nIf cond1[i][j][k] is true and cond2[i][j][k] is true, out[i][j][k] should be A[i][j][k];\nif cond1[i][j][k] is true and cond2[i][j][k] is false, out[i][j][k] should be B[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is true, out[i][j][k] should be C[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is false, out[i][j][k] should be D[i][j][k].\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 cond1_vec = _mm256_set_epi32(cond1[i+7], cond1[i+6], cond1[i+5], cond1[i+4], cond1[i+3], cond1[i+2], cond1[i+1], cond1[i]);\n        __m256 cond2_vec = _mm256_set_epi32(cond2[i+7], cond2[i+6], cond2[i+5], cond2[i+4], cond2[i+3], cond2[i+2], cond2[i+1], cond2[i]);\n\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 out_vec;\n\n        __m256 cond1_float = _mm256_cmp_ps(_mm256_setzero_ps(), cond1_vec, _CMP_EQ_OQ);\n        __m256 cond2_float = _mm256_cmp_ps(_mm256_setzero_ps(), cond2_vec, _CMP_EQ_OQ);\n\n        __m256 select1 = _mm256_blendv_ps(a_vec, b_vec, cond2_float);\n        __m256 select2 = _mm256_blendv_ps(c_vec, d_vec, cond2_float);\n\n        out_vec = _mm256_blendv_ps(select1, select2, cond1_float);\n\n        _mm256_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (C);\n- A pointer to a non-empty 32-bit floating-point tensor (D);\n- A pointer to a boolean condition tensor (cond1);\n- A pointer to a boolean condition tensor (cond2);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA, B, C, and D are 3D tensors that are flattened into 1D arrays.\nYour goal is to perform element-wise selection based on the conditions in cond1 and cond2.\nIf cond1[i][j][k] is true and cond2[i][j][k] is true, out[i][j][k] should be A[i][j][k];\nif cond1[i][j][k] is true and cond2[i][j][k] is false, out[i][j][k] should be B[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is true, out[i][j][k] should be C[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is false, out[i][j][k] should be D[i][j][k].\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 cond1_vec = _mm256_set_epi32(cond1[i+7], cond1[i+6], cond1[i+5], cond1[i+4], cond1[i+3], cond1[i+2], cond1[i+1], cond1[i]);\n        __m256 cond2_vec = _mm256_set_epi32(cond2[i+7], cond2[i+6], cond2[i+5], cond2[i+4], cond2[i+3], cond2[i+2], cond2[i+1], cond2[i]);\n\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 out_vec;\n\n        __m256 cond1_float = _mm256_cmp_ps(_mm256_setzero_ps(), cond1_vec, _CMP_EQ_OQ);\n        __m256 cond2_float = _mm256_cmp_ps(_mm256_setzero_ps(), cond2_vec, _CMP_EQ_OQ);\n\n        __m256 select1 = _mm256_blendv_ps(a_vec, b_vec, cond2_float);\n        __m256 select2 = _mm256_blendv_ps(c_vec, d_vec, cond2_float);\n\n        out_vec = _mm256_blendv_ps(select1, select2, cond1_float);\n\n        _mm256_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (C);\n- A pointer to a non-empty 32-bit floating-point tensor (D);\n- A pointer to a boolean condition tensor (cond1);\n- A pointer to a boolean condition tensor (cond2);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA, B, C, and D are 3D tensors that are flattened into 1D arrays.\nYour goal is to perform element-wise selection based on the conditions in cond1 and cond2.\nIf cond1[i][j][k] is true and cond2[i][j][k] is true, out[i][j][k] should be A[i][j][k];\nif cond1[i][j][k] is true and cond2[i][j][k] is false, out[i][j][k] should be B[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is true, out[i][j][k] should be C[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is false, out[i][j][k] should be D[i][j][k].\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 cond1_vec = _mm256_set_epi32(cond1[i+7], cond1[i+6], cond1[i+5], cond1[i+4], cond1[i+3], cond1[i+2], cond1[i+1], cond1[i]);\n        __m256 cond2_vec = _mm256_set_epi32(cond2[i+7], cond2[i+6], cond2[i+5], cond2[i+4], cond2[i+3], cond2[i+2], cond2[i+1], cond2[i]);\n\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 result_vec;\n\n        __m256 cond1_float = _mm256_cmp_ps(cond1_vec, _mm256_setzero_ps(), _CMP_NEQ_OQ);\n        __m256 cond2_float = _mm256_cmp_ps(cond2_vec, _mm256_setzero_ps(), _CMP_NEQ_OQ);\n\n        __m256 both_true = _mm256_and_ps(cond1_float, cond2_float);\n        __m256 cond1_true_cond2_false = _mm256_and_ps(cond1_float, _mm256_andnot_ps(cond2_float, _mm256_set1_ps(1.0f)));\n        __m256 cond1_false_cond2_true = _mm256_and_ps(_mm256_andnot_ps(cond1_float, _mm256_set1_ps(1.0f)), cond2_float);\n        __m256 both_false = _mm256_and_ps(_mm256_andnot_ps(cond1_float, _mm256_set1_ps(1.0f)), _mm256_andnot_ps(cond2_float, _mm256_set1_ps(1.0f)));\n\n        result_vec = _mm256_blendv_ps(d_vec, a_vec, both_true);\n        result_vec = _mm256_blendv_ps(result_vec, b_vec, cond1_true_cond2_false);\n        result_vec = _mm256_blendv_ps(result_vec, c_vec, cond1_false_cond2_true);\n\n        _mm256_storeu_ps(&out[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (C);\n- A pointer to a non-empty 32-bit floating-point tensor (D);\n- A pointer to a boolean condition tensor (cond1);\n- A pointer to a boolean condition tensor (cond2);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA, B, C, and D are 3D tensors that are flattened into 1D arrays.\nYour goal is to perform element-wise selection based on the conditions in cond1 and cond2.\nIf cond1[i][j][k] is true and cond2[i][j][k] is true, out[i][j][k] should be A[i][j][k];\nif cond1[i][j][k] is true and cond2[i][j][k] is false, out[i][j][k] should be B[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is true, out[i][j][k] should be C[i][j][k];\nif cond1[i][j][k] is false and cond2[i][j][k] is false, out[i][j][k] should be D[i][j][k].\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 cond1_vec = _mm256_set_epi32(cond1[i + 7], cond1[i + 6], cond1[i + 5], cond1[i + 4],\n                                            cond1[i + 3], cond1[i + 2], cond1[i + 1], cond1[i]);\n        __m256 cond2_vec = _mm256_set_epi32(cond2[i + 7], cond2[i + 6], cond2[i + 5], cond2[i + 4],\n                                            cond2[i + 3], cond2[i + 2], cond2[i + 1], cond2[i]);\n\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 result_vec;\n\n        __m256 cond1_true = _mm256_cmp_ps(cond1_vec, _mm256_set1_ps(1.0f), _CMP_EQ_OQ);\n        __m256 cond2_true = _mm256_cmp_ps(cond2_vec, _mm256_set1_ps(1.0f), _CMP_EQ_OQ);\n\n        __m256 both_true = _mm256_and_ps(cond1_true, cond2_true);\n        __m256 cond1_true_cond2_false = _mm256_and_ps(cond1_true, _mm256_xor_ps(cond2_true, _mm256_set1_ps(-1.0f)));\n        __m256 cond1_false_cond2_true = _mm256_and_ps(_mm256_xor_ps(cond1_true, _mm256_set1_ps(-1.0f)), cond2_true);\n        __m256 both_false = _mm256_and_ps(_mm256_xor_ps(cond1_true, _mm256_set1_ps(-1.0f)), _mm256_xor_ps(cond2_true, _mm256_set1_ps(-1.0f)));\n\n        result_vec = _mm256_blendv_ps(d_vec, a_vec, both_true);\n        result_vec = _mm256_blendv_ps(result_vec, b_vec, cond1_true_cond2_false);\n        result_vec = _mm256_blendv_ps(result_vec, c_vec, cond1_false_cond2_true);\n\n        _mm256_storeu_ps(&out[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (out);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA and out are 3D tensors flattened into 1D arrays.\nYour goal is to count the number of bits set to 1 in each element of tensor A,\nand then store the result in tensor out.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i counts = _mm256_setzero_si256();\n\n        // Count bits using popcnt instruction\n        counts = _mm256_add_epi64(counts, _mm256_set_epi64x(0, _mm_popcnt_u32(_mm256_extract_epi32(data, 0)), 0, _mm_popcnt_u32(_mm256_extract_epi32(data, 1))));\n        counts = _mm256_add_epi64(counts, _mm256_set_epi64x(0, _mm_popcnt_u32(_mm256_extract_epi32(data, 2)), 0, _mm_popcnt_u32(_mm256_extract_epi32(data, 3))));\n        counts = _mm256_add_epi64(counts, _mm256_set_epi64x(0, _mm_popcnt_u32(_mm256_extract_epi32(data, 4)), 0, _mm_popcnt_u32(_mm256_extract_epi32(data, 5))));\n        counts = _mm256_add_epi64(counts, _mm256_set_epi64x(0, _mm_popcnt_u32(_mm256_extract_epi32(data, 6)), 0, _mm_popcnt_u32(_mm256_extract_epi32(data, 7))));\n\n        uint64_t temp_counts[4];\n        _mm256_storeu_si256((__m256i*)temp_counts, counts);\n\n        out[i + 0] = (uint8_t)temp_counts[0];\n        out[i + 1] = (uint8_t)temp_counts[1];\n        out[i + 2] = (uint8_t)temp_counts[2];\n        out[i + 3] = (uint8_t)temp_counts[3];\n        out[i + 4] = (uint8_t)temp_counts[0] >> 32;\n        out[i + 5] = (uint8_t)temp_counts[1] >> 32;\n        out[i + 6] = (uint8_t)temp_counts[2] >> 32;\n        out[i + 7] = (uint8_t)temp_counts[3] >> 32;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        out[i] = (uint8_t)_mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n\n        // Horizontal sum of the counts\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_hadd_epi32(count, count);\n\n        // Store the result\n        out[i] = (uint8_t)_mm256_extract_epi32(count, 0);\n        if (i + 1 < total_elements) out[i + 1] = (uint8_t)_mm256_extract_epi32(count, 1);\n        if (i + 2 < total_elements) out[i + 2] = (uint8_t)_mm256_extract_epi32(count, 2);\n        if (i + 3 < total_elements) out[i + 3] = (uint8_t)_mm256_extract_epi32(count, 3);\n        if (i + 4 < total_elements) out[i + 4] = (uint8_t)_mm256_extract_epi32(count, 4);\n        if (i + 5 < total_elements) out[i + 5] = (uint8_t)_mm256_extract_epi32(count, 5);\n        if (i + 6 < total_elements) out[i + 6] = (uint8_t)_mm256_extract_epi32(count, 6);\n        if (i + 7 < total_elements) out[i + 7] = (uint8_t)_mm256_extract_epi32(count, 7);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n        data = _mm256_srli_epi32(data, 1);\n        count = _mm256_add_epi32(count, _mm256_and_si256(data, _mm256_set1_epi32(1)));\n\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_permute4x64_epi64(count, _MM_SHUFFLE(2, 3, 0, 1));\n        count = _mm256_add_epi32(count, _mm256_set1_epi32(0));\n\n        _mm_storeu_si128((__m128i*)&out[i], _mm_packus_epi16(_mm256_extracti128_si256(count, 0), _mm256_extracti128_si256(count, 1)));\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t value = A[i];\n        out[i] = (value & 1) + ((value >> 1) & 1) + ((value >> 2) & 1) + ((value >> 3) & 1) +\n                          ((value >> 4) & 1) + ((value >> 5) & 1) + ((value >> 6) & 1) + ((value >> 7) & 1) +\n                          ((value >> 8) & 1) + ((value >> 9) & 1) + ((value >> 10) & 1) + ((value >> 11) & 1) +\n                          ((value >> 12) & 1) + ((value >> 13) & 1) + ((value >> 14) & 1) + ((value >> 15) & 1) +\n                          ((value >> 16) & 1) + ((value >> 17) & 1) + ((value >> 18) & 1) + ((value >> 19) & 1) +\n                          ((value >> 20) & 1) + ((value >> 21) & 1) + ((value >> 22) & 1) + ((value >> 23) & 1) +\n                          ((value >> 24) & 1) + ((value >> 25) & 1) + ((value >> 26) & 1) + ((value >> 27) & 1) +\n                          ((value >> 28) & 1) + ((value >> 29) & 1) + ((value >> 30) & 1) + ((value >> 31) & 1);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        for (int j = 0; j < 32; j++) {\n            __m256i mask = _mm256_set1_epi32(1 << j);\n            __m256i bit_set = _mm256_and_si256(data, mask);\n            count = _mm256_add_epi32(count, _mm256_srli_epi32(bit_set, j));\n        }\n\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_hadd_epi32(count, count);\n        count = _mm256_permute4x64_epi64(count, _MM_SHUFFLE(2, 3, 0, 1));\n        __m128i result = _mm256_extracti128_si256(count, 0);\n        _mm_storeu_si128((__m128i*)&out[i], _mm_packus_epi16(result, result));\n    }\n\n    for (; i < total_elements; i++) {\n        uint32_t value = A[i];\n        out[i] = __builtin_popcount(value);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer tensor (A);\n- A pointer to a non-empty 8-bit unsigned integer tensor (out);\n- The first dimension of the tensor (dim1);\n- The second dimension of the tensor (dim2);\n- The third dimension of the tensor (dim3).\n\nA and out are 3D tensors flattened into 1D arrays.\nYour goal is to count the number of bits set to 1 in each element of tensor A,\nand then store the result in tensor out.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)(A + i));\n        \n        __m256i lookup_table = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n\n        __m256i low_nibble = _mm256_and_si256(data, _mm256_set1_epi32(0x0F));\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 4), _mm256_set1_epi32(0x0F));\n\n        __m256i bit_counts_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(low_nibble));\n        __m256i bit_counts_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(high_nibble));\n        \n        __m256i sum1 = _mm256_add_epi8(bit_counts_low, bit_counts_high);\n\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 8), _mm256_set1_epi32(0x0F));\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 12), _mm256_set1_epi32(0x0F));\n        bit_counts_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(high_nibble));\n        bit_counts_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(low_nibble));\n        __m256i sum2 = _mm256_add_epi8(bit_counts_low, bit_counts_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum2);\n\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 16), _mm256_set1_epi32(0x0F));\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 20), _mm256_set1_epi32(0x0F));\n        bit_counts_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(high_nibble));\n        bit_counts_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(low_nibble));\n        __m256i sum3 = _mm256_add_epi8(bit_counts_low, bit_counts_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum3);\n\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 24), _mm256_set1_epi32(0x0F));\n        low_nibble = _mm256_and_si256(_mm256_srli_epi32(data, 28), _mm256_set1_epi32(0x0F));\n        bit_counts_low = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(high_nibble));\n        bit_counts_high = _mm256_shuffle_epi8(lookup_table, _mm256_cvtepi32_epi8(low_nibble));\n        __m256i sum4 = _mm256_add_epi8(bit_counts_low, bit_counts_high);\n\n        sum1 = _mm256_add_epi8(sum1, sum4);\n\n        uint8_t temp_out[32];\n        _mm256_storeu_si256((__m256i*)temp_out, sum1);\n\n        for(int j = 0; j < 8; ++j) {\n            out[i + j] = temp_out[j*4];\n        }\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        uint8_t count = 0;\n        while (val) {\n            count += val & 1;\n            val >>= 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit integer matrix (A);\n- A pointer to a non-empty 32-bit integer destination matrix (B);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nA and B are 2D matrices flattened into 1D arrays.\nYour goal is to apply the condition to the matrix A. If the condition is true,\nthe corresponding element in C should be the absolute value of A.\nIf the condition is false, the corresponding element in C should be the same as in A.\nThe condition is whether the current element is divisible by 3.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t cols_simd = cols / 8 * 8;\n\n    __m256i divisor = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x7fffffff);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 8) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i * cols + j));\n            __m256i remainder = _mm256_sub_epi32(a, _mm256_mullo_epi32(_mm256_div_epi32(a, divisor), divisor));\n            __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n            __m256i abs_a = _mm256_and_si256(a, sign_mask);\n            __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (j = cols_simd; j < cols; j++) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit integer matrix (A);\n- A pointer to a non-empty 32-bit integer destination matrix (B);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nA and B are 2D matrices flattened into 1D arrays.\nYour goal is to apply the condition to the matrix A. If the condition is true,\nthe corresponding element in C should be the absolute value of A.\nIf the condition is false, the corresponding element in C should be the same as in A.\nThe condition is whether the current element is divisible by 3.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t cols_simd = cols / 8 * 8;\n\n    __m256i divisor = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 8) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i * cols + j));\n            __m256i remainder = _mm256_sub_epi32(a, _mm256_mullo_epi32(_mm256_div_epi32(a, divisor), divisor));\n            __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n            __m256i abs_a = _mm256_and_si256(a, sign_mask);\n            __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (; j < cols; j++) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit integer matrix (A);\n- A pointer to a non-empty 32-bit integer destination matrix (B);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nA and B are 2D matrices flattened into 1D arrays.\nYour goal is to apply the condition to the matrix A. If the condition is true,\nthe corresponding element in C should be the absolute value of A.\nIf the condition is false, the corresponding element in C should be the same as in A.\nThe condition is whether the current element is divisible by 3.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t cols_simd = cols / 8 * 8;\n\n    __m256i divisor = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x7fffffff);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 8) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i * cols + j));\n            __m256i remainder = _mm256_sub_epi32(a, _mm256_mullo_epi32(_mm256_div_epi32(a, divisor), divisor));\n            __m256i condition = _mm256_cmpeq_epi32(remainder, zero);\n            __m256i abs_a = _mm256_and_si256(a, sign_mask);\n            __m256i result = _mm256_blendv_epi8(a, abs_a, condition);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (j = cols_simd; j < cols; j++) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit integer matrix (A);\n- A pointer to a non-empty 32-bit integer destination matrix (B);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nA and B are 2D matrices flattened into 1D arrays.\nYour goal is to apply the condition to the matrix A. If the condition is true,\nthe corresponding element in C should be the absolute value of A.\nIf the condition is false, the corresponding element in C should be the same as in A.\nThe condition is whether the current element is divisible by 3.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t cols_simd = cols / 8 * 8;\n\n    __m256i divisor = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x7fffffff);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 8) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i * cols + j));\n            __m256i remainder = _mm256_sub_epi32(a, _mm256_mullo_epi32(_mm256_div_epi32(a, divisor), divisor));\n            __m256i condition = _mm256_cmpeq_epi32(remainder, zero);\n            __m256i abs_a = _mm256_and_si256(a, sign_mask);\n            __m256i result = _mm256_blendv_epi8(a, abs_a, condition);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (j = cols_simd; j < cols; j++) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit integer matrix (A);\n- A pointer to a non-empty 32-bit integer destination matrix (B);\n- The number of rows in the matrix (rows);\n- The number of columns in the matrix (cols).\n\nA and B are 2D matrices flattened into 1D arrays.\nYour goal is to apply the condition to the matrix A. If the condition is true,\nthe corresponding element in C should be the absolute value of A.\nIf the condition is false, the corresponding element in C should be the same as in A.\nThe condition is whether the current element is divisible by 3.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t cols_simd = cols / 8 * 8;\n\n    __m256i divisor = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x7fffffff);\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols_simd; j += 8) {\n            __m256i a = _mm256_loadu_si256((const __m256i*)(A + i * cols + j));\n            __m256i remainder = _mm256_sub_epi32(a, _mm256_mullo_epi32(_mm256_div_epi32(a, divisor), divisor));\n            __m256i mask = _mm256_cmpeq_epi32(remainder, zero);\n            __m256i abs_a = _mm256_and_si256(a, sign_mask);\n            __m256i result = _mm256_blendv_epi8(a, abs_a, mask);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (; j < cols; j++) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer vector (src);\n- A pointer to a non-empty 32-bit unsigned integer destination vector (dst);\n- A 8-bit integer value of the number of bits to rotate (rotate_bits);\n- The number of common length of src and dst.\n\nYour goal is to perform a conditional bitwise rotation on the src vector.\nThe condition in this task is whether the low 4 bits of current element in src are all 1.\nIf the condition is true, the element in dst should be the result of \nrotating the corresponding element in src by `rotate_bits` bits toward the low bits;\notherwise, the element in dst should be bitwise negation of the corresponding element in src.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i rotate_bits_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask_vec = _mm256_set1_epi32(0x0F);\n    __m256i all_ones_vec = _mm256_set1_epi32(0x0F);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i low_bits_vec = _mm256_and_si256(src_vec, mask_vec);\n        __m256i condition_vec = _mm256_cmpeq_epi32(low_bits_vec, all_ones_vec);\n\n        __m256i rotated_vec;\n        __m256i negated_vec = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        uint32_t* rotated_arr = (uint32_t*)&rotated_vec;\n        uint32_t src_arr[8];\n        _mm256_storeu_si256((__m256i*)src_arr, src_vec);\n\n        for(int j = 0; j < 8; j++){\n            rotated_arr[j] = _rotl(src_arr[j], rotate_bits);\n        }\n        rotated_vec = _mm256_loadu_si256((const __m256i*)rotated_arr);\n\n        __m256i result_vec = _mm256_blendv_epi8(negated_vec, rotated_vec, condition_vec);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0x0F) == 0x0F) {\n            dst[i] = _rotl(src[i], rotate_bits);\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer vector (src);\n- A pointer to a non-empty 32-bit unsigned integer destination vector (dst);\n- A 8-bit integer value of the number of bits to rotate (rotate_bits);\n- The number of common length of src and dst.\n\nYour goal is to perform a conditional bitwise rotation on the src vector.\nThe condition in this task is whether the low 4 bits of current element in src are all 1.\nIf the condition is true, the element in dst should be the result of \nrotating the corresponding element in src by `rotate_bits` bits toward the low bits;\notherwise, the element in dst should be bitwise negation of the corresponding element in src.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i rotate_bits_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask = _mm256_set1_epi32(0x0F);\n    __m256i all_ones = _mm256_set1_epi32(0x0F);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, all_ones);\n\n        __m256i rotated_vec;\n        __m256i negated_vec = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        uint32_t rotated_arr[8];\n        for(int j = 0; j < 8; ++j){\n            rotated_arr[j] = _rotl(src[i+j], rotate_bits);\n        }\n        rotated_vec = _mm256_loadu_si256((const __m256i*)rotated_arr);\n\n        __m256i result_vec = _mm256_blendv_epi8(negated_vec, rotated_vec, condition);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0x0F) == 0x0F) {\n            dst[i] = _rotl(src[i], rotate_bits);\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer vector (src);\n- A pointer to a non-empty 32-bit unsigned integer destination vector (dst);\n- A 8-bit integer value of the number of bits to rotate (rotate_bits);\n- The number of common length of src and dst.\n\nYour goal is to perform a conditional bitwise rotation on the src vector.\nThe condition in this task is whether the low 4 bits of current element in src are all 1.\nIf the condition is true, the element in dst should be the result of \nrotating the corresponding element in src by `rotate_bits` bits toward the low bits;\notherwise, the element in dst should be bitwise negation of the corresponding element in src.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i rotate_bits_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask = _mm256_set1_epi32(0x0F);\n    __m256i all_ones = _mm256_set1_epi32(0x0F);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, all_ones);\n\n        __m256i rotated_vec;\n        __m256i negated_vec = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        uint32_t rotated_arr[8];\n        for(int j = 0; j < 8; ++j){\n            rotated_arr[j] = _mm_extract_epi32(_mm256_castsi256_m128i(src_vec), j);\n            rotated_arr[j] = (rotated_arr[j] >> rotate_bits) | (rotated_arr[j] << (32 - rotate_bits));\n        }\n        rotated_vec = _mm256_loadu_si256((const __m256i*)rotated_arr);\n        \n        __m256i result_vec = _mm256_blendv_epi8(negated_vec, rotated_vec, condition);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0x0F) == 0x0F) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer vector (src);\n- A pointer to a non-empty 32-bit unsigned integer destination vector (dst);\n- A 8-bit integer value of the number of bits to rotate (rotate_bits);\n- The number of common length of src and dst.\n\nYour goal is to perform a conditional bitwise rotation on the src vector.\nThe condition in this task is whether the low 4 bits of current element in src are all 1.\nIf the condition is true, the element in dst should be the result of \nrotating the corresponding element in src by `rotate_bits` bits toward the low bits;\notherwise, the element in dst should be bitwise negation of the corresponding element in src.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    const __m256i rotate_mask = _mm256_set1_epi32(0x0F);\n    const __m256i all_ones = _mm256_set1_epi32(0x0F);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, rotate_mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, all_ones);\n\n        uint32_t cond_mask = _mm256_movemask_epi8(condition);\n\n        uint32_t result[8];\n        for (int j = 0; j < 8; ++j) {\n            if ((cond_mask & (1 << (j * 4))) != 0) {\n                result[j] = (src[i + j] >> rotate_bits) | (src[i + j] << (32 - rotate_bits));\n            } else {\n                result[j] = ~src[i + j];\n            }\n        }\n        _mm256_storeu_si256((__m256i*)&dst[i], _mm256_loadu_si256((const __m256i*)result));\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0x0F) == 0x0F) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit unsigned integer vector (src);\n- A pointer to a non-empty 32-bit unsigned integer destination vector (dst);\n- A 8-bit integer value of the number of bits to rotate (rotate_bits);\n- The number of common length of src and dst.\n\nYour goal is to perform a conditional bitwise rotation on the src vector.\nThe condition in this task is whether the low 4 bits of current element in src are all 1.\nIf the condition is true, the element in dst should be the result of \nrotating the corresponding element in src by `rotate_bits` bits toward the low bits;\notherwise, the element in dst should be bitwise negation of the corresponding element in src.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i rotate_bits_vec = _mm256_set1_epi32(rotate_bits);\n    __m256i mask = _mm256_set1_epi32(0x0F);\n    __m256i all_ones = _mm256_set1_epi32(0x0F);\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i low_bits = _mm256_and_si256(src_vec, mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_bits, all_ones);\n\n        __m256i rotated = _mm256_rorv_epi32(src_vec, rotate_bits_vec);\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((src[i] & 0x0F) == 0x0F) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (out);\n- A floating-point epsilon value (epsilon);\n- The common length of the tensors.\n\nYour goal is to perform element-wise comparison between tensors A and B.\nIf A[i] - B[i] > abs(epsilon), out[i] should be 1;\nif A[i] - B[i] < -abs(epsilon), out[i] should be -1;\nif abs(A[i] - B[i]) <= abs(epsilon), out[i] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 epsilon_vec = _mm256_set1_ps(fabsf(epsilon));\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_and_ps(diff_vec, _mm256_set1_ps(*(float*)&0x7fffffff)); \n\n        __m256 cmp_gt_eps = _mm256_cmp_ps(diff_vec, epsilon_vec, _CMP_GT_OS);\n        __m256 cmp_lt_neg_eps = _mm256_cmp_ps(diff_vec, _mm256_sub_ps(zero_vec, epsilon_vec), _CMP_LT_OS);\n        __m256 cmp_le_eps = _mm256_cmp_ps(abs_diff_vec, epsilon_vec, _CMP_LE_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(zero_vec, one_vec, cmp_gt_eps);\n        result_vec = _mm256_blendv_ps(result_vec, neg_one_vec, cmp_lt_neg_eps);\n        \n        float result_arr[8];\n        _mm256_storeu_ps(result_arr, result_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (result_arr[j] > 0) {\n                out[i + j] = 1;\n            } else if (result_arr[j] < 0) {\n                out[i + j] = -1;\n            } else {\n                out[i + j] = 0;\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (out);\n- A floating-point epsilon value (epsilon);\n- The common length of the tensors.\n\nYour goal is to perform element-wise comparison between tensors A and B.\nIf A[i] - B[i] > abs(epsilon), out[i] should be 1;\nif A[i] - B[i] < -abs(epsilon), out[i] should be -1;\nif abs(A[i] - B[i]) <= abs(epsilon), out[i] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), abs_epsilon), _CMP_LT_OS);\n        __m256 cmp_eq = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LE_OS);\n\n        __m256 result = _mm256_setzero_ps();\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(0.0f), cmp_eq);\n\n        float temp_out[8];\n        _mm256_storeu_ps(temp_out, result);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (int8_t)temp_out[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (out);\n- A floating-point epsilon value (epsilon);\n- The common length of the tensors.\n\nYour goal is to perform element-wise comparison between tensors A and B.\nIf A[i] - B[i] > abs(epsilon), out[i] should be 1;\nif A[i] - B[i] < -abs(epsilon), out[i] should be -1;\nif abs(A[i] - B[i]) <= abs(epsilon), out[i] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 epsilon_vec = _mm256_set1_ps(fabsf(epsilon));\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_and_ps(diff_vec, _mm256_set1_ps(*(float*)&0x7fffffff));\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff_vec, epsilon_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff_vec, _mm256_sub_ps(zero_vec, epsilon_vec), _CMP_LT_OS);\n        __m256 cmp_eq = _mm256_cmp_ps(abs_diff_vec, epsilon_vec, _CMP_LE_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(zero_vec, one_vec, cmp_gt);\n        result_vec = _mm256_blendv_ps(result_vec, neg_one_vec, cmp_lt);\n\n        float result_arr[8];\n        _mm256_storeu_ps(result_arr, result_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (int8_t)result_arr[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (out);\n- A floating-point epsilon value (epsilon);\n- The common length of the tensors.\n\nYour goal is to perform element-wise comparison between tensors A and B.\nIf A[i] - B[i] > abs(epsilon), out[i] should be 1;\nif A[i] - B[i] < -abs(epsilon), out[i] should be -1;\nif abs(A[i] - B[i]) <= abs(epsilon), out[i] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 epsilon_vec = _mm256_set1_ps(fabsf(epsilon));\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(A + i);\n        __m256 b_vec = _mm256_loadu_ps(B + i);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_and_ps(diff_vec, _mm256_set1_ps(*(float*)&0x7fffffff));\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff_vec, epsilon_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff_vec, _mm256_sub_ps(zero_vec, epsilon_vec), _CMP_LT_OS);\n        __m256 cmp_eq = _mm256_cmp_ps(abs_diff_vec, epsilon_vec, _CMP_LE_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(zero_vec, one_vec, cmp_gt);\n        result_vec = _mm256_blendv_ps(result_vec, neg_one_vec, cmp_lt);\n\n        float result_arr[8];\n        _mm256_storeu_ps(result_arr, result_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (int8_t)result_arr[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe inputs to this function are::\n- A pointer to a non-empty 32-bit floating-point tensor (A);\n- A pointer to a non-empty 32-bit floating-point tensor (B);\n- A pointer to a non-empty 32-bit floating-point tensor (out);\n- A floating-point epsilon value (epsilon);\n- The common length of the tensors.\n\nYour goal is to perform element-wise comparison between tensors A and B.\nIf A[i] - B[i] > abs(epsilon), out[i] should be 1;\nif A[i] - B[i] < -abs(epsilon), out[i] should be -1;\nif abs(A[i] - B[i]) <= abs(epsilon), out[i] should be 0.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), abs_epsilon), _CMP_LT_OS);\n        __m256 cmp_eq = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LE_OS);\n\n        __m256 result = _mm256_setzero_ps();\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(0.0f), cmp_eq);\n\n        float temp_out[8];\n        _mm256_storeu_ps(temp_out, result);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (int8_t)temp_out[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the float values in A to int32_t values in B.\nThe conversion should be done in a way that if the float value is greater than INT32_MAX,\nit should be set to INT32_MAX, and if its value is less than INT32_MIN, it should be set to INT32_MIN, \notherwise, it should be converted to int32_t directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const size_t vector_size = sizeof(__m256) / sizeof(float);\n    size_t i = 0;\n\n    __m256 max_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::max()));\n    __m256 min_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::min()));\n\n    __m256i max_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::max());\n    __m256i min_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::min());\n\n    for (; i + vector_size <= size; i += vector_size) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        __m256 clamped = _mm256_max_ps(_mm256_min_ps(a, max_int), min_int);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > std::numeric_limits<int32_t>::max()) {\n            B[i] = std::numeric_limits<int32_t>::max();\n        } else if (A[i] < std::numeric_limits<int32_t>::min()) {\n            B[i] = std::numeric_limits<int32_t>::min();\n        } else {\n            B[i] = static_cast<int32_t>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the float values in A to int32_t values in B.\nThe conversion should be done in a way that if the float value is greater than INT32_MAX,\nit should be set to INT32_MAX, and if its value is less than INT32_MIN, it should be set to INT32_MIN, \notherwise, it should be converted to int32_t directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const size_t vector_size = sizeof(__m256) / sizeof(float);\n    size_t i = 0;\n\n    __m256 max_val_f = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::max()));\n    __m256 min_val_f = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::min()));\n    __m256i max_val_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::max());\n    __m256i min_val_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::min());\n\n    for (; i + vector_size <= size; i += vector_size) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        \n        __m256 a_clamped_max = _mm256_min_ps(a_vec, max_val_f);\n        __m256 a_clamped = _mm256_max_ps(a_clamped_max, min_val_f);\n\n        __m256i b_vec = _mm256_cvttps_epi32(a_clamped);\n\n        __m256 cmp_gt_max = _mm256_cmp_ps(a_vec, max_val_f, _CMP_GT_OS);\n        __m256 cmp_lt_min = _mm256_cmp_ps(a_vec, min_val_f, _CMP_LT_OS);\n\n        __m256i mask_gt_max = _mm256_castps_si256(cmp_gt_max);\n        __m256i mask_lt_min = _mm256_castps_si256(cmp_lt_min);\n\n        b_vec = _mm256_blendv_epi8(b_vec, max_val_i, mask_gt_max);\n        b_vec = _mm256_blendv_epi8(b_vec, min_val_i, mask_lt_min);\n\n        _mm256_storeu_si256((__m256i*)&B[i], b_vec);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > std::numeric_limits<int32_t>::max()) {\n            B[i] = std::numeric_limits<int32_t>::max();\n        } else if (A[i] < std::numeric_limits<int32_t>::min()) {\n            B[i] = std::numeric_limits<int32_t>::min();\n        } else {\n            B[i] = static_cast<int32_t>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the float values in A to int32_t values in B.\nThe conversion should be done in a way that if the float value is greater than INT32_MAX,\nit should be set to INT32_MAX, and if its value is less than INT32_MIN, it should be set to INT32_MIN, \notherwise, it should be converted to int32_t directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const size_t vector_size = sizeof(__m256) / sizeof(float);\n    size_t i = 0;\n\n    __m256 max_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::max()));\n    __m256 min_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::min()));\n\n    __m256i max_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::max());\n    __m256i min_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::min());\n\n    for (; i + vector_size <= size; i += vector_size) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        __m256 clamped = _mm256_max_ps(_mm256_min_ps(a, max_int), min_int);\n\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > std::numeric_limits<int32_t>::max()) {\n            B[i] = std::numeric_limits<int32_t>::max();\n        } else if (A[i] < std::numeric_limits<int32_t>::min()) {\n            B[i] = std::numeric_limits<int32_t>::min();\n        } else {\n            B[i] = static_cast<int32_t>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the float values in A to int32_t values in B.\nThe conversion should be done in a way that if the float value is greater than INT32_MAX,\nit should be set to INT32_MAX, and if its value is less than INT32_MIN, it should be set to INT32_MIN, \notherwise, it should be converted to int32_t directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const size_t vector_size = sizeof(__m256) / sizeof(float);\n    size_t i = 0;\n\n    __m256 max_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::max()));\n    __m256 min_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::min()));\n\n    __m256i max_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::max());\n    __m256i min_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::min());\n\n    for (; i + vector_size <= size; i += vector_size) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        __m256 clamped = _mm256_max_ps(_mm256_min_ps(a, max_int), min_int);\n        __m256i converted = _mm256_cvttps_epi32(clamped);\n\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > std::numeric_limits<int32_t>::max()) {\n            B[i] = std::numeric_limits<int32_t>::max();\n        } else if (A[i] < std::numeric_limits<int32_t>::min()) {\n            B[i] = std::numeric_limits<int32_t>::min();\n        } else {\n            B[i] = static_cast<int32_t>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the float values in A to int32_t values in B.\nThe conversion should be done in a way that if the float value is greater than INT32_MAX,\nit should be set to INT32_MAX, and if its value is less than INT32_MIN, it should be set to INT32_MIN, \notherwise, it should be converted to int32_t directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const size_t vector_size = sizeof(__m256) / sizeof(float);\n    size_t i = 0;\n\n    __m256 max_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::max()));\n    __m256 min_int = _mm256_set1_ps(static_cast<float>(std::numeric_limits<int32_t>::min()));\n\n    __m256i max_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::max());\n    __m256i min_int_i = _mm256_set1_epi32(std::numeric_limits<int32_t>::min());\n\n    for (; i + vector_size <= size; i += vector_size) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        __m256 clamped = _mm256_max_ps(_mm256_min_ps(a, max_int), min_int);\n\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > std::numeric_limits<int32_t>::max()) {\n            B[i] = std::numeric_limits<int32_t>::max();\n        } else if (A[i] < std::numeric_limits<int32_t>::min()) {\n            B[i] = std::numeric_limits<int32_t>::min();\n        } else {\n            B[i] = static_cast<int32_t>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- An int8_t scale factor (scale);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The size of the arrays (size).\n\nYour goal is to convert the int32_t values in A to values in float type in B,\nand scale them by the given scale factor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int chunkSize = 8;\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n\n    for (; i + chunkSize <= size; i += chunkSize) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- An int8_t scale factor (scale);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The size of the arrays (size).\n\nYour goal is to convert the int32_t values in A to values in float type in B,\nand scale them by the given scale factor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int chunkSize = 8;\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n\n    for (; i + chunkSize <= size; i += chunkSize) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- An int8_t scale factor (scale);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The size of the arrays (size).\n\nYour goal is to convert the int32_t values in A to values in float type in B,\nand scale them by the given scale factor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)&A[i]);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- An int8_t scale factor (scale);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The size of the arrays (size).\n\nYour goal is to convert the int32_t values in A to values in float type in B,\nand scale them by the given scale factor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int chunkSize = 8;\n    size_t i = 0;\n\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n\n    for (; i + chunkSize <= size; i += chunkSize) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- An int8_t scale factor (scale);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The size of the arrays (size).\n\nYour goal is to convert the int32_t values in A to values in float type in B,\nand scale them by the given scale factor.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int chunkSize = 8;\n    size_t i = 0;\n\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n\n    for (; i + chunkSize <= size; i += chunkSize) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) source array (B);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (C);\n- The common length of the arrays (size).\n\nYour goal is to perform element-wise division of A by B,\nand round the result to the nearest integer in the range that can be represented by a 32-bit integer.\nThe result should be stored in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i*)(C + i), int_result);\n    }\n    for (; i < size; ++i) {\n        float div_result = A[i] / B[i];\n        C[i] = (int32_t)roundf(div_result);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) source array (B);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (C);\n- The common length of the arrays (size).\n\nYour goal is to perform element-wise division of A by B,\nand round the result to the nearest integer in the range that can be represented by a 32-bit integer.\nThe result should be stored in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i*)(C + i), int_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) source array (B);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (C);\n- The common length of the arrays (size).\n\nYour goal is to perform element-wise division of A by B,\nand round the result to the nearest integer in the range that can be represented by a 32-bit integer.\nThe result should be stored in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i*)(C + i), int_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) source array (B);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (C);\n- The common length of the arrays (size).\n\nYour goal is to perform element-wise division of A by B,\nand round the result to the nearest integer in the range that can be represented by a 32-bit integer.\nThe result should be stored in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i*)(C + i), int_result);\n    }\n    for (; i < size; ++i) {\n        float div_result = A[i] / B[i];\n        C[i] = (int32_t)roundf(div_result);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit floating-point (float) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) source array (B);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (C);\n- The common length of the arrays (size).\n\nYour goal is to perform element-wise division of A by B,\nand round the result to the nearest integer in the range that can be represented by a 32-bit integer.\nThe result should be stored in C.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i*)(C + i), int_result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the double values in A to float values in B.\nThe conversion should be done in a way that if the double value is greater than FLT_MAX,\nit should be set to FLT_MAX, and if it is less than -FLT_MAX, it should be set to -FLT_MAX, else\nit should be converted to float directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_flt = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_flt = _mm256_set1_pd(-FLT_MAX);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d clamped = _mm256_max_pd(_mm256_min_pd(a, max_flt), min_flt);\n\n        __m128 clamped_low = _mm256_castpd256_ps128(_mm256_cvtpd_ps(clamped));\n        __m128 clamped_high = _mm256_castpd256_ps128(_mm256_cvtpd_ps(_mm256_permute4x64_pd(clamped, 0b10110001)));\n\n        _mm_storeu_ps(&B[i], _mm_blend_ps(clamped_low, clamped_high, 0b1100));\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the double values in A to float values in B.\nThe conversion should be done in a way that if the double value is greater than FLT_MAX,\nit should be set to FLT_MAX, and if it is less than -FLT_MAX, it should be set to -FLT_MAX, else\nit should be converted to float directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_flt = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_flt = _mm256_set1_pd(-FLT_MAX);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d clamped = _mm256_max_pd(_mm256_min_pd(a, max_flt), min_flt);\n\n        __m128 clamped_low = _mm256_castpd256_ps128(_mm256_cvtpd_ps(clamped));\n        __m128 clamped_high = _mm256_castpd256_ps128(_mm256_cvtpd_ps(_mm256_permute4x64_pd(clamped, 0b10110001)));\n\n        _mm_storeu_ps(&B[i], _mm_blend_ps(clamped_low, clamped_high, 0b11110000));\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the double values in A to float values in B.\nThe conversion should be done in a way that if the double value is greater than FLT_MAX,\nit should be set to FLT_MAX, and if it is less than -FLT_MAX, it should be set to -FLT_MAX, else\nit should be converted to float directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_flt = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_flt = _mm256_set1_pd(-FLT_MAX);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d clamped = _mm256_max_pd(_mm256_min_pd(a, max_flt), min_flt);\n\n        __m128 clamped_low = _mm256_castpd256_ps128(_mm256_cvtpd_ps(clamped));\n        __m128 clamped_high = _mm256_castpd256_ps128(_mm256_cvtpd_ps(_mm256_permute4x64_pd(clamped, 0b10110001)));\n\n        _mm_storeu_ps(&B[i], _mm_blend_ps(clamped_low, clamped_high, 0b1100));\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the double values in A to float values in B.\nThe conversion should be done in a way that if the double value is greater than FLT_MAX,\nit should be set to FLT_MAX, and if it is less than -FLT_MAX, it should be set to -FLT_MAX, else\nit should be converted to float directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_flt = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_flt = _mm256_set1_pd(-FLT_MAX);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d clamped = _mm256_max_pd(_mm256_min_pd(a, max_flt), min_flt);\n\n        __m128 clamped_low = _mm256_castpd256_ps128(_mm256_cvtpd_ps(clamped));\n        __m128 clamped_high = _mm256_castpd256_ps128(_mm256_cvtpd_ps(_mm256_permute4x64_pd(clamped, 0x0E)));\n\n        _mm_storeu_ps(&B[i], _mm_blend_ps(clamped_low, clamped_high, 0b1100));\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the double values in A to float values in B.\nThe conversion should be done in a way that if the double value is greater than FLT_MAX,\nit should be set to FLT_MAX, and if it is less than -FLT_MAX, it should be set to -FLT_MAX, else\nit should be converted to float directly.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_flt = _mm256_set1_pd(FLT_MAX);\n    const __m256d min_flt = _mm256_set1_pd(-FLT_MAX);\n    const __m256 zero = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n\n        __m256d clamped = _mm256_max_pd(_mm256_min_pd(a, max_flt), min_flt);\n\n        __m128 clamped_low = _mm256_castpd256_ps128(_mm256_cvtpd_ps(clamped));\n        __m128 clamped_high = _mm256_castpd256_ps128(_mm256_cvtpd_ps(_mm256_permute4x64_pd(clamped, 0b10110001)));\n\n        _mm_storeu_ps(&B[i], _mm_blend_ps(clamped_low, clamped_high, 0b1100));\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 16-bit integer (int16_t) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the int16_t values in A to int32_t values in B.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 16 <= size; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a, 0));\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i result = _mm256_cvtepi16_epi32(vec);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i result = _mm256_cvtepi16_epi32(vec);\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 16-bit integer (int16_t) source array (A);\n- A pointer to a non-empty 32-bit integer (int32_t) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to convert the int16_t values in A to int32_t values in B.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    for (; i + 16 <= size; i += 16) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a, 0));\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        vec = _mm256_cvtepi16_epi32(vec);\n        _mm256_storeu_si256((__m256i*)&B[i], vec);\n    }\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    \n    size_t i;\n    for (i = 0; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 mask = _mm256_cmp_ps(control_vec, _mm256_setzero_ps(), _MM_CMPINT_GT);\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, _mm256_setzero_ps());\n        normalized = _mm256_min_ps(normalized, _mm256_set1_ps(1.0f));\n        \n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float normalized = (A[i] - min_val) / (max_val - min_val);\n            B[i] = normalized < 0.0f ? 0.0f : (normalized > 1.0f ? 1.0f : normalized);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    \n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 mask = _mm256_cmp_ps(control_vec, _mm256_setzero_ps(), _CMP_GT_OS);\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, _mm256_setzero_ps());\n        normalized = _mm256_min_ps(normalized, _mm256_set1_ps(1.0f));\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float normalized = (A[i] - min_val) / (max_val - min_val);\n            B[i] = normalized < 0.0f ? 0.0f : (normalized > 1.0f ? 1.0f : normalized);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    \n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 mask = _mm256_cmp_ps(control_vec, _mm256_setzero_ps(), _CMP_GT_OS);\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, _mm256_setzero_ps());\n        normalized = _mm256_min_ps(normalized, _mm256_set1_ps(1.0f));\n        \n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n    \n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float normalized = (A[i] - min_val) / (max_val - min_val);\n            B[i] = normalized < 0.0f ? 0.0f : (normalized > 1.0f ? 1.0f : normalized);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    \n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 mask = _mm256_cmp_ps(control_vec, _mm256_setzero_ps(), _CMP_GT_OS);\n        \n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, _mm256_setzero_ps());\n        normalized = _mm256_min_ps(normalized, _mm256_set1_ps(1.0f));\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float normalized = (A[i] - min_val) / (max_val - min_val);\n            B[i] = normalized < 0.0f ? 0.0f : (normalized > 1.0f ? 1.0f : normalized);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_set1_ps(max_val - min_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 control_vec = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)&control[i]));\n\n        __m256 mask = _mm256_cmp_ps(control_vec, zero_vec, _CMP_GT_OS);\n\n        __m256 result_vec = _mm256_div_ps(_mm256_sub_ps(a_vec, min_vec), range_vec);\n\n        result_vec = _mm256_max_ps(result_vec, zero_vec);\n        result_vec = _mm256_min_ps(result_vec, one_vec);\n\n        __m256 copy_vec = _mm256_loadu_ps(&A[i]);\n        result_vec = _mm256_blendv_ps(copy_vec, result_vec, mask);\n\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is:\n    First, convert the high 16 bits of the uint32_t value in A to float;\n    Then, take the square root of the float value;\n    Finally, store the result in B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_val = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_val);\n    }\n    for (; i < size; ++i) {\n        uint32_t val = A[i];\n        uint32_t high_bits = val >> 16;\n        float float_val = (float)high_bits;\n        B[i] = sqrtf(float_val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is:\n    First, convert the high 16 bits of the uint32_t value in A to float;\n    Then, take the square root of the float value;\n    Finally, store the result in B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, result);\n    }\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float converted = (float)high_bits;\n        B[i] = sqrtf(converted);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is:\n    First, convert the high 16 bits of the uint32_t value in A to float;\n    Then, take the square root of the float value;\n    Finally, store the result in B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_val = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_val);\n    }\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float converted = (float)high_bits;\n        B[i] = sqrtf(converted);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is:\n    First, convert the high 16 bits of the uint32_t value in A to float;\n    Then, take the square root of the float value;\n    Finally, store the result in B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 sqrt_val = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, sqrt_val);\n    }\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float converted = (float)high_bits;\n        B[i] = sqrtf(converted);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is:\n    First, convert the high 16 bits of the uint32_t value in A to float;\n    Then, take the square root of the float value;\n    Finally, store the result in B.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i shifted = _mm256_srli_epi32(a, 16);\n        __m256 converted = _mm256_cvtepi32_ps(shifted);\n        __m256 result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(B + i, result);\n    }\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float converted = (float)high_bits;\n        B[i] = sqrtf(converted);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Interpret the bit pattern of each uint32_t value in A as an IEEE 754 floating-point representation\n2. Store the resulting float value in B without any numerical conversion\n3. This is effectively a type-punning operation that reinterprets the bits directly\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t aligned_size = size - size % 8;\n\n    for (; i < aligned_size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Interpret the bit pattern of each uint32_t value in A as an IEEE 754 floating-point representation\n2. Store the resulting float value in B without any numerical conversion\n3. This is effectively a type-punning operation that reinterprets the bits directly\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t aligned_size = size - size % 8;\n\n    for (; i < aligned_size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = *((float*)(A + i));\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Interpret the bit pattern of each uint32_t value in A as an IEEE 754 floating-point representation\n2. Store the resulting float value in B without any numerical conversion\n3. This is effectively a type-punning operation that reinterprets the bits directly\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec_int = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_storeu_ps(B + i, vec_float);\n    }\n    for (; i < size; ++i) {\n        B[i] = *((float*)(A + i));\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Interpret the bit pattern of each uint32_t value in A as an IEEE 754 floating-point representation\n2. Store the resulting float value in B without any numerical conversion\n3. This is effectively a type-punning operation that reinterprets the bits directly\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (A);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (B);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Interpret the bit pattern of each uint32_t value in A as an IEEE 754 floating-point representation\n2. Store the resulting float value in B without any numerical conversion\n3. This is effectively a type-punning operation that reinterprets the bits directly\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n    for (; i < size; ++i) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- A pointer to a non-empty boolean condition array (cond);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (diff);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Compute the difference between consecutive elements (A[i] - A[i-1]) when cond[i] is true\n2. Convert the difference to float and store in diff[i]\n3. Set diff[i] to 0.0f when cond[i] is false\n4. The first element (diff[0]) is always set to 0.0f as there's no previous element\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i a_prev_vec = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i diff_int_vec = _mm256_sub_epi32(a_vec, a_prev_vec);\n\n        __m256 cond_vec = _mm256_cvtepi32_ps(_mm256_set_epi32(cond[i+7], cond[i+6], cond[i+5], cond[i+4], cond[i+3], cond[i+2], cond[i+1], cond[i]));\n\n        __m256 diff_float_vec = _mm256_cvtepi32_ps(diff_int_vec);\n\n        __m256 result_vec = _mm256_blendv_ps(_mm256_setzero_ps(), diff_float_vec, cond_vec);\n\n        _mm256_storeu_ps(diff + i, result_vec);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n\n    diff[0] = 0.0f;\n    i++;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i a_prev_vec = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i cond_vec = _mm256_loadu_si256((const __m256i*)(cond + i));\n\n        __m256i diff_int_vec = _mm256_sub_epi32(a_vec, a_prev_vec);\n\n        __m256 diff_float_vec = _mm256_cvtepi32_ps(diff_int_vec);\n\n        __m256 zero_vec = _mm256_setzero_ps();\n        __m256 blend_vec = _mm256_blendv_ps(zero_vec, diff_float_vec, _mm256_castsi256_ps(cond_vec));\n\n        _mm256_storeu_ps(diff + i, blend_vec);\n    }\n\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    diff[0] = 0.0f;\n\n    if (size <= 1) return;\n\n    for (i = 1; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a_curr = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i a_prev = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i condition = _mm256_loadu_si256((const __m256i*)(cond + i));\n\n        __m256i diff_int = _mm256_sub_epi32(a_curr, a_prev);\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        __m256 mask = _mm256_castsi256_ps(condition);\n        __m256 result = _mm256_and_ps(diff_float, mask);\n\n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit integer (int32_t) source array (A);\n- A pointer to a non-empty boolean condition array (cond);\n- A pointer to a non-empty 32-bit floating-point (float) destination array (diff);\n- The common length of the arrays (size).\n\nYour goal is to:\n1. Compute the difference between consecutive elements (A[i] - A[i-1]) when cond[i] is true\n2. Convert the difference to float and store in diff[i]\n3. Set diff[i] to 0.0f when cond[i] is false\n4. The first element (diff[0]) is always set to 0.0f as there's no previous element\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    if (size <= 1) return;\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i a_prev = _mm256_loadu_si256((const __m256i*)(A + i - 1));\n        __m256i condition = _mm256_loadu_si256((const __m256i*)(cond + i));\n\n        __m256i diff_int = _mm256_sub_epi32(a, a_prev);\n\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(_mm256_and_si256(condition, condition), _mm256_set1_epi32(1)));\n\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), diff_float, mask);\n\n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (src);\n- A pointer to a non-empty 64-bit unsigned integer (uint64_t) destination array (dst);\n- The common length of the arrays (length).\n\nYour goal is to:\n1. Convert each element in src to a 64-bit unsigned integer;\n2. Set all the high 32 bits of each value to 1 and store the results in dst.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const uint64_t mask = 0xFFFFFFFF00000000;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m256i dst_vec = _mm256_cvtepu32_epi64(src_vec);\n        __m256i mask_vec = _mm256_set1_epi64x(mask);\n        dst_vec = _mm256_or_si256(dst_vec, mask_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | mask;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (src);\n- A pointer to a non-empty 64-bit unsigned integer (uint64_t) destination array (dst);\n- The common length of the arrays (length).\n\nYour goal is to:\n1. Convert each element in src to a 64-bit unsigned integer;\n2. Set all the high 32 bits of each value to 1 and store the results in dst.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const uint64_t mask = 0xFFFFFFFF00000000;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m256i dst_vec = _mm256_cvtepu32_epi64(src_vec);\n        __m256i mask_vec = _mm256_set1_epi64x(mask);\n        dst_vec = _mm256_or_si256(dst_vec, mask_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | mask;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (src);\n- A pointer to a non-empty 64-bit unsigned integer (uint64_t) destination array (dst);\n- The common length of the arrays (length).\n\nYour goal is to:\n1. Convert each element in src to a 64-bit unsigned integer;\n2. Set all the high 32 bits of each value to 1 and store the results in dst.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const uint64_t mask = 0xFFFFFFFF00000000;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m256i dst_vec = _mm256_cvtepu32_epi64(src_vec);\n        __m256i mask_vec = _mm256_set1_epi64x(mask);\n        dst_vec = _mm256_or_si256(dst_vec, mask_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | mask;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 32-bit unsigned integer (uint32_t) source array (src);\n- A pointer to a non-empty 64-bit unsigned integer (uint64_t) destination array (dst);\n- The common length of the arrays (length).\n\nYour goal is to:\n1. Convert each element in src to a 64-bit unsigned integer;\n2. Set all the high 32 bits of each value to 1 and store the results in dst.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const uint64_t mask = 0xFFFFFFFF00000000;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((const __m128i*)(src + i));\n        __m256i dst_vec = _mm256_cvtepu32_epi64(src_vec);\n        __m256i mask_vec = _mm256_set1_epi64x(mask);\n        dst_vec = _mm256_or_si256(dst_vec, mask_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | mask;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000); // Set high 32 bits to 1\n\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 uint32_t values\n        __m256i src_vec_64 = _mm256_unpacklo_epi32(src_vec, _mm256_setzero_si256()); // Unpack to 64-bit\n        src_vec_64 = _mm256_blend_epi16(src_vec_64, _mm256_unpackhi_epi32(src_vec, _mm256_setzero_si256()), 0xCC); // Unpack high 32 bits\n        src_vec_64 = _mm256_or_si256(src_vec_64, mask); // Set high 32 bits to 1\n        _mm256_storeu_si256((__m256i*)&dst[i], src_vec_64); // Store the result\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i] << 32) | 0xFFFFFFFF; // Handle remaining elements\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    \n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256d vec_vec = _mm256_set_pd(\n            vec[_mm_extract_epi16(idx_vec, 3)],\n            vec[_mm_extract_epi16(idx_vec, 2)],\n            vec[_mm_extract_epi16(idx_vec, 1)],\n            vec[_mm_extract_epi16(idx_vec, 0)]\n        );\n        sum_vec = _mm256_add_pd(sum_vec, vec_vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    \n    double result = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point source array (vec);\n- A pointer to a non-empty 16-bit integer (int16_t) index array (index);\n- The common length of the arrays (length).\n\nYour goal is to:\nFor all legal indices `index[i]` (i.e., within the range [0, length)), sum the elements `vec[index[i]]`, \nand return the result.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    double sum = 0.0;\n    __m256d sum_vec = _mm256_setzero_pd();\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i idx_low = _mm_loadu_si128((const __m128i*)(index + i));\n        __m128i idx_high = _mm_loadu_si128((const __m128i*)(index + i + 8));\n\n        __m256i idx_256_low = _mm256_cvtepi16_epi32(idx_low);\n        __m256i idx_256_high = _mm256_cvtepi16_epi32(idx_high);\n\n        __m256d v1 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(idx_256_low), 8);\n        __m256d v2 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(_mm256_srli_epi64(idx_256_low, 32)), 8);\n        __m256d v3 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(idx_256_high), 8);\n        __m256d v4 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(_mm256_srli_epi64(idx_256_high, 32)), 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, v1);\n        sum_vec = _mm256_add_pd(sum_vec, v2);\n        sum_vec = _mm256_add_pd(sum_vec, v3);\n        sum_vec = _mm256_add_pd(sum_vec, v4);\n    }\n\n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point source array (vec);\n- A pointer to a non-empty 16-bit integer (int16_t) index array (index);\n- The common length of the arrays (length).\n\nYour goal is to:\nFor all legal indices `index[i]` (i.e., within the range [0, length)), sum the elements `vec[index[i]]`, \nand return the result.\n\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    double sum = 0.0;\n    __m256d sum_vec = _mm256_setzero_pd();\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i idx_low = _mm_loadu_si128((const __m128i*)(index + i));\n        __m128i idx_high = _mm_loadu_si128((const __m128i*)(index + i + 8));\n\n        __m256i idx256_low = _mm256_cvtepi16_epi32(idx_low);\n        __m256i idx256_high = _mm256_cvtepi16_epi32(idx_high);\n\n        __m256d v1 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(idx256_low), 8);\n        __m256d v2 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(_mm256_srli_si256(idx256_low, 8)), 8);\n        __m256d v3 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(idx256_high), 8);\n        __m256d v4 = _mm256_i32gather_pd(vec, _mm256_castsi256_si128(_mm256_srli_si256(idx256_high, 8)), 8);\n\n        sum_vec = _mm256_add_pd(sum_vec, v1);\n        sum_vec = _mm256_add_pd(sum_vec, v2);\n        sum_vec = _mm256_add_pd(sum_vec, v3);\n        sum_vec = _mm256_add_pd(sum_vec, v4);\n    }\n\n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum_vec);\n    sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n    for (; i < length; ++i) {\n        sum += vec[index[i]];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    \n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256d vec0 = _mm256_i64gather_pd(vec, idx_vec, 8);\n        sum_vec = _mm256_add_pd(sum_vec, vec0);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    \n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; i++) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256d vec_vals = _mm256_set_pd(\n            vec[_mm_extract_epi16(idx, 3)],\n            vec[_mm_extract_epi16(idx, 2)],\n            vec[_mm_extract_epi16(idx, 1)],\n            vec[_mm_extract_epi16(idx, 0)]\n        );\n        sum_vec = _mm256_add_pd(sum_vec, vec_vals);\n    }\n\n    double result[4];\n    _mm256_storeu_pd(result, sum_vec);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    __m256d k = _mm256_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    __m256d x = _mm256_loadu_pd(&input[(i + ki) * input_size + (j + kj)]);\n                    sum = _mm256_fmadd_pd(k, x, sum);\n                }\n            }\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            double conv_result = result[0] + result[1] + result[2] + result[3];\n            output[i * output_size + j] = conv_result > 0 ? conv_result : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) input array (input);\n- A pointer to a non-empty 64-bit floating-point (double) kernel array (kernel);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (output);\n- The size of the input array (input_size);\n- The size of the kernel array (kernel_size).\n\nYour goal is to implement a simple 2D Convolution function (conv2d) using ReLU activation.\nThe input array is in the shape (input_size, input_size).\nThe kernel array is in the shape (kernel_size, kernel_size). \nThe output array is in the shape (input_size - kernel_size + 1, input_size - kernel_size + 1).\nAll three arrays are flattened into 1D arrays.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    size_t input_row = i + m;\n                    size_t input_col = j + n;\n                    size_t kernel_index = m * kernel_size + n;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_row * input_size + input_col]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n            output[i * output_size + j] = (total_sum > 0.0) ? total_sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) input array (input);\n- A pointer to a non-empty 64-bit floating-point (double) kernel array (kernel);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (output);\n- The size of the input array (input_size);\n- The size of the kernel array (kernel_size).\n\nYour goal is to implement a simple 2D Convolution function (conv2d) using ReLU activation.\nThe input array is in the shape (input_size, input_size).\nThe kernel array is in the shape (kernel_size, kernel_size). \nThe output array is in the shape (input_size - kernel_size + 1, input_size - kernel_size + 1).\nAll three arrays are flattened into 1D arrays.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    size_t input_row = i + m;\n                    size_t input_col = j + n;\n                    size_t kernel_index = m * kernel_size + n;\n                    size_t input_index = input_row * input_size + input_col;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n            output[i * output_size + j] = (total_sum > 0.0) ? total_sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) input array (input);\n- A pointer to a non-empty 64-bit floating-point (double) kernel array (kernel);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (output);\n- The size of the input array (input_size);\n- The size of the kernel array (kernel_size).\n\nYour goal is to implement a simple 2D Convolution function (conv2d) using ReLU activation.\nThe input array is in the shape (input_size, input_size).\nThe kernel array is in the shape (kernel_size, kernel_size). \nThe output array is in the shape (input_size - kernel_size + 1, input_size - kernel_size + 1).\nAll three arrays are flattened into 1D arrays.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_row = i + k;\n                    size_t input_col = j + l;\n                    size_t kernel_index = k * kernel_size + l;\n                    double kernel_val = kernel[kernel_index];\n                    __m256d kernel_vec = _mm256_set1_pd(kernel_val);\n\n                    size_t input_index = input_row * input_size + input_col;\n                    __m256d input_vec = _mm256_set1_pd(input[input_index]);\n\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(input_vec, kernel_vec));\n                }\n            }\n\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n            output[i * output_size + j] = (total_sum > 0.0) ? total_sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput to this function are:\n- A pointer to a non-empty 64-bit floating-point (double) input array (input);\n- A pointer to a non-empty 64-bit floating-point (double) kernel array (kernel);\n- A pointer to a non-empty 64-bit floating-point (double) destination array (output);\n- The size of the input array (input_size);\n- The size of the kernel array (kernel_size).\n\nYour goal is to implement a simple 2D Convolution function (conv2d) using ReLU activation.\nThe input array is in the shape (input_size, input_size).\nThe kernel array is in the shape (kernel_size, kernel_size). \nThe output array is in the shape (input_size - kernel_size + 1, input_size - kernel_size + 1).\nAll three arrays are flattened into 1D arrays.\nThe requirement is to implement the function with AVX/AVX2 (Advanced Vector Extensions) intrinsics for parallelism.\n*/\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    size_t input_row = i + m;\n                    size_t input_col = j + n;\n                    size_t kernel_index = m * kernel_size + n;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_row * input_size + input_col]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n            output[i * output_size + j] = (total_sum > 0) ? total_sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCheck if in given vector of numbers, are any two numbers closer to each other than\ngiven threshold.\n>>> has_close_elements_simd({1.0, 2.0, 3.0}, 0.5)\nfalse\n>>> has_close_elements_simd({1.0, 2.8, 3.0, 4.0, 5.0, 2.0}, 0.3)\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < n; ++i) {\n        __m256d num_i_vec = _mm256_set1_pd(numbers[i]);\n        \n        int j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m256d num_j_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(num_i_vec, num_j_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, threshold_vec, _CMP_LT_OS);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                for(int k = 0; k < 4; ++k){\n                    if(i != j + k && std::abs(numbers[i] - numbers[j+k]) < threshold){\n                        return true;\n                    }\n                }\n            }\n        }\n        for(; j < n; ++j){\n            if(i != j && std::abs(numbers[i] - numbers[j]) < threshold){\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCheck if in given vector of numbers, are any two numbers closer to each other than\ngiven threshold.\n>>> has_close_elements_simd({1.0, 2.0, 3.0}, 0.5)\nfalse\n>>> has_close_elements_simd({1.0, 2.8, 3.0, 4.0, 5.0, 2.0}, 0.3)\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < n; ++i) {\n        __m256d num_i_vec = _mm256_set1_pd(numbers[i]);\n\n        int j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m256d num_j_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(num_i_vec, num_j_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, threshold_vec, _CMP_LT_OS);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                return true;\n            }\n        }\n        for (; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCheck if in given vector of numbers, are any two numbers closer to each other than\ngiven threshold.\n>>> has_close_elements_simd({1.0, 2.0, 3.0}, 0.5)\nfalse\n>>> has_close_elements_simd({1.0, 2.8, 3.0, 4.0, 5.0, 2.0}, 0.3)\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < n; ++i) {\n        __m256d num_i_vec = _mm256_set1_pd(numbers[i]);\n\n        int j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m256d num_j_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(num_i_vec, num_j_vec);\n            diff_vec = _mm256_abs_pd(diff_vec);\n\n            __m256d cmp_vec = _mm256_cmp_pd(diff_vec, threshold_vec, _CMP_LT_OS);\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                return true;\n            }\n        }\n        for (; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCheck if in given vector of numbers, are any two numbers closer to each other than\ngiven threshold.\n>>> has_close_elements_simd({1.0, 2.0, 3.0}, 0.5)\nfalse\n>>> has_close_elements_simd({1.0, 2.8, 3.0, 4.0, 5.0, 2.0}, 0.3)\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < n; ++i) {\n        __m256d num_i_vec = _mm256_set1_pd(numbers[i]);\n\n        int j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m256d num_j_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(num_i_vec, num_j_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, threshold_vec, _CMP_LT_OS);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                return true;\n            }\n        }\n        for (; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCheck if in given vector of numbers, are any two numbers closer to each other than\ngiven threshold.\n>>> has_close_elements_simd({1.0, 2.0, 3.0}, 0.5)\nfalse\n>>> has_close_elements_simd({1.0, 2.8, 3.0, 4.0, 5.0, 2.0}, 0.3)\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i < n; ++i) {\n        __m256d num_i_vec = _mm256_set1_pd(numbers[i]);\n        \n        int j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m256d num_j_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(num_i_vec, num_j_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, threshold_vec, _CMP_LT_OS);\n\n            if (_mm256_movemask_pd(cmp_vec) != 0) {\n                for(int k = 0; k < 4; ++k){\n                    if(i != j + k && std::abs(numbers[i] - numbers[j+k]) < threshold){\n                        return true;\n                    }\n                }\n            }\n        }\n        for(; j < n; ++j){\n            if(i != j && std::abs(numbers[i] - numbers[j]) < threshold){\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou\"re given a vector of deposit and withdrawal operations on a bank account that starts with\nzero balance. Your task is to detect if at any point the balance of account falls below zero, and\nat that point function should return true. Otherwise it should return false.\n>>> below_zero_simd({1, 2, 3})\nfalse\n>>> below_zero_simd({1, 2, -4, 5})\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_zero_simd(std::vector<int> operations) {\n    int balance = 0;\n    int i = 0;\n    int size = operations.size();\n\n    if (size >= 8) {\n        int aligned_size = size - (size % 8);\n        __m256i balance_vec = _mm256_setzero_si256();\n        __m256i zero_vec = _mm256_setzero_si256();\n        \n        for (; i < aligned_size; i += 8) {\n            __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n            balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n            int balances[8];\n            _mm256_storeu_si256((__m256i*)balances, balance_vec);\n\n            for(int j = 0; j < 8; ++j) {\n                if (i + j == 0) {\n                    balance = balances[j];\n                } else {\n                    balance += operations[i+j];\n                    balances[j] = balance;\n                }\n                if (balance < 0) {\n                    return true;\n                }\n            }\n            balance_vec = _mm256_set_epi32(balances[7], balances[7], balances[7], balances[7], balances[7], balances[7], balances[7], balances[7]);\n        }\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou\"re given a vector of deposit and withdrawal operations on a bank account that starts with\nzero balance. Your task is to detect if at any point the balance of account falls below zero, and\nat that point function should return true. Otherwise it should return false.\n>>> below_zero_simd({1, 2, 3})\nfalse\n>>> below_zero_simd({1, 2, -4, 5})\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_zero_simd(std::vector<int> operations){\n    int balance = 0;\n    int i = 0;\n    int size = operations.size();\n\n    if (size >= 8) {\n        int limit = size - size % 8;\n        __m256i balance_vec = _mm256_setzero_si256();\n        __m256i zero_vec = _mm256_setzero_si256();\n        \n        for (; i < limit; i += 8) {\n            __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n            balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n            int32_t balances[8];\n            _mm256_storeu_si256((__m256i*)balances, balance_vec);\n\n            for(int j = 0; j < 8; ++j) {\n                if (i + j == 0) {\n                    balance = operations[i+j];\n                } else {\n                    balance += operations[i+j];\n                }\n                if (balance < 0) return true;\n            }\n            balance_vec = _mm256_set1_epi32(balance);\n        }\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou\"re given a vector of deposit and withdrawal operations on a bank account that starts with\nzero balance. Your task is to detect if at any point the balance of account falls below zero, and\nat that point function should return true. Otherwise it should return false.\n>>> below_zero_simd({1, 2, 3})\nfalse\n>>> below_zero_simd({1, 2, -4, 5})\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_zero_simd(std::vector<int> operations){\n    int balance = 0;\n    int i = 0;\n    int size = operations.size();\n\n    if (size >= 8) {\n        int limit = size - size % 8;\n        __m256i balance_vec = _mm256_setzero_si256();\n        __m256i zero_vec = _mm256_setzero_si256();\n        \n        for (; i < limit; i += 8) {\n            __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n            balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n            int balances[8];\n            _mm256_storeu_si256((__m256i*)balances, balance_vec);\n\n            int current_balance = balance;\n            for(int j = 0; j < 8; ++j){\n                current_balance += operations[i+j];\n                if(current_balance < 0){\n                    return true;\n                }\n            }\n            balance = current_balance;\n            balance_vec = _mm256_set1_epi32(balance);\n        }\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou\"re given a vector of deposit and withdrawal operations on a bank account that starts with\nzero balance. Your task is to detect if at any point the balance of account falls below zero, and\nat that point function should return true. Otherwise it should return false.\n>>> below_zero_simd({1, 2, 3})\nfalse\n>>> below_zero_simd({1, 2, -4, 5})\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_zero_simd(std::vector<int> operations){\n    int balance = 0;\n    int i = 0;\n    int size = operations.size();\n\n    if (size >= 8) {\n        int limit = size - size % 8;\n        __m256i balance_vec = _mm256_setzero_si256();\n        __m256i zero_vec = _mm256_setzero_si256();\n        \n        for (; i < limit; i += 8) {\n            __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n            balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n            int balances[8];\n            _mm256_storeu_si256((__m256i*)balances, balance_vec);\n\n            int current_balance = balance;\n            for(int j = 0; j < 8; ++j){\n                current_balance += operations[i+j];\n                if(current_balance < 0){\n                    return true;\n                }\n            }\n            balance = current_balance;\n        }\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou\"re given a vector of deposit and withdrawal operations on a bank account that starts with\nzero balance. Your task is to detect if at any point the balance of account falls below zero, and\nat that point function should return true. Otherwise it should return false.\n>>> below_zero_simd({1, 2, 3})\nfalse\n>>> below_zero_simd({1, 2, -4, 5})\ntrue\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_zero_simd(std::vector<int> operations){\n    int balance = 0;\n    int i = 0;\n    int size = operations.size();\n\n    if (size >= 8) {\n        int limit = size - (size % 8);\n        __m256i balance_vec = _mm256_setzero_si256();\n        __m256i op_vec;\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i min_vec = _mm256_set1_epi32(0);\n\n        for (; i < limit; i += 8) {\n            op_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n            balance_vec = _mm256_add_epi32(balance_vec, op_vec);\n            min_vec = _mm256_min_epi32(min_vec, balance_vec);\n        }\n\n        int min_arr[8];\n        _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n\n        for(int j = 0; j < 8; ++j){\n            if(min_arr[j] < 0){\n                return true;\n            }\n        }\n\n        for(int j = 0; j < 8; ++j){\n            balance += operations[limit - 8 + j];\n        }\n        i = limit;\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    if (numbers.empty()) return 0.0;\n\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / numbers.size();\n\n    double mad_sum = 0.0;\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    if (size >= 4) {\n        __m256d mean_vec = _mm256_set1_pd(mean);\n        __m256d mad_sum_vec = _mm256_setzero_pd();\n\n        for (; i + 3 < size; i += 4) {\n            __m256d numbers_vec = _mm256_loadu_pd(&numbers[i]);\n            __m256d diff_vec = _mm256_sub_pd(numbers_vec, mean_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff_vec);\n        }\n\n        double temp_mad_sum[4];\n        _mm256_storeu_pd(temp_mad_sum, mad_sum_vec);\n        mad_sum = temp_mad_sum[0] + temp_mad_sum[1] + temp_mad_sum[2] + temp_mad_sum[3];\n    }\n\n    for (; i < size; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / numbers.size();\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / n;\n\n    double mad = 0.0;\n\n    if (n >= 4) {\n        int i = 0;\n        __m256d mean_vec = _mm256_set1_pd(mean);\n        __m256d mad_vec = _mm256_setzero_pd();\n\n        for (; i <= n - 4; i += 4) {\n            __m256d num_vec = _mm256_loadu_pd(&numbers[i]);\n            __m256d diff_vec = _mm256_sub_pd(num_vec, mean_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n        }\n\n        double temp_mad[4];\n        _mm256_storeu_pd(temp_mad, mad_vec);\n        mad = temp_mad[0] + temp_mad[1] + temp_mad[2] + temp_mad[3];\n\n        for (; i < n; ++i) {\n            mad += std::abs(numbers[i] - mean);\n        }\n    } else {\n        for (double num : numbers) {\n            mad += std::abs(num - mean);\n        }\n    }\n\n    return mad / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / n;\n\n    double mad = 0.0;\n\n    if (n >= 4) {\n        int i = 0;\n        __m256d mean_vec = _mm256_set1_pd(mean);\n        __m256d mad_vec = _mm256_setzero_pd();\n\n        for (; i <= n - 4; i += 4) {\n            __m256d num_vec = _mm256_loadu_pd(&numbers[i]);\n            __m256d diff_vec = _mm256_sub_pd(num_vec, mean_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n        }\n\n        double temp_mad[4];\n        _mm256_storeu_pd(temp_mad, mad_vec);\n        mad = temp_mad[0] + temp_mad[1] + temp_mad[2] + temp_mad[3];\n\n        for (; i < n; ++i) {\n            mad += std::abs(numbers[i] - mean);\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            mad += std::abs(numbers[i] - mean);\n        }\n    }\n\n    return mad / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given vector of input numbers, calculate Mean Absolute Deviation\naround the mean of this dataset.\nMean Absolute Deviation is the average absolute difference between each\nelement and a centerpoint (mean in this case):\nMAD = average | x - x_mean |\n>>> mean_absolute_deviation_simd({1.0, 2.0, 3.0, 4.0})\n1.0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    for (double x : numbers) {\n        sum += x;\n    }\n    double mean = sum / n;\n\n    double mad_sum = 0.0;\n    int i = 0;\n    if (n >= 4) {\n        __m256d mad_sum_vec = _mm256_setzero_pd();\n        __m256d mean_vec = _mm256_set1_pd(mean);\n\n        for (; i <= n - 4; i += 4) {\n            __m256d data_vec = _mm256_loadu_pd(&numbers[i]);\n            __m256d diff_vec = _mm256_sub_pd(data_vec, mean_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff_vec);\n        }\n\n        double temp_arr[4];\n        _mm256_storeu_pd(temp_arr, mad_sum_vec);\n        mad_sum = temp_arr[0] + temp_arr[1] + temp_arr[2] + temp_arr[3];\n    }\n\n    for (; i < n; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    if (numbers.empty()) return 0.0;\n\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / numbers.size();\n\n    double mad_sum = 0.0;\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    if (size >= 4) {\n        __m256d mean_vec = _mm256_set1_pd(mean);\n        __m256d mad_sum_vec = _mm256_setzero_pd();\n\n        for (; i + 3 < size; i += 4) {\n            __m256d numbers_vec = _mm256_loadu_pd(&numbers[i]);\n            __m256d diff_vec = _mm256_sub_pd(numbers_vec, mean_vec);\n            __m256d abs_diff_vec = _mm256_abs_pd(diff_vec);\n            mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff_vec);\n        }\n\n        double temp_mad[4];\n        _mm256_storeu_pd(temp_mad, mad_sum_vec);\n        mad_sum = temp_mad[0] + temp_mad[1] + temp_mad[2] + temp_mad[3];\n    }\n\n    for (; i < size; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / numbers.size();\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInsert a number \"delimeter\" between every two consecutive elements of input vector `numbers\"\n>>> intersperse_simd({}, 4)\n{}\n>>> intersperse_simd({1, 2, 3}, 4)\n{1, 4, 2, 4, 3}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.empty()) {\n        return result;\n    }\n\n    result.reserve(2 * numbers.size() - 1);\n\n    int i = 0;\n    for (; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers[i]);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInsert a number \"delimeter\" between every two consecutive elements of input vector `numbers\"\n>>> intersperse_simd({}, 4)\n{}\n>>> intersperse_simd({1, 2, 3}, 4)\n{1, 4, 2, 4, 3}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.empty()) {\n        return result;\n    }\n\n    result.reserve(2 * numbers.size() - 1);\n\n    int i = 0;\n    for (; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers[i]);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInsert a number \"delimeter\" between every two consecutive elements of input vector `numbers\"\n>>> intersperse_simd({}, 4)\n{}\n>>> intersperse_simd({1, 2, 3}, 4)\n{1, 4, 2, 4, 3}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.empty()) {\n        return result;\n    }\n\n    result.reserve(2 * numbers.size() - 1);\n\n    int i = 0;\n    for (; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers[i]);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInsert a number \"delimeter\" between every two consecutive elements of input vector `numbers\"\n>>> intersperse_simd({}, 4)\n{}\n>>> intersperse_simd({1, 2, 3}, 4)\n{1, 4, 2, 4, 3}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.empty()) {\n        return result;\n    }\n\n    result.reserve(2 * numbers.size() - 1);\n\n    int i = 0;\n    for (; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers[i]);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInsert a number \"delimeter\" between every two consecutive elements of input vector `numbers\"\n>>> intersperse_simd({}, 4)\n{}\n>>> intersperse_simd({1, 2, 3}, 4)\n{1, 4, 2, 4, 3}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.empty()) {\n        return result;\n    }\n\n    result.reserve(2 * numbers.size() - 1);\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i < numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\n/*\nFor a given vector of integers, return a vector consisting of a sum and a product of all the integers in a vector.\nEmpty sum should be equal to 0 and empty product should be equal to 1.\n>>> sum_product_simd({})\n(0, 1)\n>>> sum_product_simd({1, 2, 3, 4})\n(10, 24)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    int size = numbers.size();\n    int i = 0;\n\n    if (size >= 16) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        __m256i product_vec = _mm256_set1_epi32(1);\n\n        for (; i <= size - 16; i += 16) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n\n            __m256i data_expanded_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(data));\n            __m256i data_expanded_high = _mm256_cvtepi16_epi32(_mm_extracti128_si256(data, 1));\n\n            sum_vec = _mm256_add_epi32(sum_vec, data_expanded_low);\n            sum_vec = _mm256_add_epi32(sum_vec, data_expanded_high);\n\n            product_vec = _mm256_mullo_epi32(product_vec, data_expanded_low);\n            product_vec = _mm256_mullo_epi32(product_vec, data_expanded_high);\n        }\n\n        int32_t temp_sum[8];\n        int32_t temp_product[8];\n\n        _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n        _mm256_storeu_si256((__m256i*)temp_product, product_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n            product *= temp_product[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    int size = numbers.size();\n    int i = 0;\n\n    if (size >= 8) {\n        __m128i sum_vec = _mm_setzero_si128();\n        __m128i prod_vec = _mm_set1_epi16(1);\n\n        for (; i <= size - 8; i += 8) {\n            __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n\n            sum_vec = _mm_add_epi16(sum_vec, data);\n            prod_vec = _mm_mullo_epi16(prod_vec, data);\n        }\n\n        int16_t sum_arr[8];\n        int16_t prod_arr[8];\n\n        _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n        _mm_storeu_si128((__m128i*)prod_arr, prod_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += sum_arr[j];\n            product *= prod_arr[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given vector of integers, return a vector consisting of a sum and a product of all the integers in a vector.\nEmpty sum should be equal to 0 and empty product should be equal to 1.\n>>> sum_product_simd({})\n(0, 1)\n>>> sum_product_simd({1, 2, 3, 4})\n(10, 24)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    int size = numbers.size();\n    int i = 0;\n\n    if (size >= 16) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        __m256i product_vec = _mm256_set1_epi32(1);\n\n        for (; i <= size - 16; i += 16) {\n            __m128i low = _mm_loadu_si128((__m128i*)&numbers[i]);\n            __m128i high = _mm_loadu_si128((__m128i*)&numbers[i + 8]);\n            __m256i data = _mm256_inserti128_si256(_mm256_castsi128_si256(low), high, 1);\n\n            __m256i data_32 = _mm256_cvtepi16_epi32( _mm256_castsi256_si128(data));\n\n            sum_vec = _mm256_add_epi32(sum_vec, data_32);\n            product_vec = _mm256_mullo_epi32(product_vec, data_32);\n        }\n\n        int32_t sum_array[8];\n        int32_t product_array[8];\n        _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n        _mm256_storeu_si256((__m256i*)product_array, product_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += sum_array[j];\n            product *= product_array[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given vector of integers, return a vector consisting of a sum and a product of all the integers in a vector.\nEmpty sum should be equal to 0 and empty product should be equal to 1.\n>>> sum_product_simd({})\n(0, 1)\n>>> sum_product_simd({1, 2, 3, 4})\n(10, 24)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    int size = numbers.size();\n    int i = 0;\n\n    if (size >= 16) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        __m256i product_vec = _mm256_set1_epi32(1);\n\n        for (; i <= size - 16; i += 16) {\n            __m128i low = _mm_loadu_si128((__m128i*)&numbers[i]);\n            __m128i high = _mm_loadu_si128((__m128i*)&numbers[i + 8]);\n\n            __m256i vec = _mm256_inserti128_si256(_mm256_castsi128_si256(low), high, 1);\n\n            __m256i wide_vec = _mm256_cvtepi16_epi32(vec);\n\n            sum_vec = _mm256_add_epi32(sum_vec, wide_vec);\n\n            __m256i temp_product_vec = _mm256_mullo_epi32(product_vec, wide_vec);\n            product_vec = temp_product_vec;\n        }\n\n        int32_t sum_array[8];\n        int32_t product_array[8];\n\n        _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n        _mm256_storeu_si256((__m256i*)product_array, product_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += sum_array[j];\n            product *= product_array[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    int size = numbers.size();\n    int i = 0;\n\n    if (size >= 8) {\n        __m128i sum_vec = _mm_setzero_si128();\n        __m128i prod_vec = _mm_set1_epi16(1);\n\n        for (; i <= size - 8; i += 8) {\n            __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n\n            sum_vec = _mm_add_epi16(sum_vec, data);\n            prod_vec = _mm_mullo_epi16(prod_vec, data);\n        }\n\n        int16_t sum_arr[8];\n        int16_t prod_arr[8];\n\n        _mm_storeu_si128((__m128i*)sum_arr, sum_vec);\n        _mm_storeu_si128((__m128i*)prod_arr, prod_vec);\n\n        for(int j = 0; j < 8; ++j){\n            sum += sum_arr[j];\n            product *= prod_arr[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFrom a given vector of integers, generate a vector of rolling maximum element found until given moment\nin the sequence.\n>>> rolling_max_simd({1, 2, 3, 2, 3, 4, 2})\n{1, 2, 3, 3, 3, 4, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result = numbers;\n    int n = numbers.size();\n    if (n == 0) return result;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i current_vec;\n        if (i + 7 < n) {\n            current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, current_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n\n            int current_max = result[i + 7];\n            for(int j = i; j <= i + 7; ++j){\n                if(j < n){\n                    current_max = std::max(current_max, numbers[j]);\n                    result[j] = current_max;\n                }\n            }\n            max_vec = _mm256_set1_epi32(result[i + 7]);\n\n        } else {\n            for (int j = i; j < n; ++j) {\n                result[j] = std::max(result[j > 0 ? j - 1 : 0], numbers[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFrom a given vector of integers, generate a vector of rolling maximum element found until given moment\nin the sequence.\n>>> rolling_max_simd({1, 2, 3, 2, 3, 4, 2})\n{1, 2, 3, 3, 3, 4, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    int i = 0;\n    if (n >= 8) {\n        __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n        for (; i <= n - 8; i += 8) {\n            __m256i data_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, data_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        }\n        for (int j = i + 1; j < i + 8; ++j) {\n            result[j] = std::max(result[j - 1], numbers[j]);\n        }\n        i += (i > 0) ? 8 - (n % 8) : 0;\n    }\n    \n    if (i == 0) {\n        result[0] = numbers[0];\n        for (int j = 1; j < n; ++j) {\n            result[j] = std::max(result[j - 1], numbers[j]);\n        }\n    } else {\n        for (int j = i; j < n; ++j) {\n            result[j] = std::max(result[j - 1], numbers[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFrom a given vector of integers, generate a vector of rolling maximum element found until given moment\nin the sequence.\n>>> rolling_max_simd({1, 2, 3, 2, 3, 4, 2})\n{1, 2, 3, 3, 3, 4, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) return result;\n\n    int i = 0;\n    if (n >= 8) {\n        __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n        for (; i <= n - 8; i += 8) {\n            __m256i data_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, data_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        }\n        // Sequential computation for rolling max within the vector\n        for (int j = 1; j < 8; ++j) {\n            result[i + j] = std::max(result[i + j], result[i + j - 1]);\n        }\n    }\n    \n    // Remaining elements\n    for (; i < n; ++i) {\n        if (i == 0) {\n            result[i] = numbers[i];\n        } else {\n            result[i] = std::max(numbers[i], result[i - 1]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFrom a given vector of integers, generate a vector of rolling maximum element found until given moment\nin the sequence.\n>>> rolling_max_simd({1, 2, 3, 2, 3, 4, 2})\n{1, 2, 3, 3, 3, 4, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) {\n        return result;\n    }\n\n    int i = 0;\n    if (n >= 8) {\n        __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n        for (; i <= n - 8; i += 8) {\n            __m256i data_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, data_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        }\n\n        // Sequential computation for the first element of the vector\n        result[0] = numbers[0];\n\n        // Sequential computation for the remaining elements in the vector\n        for (int j = 1; j < 8; ++j) {\n            result[i - 8 + j] = std::max(result[i - 8 + j - 1], numbers[i - 8 + j]);\n        }\n    }\n    \n    if(i < n){\n        result[0] = numbers[0];\n        if(i == 0){\n            for(int j = 1; j < n; ++j){\n                result[j] = std::max(result[j-1], numbers[j]);\n            }\n        } else {\n            for(int j = i; j < n; ++j){\n                result[j] = std::max(result[j-1], numbers[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFrom a given vector of integers, generate a vector of rolling maximum element found until given moment\nin the sequence.\n>>> rolling_max_simd({1, 2, 3, 2, 3, 4, 2})\n{1, 2, 3, 3, 3, 4, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    if (n == 0) {\n        return result;\n    }\n\n    int i = 0;\n    if (n >= 8) {\n        __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n        for (; i <= n - 8; i += 8) {\n            __m256i data_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, data_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        }\n\n        // Sequential computation for the first element of the vector\n        result[0] = numbers[0];\n        for (int j = 1; j < 8; ++j) {\n            result[i - 8 + j] = std::max(result[i - 8 + j - 1], numbers[i - 8 + j]);\n        }\n    }\n\n    // Handle remaining elements sequentially\n    if (i < n) {\n        if(i == 0){\n            result[0] = numbers[0];\n            for(int j = 1; j < n; ++j){\n                result[j] = std::max(result[j-1], numbers[j]);\n            }\n        } else {\n            for (; i < n; ++i) {\n                result[i] = std::max(result[i - 1], numbers[i]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor_simd(\"010\", \"110\")\n\"100\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string string_xor_simd(std::string a, std::string b){\n    std::string result = \"\";\n    int len_a = a.length();\n    int len_b = b.length();\n    int len = std::min(len_a, len_b);\n\n    int i = 0;\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i vec_a = _mm256_loadu_si256((__m256i*)(a.data() + i));\n            __m256i vec_b = _mm256_loadu_si256((__m256i*)(b.data() + i));\n            __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n            char temp_result[32];\n            _mm256_storeu_si256((__m256i*)temp_result, vec_result);\n            for (int j = 0; j < 32; ++j) {\n                result += (temp_result[j] == '1' ? '1' : '0');\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        result += ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor_simd(\"010\", \"110\")\n\"100\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string string_xor_simd(std::string a, std::string b){\n    std::string result = \"\";\n    int len_a = a.length();\n    int len_b = b.length();\n    int len = std::max(len_a, len_b);\n\n    std::string padded_a = a;\n    while (padded_a.length() < len) {\n        padded_a = \"0\" + padded_a;\n    }\n\n    std::string padded_b = b;\n    while (padded_b.length() < len) {\n        padded_b = \"0\" + padded_b;\n    }\n    \n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(padded_a.c_str() + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i*)(padded_b.c_str() + i));\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n        char temp_result[32];\n        _mm256_storeu_si256((__m256i*)temp_result, vec_result);\n        result += std::string(temp_result, 32);\n    }\n\n    for (; i < len; ++i) {\n        result += ((padded_a[i] - '0') ^ (padded_b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor_simd(\"010\", \"110\")\n\"100\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string string_xor_simd(std::string a, std::string b){\n    std::string result = \"\";\n    int len_a = a.length();\n    int len_b = b.length();\n    int len = std::min(len_a, len_b);\n\n    int i = 0;\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i vec_a = _mm256_loadu_si256((__m256i*)(a.data() + i));\n            __m256i vec_b = _mm256_loadu_si256((__m256i*)(b.data() + i));\n            __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n            char temp_result[32];\n            _mm256_storeu_si256((__m256i*)temp_result, vec_result);\n            for (int j = 0; j < 32; ++j) {\n                result += (temp_result[j] == '1' ? '1' : '0');\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        result += ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor_simd(\"010\", \"110\")\n\"100\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string string_xor_simd(std::string a, std::string b){\n    std::string result = \"\";\n    int len_a = a.length();\n    int len_b = b.length();\n    int len = std::max(len_a, len_b);\n\n    std::string padded_a = a;\n    while (padded_a.length() < len) {\n        padded_a = \"0\" + padded_a;\n    }\n\n    std::string padded_b = b;\n    while (padded_b.length() < len) {\n        padded_b = \"0\" + padded_b;\n    }\n    \n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(padded_a.c_str() + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i*)(padded_b.c_str() + i));\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n        char temp_result[32];\n        _mm256_storeu_si256((__m256i*)temp_result, vec_result);\n        result += std::string(temp_result, 32);\n    }\n\n    for (; i < len; ++i) {\n        result += ((padded_a[i] - '0') ^ (padded_b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nInput are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor_simd(\"010\", \"110\")\n\"100\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string string_xor_simd(std::string a, std::string b){\n    std::string result = \"\";\n    int len_a = a.length();\n    int len_b = b.length();\n    int len = std::max(len_a, len_b);\n\n    std::string padded_a = a;\n    while (padded_a.length() < len) {\n        padded_a = \"0\" + padded_a;\n    }\n\n    std::string padded_b = b;\n    while (padded_b.length() < len) {\n        padded_b = \"0\" + padded_b;\n    }\n    \n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(padded_a.c_str() + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i*)(padded_b.c_str() + i));\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n        char temp_result[32];\n        _mm256_storeu_si256((__m256i*)temp_result, vec_result);\n        result += std::string(temp_result, 32);\n    }\n\n    for (; i < len; ++i) {\n        result += ((padded_a[i] - '0') ^ (padded_b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest_string = \"\";\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_len = str.length();\n            longest_string = str;\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nOut of vector of strings, return the longest one. Return the first one in case of multiple\nstrings of the same length. Return None in case the input vector is empty.\n>>> longest_simd({})\n\n>>> longest_simd({\"a\", \"b\", \"c\"})\n\"a\"\n>>> longest_simd({\"a\", \"bb\", \"ccc\"})\n\"ccc\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest_string = strings[0];\n    size_t max_len = longest_string.length();\n\n    for (size_t i = 1; i < strings.size(); ++i) {\n        if (strings[i].length() > max_len) {\n            longest_string = strings[i];\n            max_len = strings[i].length();\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nOut of vector of strings, return the longest one. Return the first one in case of multiple\nstrings of the same length. Return None in case the input vector is empty.\n>>> longest_simd({})\n\n>>> longest_simd({\"a\", \"b\", \"c\"})\n\"a\"\n>>> longest_simd({\"a\", \"bb\", \"ccc\"})\n\"ccc\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest_string = strings[0];\n    size_t max_len = longest_string.length();\n\n    for (size_t i = 1; i < strings.size(); ++i) {\n        if (strings[i].length() > max_len) {\n            longest_string = strings[i];\n            max_len = strings[i].length();\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nOut of vector of strings, return the longest one. Return the first one in case of multiple\nstrings of the same length. Return None in case the input vector is empty.\n>>> longest_simd({})\n\n>>> longest_simd({\"a\", \"b\", \"c\"})\n\"a\"\n>>> longest_simd({\"a\", \"bb\", \"ccc\"})\n\"ccc\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest_string = strings[0];\n    size_t max_len = longest_string.length();\n\n    for (size_t i = 1; i < strings.size(); ++i) {\n        if (strings[i].length() > max_len) {\n            longest_string = strings[i];\n            max_len = longest_string.length();\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nOut of vector of strings, return the longest one. Return the first one in case of multiple\nstrings of the same length. Return None in case the input vector is empty.\n>>> longest_simd({})\n\n>>> longest_simd({\"a\", \"b\", \"c\"})\n\"a\"\n>>> longest_simd({\"a\", \"bb\", \"ccc\"})\n\"ccc\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest_string = strings[0];\n    size_t max_len = longest_string.length();\n\n    for (size_t i = 1; i < strings.size(); ++i) {\n        if (strings[i].length() > max_len) {\n            longest_string = strings[i];\n            max_len = longest_string.length();\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn a greatest common divisor of two positive integers a and b\n>>> greatest_common_divisor_simd(3, 5)\n1\n>>> greatest_common_divisor_simd(25, 15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    while (b) {\n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn a greatest common divisor of two positive integers a and b\n>>> greatest_common_divisor_simd(3, 5)\n1\n>>> greatest_common_divisor_simd(25, 15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    while (b) {\n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn a greatest common divisor of two positive integers a and b\n>>> greatest_common_divisor_simd(3, 5)\n1\n>>> greatest_common_divisor_simd(25, 15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    while (b) {\n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn a greatest common divisor of two positive integers a and b\n>>> greatest_common_divisor_simd(3, 5)\n1\n>>> greatest_common_divisor_simd(25, 15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    while (b) {\n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn a greatest common divisor of two positive integers a and b\n>>> greatest_common_divisor_simd(3, 5)\n1\n>>> greatest_common_divisor_simd(25, 15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    while (b) {\n        uint64_t temp = b;\n        b = a % b;\n        a = temp;\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n#include <algorithm>\n\n/*\nFrom a supplied vector of numbers (of length at least two) select and return two that are the closest to each\nother and return them in order (smaller number, larger number).\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.2})\n(2.0, 2.2)\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.0})\n(2.0, 2.0)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = numbers[0];\n    float closest_b = numbers[1];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n#include <algorithm>\n\n/*\nFrom a supplied vector of numbers (of length at least two) select and return two that are the closest to each\nother and return them in order (smaller number, larger number).\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.2})\n(2.0, 2.2)\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.0})\n(2.0, 2.0)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = numbers[0];\n    float closest2 = numbers[1];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n#include <algorithm>\n\n/*\nFrom a supplied vector of numbers (of length at least two) select and return two that are the closest to each\nother and return them in order (smaller number, larger number).\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.2})\n(2.0, 2.2)\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.0})\n(2.0, 2.0)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = numbers[0];\n    float closest_b = numbers[1];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n#include <algorithm>\n\n/*\nFrom a supplied vector of numbers (of length at least two) select and return two that are the closest to each\nother and return them in order (smaller number, larger number).\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.2})\n(2.0, 2.2)\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.0})\n(2.0, 2.0)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = numbers[0];\n    float closest2 = numbers[1];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits>\n#include <algorithm>\n\n/*\nFrom a supplied vector of numbers (of length at least two) select and return two that are the closest to each\nother and return them in order (smaller number, larger number).\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.2})\n(2.0, 2.2)\n>>> find_closest_elements_simd({1.0, 2.0, 3.0, 4.0, 5.0, 2.0})\n(2.0, 2.0)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = numbers[0];\n    float closest2 = numbers[1];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven vector of numbers (of at least two elements), apply a linear transform to that vector,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> rescale_to_unit_simd({1.0, 2.0, 3.0, 4.0, 5.0})\n{0.0, 0.25, 0.5, 0.75, 1.0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n\n    for (; i + 7 < numbers.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 rescaled_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], rescaled_vec);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven vector of numbers (of at least two elements), apply a linear transform to that vector,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> rescale_to_unit_simd({1.0, 2.0, 3.0, 4.0, 5.0})\n{0.0, 0.25, 0.5, 0.75, 1.0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    std::vector<float> result(numbers.size());\n\n    size_t i = 0;\n    size_t vector_size = numbers.size() - (numbers.size() % 8);\n\n    for (; i < vector_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 res_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], res_vec);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven vector of numbers (of at least two elements), apply a linear transform to that vector,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> rescale_to_unit_simd({1.0, 2.0, 3.0, 4.0, 5.0})\n{0.0, 0.25, 0.5, 0.75, 1.0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n    size_t vector_size = numbers.size() - (numbers.size() % 8);\n\n    for (; i < vector_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 res_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], res_vec);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven vector of numbers (of at least two elements), apply a linear transform to that vector,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> rescale_to_unit_simd({1.0, 2.0, 3.0, 4.0, 5.0})\n{0.0, 0.25, 0.5, 0.75, 1.0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n    size_t vector_size = numbers.size() - (numbers.size() % 8);\n\n    for (; i < vector_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 res_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], res_vec);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven vector of numbers (of at least two elements), apply a linear transform to that vector,\nsuch that the smallest number will become 0 and the largest will become 1\n>>> rescale_to_unit_simd({1.0, 2.0, 3.0, 4.0, 5.0})\n{0.0, 0.25, 0.5, 0.75, 1.0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.empty()) {\n        return numbers;\n    }\n\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n\n    if (range == 0.0f) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    std::vector<float> result(numbers.size());\n\n    size_t i = 0;\n    size_t vector_size = numbers.size() - (numbers.size() % 8);\n\n    for (; i < vector_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 res_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], res_vec);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given positive number n, find the largest number that divides n evenly, smaller than n\n>>> largest_divisor_simd(15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    if (n % 2 == 0) return n / 2;\n\n    for (uint64_t i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return n / i;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given positive number n, find the largest number that divides n evenly, smaller than n\n>>> largest_divisor_simd(15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) {\n        return 0;\n    }\n    if (n % 2 == 0) {\n        return n / 2;\n    }\n\n    for (uint64_t i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return n / i;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given positive number n, find the largest number that divides n evenly, smaller than n\n>>> largest_divisor_simd(15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n    if (n % 2 == 0) return n / 2;\n\n    uint64_t i;\n    for (i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return n / i;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given positive number n, find the largest number that divides n evenly, smaller than n\n>>> largest_divisor_simd(15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    if (n % 2 == 0) return n / 2;\n\n    for (uint64_t i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return n / i;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nFor a given positive number n, find the largest number that divides n evenly, smaller than n\n>>> largest_divisor_simd(15)\n5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n    if (n % 2 == 0) return n / 2;\n\n    uint64_t i;\n    for (i = 3; i * i <= n; i += 2) {\n        if (n % i == 0) {\n            return n / i;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (int i = 3; i * i <= n; i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn only positive numbers in the vector.\n>>> get_positive_simd({-1, 2, -4, 5, 6})\n{2, 5, 6}\n>>> get_positive_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n{5, 3, 2, 3, 9, 123, 1}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    int i = 0;\n    int size = l.size();\n\n    if (size == 0) return result;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n\n        for (int j = 0; j < 8; ++j) {\n            if (_mm256_movemask_ps(mask) & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn only positive numbers in the vector.\n>>> get_positive_simd({-1, 2, -4, 5, 6})\n{2, 5, 6}\n>>> get_positive_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n{5, 3, 2, 3, 9, 123, 1}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    int i = 0;\n    int size = l.size();\n\n    if (size == 0) return result;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, mask);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn only positive numbers in the vector.\n>>> get_positive_simd({-1, 2, -4, 5, 6})\n{2, 5, 6}\n>>> get_positive_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n{5, 3, 2, 3, 9, 123, 1}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    int i = 0;\n    int size = l.size();\n\n    if (size == 0) return result;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n\n        for (int j = 0; j < 8; ++j) {\n            if (_mm256_movemask_ps(mask) & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn only positive numbers in the vector.\n>>> get_positive_simd({-1, 2, -4, 5, 6})\n{2, 5, 6}\n>>> get_positive_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n{5, 3, 2, 3, 9, 123, 1}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    int i = 0;\n    int size = l.size();\n\n    if (size == 0) return result;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, mask);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] != 0) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn only positive numbers in the vector.\n>>> get_positive_simd({-1, 2, -4, 5, 6})\n{2, 5, 6}\n>>> get_positive_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n{5, 3, 2, 3, 9, 123, 1}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    int i = 0;\n    int size = l.size();\n\n    if (size == 0) return result;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n\n        for (int j = 0; j < 8; ++j) {\n            if (_mm256_movemask_ps(mask) & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    if (n < 121) {\n        for (int i = 5; i * i <= n; i += 6) {\n            if (n % i == 0 || n % (i + 2) == 0)\n                return false;\n        }\n        return true;\n    }\n\n    int64_t sqrt_n = sqrt(n);\n    \n    for (int64_t i = 5; i <= sqrt_n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    if (n < 121) {\n        for (int i = 5; i * i <= n; i += 6) {\n            if (n % i == 0 || n % (i + 2) == 0)\n                return false;\n        }\n        return true;\n    }\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    \n    __m256i vec = _mm256_set_epi64x(sqrt_n, sqrt_n, sqrt_n, sqrt_n);\n    \n    for (int64_t i = 5; i <= sqrt_n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    if (n < 121) {\n        for (int64_t i = 5; i * i <= n; i += 6) {\n            if (n % i == 0 || n % (i + 2) == 0)\n                return false;\n        }\n        return true;\n    }\n\n    int64_t sqrt_n = sqrt(n);\n    int64_t base_values[8] = {5, 7, 11, 13, 17, 19, 23, 29};\n    __m256i base = _mm256_loadu_si256((__m256i*)base_values);\n    \n    for (int64_t i = 5; i <= sqrt_n;) {\n        __m256i current = _mm256_add_epi64(base, _mm256_set1_epi64x(i - 5));\n        \n        int64_t values[4];\n        _mm256_storeu_si256((__m256i*)values, current);\n        \n        for(int j = 0; j < 4; ++j){\n            if (values[j] <= sqrt_n && n % values[j] == 0) return false;\n            if (values[j] + 2 <= sqrt_n && n % (values[j] + 2) == 0) return false;\n        }\n        \n        i += 30;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    if (n < 121) {\n        for (int i = 5; i * i <= n; i += 6) {\n            if (n % i == 0 || n % (i + 2) == 0)\n                return false;\n        }\n        return true;\n    }\n\n    int64_t sqrt_n = sqrt(n);\n    \n    for (int64_t i = 5; i <= sqrt_n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    if (n < 121) {\n        for (int64_t i = 5; i * i <= n; i += 6) {\n            if (n % i == 0 || n % (i + 2) == 0)\n                return false;\n        }\n        return true;\n    }\n\n    int64_t limit = sqrt(n);\n    \n    for (int64_t i = 5; i <= limit; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nif s[i] is a letter, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse the string.\nThe function should return the resulted string.\nExamples\nsolve_simd(\"1234\") = \"4321\"\nsolve_simd(\"ab\") = \"AB\"\nsolve_simd(\"#a@C\") = \"#A@c\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(const std::string & s){\n    int n = s.length();\n    std::string res = s;\n    bool hasLetters = false;\n\n    if (n >= 32) {\n        for (int i = 0; i < n; i += 32) {\n            __m256i chars1 = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i lower = _mm256_set1_epi8('a');\n            __m256i upper = _mm256_set1_epi8('z');\n            __m256i isLower = _mm256_cmpgt_epi8(chars1, _mm256_sub_epi8(lower, _mm256_set1_epi8(1)));\n            __m256i isUpper = _mm256_cmpgt_epi8(upper, _mm256_sub_epi8(chars1, _mm256_set1_epi8(1)));\n            __m256i isLetter = _mm256_and_si256(isLower, isUpper);\n            \n            unsigned int mask = _mm256_movemask_epi8(isLetter);\n            if (mask != 0) {\n                hasLetters = true;\n                for (int j = 0; j < 32 && i + j < n; ++j) {\n                    if (s[i + j] >= 'a' && s[i + j] <= 'z') {\n                        res[i + j] = s[i + j] - 32;\n                    } else if (s[i + j] >= 'A' && s[i + j] <= 'Z') {\n                        res[i + j] = s[i + j] + 32;\n                    }\n                }\n            }\n        }\n        for (int i = (n / 32) * 32; i < n; ++i) {\n            if (s[i] >= 'a' && s[i] <= 'z') {\n                res[i] = s[i] - 32;\n                hasLetters = true;\n            } else if (s[i] >= 'A' && s[i] <= 'Z') {\n                res[i] = s[i] + 32;\n                hasLetters = true;\n            }\n        }\n    } else {\n        for (char c : s) {\n            if (isalpha(c)) {\n                hasLetters = true;\n                break;\n            }\n        }\n        if (hasLetters) {\n            for (int i = 0; i < n; ++i) {\n                if (s[i] >= 'a' && s[i] <= 'z') {\n                    res[i] = s[i] - 32;\n                } else if (s[i] >= 'A' && s[i] <= 'Z') {\n                    res[i] = s[i] + 32;\n                }\n            }\n        } else {\n            std::reverse(res.begin(), res.end());\n        }\n    }\n    \n    if (!hasLetters) {\n        std::reverse(res.begin(), res.end());\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nif s[i] is a letter, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse the string.\nThe function should return the resulted string.\nExamples\nsolve_simd(\"1234\") = \"4321\"\nsolve_simd(\"ab\") = \"AB\"\nsolve_simd(\"#a@C\") = \"#A@c\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(const std::string & s){\n    int n = s.length();\n    std::string res = s;\n    bool has_letter = false;\n\n    if (n >= 32) {\n        for (int i = 0; i <= n - 32; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i lower = _mm256_set1_epi8('a');\n            __m256i upper = _mm256_set1_epi8('z');\n            __m256i lower_mask = _mm256_cmpgt_epi8(chars, _mm256_sub_epi8(lower, _mm256_set1_epi8(1)));\n            __m256i upper_mask = _mm256_cmpgt_epi8(_mm256_add_epi8(upper, _mm256_set1_epi8(1)), chars);\n            __m256i letter_mask = _mm256_and_si256(lower_mask, upper_mask);\n\n            unsigned int mask = _mm256_movemask_epi8(letter_mask);\n\n            for (int j = 0; j < 32; ++j) {\n                if (mask & (1 << j)) {\n                    has_letter = true;\n                    if (s[i + j] >= 'a' && s[i + j] <= 'z') {\n                        res[i + j] = s[i + j] - 32;\n                    } else if (s[i + j] >= 'A' && s[i + j] <= 'Z') {\n                        res[i + j] = s[i + j] + 32;\n                    }\n                }\n            }\n        }\n    }\n\n    for (int i = (n / 32) * 32; i < n; ++i) {\n        if (isalpha(s[i])) {\n            has_letter = true;\n            if (s[i] >= 'a' && s[i] <= 'z') {\n                res[i] = s[i] - 32;\n            } else if (s[i] >= 'A' && s[i] <= 'Z') {\n                res[i] = s[i] + 32;\n            }\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(res.begin(), res.end());\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nif s[i] is a letter, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse the string.\nThe function should return the resulted string.\nExamples\nsolve_simd(\"1234\") = \"4321\"\nsolve_simd(\"ab\") = \"AB\"\nsolve_simd(\"#a@C\") = \"#A@c\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(const std::string & s){\n    int n = s.length();\n    std::string res = s;\n    bool has_letter = false;\n\n    if (n >= 32) {\n        for (int i = 0; i <= n - 32; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i lower_mask = _mm256_and_si256(chars, _mm256_set1_epi8(0x20));\n            __m256i alpha_mask = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8(64));\n            alpha_mask = _mm256_and_si256(alpha_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8(91), chars));\n            __m256i alpha_mask2 = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8(96));\n            alpha_mask2 = _mm256_and_si256(alpha_mask2, _mm256_cmpgt_epi8(_mm256_set1_epi8(123), chars));\n            alpha_mask = _mm256_or_si256(alpha_mask, alpha_mask2);\n            \n            __m256i change_case = _mm256_xor_si256(chars, _mm256_and_si256(alpha_mask, _mm256_set1_epi8(32)));\n            _mm256_storeu_si256((__m256i*)(res.data() + i), change_case);\n\n            unsigned int mask = _mm256_movemask_epi8(alpha_mask);\n            if (mask != 0) {\n                has_letter = true;\n            }\n        }\n\n        for (int i = n - (n % 32); i < n; ++i) {\n            if (isalpha(s[i])) {\n                has_letter = true;\n                if (islower(s[i])) {\n                    res[i] = toupper(s[i]);\n                } else {\n                    res[i] = tolower(s[i]);\n                }\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            if (isalpha(s[i])) {\n                has_letter = true;\n                if (islower(s[i])) {\n                    res[i] = toupper(s[i]);\n                } else {\n                    res[i] = tolower(s[i]);\n                }\n            }\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(res.begin(), res.end());\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nif s[i] is a letter, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse the string.\nThe function should return the resulted string.\nExamples\nsolve_simd(\"1234\") = \"4321\"\nsolve_simd(\"ab\") = \"AB\"\nsolve_simd(\"#a@C\") = \"#A@c\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(const std::string & s){\n    int n = s.length();\n    std::string res = s;\n    bool hasLetters = false;\n\n    if (n >= 32) {\n        for (int i = 0; i < n; i += 32) {\n            __m256i chars1 = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i lower = _mm256_set1_epi8('a');\n            __m256i upper = _mm256_set1_epi8('z');\n            __m256i isLower = _mm256_cmpgt_epi8(chars1, _mm256_sub_epi8(lower, _mm256_set1_epi8(1)));\n            __m256i isUpper = _mm256_cmpgt_epi8(upper, _mm256_sub_epi8(chars1, _mm256_set1_epi8(1)));\n            __m256i isLetter = _mm256_and_si256(isLower, isUpper);\n\n            unsigned int mask = _mm256_movemask_epi8(isLetter);\n\n            for (int j = 0; j < 32 && i + j < n; ++j) {\n                if (mask & (1 << j)) {\n                    hasLetters = true;\n                    if (s[i + j] >= 'a' && s[i + j] <= 'z') {\n                        res[i + j] = s[i + j] - 32;\n                    } else {\n                        res[i + j] = s[i + j] + 32;\n                    }\n                }\n            }\n        }\n    }\n\n    for (int i = (n / 32) * 32; i < n; ++i) {\n        if (isalpha(s[i])) {\n            hasLetters = true;\n            if (s[i] >= 'a' && s[i] <= 'z') {\n                res[i] = s[i] - 32;\n            } else {\n                res[i] = s[i] + 32;\n            }\n        }\n    }\n\n    if (!hasLetters) {\n        std::reverse(res.begin(), res.end());\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nif s[i] is a letter, reverse its case from lower to upper or vise versa, \notherwise keep it as it is.\nIf the string contains no letters, reverse the string.\nThe function should return the resulted string.\nExamples\nsolve_simd(\"1234\") = \"4321\"\nsolve_simd(\"ab\") = \"AB\"\nsolve_simd(\"#a@C\") = \"#A@c\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(const std::string & s){\n    int n = s.length();\n    std::string res = s;\n    bool has_letter = false;\n\n    if (n >= 32) {\n        for (int i = 0; i <= n - 32; i += 32) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n            __m256i lower_mask = _mm256_and_si256(chars, _mm256_set1_epi8(0x20));\n            __m256i alpha_mask = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8(64));\n            alpha_mask = _mm256_and_si256(alpha_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8(91), chars));\n            __m256i alpha_mask2 = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8(96));\n            alpha_mask2 = _mm256_and_si256(alpha_mask2, _mm256_cmpgt_epi8(_mm256_set1_epi8(123), chars));\n            alpha_mask = _mm256_or_si256(alpha_mask, alpha_mask2);\n\n            __m256i change_case = _mm256_xor_si256(chars, _mm256_and_si256(alpha_mask, _mm256_set1_epi8(32)));\n            _mm256_storeu_si256((__m256i*)(res.data() + i), change_case);\n\n            unsigned int mask = _mm256_movemask_epi8(alpha_mask);\n            if (mask != 0) {\n                has_letter = true;\n            }\n        }\n\n        for (int i = n - (n % 32); i < n; ++i) {\n            if (isalpha(s[i])) {\n                has_letter = true;\n                if (islower(s[i])) {\n                    res[i] = toupper(s[i]);\n                } else {\n                    res[i] = tolower(s[i]);\n                }\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            if (isalpha(s[i])) {\n                has_letter = true;\n                if (islower(s[i])) {\n                    res[i] = toupper(s[i]);\n                } else {\n                    res[i] = tolower(s[i]);\n                }\n            }\n        }\n    }\n\n    if (!has_letter) {\n        std::reverse(res.begin(), res.end());\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a vector l and returns a vector l' such that\nl' is identical to l in the indicies that are not divisible by three, while its values at the indicies that are divisible by three are equal\nto the values of the corresponding indicies of l, but sorted.\n>>> sort_third_simd({1, 2, 3})\n{1, 2, 3}\n>>> sort_third_simd({5, 6, 3, 4, 8, 9, 2})\n{2, 6, 3, 4, 8, 9, 5}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n\n    for (int i = 0; i < n; i += 3) {\n        if (i + 2 < n) {\n            int a = l[i];\n            int b = l[i + 1];\n            int c = l[i + 2];\n\n            if (a > b) std::swap(a, b);\n            if (a > c) std::swap(a, c);\n            if (b > c) std::swap(b, c);\n\n            result[i] = a;\n        } else if (i < n) {\n            // Handle the case where there are not enough elements for a full group of three\n            // For simplicity, we just copy the original value\n            result[i] = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function takes a vector l and returns a vector l' such that\nl' is identical to l in the indicies that are not divisible by three, while its values at the indicies that are divisible by three are equal\nto the values of the corresponding indicies of l, but sorted.\n>>> sort_third_simd({1, 2, 3})\n{1, 2, 3}\n>>> sort_third_simd({5, 6, 3, 4, 8, 9, 2})\n{2, 6, 3, 4, 8, 9, 5}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n\n    for (int i = 0; i < n; i += 3) {\n        if (i + 2 < n) {\n            int a = l[i];\n            int b = l[i + 1];\n            int c = l[i + 2];\n\n            if (a > b) std::swap(a, b);\n            if (a > c) std::swap(a, c);\n            if (b > c) std::swap(b, c);\n\n            result[i] = a;\n        } else if (i < n) {\n            // Handle the case where there are fewer than 3 elements remaining\n            // For simplicity, we'll just copy the original value\n            result[i] = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    if (n >= 3) {\n        for (int i = 0; i < n; i += 3) {\n            if (i + 2 < n) {\n                int a = l[i];\n                int b = l[i + 1];\n                int c = l[i + 2];\n\n                std::vector<int> temp = {a, b, c};\n                std::sort(temp.begin(), temp.end());\n\n                result[i] = temp[0];\n            } else if (i < n) {\n                // Handle the case where there are fewer than 3 elements remaining\n                // No sorting needed if less than 3 elements\n                // Just copy the original values\n                result[i] = l[i];\n                if (i + 1 < n) {\n                    result[i + 1] = l[i + 1];\n                }\n                break;\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    if (n >= 3) {\n        for (int i = 0; i < n; i += 3) {\n            if (i + 2 < n) {\n                int a = l[i];\n                int b = l[i + 1];\n                int c = l[i + 2];\n\n                int arr[] = {a, b, c};\n                std::sort(arr, arr + 3);\n\n                result[i] = arr[0];\n                result[i + 1] = b;\n                result[i + 2] = c;\n                if(a > arr[0]){\n                    result[i+1] = l[i+1];\n                    result[i+2] = l[i+2];\n                }\n                if(a > arr[0]){\n                    if(arr[0] == b){\n                        result[i+1] = a;\n                    }\n                    else if(arr[0] == c){\n                        result[i+2] = a;\n                    }\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    if (n >= 3) {\n        for (int i = 0; i < n; i += 3) {\n            if (i + 2 < n) {\n                int a = l[i];\n                int b = l[i + 1];\n                int c = l[i + 2];\n\n                int arr[] = {a, b, c};\n                std::sort(arr, arr + 3);\n\n                result[i] = arr[0];\n                result[i + 1] = l[i + 1];\n                result[i + 2] = l[i + 2];\n                if(arr[0] != l[i]){\n                    result[i+1] = arr[1];\n                    result[i+2] = arr[2];\n                    result[i] = arr[0];\n                }\n            } else if (i < n) {\n                // Handle remaining elements if n is not a multiple of 3\n                // In this case, we don't sort, just copy\n                result[i] = l[i];\n                if (i + 1 < n) {\n                    result[i + 1] = l[i + 1];\n                }\n            }\n        }\n        if(n >= 3){\n            for(int i = 0; i < n; i+=3){\n                if(i+2 < n){\n                    int a = l[i];\n                    int b = l[i+1];\n                    int c = l[i+2];\n                    int arr[] = {a,b,c};\n                    std::sort(arr, arr+3);\n                    result[i] = arr[0];\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nReturn maximum element in the non-empty vector.\n>>> max_element_simd({1, 2, 3})\n3\n>>> max_element_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n123\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return 0.0f;\n    }\n\n    int n = l.size();\n    float max_val = l[0];\n\n    if (n >= 8) {\n        int aligned_size = (n / 8) * 8;\n        __m256 max_vec = _mm256_loadu_ps(&l[0]);\n\n        for (int i = 8; i < aligned_size; i += 8) {\n            __m256 current_vec = _mm256_loadu_ps(&l[i]);\n            max_vec = _mm256_max_ps(max_vec, current_vec);\n        }\n\n        float temp_max[8];\n        _mm256_storeu_ps(temp_max, max_vec);\n\n        for (int i = 0; i < 8; ++i) {\n            max_val = std::max(max_val, temp_max[i]);\n        }\n\n        for (int i = aligned_size; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    } else {\n        for (int i = 1; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nReturn maximum element in the non-empty vector.\n>>> max_element_simd({1, 2, 3})\n3\n>>> max_element_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n123\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return 0.0f;\n    }\n\n    int n = l.size();\n    float max_val = l[0];\n\n    if (n >= 8) {\n        int aligned_size = (n / 8) * 8;\n        __m256 max_vec = _mm256_loadu_ps(&l[0]);\n\n        for (int i = 8; i < aligned_size; i += 8) {\n            __m256 current_vec = _mm256_loadu_ps(&l[i]);\n            max_vec = _mm256_max_ps(max_vec, current_vec);\n        }\n\n        float temp_max[8];\n        _mm256_storeu_ps(temp_max, max_vec);\n\n        for (int i = 0; i < 8; ++i) {\n            max_val = std::max(max_val, temp_max[i]);\n        }\n\n        for (int i = aligned_size; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    } else {\n        for (int i = 1; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nReturn maximum element in the non-empty vector.\n>>> max_element_simd({1, 2, 3})\n3\n>>> max_element_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n123\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return 0.0f;\n    }\n\n    int n = l.size();\n    float max_val = l[0];\n\n    if (n >= 8) {\n        int aligned_size = (n / 8) * 8;\n        __m256 max_vec = _mm256_loadu_ps(&l[0]);\n\n        for (int i = 8; i < aligned_size; i += 8) {\n            __m256 current_vec = _mm256_loadu_ps(&l[i]);\n            max_vec = _mm256_max_ps(max_vec, current_vec);\n        }\n\n        float temp_max[8];\n        _mm256_storeu_ps(temp_max, max_vec);\n\n        for (int i = 0; i < 8; ++i) {\n            max_val = std::max(max_val, temp_max[i]);\n        }\n\n        for (int i = aligned_size; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    } else {\n        for (int i = 1; i < n; ++i) {\n            max_val = std::max(max_val, l[i]);\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nReturn maximum element in the non-empty vector.\n>>> max_element_simd({1, 2, 3})\n3\n>>> max_element_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n123\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return 0.0f;\n    }\n\n    int n = l.size();\n    int i = 0;\n    __m256 max_vec = _mm256_set1_ps(l[0]);\n\n    for (i = 0; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_arr[8];\n    _mm256_storeu_ps(max_arr, max_vec);\n\n    float max_val = max_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        max_val = std::max(max_val, max_arr[j]);\n    }\n\n    for (; i < n; ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nReturn maximum element in the non-empty vector.\n>>> max_element_simd({1, 2, 3})\n3\n>>> max_element_simd({5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10})\n123\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return 0.0f;\n    }\n\n    int n = l.size();\n    int i = 0;\n    __m256 max_vec = _mm256_set1_ps(l[0]);\n\n    for (i = 0; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_arr[8];\n    _mm256_storeu_ps(max_arr, max_vec);\n\n    float max_val = max_arr[0];\n    for (int j = 1; j < 8; ++j) {\n        max_val = std::max(max_val, max_arr[j]);\n    }\n\n    for (; i < n; ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn the number of times the digit 7 appears in non-negative integers less than n which are divisible by 11 or 13.\n>>> fizz_buzz_simd(50)\n0\n>>> fizz_buzz_simd(78)\n2\n>>> fizz_buzz_simd(79)\n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint fizz_buzz_simd(int n){\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int temp = i;\n            while (temp > 0) {\n                if (temp % 10 == 7) {\n                    count++;\n                }\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn the number of times the digit 7 appears in non-negative integers less than n which are divisible by 11 or 13.\n>>> fizz_buzz_simd(50)\n0\n>>> fizz_buzz_simd(78)\n2\n>>> fizz_buzz_simd(79)\n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint fizz_buzz_simd(int n){\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int temp = i;\n            while (temp > 0) {\n                if (temp % 10 == 7) {\n                    count++;\n                }\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn the number of times the digit 7 appears in non-negative integers less than n which are divisible by 11 or 13.\n>>> fizz_buzz_simd(50)\n0\n>>> fizz_buzz_simd(78)\n2\n>>> fizz_buzz_simd(79)\n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint fizz_buzz_simd(int n){\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int temp = i;\n            while (temp > 0) {\n                if (temp % 10 == 7) {\n                    count++;\n                }\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn the number of times the digit 7 appears in non-negative integers less than n which are divisible by 11 or 13.\n>>> fizz_buzz_simd(50)\n0\n>>> fizz_buzz_simd(78)\n2\n>>> fizz_buzz_simd(79)\n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int temp = i;\n            while (temp > 0) {\n                if (temp % 10 == 7) {\n                    count++;\n                }\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn the number of times the digit 7 appears in non-negative integers less than n which are divisible by 11 or 13.\n>>> fizz_buzz_simd(50)\n0\n>>> fizz_buzz_simd(78)\n2\n>>> fizz_buzz_simd(79)\n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint fizz_buzz_simd(int n){\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int temp = i;\n            while (temp > 0) {\n                if (temp % 10 == 7) {\n                    count++;\n                }\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n <= 1) return l;\n\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n <= 1) return l;\n\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n <= 1) return l;\n\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\n/*\nThis function takes a vector l and returns a vector l' such that\nl' is identical to l in the odd indicies, while its values at the even indicies are equal\nto the values of the even indicies of l, but sorted.\n>>> sort_even_simd({1, 2, 3})\n{1, 2, 3}\n>>> sort_even_simd({5, 6, 3, 4})\n{3, 6, 5, 4}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    if (n <= 1) return l;\n\n    std::vector<float> even_elements;\n    for (int i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    std::vector<float> result = l;\n    for (int i = 0, j = 0; i < n; i += 2, ++j) {\n        result[i] = even_elements[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n <= 1) return l;\n\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size() - 2; ++i) {\n        for (size_t j = i + 1; j < l.size() - 1; ++j) {\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size() - 2; ++i) {\n        for (size_t j = i + 1; j < l.size() - 1; ++j) {\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            for (int k = j + 1; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size() - 2; ++i) {\n        for (size_t j = i + 1; j < l.size() - 1; ++j) {\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) {\n        return false;\n    }\n\n    for (size_t i = 0; i < l.size() - 2; ++i) {\n        for (size_t j = i + 1; j < l.size() - 1; ++j) {\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn vector with elements incremented by 1.\n>>> incr_list_simd({1, 2, 3})\n{2, 3, 4}\n>>> incr_list_simd({5, 3, 5, 2, 3, 3, 9, 0, 123})\n{6, 4, 6, 3, 4, 4, 10, 1, 124}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n            __m256i increment = _mm256_set1_epi32(1);\n            __m256i res_vec = _mm256_add_epi32(vec, increment);\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn vector with elements incremented by 1.\n>>> incr_list_simd({1, 2, 3})\n{2, 3, 4}\n>>> incr_list_simd({5, 3, 5, 2, 3, 3, 9, 0, 123})\n{6, 4, 6, 3, 4, 4, 10, 1, 124}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n            __m256i increment = _mm256_set1_epi32(1);\n            __m256i res_vec = _mm256_add_epi32(vec, increment);\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn vector with elements incremented by 1.\n>>> incr_list_simd({1, 2, 3})\n{2, 3, 4}\n>>> incr_list_simd({5, 3, 5, 2, 3, 3, 9, 0, 123})\n{6, 4, 6, 3, 4, 4, 10, 1, 124}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n\n    int i = 0;\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n            __m256i one = _mm256_set1_epi32(1);\n            __m256i res_vec = _mm256_add_epi32(vec, one);\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn vector with elements incremented by 1.\n>>> incr_list_simd({1, 2, 3})\n{2, 3, 4}\n>>> incr_list_simd({5, 3, 5, 2, 3, 3, 9, 0, 123})\n{6, 4, 6, 3, 4, 4, 10, 1, 124}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n            __m256i increment = _mm256_set1_epi32(1);\n            __m256i res_vec = _mm256_add_epi32(vec, increment);\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn vector with elements incremented by 1.\n>>> incr_list_simd({1, 2, 3})\n{2, 3, 4}\n>>> incr_list_simd({5, 3, 5, 2, 3, 3, 9, 0, 123})\n{6, 4, 6, 3, 4, 4, 10, 1, 124}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result(n);\n\n    int i = 0;\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&l[i]);\n            __m256i increment = _mm256_set1_epi32(1);\n            __m256i res_vec = _mm256_add_epi32(vec, increment);\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\npairs_sum_to_zero takes a vector of integers as an input.\nit returns true if there are two distinct elements in the vector that\nsum to zero, and false otherwise.\n>>> pairs_sum_to_zero_simd({1, 3, 5, 0})\nfalse\n>>> pairs_sum_to_zero_simd({1, 3, -2, 1})\nfalse\n>>> pairs_sum_to_zero_simd({1, 2, 3, 7})\nfalse\n>>> pairs_sum_to_zero_simd({2, 4, -5, 3, 5, 7})\ntrue\n>>> pairs_sum_to_zero_simd({1})\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n    for (int i = 0; i < l.size(); ++i) {\n        for (int j = i + 1; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\npairs_sum_to_zero takes a vector of integers as an input.\nit returns true if there are two distinct elements in the vector that\nsum to zero, and false otherwise.\n>>> pairs_sum_to_zero_simd({1, 3, 5, 0})\nfalse\n>>> pairs_sum_to_zero_simd({1, 3, -2, 1})\nfalse\n>>> pairs_sum_to_zero_simd({1, 2, 3, 7})\nfalse\n>>> pairs_sum_to_zero_simd({2, 4, -5, 3, 5, 7})\ntrue\n>>> pairs_sum_to_zero_simd({1})\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n    for (int i = 0; i < l.size(); ++i) {\n        for (int j = i + 1; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\npairs_sum_to_zero takes a vector of integers as an input.\nit returns true if there are two distinct elements in the vector that\nsum to zero, and false otherwise.\n>>> pairs_sum_to_zero_simd({1, 3, 5, 0})\nfalse\n>>> pairs_sum_to_zero_simd({1, 3, -2, 1})\nfalse\n>>> pairs_sum_to_zero_simd({1, 2, 3, 7})\nfalse\n>>> pairs_sum_to_zero_simd({2, 4, -5, 3, 5, 7})\ntrue\n>>> pairs_sum_to_zero_simd({1})\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n    for (int i = 0; i < l.size(); ++i) {\n        for (int j = i + 1; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n    \n    for (size_t i = 0; i < l.size(); ++i) {\n        for (size_t j = i + 1; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\npairs_sum_to_zero takes a vector of integers as an input.\nit returns true if there are two distinct elements in the vector that\nsum to zero, and false otherwise.\n>>> pairs_sum_to_zero_simd({1, 3, 5, 0})\nfalse\n>>> pairs_sum_to_zero_simd({1, 3, -2, 1})\nfalse\n>>> pairs_sum_to_zero_simd({1, 2, 3, 7})\nfalse\n>>> pairs_sum_to_zero_simd({2, 4, -5, 3, 5, 7})\ntrue\n>>> pairs_sum_to_zero_simd({1})\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n    for (int i = 0; i < l.size(); ++i) {\n        for (int j = i + 1; j < l.size(); ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::string result = \"\";\n    while (x > 0) {\n        int8_t remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::string result = \"\";\n    while (x > 0) {\n        int8_t remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::string result = \"\";\n    while (x > 0) {\n        int8_t remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::string result = \"\";\n    while (x > 0) {\n        int8_t remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::string result = \"\";\n    while (x > 0) {\n        int8_t remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int n = text.length();\n    if (n <= 1) return true;\n\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (text[i] != text[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int n = text.length();\n    if (n <= 1) return true;\n\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (text[i] != text[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int n = text.length();\n    if (n <= 1) return true;\n\n    int i = 0;\n    int j = n - 1;\n\n    if (n >= 32) {\n        while (i + 31 < j - 31) {\n            __m256i a = _mm256_loadu_si256((__m256i*)(text.data() + i));\n            __m256i b = _mm256_loadu_si256((__m256i*)(text.data() + j - 31));\n            __m256i reversed_b = _mm256_shuffle_epi8(b, _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31));\n            __m256i cmp = _mm256_cmpeq_epi8(a, reversed_b);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                for (int k = 0; k < 32; ++k) {\n                    if (text[i + k] != text[j - 31 + k]) return false;\n                }\n            }\n            i += 32;\n            j -= 32;\n        }\n    }\n\n    while (i < j) {\n        if (text[i] != text[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int n = text.length();\n    if (n <= 1) return true;\n\n    int i = 0;\n    int j = n - 1;\n\n    if (n >= 32) {\n        while (i + 31 < j - 31) {\n            __m256i a = _mm256_loadu_si256((__m256i*)&text[i]);\n            __m256i b = _mm256_loadu_si256((__m256i*)&text[j - 31]);\n\n            unsigned int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi8(a, _mm256_shuffle_epi8(b, _mm256_set_epi8(\n                31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16,\n                15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0))));\n\n            if (mask != 0xFFFFFFFF) return false;\n\n            i += 32;\n            j -= 32;\n        }\n    }\n\n    while (i < j) {\n        if (text[i] != text[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int n = text.length();\n    if (n <= 1) return true;\n\n    int i = 0;\n    int j = n - 1;\n\n    while (i < j) {\n        if (text[i] != text[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nencode_shift: returns encoded string by shifting every character by 5 in the alphabet.\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd: takes as input string encoded with encode_shift function. Returns decoded string.\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string decode_shift_simd(std::string s){\n    std::string out = s;\n    int len = s.length();\n    int i = 0;\n\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i a = _mm256_set1_epi8('a');\n    __m256i z = _mm256_set1_epi8('z');\n    __m256i _26 = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(input, shift);\n        __m256i cmp_a = _mm256_cmpgt_epi8(a, shifted);\n        __m256i add_26 = _mm256_add_epi8(shifted, _mm256_and_si256(cmp_a, _26));\n        _mm256_storeu_si256((__m256i*)(out.data() + i), add_26);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nencode_shift: returns encoded string by shifting every character by 5 in the alphabet.\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd: takes as input string encoded with encode_shift function. Returns decoded string.\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string decode_shift_simd(std::string s){\n    std::string out = s;\n    int len = s.length();\n    int i = 0;\n\n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i a = _mm256_set1_epi8('a');\n    __m256i z = _mm256_set1_epi8('z');\n    __m256i alphabet_size = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(input, shift);\n        __m256i cmp_a = _mm256_cmpgt_epi8(a, shifted);\n        __m256i add_26 = _mm256_and_si256(cmp_a, alphabet_size);\n        __m256i result = _mm256_add_epi8(shifted, add_26);\n        _mm256_storeu_si256((__m256i*)(out.data() + i), result);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nencode_shift: returns encoded string by shifting every character by 5 in the alphabet.\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd: takes as input string encoded with encode_shift function. Returns decoded string.\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string decode_shift_simd(std::string s){\n    std::string out = s;\n    int len = s.length();\n    int i = 0;\n\n    __m256i offset = _mm256_set1_epi8(5);\n    __m256i a_val = _mm256_set1_epi8('a');\n    __m256i z_val = _mm256_set1_epi8('z');\n    __m256i const_26 = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i temp = _mm256_sub_epi8(input, offset);\n        __m256i cmp_result = _mm256_cmpgt_epi8(a_val, temp);\n        __m256i and_result = _mm256_and_si256(cmp_result, const_26);\n        __m256i result = _mm256_add_epi8(temp, and_result);\n        _mm256_storeu_si256((__m256i*)(out.data() + i), result);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nencode_shift: returns encoded string by shifting every character by 5 in the alphabet.\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd: takes as input string encoded with encode_shift function. Returns decoded string.\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string decode_shift_simd(std::string s){\n    std::string out = s;\n    int len = s.length();\n    int i = 0;\n\n    __m256i offset = _mm256_set1_epi8(5);\n    __m256i a_val = _mm256_set1_epi8('a');\n    __m256i z_val = _mm256_set1_epi8('z');\n    __m256i const_26 = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i temp = _mm256_sub_epi8(input, offset);\n        __m256i cmp_result = _mm256_cmpgt_epi8(a_val, temp);\n        __m256i and_result = _mm256_and_si256(cmp_result, const_26);\n        __m256i result = _mm256_add_epi8(temp, and_result);\n        _mm256_storeu_si256((__m256i*)(out.data() + i), result);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/* \nencode_shift: returns encoded string by shifting every character by 5 in the alphabet.\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd: takes as input string encoded with encode_shift function. Returns decoded string.\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string decode_shift_simd(std::string s){\n    std::string out = s;\n    int len = s.length();\n    int i = 0;\n\n    __m256i offset = _mm256_set1_epi8(5);\n    __m256i a_val = _mm256_set1_epi8('a');\n    __m256i z_val = _mm256_set1_epi8('z');\n    __m256i const_26 = _mm256_set1_epi8(26);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i temp = _mm256_sub_epi8(input, offset);\n        __m256i cmp_result = _mm256_cmpgt_epi8(a_val, temp);\n        __m256i add_26 = _mm256_and_si256(cmp_result, const_26);\n        __m256i result = _mm256_add_epi8(temp, add_26);\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn true if all numbers in the vector l are below threshold t.\n>>> below_threshold_simd({1, 2, 4, 10}, 100)\ntrue\n>>> below_threshold_simd({1, 20, 4, 10}, 5)\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_threshold_simd(std::vector<int>l, int t){\n    int n = l.size();\n    int i = 0;\n    __m256i threshold_vec = _mm256_set1_epi32(t);\n    for (; i + 8 <= n; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp_vec = _mm256_cmpgt_epi32(data_vec, threshold_vec);\n        if (!_mm256_testz_si256(cmp_vec, cmp_vec)) {\n            return false;\n        }\n    }\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn true if all numbers in the vector l are below threshold t.\n>>> below_threshold_simd({1, 2, 4, 10}, 100)\ntrue\n>>> below_threshold_simd({1, 20, 4, 10}, 5)\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_threshold_simd(std::vector<int>l, int t){\n    int n = l.size();\n    int i = 0;\n    __m256i threshold_vec = _mm256_set1_epi32(t);\n    for (; i + 8 <= n; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp_vec = _mm256_cmpgt_epi32(data_vec, threshold_vec);\n        if (!_mm256_testz_si256(cmp_vec, cmp_vec)) {\n            return false;\n        }\n    }\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn true if all numbers in the vector l are below threshold t.\n>>> below_threshold_simd({1, 2, 4, 10}, 100)\ntrue\n>>> below_threshold_simd({1, 20, 4, 10}, 5)\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_threshold_simd(std::vector<int>l, int t){\n    int n = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn true if all numbers in the vector l are below threshold t.\n>>> below_threshold_simd({1, 2, 4, 10}, 100)\ntrue\n>>> below_threshold_simd({1, 20, 4, 10}, 5)\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_threshold_simd(std::vector<int>l, int t){\n    int n = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    for (; i + 8 <= n; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nReturn true if all numbers in the vector l are below threshold t.\n>>> below_threshold_simd({1, 2, 4, 10}, 100)\ntrue\n>>> below_threshold_simd({1, 20, 4, 10}, 5)\nfalse\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool below_threshold_simd(std::vector<int>l, int t){\n    int size = l.size();\n    int i = 0;\n    __m256i threshold_vec = _mm256_set1_epi32(t);\n    for (; i + 8 <= size; i += 8) {\n        __m256i data_vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp_vec = _mm256_cmpgt_epi32(data_vec, threshold_vec);\n        if (!_mm256_testz_si256(cmp_vec, cmp_vec)) {\n            return false;\n        }\n    }\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i less_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n            __m256i greater_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n            unsigned int lt_mask = _mm256_movemask_epi8(less_than);\n            unsigned int gt_mask = _mm256_movemask_epi8(greater_than);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((lt_mask >> j) & 1) {\n                    balance++;\n                } else if ((gt_mask >> j) & 1) {\n                    balance--;\n                }\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i less_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n            __m256i greater_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n            unsigned int lt_mask = _mm256_movemask_epi8(less_than);\n            unsigned int gt_mask = _mm256_movemask_epi8(greater_than);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((lt_mask >> j) & 1) {\n                    balance++;\n                } else if ((gt_mask >> j) & 1) {\n                    balance--;\n                }\n\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n        }\n\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i less_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n            __m256i greater_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n            unsigned int lt_mask = _mm256_movemask_epi8(less_than);\n            unsigned int gt_mask = _mm256_movemask_epi8(greater_than);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((lt_mask >> j) & 1) {\n                    balance++;\n                } else if ((gt_mask >> j) & 1) {\n                    balance--;\n                }\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i less_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n            __m256i greater_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n            unsigned int lt_mask = _mm256_movemask_epi8(less_than);\n            unsigned int gt_mask = _mm256_movemask_epi8(greater_than);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((lt_mask >> j) & 1) {\n                    balance++;\n                } else if ((gt_mask >> j) & 1) {\n                    balance--;\n                }\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i less_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('<'));\n            __m256i greater_than = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('>'));\n\n            unsigned int lt_mask = _mm256_movemask_epi8(less_than);\n            unsigned int gt_mask = _mm256_movemask_epi8(greater_than);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((lt_mask >> j) & 1) {\n                    balance++;\n                } else if ((gt_mask >> j) & 1) {\n                    balance--;\n                }\n                if (balance < 0) return false;\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('('));\n            __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8(')'));\n\n            unsigned int open_bits = _mm256_movemask_epi8(open_mask);\n            unsigned int close_bits = _mm256_movemask_epi8(close_mask);\n\n            int open_count = _mm_popcnt_u32(open_bits);\n            int close_count = _mm_popcnt_u32(close_bits);\n\n            balance += open_count - close_count;\n            if (balance < 0) return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    int n = brackets.length();\n    int i = 0;\n\n    if (n == 0) return true;\n\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('('));\n            __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8(')'));\n\n            unsigned int open_bits = _mm256_movemask_epi8(open_mask);\n            unsigned int close_bits = _mm256_movemask_epi8(close_mask);\n\n            balance += _mm_popcnt_u32(open_bits) - _mm_popcnt_u32(close_bits);\n            if (balance < 0) return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('('));\n            __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8(')'));\n\n            unsigned int open_bits = _mm256_movemask_epi8(open_mask);\n            unsigned int close_bits = _mm256_movemask_epi8(close_mask);\n\n            balance += _mm_popcnt_u32(open_bits);\n            balance -= _mm_popcnt_u32(close_bits);\n\n            if (balance < 0) return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('('));\n            __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8(')'));\n\n            unsigned int open_bits = _mm256_movemask_epi8(open_mask);\n            unsigned int close_bits = _mm256_movemask_epi8(close_mask);\n\n            int open_count = _mm_popcnt_u32(open_bits);\n            int close_count = _mm_popcnt_u32(close_bits);\n\n            balance += open_count - close_count;\n            if (balance < 0) return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    int n = brackets.length();\n\n    if (n == 0) return true;\n\n    int i = 0;\n    if (n >= 32) {\n        for (; i <= n - 32; i += 32) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n            __m256i open_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('('));\n            __m256i close_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8(')'));\n\n            unsigned int open_bits = _mm256_movemask_epi8(open_mask);\n            unsigned int close_bits = _mm256_movemask_epi8(close_mask);\n\n            balance += _mm_popcnt_u32(open_bits) - _mm_popcnt_u32(close_bits);\n\n            if (balance < 0) return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nxs represent coefficients of a polynomial.\nxs{0} + xs{1} * x + xs{2} * x^2 + ....\n Return derivative of this polynomial in the same form.\n>>> derivative_simd({3, 1, 2, 4, 5})\n{1, 4, 12, 20}\n>>> derivative_simd({1, 2, 3})\n{2, 6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    std::vector<float> result;\n    if (xs.size() <= 1) {\n        return result;\n    }\n\n    int n = xs.size() - 1;\n    result.resize(n);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 x = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 derivative_values = _mm256_mul_ps(x, indices);\n        _mm256_storeu_ps(&result[i], derivative_values);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nxs represent coefficients of a polynomial.\nxs{0} + xs{1} * x + xs{2} * x^2 + ....\n Return derivative of this polynomial in the same form.\n>>> derivative_simd({3, 1, 2, 4, 5})\n{1, 4, 12, 20}\n>>> derivative_simd({1, 2, 3})\n{2, 6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    std::vector<float> result;\n    int n = xs.size();\n    if (n <= 1) return result;\n\n    int i = 1;\n    for (; i + 7 < n; i += 8) {\n        __m256 x = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_set_ps(i + 7.0f, i + 6.0f, i + 5.0f, i + 4.0f, i + 3.0f, i + 2.0f, i + 1.0f, i + 0.0f);\n        __m256 derivative_values = _mm256_mul_ps(x, indices);\n        float temp[8];\n        _mm256_storeu_ps(temp, derivative_values);\n        for (int j = 0; j < 8; ++j) {\n            result.push_back(temp[j]);\n        }\n    }\n\n    for (; i < n; ++i) {\n        result.push_back(xs[i] * i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nxs represent coefficients of a polynomial.\nxs{0} + xs{1} * x + xs{2} * x^2 + ....\n Return derivative of this polynomial in the same form.\n>>> derivative_simd({3, 1, 2, 4, 5})\n{1, 4, 12, 20}\n>>> derivative_simd({1, 2, 3})\n{2, 6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    std::vector<float> result;\n    int n = xs.size();\n    if (n <= 1) return result;\n\n    int i = 1;\n    for (; i + 7 < n; i += 8) {\n        __m256 x = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_set_ps(i + 7.0f, i + 6.0f, i + 5.0f, i + 4.0f, i + 3.0f, i + 2.0f, i + 1.0f, i + 0.0f);\n        __m256 derivative_values = _mm256_mul_ps(x, indices);\n        float temp[8];\n        _mm256_storeu_ps(temp, derivative_values);\n        result.insert(result.end(), temp, temp + 8);\n    }\n\n    for (; i < n; ++i) {\n        result.push_back(xs[i] * i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nxs represent coefficients of a polynomial.\nxs{0} + xs{1} * x + xs{2} * x^2 + ....\n Return derivative of this polynomial in the same form.\n>>> derivative_simd({3, 1, 2, 4, 5})\n{1, 4, 12, 20}\n>>> derivative_simd({1, 2, 3})\n{2, 6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    std::vector<float> result;\n    if (xs.size() <= 1) {\n        return result;\n    }\n\n    int n = xs.size() - 1;\n    result.resize(n);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 x = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 derivative_values = _mm256_mul_ps(x, indices);\n        _mm256_storeu_ps(&result[i], derivative_values);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nxs represent coefficients of a polynomial.\nxs{0} + xs{1} * x + xs{2} * x^2 + ....\n Return derivative of this polynomial in the same form.\n>>> derivative_simd({3, 1, 2, 4, 5})\n{1, 4, 12, 20}\n>>> derivative_simd({1, 2, 3})\n{2, 6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    std::vector<float> result;\n    if (xs.size() <= 1) {\n        return result;\n    }\n\n    int n = xs.size() - 1;\n    result.resize(n);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 x = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 indices = _mm256_set_ps(i + 8 - 1, i + 8 - 2, i + 8 - 3, i + 8 - 4, i + 8 - 5, i + 8 - 6, i + 8 - 7, i + 8 - 8);\n        indices = _mm256_add_ps(indices, _mm256_set1_ps(1.0f));\n        __m256 res = _mm256_mul_ps(x, indices);\n        _mm256_storeu_ps(&result[i], res);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. \nHere, 'y' is also a vowel, but only when it is at the end of the given word.\nExample: \n>>> vowels_count_simd(\"abcde\") \n2 \n>>> vowels_count_simd(\"ACEDY\") \n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint vowels_count_simd(std::string s){\n    int count = 0;\n    int n = s.length();\n    if (n == 0) return 0;\n\n    __m256i vowels_vec = _mm256_set1_epi8('a');\n    __m256i vowels_vec_e = _mm256_set1_epi8('e');\n    __m256i vowels_vec_i = _mm256_set1_epi8('i');\n    __m256i vowels_vec_o = _mm256_set1_epi8('o');\n    __m256i vowels_vec_u = _mm256_set1_epi8('u');\n\n    for (int i = 0; i < n; i += 32) {\n        int len = std::min(32, n - i);\n        unsigned char data[32] = {0};\n        for (int j = 0; j < len; ++j) {\n            data[j] = s[i + j];\n        }\n\n        __m256i str_vec = _mm256_loadu_si256((__m256i*)data);\n        __m256i eq_a = _mm256_cmpeq_epi8(str_vec, vowels_vec);\n        __m256i eq_e = _mm256_cmpeq_epi8(str_vec, vowels_vec_e);\n        __m256i eq_i = _mm256_cmpeq_epi8(str_vec, vowels_vec_i);\n        __m256i eq_o = _mm256_cmpeq_epi8(str_vec, vowels_vec_o);\n        __m256i eq_u = _mm256_cmpeq_epi8(str_vec, vowels_vec_u);\n\n        __m256i or_ae = _mm256_or_si256(eq_a, eq_e);\n        __m256i or_io = _mm256_or_si256(eq_i, eq_o);\n        __m256i or_aeio = _mm256_or_si256(or_ae, or_io);\n        __m256i or_aeiou = _mm256_or_si256(or_aeio, eq_u);\n\n        unsigned int mask = _mm256_movemask_epi8(or_aeiou);\n\n        count += _mm_popcnt_u32(mask);\n    }\n    \n    if (s[n-1] == 'y' || s[n-1] == 'Y') count++;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. \nHere, 'y' is also a vowel, but only when it is at the end of the given word.\nExample: \n>>> vowels_count_simd(\"abcde\") \n2 \n>>> vowels_count_simd(\"ACEDY\") \n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint vowels_count_simd(std::string s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    if (n >= 32) {\n        __m256i vowels_lower = _mm256_set1_epi8('a' ^ 'e' ^ 'i' ^ 'o' ^ 'u');\n        __m256i vowels_a = _mm256_set1_epi8('a');\n        __m256i vowels_e = _mm256_set1_epi8('e');\n        __m256i vowels_i = _mm256_set1_epi8('i');\n        __m256i vowels_o = _mm256_set1_epi8('o');\n        __m256i vowels_u = _mm256_set1_epi8('u');\n        __m256i vowels_A = _mm256_set1_epi8('A');\n        __m256i vowels_E = _mm256_set1_epi8('E');\n        __m256i vowels_I = _mm256_set1_epi8('I');\n        __m256i vowels_O = _mm256_set1_epi8('O');\n        __m256i vowels_U = _mm256_set1_epi8('U');\n\n        int i = 0;\n        for (; i <= n - 32; i += 32) {\n            __m256i str_vec = _mm256_loadu_si256((__m256i*)(s.c_str() + i));\n            __m256i eq_a = _mm256_cmpeq_epi8(str_vec, vowels_a);\n            __m256i eq_e = _mm256_cmpeq_epi8(str_vec, vowels_e);\n            __m256i eq_i = _mm256_cmpeq_epi8(str_vec, vowels_i);\n            __m256i eq_o = _mm256_cmpeq_epi8(str_vec, vowels_o);\n            __m256i eq_u = _mm256_cmpeq_epi8(str_vec, vowels_u);\n            __m256i eq_A = _mm256_cmpeq_epi8(str_vec, vowels_A);\n            __m256i eq_E = _mm256_cmpeq_epi8(str_vec, vowels_E);\n            __m256i eq_I = _mm256_cmpeq_epi8(str_vec, vowels_I);\n            __m256i eq_O = _mm256_cmpeq_epi8(str_vec, vowels_O);\n            __m256i eq_U = _mm256_cmpeq_epi8(str_vec, vowels_U);\n\n            __m256i or_1 = _mm256_or_si256(eq_a, eq_e);\n            __m256i or_2 = _mm256_or_si256(eq_i, eq_o);\n            __m256i or_3 = _mm256_or_si256(eq_u, eq_A);\n            __m256i or_4 = _mm256_or_si256(eq_E, eq_I);\n            __m256i or_5 = _mm256_or_si256(eq_O, eq_U);\n            __m256i or_6 = _mm256_or_si256(or_1, or_2);\n            __m256i or_7 = _mm256_or_si256(or_3, or_4);\n            __m256i or_8 = _mm256_or_si256(or_5, or_6);\n            __m256i or_final = _mm256_or_si256(or_7, or_8);\n\n            count += _mm_popcnt_u32(_mm256_movemask_epi8(or_final));\n        }\n\n        for (; i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    if (n > 0 && (s[n - 1] == 'y' || s[n - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. \nHere, 'y' is also a vowel, but only when it is at the end of the given word.\nExample: \n>>> vowels_count_simd(\"abcde\") \n2 \n>>> vowels_count_simd(\"ACEDY\") \n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint vowels_count_simd(std::string s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    if (n >= 32) {\n        __m256i vowels_lower = _mm256_set1_epi8('a' ^ 'e' ^ 'i' ^ 'o' ^ 'u');\n        __m256i vowels_a = _mm256_set1_epi8('a');\n        __m256i vowels_e = _mm256_set1_epi8('e');\n        __m256i vowels_i = _mm256_set1_epi8('i');\n        __m256i vowels_o = _mm256_set1_epi8('o');\n        __m256i vowels_u = _mm256_set1_epi8('u');\n        __m256i vowels_A = _mm256_set1_epi8('A');\n        __m256i vowels_E = _mm256_set1_epi8('E');\n        __m256i vowels_I = _mm256_set1_epi8('I');\n        __m256i vowels_O = _mm256_set1_epi8('O');\n        __m256i vowels_U = _mm256_set1_epi8('U');\n\n        int i = 0;\n        for (; i <= n - 32; i += 32) {\n            __m256i str_vec = _mm256_loadu_si256((__m256i*)(s.c_str() + i));\n            __m256i eq_a = _mm256_cmpeq_epi8(str_vec, vowels_a);\n            __m256i eq_e = _mm256_cmpeq_epi8(str_vec, vowels_e);\n            __m256i eq_i = _mm256_cmpeq_epi8(str_vec, vowels_i);\n            __m256i eq_o = _mm256_cmpeq_epi8(str_vec, vowels_o);\n            __m256i eq_u = _mm256_cmpeq_epi8(str_vec, vowels_u);\n            __m256i eq_A = _mm256_cmpeq_epi8(str_vec, vowels_A);\n            __m256i eq_E = _mm256_cmpeq_epi8(str_vec, vowels_E);\n            __m256i eq_I = _mm256_cmpeq_epi8(str_vec, vowels_I);\n            __m256i eq_O = _mm256_cmpeq_epi8(str_vec, vowels_O);\n            __m256i eq_U = _mm256_cmpeq_epi8(str_vec, vowels_U);\n\n            __m256i or_1 = _mm256_or_si256(eq_a, eq_e);\n            __m256i or_2 = _mm256_or_si256(eq_i, eq_o);\n            __m256i or_3 = _mm256_or_si256(eq_u, eq_A);\n            __m256i or_4 = _mm256_or_si256(eq_E, eq_I);\n            __m256i or_5 = _mm256_or_si256(eq_O, eq_U);\n            __m256i or_6 = _mm256_or_si256(or_1, or_2);\n            __m256i or_7 = _mm256_or_si256(or_3, or_4);\n            __m256i or_8 = _mm256_or_si256(or_5, or_6);\n            __m256i or_final = _mm256_or_si256(or_7, or_8);\n\n            count += _mm_popcnt_u32(_mm256_movemask_epi8(or_final));\n        }\n        for (; i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    if (n > 0 && (s[n - 1] == 'y' || s[n - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. \nHere, 'y' is also a vowel, but only when it is at the end of the given word.\nExample: \n>>> vowels_count_simd(\"abcde\") \n2 \n>>> vowels_count_simd(\"ACEDY\") \n3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint vowels_count_simd(std::string s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    if (n >= 32) {\n        __m256i vowels_lower = _mm256_set1_epi8('a' | 'e' | 'i' | 'o' | 'u');\n        __m256i vowels_upper = _mm256_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n        for (int i = 0; i <= n - 32; i += 32) {\n            __m256i str_vec = _mm256_loadu_si256((__m256i*)&s[i]);\n            __m256i lower_mask = _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('a')),\n                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('e')),\n                                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('i')),\n                                                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('o')),\n                                                                                                _mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('u'))))));\n\n            __m256i upper_mask = _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('A')),\n                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('E')),\n                                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('I')),\n                                                                                _mm256_or_si256(_mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('O')),\n                                                                                                _mm256_cmpeq_epi8(str_vec, _mm256_set1_epi8('U'))))));\n\n            __m256i combined_mask = _mm256_or_si256(lower_mask, upper_mask);\n            count += _mm_popcnt_u32(_mm256_movemask_epi8(combined_mask));\n        }\n\n        for (int i = n - (n % 32); i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            char c = s[i];\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    if (n > 0 && (s[n - 1] == 'y' || s[n - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) {\n        return 0;\n    }\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'y', 'A', 'E', 'I', 'O', 'U', 'Y', 0, 0, 0, 0,\n                                      'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0);\n\n    int i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i str = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i eq = _mm256_cmpeq_epi8(str, vowels);\n        unsigned int mask = _mm256_movemask_epi8(eq);\n\n        for (int j = 0; j < 32; ++j) {\n            if ((mask & (1 << j)) != 0) {\n                if (s[i + j] == 'y' || s[i + j] == 'Y') {\n                    if (i + j == n - 1) {\n                        count++;\n                    }\n                } else {\n                    count++;\n                }\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n            s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        } else if ((s[i] == 'y' || s[i] == 'Y') && i == n - 1) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a string as input and returns the sum of the upper characters only's\nASCII codes.\nExamples:\n    digitSum_simd(\"\") => 0\n    digitSum_simd(\"abAB\") => 131\n    digitSum_simd(\"abcCd\") => 67\n    digitSum_simd(\"helloE\") => 69\n    digitSum_simd(\"woArBld\") => 131\n    digitSum_simd(\"aAaaaXa\") => 153\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint digitSum_simd(std::string s){\n    int sum = 0;\n    int i = 0;\n    int len = s.length();\n\n    if (len == 0) return 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256((__m256i*)(s.c_str() + i));\n        __m256i mask_vec = _mm256_and_si256(str_vec, _mm256_set1_epi8(0b01000000));\n        __m256i cmp_vec = _mm256_cmpeq_epi8(mask_vec, _mm256_set1_epi8(0));\n        __m256i result_vec = _mm256_andnot_si256(cmp_vec, str_vec);\n\n        unsigned char result_arr[32];\n        _mm256_storeu_si256((__m256i*)result_arr, result_vec);\n\n        for (int j = 0; j < 32; ++j) {\n            if (result_arr[j] >= 'A' && result_arr[j] <= 'Z') {\n                sum += result_arr[j];\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a string as input and returns the sum of the upper characters only's\nASCII codes.\nExamples:\n    digitSum_simd(\"\") => 0\n    digitSum_simd(\"abAB\") => 131\n    digitSum_simd(\"abcCd\") => 67\n    digitSum_simd(\"helloE\") => 69\n    digitSum_simd(\"woArBld\") => 131\n    digitSum_simd(\"aAaaaXa\") => 153\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint digitSum_simd(std::string s){\n    int sum = 0;\n    int len = s.length();\n\n    if (len == 0) return 0;\n\n    int i = 0;\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i str_vec1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i str_vec2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 16));\n\n            __m256i lower_bound = _mm256_set1_epi8('A');\n            __m256i upper_bound = _mm256_set1_epi8('Z');\n\n            __m256i cmp_ge1 = _mm256_cmpgt_epi8(str_vec1, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1)));\n            __m256i cmp_le1 = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), str_vec1);\n            __m256i mask1 = _mm256_and_si256(cmp_ge1, cmp_le1);\n\n            __m256i cmp_ge2 = _mm256_cmpgt_epi8(str_vec2, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1)));\n            __m256i cmp_le2 = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), str_vec2);\n            __m256i mask2 = _mm256_and_si256(cmp_ge2, cmp_le2);\n\n            unsigned int mask_int1 = _mm256_movemask_epi8(mask1);\n            unsigned int mask_int2 = _mm256_movemask_epi8(mask2);\n\n            for (int j = 0; j < 32; ++j) {\n                if ((j < 16 && (mask_int1 & (1 << j))) || (j >= 16 && (mask_int2 & (1 << (j - 16))))) {\n                    sum += s[i + j];\n                }\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a string as input and returns the sum of the upper characters only's\nASCII codes.\nExamples:\n    digitSum_simd(\"\") => 0\n    digitSum_simd(\"abAB\") => 131\n    digitSum_simd(\"abcCd\") => 67\n    digitSum_simd(\"helloE\") => 69\n    digitSum_simd(\"woArBld\") => 131\n    digitSum_simd(\"aAaaaXa\") => 153\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint digitSum_simd(std::string s){\n    int sum = 0;\n    int i = 0;\n    int len = s.length();\n\n    if (len == 0) return 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256((__m256i*)(s.c_str() + i));\n        __m256i mask_vec = _mm256_and_si256(str_vec, _mm256_set1_epi8(0b01000000));\n        __m256i cmp_vec = _mm256_cmpeq_epi8(mask_vec, _mm256_set1_epi8(0));\n        __m256i result_vec = _mm256_andnot_si256(cmp_vec, str_vec);\n\n        unsigned char result_arr[32];\n        _mm256_storeu_si256((__m256i*)result_arr, result_vec);\n\n        for (int j = 0; j < 32; ++j) {\n            if (result_arr[j] >= 'A' && result_arr[j] <= 'Z') {\n                sum += result_arr[j];\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a string as input and returns the sum of the upper characters only's\nASCII codes.\nExamples:\n    digitSum_simd(\"\") => 0\n    digitSum_simd(\"abAB\") => 131\n    digitSum_simd(\"abcCd\") => 67\n    digitSum_simd(\"helloE\") => 69\n    digitSum_simd(\"woArBld\") => 131\n    digitSum_simd(\"aAaaaXa\") => 153\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint digitSum_simd(std::string s){\n    int sum = 0;\n    int len = s.length();\n    int i = 0;\n\n    if (len == 0) return 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256((const __m256i*)(s.c_str() + i));\n        __m256i lower_bound = _mm256_set1_epi8('A');\n        __m256i upper_bound = _mm256_set1_epi8('Z');\n\n        __m256i cmp_lower = _mm256_cmpgt_epi8(str_vec, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1)));\n        __m256i cmp_upper = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), str_vec);\n\n        __m256i and_result = _mm256_and_si256(cmp_lower, cmp_upper);\n\n        unsigned int mask = _mm256_movemask_epi8(and_result);\n\n        for (int j = 0; j < 32; ++j) {\n            if ((mask >> j) & 1) {\n                sum += (int)s[i + j];\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += (int)s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a string as input and returns the sum of the upper characters only's\nASCII codes.\nExamples:\n    digitSum_simd(\"\") => 0\n    digitSum_simd(\"abAB\") => 131\n    digitSum_simd(\"abcCd\") => 67\n    digitSum_simd(\"helloE\") => 69\n    digitSum_simd(\"woArBld\") => 131\n    digitSum_simd(\"aAaaaXa\") => 153\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint digitSum_simd(std::string s){\n    int sum = 0;\n    int len = s.length();\n    int i = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i str_vec1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i str_vec2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 16));\n\n            __m256i lower_bound = _mm256_set1_epi8('A');\n            __m256i upper_bound = _mm256_set1_epi8('Z');\n\n            __m256i cmp_ge1 = _mm256_cmpgt_epi8(str_vec1, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1)));\n            __m256i cmp_le1 = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), str_vec1);\n            __m256i mask1 = _mm256_and_si256(cmp_ge1, cmp_le1);\n\n            __m256i cmp_ge2 = _mm256_cmpgt_epi8(str_vec2, _mm256_sub_epi8(lower_bound, _mm256_set1_epi8(1)));\n            __m256i cmp_le2 = _mm256_cmpgt_epi8(_mm256_add_epi8(upper_bound, _mm256_set1_epi8(1)), str_vec2);\n            __m256i mask2 = _mm256_and_si256(cmp_ge2, cmp_le2);\n\n            unsigned int mask_int1 = _mm256_movemask_epi8(mask1);\n            unsigned int mask_int2 = _mm256_movemask_epi8(mask2);\n\n            for (int j = 0; j < 16; ++j) {\n                if ((mask_int1 >> j) & 1) {\n                    sum += s[i + j];\n                }\n                if ((mask_int2 >> j) & 1) {\n                    sum += s[i + j + 16];\n                }\n            }\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    int i = 0;\n    if (arr.size() >= 8) {\n        for (; i <= (int)arr.size() - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i even_mask = _mm256_andnot_si256(_mm256_set1_epi32(1), vec);\n            \n            alignas(32) int even_nums[8];\n            _mm256_store_si256((__m256i*)even_nums, even_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (even_nums[j] != 0) {\n                    if (even_nums[j] < min_even) {\n                        min_even = even_nums[j];\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < (int)arr.size(); ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    int i = 0;\n    int size = arr.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i even_mask = _mm256_andnot_si256(_mm256_set1_epi32(1), vec); // Check if even\n\n            // Extract even numbers\n            int even_nums[8];\n            _mm256_storeu_si256((__m256i*)even_nums, even_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (even_nums[j] != 1) { // Check if even_nums[j] is even\n                    if (arr[i + j] < min_even) {\n                        min_even = arr[i + j];\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    int i = 0;\n    int size = arr.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i even_mask = _mm256_andnot_si256(_mm256_set1_epi32(1), vec); // Check if even\n\n            // Find minimum even value and its index\n            for (int j = 0; j < 8; ++j) {\n                if ((arr[i + j] % 2 == 0) && (arr[i + j] < min_even)) {\n                    min_even = arr[i + j];\n                    min_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if ((arr[i] % 2 == 0) && (arr[i] < min_even)) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    int i = 0;\n    int size = arr.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i even_mask = _mm256_andnot_si256(_mm256_set1_epi32(1), vec); // Keep only even numbers\n\n            alignas(32) int even_nums[8];\n            _mm256_store_si256((__m256i*)even_nums, even_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (even_nums[j] != 0) {\n                    if (even_nums[j] < min_even) {\n                        min_even = even_nums[j];\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) {\n        return {};\n    }\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    int i = 0;\n    int size = arr.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i even_mask = _mm256_andnot_si256(_mm256_set1_epi32(1), vec); // Check if even\n\n            alignas(32) int even_arr[8];\n            _mm256_store_si256((__m256i*)even_arr, even_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (even_arr[j] != 0) {\n                    if (arr[i + j] < min_even) {\n                        min_even = arr[i + j];\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (arr[i] < min_even) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <unordered_map>\n\nint search_simd(std::vector<int> lst){\n    std::unordered_map<int, int> counts;\n    for (int x : lst) {\n        counts[x]++;\n    }\n\n    int result = -1;\n    for (auto const& [key, val] : counts) {\n        if (key <= val && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <map>\n\nint search_simd(std::vector<int> lst) {\n    std::map<int, int> counts;\n    for (int x : lst) {\n        counts[x]++;\n    }\n\n    int result = -1;\n    for (auto const& [key, val] : counts) {\n        if (key <= val && key > result) {\n            result = key;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <map>\n\nint search_simd(std::vector<int> lst){\n    std::map<int, int> counts;\n    for (int x : lst) {\n        counts[x]++;\n    }\n\n    int result = -1;\n    for (auto const& [key, val] : counts) {\n        if (key <= val && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <map>\n\nint search_simd(std::vector<int> lst) {\n    std::map<int, int> counts;\n    for (int x : lst) {\n        counts[x]++;\n    }\n\n    int result = -1;\n    for (auto const& [key, val] : counts) {\n        if (key <= val && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#include <map>\n\nint search_simd(std::vector<int> lst) {\n    std::map<int, int> counts;\n    for (int x : lst) {\n        counts[x]++;\n    }\n\n    int result = -1;\n    for (auto const& [key, val] : counts) {\n        if (key <= val && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    if (n >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        int i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&q[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal add to sum the vector elements\n        __m128i sum_low = _mm_add_epi32(_mm256_castsi256_si128(sum_vec), _mm256_extracti128_si256(sum_vec, 1));\n        __m128i sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(1, 0, 3, 2));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n        sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(0, 0, 0, 1));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n\n        int sum = _mm_cvtsi128_si32(sum_low);\n\n        // Add remaining elements\n        for (; i < n; ++i) {\n            sum += q[i];\n        }\n\n        return sum <= w;\n    } else {\n        // If the size is less than 8, calculate the sum sequentially\n        int sum = 0;\n        for (int i = 0; i < n; ++i) {\n            sum += q[i];\n        }\n        return sum <= w;\n    }\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    if (n >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        int i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&q[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal add to sum the vector elements\n        __m128i sum_low = _mm_add_epi32(_mm256_castsi256_si128(sum_vec), _mm256_extracti128_si256(sum_vec, 1));\n        __m128i sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(1, 0, 3, 2));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n        sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(0, 0, 0, 1));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n\n        int sum = _mm_cvtsi128_si32(sum_low);\n\n        // Add remaining elements\n        for (; i < n; ++i) {\n            sum += q[i];\n        }\n\n        return sum <= w;\n    } else {\n        int sum = 0;\n        for (int x : q) {\n            sum += x;\n        }\n        return sum <= w;\n    }\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    if (n >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        int i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&q[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal add to get the sum from the vector\n        __m128i sum_low = _mm_add_epi32(_mm256_castsi256_si128(sum_vec), _mm256_extracti128_si256(sum_vec, 1));\n        __m128i sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(1, 0, 3, 2));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n        sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(1, 0, 3, 2));\n        sum_low = _mm_add_epi32(sum_low, sum_high);\n\n        int sum = _mm_cvtsi128_si32(sum_low);\n\n        // Add remaining elements\n        for (; i < n; ++i) {\n            sum += q[i];\n        }\n\n        return sum <= w;\n    } else {\n        int sum = 0;\n        for (int x : q) {\n            sum += x;\n        }\n        return sum <= w;\n    }\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    if (n >= 8) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        int i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&q[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal add to sum the vector elements\n        __m128i sum128_1 = _mm256_extracti128_si256(sum_vec, 0);\n        __m128i sum128_2 = _mm256_extracti128_si256(sum_vec, 1);\n        __m128i sum128 = _mm_add_epi32(sum128_1, sum128_2);\n        __m128i shuf = _mm_shuffle_epi32(sum128, _MM_SHUFFLE(1, 0, 3, 2));\n        sum128 = _mm_add_epi32(sum128, shuf);\n        shuf = _mm_shuffle_epi32(sum128, _MM_SHUFFLE(0, 0, 0, 1));\n        sum128 = _mm_add_epi32(sum128, shuf);\n        int sum = _mm_cvtsi128_si32(sum128);\n\n        // Add remaining elements\n        for (; i < n; ++i) {\n            sum += q[i];\n        }\n\n        return sum <= w;\n    } else {\n        int sum = 0;\n        for (int x : q) {\n            sum += x;\n        }\n        return sum <= w;\n    }\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2\n    int sum = 0;\n    if (n >= 8) {\n        int i = 0;\n        __m256i sum_vec = _mm256_setzero_si256();\n        for (; i <= n - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&q[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n        int temp_sum[8];\n        _mm256_storeu_si256((__m256i*)temp_sum, sum_vec);\n        sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] +\n              temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n\n        for (; i < n; ++i) {\n            sum += q[i];\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            sum += q[i];\n        }\n    }\n\n    // Check if the sum is less than or equal to the maximum weight\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector arr of integers, find the minimum number of elements that\nneed to be changed to make the vector palindromic. A palindromic vector is a vector that\nis read the same backwards and forwards. In one change, you can change one element to any other element.\n\nFor example:\nsmallest_change_simd({1,2,3,5,4,7,9,6}) == 4\nsmallest_change_simd({1, 2, 3, 4, 3, 2, 2}) == 1\nsmallest_change_simd({1, 2, 3, 2, 1}) == 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector arr of integers, find the minimum number of elements that\nneed to be changed to make the vector palindromic. A palindromic vector is a vector that\nis read the same backwards and forwards. In one change, you can change one element to any other element.\n\nFor example:\nsmallest_change_simd({1,2,3,5,4,7,9,6}) == 4\nsmallest_change_simd({1, 2, 3, 4, 3, 2, 2}) == 1\nsmallest_change_simd({1, 2, 3, 2, 1}) == 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector arr of integers, find the minimum number of elements that\nneed to be changed to make the vector palindromic. A palindromic vector is a vector that\nis read the same backwards and forwards. In one change, you can change one element to any other element.\n\nFor example:\nsmallest_change_simd({1,2,3,5,4,7,9,6}) == 4\nsmallest_change_simd({1, 2, 3, 4, 3, 2, 2}) == 1\nsmallest_change_simd({1, 2, 3, 2, 1}) == 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    for (int i = 0; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            changes++;\n        }\n    }\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector arr of integers, find the minimum number of elements that\nneed to be changed to make the vector palindromic. A palindromic vector is a vector that\nis read the same backwards and forwards. In one change, you can change one element to any other element.\n\nFor example:\nsmallest_change_simd({1,2,3,5,4,7,9,6}) == 4\nsmallest_change_simd({1, 2, 3, 4, 3, 2, 2}) == 1\nsmallest_change_simd({1, 2, 3, 2, 1}) == 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector arr of integers, find the minimum number of elements that\nneed to be changed to make the vector palindromic. A palindromic vector is a vector that\nis read the same backwards and forwards. In one change, you can change one element to any other element.\n\nFor example:\nsmallest_change_simd({1,2,3,5,4,7,9,6}) == 4\nsmallest_change_simd({1, 2, 3, 4, 3, 2, 2}) == 1\nsmallest_change_simd({1, 2, 3, 2, 1}) == 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\n/*\nWrite a function that accepts two vectors of strings and returns the vector that has \ntotal number of chars in the all strings of the vector less than the other vector.\n\nif the two vectors have the same number of chars, return the first vector.\n\nExamples\ntotal_match_simd({}, {}) \u279e {}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"Hi\"}) \u279e {\"hI\", \"Hi\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hi\", \"hi\", \"admin\", \"project\"}) \u279e {\"hi\", \"admin\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"hi\", \"hi\"}) \u279e {\"hI\", \"hi\", \"hi\"}\ntotal_match_simd({\"4\"}, {\"1\", \"2\", \"3\", \"4\", \"5\"}) \u279e {\"4\"}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    size_t sum1 = 0;\n    for (const auto& str : lst1) {\n        sum1 += str.length();\n    }\n\n    size_t sum2 = 0;\n    for (const auto& str : lst2) {\n        sum2 += str.length();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that accepts two vectors of strings and returns the vector that has \ntotal number of chars in the all strings of the vector less than the other vector.\n\nif the two vectors have the same number of chars, return the first vector.\n\nExamples\ntotal_match_simd({}, {}) \u279e {}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"Hi\"}) \u279e {\"hI\", \"Hi\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hi\", \"hi\", \"admin\", \"project\"}) \u279e {\"hi\", \"admin\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"hi\", \"hi\"}) \u279e {\"hI\", \"hi\", \"hi\"}\ntotal_match_simd({\"4\"}, {\"1\", \"2\", \"3\", \"4\", \"5\"}) \u279e {\"4\"}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    size_t sum1 = 0;\n    for (const auto& str : lst1) {\n        sum1 += str.length();\n    }\n\n    size_t sum2 = 0;\n    for (const auto& str : lst2) {\n        sum2 += str.length();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that accepts two vectors of strings and returns the vector that has \ntotal number of chars in the all strings of the vector less than the other vector.\n\nif the two vectors have the same number of chars, return the first vector.\n\nExamples\ntotal_match_simd({}, {}) \u279e {}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"Hi\"}) \u279e {\"hI\", \"Hi\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hi\", \"hi\", \"admin\", \"project\"}) \u279e {\"hi\", \"admin\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"hi\", \"hi\"}) \u279e {\"hI\", \"hi\", \"hi\"}\ntotal_match_simd({\"4\"}, {\"1\", \"2\", \"3\", \"4\", \"5\"}) \u279e {\"4\"}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    size_t sum1 = 0;\n    for (const auto& str : lst1) {\n        sum1 += str.length();\n    }\n\n    size_t sum2 = 0;\n    for (const auto& str : lst2) {\n        sum2 += str.length();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\n/*\nWrite a function that accepts two vectors of strings and returns the vector that has \ntotal number of chars in the all strings of the vector less than the other vector.\n\nif the two vectors have the same number of chars, return the first vector.\n\nExamples\ntotal_match_simd({}, {}) \u279e {}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"Hi\"}) \u279e {\"hI\", \"Hi\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hi\", \"hi\", \"admin\", \"project\"}) \u279e {\"hi\", \"admin\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"hi\", \"hi\"}) \u279e {\"hI\", \"hi\", \"hi\"}\ntotal_match_simd({\"4\"}, {\"1\", \"2\", \"3\", \"4\", \"5\"}) \u279e {\"4\"}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    size_t sum1 = 0;\n    for (const auto& str : lst1) {\n        sum1 += str.length();\n    }\n\n    size_t sum2 = 0;\n    for (const auto& str : lst2) {\n        sum2 += str.length();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\n/*\nWrite a function that accepts two vectors of strings and returns the vector that has \ntotal number of chars in the all strings of the vector less than the other vector.\n\nif the two vectors have the same number of chars, return the first vector.\n\nExamples\ntotal_match_simd({}, {}) \u279e {}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"Hi\"}) \u279e {\"hI\", \"Hi\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hi\", \"hi\", \"admin\", \"project\"}) \u279e {\"hi\", \"admin\"}\ntotal_match_simd({\"hi\", \"admin\"}, {\"hI\", \"hi\", \"hi\"}) \u279e {\"hI\", \"hi\", \"hi\"}\ntotal_match_simd({\"4\"}, {\"1\", \"2\", \"3\", \"4\", \"5\"}) \u279e {\"4\"}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    size_t sum1 = 0;\n    for (const auto& str : lst1) {\n        sum1 += str.length();\n    }\n\n    size_t sum2 = 0;\n    for (const auto& str : lst2) {\n        sum2 += str.length();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYour task is to write a function that returns true if a number x is a simple\npower of n and false in other cases.\nx is a simple power of n if n**int=x\nFor example:\nis_simple_power_simd(1, 4) => true\nis_simple_power_simd(2, 2) => true\nis_simple_power_simd(8, 2) => true\nis_simple_power_simd(3, 2) => false\nis_simple_power_simd(3, 1) => false\nis_simple_power_simd(5, 3) => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 0) {\n        return n == 0;\n    }\n    if (n == 0) {\n        return x == 1;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    double log_x = log(x);\n    double log_n = log(n);\n    double result = log_x / log_n;\n    return (fabs(result - round(result)) < 1e-10);\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYour task is to write a function that returns true if a number x is a simple\npower of n and false in other cases.\nx is a simple power of n if n**int=x\nFor example:\nis_simple_power_simd(1, 4) => true\nis_simple_power_simd(2, 2) => true\nis_simple_power_simd(8, 2) => true\nis_simple_power_simd(3, 2) => false\nis_simple_power_simd(3, 1) => false\nis_simple_power_simd(5, 3) => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 0) {\n        return n == 0;\n    }\n    if (n == 0) {\n        return x == 1;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    double log_x = log(x);\n    double log_n = log(n);\n    double power = log_x / log_n;\n\n    return fabs(power - round(power)) < 1e-10;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYour task is to write a function that returns true if a number x is a simple\npower of n and false in other cases.\nx is a simple power of n if n**int=x\nFor example:\nis_simple_power_simd(1, 4) => true\nis_simple_power_simd(2, 2) => true\nis_simple_power_simd(8, 2) => true\nis_simple_power_simd(3, 2) => false\nis_simple_power_simd(3, 1) => false\nis_simple_power_simd(5, 3) => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 0) {\n        return n == 0;\n    }\n    if (n == 0) {\n        return x == 1;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    double log_x = log(x);\n    double log_n = log(n);\n    double result = log_x / log_n;\n    return (fabs(result - round(result)) < 1e-10);\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYour task is to write a function that returns true if a number x is a simple\npower of n and false in other cases.\nx is a simple power of n if n**int=x\nFor example:\nis_simple_power_simd(1, 4) => true\nis_simple_power_simd(2, 2) => true\nis_simple_power_simd(8, 2) => true\nis_simple_power_simd(3, 2) => false\nis_simple_power_simd(3, 1) => false\nis_simple_power_simd(5, 3) => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 0) {\n        return n == 0;\n    }\n    if (x == 1) {\n        return true;\n    }\n    if (n == 0) {\n        return false;\n    }\n\n    double log_x = log(x);\n    double log_n = log(n);\n    double result = log_x / log_n;\n    return (fabs(result - round(result)) < 1e-10);\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYour task is to write a function that returns true if a number x is a simple\npower of n and false in other cases.\nx is a simple power of n if n**int=x\nFor example:\nis_simple_power_simd(1, 4) => true\nis_simple_power_simd(2, 2) => true\nis_simple_power_simd(8, 2) => true\nis_simple_power_simd(3, 2) => false\nis_simple_power_simd(3, 1) => false\nis_simple_power_simd(5, 3) => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 1) {\n        return x == 1;\n    }\n    if (x == 0) {\n        return n == 0;\n    }\n    if (n == 0) {\n        return x == 1;\n    }\n    if (x == 1) {\n        return true;\n    }\n\n    double log_x = log(x);\n    double log_n = log(n);\n    double result = log_x / log_n;\n    return (fabs(result - round(result)) < 1e-10);\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    int count = 0;\n    int len = num.length();\n\n    if (len == 0) return 0;\n\n    for (int i = 0; i < len; i += 32) {\n        int size = std::min(32, len - i);\n        __m256i input = _mm256_loadu_si256((__m256i*)(num.data() + i));\n\n        __m256i prime_mask = _mm256_setzero_si256();\n\n        for (int j = 0; j < size; ++j) {\n            char c = num[i + j];\n            if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    int count = 0;\n    int len = num.length();\n\n    if (len == 0) return 0;\n\n    int i = 0;\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i input1 = _mm256_loadu_si256((const __m256i*)(num.data() + i));\n            __m256i mask2 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0); // '2'\n            __m256i mask3 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0); // '3'\n            __m256i mask5 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0); // '5'\n            __m256i mask7 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0); // '7'\n            __m256i maskB = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0); // 'B'\n            __m256i maskD = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0); // 'D'\n\n            __m256i eq2 = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('2'));\n            __m256i eq3 = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('3'));\n            __m256i eq5 = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('5'));\n            __m256i eq7 = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('7'));\n            __m256i eqB = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('B'));\n            __m256i eqD = _mm256_cmpeq_epi8(input1, _mm256_set1_epi8('D'));\n\n            __m256i or1 = _mm256_or_si256(eq2, eq3);\n            __m256i or2 = _mm256_or_si256(eq5, eq7);\n            __m256i or3 = _mm256_or_si256(eqB, eqD);\n            __m256i or4 = _mm256_or_si256(or1, or2);\n            __m256i or5 = _mm256_or_si256(or4, or3);\n\n            int mask = _mm256_movemask_epi8(or5);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    int count = 0;\n    int len = num.length();\n\n    if (len == 0) return 0;\n\n    for (int i = 0; i < len; i += 32) {\n        int size = std::min(32, len - i);\n        __m256i input = _mm256_loadu_si256((__m256i*)(num.data() + i));\n\n        __m256i prime_mask = _mm256_setzero_si256();\n\n        for (int j = 0; j < size; ++j) {\n            char c = num[i + j];\n            if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    int len = num.length();\n\n    if (len == 0) {\n        return 0;\n    }\n\n    __m256i prime_chars = _mm256_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\n    );\n\n    for (int i = 0; i < len; i += 32) {\n        int remaining = std::min(32, len - i);\n        unsigned char block[32] = {0};\n\n        for (int j = 0; j < remaining; ++j) {\n            block[j] = num[i + j];\n        }\n\n        __m256i data = _mm256_loadu_si256((__m256i*)block);\n\n        __m256i shifted_data;\n        __m256i mask_digits = _mm256_set1_epi8('0');\n        __m256i mask_letters = _mm256_set1_epi8('A' - 10);\n\n        __m256i cmp_digits = _mm256_cmpgt_epi8(data, mask_digits);\n        __m256i cmp_letters = _mm256_cmpgt_epi8(data, mask_letters);\n        \n        __m256i digits_shift_mask = _mm256_and_si256(cmp_digits, _mm256_set1_epi8(0xFF));\n        __m256i letters_shift_mask = _mm256_and_si256(cmp_letters, _mm256_set1_epi8(0xFF));\n\n        __m256i digits_shift = _mm256_sub_epi8(data, _mm256_set1_epi8('0'));\n        __m256i letters_shift = _mm256_sub_epi8(data, _mm256_set1_epi8('A' - 10));\n\n        __m256i shifted_digits = _mm256_and_si256(digits_shift, digits_shift_mask);\n        __m256i shifted_letters = _mm256_and_si256(letters_shift, letters_shift_mask);\n\n        __m256i final_shift = _mm256_or_si256(shifted_digits, shifted_letters);\n\n        __m256i prime_check = _mm256_shuffle_epi8(prime_chars, final_shift);\n\n        unsigned char* prime_results = (unsigned char*)&prime_check;\n\n        for(int k = 0; k < remaining; ++k){\n            count += prime_results[k];\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num){\n    int count = 0;\n    int len = num.length();\n\n    if (len == 0) return 0;\n\n    for (int i = 0; i < len; i += 32) {\n        int size = std::min(32, len - i);\n        __m256i input_vec;\n        char buffer[32];\n\n        for (int j = 0; j < size; ++j) {\n            buffer[j] = num[i + j];\n        }\n        for (int j = size; j < 32; ++j) {\n            buffer[j] = 0; \n        }\n        \n        input_vec = _mm256_loadu_si256((__m256i*)buffer);\n\n        __m256i prime_chars = _mm256_set_epi8(\n            'D', 'B', '7', '5', '3', '2', 0, 0,\n            'D', 'B', '7', '5', '3', '2', 0, 0,\n            'D', 'B', '7', '5', '3', '2', 0, 0,\n            'D', 'B', '7', '5', '3', '2', 0, 0\n        );\n\n        __m256i mask1 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('2'));\n        __m256i mask2 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('3'));\n        __m256i mask3 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('5'));\n        __m256i mask4 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('7'));\n        __m256i mask5 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('B'));\n        __m256i mask6 = _mm256_cmpeq_epi8(input_vec, _mm256_set1_epi8('D'));\n\n        __m256i combined_mask = _mm256_or_si256(mask1, mask2);\n        combined_mask = _mm256_or_si256(combined_mask, mask3);\n        combined_mask = _mm256_or_si256(combined_mask, mask4);\n        combined_mask = _mm256_or_si256(combined_mask, mask5);\n        combined_mask = _mm256_or_si256(combined_mask, mask6);\n\n        unsigned int mask = _mm256_movemask_epi8(combined_mask);\n\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nYour task is to check if the string is happy or not.\nA string is happy if its length is at least 3 and every 3 consecutive letters are distinct\nFor example:\nis_happy_simd(\"a\") => false\nis_happy_simd(\"aa\") => false\nis_happy_simd(\"abcd\") => true\nis_happy_simd(\"aabb\") => false\nis_happy_simd(\"adb\") => true\nis_happy_simd(\"xyy\") => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if (n < 3) return false;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nYour task is to check if the string is happy or not.\nA string is happy if its length is at least 3 and every 3 consecutive letters are distinct\nFor example:\nis_happy_simd(\"a\") => false\nis_happy_simd(\"aa\") => false\nis_happy_simd(\"abcd\") => true\nis_happy_simd(\"aabb\") => false\nis_happy_simd(\"adb\") => true\nis_happy_simd(\"xyy\") => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if (n < 3) return false;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nYour task is to check if the string is happy or not.\nA string is happy if its length is at least 3 and every 3 consecutive letters are distinct\nFor example:\nis_happy_simd(\"a\") => false\nis_happy_simd(\"aa\") => false\nis_happy_simd(\"abcd\") => true\nis_happy_simd(\"aabb\") => false\nis_happy_simd(\"adb\") => true\nis_happy_simd(\"xyy\") => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if (n < 3) return false;\n    for (int i = 0; i <= n - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nYour task is to check if the string is happy or not.\nA string is happy if its length is at least 3 and every 3 consecutive letters are distinct\nFor example:\nis_happy_simd(\"a\") => false\nis_happy_simd(\"aa\") => false\nis_happy_simd(\"abcd\") => true\nis_happy_simd(\"aabb\") => false\nis_happy_simd(\"adb\") => true\nis_happy_simd(\"xyy\") => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if (n < 3) return false;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        if (s[i] == s[i+1] || s[i] == s[i+2] || s[i+1] == s[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a string s.\nYour task is to check if the string is happy or not.\nA string is happy if its length is at least 3 and every 3 consecutive letters are distinct\nFor example:\nis_happy_simd(\"a\") => false\nis_happy_simd(\"aa\") => false\nis_happy_simd(\"abcd\") => true\nis_happy_simd(\"aabb\") => false\nis_happy_simd(\"adb\") => true\nis_happy_simd(\"xyy\") => false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if (n < 3) return false;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a non-empty string and returns true if the string\nlength is a prime number or false otherwise\nExamples\nprime_length_simd(\"Hello\") == true\nprime_length_simd(\"abcdcba\") == true\nprime_length_simd(\"kittens\") == true\nprime_length_simd(\"orange\") == false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool prime_length_simd(const std::string & str){\n    int n = str.length();\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    for (int i = 5; i * i <= n; i = i + 6)\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a non-empty string and returns true if the string\nlength is a prime number or false otherwise\nExamples\nprime_length_simd(\"Hello\") == true\nprime_length_simd(\"abcdcba\") == true\nprime_length_simd(\"kittens\") == true\nprime_length_simd(\"orange\") == false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool prime_length_simd(const std::string & str){\n    int n = str.length();\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i vec = _mm256_set_epi32(5, 7, 11, 13, 17, 19, 23, 29);\n    __m256i inc = _mm256_set_epi32(30, 30, 30, 30, 30, 30, 30, 30);\n    __m256i n_vec = _mm256_set1_epi32(n);\n\n    for (int i = 5; i * i <= n; ) {\n        __m256i cmp = _mm256_cmpeq_epi32(_mm256_mullo_epi32(_mm256_div_epi32(n_vec, vec), vec), n_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n\n        vec = _mm256_add_epi32(vec, inc);\n        \n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec);\n        i = arr[0];\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a non-empty string and returns true if the string\nlength is a prime number or false otherwise\nExamples\nprime_length_simd(\"Hello\") == true\nprime_length_simd(\"abcdcba\") == true\nprime_length_simd(\"kittens\") == true\nprime_length_simd(\"orange\") == false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool prime_length_simd(const std::string & str){\n    int n = str.length();\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    for (int i = 5; i * i <= n; i = i + 6)\n        if (n % i == 0 || n % (i + 2) == 0)\n            return false;\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a non-empty string and returns true if the string\nlength is a prime number or false otherwise\nExamples\nprime_length_simd(\"Hello\") == true\nprime_length_simd(\"abcdcba\") == true\nprime_length_simd(\"kittens\") == true\nprime_length_simd(\"orange\") == false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool prime_length_simd(const std::string & str){\n    int n = str.length();\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i vec = _mm256_set_epi32(5, 7, 11, 13, 17, 19, 23, 29);\n    __m256i inc = _mm256_set_epi32(30, 30, 30, 30, 30, 30, 30, 30);\n    __m256i n_vec = _mm256_set1_epi32(n);\n\n    for (int i = 5; i * i <= n; ) {\n        __m256i cmp = _mm256_cmpeq_epi32(_mm256_mullo_epi32(_mm256_div_epi32(n_vec, vec), vec), n_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n\n        vec = _mm256_add_epi32(vec, inc);\n\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec);\n        i = arr[0];\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function that takes a non-empty string and returns true if the string\nlength is a prime number or false otherwise\nExamples\nprime_length_simd(\"Hello\") == true\nprime_length_simd(\"abcdcba\") == true\nprime_length_simd(\"kittens\") == true\nprime_length_simd(\"orange\") == false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool prime_length_simd(const std::string & str){\n    int n = str.length();\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i vec = _mm256_set_epi32(5, 7, 11, 13, 17, 19, 23, 29);\n    __m256i inc = _mm256_set_epi32(30, 30, 30, 30, 30, 30, 30, 30);\n    \n    for (int i = 5; i * i <= n; ) {\n        \n        int values[8];\n        _mm256_storeu_si256((__m256i*)values, vec);\n        for(int j = 0; j < 8; ++j){\n            if (n % values[j] == 0) return false;\n        }\n        vec = _mm256_add_epi32(vec, inc);\n        i = values[0] + 30;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nGiven a positive integer N, return the total sum of its digits in binary.\n\nExample\n    For N = 1000, the sum of digits will be 1 the output should be \"1\".\n    For N = 150, the sum of digits will be 6 the output should be \"110\".\n    For N = 147, the sum of digits will be 12 the output should be \"1100\".\n\nVariables:\n    @N 64-bit integer\nOutput:\n     a string of binary number\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary = \"\";\n    while (sum > 0) {\n        binary += (sum % 2 == 0) ? '0' : '1';\n        sum /= 2;\n    }\n\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nGiven a positive integer N, return the total sum of its digits in binary.\n\nExample\n    For N = 1000, the sum of digits will be 1 the output should be \"1\".\n    For N = 150, the sum of digits will be 6 the output should be \"110\".\n    For N = 147, the sum of digits will be 12 the output should be \"1100\".\n\nVariables:\n    @N 64-bit integer\nOutput:\n     a string of binary number\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary = \"\";\n    while (sum > 0) {\n        binary += (sum % 2 == 0) ? '0' : '1';\n        sum /= 2;\n    }\n\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nGiven a positive integer N, return the total sum of its digits in binary.\n\nExample\n    For N = 1000, the sum of digits will be 1 the output should be \"1\".\n    For N = 150, the sum of digits will be 6 the output should be \"110\".\n    For N = 147, the sum of digits will be 12 the output should be \"1100\".\n\nVariables:\n    @N 64-bit integer\nOutput:\n     a string of binary number\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary = \"\";\n    while (sum > 0) {\n        binary += (sum % 2 == 0) ? '0' : '1';\n        sum /= 2;\n    }\n\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nGiven a positive integer N, return the total sum of its digits in binary.\n\nExample\n    For N = 1000, the sum of digits will be 1 the output should be \"1\".\n    For N = 150, the sum of digits will be 6 the output should be \"110\".\n    For N = 147, the sum of digits will be 12 the output should be \"1100\".\n\nVariables:\n    @N 64-bit integer\nOutput:\n     a string of binary number\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary = \"\";\n    while (sum > 0) {\n        binary += (sum % 2 == 0) ? '0' : '1';\n        sum /= 2;\n    }\n\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nGiven a positive integer N, return the total sum of its digits in binary.\n\nExample\n    For N = 1000, the sum of digits will be 1 the output should be \"1\".\n    For N = 150, the sum of digits will be 6 the output should be \"110\".\n    For N = 147, the sum of digits will be 12 the output should be \"1100\".\n\nVariables:\n    @N 64-bit integer\nOutput:\n     a string of binary number\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    if (sum == 0) {\n        return \"0\";\n    }\n\n    std::string binary = \"\";\n    while (sum > 0) {\n        binary += (sum % 2 == 0) ? '0' : '1';\n        sum /= 2;\n    }\n\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers lst. add the even elements that are at odd indices.\nExamples:\n    add_simd({4, 2, 6, 7}) ==> 2 \n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 1;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(vec, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_result);\n\n        for (int j = 0; j < 8; ++j) {\n            if ((mask & (1 << (j * 4))) != 0) {\n                sum += lst[i + j];\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers lst. add the even elements that are at odd indices.\nExamples:\n    add_simd({4, 2, 6, 7}) ==> 2 \n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 1;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(vec, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_result);\n\n        if ((mask & 0b11111111) == 0) sum += lst[i];\n        if ((mask & 0b1111111100000000) == 0) sum += lst[i+1];\n        if ((mask & 0b111111110000000000000000) == 0) sum += lst[i+2];\n        if ((mask & 0b11111111000000000000000000000000) == 0) sum += lst[i+3];\n        if ((mask & 0b1111111100000000000000000000000000000000) == 0) sum += lst[i+4];\n        if ((mask & 0b111111110000000000000000000000000000000000000000) == 0) sum += lst[i+5];\n        if ((mask & 0b11111111000000000000000000000000000000000000000000000000) == 0) sum += lst[i+6];\n        if ((mask & 0b1111111100000000000000000000000000000000000000000000000000000000) == 0) sum += lst[i+7];\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of integers lst. add the even elements that are at odd indices.\nExamples:\n    add_simd({4, 2, 6, 7}) ==> 2 \n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    for(size_t i = 1; i < lst.size(); i += 2){\n        if(lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers lst. add the even elements that are at odd indices.\nExamples:\n    add_simd({4, 2, 6, 7}) ==> 2 \n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 1;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_set1_epi32(1);\n        __m256i and_result = _mm256_and_si256(vec, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(and_result, _mm256_setzero_si256());\n\n        int mask = _mm256_movemask_epi8(cmp_result);\n\n        if ((mask & 0b11111111) == 0) sum += 0;\n        else {\n            if ((mask & 0b00000001) == 0) sum += lst[i];\n            if ((mask & 0b00000010) == 0) sum += lst[i+1];\n            if ((mask & 0b00000100) == 0) sum += lst[i+2];\n            if ((mask & 0b00001000) == 0) sum += lst[i+3];\n            if ((mask & 0b00010000) == 0) sum += lst[i+4];\n            if ((mask & 0b00100000) == 0) sum += lst[i+5];\n            if ((mask & 0b01000000) == 0) sum += lst[i+6];\n            if ((mask & 0b10000000) == 0) sum += lst[i+7];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers lst. add the even elements that are at odd indices.\nExamples:\n    add_simd({4, 2, 6, 7}) ==> 2 \n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function encrypt that takes a string as an argument and\nreturns a string encrypted with the alphabet being rotated. \nThe alphabet should be rotated in a manner such that the letters \nshift down by two multiplied to two places.\nFor example:\nencrypt_simd(\"hi\") returns \"lm\"\nencrypt_simd(\"asdfghjkl\") returns \"ewhjklnop\"\nencrypt_simd(\"gf\") returns \"kj\"\nencrypt_simd(\"et\") returns \"ix\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string encrypt_simd(const std::string & s){\n    std::string encrypted_string = s;\n    int len = s.length();\n    int i = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i rotated = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n\n            __m256i lower_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), input);\n            __m256i upper_mask = _mm256_cmpgt_epi8(input, _mm256_set1_epi8('a' - 1));\n            __m256i alpha_mask = _mm256_and_si256(lower_mask, upper_mask);\n\n            __m256i overflow_mask = _mm256_cmpgt_epi8(rotated, _mm256_set1_epi8('z'));\n            __m256i need_rotate_back = _mm256_and_si256(alpha_mask, overflow_mask);\n            __m256i rotate_back_amount = _mm256_and_si256(need_rotate_back, _mm256_set1_epi8(26));\n            rotated = _mm256_sub_epi8(rotated, rotate_back_amount);\n\n            _mm256_storeu_si256((__m256i*)(encrypted_string.data() + i), rotated);\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            encrypted_string[i] = (s[i] >= 'a' && s[i] <= 'z') ? (((s[i] - 'a' + 4) % 26) + 'a') : (((s[i] - 'A' + 4) % 26) + 'A');\n        }\n    }\n\n    return encrypted_string;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encrypt_simd(const std::string & s){\n    std::string result = s;\n    size_t len = s.length();\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        __m256i rotated = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n\n        __m256i lower_bound = _mm256_set1_epi8('a' - 1);\n        __m256i upper_bound = _mm256_set1_epi8('z');\n\n        __m256i lower_mask = _mm256_cmpgt_epi8(input, lower_bound);\n        __m256i upper_mask = _mm256_cmpgt_epi8(upper_bound, input);\n\n        __m256i and_mask = _mm256_and_si256(lower_mask, upper_mask);\n\n        __m256i overflow_check = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), rotated);\n        __m256i overflow_mask = _mm256_and_si256(and_mask, overflow_check);\n\n        __m256i reset_mask = _mm256_andnot_si256(overflow_mask, and_mask);\n        __m256i reset_value = _mm256_add_epi8(rotated, _mm256_set1_epi8(-26));\n\n        rotated = _mm256_blendv_epi8(rotated, reset_value, reset_mask);\n\n        _mm256_storeu_si256((__m256i*)(result.data() + i), rotated);\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'a' && s[i] <= 'z') {\n            result[i] = (s[i] - 'a' + 4) % 26 + 'a';\n        } else if (s[i] >= 'A' && s[i] <= 'Z') {\n            result[i] = (s[i] - 'A' + 4) % 26 + 'A';\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function encrypt that takes a string as an argument and\nreturns a string encrypted with the alphabet being rotated. \nThe alphabet should be rotated in a manner such that the letters \nshift down by two multiplied to two places.\nFor example:\nencrypt_simd(\"hi\") returns \"lm\"\nencrypt_simd(\"asdfghjkl\") returns \"ewhjklnop\"\nencrypt_simd(\"gf\") returns \"kj\"\nencrypt_simd(\"et\") returns \"ix\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string encrypt_simd(const std::string & s){\n    std::string encrypted_string = s;\n    int len = s.length();\n    int i = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i rotated = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n\n            __m256i lower_bound = _mm256_set1_epi8('a' - 1);\n            __m256i upper_bound = _mm256_set1_epi8('z');\n            __m256i mask_lower = _mm256_cmpgt_epi8(input, lower_bound);\n            __m256i mask_upper = _mm256_cmplt_epi8(input, upper_bound);\n            __m256i mask = _mm256_and_si256(mask_lower, mask_upper);\n\n            __m256i overflow = _mm256_cmpgt_epi8(rotated, upper_bound);\n            __m256i sub_val = _mm256_and_si256(overflow, _mm256_set1_epi8(26));\n            rotated = _mm256_sub_epi8(rotated, sub_val);\n\n            __m256i not_alpha = _mm256_xor_si256(mask, _mm256_set1_epi8(-1));\n            rotated = _mm256_and_si256(mask, rotated);\n\n            __m256i original_chars = _mm256_and_si256(not_alpha, input);\n            rotated = _mm256_or_si256(rotated, original_chars);\n\n            _mm256_storeu_si256((__m256i*)(encrypted_string.data() + i), rotated);\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            encrypted_string[i] = (s[i] >= 'a' && s[i] <= 'z') ? (((s[i] - 'a' + 4) % 26) + 'a') : (((s[i] - 'A' + 4) % 26) + 'A');\n        }\n    }\n\n    return encrypted_string;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function encrypt that takes a string as an argument and\nreturns a string encrypted with the alphabet being rotated. \nThe alphabet should be rotated in a manner such that the letters \nshift down by two multiplied to two places.\nFor example:\nencrypt_simd(\"hi\") returns \"lm\"\nencrypt_simd(\"asdfghjkl\") returns \"ewhjklnop\"\nencrypt_simd(\"gf\") returns \"kj\"\nencrypt_simd(\"et\") returns \"ix\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string encrypt_simd(const std::string & s){\n    std::string encrypted_string = s;\n    int len = s.length();\n    int i = 0;\n\n    if (len >= 32) {\n        for (; i <= len - 32; i += 32) {\n            __m256i input = _mm256_loadu_si256((__m256i*)(s.data() + i));\n            __m256i rotated = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n\n            __m256i lower_bound = _mm256_set1_epi8('a' - 1);\n            __m256i upper_bound = _mm256_set1_epi8('z');\n            __m256i mask_lower = _mm256_cmpgt_epi8(input, lower_bound);\n            __m256i mask_upper = _mm256_cmplt_epi8(input, upper_bound);\n            __m256i mask = _mm256_and_si256(mask_lower, mask_upper);\n\n            __m256i overflow = _mm256_cmpgt_epi8(rotated, upper_bound);\n            __m256i sub_val = _mm256_and_si256(overflow, _mm256_set1_epi8(26));\n            rotated = _mm256_sub_epi8(rotated, sub_val);\n\n            __m256i not_alpha = _mm256_xor_si256(mask, _mm256_set1_epi8(-1));\n            rotated = _mm256_blendv_epi8(rotated, input, not_alpha);\n\n            _mm256_storeu_si256((__m256i*)(encrypted_string.data() + i), rotated);\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (isalpha(s[i])) {\n            encrypted_string[i] = (s[i] >= 'a' && s[i] <= 'z') ? (((s[i] - 'a' + 4) % 26) + 'a') : (((s[i] - 'A' + 4) % 26) + 'A');\n        }\n    }\n\n    return encrypted_string;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encrypt_simd(const std::string & s){\n    std::string result = s;\n    size_t len = s.length();\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        __m256i rotated = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n\n        __m256i lower_bound = _mm256_set1_epi8('a' - 1);\n        __m256i upper_bound = _mm256_set1_epi8('z');\n        __m256i mask_lower = _mm256_cmpgt_epi8(input, lower_bound);\n        __m256i mask_upper = _mm256_cmplt_epi8(input, upper_bound);\n        __m256i mask = _mm256_and_si256(mask_lower, mask_upper);\n\n        __m256i overflow_check = _mm256_cmpgt_epi8(rotated, upper_bound);\n        __m256i overflow_amount = _mm256_sub_epi8(rotated, upper_bound);\n        __m256i adjustment = _mm256_sub_epi8(_mm256_set1_epi8('a' - 1), overflow_amount);\n        __m256i adjusted_rotated = _mm256_blendv_epi8(rotated, adjustment, overflow_check);\n\n        __m256i final_result = _mm256_blendv_epi8(input, adjusted_rotated, mask);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), final_result);\n    }\n\n    for (; i < len; ++i) {\n        if (s[i] >= 'a' && s[i] <= 'z') {\n            result[i] = (s[i] - 'a' + 4) % 26 + 'a';\n        } else if (s[i] >= 'A' && s[i] <= 'Z') {\n            result[i] = (s[i] - 'A' + 4) % 26 + 'A';\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    std::string encoded_message = message;\n    int len = message.length();\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)&message[i]);\n        __m256i case_mask = _mm256_set1_epi8(0x20);\n        __m256i lower_mask = _mm256_set1_epi8(0x5F);\n        __m256i upper_mask = _mm256_set1_epi8(0x7A);\n        __m256i A = _mm256_set1_epi8('A');\n        __m256i Z = _mm256_set1_epi8('Z');\n        __m256i a = _mm256_set1_epi8('a');\n        __m256i z = _mm256_set1_epi8('z');\n\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_sub_epi8(A, _mm256_set1_epi8(1))), _mm256_cmpgt_epi8(_mm256_add_epi8(Z, _mm256_set1_epi8(1)), data));\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_sub_epi8(a, _mm256_set1_epi8(1))), _mm256_cmpgt_epi8(_mm256_add_epi8(z, _mm256_set1_epi8(1)), data));\n        __m256i is_letter = _mm256_or_si256(is_upper, is_lower);\n        __m256i case_change = _mm256_and_si256(is_letter, case_mask);\n        data = _mm256_xor_si256(data, case_change);\n\n        __m256i vowel_mask = _mm256_set1_epi8(0);\n        __m256i Ae = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('A'));\n        __m256i ae = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('a'));\n        __m256i Ee = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('E'));\n        __m256i ee = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('e'));\n        __m256i Ie = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('I'));\n        __m256i ie = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('i'));\n        __m256i Oe = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('O'));\n        __m256i oe = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('o'));\n        __m256i Ue = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('U'));\n        __m256i ue = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('u'));\n\n        vowel_mask = _mm256_or_si256(vowel_mask, Ae);\n        vowel_mask = _mm256_or_si256(vowel_mask, ae);\n        vowel_mask = _mm256_or_si256(vowel_mask, Ee);\n        vowel_mask = _mm256_or_si256(vowel_mask, ee);\n        vowel_mask = _mm256_or_si256(vowel_mask, Ie);\n        vowel_mask = _mm256_or_si256(vowel_mask, ie);\n        vowel_mask = _mm256_or_si256(vowel_mask, Oe);\n        vowel_mask = _mm256_or_si256(vowel_mask, oe);\n        vowel_mask = _mm256_or_si256(vowel_mask, Ue);\n        vowel_mask = _mm256_or_si256(vowel_mask, ue);\n\n        __m256i two = _mm256_set1_epi8(2);\n        data = _mm256_add_epi8(_mm256_and_si256(vowel_mask, data), _mm256_and_si256(vowel_mask, two));\n\n        _mm256_storeu_si256((__m256i*)&encoded_message[i], data);\n    }\n\n    for (; i < len; ++i) {\n        if (isalpha(encoded_message[i])) {\n            if (islower(encoded_message[i])) {\n                encoded_message[i] = toupper(encoded_message[i]);\n            } else {\n                encoded_message[i] = tolower(encoded_message[i]);\n            }\n        }\n        char c = encoded_message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            encoded_message[i] += 2;\n        }\n    }\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    std::string encoded_message = message;\n    int len = message.length();\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Swap case\n        __m256i lower_mask = _mm256_and_si256(data, _mm256_set1_epi8(0x20));\n        __m256i alpha_mask = _mm256_and_si256(data, _mm256_set1_epi8(0xDF));\n        __m256i case_swap_mask = _mm256_cmpeq_epi8(lower_mask, _mm256_setzero_si256());\n        __m256i case_change = _mm256_and_si256(alpha_mask, case_swap_mask);\n        data = _mm256_xor_si256(data, _mm256_and_si256(case_change, _mm256_set1_epi8(0x20)));\n\n        // Replace vowels\n        __m256i a_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('a'));\n        __m256i e_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('e'));\n        __m256i i_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('i'));\n        __m256i o_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('o'));\n        __m256i u_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('u'));\n        __m256i A_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('A'));\n        __m256i E_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('E'));\n        __m256i I_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('I'));\n        __m256i O_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('O'));\n        __m256i U_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('U'));\n\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('c'), a_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('g'), e_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('k'), i_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('q'), o_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('w'), u_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('C'), A_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('G'), E_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('K'), I_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('Q'), O_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('W'), U_mask);\n\n        _mm256_storeu_si256((__m256i*)&encoded_message[i], data);\n    }\n\n    for (; i < len; ++i) {\n        char c = encoded_message[i];\n        if (isalpha(c)) {\n            if (islower(c)) {\n                encoded_message[i] = toupper(c);\n            } else {\n                encoded_message[i] = tolower(c);\n            }\n        }\n        c = encoded_message[i];\n        switch (c) {\n            case 'a': encoded_message[i] = 'c'; break;\n            case 'e': encoded_message[i] = 'g'; break;\n            case 'i': encoded_message[i] = 'k'; break;\n            case 'o': encoded_message[i] = 'q'; break;\n            case 'u': encoded_message[i] = 'w'; break;\n            case 'A': encoded_message[i] = 'C'; break;\n            case 'E': encoded_message[i] = 'G'; break;\n            case 'I': encoded_message[i] = 'K'; break;\n            case 'O': encoded_message[i] = 'Q'; break;\n            case 'U': encoded_message[i] = 'W'; break;\n        }\n    }\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    std::string encoded_message = message;\n    int len = message.length();\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((const __m256i*)(message.data() + i));\n\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('z' + 1);\n        __m256i is_lower = _mm256_cmpgt_epi8(chars, lower_mask);\n        __m256i is_upper = _mm256_cmpgt_epi8(upper_mask, chars);\n        __m256i is_letter = _mm256_and_si256(is_lower, is_upper);\n\n        __m256i case_change = _mm256_and_si256(is_letter, _mm256_set1_epi8(32));\n        chars = _mm256_xor_si256(chars, case_change);\n\n        __m256i a_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('a'));\n        __m256i e_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('e'));\n        __m256i i_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('i'));\n        __m256i o_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('o'));\n        __m256i u_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('u'));\n        __m256i A_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('A'));\n        __m256i E_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('E'));\n        __m256i I_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('I'));\n        __m256i O_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('O'));\n        __m256i U_mask = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('U'));\n\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('c'), a_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('g'), e_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('k'), i_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('q'), o_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('w'), u_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('C'), A_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('G'), E_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('K'), I_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('Q'), O_mask);\n        chars = _mm256_blendv_epi8(chars, _mm256_set1_epi8('W'), U_mask);\n\n        _mm256_storeu_si256((__m256i*)(encoded_message.data() + i), chars);\n    }\n\n    for (; i < len; ++i) {\n        char c = encoded_message[i];\n        if (isalpha(c)) {\n            if (islower(c)) {\n                encoded_message[i] = toupper(c);\n            } else {\n                encoded_message[i] = tolower(c);\n            }\n        }\n        \n        c = encoded_message[i];\n        switch (c) {\n            case 'a': encoded_message[i] = 'c'; break;\n            case 'e': encoded_message[i] = 'g'; break;\n            case 'i': encoded_message[i] = 'k'; break;\n            case 'o': encoded_message[i] = 'q'; break;\n            case 'u': encoded_message[i] = 'w'; break;\n            case 'A': encoded_message[i] = 'C'; break;\n            case 'E': encoded_message[i] = 'G'; break;\n            case 'I': encoded_message[i] = 'K'; break;\n            case 'O': encoded_message[i] = 'Q'; break;\n            case 'U': encoded_message[i] = 'W'; break;\n        }\n    }\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    std::string encoded_message = message;\n    int len = message.length();\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Swap case\n        __m256i lower_mask = _mm256_and_si256(data, _mm256_set1_epi8(0x20));\n        __m256i alpha_mask = _mm256_cmpgt_epi8(data, _mm256_set1_epi8(64));\n        __m256i alpha_mask2 = _mm256_cmplt_epi8(data, _mm256_set1_epi8(91));\n        __m256i alpha_mask_final = _mm256_and_si256(alpha_mask, alpha_mask2);\n\n        __m256i alpha_mask3 = _mm256_cmpgt_epi8(data, _mm256_set1_epi8(96));\n        __m256i alpha_mask4 = _mm256_cmplt_epi8(data, _mm256_set1_epi8(123));\n        __m256i alpha_mask_final2 = _mm256_and_si256(alpha_mask3, alpha_mask4);\n\n        __m256i alpha_final = _mm256_or_si256(alpha_mask_final, alpha_mask_final2);\n        __m256i case_change = _mm256_xor_si256(data, _mm256_and_si256(alpha_final, _mm256_set1_epi8(32)));\n\n        //Vowel replacement\n        __m256i a_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('a'));\n        __m256i e_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('e'));\n        __m256i i_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('i'));\n        __m256i o_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('o'));\n        __m256i u_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('u'));\n\n        __m256i A_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('A'));\n        __m256i E_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('E'));\n        __m256i I_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('I'));\n        __m256i O_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('O'));\n        __m256i U_mask = _mm256_cmpeq_epi8(case_change, _mm256_set1_epi8('U'));\n\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('c'), a_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('g'), e_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('k'), i_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('q'), o_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('w'), u_mask);\n\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('C'), A_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('G'), E_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('K'), I_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('Q'), O_mask);\n        case_change = _mm256_blendv_epi8(case_change, _mm256_set1_epi8('W'), U_mask);\n\n        _mm256_storeu_si256((__m256i*)&encoded_message[i], case_change);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = encoded_message[i];\n        if (isalpha(c)) {\n            if (islower(c)) {\n                encoded_message[i] = toupper(c);\n            } else {\n                encoded_message[i] = tolower(c);\n            }\n\n            c = encoded_message[i];\n\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                encoded_message[i] = c + 2;\n            } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                encoded_message[i] = c + 2;\n            }\n        }\n    }\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    std::string encoded_message = message;\n    int len = message.length();\n    int i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Swap case\n        __m256i lower_mask = _mm256_and_si256(data, _mm256_set1_epi8(0x20));\n        __m256i alpha_mask = _mm256_cmpgt_epi8(data, _mm256_set1_epi8(64));\n        __m256i alpha_mask2 = _mm256_cmplt_epi8(data, _mm256_set1_epi8(91));\n        __m256i alpha_mask_final = _mm256_and_si256(alpha_mask, alpha_mask2);\n\n        __m256i alpha_mask3 = _mm256_cmpgt_epi8(data, _mm256_set1_epi8(96));\n        __m256i alpha_mask4 = _mm256_cmplt_epi8(data, _mm256_set1_epi8(123));\n        __m256i alpha_mask_final2 = _mm256_and_si256(alpha_mask3, alpha_mask4);\n\n        __m256i alpha_final = _mm256_or_si256(alpha_mask_final, alpha_mask_final2);\n        __m256i case_change = _mm256_xor_si256(data, _mm256_and_si256(alpha_final, _mm256_set1_epi8(32)));\n        data = case_change;\n\n        // Replace vowels\n        __m256i a_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('a'));\n        __m256i e_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('e'));\n        __m256i i_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('i'));\n        __m256i o_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('o'));\n        __m256i u_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('u'));\n        __m256i A_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('A'));\n        __m256i E_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('E'));\n        __m256i I_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('I'));\n        __m256i O_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('O'));\n        __m256i U_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('U'));\n\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('c'), a_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('g'), e_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('k'), i_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('q'), o_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('w'), u_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('C'), A_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('G'), E_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('K'), I_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('Q'), O_mask);\n        data = _mm256_blendv_epi8(data, _mm256_set1_epi8('W'), U_mask);\n\n        _mm256_storeu_si256((__m256i*)&encoded_message[i], data);\n    }\n\n    for (; i < len; ++i) {\n        if (isalpha(message[i])) {\n            if (islower(message[i])) {\n                encoded_message[i] = toupper(message[i]);\n            } else {\n                encoded_message[i] = tolower(message[i]);\n            }\n        }\n        char c = encoded_message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            encoded_message[i] = c + 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            encoded_message[i] = c + 2;\n        }\n    }\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        const char* key_str = key.c_str();\n        size_t key_len = key.length();\n\n        if (key_len == 0) {\n            all_lower = false;\n            all_upper = false;\n            break;\n        }\n\n        bool is_lower = true;\n        bool is_upper = true;\n\n        size_t i = 0;\n        if (key_len >= 32) {\n            size_t num_iters = key_len / 32;\n            for (size_t j = 0; j < num_iters; ++j) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(key_str + i));\n                __m256i lower_bound = _mm256_set1_epi8('a');\n                __m256i upper_bound = _mm256_set1_epi8('z');\n                __m256i upper_lower_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(upper_lower_cmp, upper_lower_cmp) != 1) {\n                    is_upper = false;\n                }\n\n                lower_bound = _mm256_set1_epi8('A');\n                upper_bound = _mm256_set1_epi8('Z');\n                __m256i lower_upper_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(lower_upper_cmp, lower_upper_cmp) != 1) {\n                    is_lower = false;\n                }\n                i += 32;\n            }\n        }\n        \n        for (; i < key_len; ++i) {\n            if (!((key_str[i] >= 'a' && key_str[i] <= 'z') || (key_str[i] >= '0' && key_str[i] <= '9'))) {\n                is_lower = false;\n            }\n            if (!((key_str[i] >= 'A' && key_str[i] <= 'Z') || (key_str[i] >= '0' && key_str[i] <= '9'))) {\n                is_upper = false;\n            }\n        }\n\n        if (!is_lower) {\n            all_lower = false;\n        }\n        if (!is_upper) {\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            break;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        const char* key_str = key.c_str();\n        size_t key_len = key.length();\n\n        if (key_len == 0) {\n            all_lower = false;\n            all_upper = false;\n            break;\n        }\n\n        bool is_lower = true;\n        bool is_upper = true;\n\n        size_t i = 0;\n        if (key_len >= 32) {\n            size_t num_iters = key_len / 32;\n            for (size_t j = 0; j < num_iters; ++j) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(key_str + i));\n                __m256i lower_bound = _mm256_set1_epi8('a');\n                __m256i upper_bound = _mm256_set1_epi8('z');\n                __m256i upper_lower_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(upper_lower_cmp, upper_lower_cmp) != 1) {\n                    is_upper = false;\n                }\n\n                lower_bound = _mm256_set1_epi8('A');\n                upper_bound = _mm256_set1_epi8('Z');\n                __m256i lower_upper_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(lower_upper_cmp, lower_upper_cmp) != 1) {\n                    is_lower = false;\n                }\n                i += 32;\n            }\n        }\n\n        for (; i < key_len; ++i) {\n            if (!((key_str[i] >= 'a' && key_str[i] <= 'z'))) {\n                is_lower = false;\n            }\n            if (!((key_str[i] >= 'A' && key_str[i] <= 'Z'))) {\n                is_upper = false;\n            }\n        }\n\n        if (!is_lower) {\n            all_lower = false;\n        }\n        if (!is_upper) {\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            break;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        const char* key_str = key.c_str();\n        size_t key_len = key.length();\n\n        bool key_all_lower = true;\n        bool key_all_upper = true;\n\n        size_t i = 0;\n        if (key_len >= 32) {\n            for (; i <= key_len - 32; i += 32) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(key_str + i));\n                \n                __m256i lower_bound = _mm256_set1_epi8('a');\n                __m256i upper_bound = _mm256_set1_epi8('z');\n                __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(lower_mask, lower_mask) == 0) {\n                    key_all_upper = false;\n                }\n                \n                lower_bound = _mm256_set1_epi8('A');\n                upper_bound = _mm256_set1_epi8('Z');\n                __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(upper_mask, upper_mask) == 0) {\n                    key_all_lower = false;\n                }\n            }\n        }\n\n        for (; i < key_len; ++i) {\n            if (key_all_lower && !(islower(key_str[i]) != 0)) {\n                key_all_lower = false;\n            }\n            if (key_all_upper && !(isupper(key_str[i]) != 0)) {\n                key_all_upper = false;\n            }\n        }\n\n        if (!key_all_lower) {\n            all_lower = false;\n        }\n        if (!key_all_upper) {\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        const char* key_str = key.c_str();\n        size_t key_len = key.length();\n\n        if (key_len == 0) {\n            all_lower = false;\n            all_upper = false;\n            break;\n        }\n\n        bool is_lower = true;\n        bool is_upper = true;\n\n        size_t i = 0;\n        if (key_len >= 32) {\n            size_t num_iters = key_len / 32;\n            for (size_t j = 0; j < num_iters; ++j) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(key_str + i));\n                __m256i lower_bound = _mm256_set1_epi8('a');\n                __m256i upper_bound = _mm256_set1_epi8('z');\n                __m256i upper_lower_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(upper_lower_cmp, upper_lower_cmp) != 1) {\n                    is_upper = false;\n                }\n                \n                lower_bound = _mm256_set1_epi8('A');\n                upper_bound = _mm256_set1_epi8('Z');\n                __m256i lower_upper_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(lower_upper_cmp, lower_upper_cmp) != 1) {\n                    is_lower = false;\n                }\n                i += 32;\n            }\n        }\n\n        for (; i < key_len; ++i) {\n            if (!std::islower(key_str[i])) {\n                is_lower = false;\n            }\n            if (!std::isupper(key_str[i])) {\n                is_upper = false;\n            }\n        }\n\n        if (!is_lower) {\n            all_lower = false;\n        }\n        if (!is_upper) {\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        const char* key_str = key.c_str();\n        size_t key_len = key.length();\n\n        if (key_len == 0) {\n            all_lower = false;\n            all_upper = false;\n            break;\n        }\n\n        bool is_lower = true;\n        bool is_upper = true;\n\n        size_t i = 0;\n        if (key_len >= 32) {\n            size_t num_iters = key_len / 32;\n            for (size_t j = 0; j < num_iters; ++j) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(key_str + i));\n                __m256i lower_bound = _mm256_set1_epi8('a');\n                __m256i upper_bound = _mm256_set1_epi8('z');\n                __m256i upper_lower_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(upper_lower_cmp, upper_lower_cmp) != 1) {\n                    is_upper = false;\n                }\n\n                lower_bound = _mm256_set1_epi8('A');\n                upper_bound = _mm256_set1_epi8('Z');\n                __m256i lower_upper_cmp = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_bound), _mm256_cmpgt_epi8(upper_bound, chars));\n                if (_mm256_testz_si256(lower_upper_cmp, lower_upper_cmp) != 1) {\n                    is_lower = false;\n                }\n                i += 32;\n            }\n        }\n\n        for (; i < key_len; ++i) {\n            if (!std::islower(key_str[i])) {\n                is_lower = false;\n            }\n            if (!std::isupper(key_str[i])) {\n                is_upper = false;\n            }\n        }\n\n        if (!is_lower) {\n            all_lower = false;\n        }\n        if (!is_upper) {\n            all_upper = false;\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nImplement a function that takes an non-negative integer and returns a vector of the first n\nintegers that are prime numbers and less than n.\nfor example:\ncount_up_to_simd(5) => {2,3}\ncount_up_to_simd(11) => {2,3,5,7}\ncount_up_to_simd(0) => {}\ncount_up_to_simd(20) => {2,3,5,7,11,13,17,19}\ncount_up_to_simd(1) => {}\ncount_up_to_simd(18) => {2,3,5,7,11,13,17}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nImplement a function that takes an non-negative integer and returns a vector of the first n\nintegers that are prime numbers and less than n.\nfor example:\ncount_up_to_simd(5) => {2,3}\ncount_up_to_simd(11) => {2,3,5,7}\ncount_up_to_simd(0) => {}\ncount_up_to_simd(20) => {2,3,5,7,11,13,17,19}\ncount_up_to_simd(1) => {}\ncount_up_to_simd(18) => {2,3,5,7,11,13,17}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nImplement a function that takes an non-negative integer and returns a vector of the first n\nintegers that are prime numbers and less than n.\nfor example:\ncount_up_to_simd(5) => {2,3}\ncount_up_to_simd(11) => {2,3,5,7}\ncount_up_to_simd(0) => {}\ncount_up_to_simd(20) => {2,3,5,7,11,13,17,19}\ncount_up_to_simd(1) => {}\ncount_up_to_simd(18) => {2,3,5,7,11,13,17}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    std::vector<bool> is_prime(n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a string s, count the number of uppercase vowels in even indices.\n\nFor example:\ncount_upper_simd(\"aBCdEf\") returns 1\ncount_upper_simd(\"abcdefg\") returns 0\ncount_upper_simd(\"dBBE\") returns 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 8) {\n        if (i + 7 >= n) {\n            for (int j = i; j < n; j += 2) {\n                char c = s[j];\n                if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                    count++;\n                }\n            }\n            break;\n        }\n\n        __m256i chars = _mm256_loadu_si256((const __m256i*) &s[i]);\n\n        unsigned int mask = 0;\n        for (int j = 0; j < 8; ++j) {\n            char c = ((char*)&chars)[2*j];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a string s, count the number of uppercase vowels in even indices.\n\nFor example:\ncount_upper_simd(\"aBCdEf\") returns 1\ncount_upper_simd(\"abcdefg\") returns 0\ncount_upper_simd(\"dBBE\") returns 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 8) {\n        if (i + 7 >= n) {\n            for (int j = i; j < n; j += 2) {\n                char c = s[j];\n                if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                    count++;\n                }\n            }\n            break;\n        }\n\n        __m256i chars = _mm256_loadu_si256((const __m256i*) &s[i]);\n        unsigned int mask = 0;\n\n        for(int j = 0; j < 8; j++){\n            char c = ((char*)&chars)[j*4];\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a string s, count the number of uppercase vowels in even indices.\n\nFor example:\ncount_upper_simd(\"aBCdEf\") returns 1\ncount_upper_simd(\"abcdefg\") returns 0\ncount_upper_simd(\"dBBE\") returns 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 8) {\n        if (i + 7 >= n) {\n            for (int j = i; j < n; j += 2) {\n                if (s[j] == 'A' || s[j] == 'E' || s[j] == 'I' || s[j] == 'O' || s[j] == 'U') {\n                    count++;\n                }\n            }\n            break;\n        }\n\n        __m256i str = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        unsigned int mask = 0b01010101010101010101010101010101;\n        __m256i even_indices_mask = _mm256_set_epi8(\n            (mask >> 0) & 1, (mask >> 1) & 1, (mask >> 2) & 1, (mask >> 3) & 1,\n            (mask >> 4) & 1, (mask >> 5) & 1, (mask >> 6) & 1, (mask >> 7) & 1,\n            (mask >> 8) & 1, (mask >> 9) & 1, (mask >> 10) & 1, (mask >> 11) & 1,\n            (mask >> 12) & 1, (mask >> 13) & 1, (mask >> 14) & 1, (mask >> 15) & 1,\n            (mask >> 16) & 1, (mask >> 17) & 1, (mask >> 18) & 1, (mask >> 19) & 1,\n            (mask >> 20) & 1, (mask >> 21) & 1, (mask >> 22) & 1, (mask >> 23) & 1,\n            (mask >> 24) & 1, (mask >> 25) & 1, (mask >> 26) & 1, (mask >> 27) & 1,\n            (mask >> 28) & 1, (mask >> 29) & 1, (mask >> 30) & 1, (mask >> 31) & 1\n        );\n\n        __m256i and_result = _mm256_and_si256(str, even_indices_mask);\n\n        char even_chars[32];\n        _mm256_storeu_si256((__m256i*)even_chars, and_result);\n\n        for (int j = 0; j < 32; ++j) {\n            if (even_chars[j] != 0) {\n                if (s[i + j] == 'A' || s[i + j] == 'E' || s[i + j] == 'I' || s[i + j] == 'O' || s[i + j] == 'U') {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a string s, count the number of uppercase vowels in even indices.\n\nFor example:\ncount_upper_simd(\"aBCdEf\") returns 1\ncount_upper_simd(\"abcdefg\") returns 0\ncount_upper_simd(\"dBBE\") returns 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 8) {\n        if (i + 7 >= n) {\n            for (int j = i; j < n; ++j) {\n                if (j % 2 == 0) {\n                    char c = s[j];\n                    if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                        count++;\n                    }\n                }\n            }\n            break;\n        }\n\n        __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        unsigned int mask = 0;\n        for(int j = 0; j < 8; ++j){\n            if((i+j) % 2 == 0){\n                char c = ((char*)&chars)[j];\n                if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a string s, count the number of uppercase vowels in even indices.\n\nFor example:\ncount_upper_simd(\"aBCdEf\") returns 1\ncount_upper_simd(\"abcdefg\") returns 0\ncount_upper_simd(\"dBBE\") returns 0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    int n = s.length();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 8) {\n        if (i + 7 >= n) {\n            for (int j = i; j < n; ++j) {\n                if (j % 2 == 0) {\n                    char c = s[j];\n                    if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                        count++;\n                    }\n                }\n            }\n            break;\n        }\n\n        __m256i chars = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        unsigned int mask = 0;\n        for(int j = 0; j < 8; ++j){\n            if((i+j) % 2 == 0){\n                char c = ((char*)&chars)[j];\n                if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    int start = n;\n    if (n % 2 == 0) {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    int start = n;\n    if (n % 2 == 0) {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    int start = n;\n    if (n % 2 == 0) {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    int start = n;\n    if (n % 2 == 0) {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    int start = n;\n    if (n % 2 == 0) {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            result[i] = start + 2 * i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nYou are given two positive integers n and m, and your task is to compute the\naverage of the integers from n through m (including n and m). \nRound the answer to the nearest integer(smaller one) and convert that to binary.\nIf n is greater than m, return \"-1\".\nExample:\nrounded_avg_simd(1, 5) => \"11\"\nrounded_avg_simd(7, 5) => \"-1\"\nrounded_avg_simd(10, 20) => \"1111\"\nrounded_avg_simd(20, 33) => \"11010\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    if (count < 8) {\n        for (int64_t i = n; i <= m; ++i) {\n            sum += i;\n        }\n    } else {\n        int64_t i = n;\n        for (; i <= m - 7; i += 8) {\n            __m256i vec = _mm256_set_epi64x(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n            \n            int64_t temp_sum[4];\n            _mm256_storeu_si256((__m256i*)temp_sum, vec);\n            sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] + temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n        }\n        for (; i <= m; ++i) {\n            sum += i;\n        }\n    }\n    \n    int64_t avg = sum / count;\n\n    std::string binary = \"\";\n    if (avg == 0) {\n        return \"0\";\n    }\n    while (avg > 0) {\n        binary = std::to_string(avg % 2) + binary;\n        avg /= 2;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nYou are given two positive integers n and m, and your task is to compute the\naverage of the integers from n through m (including n and m). \nRound the answer to the nearest integer(smaller one) and convert that to binary.\nIf n is greater than m, return \"-1\".\nExample:\nrounded_avg_simd(1, 5) => \"11\"\nrounded_avg_simd(7, 5) => \"-1\"\nrounded_avg_simd(10, 20) => \"1111\"\nrounded_avg_simd(20, 33) => \"11010\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    if (count < 8) {\n        for (int64_t i = n; i <= m; ++i) {\n            sum += i;\n        }\n    } else {\n        int64_t i = n;\n        for (; i <= n + (count % 8) -1; ++i)\n        {\n            sum += i;\n        }\n        \n        __m256i vec_sum = _mm256_setzero_si256();\n        \n        for (; i <= m - 7; i += 8) {\n            __m256i vec = _mm256_set_epi64x(i + 7, i + 6, i + 5, i + 4);\n            __m256i vec2 = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n            vec_sum = _mm256_add_epi64(vec_sum, vec);\n            vec_sum = _mm256_add_epi64(vec_sum, vec2);\n        }\n\n        int64_t temp_sum[4];\n        _mm256_storeu_si256((__m256i*)temp_sum, vec_sum);\n        sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n        \n        for (; i <= m; ++i) {\n            sum += i;\n        }\n    }\n\n    int64_t avg = sum / count;\n\n    std::string binary = \"\";\n    if (avg == 0) {\n        return \"0\";\n    }\n    while (avg > 0) {\n        binary = ((avg % 2 == 0) ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nYou are given two positive integers n and m, and your task is to compute the\naverage of the integers from n through m (including n and m). \nRound the answer to the nearest integer(smaller one) and convert that to binary.\nIf n is greater than m, return \"-1\".\nExample:\nrounded_avg_simd(1, 5) => \"11\"\nrounded_avg_simd(7, 5) => \"-1\"\nrounded_avg_simd(10, 20) => \"1111\"\nrounded_avg_simd(20, 33) => \"11010\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    if (count < 8) {\n        for (int64_t i = n; i <= m; ++i) {\n            sum += i;\n        }\n    } else {\n        int64_t i = n;\n        for (; i <= m - 7; i += 8) {\n            __m256i vec = _mm256_set_epi64x(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n            \n            int64_t temp_sum[4];\n            _mm256_storeu_si256((__m256i*)temp_sum, vec);\n            sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n            sum += temp_sum[0+4] + temp_sum[1+4] + temp_sum[2+4] + temp_sum[3+4];\n        }\n        for (; i <= m; ++i) {\n            sum += i;\n        }\n    }\n    \n    int64_t avg = sum / count;\n\n    std::string binary = \"\";\n    if (avg == 0) {\n        return \"0\";\n    }\n    while (avg > 0) {\n        binary = ((avg % 2 == 0) ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nYou are given two positive integers n and m, and your task is to compute the\naverage of the integers from n through m (including n and m). \nRound the answer to the nearest integer(smaller one) and convert that to binary.\nIf n is greater than m, return \"-1\".\nExample:\nrounded_avg_simd(1, 5) => \"11\"\nrounded_avg_simd(7, 5) => \"-1\"\nrounded_avg_simd(10, 20) => \"1111\"\nrounded_avg_simd(20, 33) => \"11010\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    if (count < 8) {\n        for (int64_t i = n; i <= m; ++i) {\n            sum += i;\n        }\n    } else {\n        int64_t i = n;\n        for (; i <= n + (8 - (n % 8)) % 8 - 1 && i <= m; ++i) {\n            sum += i;\n        }\n\n        __m256i vec_sum = _mm256_setzero_si256();\n        for (; i <= m - 7; i += 8) {\n            __m256i vec = _mm256_set_epi64x(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n            vec_sum = _mm256_add_epi64(vec_sum, vec);\n        }\n\n        int64_t temp_sum[4];\n        _mm256_storeu_si256((__m256i*)temp_sum, vec_sum);\n        sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n        for (; i <= m; ++i) {\n            sum += i;\n        }\n    }\n\n    int64_t avg = sum / count;\n\n    std::string binary = \"\";\n    if (avg == 0) {\n        return \"0\";\n    }\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\n/*\nYou are given two positive integers n and m, and your task is to compute the\naverage of the integers from n through m (including n and m). \nRound the answer to the nearest integer(smaller one) and convert that to binary.\nIf n is greater than m, return \"-1\".\nExample:\nrounded_avg_simd(1, 5) => \"11\"\nrounded_avg_simd(7, 5) => \"-1\"\nrounded_avg_simd(10, 20) => \"1111\"\nrounded_avg_simd(20, 33) => \"11010\"\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n\n    if (count < 8) {\n        for (int64_t i = n; i <= m; ++i) {\n            sum += i;\n        }\n    } else {\n        int64_t i = n;\n        for (; i <= m - 7; i += 8) {\n            __m256i vec = _mm256_set_epi64x(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n            \n            int64_t temp_sum[4];\n            _mm256_storeu_si256((__m256i*)temp_sum, vec);\n\n            sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n            sum += temp_sum[0+4] + temp_sum[1+4] + temp_sum[2+4] + temp_sum[3+4];\n        }\n        for (; i <= m; ++i) {\n            sum += i;\n        }\n    }\n    \n    int64_t avg = sum / count;\n\n    std::string binary = \"\";\n    if (avg == 0) {\n        return \"0\";\n    }\n    while (avg > 0) {\n        binary = ((avg % 2 == 0) ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i even_indices, odd_indices, factorial_vec, sum_vec, mod_vec;\n    int factorial_arr[8], sum_arr[8];\n\n    mod_vec = _mm256_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 8) {\n        int count = std::min(8, n - i);\n\n        int even_arr[8], odd_arr[8];\n        for (int j = 0; j < 8; ++j) {\n            even_arr[j] = 0;\n            odd_arr[j] = 0;\n            factorial_arr[j] = 1;\n            sum_arr[j] = 0;\n        }\n\n        for (int j = 0; j < count; ++j) {\n            int index = i + j + 1; \n            if (index % 2 == 0) {\n                int factorial = 1;\n                for (int k = 1; k <= index; ++k) {\n                    factorial = (factorial * k) % 10000;\n                }\n                even_arr[j] = factorial;\n            } else {\n                int sum = 0;\n                for (int k = 1; k <= index; ++k) {\n                    sum += k;\n                }\n                odd_arr[j] = sum;\n            }\n        }\n\n        even_indices = _mm256_loadu_si256((__m256i*)even_arr);\n        odd_indices = _mm256_loadu_si256((__m256i*)odd_arr);\n\n        for (int j = 0; j < count; ++j) {\n            int index = i + j + 1;\n            if (index % 2 == 0) {\n                result[i + j] = even_arr[j];\n            } else {\n                result[i + j] = odd_arr[j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    int i = 0;\n    if (n >= 1) result[0] = 1;\n    for (i = 1; i < n; ++i) {\n        if ((i + 1) % 2 == 0) {\n            long long factorial = 1;\n            for (int j = 1; j <= (i + 1); ++j) {\n                factorial = (factorial * j) % 10000;\n            }\n            result[i] = (int)factorial;\n        } else {\n            result[i] = (i + 2) * (i + 1) / 2;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i even_indices, odd_indices;\n    __m256i factorial_vec, sum_vec;\n    __m256i mod_val = _mm256_set1_epi32(10000);\n    __m256i one = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        even_indices = _mm256_set_epi32(i + 7 + 1, i + 5 + 1, i + 3 + 1, i + 1 + 1, i + 6 + 1, i + 4 + 1, i + 2 + 1, i + 0 + 1);\n        odd_indices = _mm256_set_epi32(i + 7 + 1, i + 5 + 1, i + 3 + 1, i + 1 + 1, i + 6 + 1, i + 4 + 1, i + 2 + 1, i + 0 + 1);\n\n        __m256i even_mask = _mm256_set_epi32(0, 0, 0, 0, -1, 0, -1, 0);\n        __m256i odd_mask = _mm256_set_epi32(-1, -1, -1, -1, 0, -1, 0, -1);\n\n        factorial_vec = _mm256_set1_epi32(1);\n        __m256i factorial_temp = _mm256_set1_epi32(1);\n        __m256i indices = _mm256_set_epi32(i + 7 + 1, i + 6 + 1, i + 5 + 1, i + 4 + 1, i + 3 + 1, i + 2 + 1, i + 1 + 1, i + 0 + 1);\n\n        for (int j = 1; j <= i + 8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            factorial_temp = _mm256_mullo_epi32(factorial_temp, j_vec);\n            factorial_temp = _mm256_and_si256(factorial_temp, even_mask);\n            factorial_vec = _mm256_blendv_epi8(factorial_vec, factorial_temp, even_mask);\n            factorial_vec = _mm256_mod_epu32(factorial_vec, mod_val);\n        }\n\n        sum_vec = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, one));\n        sum_vec = _mm256_srli_epi32(sum_vec, 1);\n        sum_vec = _mm256_and_si256(sum_vec, odd_mask);\n\n        __m256i result_vec = _mm256_blendv_epi8(sum_vec, factorial_vec, even_mask);\n\n        int* res_ptr = (int*)&result_vec;\n        for (int j = 0; j < 8; ++j) {\n            result[i + j] = res_ptr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if ((i + 1) % 2 == 0) {\n            long long factorial = 1;\n            for (int j = 1; j <= i + 1; ++j) {\n                factorial = (factorial * j) % 10000;\n            }\n            result[i] = (int)factorial;\n        } else {\n            result[i] = (1 + i + 1) * (i + 1) / 2;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i even_indices, odd_indices, factorial_vec, sum_vec, mod_vec;\n    int factorial_arr[8], sum_arr[8];\n\n    mod_vec = _mm256_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 8) {\n        int count = std::min(8, n - i);\n\n        int even_arr[8], odd_arr[8];\n        for (int j = 0; j < 8; ++j) {\n            even_arr[j] = 0;\n            odd_arr[j] = 0;\n            factorial_arr[j] = 1;\n            sum_arr[j] = 0;\n        }\n\n        for (int j = 0; j < count; ++j) {\n            int index = i + j + 1; \n            if (index % 2 == 0) {\n                int fact = 1;\n                for (int k = 1; k <= index; ++k) {\n                    fact = (fact * k) % 10000;\n                }\n                even_arr[j] = fact;\n            } else {\n                int sum = 0;\n                for (int k = 1; k <= index; ++k) {\n                    sum += k;\n                }\n                odd_arr[j] = sum;\n            }\n        }\n\n        for (int j = 0; j < count; ++j) {\n            int index = i + j + 1;\n            if (index % 2 == 0) {\n                result[i + j] = even_arr[j];\n            } else {\n                result[i + j] = odd_arr[j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i even_indices, odd_indices, factorial_vec, sum_vec, mod_vec;\n    int factorial_arr[8], sum_arr[8];\n    int mod_val = 10000;\n\n    mod_vec = _mm256_set1_epi32(mod_val);\n\n    for (int i = 0; i < n; i += 8) {\n        int count = std::min(8, n - i);\n\n        int even_start = i + 1;\n        if (even_start % 2 != 0) even_start++;\n\n        int odd_start = i + 1;\n        if (odd_start % 2 == 0) odd_start++;\n\n        even_indices = _mm256_set_epi32(even_start + 6, even_start + 4, even_start + 2, even_start, even_start + 6, even_start + 4, even_start + 2, even_start);\n        odd_indices = _mm256_set_epi32(odd_start + 6, odd_start + 4, odd_start + 2, odd_start, odd_start + 6, odd_start + 4, odd_start + 2, odd_start);\n\n        factorial_vec = _mm256_set1_epi32(1);\n        sum_vec = _mm256_setzero_si256();\n\n        int factorial_init[8];\n        for (int j = 0; j < 8; ++j) factorial_init[j] = 1;\n\n        int sum_init[8];\n        for (int j = 0; j < 8; ++j) sum_init[j] = 0;\n\n        for (int j = 0; j < 8; ++j) {\n            factorial_arr[j] = 1;\n            sum_arr[j] = 0;\n        }\n\n        for (int j = 1; j <= i + count; ++j) {\n            if (j % 2 == 0 && j >= i + 1 && j <= i + count) {\n                int index = j - (i + 1);\n                factorial_arr[index] = (factorial_arr[index] * j) % mod_val;\n            } else if (j % 2 != 0 && j >= i + 1 && j <= i + count) {\n                int index = j - (i + 1);\n                sum_arr[index] += j;\n            }\n        }\n\n        for (int j = 0; j < count; ++j) {\n            if ((i + 1 + j) % 2 == 0) {\n                result[i + j] = factorial_arr[j];\n            } else {\n                result[i + j] = sum_arr[j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a positive integer n, return a vector that has the number of even and odd\ninteger palindromes that fall within the range(1, n), inclusive.\n\nExample 1:\n\n    Input: 3\n    Output: (1, 2)\n    Explanation:\n    Integer palindrome are 1, 2, 3. one of them is even, and two of them are odd.\n\nExample 2:\n\n    Input: 12\n    Output: (4, 6)\n    Explanation:\n    Integer palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9, 11. four of them are even, and 6 of them are odd.\n\nNote:\n    1. 1 <= n <= 2^10\n    2. returned vector has the number of even and odd integer palindromes respectively.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        std::string s = std::to_string(i);\n        std::string rs = s;\n        std::reverse(rs.begin(), rs.end());\n        if (s == rs) {\n            if (i % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a positive integer n, return a vector that has the number of even and odd\ninteger palindromes that fall within the range(1, n), inclusive.\n\nExample 1:\n\n    Input: 3\n    Output: (1, 2)\n    Explanation:\n    Integer palindrome are 1, 2, 3. one of them is even, and two of them are odd.\n\nExample 2:\n\n    Input: 12\n    Output: (4, 6)\n    Explanation:\n    Integer palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9, 11. four of them are even, and 6 of them are odd.\n\nNote:\n    1. 1 <= n <= 2^10\n    2. returned vector has the number of even and odd integer palindromes respectively.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        std::string s = std::to_string(i);\n        std::string reversed_s = s;\n        std::reverse(reversed_s.begin(), reversed_s.end());\n\n        if (s == reversed_s) {\n            if (i % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a positive integer n, return a vector that has the number of even and odd\ninteger palindromes that fall within the range(1, n), inclusive.\n\nExample 1:\n\n    Input: 3\n    Output: (1, 2)\n    Explanation:\n    Integer palindrome are 1, 2, 3. one of them is even, and two of them are odd.\n\nExample 2:\n\n    Input: 12\n    Output: (4, 6)\n    Explanation:\n    Integer palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9, 11. four of them are even, and 6 of them are odd.\n\nNote:\n    1. 1 <= n <= 2^10\n    2. returned vector has the number of even and odd integer palindromes respectively.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        std::string s = std::to_string(i);\n        std::string reversed_s = s;\n        std::reverse(reversed_s.begin(), reversed_s.end());\n\n        if (s == reversed_s) {\n            if (i % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a positive integer n, return a vector that has the number of even and odd\ninteger palindromes that fall within the range(1, n), inclusive.\n\nExample 1:\n\n    Input: 3\n    Output: (1, 2)\n    Explanation:\n    Integer palindrome are 1, 2, 3. one of them is even, and two of them are odd.\n\nExample 2:\n\n    Input: 12\n    Output: (4, 6)\n    Explanation:\n    Integer palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9, 11. four of them are even, and 6 of them are odd.\n\nNote:\n    1. 1 <= n <= 2^10\n    2. returned vector has the number of even and odd integer palindromes respectively.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        std::string s = std::to_string(i);\n        std::string rs = s;\n        std::reverse(rs.begin(), rs.end());\n        if (s == rs) {\n            if (i % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a positive integer n, return a vector that has the number of even and odd\ninteger palindromes that fall within the range(1, n), inclusive.\n\nExample 1:\n\n    Input: 3\n    Output: (1, 2)\n    Explanation:\n    Integer palindrome are 1, 2, 3. one of them is even, and two of them are odd.\n\nExample 2:\n\n    Input: 12\n    Output: (4, 6)\n    Explanation:\n    Integer palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9, 11. four of them are even, and 6 of them are odd.\n\nNote:\n    1. 1 <= n <= 2^10\n    2. returned vector has the number of even and odd integer palindromes respectively.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even_count = 0;\n    int odd_count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        std::string s = std::to_string(i);\n        std::string rs = s;\n        std::reverse(rs.begin(), rs.end());\n        if (s == rs) {\n            if (i % 2 == 0) {\n                even_count++;\n            } else {\n                odd_count++;\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function count_nums which takes a vector of integers and returns\nthe number of elements which has a sum of digits > 0.\nIf a number is negative, then its first signed digit will be negative:\ne.g. -123 has signed digits -1, 2, and 3.\n>>> count_nums_simd({}) == 0\n>>> count_nums_simd({-1, 11, -11}) == 1\n>>> count_nums_simd({1, 1, 2}) == 3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    if (size == 0) return 0;\n\n    for (i = 0; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n        \n        if (num < 0) {\n            sum -= temp % 10;\n            temp /= 10;\n        }\n        \n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        \n        if (num >= 0) {\n            temp = num;\n            while (temp >= 10) {\n                temp /= 10;\n            }\n        }\n        \n        if (num < 0) {\n            temp = abs(num);\n            int num_digits = 0;\n            int temp2 = temp;\n            while (temp2 > 0) {\n                temp2 /= 10;\n                num_digits++;\n            }\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function count_nums which takes a vector of integers and returns\nthe number of elements which has a sum of digits > 0.\nIf a number is negative, then its first signed digit will be negative:\ne.g. -123 has signed digits -1, 2, and 3.\n>>> count_nums_simd({}) == 0\n>>> count_nums_simd({-1, 11, -11}) == 1\n>>> count_nums_simd({1, 1, 2}) == 3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    if (size == 0) return 0;\n\n    for (i = 0; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n        \n        if (num < 0) {\n            sum -= temp % 10;\n            temp /= 10;\n        }\n        \n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        \n        if (num >= 0) {\n            temp = num;\n            int pow10 = 1;\n            while(temp >= 10){\n                temp /= 10;\n                pow10 *= 10;\n            }\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function count_nums which takes a vector of integers and returns\nthe number of elements which has a sum of digits > 0.\nIf a number is negative, then its first signed digit will be negative:\ne.g. -123 has signed digits -1, 2, and 3.\n>>> count_nums_simd({}) == 0\n>>> count_nums_simd({-1, 11, -11}) == 1\n>>> count_nums_simd({1, 1, 2}) == 3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    if (size == 0) return 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        __m256i temp_vec = vec;\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        __m256i digits1 = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n        sum_vec = _mm256_add_epi32(sum_vec, digits1);\n        abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n\n        __m256i digits2 = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n        sum_vec = _mm256_add_epi32(sum_vec, digits2);\n        abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n\n        __m256i digits3 = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n        sum_vec = _mm256_add_epi32(sum_vec, digits3);\n        abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n\n        __m256i digits4 = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n        sum_vec = _mm256_add_epi32(sum_vec, digits4);\n\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i first_digit = _mm256_and_si256(digits1, sign_mask);\n        sum_vec = _mm256_sub_epi32(sum_vec, _mm256_and_si256(sign_mask, digits1));\n\n        int sum_array[8];\n        _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n\n        for (int j = 0; j < 8; j++) {\n            if (sum_array[j] > 0) {\n                count++;\n            }\n        }\n    }\n\n    for (; i < size; i++) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n\n        sum += temp % 10;\n        temp /= 10;\n        sum += temp % 10;\n        temp /= 10;\n        sum += temp % 10;\n        temp /= 10;\n        sum += temp % 10;\n\n        if (num < 0) {\n            sum -= abs(n[i]) % 10;\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function count_nums which takes a vector of integers and returns\nthe number of elements which has a sum of digits > 0.\nIf a number is negative, then its first signed digit will be negative:\ne.g. -123 has signed digits -1, 2, and 3.\n>>> count_nums_simd({}) == 0\n>>> count_nums_simd({-1, 11, -11}) == 1\n>>> count_nums_simd({1, 1, 2}) == 3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    if (size == 0) return 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int num = ((int*)&vec)[j];\n            int sum = 0;\n            int temp = abs(num);\n\n            if (num < 0) {\n                sum += (num < 0) ? (num % 10) : (num % 10);\n                temp /= 10;\n            }\n            \n            while (temp > 0) {\n                sum += temp % 10;\n                temp /= 10;\n            }\n            ((int*)&sum_vec)[j] = sum;\n        }\n\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp_vec = _mm256_cmpgt_epi32(sum_vec, zero_vec);\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if ((mask & (1 << (j * 4))) != 0) {\n                count++;\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n\n        if (num < 0) {\n            sum += (num < 0) ? (num % 10) : (num % 10);\n            temp /= 10;\n        }\n\n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nWrite a function count_nums which takes a vector of integers and returns\nthe number of elements which has a sum of digits > 0.\nIf a number is negative, then its first signed digit will be negative:\ne.g. -123 has signed digits -1, 2, and 3.\n>>> count_nums_simd({}) == 0\n>>> count_nums_simd({-1, 11, -11}) == 1\n>>> count_nums_simd({1, 1, 2}) == 3\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    if (size == 0) return 0;\n\n    for (i = 0; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n        \n        if (num < 0) {\n            sum -= temp % 10;\n            temp /= 10;\n        }\n        \n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        \n        if (num >= 0) {\n            temp = num;\n            int pow10 = 1;\n            while (temp >= 10) {\n                temp /= 10;\n                pow10 *= 10;\n            }\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    for (int shift = 0; shift < n; ++shift) {\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        bool sorted = true;\n        for (int i = 0; i < n - 1; ++i) {\n            if (shifted_arr[i] > shifted_arr[i + 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    for (int shift = 0; shift < n; ++shift) {\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        bool sorted = true;\n        for (int i = 1; i < n; ++i) {\n            if (shifted_arr[i] < shifted_arr[i - 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    for (int shift = 0; shift < n; ++shift) {\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        bool sorted = true;\n        for (int i = 1; i < n; ++i) {\n            if (shifted_arr[i] < shifted_arr[i - 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    for (int shift = 0; shift < n; ++shift) {\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        bool sorted = true;\n        for (int i = 1; i < n; ++i) {\n            if (shifted_arr[i] < shifted_arr[i - 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return true;\n    }\n\n    int n = arr.size();\n    for (int shift = 0; shift < n; ++shift) {\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        bool sorted = true;\n        for (int i = 1; i < n; ++i) {\n            if (shifted_arr[i] < shifted_arr[i - 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nIn this problem, you will implement a function that takes two vectors of numbers,\nand determines whether it is possible to perform an exchange of elements\nbetween them to make lst1 a vector of only even numbers.\nThere is no limit on the number of exchanged elements between lst1 and lst2.\nIf it is possible to exchange elements between the lst1 and lst2 to make\nall the elements of lst1 to be even, return \"YES\".\nOtherwise, return \"NO\".\nFor example:\nexchange_simd({1, 2, 3, 4}, {1, 2, 3, 4}) => \"YES\"\nexchange_simd({1, 2, 3, 4}, {1, 5, 3, 4}) => \"NO\"\nIt is assumed that the input vectors will be non-empty.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count1 = 0;\n    int even_count2 = 0;\n\n    for (int i = 0; i < lst1.size(); ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n    }\n\n    for (int i = 0; i < lst2.size(); ++i) {\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    if (odd_count1 <= even_count2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nIn this problem, you will implement a function that takes two vectors of numbers,\nand determines whether it is possible to perform an exchange of elements\nbetween them to make lst1 a vector of only even numbers.\nThere is no limit on the number of exchanged elements between lst1 and lst2.\nIf it is possible to exchange elements between the lst1 and lst2 to make\nall the elements of lst1 to be even, return \"YES\".\nOtherwise, return \"NO\".\nFor example:\nexchange_simd({1, 2, 3, 4}, {1, 2, 3, 4}) => \"YES\"\nexchange_simd({1, 2, 3, 4}, {1, 5, 3, 4}) => \"NO\"\nIt is assumed that the input vectors will be non-empty.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count1 = 0;\n    int even_count2 = 0;\n\n    for (int i = 0; i < lst1.size(); ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n    }\n\n    for (int i = 0; i < lst2.size(); ++i) {\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    if (odd_count1 <= even_count2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nIn this problem, you will implement a function that takes two vectors of numbers,\nand determines whether it is possible to perform an exchange of elements\nbetween them to make lst1 a vector of only even numbers.\nThere is no limit on the number of exchanged elements between lst1 and lst2.\nIf it is possible to exchange elements between the lst1 and lst2 to make\nall the elements of lst1 to be even, return \"YES\".\nOtherwise, return \"NO\".\nFor example:\nexchange_simd({1, 2, 3, 4}, {1, 2, 3, 4}) => \"YES\"\nexchange_simd({1, 2, 3, 4}, {1, 5, 3, 4}) => \"NO\"\nIt is assumed that the input vectors will be non-empty.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count1 = 0;\n    int even_count2 = 0;\n\n    for (int i = 0; i < lst1.size(); ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n    }\n\n    for (int i = 0; i < lst2.size(); ++i) {\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    if (odd_count1 <= even_count2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nIn this problem, you will implement a function that takes two vectors of numbers,\nand determines whether it is possible to perform an exchange of elements\nbetween them to make lst1 a vector of only even numbers.\nThere is no limit on the number of exchanged elements between lst1 and lst2.\nIf it is possible to exchange elements between the lst1 and lst2 to make\nall the elements of lst1 to be even, return \"YES\".\nOtherwise, return \"NO\".\nFor example:\nexchange_simd({1, 2, 3, 4}, {1, 2, 3, 4}) => \"YES\"\nexchange_simd({1, 2, 3, 4}, {1, 5, 3, 4}) => \"NO\"\nIt is assumed that the input vectors will be non-empty.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count1 = 0;\n    int even_count2 = 0;\n\n    for (int i = 0; i < lst1.size(); ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n    }\n\n    for (int i = 0; i < lst2.size(); ++i) {\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    if (odd_count1 <= even_count2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nIn this problem, you will implement a function that takes two vectors of numbers,\nand determines whether it is possible to perform an exchange of elements\nbetween them to make lst1 a vector of only even numbers.\nThere is no limit on the number of exchanged elements between lst1 and lst2.\nIf it is possible to exchange elements between the lst1 and lst2 to make\nall the elements of lst1 to be even, return \"YES\".\nOtherwise, return \"NO\".\nFor example:\nexchange_simd({1, 2, 3, 4}, {1, 2, 3, 4}) => \"YES\"\nexchange_simd({1, 2, 3, 4}, {1, 5, 3, 4}) => \"NO\"\nIt is assumed that the input vectors will be non-empty.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count1 = 0;\n    int even_count2 = 0;\n\n    for (int i = 0; i < lst1.size(); ++i) {\n        if (lst1[i] % 2 != 0) {\n            odd_count1++;\n        }\n    }\n\n    for (int i = 0; i < lst2.size(); ++i) {\n        if (lst2[i] % 2 == 0) {\n            even_count2++;\n        }\n    }\n\n    if (odd_count1 <= even_count2) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        int len = str.length();\n        int i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i vec = _mm256_loadu_si256((const __m256i*)(str.data() + i));\n                __m256i mask = _mm256_set1_epi8(0x01);\n                __m256i and_result = _mm256_and_si256(vec, mask);\n                __m256i cmp_result = _mm256_cmpeq_epi8(and_result, mask);\n                unsigned int movemask = _mm256_movemask_epi8(cmp_result);\n\n                count += _mm_popcnt_u32(movemask);\n            }\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        std::string new_str = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        int len = str.length();\n        int i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i chars = _mm256_loadu_si256((const __m256i*)(str.data() + i));\n                __m256i offsets = _mm256_set1_epi8('0');\n                __m256i digits = _mm256_sub_epi8(chars, offsets);\n                __m256i odd_mask = _mm256_set1_epi8(1);\n                __m256i and_result = _mm256_and_si256(digits, odd_mask);\n                __m256i compare_result = _mm256_cmpeq_epi8(and_result, odd_mask);\n                unsigned int mask = _mm256_movemask_epi8(compare_result);\n\n                count += _mm_popcnt_u32(mask);\n            }\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        result.push_back(\"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        int len = str.length();\n        int i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i vec = _mm256_loadu_si256((const __m256i*)(str.data() + i));\n                __m256i mask = _mm256_set1_epi8(0x01);\n                __m256i and_result = _mm256_and_si256(vec, mask);\n                __m256i cmp_result = _mm256_cmpeq_epi8(and_result, mask);\n                unsigned int movemask = _mm256_movemask_epi8(cmp_result);\n\n                count += _mm_popcnt_u32(movemask);\n            }\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        std::string new_str = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        int len = str.length();\n        int i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i vec = _mm256_loadu_si256((const __m256i*)(str.data() + i));\n                __m256i mask = _mm256_set1_epi8(0x01);\n                __m256i and_result = _mm256_and_si256(vec, mask);\n                __m256i cmp_result = _mm256_cmpeq_epi8(and_result, mask);\n                unsigned int movemask = _mm256_movemask_epi8(cmp_result);\n\n                count += _mm_popcnt_u32(movemask);\n            }\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        std::string new_str = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        int len = str.length();\n        int i = 0;\n\n        if (len >= 32) {\n            for (; i <= len - 32; i += 32) {\n                __m256i vec = _mm256_loadu_si256((const __m256i*)(str.data() + i));\n                __m256i mask = _mm256_set1_epi8(0x01);\n                __m256i and_result = _mm256_and_si256(vec, mask);\n                __m256i cmp_result = _mm256_cmpeq_epi8(and_result, mask);\n                unsigned int movemask = _mm256_movemask_epi8(cmp_result);\n\n                count += _mm_popcnt_u32(movemask);\n            }\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        std::string new_str = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of integers nums, find the minimum sum of any non-empty sub-vector\nof nums.\nExample\nminSubArraySum_simd({2, 3, 4, 1, 2, 4}) == 1\nminSubArraySum_simd({-1, -2, -3}) == -6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) {\n        return 0;\n    }\n\n    int64_t min_so_far = nums[0];\n    int64_t current_min = nums[0];\n\n    for (size_t i = 1; i < nums.size(); ++i) {\n        current_min = std::min(nums[i], current_min + nums[i]);\n        min_so_far = std::min(min_so_far, current_min);\n    }\n\n    return min_so_far;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of integers nums, find the minimum sum of any non-empty sub-vector\nof nums.\nExample\nminSubArraySum_simd({2, 3, 4, 1, 2, 4}) == 1\nminSubArraySum_simd({-1, -2, -3}) == -6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) {\n        return 0;\n    }\n\n    int64_t min_so_far = nums[0];\n    int64_t current_min = nums[0];\n\n    for (size_t i = 1; i < nums.size(); ++i) {\n        current_min = std::min(nums[i], current_min + nums[i]);\n        min_so_far = std::min(min_so_far, current_min);\n    }\n\n    return min_so_far;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of integers nums, find the minimum sum of any non-empty sub-vector\nof nums.\nExample\nminSubArraySum_simd({2, 3, 4, 1, 2, 4}) == 1\nminSubArraySum_simd({-1, -2, -3}) == -6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) {\n        return 0;\n    }\n\n    int64_t min_so_far = nums[0];\n    int64_t current_min = nums[0];\n\n    for (size_t i = 1; i < nums.size(); ++i) {\n        current_min = std::min(nums[i], current_min + nums[i]);\n        min_so_far = std::min(min_so_far, current_min);\n    }\n\n    return min_so_far;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of integers nums, find the minimum sum of any non-empty sub-vector\nof nums.\nExample\nminSubArraySum_simd({2, 3, 4, 1, 2, 4}) == 1\nminSubArraySum_simd({-1, -2, -3}) == -6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) {\n        return 0;\n    }\n\n    int64_t min_so_far = nums[0];\n    int64_t current_min = nums[0];\n\n    for (size_t i = 1; i < nums.size(); ++i) {\n        current_min = std::min(nums[i], current_min + nums[i]);\n        min_so_far = std::min(min_so_far, current_min);\n    }\n\n    return min_so_far;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of integers nums, find the minimum sum of any non-empty sub-vector\nof nums.\nExample\nminSubArraySum_simd({2, 3, 4, 1, 2, 4}) == 1\nminSubArraySum_simd({-1, -2, -3}) == -6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) {\n        return 0;\n    }\n\n    int64_t min_so_far = nums[0];\n    int64_t current_min = nums[0];\n\n    for (size_t i = 1; i < nums.size(); ++i) {\n        current_min = std::min(nums[i], current_min + nums[i]);\n        min_so_far = std::min(min_so_far, current_min);\n    }\n\n    return min_so_far;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a rectangular grid of wells. Each row represents a single well,\nand each 1 in a row represents a single unit of water.\nEach well has a corresponding bucket that can be used to extract water from it, \nand all buckets have the same capacity.\nYour task is to use the buckets to empty the wells.\nOutput the number of times you need to lower the buckets.\n\nExample 1:\n    Input: \n        grid : {{0,0,1,0}, {0,1,0,0}, {1,1,1,1}}\n        bucket_capacity : 1\n    Output: 6\n\nExample 2:\n    Input: \n        grid : {{0,0,1,1}, {0,0,0,0}, {1,1,1,1}, {0,1,1,1}}\n        bucket_capacity : 2\n    Output: 5\n\nExample 3:\n    Input: \n        grid : {{0,0,0}, {0,0,0}}\n        bucket_capacity : 5\n    Output: 0\n\nConstraints:\n    * all wells have the same length\n    * 1 <= grid.length <= 10^5\n    * 1 <= grid{:,1}.length <= 10^5\n    * grid{i}{j} -> 0 | 1\n    * 1 <= capacity <= 10\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int count = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n\n    for (int i = 0; i < rows; ++i) {\n        int row_sum = 0;\n        for (int j = 0; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n        count += (row_sum + capacity - 1) / capacity;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a rectangular grid of wells. Each row represents a single well,\nand each 1 in a row represents a single unit of water.\nEach well has a corresponding bucket that can be used to extract water from it, \nand all buckets have the same capacity.\nYour task is to use the buckets to empty the wells.\nOutput the number of times you need to lower the buckets.\n\nExample 1:\n    Input: \n        grid : {{0,0,1,0}, {0,1,0,0}, {1,1,1,1}}\n        bucket_capacity : 1\n    Output: 6\n\nExample 2:\n    Input: \n        grid : {{0,0,1,1}, {0,0,0,0}, {1,1,1,1}, {0,1,1,1}}\n        bucket_capacity : 2\n    Output: 5\n\nExample 3:\n    Input: \n        grid : {{0,0,0}, {0,0,0}}\n        bucket_capacity : 5\n    Output: 0\n\nConstraints:\n    * all wells have the same length\n    * 1 <= grid.length <= 10^5\n    * 1 <= grid{:,1}.length <= 10^5\n    * grid{i}{j} -> 0 | 1\n    * 1 <= capacity <= 10\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int count = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n\n    for (int i = 0; i < rows; ++i) {\n        int row_sum = 0;\n        for (int j = 0; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n        count += (row_sum + capacity - 1) / capacity;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a rectangular grid of wells. Each row represents a single well,\nand each 1 in a row represents a single unit of water.\nEach well has a corresponding bucket that can be used to extract water from it, \nand all buckets have the same capacity.\nYour task is to use the buckets to empty the wells.\nOutput the number of times you need to lower the buckets.\n\nExample 1:\n    Input: \n        grid : {{0,0,1,0}, {0,1,0,0}, {1,1,1,1}}\n        bucket_capacity : 1\n    Output: 6\n\nExample 2:\n    Input: \n        grid : {{0,0,1,1}, {0,0,0,0}, {1,1,1,1}, {0,1,1,1}}\n        bucket_capacity : 2\n    Output: 5\n\nExample 3:\n    Input: \n        grid : {{0,0,0}, {0,0,0}}\n        bucket_capacity : 5\n    Output: 0\n\nConstraints:\n    * all wells have the same length\n    * 1 <= grid.length <= 10^5\n    * 1 <= grid{:,1}.length <= 10^5\n    * grid{i}{j} -> 0 | 1\n    * 1 <= capacity <= 10\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int count = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n\n    for (int i = 0; i < rows; ++i) {\n        int row_sum = 0;\n        for (int j = 0; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n        count += (row_sum + capacity - 1) / capacity;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a rectangular grid of wells. Each row represents a single well,\nand each 1 in a row represents a single unit of water.\nEach well has a corresponding bucket that can be used to extract water from it, \nand all buckets have the same capacity.\nYour task is to use the buckets to empty the wells.\nOutput the number of times you need to lower the buckets.\n\nExample 1:\n    Input: \n        grid : {{0,0,1,0}, {0,1,0,0}, {1,1,1,1}}\n        bucket_capacity : 1\n    Output: 6\n\nExample 2:\n    Input: \n        grid : {{0,0,1,1}, {0,0,0,0}, {1,1,1,1}, {0,1,1,1}}\n        bucket_capacity : 2\n    Output: 5\n\nExample 3:\n    Input: \n        grid : {{0,0,0}, {0,0,0}}\n        bucket_capacity : 5\n    Output: 0\n\nConstraints:\n    * all wells have the same length\n    * 1 <= grid.length <= 10^5\n    * 1 <= grid{:,1}.length <= 10^5\n    * grid{i}{j} -> 0 | 1\n    * 1 <= capacity <= 10\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int count = 0;\n    int rows = grid.size();\n    if (rows == 0) return 0;\n    int cols = grid[0].size();\n\n    for (int i = 0; i < rows; ++i) {\n        int row_sum = 0;\n        for (int j = 0; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n        count += (row_sum + capacity - 1) / capacity;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a rectangular grid of wells. Each row represents a single well,\nand each 1 in a row represents a single unit of water.\nEach well has a corresponding bucket that can be used to extract water from it, \nand all buckets have the same capacity.\nYour task is to use the buckets to empty the wells.\nOutput the number of times you need to lower the buckets.\n\nExample 1:\n    Input: \n        grid : {{0,0,1,0}, {0,1,0,0}, {1,1,1,1}}\n        bucket_capacity : 1\n    Output: 6\n\nExample 2:\n    Input: \n        grid : {{0,0,1,1}, {0,0,0,0}, {1,1,1,1}, {0,1,1,1}}\n        bucket_capacity : 2\n    Output: 5\n\nExample 3:\n    Input: \n        grid : {{0,0,0}, {0,0,0}}\n        bucket_capacity : 5\n    Output: 0\n\nConstraints:\n    * all wells have the same length\n    * 1 <= grid.length <= 10^5\n    * 1 <= grid{:,1}.length <= 10^5\n    * grid{i}{j} -> 0 | 1\n    * 1 <= capacity <= 10\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint max_fill_simd(const std::vector<std::vector<int> > & grid,int capacity){\n    int count = 0;\n    for(const auto& row : grid){\n        int row_sum = 0;\n        int i = 0;\n        int size = row.size();\n\n        if (size >= 8) {\n            int aligned_size = size - (size % 8);\n            for (; i < aligned_size; i += 8) {\n                __m256i vec = _mm256_loadu_si256((const __m256i*)&row[i]);\n                __m256i sum_vec = _mm256_cvtepi8_epi32(vec);\n                __m128i low = _mm256_castsi256_si128(sum_vec);\n                __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n                __m128i final_sum = _mm_add_epi32(low, high);\n                __m128i shifted = _mm_shuffle_epi32(final_sum, _MM_SHUFFLE(1, 0, 3, 2));\n                final_sum = _mm_add_epi32(final_sum, shifted);\n                shifted = _mm_shuffle_epi32(final_sum, _MM_SHUFFLE(0, 0, 0, 1));\n                final_sum = _mm_add_epi32(final_sum, shifted);\n                row_sum += _mm_cvtsi128_si32(final_sum);\n            }\n        }\n        for (; i < size; ++i) {\n            row_sum += row[i];\n        }\n        count += (row_sum + capacity - 1) / capacity;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of 16-bit integers, return the sum of all of the odd elements that are in even positions.\nExamples\nsolution_simd({5, 8, 7, 1}) ==> 12\nsolution_simd({3, 3, 3, 3, 3}) ==> 9\nsolution_simd({30, 13, 24, 321}) ==>0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i masked_vec = _mm256_and_si256(vec, even_mask);\n\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i odd_elements = _mm256_and_si256(masked_vec, odd_mask);\n\n        int16_t temp_sum[16];\n        _mm256_storeu_si256((__m256i*)temp_sum, odd_elements);\n\n        for (int j = 0; j < 16 && i + j < n; ++j) {\n            if ((i + j) % 2 == 0 && (lst[i + j] % 2 != 0)) {\n                sum += lst[i + j];\n            }\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of 16-bit integers, return the sum of all of the odd elements that are in even positions.\nExamples\nsolution_simd({5, 8, 7, 1}) ==> 12\nsolution_simd({3, 3, 3, 3, 3}) ==> 9\nsolution_simd({30, 13, 24, 321}) ==>0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i masked_vec = _mm256_and_si256(vec, even_mask);\n\n        __m256i one = _mm256_set1_epi16(1);\n        __m256i odd_mask = _mm256_and_si256(masked_vec, one);\n\n        int16_t temp_arr[16];\n        _mm256_storeu_si256((__m256i*)temp_arr, odd_mask);\n\n        for (int j = 0; j < 16 && i + j < n; ++j) {\n            if ((i + j) % 2 == 0 && (lst[i+j] % 2 != 0)) {\n                sum += lst[i + j];\n            }\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of 16-bit integers, return the sum of all of the odd elements that are in even positions.\nExamples\nsolution_simd({5, 8, 7, 1}) ==> 12\nsolution_simd({3, 3, 3, 3, 3}) ==> 9\nsolution_simd({30, 13, 24, 321}) ==>0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n / 16 * 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        // Create a mask for even positions\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Apply the even position mask\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n\n        // Check for odd numbers (LSB is 1)\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i odd_vec = _mm256_and_si256(even_vec, odd_mask);\n\n        // Create a mask where odd numbers at even positions are 1, others are 0\n        __m256i combined_mask = _mm256_cmpeq_epi16(odd_vec, odd_mask);\n\n        // Extract the odd numbers at even positions\n        __m256i masked_vec = _mm256_and_si256(even_vec, combined_mask);\n\n        // Sum the elements in masked_vec\n        int16_t temp_sum[16];\n        _mm256_storeu_si256((__m256i*)temp_sum, masked_vec);\n\n        for (int j = 0; j < 16; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n / 16 * 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of 16-bit integers, return the sum of all of the odd elements that are in even positions.\nExamples\nsolution_simd({5, 8, 7, 1}) ==> 12\nsolution_simd({3, 3, 3, 3, 3}) ==> 9\nsolution_simd({30, 13, 24, 321}) ==>0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n / 16 * 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        // Create a mask for even positions\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Apply the even position mask\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n\n        // Check for odd numbers (LSB is 1)\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i odd_vec = _mm256_and_si256(even_vec, odd_mask);\n\n        // Create a mask where odd numbers are 1 and even numbers are 0\n        __m256i cmp_result = _mm256_cmpeq_epi16(odd_vec, odd_mask);\n\n        // Zero out even numbers in the original vector\n        __m256i masked_vec = _mm256_and_si256(even_vec, cmp_result);\n\n        // Sum the elements of the vector\n        int16_t temp_sum[16];\n        _mm256_storeu_si256((__m256i*)temp_sum, masked_vec);\n\n        for (int j = 0; j < 16; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n / 16 * 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a non-empty vector of 16-bit integers, return the sum of all of the odd elements that are in even positions.\nExamples\nsolution_simd({5, 8, 7, 1}) ==> 12\nsolution_simd({3, 3, 3, 3, 3}) ==> 9\nsolution_simd({30, 13, 24, 321}) ==>0\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    for (int i = 0; i < n / 16 * 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        // Create a mask for even positions\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Apply the even position mask\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n\n        // Check for odd numbers (LSB is 1)\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i odd_vec = _mm256_and_si256(even_vec, odd_mask);\n\n        // Create a mask where odd numbers are 1 and even numbers are 0\n        __m256i cmp_result = _mm256_cmpeq_epi16(odd_vec, odd_mask);\n\n        // Zero out even numbers\n        __m256i masked_vec = _mm256_and_si256(even_vec, cmp_result);\n\n        // Sum the elements of the vector\n        int16_t temp_sum[16];\n        _mm256_storeu_si256((__m256i*)temp_sum, masked_vec);\n\n        for (int j = 0; j < 16; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n / 16 * 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers arr and an integer k, return\nthe sum of the elements with at most two digits from the first k elements of arr.\n\nExample:\n    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n    Output: 24 # sum of 21 + 3\n\nConstraints:\n    1. 1 <= len(arr) <= 2^24\n    2. 1 <= k <= len(arr)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int i = 0;\n\n    if (k >= 8) {\n        for (; i <= k - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n            __m256i less_than_100 = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n\n            int mask = _mm256_movemask_epi8(less_than_100);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    sum += arr[i + j];\n                }\n            }\n        }\n    }\n\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers arr and an integer k, return\nthe sum of the elements with at most two digits from the first k elements of arr.\n\nExample:\n    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n    Output: 24 # sum of 21 + 3\n\nConstraints:\n    1. 1 <= len(arr) <= 2^24\n    2. 1 <= k <= len(arr)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int i = 0;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + 8 <= k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_vec = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n\n        // Create a mask from the comparison result\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        // Iterate through the elements in the vector and add to the sum if the mask bit is set\n        for (int j = 0; j < 8; ++j) {\n            if ((mask & (1 << (j * 4))) != 0) {\n                sum += arr[i + j];\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers arr and an integer k, return\nthe sum of the elements with at most two digits from the first k elements of arr.\n\nExample:\n    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n    Output: 24 # sum of 21 + 3\n\nConstraints:\n    1. 1 <= len(arr) <= 2^24\n    2. 1 <= k <= len(arr)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int i = 0;\n\n    if (k >= 8) {\n        for (; i <= k - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n            __m256i less_than_100 = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n            int mask = _mm256_movemask_epi8(less_than_100);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    sum += arr[i + j];\n                }\n            }\n        }\n    }\n\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers arr and an integer k, return\nthe sum of the elements with at most two digits from the first k elements of arr.\n\nExample:\n    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n    Output: 24 # sum of 21 + 3\n\nConstraints:\n    1. 1 <= len(arr) <= 2^24\n    2. 1 <= k <= len(arr)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int i = 0;\n\n    if (k >= 8) {\n        for (; i <= k - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n            __m256i cmp_vec = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n            int mask = _mm256_movemask_epi8(cmp_vec);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    sum += arr[i + j];\n                }\n            }\n        }\n    }\n\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nGiven a non-empty vector of integers arr and an integer k, return\nthe sum of the elements with at most two digits from the first k elements of arr.\n\nExample:\n    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n    Output: 24 # sum of 21 + 3\n\nConstraints:\n    1. 1 <= len(arr) <= 2^24\n    2. 1 <= k <= len(arr)\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int i = 0;\n\n    if (k >= 8) {\n        for (; i <= k - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n            __m256i less_than_100 = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n\n            int mask = _mm256_movemask_epi8(less_than_100);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    sum += arr[i + j];\n                }\n            }\n        }\n    }\n\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return whether or not they are sorted\nin ascending order. If vector has more than 1 duplicate of the same\nnumber, return false. Assume no negative numbers and only integers.\n\nExamples\nis_sorted_simd({5}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5}) \u279e false\nis_sorted_simd({1, 2, 3, 4, 5, 6}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5, 6, 7}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5, 6, 7}) \u279e false\nis_sorted_simd({1, 2, 2, 3, 3, 4}) \u279e true\nis_sorted_simd({1, 2, 2, 2, 3, 4}) \u279e false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((const __m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmplt_epi32(next, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, next);\n        int mask = _mm256_movemask_epi8(eq);\n        if (mask) {\n            for(int j = 0; j < 8; ++j) {\n                if((mask & (0xFF << (j * 4))) != 0) {\n                    int count = 0;\n                    for(int k = 0; k < n; ++k) {\n                        if(lst[k] == lst[i+j]) {\n                            count++;\n                        }\n                    }\n                    if(count > 1) return false;\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        int count = 0;\n        if(lst[i] == lst[i+1]) {\n            for(int k = 0; k < n; ++k) {\n                if(lst[k] == lst[i]) {\n                    count++;\n                }\n            }\n            if(count > 1) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return whether or not they are sorted\nin ascending order. If vector has more than 1 duplicate of the same\nnumber, return false. Assume no negative numbers and only integers.\n\nExamples\nis_sorted_simd({5}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5}) \u279e false\nis_sorted_simd({1, 2, 3, 4, 5, 6}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5, 6, 7}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5, 6, 7}) \u279e false\nis_sorted_simd({1, 2, 2, 3, 3, 4}) \u279e true\nis_sorted_simd({1, 2, 2, 2, 3, 4}) \u279e false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((const __m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, next);\n        int mask = _mm256_movemask_epi8(eq);\n        if (mask) {\n            for (int j = 0; j < 7; ++j) {\n                if (lst[i + j] == lst[i + j + 1]) {\n                    for (int k = j + 2; k < 8; ++k) {\n                        if (lst[i + j] == lst[i + k]) {\n                            return false;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        if (lst[i] == lst[i + 1]) {\n            for (int k = i + 2; k < n; ++k) {\n                if (lst[i] == lst[k]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return whether or not they are sorted\nin ascending order. If vector has more than 1 duplicate of the same\nnumber, return false. Assume no negative numbers and only integers.\n\nExamples\nis_sorted_simd({5}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5}) \u279e false\nis_sorted_simd({1, 2, 3, 4, 5, 6}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5, 6, 7}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5, 6, 7}) \u279e false\nis_sorted_simd({1, 2, 2, 3, 3, 4}) \u279e true\nis_sorted_simd({1, 2, 2, 2, 3, 4}) \u279e false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((const __m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmplt_epi32(next, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, next);\n        int mask = _mm256_movemask_epi8(eq);\n\n        if(mask){\n            int arr[8];\n            _mm256_storeu_si256((__m256i*)arr, current);\n            for(int j = 0; j < 8; ++j){\n                int count = 0;\n                for(int k = 0; k < lst.size(); ++k){\n                    if(lst[k] == arr[j]){\n                        count++;\n                    }\n                }\n                if(count > 1){\n                    return false;\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        int count = 0;\n        for(int k = 0; k < lst.size(); ++k){\n            if(lst[k] == lst[i]){\n                count++;\n            }\n        }\n        if(count > 1 && lst[i] == lst[i+1]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return whether or not they are sorted\nin ascending order. If vector has more than 1 duplicate of the same\nnumber, return false. Assume no negative numbers and only integers.\n\nExamples\nis_sorted_simd({5}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5}) \u279e false\nis_sorted_simd({1, 2, 3, 4, 5, 6}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5, 6, 7}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5, 6, 7}) \u279e false\nis_sorted_simd({1, 2, 2, 3, 3, 4}) \u279e true\nis_sorted_simd({1, 2, 2, 2, 3, 4}) \u279e false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((const __m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, next);\n        int mask = _mm256_movemask_epi8(eq);\n\n        if (mask) {\n            int arr[8];\n            _mm256_storeu_si256((__m256i*)arr, current);\n            for(int j = 0; j < 7; ++j){\n                if(arr[j] == arr[j+1]){\n                    int count = 0;\n                    for(int k = 0; k < n; ++k){\n                        if(lst[k] == arr[j]){\n                            count++;\n                        }\n                    }\n                    if(count > 2){\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        if (lst[i] == lst[i+1]){\n            int count = 0;\n            for(int k = 0; k < n; ++k){\n                if(lst[k] == lst[i]){\n                    count++;\n                }\n            }\n            if(count > 2){\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return whether or not they are sorted\nin ascending order. If vector has more than 1 duplicate of the same\nnumber, return false. Assume no negative numbers and only integers.\n\nExamples\nis_sorted_simd({5}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5}) \u279e false\nis_sorted_simd({1, 2, 3, 4, 5, 6}) \u279e true\nis_sorted_simd({1, 2, 3, 4, 5, 6, 7}) \u279e true\nis_sorted_simd({1, 3, 2, 4, 5, 6, 7}) \u279e false\nis_sorted_simd({1, 2, 2, 3, 3, 4}) \u279e true\nis_sorted_simd({1, 2, 2, 2, 3, 4}) \u279e false\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((const __m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmplt_epi32(next, current);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, next);\n        int mask = _mm256_movemask_epi8(eq);\n\n        if(mask){\n            for(int j = 0; j < 8; ++j){\n                if((mask & (1 << (j*4))) != 0){\n                    if(i+j+2 < n && lst[i+j+2] == lst[i+j]){\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n        if(lst[i] == lst[i+1]){\n            if(i+2 < n && lst[i+2] == lst[i]){\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nYou are given a vector arr of integers and you need to return\nsum of magnitudes of integers multiplied by product of all signs\nof each number in the vector, represented by 1, -1 or 0.\nNote: return -32768 for empty arr.\n\nExample:\n>>> prod_signs_simd({1, 2, 2, -4}) == -9\n>>> prod_signs_simd({0, 1}) == 0\n>>> prod_signs_simd({}) == -32768\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    int sum_abs = 0;\n    int sign_prod = 1;\n\n    if (n >= 8) {\n        int i = 0;\n        for (; i <= n - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&arr[i]);\n\n            // Compute absolute values\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n\n            // Sum absolute values\n            int abs_arr[8];\n            _mm256_storeu_si256((__m256i*)abs_arr, abs_vec);\n            for (int j = 0; j < 8; ++j) {\n                sum_abs += abs_arr[j];\n            }\n\n            // Compute signs\n            __m256i sign_vec = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n\n            // Multiply signs\n            int sign_arr[8];\n            _mm256_storeu_si256((__m256i*)sign_arr, sign_vec);\n            for (int j = 0; j < 8; ++j) {\n                sign_prod *= sign_arr[j];\n            }\n        }\n\n        // Handle remaining elements\n        for (; i < n; ++i) {\n            sum_abs += std::abs(arr[i]);\n            if (arr[i] > 0) {\n                sign_prod *= 1;\n            } else if (arr[i] < 0) {\n                sign_prod *= -1;\n            } else {\n                sign_prod *= 0;\n            }\n        }\n    } else {\n        for (int i = 0; i < n; ++i) {\n            sum_abs += std::abs(arr[i]);\n            if (arr[i] > 0) {\n                sign_prod *= 1;\n            } else if (arr[i] < 0) {\n                sign_prod *= -1;\n            } else {\n                sign_prod *= 0;\n            }\n        }\n    }\n\n    if (sign_prod == 0) return 0;\n    return sum_abs * sign_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector arr of integers and you need to return\nsum of magnitudes of integers multiplied by product of all signs\nof each number in the vector, represented by 1, -1 or 0.\nNote: return -32768 for empty arr.\n\nExample:\n>>> prod_signs_simd({1, 2, 2, -4}) == -9\n>>> prod_signs_simd({0, 1}) == 0\n>>> prod_signs_simd({}) == -32768\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    int sum_abs = 0;\n    int prod_sign = 1;\n\n    for (int i = 0; i < n; ++i) {\n        sum_abs += std::abs(arr[i]);\n        if (arr[i] > 0) {\n            prod_sign *= 1;\n        } else if (arr[i] < 0) {\n            prod_sign *= -1;\n        } else {\n            prod_sign = 0;\n            break;\n        }\n    }\n\n    return sum_abs * prod_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector arr of integers and you need to return\nsum of magnitudes of integers multiplied by product of all signs\nof each number in the vector, represented by 1, -1 or 0.\nNote: return -32768 for empty arr.\n\nExample:\n>>> prod_signs_simd({1, 2, 2, -4}) == -9\n>>> prod_signs_simd({0, 1}) == 0\n>>> prod_signs_simd({}) == -32768\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    int sum_abs = 0;\n    int prod_sign = 1;\n\n    for (int i = 0; i < n; ++i) {\n        sum_abs += std::abs(arr[i]);\n        if (arr[i] > 0) {\n            prod_sign *= 1;\n        } else if (arr[i] < 0) {\n            prod_sign *= -1;\n        } else {\n            prod_sign = 0;\n            break;\n        }\n    }\n\n    return sum_abs * prod_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector arr of integers and you need to return\nsum of magnitudes of integers multiplied by product of all signs\nof each number in the vector, represented by 1, -1 or 0.\nNote: return -32768 for empty arr.\n\nExample:\n>>> prod_signs_simd({1, 2, 2, -4}) == -9\n>>> prod_signs_simd({0, 1}) == 0\n>>> prod_signs_simd({}) == -32768\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    int sum_abs = 0;\n    int prod_sign = 1;\n\n    for (int i = 0; i < n; ++i) {\n        sum_abs += std::abs(arr[i]);\n        if (arr[i] > 0) {\n            prod_sign *= 1;\n        } else if (arr[i] < 0) {\n            prod_sign *= -1;\n        } else {\n            prod_sign = 0;\n            break;\n        }\n    }\n\n    return sum_abs * prod_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector arr of integers and you need to return\nsum of magnitudes of integers multiplied by product of all signs\nof each number in the vector, represented by 1, -1 or 0.\nNote: return -32768 for empty arr.\n\nExample:\n>>> prod_signs_simd({1, 2, 2, -4}) == -9\n>>> prod_signs_simd({0, 1}) == 0\n>>> prod_signs_simd({}) == -32768\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int n = arr.size();\n    int sum_abs = 0;\n    int prod_sign = 1;\n\n    for (int i = 0; i < n; ++i) {\n        sum_abs += std::abs(arr[i]);\n        if (arr[i] > 0) {\n            prod_sign *= 1;\n        } else if (arr[i] < 0) {\n            prod_sign *= -1;\n        } else {\n            prod_sign = 0;\n            break;\n        }\n    }\n\n    return sum_abs * prod_sign;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int n = grid.size();\n    std::vector<int> best_path;\n    std::vector<int> current_path;\n\n    std::function<void(int, int)> solve = \n        [&](int row, int col) {\n        current_path.push_back(grid[row][col]);\n\n        if (current_path.size() == k) {\n            if (best_path.empty() || current_path < best_path) {\n                best_path = current_path;\n            }\n        } else {\n            // Explore neighbors\n            int dr[] = {-1, 1, 0, 0};\n            int dc[] = {0, 0, -1, 1};\n\n            for (int i = 0; i < 4; ++i) {\n                int new_row = row + dr[i];\n                int new_col = col + dc[i];\n\n                if (new_row >= 0 && new_row < n && new_col >= 0 && new_col < n) {\n                    solve(new_row, new_col);\n                }\n            }\n        }\n\n        current_path.pop_back();\n    };\n\n    // Iterate through all starting cells to find the minimum path\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            solve(i, j);\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int n = grid.size();\n    std::vector<int> best_path;\n\n    std::function<void(int, int, std::vector<int>)> solve = \n        [&](int row, int col, std::vector<int> current_path) {\n        \n        current_path.push_back(grid[row][col]);\n\n        if (current_path.size() == k) {\n            if (best_path.empty() || current_path < best_path) {\n                best_path = current_path;\n            }\n            return;\n        }\n\n        // Possible moves: up, down, left, right\n        int dr[] = {-1, 1, 0, 0};\n        int dc[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int new_row = row + dr[i];\n            int new_col = col + dc[i];\n\n            if (new_row >= 0 && new_row < n && new_col >= 0 && new_col < n) {\n                solve(new_row, new_col, current_path);\n            }\n        }\n    };\n\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            std::vector<int> current_path;\n            solve(i, j, current_path);\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int n = grid.size();\n    std::vector<int> best_path;\n    std::vector<int> current_path;\n\n    std::function<void(int, int)> solve = \n        [&](int row, int col) {\n        current_path.push_back(grid[row][col]);\n\n        if (current_path.size() == k) {\n            if (best_path.empty() || current_path < best_path) {\n                best_path = current_path;\n            }\n        } else {\n            // Explore neighbors\n            int dr[] = {-1, 1, 0, 0};\n            int dc[] = {0, 0, -1, 1};\n\n            for (int i = 0; i < 4; ++i) {\n                int new_row = row + dr[i];\n                int new_col = col + dc[i];\n\n                if (new_row >= 0 && new_row < n && new_col >= 0 && new_col < n) {\n                    solve(new_row, new_col);\n                }\n            }\n        }\n\n        current_path.pop_back();\n    };\n\n    // Iterate through all starting cells\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            solve(i, j);\n            current_path.clear();\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int n = grid.size();\n    std::vector<int> best_path;\n\n    std::function<void(int, int, std::vector<int>)> solve = \n        [&](int row, int col, std::vector<int> current_path) {\n        \n        current_path.push_back(grid[row][col]);\n\n        if (current_path.size() == k) {\n            if (best_path.empty() || current_path < best_path) {\n                best_path = current_path;\n            }\n            return;\n        }\n\n        // Possible moves: up, down, left, right\n        int dr[] = {-1, 1, 0, 0};\n        int dc[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int new_row = row + dr[i];\n            int new_col = col + dc[i];\n\n            if (new_row >= 0 && new_row < n && new_col >= 0 && new_col < n) {\n                solve(new_row, new_col, current_path);\n            }\n        }\n    };\n\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            std::vector<int> current_path;\n            solve(i, j, current_path);\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int n = grid.size();\n    std::vector<int> best_path;\n\n    std::function<void(std::vector<int>, int, int)> solve = \n        [&](std::vector<int> current_path, int row, int col) {\n        \n        if (current_path.size() == k) {\n            if (best_path.empty() || current_path < best_path) {\n                best_path = current_path;\n            }\n            return;\n        }\n\n        int dr[] = {-1, 1, 0, 0};\n        int dc[] = {0, 0, -1, 1};\n\n        for (int i = 0; i < 4; ++i) {\n            int new_row = row + dr[i];\n            int new_col = col + dc[i];\n\n            if (new_row >= 0 && new_row < n && new_col >= 0 && new_col < n) {\n                std::vector<int> next_path = current_path;\n                next_path.push_back(grid[new_row][new_col]);\n                solve(next_path, new_row, new_col);\n            }\n        }\n    };\n\n    std::vector<std::vector<int>> all_start_paths;\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            std::vector<int> start_path;\n            start_path.push_back(grid[i][j]);\n            all_start_paths.push_back(start_path);\n        }\n    }\n\n    for(int i = 0; i < n; ++i){\n        for(int j = 0; j < n; ++j){\n            std::vector<int> current_path;\n            current_path.push_back(grid[i][j]);\n            solve(current_path, i, j);\n        }\n    }\n    \n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a positive integer n, return the product of the odd digits.\nReturn 0 if all digits are even.\nFor example:\ndigits_simd(1)  == 1\ndigits_simd(4)  == 0\ndigits_simd(235) == 15\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t digits_simd(uint64_t n){\n    uint64_t product = 1;\n    bool hasOdd = false;\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        if (digit % 2 != 0) {\n            product *= digit;\n            hasOdd = true;\n        }\n        n /= 10;\n    }\n    return hasOdd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a positive integer n, return the product of the odd digits.\nReturn 0 if all digits are even.\nFor example:\ndigits_simd(1)  == 1\ndigits_simd(4)  == 0\ndigits_simd(235) == 15\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t digits_simd(uint64_t n){\n    uint64_t product = 1;\n    bool hasOdd = false;\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        if (digit % 2 != 0) {\n            product *= digit;\n            hasOdd = true;\n        }\n        n /= 10;\n    }\n    return hasOdd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a positive integer n, return the product of the odd digits.\nReturn 0 if all digits are even.\nFor example:\ndigits_simd(1)  == 1\ndigits_simd(4)  == 0\ndigits_simd(235) == 15\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t digits_simd(uint64_t n){\n    uint64_t product = 1;\n    bool hasOdd = false;\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        if (digit % 2 != 0) {\n            product *= digit;\n            hasOdd = true;\n        }\n        n /= 10;\n    }\n    return hasOdd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a positive integer n, return the product of the odd digits.\nReturn 0 if all digits are even.\nFor example:\ndigits_simd(1)  == 1\ndigits_simd(4)  == 0\ndigits_simd(235) == 15\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t digits_simd(uint64_t n){\n    uint64_t product = 1;\n    bool hasOdd = false;\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        if (digit % 2 != 0) {\n            product *= digit;\n            hasOdd = true;\n        }\n        n /= 10;\n    }\n\n    if (!hasOdd) {\n        return 0;\n    }\n\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a positive integer n, return the product of the odd digits.\nReturn 0 if all digits are even.\nFor example:\ndigits_simd(1)  == 1\ndigits_simd(4)  == 0\ndigits_simd(235) == 15\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t digits_simd(uint64_t n){\n    uint64_t product = 1;\n    bool hasOdd = false;\n\n    if (n == 0) return 0;\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        if (digit % 2 != 0) {\n            product *= digit;\n            hasOdd = true;\n        }\n        n /= 10;\n    }\n\n    if (!hasOdd) {\n        return 0;\n    }\n\n    return product;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector of numbers.\nYou need to return the sum of squared numbers in the given vector,\nround each element in the vector to the upper int (ceiling) first.\nExamples:\nFor lst = {1,2,3} the output should be 14\nFor lst = {1,4,9} the output should be 98\nFor lst = {1,3,5,7} the output should be 84\nFor lst = {1.4,4.2,0} the output should be 29\nFor lst = {-2.4,1,1} the output should be 6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    if (size >= 8) {\n        __m256 vec_sum = _mm256_setzero_ps();\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 vec_ceil = _mm256_ceil_ps(vec);\n            __m256 vec_int = _mm256_round_ps(vec_ceil, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 vec_sq = _mm256_mul_ps(vec_int, vec_int);\n            vec_sum = _mm256_add_ps(vec_sum, vec_sq);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, vec_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += (uint64_t)ceil(lst[i]) * (uint64_t)ceil(lst[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector of numbers.\nYou need to return the sum of squared numbers in the given vector,\nround each element in the vector to the upper int (ceiling) first.\nExamples:\nFor lst = {1,2,3} the output should be 14\nFor lst = {1,4,9} the output should be 98\nFor lst = {1,3,5,7} the output should be 84\nFor lst = {1.4,4.2,0} the output should be 29\nFor lst = {-2.4,1,1} the output should be 6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int i = 0;\n    int size = lst.size();\n\n    if (size >= 8) {\n        __m256 vec_sum = _mm256_setzero_ps();\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 vec_ceil = _mm256_ceil_ps(vec);\n            __m256 vec_int = _mm256_round_ps(vec_ceil, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 vec_sq = _mm256_mul_ps(vec_int, vec_int);\n            vec_sum = _mm256_add_ps(vec_sum, vec_sq);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, vec_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += (uint64_t)(ceil(lst[i]) * ceil(lst[i]));\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector of numbers.\nYou need to return the sum of squared numbers in the given vector,\nround each element in the vector to the upper int (ceiling) first.\nExamples:\nFor lst = {1,2,3} the output should be 14\nFor lst = {1,4,9} the output should be 98\nFor lst = {1,3,5,7} the output should be 84\nFor lst = {1.4,4.2,0} the output should be 29\nFor lst = {-2.4,1,1} the output should be 6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int i = 0;\n    int size = lst.size();\n\n    if (size >= 8) {\n        __m256 vec_sum = _mm256_setzero_ps();\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 vec_ceil = _mm256_ceil_ps(vec);\n            __m256 vec_int = _mm256_round_ps(vec_ceil, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 vec_sq = _mm256_mul_ps(vec_int, vec_int);\n            vec_sum = _mm256_add_ps(vec_sum, vec_sq);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, vec_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += (uint64_t)(ceil(lst[i]) * ceil(lst[i]));\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector of numbers.\nYou need to return the sum of squared numbers in the given vector,\nround each element in the vector to the upper int (ceiling) first.\nExamples:\nFor lst = {1,2,3} the output should be 14\nFor lst = {1,4,9} the output should be 98\nFor lst = {1,3,5,7} the output should be 84\nFor lst = {1.4,4.2,0} the output should be 29\nFor lst = {-2.4,1,1} the output should be 6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    if (size >= 8) {\n        __m256 vec_sum = _mm256_setzero_ps();\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 vec_ceil = _mm256_ceil_ps(vec);\n            __m256 vec_int = _mm256_round_ps(vec_ceil, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 vec_sq = _mm256_mul_ps(vec_int, vec_int);\n            vec_sum = _mm256_add_ps(vec_sum, vec_sq);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, vec_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += (uint64_t)ceil(lst[i]) * (uint64_t)ceil(lst[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a vector of numbers.\nYou need to return the sum of squared numbers in the given vector,\nround each element in the vector to the upper int (ceiling) first.\nExamples:\nFor lst = {1,2,3} the output should be 14\nFor lst = {1,4,9} the output should be 98\nFor lst = {1,3,5,7} the output should be 84\nFor lst = {1.4,4.2,0} the output should be 29\nFor lst = {-2.4,1,1} the output should be 6\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    int i = 0;\n\n    if (size >= 8) {\n        __m256 vec_sum = _mm256_setzero_ps();\n        for (; i <= size - 8; i += 8) {\n            __m256 vec = _mm256_loadu_ps(&lst[i]);\n            __m256 vec_ceil = _mm256_ceil_ps(vec);\n            __m256 vec_int = _mm256_round_ps(vec_ceil, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 vec_sq = _mm256_mul_ps(vec_int, vec_int);\n            vec_sum = _mm256_add_ps(vec_sum, vec_sq);\n        }\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, vec_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        sum += (uint64_t)ceil(lst[i]) * (uint64_t)ceil(lst[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function which returns the largest index of an element which\nis not greater than or equal to the element immediately preceding it. If\nno such element exists then return -1. The given vector will not contain\nduplicate values.\n\nExamples:\ncan_arrange_simd({1,2,4,3,5}) = 3\ncan_arrange_simd({1,2,3}) = -1\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function which returns the largest index of an element which\nis not greater than or equal to the element immediately preceding it. If\nno such element exists then return -1. The given vector will not contain\nduplicate values.\n\nExamples:\ncan_arrange_simd({1,2,4,3,5}) = 3\ncan_arrange_simd({1,2,3}) = -1\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function which returns the largest index of an element which\nis not greater than or equal to the element immediately preceding it. If\nno such element exists then return -1. The given vector will not contain\nduplicate values.\n\nExamples:\ncan_arrange_simd({1,2,4,3,5}) = 3\ncan_arrange_simd({1,2,3}) = -1\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function which returns the largest index of an element which\nis not greater than or equal to the element immediately preceding it. If\nno such element exists then return -1. The given vector will not contain\nduplicate values.\n\nExamples:\ncan_arrange_simd({1,2,4,3,5}) = 3\ncan_arrange_simd({1,2,3}) = -1\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function which returns the largest index of an element which\nis not greater than or equal to the element immediately preceding it. If\nno such element exists then return -1. The given vector will not contain\nduplicate values.\n\nExamples:\ncan_arrange_simd({1,2,4,3,5}) = 3\ncan_arrange_simd({1,2,3}) = -1\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nCreate a function that returns a vector (a, b), where \"a\" is\nthe largest of negative integers, and \"b\" is the smallest\nof positive integers in a vector.\nIf there is no negative or positive integers, return them as 0.\n\nExamples:\nlargest_smallest_integers_simd({2, 4, 1, 3, 5, 7}) == {0, 1}\nlargest_smallest_integers_simd({}) == {0,0}\nlargest_smallest_integers_simd({0}) == {0,0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int n = lst.size();\n    const int simd_width = 8; \n    const int num_simd_iterations = n / simd_width;\n\n    __m256i largest_negative_vec = _mm256_set1_epi32(0);\n    __m256i smallest_positive_vec = _mm256_set1_epi32(0x7FFFFFFF); \n\n    for (int i = 0; i < num_simd_iterations; ++i) {\n        __m256i data_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i * simd_width]));\n\n        __m256i sign_bit_mask = _mm256_srai_epi32(data_vec, 31); \n        __m256i is_negative = _mm256_cmplt_epi32(data_vec, _mm256_set1_epi32(0));\n        __m256i is_positive = _mm256_cmpgt_epi32(data_vec, _mm256_set1_epi32(0));\n\n        __m256i masked_negative = _mm256_and_si256(data_vec, is_negative);\n        __m256i masked_positive = _mm256_and_si256(data_vec, is_positive);\n\n        largest_negative_vec = _mm256_max_epi32(largest_negative_vec, masked_negative);\n        smallest_positive_vec = _mm256_min_epi32(smallest_positive_vec, masked_positive);\n    }\n\n    int largest_negative_arr[8];\n    int smallest_positive_arr[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(largest_negative_arr), largest_negative_vec);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(smallest_positive_arr), smallest_positive_vec);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (largest_negative_arr[i] > largest_negative) {\n            largest_negative = largest_negative_arr[i];\n        }\n        if (smallest_positive_arr[i] != 0 && (smallest_positive == 0 || smallest_positive_arr[i] < smallest_positive)) {\n            smallest_positive = smallest_positive_arr[i];\n        }\n    }\n\n    for (int i = num_simd_iterations * simd_width; i < n; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        } else if (lst[i] > 0 && (smallest_positive == 0 || lst[i] < smallest_positive)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nCreate a function that returns a vector (a, b), where \"a\" is\nthe largest of negative integers, and \"b\" is the smallest\nof positive integers in a vector.\nIf there is no negative or positive integers, return them as 0.\n\nExamples:\nlargest_smallest_integers_simd({2, 4, 1, 3, 5, 7}) == {0, 1}\nlargest_smallest_integers_simd({}) == {0,0}\nlargest_smallest_integers_simd({0}) == {0,0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int n = lst.size();\n    const int simd_width = 8; \n\n    if (n == 0) return {0, 0};\n\n    __m256i largest_neg_vec = _mm256_set1_epi32(0);\n    __m256i smallest_pos_vec = _mm256_set1_epi32(INT_MAX);\n    \n    int i = 0;\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i data_vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n\n        __m256i zero_vec = _mm256_set1_epi32(0);\n        \n        __m256i neg_mask = _mm256_cmpgt_epi32(zero_vec, data_vec);\n        __m256i pos_mask = _mm256_cmpgt_epi32(data_vec, zero_vec);\n\n        __m256i masked_neg_vec = _mm256_and_si256(neg_mask, data_vec);\n        __m256i masked_pos_vec = _mm256_and_si256(pos_mask, data_vec);\n\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, masked_neg_vec);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, masked_pos_vec);\n    }\n\n    int largest_neg_arr[8];\n    int smallest_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)largest_neg_arr, largest_neg_vec);\n    _mm256_storeu_si256((__m256i*)smallest_pos_arr, smallest_pos_vec);\n\n    for (int j = 0; j < simd_width; ++j) {\n        if (largest_neg_arr[j] > largest_negative) {\n            largest_negative = largest_neg_arr[j];\n        }\n        if (smallest_pos_arr[j] != INT_MAX && (smallest_positive == 0 || smallest_pos_arr[j] < smallest_positive)) {\n            smallest_positive = smallest_pos_arr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        } else if (lst[i] > 0 && (smallest_positive == 0 || lst[i] < smallest_positive)) {\n            smallest_positive = lst[i];\n        }\n    }\n    \n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nCreate a function that returns a vector (a, b), where \"a\" is\nthe largest of negative integers, and \"b\" is the smallest\nof positive integers in a vector.\nIf there is no negative or positive integers, return them as 0.\n\nExamples:\nlargest_smallest_integers_simd({2, 4, 1, 3, 5, 7}) == {0, 1}\nlargest_smallest_integers_simd({}) == {0,0}\nlargest_smallest_integers_simd({0}) == {0,0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int n = lst.size();\n    const int simd_width = 8; \n    const int num_simd_iterations = n / simd_width;\n\n    __m256i largest_negative_vec = _mm256_set1_epi32(0);\n    __m256i smallest_positive_vec = _mm256_set1_epi32(0x7FFFFFFF); \n\n    for (int i = 0; i < num_simd_iterations; ++i) {\n        __m256i data_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i * simd_width]));\n\n        __m256i sign_bit_mask = _mm256_srai_epi32(data_vec, 31); \n        __m256i is_negative = _mm256_cmplt_epi32(data_vec, _mm256_set1_epi32(0));\n        __m256i is_positive = _mm256_cmpgt_epi32(data_vec, _mm256_set1_epi32(0));\n\n        __m256i masked_negative = _mm256_and_si256(data_vec, is_negative);\n        __m256i masked_positive = _mm256_and_si256(data_vec, is_positive);\n\n        largest_negative_vec = _mm256_max_epi32(largest_negative_vec, masked_negative);\n        smallest_positive_vec = _mm256_min_epi32(smallest_positive_vec, masked_positive);\n    }\n\n    int largest_negative_arr[8];\n    int smallest_positive_arr[8];\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(largest_negative_arr), largest_negative_vec);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(smallest_positive_arr), smallest_positive_vec);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (largest_negative_arr[i] > largest_negative) {\n            largest_negative = largest_negative_arr[i];\n        }\n        if (smallest_positive_arr[i] != 0 && (smallest_positive == 0 || smallest_positive_arr[i] < smallest_positive)) {\n            smallest_positive = smallest_positive_arr[i];\n        }\n    }\n\n    for (int i = num_simd_iterations * simd_width; i < n; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        } else if (lst[i] > 0 && (smallest_positive == 0 || lst[i] < smallest_positive)) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nCreate a function that returns a vector (a, b), where \"a\" is\nthe largest of negative integers, and \"b\" is the smallest\nof positive integers in a vector.\nIf there is no negative or positive integers, return them as 0.\n\nExamples:\nlargest_smallest_integers_simd({2, 4, 1, 3, 5, 7}) == {0, 1}\nlargest_smallest_integers_simd({}) == {0,0}\nlargest_smallest_integers_simd({0}) == {0,0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int n = lst.size();\n    const int simd_width = 8; \n\n    if (n >= simd_width) {\n        int i = 0;\n        for (; i <= n - simd_width; i += simd_width) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n            \n            // Find negative numbers\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp_lt = _mm256_cmpgt_epi32(zero, vec);\n\n            // Process negative numbers\n            if (_mm256_movemask_epi8(cmp_lt) != 0) {\n                int neg_nums[simd_width];\n                _mm256_storeu_si256((__m256i*)neg_nums, vec);\n\n                for (int j = 0; j < simd_width; ++j) {\n                    if (neg_nums[j] < 0) {\n                        if (largest_negative == 0 || neg_nums[j] > largest_negative) {\n                            largest_negative = neg_nums[j];\n                        }\n                    }\n                }\n            }\n\n            // Find positive numbers\n            __m256i cmp_gt = _mm256_cmpgt_epi32(vec, zero);\n\n            // Process positive numbers\n            if (_mm256_movemask_epi8(cmp_gt) != 0) {\n                int pos_nums[simd_width];\n                _mm256_storeu_si256((__m256i*)pos_nums, vec);\n\n                for (int j = 0; j < simd_width; ++j) {\n                    if (pos_nums[j] > 0) {\n                        if (smallest_positive == 0 || pos_nums[j] < smallest_positive) {\n                            smallest_positive = pos_nums[j];\n                        } else if (smallest_positive == 0){\n                            smallest_positive = pos_nums[j];\n                        }\n                    }\n                }\n            }\n        }\n\n        // Process remaining elements\n        for (; i < n; ++i) {\n            if (lst[i] < 0) {\n                if (largest_negative == 0 || lst[i] > largest_negative) {\n                    largest_negative = lst[i];\n                }\n            } else if (lst[i] > 0) {\n                if (smallest_positive == 0 || lst[i] < smallest_positive) {\n                    smallest_positive = lst[i];\n                } else if (smallest_positive == 0){\n                    smallest_positive = lst[i];\n                }\n            }\n        }\n    } else {\n        // Handle cases where the input size is less than SIMD width\n        for (int val : lst) {\n            if (val < 0) {\n                if (largest_negative == 0 || val > largest_negative) {\n                    largest_negative = val;\n                }\n            } else if (val > 0) {\n                if (smallest_positive == 0 || val < smallest_positive) {\n                    smallest_positive = val;\n                } else if (smallest_positive == 0){\n                    smallest_positive = val;\n                }\n            }\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\n/*\nCreate a function that returns a vector (a, b), where \"a\" is\nthe largest of negative integers, and \"b\" is the smallest\nof positive integers in a vector.\nIf there is no negative or positive integers, return them as 0.\n\nExamples:\nlargest_smallest_integers_simd({2, 4, 1, 3, 5, 7}) == {0, 1}\nlargest_smallest_integers_simd({}) == {0,0}\nlargest_smallest_integers_simd({0}) == {0,0}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int n = lst.size();\n    const int simd_width = 8; \n\n    if (n >= simd_width) {\n        int i = 0;\n        __m256i largest_neg_vec = _mm256_set1_epi32(0);\n        __m256i smallest_pos_vec = _mm256_set1_epi32(INT_MAX);\n        __m256i zero_vec = _mm256_set1_epi32(0);\n\n        for (; i <= n - simd_width; i += simd_width) {\n            __m256i data_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n            __m256i sign_bit_vec = _mm256_srai_epi32(data_vec, 31); \n\n            __m256i is_negative_vec = _mm256_cmplt_epi32(data_vec, zero_vec);\n            __m256i is_positive_vec = _mm256_cmpgt_epi32(data_vec, zero_vec);\n\n            __m256i masked_neg_vec = _mm256_and_si256(data_vec, is_negative_vec);\n            __m256i masked_pos_vec = _mm256_and_si256(data_vec, is_positive_vec);\n\n            largest_neg_vec = _mm256_max_epi32(largest_neg_vec, masked_neg_vec);\n            smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, masked_pos_vec);\n        }\n\n        int temp_largest_neg[simd_width];\n        int temp_smallest_pos[simd_width];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_largest_neg), largest_neg_vec);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_smallest_pos), smallest_pos_vec);\n\n        largest_negative = temp_largest_neg[0];\n        for (int j = 1; j < simd_width; ++j) {\n            largest_negative = std::max(largest_negative, temp_largest_neg[j]);\n        }\n\n        if (largest_negative < 0) {\n            largest_negative = largest_negative;\n        } else {\n            largest_negative = 0;\n        }\n\n        smallest_positive = (temp_smallest_pos[0] == INT_MAX) ? 0 : temp_smallest_pos[0];\n        for (int j = 1; j < simd_width; ++j) {\n            if (temp_smallest_pos[j] != INT_MAX) {\n                smallest_positive = std::min(smallest_positive, temp_smallest_pos[j]);\n            }\n        }\n        if (smallest_positive == INT_MAX) {\n            smallest_positive = 0;\n        }\n\n        for (; i < n; ++i) {\n            if (lst[i] < 0) {\n                largest_negative = std::max(largest_negative, lst[i]);\n            } else if (lst[i] > 0 && (smallest_positive == 0 || lst[i] < smallest_positive)) {\n                smallest_positive = lst[i];\n            }\n        }\n    } else {\n        for (int val : lst) {\n            if (val < 0) {\n                largest_negative = std::max(largest_negative, val);\n            } else if (val > 0 && (smallest_positive == 0 || val < smallest_positive)) {\n                smallest_positive = val;\n            }\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe Brazilian factorial is defined as:\nbrazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!\nwhere n > 0\n\nFor example:\n>>> special_factorial_simd(4)\n288\n\nThe function will receive an integer as input and should return the special\nfactorial of this integer.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t special_factorial_simd(uint64_t n){\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t factorial = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            factorial *= j;\n        }\n        result *= factorial;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe Brazilian factorial is defined as:\nbrazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!\nwhere n > 0\n\nFor example:\n>>> special_factorial_simd(4)\n288\n\nThe function will receive an integer as input and should return the special\nfactorial of this integer.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t special_factorial_simd(uint64_t n){\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t factorial = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            factorial *= j;\n        }\n        result *= factorial;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe Brazilian factorial is defined as:\nbrazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!\nwhere n > 0\n\nFor example:\n>>> special_factorial_simd(4)\n288\n\nThe function will receive an integer as input and should return the special\nfactorial of this integer.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t special_factorial_simd(uint64_t n){\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t factorial = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            factorial *= j;\n        }\n        result *= factorial;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe Brazilian factorial is defined as:\nbrazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!\nwhere n > 0\n\nFor example:\n>>> special_factorial_simd(4)\n288\n\nThe function will receive an integer as input and should return the special\nfactorial of this integer.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t special_factorial_simd(uint64_t n){\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t factorial = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            factorial *= j;\n        }\n        result *= factorial;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThe Brazilian factorial is defined as:\nbrazilian_factorial(n) = n! * (n-1)! * (n-2)! * ... * 1!\nwhere n > 0\n\nFor example:\n>>> special_factorial_simd(4)\n288\n\nThe function will receive an integer as input and should return the special\nfactorial of this integer.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t special_factorial_simd(uint64_t n){\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t factorial = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            factorial *= j;\n        }\n        result *= factorial;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function will take a vector of integers. For all entries in the vector, the function shall square the integer entry if its index is a \nmultiple of 3 and will cube the integer entry if its index is a multiple of 4 and not a multiple of 3. The function will not \nchange the entries in the vector whose indexes are not a multiple of 3 or 4. The function shall then return the sum of all entries. \n\nExamples:\nFor lst = {1,2,3} the output should be 6\nFor lst = {}  the output should be 0\nFor lst = {-1,-5,2,-1,-5}  the output should be -126\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_3 = _mm256_set1_epi32(3);\n        __m256i mask_4 = _mm256_set1_epi32(4);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i mod_3 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_3), mask_3));\n        __m256i mod_4 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_4), mask_4));\n\n        __m256i cmp_3 = _mm256_cmpeq_epi32(mod_3, zero);\n        __m256i cmp_4 = _mm256_cmpeq_epi32(mod_4, zero);\n\n        __m256i not_cmp_3 = _mm256_xor_si256(cmp_3, _mm256_set1_epi32(-1));\n        __m256i mask_4_not_3 = _mm256_and_si256(cmp_4, not_cmp_3);\n\n        __m256i sq_mask = cmp_3;\n        __m256i cb_mask = mask_4_not_3;\n\n        __m256i sq = _mm256_mullo_epi32(vec, vec);\n        __m256i cb = _mm256_mullo_epi32(sq, vec);\n\n        __m256i masked_sq = _mm256_blendv_epi8(vec, sq, sq_mask);\n        __m256i masked_cb = _mm256_blendv_epi8(masked_sq, cb, cb_mask);\n\n        int temp_arr[8];\n        _mm256_storeu_si256((__m256i*)temp_arr, masked_cb);\n        for(int j = 0; j < 8; ++j){\n            sum += temp_arr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function will take a vector of integers. For all entries in the vector, the function shall square the integer entry if its index is a \nmultiple of 3 and will cube the integer entry if its index is a multiple of 4 and not a multiple of 3. The function will not \nchange the entries in the vector whose indexes are not a multiple of 3 or 4. The function shall then return the sum of all entries. \n\nExamples:\nFor lst = {1,2,3} the output should be 6\nFor lst = {}  the output should be 0\nFor lst = {-1,-5,2,-1,-5}  the output should be -126\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_3 = _mm256_set1_epi32(3);\n        __m256i mask_4 = _mm256_set1_epi32(4);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i mod_3 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_3), mask_3));\n        __m256i mod_4 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_4), mask_4));\n\n        __m256i cmp_3 = _mm256_cmpeq_epi32(mod_3, zero);\n        __m256i cmp_4 = _mm256_cmpeq_epi32(mod_4, zero);\n\n        __m256i not_cmp_3 = _mm256_xor_si256(cmp_3, _mm256_set1_epi32(-1));\n        __m256i mask_4_not_3 = _mm256_and_si256(cmp_4, not_cmp_3);\n\n        __m256i sq_mask = cmp_3;\n        __m256i cb_mask = mask_4_not_3;\n\n        __m256i sq = _mm256_mullo_epi32(vec, vec);\n        __m256i cb = _mm256_mullo_epi32(sq, vec);\n\n        __m256i masked_sq = _mm256_blendv_epi8(vec, sq, sq_mask);\n        __m256i masked_cb = _mm256_blendv_epi8(masked_sq, cb, cb_mask);\n\n        int temp_arr[8];\n        _mm256_storeu_si256((__m256i*)temp_arr, masked_cb);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_arr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function will take a vector of integers. For all entries in the vector, the function shall square the integer entry if its index is a \nmultiple of 3 and will cube the integer entry if its index is a multiple of 4 and not a multiple of 3. The function will not \nchange the entries in the vector whose indexes are not a multiple of 3 or 4. The function shall then return the sum of all entries. \n\nExamples:\nFor lst = {1,2,3} the output should be 6\nFor lst = {}  the output should be 0\nFor lst = {-1,-5,2,-1,-5}  the output should be -126\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_3 = _mm256_set1_epi32(3);\n        __m256i mask_4 = _mm256_set1_epi32(4);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i mod_3 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_3), mask_3));\n        __m256i mod_4 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_4), mask_4));\n\n        __m256i cmp_3 = _mm256_cmpeq_epi32(mod_3, zero);\n        __m256i cmp_4 = _mm256_cmpeq_epi32(mod_4, zero);\n\n        __m256i not_cmp_3 = _mm256_xor_si256(cmp_3, _mm256_set1_epi32(-1));\n        __m256i mask = _mm256_and_si256(cmp_4, not_cmp_3);\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(squared, vec);\n\n        vec = _mm256_blendv_epi8(vec, squared, cmp_3);\n        vec = _mm256_blendv_epi8(vec, cubed, mask);\n\n        int temp_sum[8];\n        _mm256_storeu_si256((__m256i*)temp_sum, vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function will take a vector of integers. For all entries in the vector, the function shall square the integer entry if its index is a \nmultiple of 3 and will cube the integer entry if its index is a multiple of 4 and not a multiple of 3. The function will not \nchange the entries in the vector whose indexes are not a multiple of 3 or 4. The function shall then return the sum of all entries. \n\nExamples:\nFor lst = {1,2,3} the output should be 6\nFor lst = {}  the output should be 0\nFor lst = {-1,-5,2,-1,-5}  the output should be -126\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_3 = _mm256_set1_epi32(3);\n        __m256i mask_4 = _mm256_set1_epi32(4);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i mod_3 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_3), mask_3));\n        __m256i mod_4 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_4), mask_4));\n\n        __m256i cmp_3 = _mm256_cmpeq_epi32(mod_3, zero);\n        __m256i cmp_4 = _mm256_cmpeq_epi32(mod_4, zero);\n\n        __m256i not_cmp_3 = _mm256_xor_si256(cmp_3, _mm256_set1_epi32(-1));\n        __m256i mask = _mm256_and_si256(cmp_4, not_cmp_3);\n\n        __m256i sq = _mm256_mullo_epi32(vec, vec);\n        __m256i cb = _mm256_mullo_epi32(vec, sq);\n\n        vec = _mm256_blendv_epi8(vec, sq, cmp_3);\n        vec = _mm256_blendv_epi8(vec, cb, mask);\n\n        int temp_sum[8];\n        _mm256_storeu_si256((__m256i*)temp_sum, vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nThis function will take a vector of integers. For all entries in the vector, the function shall square the integer entry if its index is a \nmultiple of 3 and will cube the integer entry if its index is a multiple of 4 and not a multiple of 3. The function will not \nchange the entries in the vector whose indexes are not a multiple of 3 or 4. The function shall then return the sum of all entries. \n\nExamples:\nFor lst = {1,2,3} the output should be 6\nFor lst = {}  the output should be 0\nFor lst = {-1,-5,2,-1,-5}  the output should be -126\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_3 = _mm256_set1_epi32(3);\n        __m256i mask_4 = _mm256_set1_epi32(4);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i mod_3 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_3), mask_3));\n        __m256i mod_4 = _mm256_sub_epi32(indices, _mm256_mullo_epi32(_mm256_div_epi32(indices, mask_4), mask_4));\n\n        __m256i cmp_3 = _mm256_cmpeq_epi32(mod_3, zero);\n        __m256i cmp_4 = _mm256_cmpeq_epi32(mod_4, zero);\n\n        __m256i not_cmp_3 = _mm256_xor_si256(cmp_3, _mm256_set1_epi32(-1));\n        __m256i mask = _mm256_and_si256(cmp_4, not_cmp_3);\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(squared, vec);\n\n        vec = _mm256_blendv_epi8(vec, squared, cmp_3);\n        vec = _mm256_blendv_epi8(vec, cubed, mask);\n\n        int temp_sum[8];\n        _mm256_storeu_si256((__m256i*)temp_sum, vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp_sum[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nWrite a function that takes a vector of numbers as input and returns \nthe number of elements in the vector that are greater than 10 and both \nfirst and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter_simd({15, -73, 14, -15}) => 1 \nspecialFilter_simd({33, -2, -3, 45, 21, 109}) => 2\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&nums[i]);\n            __m256i greater_than_10 = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(10));\n            \n            int mask = _mm256_movemask_epi8(greater_than_10);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    int num = nums[i + j];\n                    if (num > 0) {\n                        int last_digit = num % 10;\n                        int first_digit = num;\n                        while (first_digit >= 10) {\n                            first_digit /= 10;\n                        }\n                        if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                            count++;\n                        }\n                    } else {\n                        num = -num;\n                        int last_digit = num % 10;\n                        int first_digit = num;\n                        while (first_digit >= 10) {\n                            first_digit /= 10;\n                        }\n                        if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                            count++;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (nums[i] > 10) {\n            int num = nums[i];\n            if (num > 0) {\n                int last_digit = num % 10;\n                int first_digit = num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                    count++;\n                }\n            } else {\n                num = -num;\n                int last_digit = num % 10;\n                int first_digit = num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nWrite a function that takes a vector of numbers as input and returns \nthe number of elements in the vector that are greater than 10 and both \nfirst and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter_simd({15, -73, 14, -15}) => 1 \nspecialFilter_simd({33, -2, -3, 45, 21, 109}) => 2\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i *)&nums[i]);\n            __m256i greater_than_10 = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(10));\n            \n            int mask = _mm256_movemask_epi8(greater_than_10);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << (j * 4))) != 0) {\n                    int num = nums[i + j];\n                    if (num > 0) {\n                        int last_digit = num % 10;\n                        int first_digit = num;\n                        while (first_digit >= 10) {\n                            first_digit /= 10;\n                        }\n                        if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                            count++;\n                        }\n                    } else {\n                        num = -num;\n                        int last_digit = num % 10;\n                        int first_digit = num;\n                        while (first_digit >= 10) {\n                            first_digit /= 10;\n                        }\n                        if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                            count++;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (nums[i] > 10) {\n            int num = nums[i];\n            if (num > 0) {\n                int last_digit = num % 10;\n                int first_digit = num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                    count++;\n                }\n            } else {\n                num = -num;\n                int last_digit = num % 10;\n                int first_digit = num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nWrite a function that takes a vector of numbers as input and returns \nthe number of elements in the vector that are greater than 10 and both \nfirst and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter_simd({15, -73, 14, -15}) => 1 \nspecialFilter_simd({33, -2, -3, 45, 21, 109}) => 2\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&nums[i]);\n            \n            // Create a mask for elements > 10\n            __m256i ten = _mm256_set1_epi32(10);\n            __m256i greater_than_ten_mask = _mm256_cmpgt_epi32(vec, ten);\n\n            // Extract the elements that are greater than 10\n            int greater_than_ten_arr[8];\n            _mm256_storeu_si256((__m256i*)greater_than_ten_arr, greater_than_ten_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (greater_than_ten_arr[j] != 0) {\n                    int num = nums[i + j];\n                    if (num > 10) {\n                        int last_digit = abs(num) % 10;\n                        int first_digit = abs(num);\n                        while (first_digit >= 10) {\n                            first_digit /= 10;\n                        }\n\n                        if ((last_digit % 2 != 0) && (first_digit % 2 != 0)) {\n                            count++;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (nums[i] > 10) {\n            int last_digit = abs(nums[i]) % 10;\n            int first_digit = abs(nums[i]);\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n\n            if ((last_digit % 2 != 0) && (first_digit % 2 != 0)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nWrite a function that takes a vector of numbers as input and returns \nthe number of elements in the vector that are greater than 10 and both \nfirst and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter_simd({15, -73, 14, -15}) => 1 \nspecialFilter_simd({33, -2, -3, 45, 21, 109}) => 2\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)&nums[i]);\n            \n            // Create a mask for elements greater than 10\n            __m256i ten = _mm256_set1_epi32(10);\n            __m256i greater_than_ten_mask = _mm256_cmpgt_epi32(vec, ten);\n\n            // Extract the elements that are greater than 10\n            int greater_than_ten_elements[8];\n            _mm256_storeu_si256((__m256i*)greater_than_ten_elements, _mm256_and_si256(vec, greater_than_ten_mask));\n\n            for (int j = 0; j < 8; ++j) {\n                if (greater_than_ten_elements[j] > 10) {\n                    int num = greater_than_ten_elements[j];\n                    if (num < 0)\n                        num = -num;\n\n                    int last_digit = num % 10;\n                    int first_digit = num;\n                    while (first_digit >= 10) {\n                        first_digit /= 10;\n                    }\n\n                    if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (nums[i] > 10) {\n            int num = nums[i];\n            if (num < 0)\n                num = -num;\n\n            int last_digit = num % 10;\n            int first_digit = num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n\n            if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\n/*\nWrite a function that takes a vector of numbers as input and returns \nthe number of elements in the vector that are greater than 10 and both \nfirst and last digits of a number are odd (1, 3, 5, 7, 9).\nFor example:\nspecialFilter_simd({15, -73, 14, -15}) => 1 \nspecialFilter_simd({33, -2, -3, 45, 21, 109}) => 2\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int n = nums.size();\n    int i = 0;\n\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i vec = _mm256_loadu_si256((const __m256i *)&nums[i]);\n            \n            // Create a mask for elements > 10\n            __m256i ten = _mm256_set1_epi32(10);\n            __m256i greater_than_ten_mask = _mm256_cmpgt_epi32(vec, ten);\n\n            // Extract the elements that are greater than 10\n            int greater_than_ten_arr[8];\n            _mm256_storeu_si256((__m256i*)greater_than_ten_arr, greater_than_ten_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (greater_than_ten_arr[j] != 0) {\n                    int num = nums[i + j];\n                    if (num > 10) {\n                        int last_digit = abs(num) % 10;\n                        int first_digit = 0;\n                        int temp = abs(num);\n                        while (temp >= 10) {\n                            temp /= 10;\n                        }\n                        first_digit = temp;\n\n                        if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                            count++;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (nums[i] > 10) {\n            int last_digit = abs(nums[i]) % 10;\n            int first_digit = 0;\n            int temp = abs(nums[i]);\n            while (temp >= 10) {\n                temp /= 10;\n            }\n            first_digit = temp;\n\n            if (last_digit % 2 != 0 && first_digit % 2 != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a positive integer n. You have to create an integer vector a of length n.\n    For each i (1 \u2264 i \u2264 n), the value of a{i} = i * i - i + 1.\n    Return the number of triples (a{i}, a{j}, a{k}) of a where i < j < k, \nand a[i] + a[j] + a[k] is a multiple of 3.\n\nExample :\n    Input: n = 5\n    Output: 1\n    Explanation: \n    a = {1, 3, 7, 13, 21}\n    The only valid triple is (1, 7, 13).\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t get_max_triples_simd(uint64_t n){\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t val = (i * i - i + 1) % 3;\n        count[val]++;\n    }\n    return count[0] * (count[0] - 1) * (count[0] - 2) / 6 +\n           count[1] * (count[1] - 1) * (count[1] - 2) / 6 +\n           count[2] * (count[2] - 1) * (count[2] - 2) / 6 +\n           count[0] * count[1] * count[2];\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a positive integer n. You have to create an integer vector a of length n.\n    For each i (1 \u2264 i \u2264 n), the value of a{i} = i * i - i + 1.\n    Return the number of triples (a{i}, a{j}, a{k}) of a where i < j < k, \nand a[i] + a[j] + a[k] is a multiple of 3.\n\nExample :\n    Input: n = 5\n    Output: 1\n    Explanation: \n    a = {1, 3, 7, 13, 21}\n    The only valid triple is (1, 7, 13).\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t get_max_triples_simd(uint64_t n){\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t val = (i * i - i + 1) % 3;\n        count[val]++;\n    }\n    return count[0] * (count[0] - 1) * (count[0] - 2) / 6 +\n           count[1] * (count[1] - 1) * (count[1] - 2) / 6 +\n           count[2] * (count[2] - 1) * (count[2] - 2) / 6 +\n           count[0] * count[1] * count[2];\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a positive integer n. You have to create an integer vector a of length n.\n    For each i (1 \u2264 i \u2264 n), the value of a{i} = i * i - i + 1.\n    Return the number of triples (a{i}, a{j}, a{k}) of a where i < j < k, \nand a[i] + a[j] + a[k] is a multiple of 3.\n\nExample :\n    Input: n = 5\n    Output: 1\n    Explanation: \n    a = {1, 3, 7, 13, 21}\n    The only valid triple is (1, 7, 13).\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t get_max_triples_simd(uint64_t n){\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t val = (i * i - i + 1) % 3;\n        count[val]++;\n    }\n    return count[0] * (count[0] - 1) * (count[0] - 2) / 6 +\n           count[1] * (count[1] - 1) * (count[1] - 2) / 6 +\n           count[2] * (count[2] - 1) * (count[2] - 2) / 6 +\n           count[0] * count[1] * count[2];\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a positive integer n. You have to create an integer vector a of length n.\n    For each i (1 \u2264 i \u2264 n), the value of a{i} = i * i - i + 1.\n    Return the number of triples (a{i}, a{j}, a{k}) of a where i < j < k, \nand a[i] + a[j] + a[k] is a multiple of 3.\n\nExample :\n    Input: n = 5\n    Output: 1\n    Explanation: \n    a = {1, 3, 7, 13, 21}\n    The only valid triple is (1, 7, 13).\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t get_max_triples_simd(uint64_t n){\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t val = (i * i - i + 1) % 3;\n        count[val]++;\n    }\n    return count[0] * (count[0] - 1) * (count[0] - 2) / 6 +\n           count[1] * (count[1] - 1) * (count[1] - 2) / 6 +\n           count[2] * (count[2] - 1) * (count[2] - 2) / 6 +\n           count[0] * count[1] * count[2];\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nYou are given a positive integer n. You have to create an integer vector a of length n.\n    For each i (1 \u2264 i \u2264 n), the value of a{i} = i * i - i + 1.\n    Return the number of triples (a{i}, a{j}, a{k}) of a where i < j < k, \nand a[i] + a[j] + a[k] is a multiple of 3.\n\nExample :\n    Input: n = 5\n    Output: 1\n    Explanation: \n    a = {1, 3, 7, 13, 21}\n    The only valid triple is (1, 7, 13).\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t get_max_triples_simd(uint64_t n){\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t val = (i * i - i + 1) % 3;\n        count[val]++;\n    }\n    return count[0] * (count[0] - 1) * (count[0] - 2) / 6 +\n           count[1] * (count[1] - 1) * (count[1] - 2) / 6 +\n           count[2] * (count[2] - 1) * (count[2] - 2) / 6 +\n           count[0] * count[1] * count[2];\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nA simple program which should return the value of x if n is \na prime number and should return the value of y otherwise.\nExamples:\nfor x_or_y_simd(7, 34, 12) == 34\nfor x_or_y_simd(15, 8, 5) == 5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) return y;\n    if (n <= 3) return x;\n    if (n % 2 == 0 || n % 3 == 0) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(11, 9, 7, 5, 11, 9, 7, 5);\n    __m256i vec_6 = _mm256_set1_epi32(6);\n    __m256i vec_x = _mm256_set1_epi32(x);\n    __m256i vec_y = _mm256_set1_epi32(y);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int i = 5; i * i <= n; i += 6) {\n        __m256i vec_i_current = _mm256_add_epi32(vec_i, _mm256_set1_epi32(i - 5));\n        __m256i vec_n_mod_i = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i_current), vec_i_current));\n        __m256i cmp_result = _mm256_cmpeq_epi32(vec_n_mod_i, vec_zero);\n        if (!_mm256_testz_si256(cmp_result, cmp_result)) return y;\n        \n        __m256i vec_i_plus_2 = _mm256_add_epi32(vec_i_current, _mm256_set1_epi32(2));\n        __m256i vec_n_mod_i_plus_2 = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i_plus_2), vec_i_plus_2));\n        __m256i cmp_result_plus_2 = _mm256_cmpeq_epi32(vec_n_mod_i_plus_2, vec_zero);\n        if (!_mm256_testz_si256(cmp_result_plus_2, cmp_result_plus_2)) return y;\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nA simple program which should return the value of x if n is \na prime number and should return the value of y otherwise.\nExamples:\nfor x_or_y_simd(7, 34, 12) == 34\nfor x_or_y_simd(15, 8, 5) == 5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) return y;\n    if (n <= 3) return x;\n    if (n % 2 == 0 || n % 3 == 0) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(11, 9, 7, 5, 11, 9, 7, 5);\n    __m256i vec_6 = _mm256_set1_epi32(6);\n\n    for (int i = 5; i * i <= n; ) {\n        __m256i vec_i_squared = _mm256_mullo_epi32(vec_i, vec_i);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(vec_i_squared, vec_n);\n        int mask = _mm256_movemask_epi8(cmp_mask);\n        if(mask != -1){\n            break;\n        }\n\n        __m256i vec_n_mod_i = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i), vec_i));\n        __m256i vec_zero = _mm256_setzero_si256();\n        cmp_mask = _mm256_cmpeq_epi32(vec_n_mod_i, vec_zero);\n        mask = _mm256_movemask_epi8(cmp_mask);\n\n        if (mask != 0) return y;\n\n        vec_i = _mm256_add_epi32(vec_i, vec_6);\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec_i);\n        i = arr[3];\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nA simple program which should return the value of x if n is \na prime number and should return the value of y otherwise.\nExamples:\nfor x_or_y_simd(7, 34, 12) == 34\nfor x_or_y_simd(15, 8, 5) == 5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) return y;\n    if (n <= 3) return x;\n    if (n % 2 == 0 || n % 3 == 0) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(11, 9, 7, 5, 11, 9, 7, 5);\n    __m256i vec_6 = _mm256_set1_epi32(6);\n\n    for (int i = 5; i * i <= n; ) {\n        __m256i vec_i_squared = _mm256_mullo_epi32(vec_i, vec_i);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(vec_i_squared, vec_n);\n        int mask = _mm256_movemask_epi8(cmp_mask);\n\n        if(mask == -1){\n            break;\n        }\n\n        __m256i vec_n_mod_i = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i), vec_i));\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mod_is_zero = _mm256_cmpeq_epi32(vec_n_mod_i, zero);\n        int mod_mask = _mm256_movemask_epi8(mod_is_zero);\n\n        if (mod_mask != 0) return y;\n\n        vec_i = _mm256_add_epi32(vec_i, vec_6);\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec_i);\n        i = arr[3];\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nA simple program which should return the value of x if n is \na prime number and should return the value of y otherwise.\nExamples:\nfor x_or_y_simd(7, 34, 12) == 34\nfor x_or_y_simd(15, 8, 5) == 5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) return y;\n    if (n <= 3) return x;\n    if (n % 2 == 0 || n % 3 == 0) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(11, 9, 7, 5, 11, 9, 7, 5);\n    __m256i vec_inc = _mm256_set1_epi32(6);\n    \n    for (int i = 5; i * i <= n; ) {\n        __m256i vec_i_squared = _mm256_mullo_epi32(vec_i, vec_i);\n        __m256i cmp_result = _mm256_cmpgt_epi32(vec_i_squared, vec_n);\n        int mask = _mm256_movemask_epi8(cmp_result);\n        if(mask != -1){\n            break;\n        }\n\n        __m256i vec_n_mod_i = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i), vec_i));\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i mod_cmp = _mm256_cmpeq_epi32(vec_n_mod_i, vec_zero);\n        int mod_mask = _mm256_movemask_epi8(mod_cmp);\n        if(mod_mask != 0){\n            return y;\n        }\n        \n        __m256i vec_i_plus_2 = _mm256_add_epi32(vec_i, _mm256_set1_epi32(2));\n        vec_n_mod_i = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i_plus_2), vec_i_plus_2));\n        mod_cmp = _mm256_cmpeq_epi32(vec_n_mod_i, vec_zero);\n        mod_mask = _mm256_movemask_epi8(mod_cmp);\n        if(mod_mask != 0){\n            return y;\n        }\n\n        vec_i = _mm256_add_epi32(vec_i, vec_inc);\n        i += 30;\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nA simple program which should return the value of x if n is \na prime number and should return the value of y otherwise.\nExamples:\nfor x_or_y_simd(7, 34, 12) == 34\nfor x_or_y_simd(15, 8, 5) == 5\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nint x_or_y_simd(int n, int x, int y){\n    if (n <= 1) return y;\n    if (n <= 3) return x;\n    if (n % 2 == 0 || n % 3 == 0) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_i = _mm256_set_epi32(11, 9, 7, 5, 11, 9, 7, 5);\n    __m256i vec_6 = _mm256_set1_epi32(6);\n\n    for (int i = 5; i * i <= n; i += 30) {\n        __m256i vec_i_current = _mm256_add_epi32(_mm256_set1_epi32(i), vec_i);\n        __m256i vec_rem = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i_current), vec_i_current));\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_rem, _mm256_setzero_si256());\n\n        if (_mm256_movemask_epi8(vec_cmp) != 0) return y;\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return the sum of squares of the numbers\nin the vector that are odd. Ignore numbers that are negative or not integers.\n\ndouble_the_difference_simd({1, 3, 2, 0}) == 1 + 9 + 0 + 0 = 10\ndouble_the_difference_simd({-1, -2, 0}) == 0\ndouble_the_difference_simd({9, -2}) == 81\ndouble_the_difference_simd({0}) == 0  \n\nIf the input vector is empty, return 0.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    if (size == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 trunc = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO);\n        __m256 diff = _mm256_sub_ps(vec, trunc);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 cmp_res = _mm256_cmp_ps(abs_diff, _mm256_set1_ps(1e-6), _CMP_LT_OS);\n\n        __m256 is_integer = _mm256_and_ps(cmp_res, _mm256_cmp_ps(vec, _mm256_set1_ps(0.0f), _CMP_GE_OS));\n\n        __m256 mod_2 = _mm256_and_ps(trunc, _mm256_set1_ps(1.0f));\n        __m256 is_odd = _mm256_cmp_ps(mod_2, _mm256_set1_ps(1.0f), _CMP_EQ_OS);\n\n        __m256 valid = _mm256_and_ps(is_integer, is_odd);\n\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked_squared = _mm256_and_ps(squared, valid);\n\n        float temp_arr[8];\n        _mm256_storeu_ps(temp_arr, masked_squared);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_arr[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] >= 0 && (lst[i] - (int)lst[i]) < 1e-6 && (int)lst[i] % 2 != 0) {\n            sum += (uint64_t)(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return the sum of squares of the numbers\nin the vector that are odd. Ignore numbers that are negative or not integers.\n\ndouble_the_difference_simd({1, 3, 2, 0}) == 1 + 9 + 0 + 0 = 10\ndouble_the_difference_simd({-1, -2, 0}) == 0\ndouble_the_difference_simd({9, -2}) == 81\ndouble_the_difference_simd({0}) == 0  \n\nIf the input vector is empty, return 0.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    if (size == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 trunc = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO);\n        __m256 diff = _mm256_sub_ps(vec, trunc);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        __m256 cmp_res = _mm256_cmp_ps(abs_diff, _mm256_set1_ps(1e-6), _CMP_LT_OS);\n\n        __m256 is_integer = _mm256_and_ps(cmp_res, _mm256_cmp_ps(vec, _mm256_set1_ps(0.0f), _CMP_GE_OS));\n\n        __m256 mod_2 = _mm256_and_ps(trunc, _mm256_set1_ps(1.0f));\n        __m256 is_odd = _mm256_cmp_ps(mod_2, _mm256_set1_ps(1.0f), _CMP_EQ_OS);\n\n        __m256 valid = _mm256_and_ps(is_integer, is_odd);\n\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked_squared = _mm256_and_ps(squared, valid);\n\n        float temp_arr[8];\n        _mm256_storeu_ps(temp_arr, masked_squared);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_arr[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] >= 0 && lst[i] == (int)lst[i] && (int)lst[i] % 2 != 0) {\n            sum += (uint64_t)(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return the sum of squares of the numbers\nin the vector that are odd. Ignore numbers that are negative or not integers.\n\ndouble_the_difference_simd({1, 3, 2, 0}) == 1 + 9 + 0 + 0 = 10\ndouble_the_difference_simd({-1, -2, 0}) == 0\ndouble_the_difference_simd({9, -2}) == 81\ndouble_the_difference_simd({0}) == 0  \n\nIf the input vector is empty, return 0.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    if (size == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 trunc = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO);\n        __m256 diff = _mm256_sub_ps(vec, trunc);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 cmp_res = _mm256_cmp_ps(abs_diff, _mm256_set1_ps(0.0001), _CMP_LT_OS);\n\n        __m256 is_odd = _mm256_and_ps(cmp_res, _mm256_and_ps(vec, _mm256_set1_ps(1.0f)));\n        \n        __m256 is_positive = _mm256_cmp_ps(vec, _mm256_set1_ps(0.0f), _CMP_GE_OS);\n        __m256 valid = _mm256_and_ps(is_odd, is_positive);\n\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked_squared = _mm256_and_ps(squared, valid);\n\n        float temp_arr[8];\n        _mm256_storeu_ps(temp_arr, masked_squared);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_arr[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] >= 0 && (int)lst[i] == lst[i] && ((int)lst[i] % 2 != 0)) {\n            sum += (uint64_t)(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return the sum of squares of the numbers\nin the vector that are odd. Ignore numbers that are negative or not integers.\n\ndouble_the_difference_simd({1, 3, 2, 0}) == 1 + 9 + 0 + 0 = 10\ndouble_the_difference_simd({-1, -2, 0}) == 0\ndouble_the_difference_simd({9, -2}) == 81\ndouble_the_difference_simd({0}) == 0  \n\nIf the input vector is empty, return 0.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    if (size == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 rounded = _mm256_round_ps(vec, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256 diff = _mm256_sub_ps(vec, rounded);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 is_integer = _mm256_cmp_ps(abs_diff, _mm256_setzero_ps(), _CMP_EQ_OQ);\n\n        __m256 is_positive = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GE_OQ);\n        __m256 is_odd = _mm256_and_ps(is_integer, _mm256_and_ps(is_positive, _mm256_castsi256_ps(_mm256_set1_epi32(1))));\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked_squared = _mm256_and_ps(squared, is_odd);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, masked_squared);\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] >= 0 && (lst[i] - (int)lst[i]) == 0 && ((int)lst[i] % 2) != 0) {\n            sum += (uint64_t)(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nGiven a vector of numbers, return the sum of squares of the numbers\nin the vector that are odd. Ignore numbers that are negative or not integers.\n\ndouble_the_difference_simd({1, 3, 2, 0}) == 1 + 9 + 0 + 0 = 10\ndouble_the_difference_simd({-1, -2, 0}) == 0\ndouble_the_difference_simd({9, -2}) == 81\ndouble_the_difference_simd({0}) == 0  \n\nIf the input vector is empty, return 0.\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    int size = lst.size();\n    if (size == 0) return 0;\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 trunc = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO);\n        __m256 diff = _mm256_sub_ps(vec, trunc);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 cmp_diff = _mm256_cmp_ps(abs_diff, zero, _CMP_EQ_OQ);\n\n        __m256 is_positive = _mm256_cmp_ps(vec, zero, _CMP_GE_OQ);\n        __m256 is_integer = cmp_diff;\n        __m256 is_valid = _mm256_and_ps(is_positive, is_integer);\n\n        __m256 mod_result = _mm256_fmod_ps(trunc, _mm256_set1_ps(2.0f));\n        __m256 is_odd = _mm256_cmp_ps(mod_result, _mm256_set1_ps(1.0f), _CMP_EQ_OQ);\n\n        __m256 is_odd_and_valid = _mm256_and_ps(is_odd, is_valid);\n\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked_squared = _mm256_and_ps(squared, is_odd_and_valid);\n\n        float temp_sum[8];\n        _mm256_storeu_ps(temp_sum, masked_squared);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += (uint64_t)temp_sum[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] >= 0 && lst[i] == (int)lst[i] && (int)lst[i] % 2 != 0) {\n            sum += (uint64_t)(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nI think we all remember that feeling when the result of some long-awaited\nevent is finally known. The feelings and thoughts you have at that moment are\ndefinitely worth noting down and comparing.\nYour task is to determine if a person correctly guessed the results of a number of matches.\nYou are given two vectors of scores and guesses of equal length, where each index shows a match. \nReturn a vector of the same length denoting how far off each guess was. If they have guessed correctly,\nthe value is 0, and if not, the value is the absolute difference between the guess and the score.\nExample:\ncompare_simd({1,2,3,4,5,1},{1,2,3,4,2,-2}) -> {0,0,0,0,3,3}\ncompare_simd({0,5,0,0,0,4},{4,1,1,0,0,-2}) -> {4,4,1,0,0,6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    std::vector<int> result(game.size());\n    int i = 0;\n    int size = game.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n            __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n\n            __m256i cmp_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n            __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n            __m256i abs_vec = _mm256_abs_epi32(diff_vec);\n\n            __m256i res_vec = _mm256_blendv_epi8(abs_vec, _mm256_setzero_si256(), cmp_vec);\n\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (game[i] == guess[i]) {\n            result[i] = 0;\n        } else {\n            result[i] = std::abs(game[i] - guess[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nI think we all remember that feeling when the result of some long-awaited\nevent is finally known. The feelings and thoughts you have at that moment are\ndefinitely worth noting down and comparing.\nYour task is to determine if a person correctly guessed the results of a number of matches.\nYou are given two vectors of scores and guesses of equal length, where each index shows a match. \nReturn a vector of the same length denoting how far off each guess was. If they have guessed correctly,\nthe value is 0, and if not, the value is the absolute difference between the guess and the score.\nExample:\ncompare_simd({1,2,3,4,5,1},{1,2,3,4,2,-2}) -> {0,0,0,0,3,3}\ncompare_simd({0,5,0,0,0,4},{4,1,1,0,0,-2}) -> {4,4,1,0,0,6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    std::vector<int> result(game.size());\n    int i = 0;\n    int size = game.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n            __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n\n            __m256i cmp_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n            __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n            __m256i abs_vec = _mm256_abs_epi32(diff_vec);\n\n            __m256i res_vec = _mm256_blendv_epi8(abs_vec, _mm256_setzero_si256(), cmp_vec);\n\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (game[i] == guess[i]) {\n            result[i] = 0;\n        } else {\n            result[i] = std::abs(game[i] - guess[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n);\n\n    int i = 0;\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n            __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n\n            __m256i cmp_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n            __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n\n            __m256i abs_vec = _mm256_abs_epi32(diff_vec);\n\n            __m256i mask = _mm256_blendv_epi8(abs_vec, _mm256_setzero_si256(), cmp_vec);\n\n            _mm256_storeu_si256((__m256i*)&result[i], mask);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (game[i] == guess[i]) {\n            result[i] = 0;\n        } else {\n            result[i] = std::abs(game[i] - guess[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\n/*\nI think we all remember that feeling when the result of some long-awaited\nevent is finally known. The feelings and thoughts you have at that moment are\ndefinitely worth noting down and comparing.\nYour task is to determine if a person correctly guessed the results of a number of matches.\nYou are given two vectors of scores and guesses of equal length, where each index shows a match. \nReturn a vector of the same length denoting how far off each guess was. If they have guessed correctly,\nthe value is 0, and if not, the value is the absolute difference between the guess and the score.\nExample:\ncompare_simd({1,2,3,4,5,1},{1,2,3,4,2,-2}) -> {0,0,0,0,3,3}\ncompare_simd({0,5,0,0,0,4},{4,1,1,0,0,-2}) -> {4,4,1,0,0,6}\n\nThe requirement is to implement the function using AVX/AVX2 (Advanced Vector Extensions) intrinsics to achieve parallelism.\n*/\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    std::vector<int> result(game.size());\n    int i = 0;\n    int size = game.size();\n\n    if (size >= 8) {\n        for (; i <= size - 8; i += 8) {\n            __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n            __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n\n            __m256i cmp_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n            __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n            __m256i abs_vec = _mm256_abs_epi32(diff_vec);\n\n            __m256i res_vec = _mm256_blendv_epi8(abs_vec, _mm256_setzero_si256(), cmp_vec);\n\n            _mm256_storeu_si256((__m256i*)&result[i], res_vec);\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (game[i] == guess[i]) {\n            result[i] = 0;\n        } else {\n            result[i] = std::abs(game[i] - guess[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n);\n\n    int i = 0;\n    if (n >= 8) {\n        for (; i <= n - 8; i += 8) {\n            __m256i game_vec = _mm256_loadu_si256((const __m256i*)&game[i]);\n            __m256i guess_vec = _mm256_loadu_si256((const __m256i*)&guess[i]);\n\n            __m256i cmp_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n            __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n\n            __m256i abs_vec = _mm256_abs_epi32(diff_vec);\n\n            __m256i mask = _mm256_blendv_epi8(abs_vec, _mm256_setzero_si256(), cmp_vec);\n\n            _mm256_storeu_si256((__m256i*)&result[i], mask);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (game[i] == guess[i]) {\n            result[i] = 0;\n        } else {\n            result[i] = std::abs(game[i] - guess[i]);\n        }\n    }\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_RVV", "completion": "```\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvli(0, RVV_E64, RVV_M8);\n    for (size_t i = 0; i < length; i += vl) {\n        vbool8_t vm = vmca_v_x_b(*mask, vl);\n        vint64m8_t vsrc = vle8_v_i64m8(&src[i], vm);\n        vse8_v_i64m8(&dst[i], vsrc, vm);\n    }\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += 8) {\n        vint64m8_t v_src = vle64_v_i64m8(&src[i]);\n        vint64m8_t v_dst = vle64_v_i64m8(&dst[i]);\n        vbool8_t v_mask = vmsne_v_b8_b8(&mask[i], false, 8);\n        v_dst = vmerge_vvm_i64m8(v_mask, v_src, v_dst);\n        vse64_v_i64m8(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    const int64_t *src_ptr = src;\n    int64_t *dst_ptr = dst;\n    const bool *mask_ptr = mask;\n    const int64_tvl vlen = vsetvl_e64m1();\n\n    for (i = 0; i < length; i += vlen) {\n        vbool_t vmask = vmset_v_i64m1(*mask_ptr, vlen);\n        vint64m1_t vsrc = vle64_v_i64m1(src_ptr, vmask, vlen);\n        vse64_v_i64m1(dst_ptr, vsrc, vmask, vlen);\n\n        src_ptr += vlen;\n        dst_ptr += vlen;\n        mask_ptr += vlen;\n    }\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvli(0, RVV_E64, RVV_M8);\n    for (size_t i = 0; i < length; i += vl) {\n        vbool8_t vm = vmv_x_s(mask + i, vl);\n        vint64m8_t v_src = vle64_v_i64m8(src + i, vm);\n        vse64_v_i64m8(dst + i, v_src, vm);\n    }\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    size_t vl = vsetvli(0, RVV_E64, RVV_M8);\n    for (i = 0; i < length; i += vl) {\n        vbool8_t vm = VMSET(0);\n        vint64m8_t v_src = vle64_v_i64m8(&src[i], vm);\n        vbool8_t v_mask = vmsne_v_b64m8_b64m8(vle64_v_i64m8(&mask[i], vm), VMSET(0), vm);\n        vse64_v_i64m8(&dst[i], vmerge_vxm_i64m8(v_src, 0, v_mask, vm), vm);\n    }\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    int32_t step = 32;\n    int32_t vl = vsetvl_e32m1(step);\n\n    for (size_t i = 0; i < length; i += step) {\n        vint32m1_t vin = vle32_v_i32m1(&src[i], vl);\n        vint32m1_t tmp1 = vsra_vx_i32m1(vmul_vx_i32m1(vin, scale, vl), 3, vl);\n        vint32m1_t tmp2 = vsll_vx_i32m1(vmul_vx_i32m1(vin, scale, vl), 3, vl);\n        vint32m1_t result = vdiv_vx_i32m1(vadd_vv_i32m1(vmul_vx_i32m1(vin, scale, vl), vmul_vv_i32m1(tmp1, tmp2, vl), vl), 7, vl);\n        vse32_v_i32m1(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    int32_t step = vsetvl_e32m1(0);\n    int32_t v_scale = vsll_vx_i32m1(vmv_v_x_i32m1(scale), 3);\n    int32_t v_scale_shifted = vsra_vx_i32m1(vmv_v_x_i32m1(scale), 3);\n\n    for (size_t i = 0; i < length; i += step) {\n        step = vsetvl_e32m1(length - i);\n        vint32m1_t v_src = vle32_v_i32m1(&src[i], step);\n        vint32m1_t v_tmp1 = vsra_vv_i32m1(vwmul_vv_i64m1(v_src, v_scale, step), 3, step);\n        vint32m1_t v_tmp2 = vsll_vv_i32m1(vwmul_vv_i64m1(v_src, v_scale_shifted, step), 3, step);\n        vint32m1_t v_result = vdiv_vv_i32m1(vadd_vv_i32m1(vwmul_vv_i64m1(v_src, v_scale, step), vwmul_vv_i64m1(v_tmp1, v_tmp2, step), step), 7, step);\n        vse32_v_i32m1(&dst[i], v_result, step);\n    }\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    int32_t step = vsetvl_e32m1(0);\n    int32_t v_scale = vmv_s_x_i32(scale, step);\n    int32_t v_shift = vmv_s_x_i32(3, step);\n    int32_t v_divisor = vmv_s_x_i32(7, step);\n\n    for (size_t i = 0; i < length; i += step) {\n        step = vsetvl_e32m1(length - i);\n\n        vint32m1_t v_src = vle32_v_i32m1(&src[i], step);\n        vint32m1_t v_tmp1 = vssra_vx_i32m1(vwmul_vx_i32m1(v_src, v_scale, step), 3, step);\n        vint32m1_t v_tmp2 = vsll_vx_i32m1(vwmul_vx_i32m1(v_src, v_scale, step), 3, step);\n        vint32m1_t v_result = vdiv_vx_i32m1(vadd_vv_i32m1(vwmul_vv_i32m1(v_src, v_scale, step), vwmul_vv_i32m1(v_tmp1, v_tmp2, step), step), v_divisor, step);\n\n        vse32_v_i32m1(&dst[i], v_result, step);\n    }\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    int32_t step = vsetvl_e32m1(0);\n    int32_t v_scale = vmv_s_x_i32(scale, step);\n    int32_t v_shifted_scale_right = vsra_vx_i32m1_i32(v_scale, 3, step);\n    int32_t v_shifted_scale_left = vsll_vx_i32m1_i32(v_scale, 3, step);\n    int32_t v_divisor = vmv_s_x_i32(7, step);\n\n    for (size_t i = 0; i < length; i += VL) {\n        step = vsetvl_e32m1(length - i);\n\n        vint32m1_t v_src = vle32_v_i32m1(&src[i], step);\n        vint32m1_t v_tmp1 = vssra_vx_i32m1(vwmul_vv_i64m1(v_src, v_scale, step), 3, step);\n        vint32m1_t v_tmp2 = vsll_vx_i32m1_i32(vwmul_vv_i64m1(v_src, v_scale, step), 3, step);\n        vint32m1_t v_result = vdiv_vv_i32m1(vadd_vv_i32m1(vwmul_vv_i64m1(v_src, v_scale, step), vwmul_vv_i64m1(v_tmp1, v_tmp2, step), step), v_divisor, step);\n\n        vse32_v_i32m1(&dst[i], v_result, step);\n    }\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    int32_t step = vsetvl_e32m1(0);\n    int32_t v_scale = vsll_vx_i32m1(vmv_v_x_i32m1(scale), 3);\n    int32_t v_scale_shifted = vsra_vx_i32m1(vmv_v_x_i32m1(scale), 3);\n    int32_t v_seven = vmv_v_x_i32m1(7);\n\n    for (size_t i = 0; i < length; i += step) {\n        step = vsetvl_e32m1(length - i);\n\n        vint32m1_t v_src = vle32_v_i32m1(&src[i], step);\n        vint32m1_t v_tmp1 = vdiv_vv_i32m1(vwmul_vv_i32m1(v_src, v_scale, step), v_seven, step);\n        vint32m1_t v_tmp2 = vdiv_vv_i32m1(vwmul_vv_i32m1(v_src, v_scale_shifted, step), v_seven, step);\n        vint32m1_t v_result = vadd_vv_i32m1(v_tmp1, v_tmp2, step);\n\n        vse32_v_i32m1(&dst[i], v_result, step);\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    vfloat64m8_t v_src, v_dst;\n\n    for (i = 0; i < rows; i++) {\n        v_src = vlse64_v_f64m8(src + i * cols, stride * sizeof(double));\n        v_dst = vfmv_v_f_f64m8(v_src);\n        vse64_v_f64m8(dst + i, v_dst);\n    }\n}\n```  "}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    vfloat64m8_t v_src, v_dst;\n    vint64m8_t v_stride = vmv_v_x_i64m8(stride);\n\n    for (i = 0; i < rows; i++) {\n        size_t src_index = i * cols + stride;\n        size_t dst_index = i;\n        v_src = vlse64_v_f64m8(src + src_index, v_stride);\n        v_dst = vfmv_v_f_f64m8(v_src);\n        vse64_v_f64m8(dst + dst_index, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    vfloat64m8_t v_src, v_dst;\n\n    for (i = 0; i < rows; i++) {\n        v_src = vlse64_v_f64m8(src + i * cols, stride * sizeof(double));\n        v_dst = vfmv_v_f_f64m8_f64(v_src);\n        vse64_v_f64m8(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    vfloat64m8_t v_src;\n    vfloat64m8_t v_dst;\n\n    for (i = 0; i < rows; i++) {\n        v_src = vlse64_v_f64m8(src + i * cols, stride * sizeof(double));\n        v_dst = vfmv_v_f_f64m8(v_src);\n        vse64_v_f64m8(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    size_t vec_len = vlenf64;\n    size_t remaining = cols % vec_len;\n    size_t full_vecs = cols - remaining;\n\n    for (i = 0; i < rows; i++) {\n        size_t offset = i * cols + stride;\n        vfloat64m8_t v_src = vle64_v_f64m8(&src[offset]);\n        vse64_v_f64m8(&dst[i], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const int vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t src_vec = vle32_v_f32m1(&src[i], vl);\n        vint32m1_t idx_vec = vle32_v_i32m1(&indices[i], vl);\n\n        vfloat32m1_t gather_result = vrgather_vx_f32m1(src_vec, idx_vec, vl);\n        vse32_v_f32m1(&dst[i], gather_result, vl);\n\n        vse32_v_f32m1(&dst[i], src_vec, idx_vec, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    int vl = vsetvli(0, RVV_E32, RVV_M8);\n    vfloat32m8_t src_v, dst_v;\n    vint32m8_t indices_v;\n\n    for (size_t i = 0; i < length; i += VL) {\n        src_v = vle32_v_f32m8(&src[i]);\n        indices_v = vle32_v_i32m8(&indices[i]);\n\n        dst_v = vrgather_vx_f32m8(src_v, indices_v, vl);\n        vse32_v_f32m8(&dst[i], dst_v);\n\n        dst_v = vrgather_vx_f32m8(src_v, indices_v, vl);\n        vssseg_v_f32m8(&dst[i], indices_v, dst_v, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const int vl = vsetvli(0, RVV_E32, RVV_M8);\n    const int *indices_ptr = indices;\n    float32_t *src_ptr = (float32_t *)src;\n    float32_t *dst_ptr = (float32_t *)dst;\n\n    for (size_t i = 0; i < length; i += VL) {\n        vfloat32m8_t src_vec = vle32_v_f32m8(src_ptr + i, vl);\n        vint32m8_t indices_vec = vle32_v_i32m8(indices_ptr + i, vl);\n\n        vfloat32m8_t gather_result = vrgather_vx_f32m8(src_vec, indices_vec, vl);\n        vse32_v_f32m8(dst_ptr + i, gather_result, vl);\n\n        vfloat32m8_t scatter_result = vrgather_vx_f32m8(src_vec, indices_vec, vl);\n        vssseg_v_f32m8(dst_ptr, indices_vec, scatter_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const int vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n    vfloat32m1_t v_dst_gather = vrgather_vx_f32m1(v_src, v_indices, vl);\n    vse32_v_f32m1(dst, v_dst_gather, vl);\n    vfloat32m1_t v_dst_scatter = vle32_v_f32m1(dst, vl);\n    vse32_v_f32m1(dst, vscatter_vx_f32m1(v_dst_scatter, v_indices, vl), vl);\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    int vl = vsetvli(0, RVV_E32, RVV_M8);\n    vfloat32m8_t src_v, dst_v;\n    vint32m8_t indices_v;\n\n    for (size_t i = 0; i < length; i += VL) {\n        size_t chunk_size = (i + VL <= length) ? VL : (length - i);\n\n        indices_v = vle32_v_i32m8(&indices[i], vl);\n        src_v = vle32_v_f32m8(&src[i], vl);\n\n        dst_v = vgather_vv_f32m8(dst_v, src_v, indices_v, vl);\n        vse32_v_f32m8(&dst[i], dst_v, vl);\n\n        dst_v = vle32_v_f32m8(&dst[i], vl);\n        vscatter_vv_f32m8(&dst[i], src_v, indices_v, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int64_t length = end - start + 1;\n    int64_t i;\n    int16_t *ptr = arr + start;\n    int16x8_t vec1, vec2;\n\n    for (i = 0; i < length / 16; i++) {\n        vec1 = vle16_v_i16m8(ptr + i * 16, VL8);\n        vec2 = vle16_v_i16m8(ptr + length - 16 - i * 16, VL8);\n        vse16_v_i16m8(ptr + i * 16, vec2, VL8);\n        vse16_v_i16m8(ptr + length - 16 - i * 16, vec1, VL8);\n    }\n\n    for (i = length - length % 16; i < length; i++) {\n        int16_t temp = ptr[i];\n        ptr[i] = ptr[length - 1 - i];\n        ptr[length - 1 - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int64_t length = end - start + 1;\n    int64_t full_chunks = length / 16;\n    int64_t remainder = length % 16;\n\n    vint16m8_t vec;\n    for (int64_t i = 0; i < full_chunks; i++) {\n        vec = vlh_v_i16m8(&arr[start + i * 16]);\n        vec = vrgather_vx_i16m8(vec, &arr[end - i * 16]);\n        vse_v_i16m8(&arr[start + i * 16], vec);\n    }\n\n    if (remainder > 0) {\n        vec = vlh_v_i16m8(&arr[start + full_chunks * 16]);\n        vec = vrgather_vx_i16m8(vec, &arr[end - full_chunks * 16]);\n        vse_v_i16m8(&arr[start + full_chunks * 16], vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int64_t length = end - start + 1;\n    int64_t full_chunks = length / 16;\n    int64_t remainder = length % 16;\n\n    int16_t *ptr = arr + start;\n\n    // Reverse full chunks\n    for (int64_t i = 0; i < full_chunks; i++) {\n        vint16m8_t vec = vlseg16_v_i16m8(ptr);\n        vec = vslideup_vx_i16m8(vec, 8);\n        vse16_v_i16m8(ptr, vec);\n        ptr += 16;\n    }\n\n    // Reverse remaining elements\n    if (remainder > 0) {\n        vint16m8_t vec = vlseg16_v_i16m8(ptr);\n        vec = vslideup_vx_i16m8(vec, 8);\n        vse16_v_i16m8(ptr, vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int64_t length = end - start + 1;\n    int64_t full_chunks = length / 16;\n    int64_t remainder = length % 16;\n\n    int16_t *ptr = arr + start;\n\n    // Reverse full chunks\n    for (int64_t i = 0; i < full_chunks; i++) {\n        vint16m8_t vec = vlh_v_i16m8(ptr + i * 16);\n        vec = vrgather_vx_i16m8(vec, ptr + (full_chunks - i - 1) * 16);\n        vse_v_i16m8(ptr + i * 16, vec);\n    }\n\n    // Reverse remaining elements\n    if (remainder > 0) {\n        vint16m8_t vec = vlh_v_i16m8(ptr + full_chunks * 16);\n        vec = vrgather_vx_i16m8(vec, ptr + length - 1);\n        vse_v_i16m8(ptr + full_chunks * 16, vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int16_t *ptr = arr + start;\n    int64_t length = end - start + 1;\n    int64_t num_lanes = vsetvl_e16m1(0);\n\n    for (int64_t i = 0; i < length / num_lanes; i++) {\n        vint16m1_t vec = vlseg2h_v_i16m1(ptr);\n        vec = vrev_v_i16m1(vec);\n        vseg2h_v_i16m1(ptr, vec);\n        ptr += num_lanes * 2;\n    }\n\n    int64_t remaining = length % num_lanes;\n    if (remaining > 0) {\n        vint16m1_t vec = vlseg2h_v_i16m1(ptr);\n        vec = vrev_v_i16m1(vec);\n        vse2h_v_i16m1(ptr, vec);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_idx = slice_idx * slice_size;\n    \n    size_t i;\n    for (i = 0; i < slice_size; i += VL) {\n        vuint8m8_t v_src = vle8_v_u8m8(&tensor[start_idx + i]);\n        vse8_v_u8m8(&slice[i], v_src);\n    }\n}\n```  "}
{"task_id": "SimdBench_5_RVV", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_idx = slice_idx * slice_size;\n    \n    for (size_t i = 0; i < dim1; i += 8) {\n        vuint8m8_t v_slice = vle8_v_u8m8(&tensor[start_idx + i * dim2]);\n        vse8_v_u8m8(&slice[i * dim2], v_slice);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_idx = slice_idx * slice_size;\n    \n    for (size_t i = 0; i < dim1; i += 8) {\n        vuint8m8_t v_src = vle8ff_v_u8m8(&tensor[start_idx + i * dim2]);\n        vse8_v_u8m8(&slice[i * dim2], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_idx = slice_idx * slice_size;\n    \n    vuint8m8_t v_slice;\n    vuint8m8_t v_tensor = vle8_v_u8m8(tensor + start_idx);\n    \n    for (size_t i = 0; i < slice_size; i += VL * sizeof(uint8_t)) {\n        v_slice = vle8_v_u8m8(slice + i);\n        vse8_v_u8m8(slice + i, v_tensor);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_idx = slice_idx * slice_size;\n    \n    size_t i;\n    for (i = 0; i < slice_size; i += VL) {\n        vuint8m8_t v_src = vle8_v_u8m8(&tensor[start_idx + i]);\n        vse8_v_u8m8(&slice[i], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += VL) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += VL) {\n                    vint16m8_t vin = vle16_v_i16m8(&src[ii * cols + jj]);\n                    vse16_v_i16m8(&dst[jj * rows + ii], vin);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    int16_t *src_ptr, *dst_ptr;\n    vint16m8_t src_vec, dst_vec;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    src_ptr = (int16_t *)(src + ii * cols + jj);\n                    dst_ptr = (int16_t *)(dst + jj * rows + ii);\n                    src_vec = vlse16_v_i16m8(src_ptr, 2);\n                    vse16_v_i16m8(dst_ptr, src_vec, 2);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    vint16m8_t vin, vout;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    vin = vlse16_v_i16m8(&src[ii * cols + jj], cols * sizeof(int16_t));\n                    vout = vtranspose_v_i16m8(vin);\n                    vse16_v_i16m8(&dst[jj * rows + ii], vout, cols * sizeof(int16_t));\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    int16_t *src_ptr, *dst_ptr;\n    vint16m4_t src_vec, dst_vec;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 4) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 4) {\n                    src_ptr = (int16_t *)(src + ii * cols + jj);\n                    dst_ptr = (int16_t *)(dst + jj * rows + ii);\n\n                    src_vec = vlh_v_i16m4(src_ptr, cols - jj < 4 ? cols - jj : 4);\n                    dst_vec = vtranspose_v_i16m4(src_vec);\n\n                    vsh_v_i16m4(dst_ptr, dst_vec, rows - ii < 4 ? rows - ii : 4);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    vint16m8_t vsrc = vlse16_v_i16m8(src + ii * cols + jj, 2);\n                    vint16m8_t vdst = vtranspose_v_i16m8(vsrc);\n                    vse16_v_i16m8(dst + jj * rows + ii, vdst, 2);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m1_t diagonal_sum = vmv_v_f_f32m1(0.0f); // Initialize the sum to 0\n    for (size_t i = 0; i < dim; i++) {\n        vfloat32m1_t values = vlse32_v_f32m1(&array[i*dim*dim + i*dim + i], dim*sizeof(float)); // Load diagonal elements\n        diagonal_sum = vfadd_vv_f32m1(diagonal_sum, values); // Add diagonal elements to the sum\n    }\n    float result;\n    vse32_v_f32m1(&result, diagonal_sum); // Store the final sum in result\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m1_t diagonal_sum = vmv_v_f_f32m1(0.0f); // Initialize the sum to 0\n    for (size_t i = 0; i < dim; i++) {\n        vfloat32m8_t values = vlse32_v_f32m8(&array[i*dim*dim + i*dim + i], dim * sizeof(float)); // Load the diagonal elements\n        diagonal_sum = vfredsum_vs_f32m8_f32m1(diagonal_sum, values, diagonal_sum); // Sum the diagonal elements\n    }\n    float result;\n    vse32_v_f32m1(&result, diagonal_sum); // Store the final sum\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t sum = vmv_v_f_f32m8(0.0f); // Initialize sum to 0\n    for (size_t i = 0; i < dim; i++) {\n        vfloat32m8_t values = vlse32_v_f32m8(&array[i*dim*dim + i*dim + i], dim*sizeof(float)); // Load diagonal elements\n        sum = vfadd_vv_f32m8(sum, values); // Add diagonal elements to sum\n    }\n    float result;\n    vse32_v_f32m8(&result, sum); // Store the final sum\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m1_t diagonal_sum = vmv_v_f_f32m1(0.0); // Initialize sum to 0\n    for (size_t i = 0; i < dim; i++) {\n        vfloat32m8_t values = vlse32_v_f32m8(&array[i*dim*dim + i*dim + i], dim*sizeof(float)); // Load diagonal elements\n        diagonal_sum = vfadd_vv_f32m1(diagonal_sum, vfredsum_vs_f32m8_f32m1(values, values, diagonal_sum)); // Add diagonal elements to sum\n    }\n    float result;\n    vse32_v_f32m1(&result, diagonal_sum); // Store the final sum\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t diagonal_sum = vmv_v_f_f32m8(0.0f); // Initialize diagonal sum to 0\n    for (size_t i = 0; i < dim; i++) {\n        vfloat32m8_t values = vlse32_v_f32m8(&array[i*dim*dim + i*dim + i], dim*sizeof(float)); // Load diagonal elements\n        diagonal_sum = vfredsum_vs_f32m8_f32m8(diagonal_sum, values, diagonal_sum); // Accumulate sum of diagonal elements\n    }\n    float result;\n    vse32_v_f32m8(&result, diagonal_sum); // Store the final sum\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t elements = rows * cols;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < elements; i += vl) {\n        double* src_ptr = src + i;\n        double* dst_ptr = dst + i;\n        vfloat64m1_t src_v = vle64_v_f64m1(src_ptr, vl);\n        vfloat64m1_t dst_v = vle64_v_f64m1(dst_ptr, vl);\n        vbool64_t mask = vfgt_vf_f64m1_b64(src_v, threshold, vl);\n        vfloat64m1_t result = vfmul_vf_f64m1(src_v, scale, vl);\n        vse64_v_f64m1(dst_ptr, vmerge_vfmul_vf_f64m1(result, src_v, mask, vl), vl);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t elements = rows * cols;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < elements; i += vl) {\n        size_t vl_actual = vl < elements - i ? vl : elements - i;\n\n        vfloat64m1_t v_src = vle64_v_f64m1(&src[i], vl_actual);\n        vbool64_t mask = vfgt_vf_f64m1_b64(v_src, threshold, vl_actual);\n        vfloat64m1_t v_result = vfmul_vf_f64m1(v_src, scale, vl_actual);\n        vfloat64m1_t v_dst = vmerge_vfmul_vf_f64m1(v_result, v_src, mask, vl_actual);\n\n        vse64_v_f64m1(&dst[i], v_dst, vl_actual);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t elements = rows * cols;\n    size_t vl = vsetvl_e64m1(0);\n\n    double* src_ptr = (double*)src;\n    double* dst_ptr = (double*)dst;\n\n    double vf_threshold = threshold;\n    double vf_scale = scale;\n\n    doublevd_t vd_threshold = vfmv_v_f_e64m1(vf_threshold, vl);\n    doublevd_t vd_scale = vfmv_v_f_e64m1(vf_scale, vl);\n\n    for (size_t i = 0; i < elements; i += vl) {\n        size_t cur_vl = vl;\n        if (i + vl > elements) {\n            cur_vl = elements - i;\n        }\n\n        doublevd_t vd_src = vle64_v_f64m1(src_ptr + i, cur_vl);\n        vbool64_t mask = vfgt_vf_f64m1_b64(vd_src, vd_threshold, cur_vl);\n        doublevd_t vd_result = vmerge_vvm_f64m1(mask, vd_scale * vd_src, vd_src, cur_vl);\n        vse64_v_f64m1(dst_ptr + i, vd_result, cur_vl);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    const int lane_count = vsetvli(cols, RVV_E64, RVV_M8);\n    double* src_ptr = (double*)src;\n    double* dst_ptr = (double*)dst;\n    double64_t v_src, v_dst, v_threshold, v_scale, v_result;\n\n    v_threshold = vslide1up_vd_d(vle64_v_f64m1(src_ptr, lane_count), 1);\n    v_threshold = vslide1down_vd_d(v_threshold, 1);\n    v_threshold = vfmax_vf_f64m1(v_threshold, threshold, lane_count);\n\n    v_scale = vfmul_vf_f64m1(vmv_v_f_f64m1(1.0, scale, lane_count), scale, lane_count);\n\n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = 0; j < cols; j += VL_E64) {\n            v_src = vle64_v_f64m1(src_ptr + i * cols + j, lane_count);\n            v_dst = vfmul_vf_f64m1(v_src, v_scale, lane_count);\n            v_result = vfsgnj_vf_f64m1(v_dst, v_src, v_threshold, lane_count);\n            vse64_v_f64m1(dst_ptr + i * cols + j, v_result, lane_count);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t elements = rows * cols;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < elements; i += vl) {\n        double* src_ptr = src + i;\n        double* dst_ptr = dst + i;\n        vfloat64m1_t src_v = vle64_v_f64m1(src_ptr, vl);\n        vbool64_t mask = vfgt_vf_f64m1_b64(src_v, threshold, vl);\n        vfloat64m1_t result = vmerge_vfm_f64m1(mask, vfmul_vf_f64m1(src_v, scale, vl), src_v, vl);\n        vse64_v_f64m1(dst_ptr, result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    vfloat64m8_t src_vec, dst_vec;\n    vint64m8_t indices_vec;\n    \n    for (i = 0; i < rows; i += VL) {\n        indices_vec = vle64_v_i64m8(&indices[i], VL);\n        for (j = 0; j < cols; j += VL) {\n            src_vec = vle64_v_f64m8(&src[(indices[i] * cols) + j], VL);\n            vse64_v_f64m8(&dst[i * cols + j], src_vec, VL);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    vfloat64m8_t src_vec, dst_vec;\n    vindex_t indices_vec;\n    \n    for (i = 0; i < rows; i += VL) {\n        indices_vec = vle64_v_u64m8(&indices[i], VL);\n        for (j = 0; j < cols; j += VL) {\n            src_vec = vle64_v_f64m8(&src[i * cols + j], VL);\n            vse64_v_f64m8(&dst[indices[i] * cols + j], src_vec, VL);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    vdouble vd_src, vd_dst;\n\n    for (i = 0; i < rows; i += VL) {\n        size_t indices_idx = i;\n        vint64 vindices = vle64_v_i64m1(&indices[indices_idx]);\n        vint64 voffset = vmul_vx_i64m1(vindices, cols * sizeof(double));\n        \n        for (j = 0; j < cols; j += VL) {\n            vd_src = vle64_v_f64m1(&src[i * cols + j]);\n            vd_dst = vle64_v_f64m1(&src[i * cols + voffset[j]]);\n            vse64_v_f64m1(&dst[i * cols + j], vd_dst);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    vfloat64m8_t src_vec, dst_vec;\n    vindex_t index_vec;\n    \n    for (i = 0; i < rows; i += VL) {\n        index_vec = vle64_v_u64m8(&indices[i], VL);\n        for (j = 0; j < cols; j += VL) {\n            src_vec = vle64_v_f64m8(&src[i * cols + j], VL);\n            vse64_v_f64m8(&dst[index_vec[j] * cols], src_vec, VL);\n        }\n    }\n}\n```  "}
{"task_id": "SimdBench_9_RVV", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i;\n    for (i = 0; i < rows; i += VL) {\n        vint64m8_t v_indices = vle64_v_i64m8(&indices[i], VL);\n        vfloat64m8_t v_src = vle64_v_f64m8(&src[vslide1up_v_i64m8(v_indices, 0)], VL);\n        vse64_v_f64m8(&dst[i * cols], v_src, VL);\n    }\n}\n```  "}
{"task_id": "SimdBench_10_RVV", "completion": "```\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvli(0, RVV_E64, RVV_M8);\n    for (size_t i = 0; i < length; i += vl) {\n        vint64m8_t vin1 = vle64_v_i64m8(&src1[i]);\n        vint64m8_t vin2 = vle64_v_i64m8(&src2[i]);\n        vint64m8_t vresult = vadd_vx_i64m8(vin1, vin2, 0);\n        vresult = vadd_vx_i64m8(vresult, scalar, 0);\n        vresult = vsra_vx_i64m8(vresult, 1, 0);\n        vse64_v_i64m8(&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(0); // Set the vector length to the maximum supported by the hardware\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint64m1_t vin1 = vle64_v_i64m1(&src1[i], vl);\n        vint64m1_t vin2 = vle64_v_i64m1(&src2[i], vl);\n        vint64m1_t vscalar = vmv_s_i_i64m1(scalar);\n        \n        vint64m1_t vsum = vadd_vv_i64m1(vin1, vin2);\n        vsum = vadd_vx_i64m1(vsum, vscalar);\n        vsum = vsra_vx_i64m1(vsum, 1); // Divide by 2 (round-toward-zero)\n\n        vse64_v_i64m1(&dst[i], vsum, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvli(0, RVV_E64, RVV_M8);\n    for (size_t i = 0; i < length; i += vl) {\n        vint64m8_t vec1 = vle64_v_i64m8(&src1[i]);\n        vint64m8_t vec2 = vle64_v_i64m8(&src2[i]);\n        vint64m8_t result = vadd_vv_i64m8(vec1, vec2);\n        vint64m8_t scalar_vec = vmv_v_x_i64m8(scalar);\n        result = vadd_vx_i64m8(result, scalar_vec);\n        result = vsra_vx_i64m8(result, 1);\n        vse64_v_i64m8(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint64m1_t vec1 = vle64_v_i64m1(&src1[i], vl);\n        vint64m1_t vec2 = vle64_v_i64m1(&src2[i], vl);\n        vint64m1_t result = vadd_vv_i64m1(vec1, vec2);\n        vint64m1_t scalar_vec = vmv_s_x_i64m1(scalar);\n        result = vadd_vx_i64m1(result, scalar_vec);\n        result = vsra_vx_i64m1(result, 1);\n        vse64_v_i64m1(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvli(length, sizeof(int64_t));\n    vint64m1_t vin1, vin2, vout;\n    vin1 = vle64_v_i64m1(src1, vl);\n    vin2 = vle64_v_i64m1(src2, vl);\n    vout = vadd_vv_i64m1(vin1, vin2, vl);\n    vout = vadd_vx_i64m1(vout, scalar, vl);\n    vout = vsra_vx_i64m1(vout, 1, vl);\n    vse64_v_i64m1(dst, vout, vl);\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) {\n        return -1;\n    }\n\n    int min_val = INT_MAX;\n    int vl = vsetvli(length, RVV_E32, RVV_M8);\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n\n    for (size_t i = 0; i < length; i += VL * 8) {\n        vbool4_t mask = VM4;\n        vint32m8_t v_data = vle32_v_i32m8(&src[i], vl);\n        vbool4_t even_mask = vmseq_vx_i32m8_b4(v_data, 0, mask);\n        vint32m8_t even_data = vcompress_vm_b4_i32m8(v_data, even_mask);\n        min_val = vminv_x_i32(min_val, vssegfirst_v_i32m8_i32(even_data, 0, vl));\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) {\n        return -1;\n    }\n\n    int min_val = INT_MAX;\n    int vl = vsetvli(0, RVV_E32, RVV_M8);\n    vint32m8_t vec_src;\n\n    for (size_t i = 0; i < length; i += VL * 8) {\n        size_t chunk_size = (i + VL * 8 <= length) ? VL * 8 : length - i;\n        vec_src = vle32_v_i32m8(&src[i], vl);\n\n        vbool4_t mask = VM4(0, 1, 0, 1);\n        vint32m8_t even_elements = vcompress_vm_v_i32m8(mask, vec_src);\n\n        int32_t temp_min = vminv_x_i32m8(even_elements);\n\n        if (temp_min < min_val) {\n            min_val = temp_min;\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) {\n        return -1;\n    }\n\n    const int lane_size = vsetvl_e32m1();\n    vint32m1_t v_min = vmv_v_x_i32m1(-1);\n    \n    for (size_t i = 0; i < length; i += lane_size) {\n        vint32m1_t v_src = vle32_v_i32m1(&src[i], lane_size);\n        vbool32_t mask = VMSEQ_VI_I32m1_b32(vint32m1_t(), i % 2, lane_size);\n        v_min = vmin_vx_i32m1(v_min, vslide1down_vx_i32m1(v_src, 1, 0, lane_size), mask);\n    }\n\n    int result;\n    vse32_v_i32m1(&result, v_min);\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) {\n        return -1;\n    }\n\n    const int lane_size = vsetvl_e32m1(0);\n    int min_val = INT_MAX;\n    vint32m1_t v_min_val = vslide1down_vx_i32m1(vmv_v_x_i32m1(0), min_val, lane_size);\n\n    for (size_t i = 0; i < length; i += lane_size) {\n        vint32m1_t v_src = vlse32_v_i32m1(src + i, sizeof(int) * lane_size);\n        vbool32_t mask = VMSEQ_VI_I32m1_b32(vsrc, 0, lane_size);\n        vint32m1_t v_even = vcompress_vm_b32(mask, v_src);\n\n        v_min_val = vmin_vx_i32m1(v_min_val, v_even, lane_size);\n    }\n\n    int result;\n    vse32_v_i32m1(&result, v_min_val);\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) {\n        return -1;\n    }\n\n    const int lane_size = vsetvl_e32m1();\n    int min_val = INT_MAX;\n    vint32m1_t v_min_val = vmv_s_x_i32m1(min_val);\n\n    for (size_t i = 0; i < length; i += lane_size) {\n        vint32m1_t v_src = vlse32_v_i32m1(src + i, lane_size);\n        vbool32_t mask = VMSEQ_VX_i32m1_b32(vsrc, 0);\n        vint32m1_t v_even = vrgather_vx_i32m1(vsrc, mask);\n        v_min_val = vmin_vv_i32m1(v_min_val, v_even);\n    }\n\n    int result;\n    vse32_v_i32m1(&result, v_min_val);\n    return result;\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const int32_t vl = vsetvl_e32m1(length);\n    const int32_t vs2 = shift;\n    \n    for (size_t i = 0; i < length; i += VL) {\n        const int32_t *src_ptr = &src[i];\n        int32_t *dst_ptr = &dst[i];\n        \n        vint32m1_t vin = vle32_v_i32m1(src_ptr, vl);\n        \n        vint32m1_t arith_shift = vasr_vx_i32m1(vin, vs2, vl);\n        \n        vuint32m1_t vin_uint = vreinterpretu_v_i32m1_u32m1(vin);\n        vuint32m1_t log_shift = vrshr_vx_u32m1(vin_uint, vs2, vl);\n        vint32m1_t log_shift_int = vreinterpret_v_u32m1_i32m1(log_shift);\n        \n        vint32m1_t result = vadd_vv_i32m1(arith_shift, log_shift_int, vl);\n        \n        vse32_v_i32m1(dst_ptr, result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const int VL = vsetvl_e32m1(0);\n    int32_t *src_ptr = (int32_t *)src;\n    int32_t *dst_ptr = (int32_t *)dst;\n    int32_t arith_result, logic_result, final_result;\n    vint32m1_t vin, vout;\n\n    for (size_t i = 0; i < length; i += VL) {\n        vin = vle32_v_i32m1(src_ptr + i);\n        vout = vasr_vx_i32m1(vin, shift);\n        vse32_v_i32m1(dst_ptr + i, vout);\n\n        vin = vreinterpret_v_i32m1_u32m1(vin);\n        vout = vrshr_vx_i32m1(vin, shift);\n        vse32_v_i32m1(dst_ptr + i, vout);\n\n        arith_result = vse32_v_i32m1(dst_ptr + i);\n        logic_result = vse32_v_i32m1(dst_ptr + i);\n        final_result = arith_result + logic_result;\n        vse32_v_i32m1(dst_ptr + i, final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvli(0, RVV_E32, RVV_M8);\n    int32_t step = vmsbf_m_b32(1);\n    int32_t mask = vmsif_m_b32(1, 0xFFFFFFFF);\n    int32_t shift_mask = vmsif_m_b32(1, (1 << shift) - 1);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint32m8_t vin = vle32_v_i32m8(&src[i]);\n        vint32m8_t arith_shift = vasr_vx_i32m8(vin, shift);\n        vuint32m8_t uint_vin = vreinterpretu_v_i32m8(vin);\n        vuint32m8_t log_shift = vlsr_vx_u32m8(uint_vin, shift);\n        vint32m8_t result = vadd_vv_i32m8(arith_shift, vreinterpret_v_i32m8(log_shift));\n        vse32_v_i32m8(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const int32_t vl = vsetvl_e32m1(length);\n    const int32_t vs2 = vmv_s_x_i32(shift);\n    \n    for (size_t i = 0; i < length; i += VL) {\n        const int32_t *src_ptr = &src[i];\n        int32_t *dst_ptr = &dst[i];\n        \n        vint32m1_t vin = vle32_v_i32m1(src_ptr, vl);\n        vint32m1_t arith_shift = vasr_vx_i32m1(vin, vs2, vl);\n        \n        vuint32m1_t vin_uint = vreinterpret_v_u32_i32(vin);\n        vuint32m1_t log_shift = vrshr_vx_u32m1(vin_uint, shift, vl);\n        vint32m1_t log_shift_int = vreinterpret_v_i32_u32(log_shift);\n        \n        vint32m1_t result = vadd_vv_i32m1(arith_shift, log_shift_int, vl);\n        \n        vse32_v_i32m1(dst_ptr, result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint32m1_t vin = vle32_v_i32m1(&src[i]);\n        vuint32m1_t vuint_in = vreinterpret_v_u_i32m1(vin);\n\n        vint32m1_t arith_shift = vasr_vx_i32m1(vin, shift);\n        vuint32m1_t log_shift = vlsr_vx_u32m1(vuint_in, shift);\n        vint32m1_t result = vadd_vv_i32m1(arith_shift, vreinterpret_v_i_u32m1(log_shift));\n\n        vse32_v_i32m1(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    vint32m4_t vec_sum = vmv_v_x_i32m4(0, VL1);\n    for (size_t i = 0; i < rows * cols; i += VL1) {\n        vint32m4_t vec_data = vle32_v_i32m4(&matrix[i]);\n        vec_sum = vadd_vv_i32m4(vec_sum, vec_data);\n    }\n    int64_t sum = vredsum_vs_i64_i32m4_i64(vec_sum, 0);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    int64_t sum = 0;\n    size_t i, j;\n    vint32m4_t vec, acc = vmv_v_x_i32m4(0, VL1);\n    \n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += VL1) {\n            vec = vle32_v_i32m4(&matrix[i * cols + j]);\n            acc = vadd_vv_i32m4(acc, vec);\n        }\n    }\n    \n    sum = vredsum_vs_i32m4_i64(acc, vmv_x_s_i64(0, VL1));\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    vint32m4_t vec_sum = vmv_v_x_i32m4(0, VL);\n    for (size_t i = 0; i < rows * cols; i += VL) {\n        vint32m4_t vec_data = vle32_v_i32m4(&matrix[i]);\n        vec_sum = vadd_vv_i32m4(vec_sum, vec_data);\n    }\n    vint32m1_t vec_sum_scalar = vfredsum_vs_i32m4_i32m1(vec_sum, vec_sum, vmv_x_s_i32m1(0, VL));\n    return vslide1down_vx_i64m1_i32m1(vadd_vx_i32m1(vslide1down_vx_i32m1_i32m1(vec_sum_scalar), vec_sum_scalar));\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    int64_t sum = 0;\n    size_t i, j;\n    vint32m4_t vec;\n    vint32m4_t acc = vmv_v_x_i32m4(0, VL);\n    \n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += VL) {\n            vec = vle32_v_i32m4(&matrix[i * cols + j]);\n            acc = vadd_vv_i32m4(acc, vec);\n        }\n    }\n    \n    for (i = 0; i < VL; i++) {\n        sum += vslide1down_vx_i32m4_i32(acc, i);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    vint32m4_t vec;\n    vint32m4_t sum = vmv_v_x_i32m4(0, VL);\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows * cols; i += VL) {\n        vec = vle32_v_i32m4(&matrix[i]);\n        sum = vwadd_vv_i32m4(sum, vec);\n    }\n\n    for (size_t i = 0; i < VL; i++) {\n        total_sum += vslide1down_vx_i32m4_i32(sum, i);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const int8_t *ptr = src;\n    size_t max_index = 0;\n    int8_t max_value = *ptr;\n    size_t i = 0;\n\n    const int8_t *end = src + length;\n    const int step = vsetvl_e8m1();\n\n    vint8m1_t vmax = vmv_s_x_i8m1(max_value);\n    vbool8_t mask = vmv_v_x_b8(true);\n\n    for (; ptr < end; ptr += step, i += step) {\n        vint8m1_t vsrc = vle8_v_i8m1(ptr, step);\n        mask = vmslt_vx_i8m1_b8(vsrc, max_value, mask);\n        vmax = vmerge_vvm_i8m1(vmax, i, mask, vmax, vsrc);\n        max_value = vslide1down_vx_i8(vmax, 0);\n        max_index = vslide1down_vx_i8(vmax, 1);\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t index = 0;\n    int8_t max_val = src[0];\n    size_t i = 0;\n\n    const int8_t *src_ptr = src;\n\n    vint8m1_t v_src;\n    vint8m1_t v_max_val = vmv_s_x_i8(max_val);\n    vbool8_t mask = vmv_v_x_u_i8(1);\n\n    for (; i < length; i += VL) {\n        v_src = vle8_v_i8m1(src_ptr, mask);\n\n        vbool8_t cmp_mask = vmslt_vx_i8m1_b8(v_src, max_val, mask);\n        v_max_val = vmerge_vvm_i8m1(cmp_mask, v_max_val, v_src);\n\n        index += vfirst_m_i8m1(cmp_mask);\n\n        src_ptr += VL;\n    }\n\n    return index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t i = 0;\n    const int8_t *src_ptr = src;\n\n    vint8m1_t v_src;\n    vint8m1_t v_max_value = vmv_s_x_i8(max_value);\n    vbool8_t mask = vmslt_vx_i8m1_b8(vmax_value, 0);\n\n    for (; i < length; i += VL) {\n        v_src = vle8_v_i8m1(src_ptr);\n\n        mask = vmsgt_vv_i8m1_b8(mask, v_src, v_max_value);\n        max_index = i + vfirst_m_i8m1(mask);\n\n        v_max_value = vmerge_vvm_i8m1(v_max_value, v_src, mask);\n        src_ptr += VL;\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t index = 0;\n    int8_t max_val = src[0];\n\n    const int8_t *src_ptr = src;\n    size_t i = 0;\n    const int8_t step = vsetvl_e8m1();\n\n    vint8m1_t vmax_val = vmv_s_x_i8m1(max_val);\n    vbool8_t mask = vmv_v_x_b8(true);\n\n    for (; i < length; i += VL) {\n        vint8m1_t vsrc = vle8_v_i8m1(src_ptr, step);\n\n        mask = vmslt_vx_i8m1_b8(vsrc, vmax_val, mask);\n        vmax_val = vmerge_vvm_i8m1(mask, vmax_val, vsrc);\n\n        index += vpopc_m_b8(mask);\n\n        src_ptr += VL;\n    }\n\n    return index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t i = 0;\n    const int8_t *src_ptr = src;\n\n    const size_t block_size = vsetvl_e8m1(0);\n    const size_t num_blocks = length / block_size;\n\n    vint8m1_t vmax_values = vmv_s_x_i8m1(max_value);\n    vbool8_t mask = vmseq_vx_i8m1_b8(vmax_values, max_value);\n\n    for (size_t block = 0; block < num_blocks; block++) {\n        vint8m1_t current_values = vle8_v_i8m1(src_ptr, block_size);\n\n        mask = vmsltu_vv_i8m1_b8(mask, current_values, vmax_values);\n        max_index += vfirst_m_i8m1(mask);\n\n        vmax_values = vmerge_vvm_i8m1(mask, vmax_values, current_values);\n\n        src_ptr += block_size;\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvli(0, RVV_EEW, RVV_M1);\n    vuint16m1_t v_mask = vmv_s_x_u16m1(mask);\n    size_t i = 0;\n\n    for (; i < length; i += vl) {\n        size_t chunk_size = (i + vl <= length) ? vl : (length - i);\n        vuint16m1_t v_src1 = vle16_v_u16m1(&src1[i], chunk_size);\n        vuint16m1_t v_src2 = vle16_v_u16m1(&src2[i], chunk_size);\n        vbool16_t v_pred = vmsne_vx_u16m1_b16(vseq_v_u16m1(chunk_size, 0), 0, 0);\n        vuint16m1_t v_dst = vmerge_vvm_u16m1(v_mask, v_src1, v_src2, v_pred);\n        vse16_v_u16m1(&dst[i], v_dst, chunk_size);\n    }\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvli(0, RVV_EEW, RVV_M1);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint16m1_t v_src1 = vle16_v_u16m1(&src1[i]);\n        vuint16m1_t v_src2 = vle16_v_u16m1(&src2[i]);\n        vbool16_t v_mask = vmseq_vx_u16m1_b16(vseq_vx_u16m1(i, vl), mask);\n        vuint16m1_t v_dst = vmerge_vvm_u16m1(v_mask, v_src1, v_src2);\n        vse16_v_u16m1(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvli(0, RVV_EEW, RVV_M);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint16m4_t v_src1 = vle16_v_u16m4(&src1[i]);\n        vuint16m4_t v_src2 = vle16_v_u16m4(&src2[i]);\n        vbool64_t v_mask = vmseq_vx_u16m4_b64(vseq_v_u16m4(i, vl), mask);\n        vuint16m4_t v_dst = vmerge_vvm_u16m4(v_mask, v_src1, v_src2);\n        vse16_v_u16m4(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvli(0, RVV_EEW, RVV_M1);\n    uint16_t* src1_ptr = (uint16_t*)src1;\n    uint16_t* src2_ptr = (uint16_t*)src2;\n    uint16_t* dst_ptr = (uint16_t*)dst;\n    uint16_t mask_v = (uint16_t)mask;\n    \n    for (size_t i = 0; i < length; i += vl) {\n        vuint16m1_t v_src1 = vle16_v_u16m1(src1_ptr + i, vl);\n        vuint16m1_t v_src2 = vle16_v_u16m1(src2_ptr + i, vl);\n        vbool16_t v_mask = vmseq_vx_u16m1_b16(vseq_vx_u16m1(i, vl), 0, vl);\n        vuint16m1_t v_dst = vmerge_vvm_u16m1(v_mask, v_src1, v_src2, vl);\n        vse16_v_u16m1(dst_ptr + i, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvli(0, RVV_E16, RVV_M1);\n    vuint16m1_t v_mask = vmv_x_u_i(mask, vl);\n\n    for (size_t i = 0; i < length; i += VL) {\n        size_t chunk_size = (i + VL <= length) ? VL : (length - i);\n        vuint16m1_t v_src1 = vle16_v_u16m1(&src1[i], vl);\n        vuint16m1_t v_src2 = vle16_v_u16m1(&src2[i], vl);\n        vbool16_t v_pred = vmsne_vx_u_b16m1_b16(vand_vx_u_b16m1(v_mask, i, vl), 0, vl);\n        vuint16m1_t v_dst = vmerge_vvm_u16m1(v_src2, v_src1, v_pred, vl);\n        vse16_v_u16m1(&dst[i], v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    size_t vl = vsetvl_e32m1(0);\n\n    for (i = 0; i < length; i += vl) {\n        vuint32m1_t vsrc = vle32_v_u32m1(&src[i]);\n        vuint32m1_t vpopcnt = vpopc_v_u32m1(vsrc);\n        vse32_v_u32m1(&dst[i], vpopcnt);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += 32) {\n        vuint32m8_t vsrc = vlse32_v_u32m8(&src[i], 4);\n        vuint32m8_t vpopcnt = vpopc_v_u32m8(vsrc);\n        vse32_v_u32m8(&dst[i], vpopcnt, 4);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += vsetvlmax()) {\n        size_t vl = vsetvl(length - i);\n        vuint32m4_t vin = vle32_v_u32m4(&src[i], vl);\n        vuint32m4_t vpopcnt = vpopc_v_u32m4(vin, vl);\n        vse32_v_u32m4(&dst[i], vpopcnt, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += vsetvlmax()) {\n        size_t vl = vsetvl(length - i);\n        vuint32m4_t vin = vle32_v_u32m4(&src[i], vl);\n        vuint32m4_t vpopcnt = vpopc_v_u32m4(vin, vl);\n        vse32_v_u32m4(&dst[i], vpopcnt, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i < length; i += vsetvlmax()) {\n        size_t vl = vsetvl(length - i);\n        vuint32m4_t vsrc = vle32_v_u32m4(&src[i], vl);\n        vuint32m4_t vpopcnt = vpopc_v_u32m4(vsrc, vl);\n        vse32_v_u32m4(&dst[i], vpopcnt, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m1(0);\n    uint16_t* src1_ptr = (uint16_t*)src1;\n    uint16_t* src2_ptr = (uint16_t*)src2;\n    uint16_t* dst_ptr = (uint16_t*)dst;\n\n    for (size_t i = 0; i < length; i += vl) {\n        vuint16m1_t vin1 = vle16_v_u16m1(src1_ptr + i, vl);\n        vuint16m1_t vin2 = vle16_v_u16m1(src2_ptr + i, vl);\n        vuint16m1_t vsum = vadd_vv_u16m1(vin1, vin2, vl);\n        vse16_v_u16m1(dst_ptr + i, vsaddu_vx_u16m1(vsum, 0, vl), vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m1(0);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint16m1_t vin1 = vle16_v_u16m1(&src1[i], vl);\n        vuint16m1_t vin2 = vle16_v_u16m1(&src2[i], vl);\n        vuint16m1_t vsum = vadd_vv_u16m1(vin1, vin2, vl);\n        vuint16m1_t vmax = vmaxu_vx_u16m1(vsum, UINT16_MAX, vl);\n        vse16_v_u16m1(&dst[i], vmax, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m1(0);\n    size_t i = 0;\n    for (; i < length; i += vl) {\n        vuint16m1_t vin1 = vle16_v_u16m1(&src1[i]);\n        vuint16m1_t vin2 = vle16_v_u16m1(&src2[i]);\n        vuint16m1_t vsum = vadd_vv_u16m1(vin1, vin2);\n        vuint16m1_t vresult = vminu_vx_u16m1(vsum, UINT16_MAX);\n        vse16_v_u16m1(&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m1(0);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint16m1_t vin1 = vle16_v_u16m1(&src1[i]);\n        vuint16m1_t vin2 = vle16_v_u16m1(&src2[i]);\n        vuint16m1_t vsum = vadd_vv_u16m1(vin1, vin2);\n        vuint16m1_t vclamped = vminu_vx_u16m1(vsum, UINT16_MAX);\n        vse16_v_u16m1(&dst[i], vclamped);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m1(0);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint16m1_t vin1 = vle16_v_u16m1(&src1[i]);\n        vuint16m1_t vin2 = vle16_v_u16m1(&src2[i]);\n        vuint16m1_t vsum = vadd_vv_u16m1(vin1, vin2);\n        vuint16m1_t vclamped = vminu_vx_u16m1(vsum, UINT16_MAX);\n        vse16_v_u16m1(&dst[i], vclamped);\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    size_t chunk_size = vsetvl_e64m1(0);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j += chunk_size) {\n            double* C_ptr = &C[i*p + j];\n            vfloat64m1_t acc = vfmv_v_f_f64m1(0.0);\n\n            for (k = 0; k < n; k += chunk_size) {\n                vfloat64m1_t a = vle64_v_f64m1(&A[i*n + k]);\n                vfloat64m1_t b = vle64_v_f64m1(&B[k*p + j]);\n                vbool64_t mask = vmsne_vx_i64m1_b64(a, 0) & vmsne_vx_i64m1_b64(b, 0) & vmsleu_vx_i64m1_b64(a, 100) & vmsgeu_vx_i64m1_b64(a, -100) & vmsleu_vx_i64m1_b64(b, 100) & vmsgeu_vx_i64m1_b64(b, -100);\n                acc = vfmacc_vf_f64m1(acc, a * b, mask);\n            }\n\n            vse64_v_f64m1(C_ptr, acc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t va, vb, vc;\n    vfloat64m8_t zero = vmv_v_f_f64m8(0.0, 0.0);\n    vfloat64m8_t range_min = vmv_v_f_f64m8(-100.0, -100.0);\n    vfloat64m8_t range_max = vmv_v_f_f64m8(100.0, 100.0);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0, 0.0);\n            for (k = 0; k < n; k+=8) {\n                va = vlseg8e64_v_f64m8(&A[i*n + k]);\n                vb = vlseg8e64_v_f64m8(&B[k*p + j]);\n                va = vfredosum_vs_f64m8_f64m8(va, zero, va);\n                vb = vfredosum_vs_f64m8_f64m8(vb, zero, vb);\n                vc = vfsgnjn_vv_f64m8(vc, vc, vmflt_vv_f64m8_b8(va, range_min, vmflt_vv_f64m8_b8(range_max, va, vb)));\n            }\n            vsseg8e64_v_f64m8(&C[i*p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    size_t chunk_size = vsetvl_e64m1(0);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j += chunk_size) {\n            double* C_ptr = &C[i*p + j];\n            vfloat64m1_t acc = vfmv_v_f_f64m1(0.0);\n\n            for (k = 0; k < n; k++) {\n                vfloat64m1_t a = vle64_v_f64m1(&A[i*n + k]);\n                vfloat64m1_t b = vle64_v_f64m1(&B[k*p + j]);\n                vbool64_t mask = vmsle_vx_i64m1_b64(a, -100, 0) & vmsle_vx_i64m1_b64(b, -100, 0) &\n                                 vmsle_vx_i64m1_b64(a, 100, 0) & vmsle_vx_i64m1_b64(b, 100, 0);\n                acc = vfwmacc_vf_f64m1(acc, a, b, mask);\n            }\n\n            vse64_v_f64m1(C_ptr, acc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t va, vb, vc;\n    vfloat64m8_t zero = vmv_v_f_f64m8(0.0, 0.0);\n    vfloat64m8_t range_min = vmv_v_f_f64m8(-100.0, -100.0);\n    vfloat64m8_t range_max = vmv_v_f_f64m8(100.0, 100.0);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0, 0.0);\n            for (k = 0; k < n; k+=8) {\n                va = vlseg8e64_v_f64m8(&A[i*n + k]);\n                vb = vlseg8e64_v_f64m8(&B[k*p + j]);\n                va = vfredsum_vs_f64m8_f64m8(va, zero, va, vmv_x_s_f64m8(0, 0));\n                vb = vfredsum_vs_f64m8_f64m8(vb, zero, vb, vmv_x_s_f64m8(0, 0));\n                vc = vfsgnjx_vv_f64m8(vc, vc, va, VM_FLT);\n                vc = vfsgnjx_vv_f64m8(vc, vc, vb, VM_FLT);\n                vc = vfmv_v_f_f64m8(fmaxf64_vv_f64m8(vc, range_min), fmaxf64_vv_f64m8(vc, range_max));\n            }\n            vse8_v_f64m8(&C[i*p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t va, vb, vc;\n    vfloat64m8_t zero = vmv_v_f_f64m8(0.0, 0);\n    vfloat64m8_t range_min = vmv_v_f_f64m8(-100.0, -100);\n    vfloat64m8_t range_max = vmv_v_f_f64m8(100.0, 100);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(zero, 0);\n            for (k = 0; k < n; k+=8) {\n                va = vlseg8e64_v_f64m8(&A[i*n + k]);\n                vb = vlseg8e64_v_f64m8(&B[k*p + j]);\n                va = vfredsum_vs_f64m8_f64m8(va, zero, va, range_min, range_max, 1);\n                vb = vfredsum_vs_f64m8_f64m8(vb, zero, vb, range_min, range_max, 1);\n                vc = vfmacc_vf_f64m8(vc, va, vb);\n            }\n            vsseg8e64_v_f64m8(&C[i*p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint64m1_t va = vle64_v_i64m1(&A[i], vl);\n        vint64m1_t vb = vle64_v_i64m1(&B[i], vl);\n        vint64m1_t vc = vadd_vv_i64m1(va, vb, vl);\n        vse64_v_i64m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint64m1_t va = vle64_v_i64m1(&A[i], vl);\n        vint64m1_t vb = vle64_v_i64m1(&B[i], vl);\n        vint64m1_t vc = vadd_vv_i64m1(va, vb, vl);\n        vse64_v_i64m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint64m1_t va = vle64_v_i64m1(&A[i], vl);\n        vint64m1_t vb = vle64_v_i64m1(&B[i], vl);\n        vint64m1_t vc = vadd_vv_i64m1(va, vb, vl);\n        vse64_v_i64m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint64m1_t va = vle64_v_i64m1(&A[i], vl);\n        vint64m1_t vb = vle64_v_i64m1(&B[i], vl);\n        vint64m1_t vc = vadd_vv_i64m1(va, vb, vl);\n        vse64_v_i64m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint64m1_t va = vle64_v_i64m1(&A[i], vl);\n        vint64m1_t vb = vle64_v_i64m1(&B[i], vl);\n        vint64m1_t vc = vadd_vv_i64m1(va, vb, vl);\n        vse64_v_i64m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvli(length, RVV_E32);\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m8_t vec1 = vle32_v_f32m8(&src1[i], vl);\n        vfloat32m8_t vec2 = vle32_v_f32m8(&src2[i], vl);\n        vfloat32m8_t result = vfadd_vv_f32m8(vec1, vec2, vl);\n        vint32m4_t rounded = vfcvt_x_f_v_i32m4(result, 0, vl);\n        vse32_v_i32m4(&dst[i], rounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const int step = vsetvl_e32m1(0);\n    for (size_t i = 0; i < length; i += step) {\n        vfloat32m1_t vin1 = vle32_v_f32m1(&src1[i]);\n        vfloat32m1_t vin2 = vle32_v_f32m1(&src2[i]);\n        vfloat32m1_t vsum = vfadd_vf_f32m1(vin1, vin2);\n        vint32m1_t vrounded = vfcvt_x_f_v_i32m1_b32(vsum);\n        vse32_v_f32m1(&dst[i], vreinterpret_v_f32m1_i32m1(vrounded));\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n\n    for (; i < length; i += vl) {\n        vfloat32m1_t vin1 = vle32_v_f32m1(&src1[i], vl);\n        vfloat32m1_t vin2 = vle32_v_f32m1(&src2[i], vl);\n        vfloat32m1_t vsum = vfadd_vf_f32m1(vin1, vin2, vl);\n        vint32m1_t vrounded = vfcvt_x_f_v_i32m1_f32m1(vsum, l);\n        vse32_v_f32m1(&dst[i], vrounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&src1[i], vl);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&src2[i], vl);\n        vfloat32m1_t result = vfadd_vv_f32m1(vec1, vec2, vl);\n        vint32m1_t rounded = vfcvt_x_f_v_i32m1_f32(result, l, vl);\n        vse32_v_i32m1(&dst[i], rounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&src1[i], vl);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&src2[i], vl);\n        vfloat32m1_t result = vfadd_vv_f32m1(vec1, vec2, vl);\n        vint32m1_t rounded = vfcvt_x_f_v_i32m1_f32m1(result, l);\n        vse32_v_i32m1(&dst[i], rounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const int VL = vsetvl_e32m1(0);\n    const float32x4_t threshold = vmv_v_f_f32m1(50.0, VL);\n\n    for (size_t i = 0; i < length; i += VL) {\n        vfloat32m1_t va = vle32_v_f32m1(&src1[i], VL);\n        vfloat32m1_t vb = vle32_v_f32m1(&src2[i], VL);\n        vbool32_t mask = vmslt_vf_f32m1_b32(va, threshold, VL) & vmseq_vi_i32m1_b32(i, 2, VL);\n        vfloat32m1_t result = vmul_vv_f32m1(va, vb, VL);\n        vfloat32m1_t rounded_result = vfcvt_x_f_v_f32m1_f32(result, VUP, VL);\n        vfloat32m1_t final_result = vsel_vf_f32m1_b32(rounded_result, vmv_v_f_f32m1(-1.0, VL), mask, VL);\n        vse32_v_f32m1(&dst[i], final_result, VL);\n    }\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t vl = vsetvli(0, RVV_E32, RVV_M2);\n    vfloat32m2_t vin1, vin2, vout;\n    vin1 = vle32_v_f32m2(src1, vl);\n    vin2 = vle32_v_f32m2(src2, vl);\n    vfloat32m2_t vdiff = vfsub_vv_f32m2(vin1, vin2, vl);\n    vbool64_t mask = vmflt_vf_f32m2_b64(vfabs_v_f32m2(vdiff, vl), 50.0, vl) & vmseq_vx_i32m2_b64(vmv_v_x_i32m2(vl, 0), 0, vl);\n    vfloat32m2_t vresult = vfmul_vf_f32m2(vin1, vin2, vl);\n    vout = vfcvt_x_f_v_f32m2_f32(vresult, 0, vl);\n    vout = vsel_v_f32m2_b64(vout, vfmv_f_f32m2(-1.0, vl), mask, vl);\n    vse32_v_f32m2(dst, vout, vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    int vl = vsetvli(0, RVV_E32, RVV_M2);\n    vfloat32m2_t vin1, vin2, vdiff, vabsdiff, vresult;\n    vfloat32m2_t vnegone = vmv_v_f_f32m2(-1.0f);\n    vfloat32m2_t vzero = vmv_v_f_f32m2(0.0f);\n    vfloat32m2_t vthreshold = vmv_v_f_f32m2(50.0f);\n    size_t i;\n\n    for (i = 0; i < length; i += VL * 2) {\n        vin1 = vle32_v_f32m2(&src1[i], vl);\n        vin2 = vle32_v_f32m2(&src2[i], vl);\n        vdiff = vfsub_vv_f32m2(vin1, vin2, vl);\n        vabsdiff = vfsgnjn_vv_f32m2(vdiff, vzero, vl);\n        vabsdiff = vfmax_vf_f32m2(vabsdiff, vthreshold, vl);\n        vresult = vfmul_vv_f32m2(vin1, vin2, vl);\n        vresult = vfredosum_vs_f32m2_f32m1(vresult, vnegone, vabsdiff, vl);\n        vse32_v_f32m2(&dst[i], vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const int vl = vsetvl_e32m2(length);\n    vfloat32m2_t v_src1, v_src2, v_result;\n    vfloat32m2_t v_threshold = vmv_v_f_f32m2(50.0f);\n    vint32m2_t v_mask1, v_mask2, v_mask;\n    v_mask1 = vmslt_vx_i32m2_b32(vl, 0);\n    v_mask2 = vmslt_vx_i32m2_b32(vl, 1);\n    v_mask = vmand_mm_b32(v_mask1, v_mask2);\n    \n    for (size_t i = 0; i < length; i += vl) {\n        v_src1 = vle32_v_f32m2(&src1[i]);\n        v_src2 = vle32_v_f32m2(&src2[i]);\n        \n        vfloat32m2_t v_diff = vfsub_vv_f32m2(v_src1, v_src2);\n        vfloat32m2_t v_abs_diff = vfsgnjn_vf_f32m2(vdiff, 0.0f);\n        v_mask = vmslt_vf_f32m2_b32(v_abs_diff, v_threshold);\n        \n        v_result = vfmul_vv_f32m2(v_src1, v_src2);\n        v_result = vfredosum_vs_f32m2_f32m1(v_result, vfmv_s_f_f32m1(0.0f), vl);\n        \n        v_result = vmerge_vvm_f32m2(v_result, -1.0f, v_mask);\n        \n        vse32_v_f32m2(&dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const int vl = vsetvli(0, RVV_E32, RVV_M2);\n    const float32x2_t threshold = vmv_v_f_f32(50.0f);\n    const float32x2_t neg_one = vmv_v_f_f32(-1.0f);\n    \n    for (size_t i = 0; i < length; i += 2) {\n        const float32x2_t vec1 = vle32_v_f32m2(&src1[i], vl);\n        const float32x2_t vec2 = vle32_v_f32m2(&src2[i], vl);\n        \n        const float32x2_t diff = vsub_vv_f32m2(vec1, vec2, vl);\n        const float32x2_t abs_diff = vabs_v_f32m2(diff, vl);\n        \n        const vbool64_t mask = vmsltu_vx_f32m2_b64(abs_diff, threshold, vl);\n        const vbool64_t mask_index = vmseq_vx_i32m1_b64(i, 0, vl);\n        const vbool64_t final_mask = vmand_mm_b64(mask, mask_index);\n        \n        const float32x2_t result = vfredosum_vs_f32m2_f32m1(vfadd_vf_f32m2(vfncvt_x_f_v_f32m2_f32(vec1, vl), vfncvt_x_f_v_f32m2_f32(vec2, vl), vl), 0.0f, final_mask, vl);\n        \n        vse32_v_f32m2(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    double* src_ptr = (double*)mat;\n    double* dst_ptr = (double*)dst;\n    doublev1_t scalar_v = vfmv_v_f_f64(scalar);\n\n    for (i = 0; i < size; i += vl) {\n        size_t vl_actual = (size - i < vl) ? (size - i) : vl;\n        doublev1_t src_v = vle64_v_f64m1(src_ptr + i, vl_actual);\n        doublev1_t result_v = vfwmul_vf_f64m1(src_v, scalar_v, vl_actual);\n        vse64_v_f64m1(dst_ptr + i, result_v, vl_actual);\n    }\n}\n```  "}
{"task_id": "SimdBench_22_RVV", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    double* p_dst = dst;\n    const double* p_mat = mat;\n\n    double scalar_vec = scalar;\n    vfloat64m1_t v_scalar = vfmv_v_f_f64m1(scalar_vec);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t v_mat = vle64_v_f64m1(p_mat, vl);\n        vfloat64m1_t v_result = vfmul_vf_f64m1(v_mat, v_scalar);\n        vse64_v_f64m1(p_dst, v_result, vl);\n\n        p_mat += vl;\n        p_dst += vl;\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i;\n    size_t j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    double* result = (double*)malloc(sizeof(double) * vl);\n    vfloat64m1_t v_mat, v_dst, v_result;\n    vfloat64m1_t v_scalar = vfmv_v_f_f64m1(scalar);\n\n    for (i = 0; i < size; i += vl) {\n        v_mat = vle64_v_f64m1(&mat[i], vl);\n        v_result = vfmul_vf_f64m1(v_mat, v_scalar);\n        vse64_v_f64m1(&dst[i], v_result, vl);\n    }\n\n    free(result);\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    double* src_ptr = (double*)mat;\n    double* dst_ptr = (double*)dst;\n    doublev1_t scalar_v = vfmv_v_f_f64(scalar);\n\n    for (i = 0; i < size; i += vl) {\n        size_t vl_actual = (size - i < vl) ? (size - i) : vl;\n        doublev1_t src_v = vle64_v_f64m1(src_ptr + i, vl_actual);\n        doublev1_t result_v = vfwmul_vf_f64m1(src_v, scalar_v, vl_actual);\n        vse64_v_f64m1(dst_ptr + i, result_v, vl_actual);\n    }\n}\n```  "}
{"task_id": "SimdBench_22_RVV", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        double* src_ptr = (double*)&mat[i];\n        double* dst_ptr = (double*)&dst[i];\n        vfloat64m1_t src_v = vle64_v_f64m1(src_ptr, vl);\n        vfloat64m1_t scalar_v = vfmul_vf_f64m1(src_v, scalar, vl);\n        vse64_v_f64m1(dst_ptr, scalar_v, vl);\n    }\n}\n```  "}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        vfloat32m8_t row = vle32_v_f32m8(&mat[i * n], n);\n        vfloat32m8_t row_squared = vfwmul_vf_f32m8(row, row, 1, VL1);\n        vfloat32m8_t row_sum = vfredsum_vs_f32m8_f32m8(row_squared, row_squared, VFMUL, 0, VL1);\n        float norm = vfmv_f_s_f32m1_f32(row_sum);\n        vfloat32m8_t norm_v = vfmv_v_f_f32m8(norm, VL1);\n        vfloat32m8_t normalized_row = vfdiv_vv_f32m8(row, norm_v, VL1);\n        vse32_v_f32m8(&mat[i * n], normalized_row, n);\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    vfloat32m8_t vec_sum;\n    vfloat32m8_t vec_norm;\n    vfloat32m8_t vec_mat;\n    \n    for (size_t i = 0; i < m; i++) {\n        vec_sum = vfmv_v_f_f32m8(0.0, VL8);\n        \n        // Calculate the sum of squares of elements in the row\n        for (size_t j = 0; j < n; j+=VL8.ELEN) {\n            vec_mat = vle32_v_f32m8(&mat[i*n + j]);\n            vec_sum = vfadd_vv_f32m8(vec_sum, vfmul_vv_f32m8(vec_mat, vec_mat, VL8), VL8);\n        }\n        \n        // Calculate the L^2 norm\n        vec_norm = vfredsum_vs_f32m8_f32m1(vec_sum, vec_sum, vfmv_s_f_f32m1(0.0), VL1);\n        vec_norm = vfmv_v_f_f32m8(1.0/vfsqrt_v_f32m8(vec_norm, VL8), VL8);\n        \n        // Normalize the row\n        for (size_t j = 0; j < n; j+=VL8.ELEN) {\n            vec_mat = vle32_v_f32m8(&mat[i*n + j]);\n            vec_mat = vfmul_vv_f32m8(vec_mat, vec_norm, VL8);\n            vse32_v_f32m8(&mat[i*n + j], vec_mat);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        vfloat32m8_t row_vec = vle32_v_f32m8(&mat[i * n], n);\n        vfloat32m8_t row_squares = vfmul_vv_f32m8(row_vec, row_vec, n);\n        vfloat32m8_t row_sums = vfadd_vv_f32m8(vfredsum_vs_f32m8_f32m8(row_squares, vfmv_v_f_f32m8(0.0), vfmv_v_f_f32m8(0.0), n), vfmv_v_f_f32m8(0.0), n);\n        vfloat32m8_t row_norm = vfdiv_vf_f32m8(row_vec, vfmv_v_f_f32m8(vfsqrt_v_f32m8(row_sums, n), n), n);\n        vse32_v_f32m8(&mat[i * n], row_norm, n);\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    vfloat32m8_t vec_sum;\n    vfloat32m8_t vec_norm;\n    vfloat32m8_t vec_elem;\n    \n    for (size_t i = 0; i < m; i++) {\n        vec_sum = vfmv_v_f_f32m8(0.0, VL1, vstart);\n        \n        // Calculate the sum of squares of elements in the row\n        for (size_t j = 0; j < n; j += VL1) {\n            vec_elem = vle32_v_f32m8(&mat[i*n + j]);\n            vec_sum = vfadd_vv_f32m8(vec_sum, vfmul_vv_f32m8(vec_elem, vec_elem));\n        }\n        \n        // Calculate the L^2 norm\n        vec_norm = vfredsum_vs_f32m8_f32m1(vec_sum, vec_sum, vfmv_v_f_f32m1(0.0), VL1);\n        vec_norm = vfmv_v_f_f32m8(1.0 / sqrt(vfmv_f_f32m1_f32(vec_norm)));\n        \n        // Normalize each element in the row\n        for (size_t j = 0; j < n; j += VL1) {\n            vec_elem = vle32_v_f32m8(&mat[i*n + j]);\n            vec_elem = vfmul_vv_f32m8(vec_elem, vec_norm);\n            vse32_v_f32m8(&mat[i*n + j], vec_elem);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t vec_sum, vec_norm;\n    vfloat32m8_t *mat_ptr = (vfloat32m8_t*)mat;\n\n    for (i = 0; i < m; i++) {\n        vec_sum = vfmv_v_f_f32m8(0.0, VL8);\n        for (j = 0; j < n; j += VL8) {\n            vec_sum = vfadd_vv_f32m8(vec_sum, vfmul_vf_f32m8(vfncvt_f_f_w_f32m8(vle32_v_f32m8(mat_ptr + i*n + j, VL8)), vfncvt_f_f_w_f32m8(vle32_v_f32m8(mat_ptr + i*n + j, VL8))));\n        }\n        vec_norm = vfmv_v_f_f32m8(0.0, VL8);\n        vec_norm = vfredsum_vs_f32m8_f32m8(vec_sum, vec_sum, vec_norm, VL8);\n        vec_norm = vsqrt_v_f32m8(vec_norm, VL8);\n\n        for (j = 0; j < n; j += VL8) {\n            vse32_v_f32m8(mat_ptr + i*n + j, VL8, vdiv_vv_f32m8(vle32_v_f32m8(mat_ptr + i*n + j, VL8), vec_norm));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t v_res;\n    vfloat64m8_t v_mat1, v_mat2;\n    vint64m8_t v_rounded;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            v_res = vfmul_vv_f64m8(v_res, v_mat1, v_mat2);\n\n            for (k = 0; k < n; k += VL) {\n                v_mat1 = vle64_v_f64m8(&mat1[i * n + k]);\n                v_mat2 = vle64_v_f64m8(&mat2[k * p + j]);\n                v_res = vfmacc_vv_f64m8(v_res, v_mat1, v_mat2);\n            }\n\n            v_rounded = vfcvt_x_f_v_i64m8(v_res);\n            vse64_v_i64m8(&dst[i * p + j], v_rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t v_res;\n    vfloat64m8_t v_mat1, v_mat2;\n    vint64m8_t v_dst;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            v_dst = vmv_v_x_i64m8(0, v_dst);\n            for (k = 0; k < n; k += VL * 8) {\n                v_mat1 = vle64_v_f64m8(&mat1[i * n + k]);\n                v_mat2 = vle64_v_f64m8(&mat2[k * p + j]);\n                v_res = vfmul_vv_f64m8(v_mat1, v_mat2);\n                v_dst = vfadd_vv_f64m8_m(v_dst, v_res, v_dst, VL);\n            }\n            v_dst = vfcvt_xu_v_f64m8_i64m8(v_dst);\n            vse64_v_i64m8(&dst[i * p + j], v_dst);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t vec1, vec2, vec_dst;\n    vint64m8_t vec_int;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vec_dst = vfmv_v_f_f64m8(0.0);\n            for (k = 0; k < n; k += VL * 8) {\n                vec1 = vle64_v_f64m8(&mat1[i * n + k]);\n                vec2 = vle64_v_f64m8(&mat2[k * p + j]);\n                vec_dst = vfma_vv_f64m8(vec_dst, vec1, vec2);\n            }\n            vec_int = vfcvt_x_f_v_i64m8_i64(vec_dst, 0);\n            vse64_v_i64m8(&dst[i * p + j], vec_int);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vfloat64m8_t acc = vfmv_v_f_f64m8(0.0);\n            for (k = 0; k < n; k += VL) {\n                vfloat64m8_t a = vle64_v_f64m8(&mat1[i * n + k]);\n                vfloat64m8_t b = vle64_v_f64m8(&mat2[k * p + j]);\n                acc = vfredsum_vs_f64m8_f64m8(acc, vfmul_vv_f64m8(a, b), acc);\n            }\n            dst[i * p + j] = vfcvt_x_f_v_f64m8_i64(acc, 0);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t v_sum, v_rounded;\n    vfloat64m8_t v_mat1, v_mat2, v_result;\n    \n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            v_sum = vfmv_v_f_f64m8(0.0);\n            for (k = 0; k < n; k += VL) {\n                v_mat1 = vle64_v_f64m8(&mat1[i * n + k]);\n                v_mat2 = vle64_v_f64m8(&mat2[k * p + j]);\n                v_result = vfmul_vv_f64m8(v_mat1, v_mat2);\n                v_sum = vfadd_vv_f64m8(v_sum, v_result);\n            }\n            v_rounded = vfwcvt_x_f_v_f64m8_f64m1(v_sum);\n            vse64_v_f64m8(&dst[i * p + j], v_rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e32m1(0);\n    const int col_stride = rows;\n    \n    for (int i = 0; i < cols; i += lane_count) {\n        int vl = (i + lane_count <= cols) ? lane_count : cols - i;\n        \n        vfloat32m1_t src_vec = vle32_v_f32m1(&src[i * rows], vl);\n        vfloat32m1_t transposed_vec = vfmv_v_f_f32m1(vfncvt_x_f_v_f32m1_f32m1(src_vec), 0.25, vl);\n        \n        vse32_v_f32m1(&dst[i], transposed_vec, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e32m1(0);\n    const int col_stride = rows;\n    \n    for (size_t i = 0; i < rows; i += lane_count) {\n        int vl = vsetvl_e32m1(rows - i);\n        vfloat32m1_t vin = vle32_v_f32m1(&src[i], vl);\n        \n        for (size_t j = 0; j < cols; j += lane_count) {\n            int vl2 = vsetvl_e32m1(cols - j);\n            vfloat32m1_t vout = vle32_v_f32m1(&dst[j * rows + i], vl2);\n            vfloat32m1_t vtransposed = vfmv_v_f_f32m1(vin, vl);\n            vfloat32m1_t vrounded = vfwcvt_rtzi_f_f32m1_f32m1(vtransposed, vl);\n            vse32_v_f32m1(&dst[j * rows + i], vrounded, vl2);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int lane_count = vsetvli(0, RVV_E32, RVV_M8);\n    const int col_stride = rows * sizeof(float);\n\n    for (size_t i = 0; i < cols; i += VL) {\n        int vl = (cols - i < VL) ? cols - i : VL;\n\n        vfloat32m8_t in = vle32_v_f32m8(&src[i * rows], vl * lane_count);\n        vfloat32m8_t out = vfmv_v_f_f32m8(0.0, vl * lane_count);\n\n        for (size_t j = 0; j < rows; j++) {\n            vfloat32m8_t tmp = vslideup_vx_f32m8(in, j, vl * lane_count);\n            vfloat32m8_t rounded = vfcvt_x_f_v_f32m8_f32m1(tmp, 2, vl * lane_count);\n            vse32_v_f32m8(&dst[j * cols + i], rounded, vl * lane_count);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int lane_count = vsetvli(0, RVV_E32, RVV_M8);\n    const int col_stride = cols * sizeof(float);\n\n    for (size_t i = 0; i < rows; i += VLMAX) {\n        int vl = vsetvli(0, RVV_E32, RVV_M8);\n        vl = (rows - i < vl) ? rows - i : vl;\n\n        for (size_t j = 0; j < cols; j += VLMAX) {\n            int vj = vsetvli(0, RVV_E32, RVV_M8);\n            vj = (cols - j < vj) ? cols - j : vj;\n\n            vfloat32m8_t in_vec = vle32_v_f32m8(&src[i * cols + j], vl);\n            vfloat32m8_t out_vec = vfwcvt_rtzs_x_f_v_f32m8(in_vec, 2, vl);\n\n            vfloat32m8_t transposed = vtranspose_v_f32m8(out_vec, vl);\n            vse32_v_f32m8(&dst[j * rows + i], transposed, vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < rows; i += lane_count) {\n        int vl = vsetvl_e32m1(rows - i);\n\n        vfloat32m1_t src_vec = vle32_v_f32m1(&src[i * cols], vl);\n        vfloat32m1_t transposed_vec = vrgather_vx_f32m1(src_vec, (size_t)cols, vl);\n        vfloat32m1_t rounded_vec = vfmv_v_f_f32m1(transposed_vec, 0.25, vl);\n\n        vse32_v_f32m1(&dst[i], rounded_vec, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&tensor1[i], vl);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&tensor2[i], vl);\n        vfloat32m1_t result = vfsub_vv_f32m1(vec1, vec2, vl);\n        vfloat32m1_t rounded_result = vfcvt_x_f_v_f32m1_f32(result, vl);\n        vse32_v_f32m1(&dst[i], rounded_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&tensor1[i]);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&tensor2[i]);\n        vfloat32m1_t result = vfsub_vv_f32m1(vec1, vec2);\n        vint32m1_t rounded_result = vfcvt_x_f_v_i32m1(result, 0);\n        vse32_v_f32m1(&dst[i], vfwcvt_f_x_v_f32m1(rounded_result, 0));\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&tensor1[i], vl);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&tensor2[i], vl);\n        vfloat32m1_t result = vfsub_vv_f32m1(vec1, vec2, vl);\n        vfloat32m1_t rounded_result = vfcvt_x_f_v_f32m1_f32m1(result, vl);\n        vse32_v_f32m1(&dst[i], rounded_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&tensor1[i]);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&tensor2[i]);\n        vfloat32m1_t result = vfsub_vv_f32m1(vec1, vec2);\n        vint32m1_t rounded = vfcvt_x_f_v_i32m1(result, 0);\n        vse32_v_f32m1(&dst[i], vfwcvt_f_x_v_f32m1(rounded, 0));\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t vec1 = vle32_v_f32m1(&tensor1[i], vl);\n        vfloat32m1_t vec2 = vle32_v_f32m1(&tensor2[i], vl);\n        vfloat32m1_t result = vfsub_vv_f32m1(vec1, vec2, vl);\n        vfloat32m1_t rounded_result = vfcvt_x_f_v_f32m1_f32(result, 0, vl);\n        vse32_v_f32m1(&dst[i], rounded_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i;\n    size_t j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t v_mat1 = vle64_v_f64m1(&mat1[i], vl);\n        vfloat64m1_t v_mat2 = vle64_v_f64m1(&mat2[i], vl);\n        vfloat64m1_t v_result = vfwmul_vv_f64m1(v_mat1, v_mat2, vl);\n        vse64_v_f64m1(&dst[i], v_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t v_mat1 = vle64_v_f64m1(&mat1[i], vl);\n        vfloat64m1_t v_mat2 = vle64_v_f64m1(&mat2[i], vl);\n        vfloat64m1_t v_result = vfwmul_vv_f64m1(v_mat1, v_mat2, vl);\n        vse64_v_f64m1(&dst[i], v_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i;\n    size_t j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t vec1 = vle64_v_f64m1(&mat1[i], vl);\n        vfloat64m1_t vec2 = vle64_v_f64m1(&mat2[i], vl);\n        vfloat64m1_t result = vfwmul_vv_f64m1(vec1, vec2, vl);\n        vse64_v_f64m1(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t vec1 = vle64_v_f64m1(&mat1[i], vl);\n        vfloat64m1_t vec2 = vle64_v_f64m1(&mat2[i], vl);\n        vfloat64m1_t result = vfwmul_vv_f64m1(vec1, vec2, vl);\n        vse64_v_f64m1(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i;\n    size_t j;\n    size_t size = m * n;\n    size_t vl = vsetvl_e64m1(0);\n\n    for (i = 0; i < size; i += vl) {\n        vfloat64m1_t vec1 = vle64_v_f64m1(&mat1[i], vl);\n        vfloat64m1_t vec2 = vle64_v_f64m1(&mat2[i], vl);\n        vfloat64m1_t result = vfwmul_vv_f64m1(vec1, vec2, vl);\n        vse64_v_f64m1(&dst[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t result, vector, matrix_row;\n    \n    for (i = 0; i < m; i++) {\n        result = vfmv_v_f_f32m8(0.0f); // Initialize result to 0\n        vector = vlse32_v_f32m8(&vec[0], 4); // Load vector elements\n        \n        for (j = 0; j < n; j += VL) {\n            matrix_row = vle32_v_f32m8(&mat[i * n + j], 4 * VL); // Load matrix row\n            result = vfmacc_vf_f32m8(result, vslide1down_vx_f32m8(matrix_row, 1), vector); // Multiply and accumulate\n        }\n        \n        vse32_v_f32m8(&dst[i], result, 4); // Store result in destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t result, vector, matrix_row;\n    \n    for (i = 0; i < m; i++) {\n        result = vfmv_v_f_f32m8(0.0, result); // Initialize result to 0\n        vector = vlse32_v_f32m8(&vec[0], 4); // Load vector elements\n        \n        for (j = 0; j < n; j += VLSEWMAX) {\n            matrix_row = vlse32_v_f32m8(&mat[i * n + j], 4); // Load matrix row elements\n            result = vfmacc_vf_f32m8(result, vector, matrix_row); // Perform matrix-vector multiplication\n        }\n        \n        vse32_v_f32m8(&dst[i], result, 4); // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t result, vector, matrix_row;\n    \n    for (i = 0; i < m; i++) {\n        result = vfmv_v_f_f32m8(0.0, result); // Initialize result to 0\n        vector = vle32_v_f32m8(&vec[0]); // Load vector elements\n        \n        for (j = 0; j < n; j += VL) {\n            matrix_row = vle32_v_f32m8(&mat[i * n + j]); // Load matrix row elements\n            result = vfmacc_vf_f32m8(result, vector, matrix_row); // Perform matrix-vector multiplication\n        }\n        \n        vse32_v_f32m8(&dst[i], result); // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t result, vector, matrix_row;\n    \n    for (i = 0; i < m; i++) {\n        result = vfmv_v_f_f32m8(0.0, result); // Initialize result to 0\n        vector = vlse32_v_f32m8(&vec[0], 4); // Load vector elements\n        \n        for (j = 0; j < n; j += VLSEWMAX) {\n            matrix_row = vlse32_v_f32m8(&mat[i * n + j], 4); // Load matrix row elements\n            result = vfmacc_vf_f32m8(result, vector, matrix_row); // Perform matrix-vector multiplication\n        }\n        \n        vse32_v_f32m8(&dst[i], result, 4); // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    vfloat32m8_t result, vector, matrix_row;\n    \n    for (i = 0; i < m; i++) {\n        result = vfmv_v_f_f32m8(0.0, result); // Initialize result to 0\n        vector = vlse32_v_f32m8(&vec[0], 4); // Load vector elements\n        \n        for (j = 0; j < n; j += VLSEWMAX) {\n            matrix_row = vlse32_v_f32m8(&mat[i * n + j], 4); // Load matrix row elements\n            result = vfmacc_vf_f32m8(result, vector, matrix_row); // Perform matrix-vector multiplication\n        }\n        \n        vse32_v_f32m8(&dst[i], result, 4); // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    const size_t block_size = vsetvli(0, RVV_E64, RVV_M8);\n    const size_t total_elements = m * n;\n    const size_t num_blocks = total_elements / block_size + (total_elements % block_size != 0);\n    \n    double avg = 0.0;\n    double* partial_sums = (double*)malloc(num_blocks * sizeof(double));\n    \n    for (size_t i = 0; i < num_blocks; i++) {\n        vfloat64m8_t v_mat = vle64_v_f64m8(&mat[i * block_size], block_size);\n        vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(vfmv_f_s_f64m8(0.0, block_size), v_mat, v_mat, block_size);\n        partial_sums[i] = vslide1down_vx_f64(v_sum, 0);\n    }\n    \n    for (size_t i = 0; i < num_blocks; i++) {\n        avg += partial_sums[i];\n    }\n    \n    avg /= total_elements;\n    \n    free(partial_sums);\n    \n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    const size_t block_size = vlenb / sizeof(double);\n    const size_t total_elements = m * n;\n    const size_t total_blocks = total_elements / block_size;\n    \n    vfloat64m8_t v_sum = vmv_v_f_f64m8(0.0, vl1);\n    \n    for (size_t i = 0; i < total_blocks; i++) {\n        vfloat64m8_t v_mat = vle64_v_f64m8(&mat[i * block_size], vl1);\n        v_sum = vfadd_vv_f64m8(v_sum, v_mat, vl1);\n    }\n    \n    double sum = vfredsum_vs_f64m8_f64m1(v_sum, v_sum, 0.0, vl1);\n    double average = sum / total_elements;\n    \n    return average;\n}\n```  "}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    const size_t block_size = vlenb / sizeof(double);\n    const size_t total_elements = m * n;\n    const size_t num_blocks = total_elements / block_size;\n    \n    vfloat64m8_t v_sum = vmv_v_f_f64m8(0.0, vl1);\n    \n    for (size_t i = 0; i < num_blocks; i++) {\n        vfloat64m8_t v_mat = vle64_v_f64m8(&mat[i * block_size], vl1);\n        v_sum = vfadd_vv_f64m8(v_sum, v_mat, vl1);\n    }\n    \n    double sum = vfredsum_vs_f64m8_f64m1(v_sum, v_sum, 0.0, vl1);\n    \n    for (size_t i = num_blocks * block_size; i < total_elements; i++) {\n        sum += mat[i];\n    }\n    \n    return sum / total_elements;\n}\n```  "}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    const size_t simd_width = vlenb / sizeof(double);\n    const size_t total_elements = m * n;\n    const size_t num_simd_iterations = total_elements / simd_width;\n\n    vfloat64m8_t avg_acc = vfmv_v_f_f64m8(0.0, VL1);\n    vfloat64m8_t sum_acc = vfmv_v_f_f64m8(0.0, VL1);\n\n    for (size_t i = 0; i < num_simd_iterations; i++) {\n        vfloat64m8_t vec = vle64_v_f64m8(&mat[i * simd_width], VL1);\n        sum_acc = vfadd_vv_f64m8(sum_acc, vec, VL1);\n    }\n\n    double sum = vfredsum_vs_f64_f64m8_f64(sum_acc, sum_acc, 0.0, VL1);\n    double avg = sum / total_elements;\n\n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t remaining_elements = total_elements % vlenb(0);\n\n    vfloat64m1_t v_sum = vmv_v_f_f64m1(0.0, VL1);\n    vfloat64m1_t v_avg = vmv_v_f_f64m1(0.0, VL1);\n\n    for (size_t i = 0; i < total_elements - remaining_elements; i += vlenb(0)) {\n        vfloat64m1_t v_mat = vle64_v_f64m1(&mat[i]);\n        v_sum = vfadd_vv_f64m1(v_sum, v_mat, VL1);\n    }\n\n    double sum = vfredsum_vs_f64_f64m1_f64(v_sum, v_sum, 0.0, VL1);\n    \n    for (size_t i = total_elements - remaining_elements; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        vint32m4_t row = vle32_v_i32m4(&matrix[i * cols]);\n        bool direction = directions[i];\n        vbool8_t mask = vmslt_vx_i32m4_b8(row, vslideup_vx_i32m4(row, 1, 0), 0);\n        if (!direction) {\n            mask = vmseq_vx_i32m4_b8(mask, 0);\n        }\n        if (vmpopc_m_b8(mask) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        vint32m4_t row = vle32_v_i32m4(matrix + i * cols);\n        bool direction = directions[i];\n        vbool8_t mask = vmslt_vx_i32m4_b8(row, vslideup_vx_i32m4(row, 1, 0), direction);\n        if (vmfirst_m_b8(mask) != -1) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        vint32m4_t row_vec = vle32_v_i32m4(&matrix[i * cols]);\n        bool direction = directions[i];\n        vbool8_t mask = vmslt_vx_i32m4_b8(row_vec, vslideup_vx_i32m4(row_vec, 1));\n        if (!direction) {\n            mask = vmseq_vx_i32m4_b8(mask, 0);\n        }\n        if (vmpopc_m_b8(mask) != cols - 1) {\n            return false;\n        }\n    }\n    return true;\n}\n```  "}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        vint32m4_t row = vle32_v_i32m4(&matrix[i * cols]);\n        bool direction = directions[i];\n        vbool8_t mask = vmslt_vx_i32m4_b8(row, vslide1down_vx_i32m4(row, 1));\n        if (!direction) {\n            mask = vmseq_vx_i32m4_b8(mask, 0);\n        }\n        if (vmpopc_m_b8(mask) != cols - 1) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        vint32m4_t row = vle32_v_i32m4(&matrix[i * cols], vlh);\n        vbool8_t dir = vmv_v_x_b8(directions[i]);\n        vbool32_t mask = vmslt_vv_i32m4_b32(row, vslide1down_vx_i32m4(row, 1, vlh));\n        if (vmand_mm_b32(mask, dir) != vmand_mm_b32(mask, vnot_m_b(dir))) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    int16x8_t base_v = vmv_v_x_i16(base);\n    int16x8_t zero_v = vmv_v_x_i16(0);\n    int16x8_t max_val_v = vmv_v_x_i16(INT16_MAX);\n\n    size_t i;\n    for (i = 0; i < length; i += 8) {\n        int16x8_t src_v = vlseg8w_v_i16(src + i);\n        int16x8_t div_v = vdiv_vv_i16(src_v, base_v);\n        int16x8_t rounded_v = vfcvt_x_f_v_i16(div_v, 0);\n        int16x8_t result_v = vmul_vv_i16(rounded_v, base_v);\n        result_v = vmin_vv_i16(result_v, src_v);\n        result_v = vmax_vv_i16(result_v, zero_v);\n        result_v = vmin_vv_i16(result_v, max_val_v);\n        vseg8w_v_i16(dst + i, result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    int16x8_t v_base = vmv_v_x_i16(base);\n    int16x8_t v_src, v_dst, v_quotient, v_result;\n\n    for (size_t i = 0; i < length; i += 8) {\n        v_src = vlseg8h_v_i16m1(src);\n        v_quotient = vdiv_vx_i16m1(v_src, base);\n        v_result = vwmul_vx_i16m1(v_quotient, base);\n        v_dst = vmax_vv_i16m1(v_result, v_src);\n        vssseg8h_v_i16m1(dst, v_dst);\n        src += 8;\n        dst += 8;\n    }\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    int16x8_t v_base = vmv_v_x_i16(base);\n    int16x8_t v_src, v_dst, v_quotient, v_result;\n\n    for (size_t i = 0; i < length; i += 8) {\n        v_src = vlseg8h_v_i16(src);\n        v_quotient = vdiv_vv_i16(v_src, v_base);\n        v_result = vmul_vv_i16(v_base, v_quotient);\n        v_dst = vmin_vx_i16(v_result, v_src);\n        vse8_v_i16(dst, v_dst);\n        src += 8;\n        dst += 8;\n    }\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const int16_t base_v = base;\n    const int16_t max_val = base * (INT16_MAX / base); // Prevent overflow\n\n    int16_t* src_ptr = (int16_t*)src;\n    int16_t* dst_ptr = (int16_t*)dst;\n\n    size_t vl = vsetvli(length, RVV_E8, RVV_M8);\n\n    vint16m8_t src_v = vle16_v_i16m8(src_ptr, vl);\n    vint16m8_t base_vv = vmv_v_x_i16m8(base_v, vl);\n    vint16m8_t max_val_v = vmv_v_x_i16m8(max_val, vl);\n\n    vint16m8_t div = vdiv_vv_i16m8(src_v, base_vv, vl);\n    vint16m8_t nearest_multiple = vmul_vv_i16m8(div, base_vv, vl);\n\n    vbool2_t mask = vmsltu_vx_i16m8_b2(src_v, max_val, vl);\n    vint16m8_t result = vsel_vvm_i16m8(mask, nearest_multiple, max_val_v, vl);\n\n    vse16_v_i16m8(dst_ptr, result, vl);\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    int16x8_t base_v = vmv_v_x_i16(base);\n    int16x8_t zero_v = vmv_v_x_i16(0);\n\n    for (size_t i = 0; i < length; i += 8) {\n        int16x8_t src_v = vlseg8w_v_i16(src + i);\n        int16x8_t div_v = vdiv_vv_i16(src_v, base_v);\n        int16x8_t rounded_v = vpopc_m_i16(vmsltu_vx_i16(div_v, 0), zero_v); // Nearest integer that is a multiple of base\n        int16x8_t result_v = vmul_vv_i16(rounded_v, base_v);\n        vse8_v_i16(dst + i, result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e64m1(VECTORIZATION_WIDTH);\n    const int col_chunks = cols / lane_count;\n    const double* matrix_ptr = matrix;\n    const double* vector_ptr = vector;\n\n    vfloat64m1_t v_input = vle64_v_f64m1(vector_ptr, lane_count);\n\n    for (size_t i = 0; i < rows; i++) {\n        const double* matrix_row_ptr = matrix_ptr;\n        bool row_match = true;\n\n        for (int j = 0; j < col_chunks; j++) {\n            vfloat64m1_t v_matrix = vle64_v_f64m1(matrix_row_ptr, lane_count);\n            vbool64_t mask = vfmv_v_f_f64m1_b(mask, v_matrix == v_input);\n            if (vmfirst_v_i_b(mask) < lane_count) {\n                row_match = false;\n                break;\n            }\n            matrix_row_ptr += lane_count;\n        }\n\n        if (row_match) {\n            return true;\n        }\n\n        matrix_ptr += cols;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    const int lane_size = vlenb_e32;\n    const int lane_num = VLMAX / lane_size;\n    \n    for (size_t i = 0; i < rows; i += lane_num) {\n        vfloat64m8_t acc = vmv_v_f_f64m8(0.0);\n        vfloat64m8_t vec = vle64_v_f64m8(vector);\n        \n        for (size_t j = 0; j < cols; j++) {\n            vfloat64m8_t mat = vle64_v_f64m8(&matrix[i * cols + j]);\n            vbool8_t mask = vfmv_v_f_f64_b8(mat, vec);\n            acc = vor_vv_f64m8(acc, mask);\n        }\n        \n        if (vmpopc_m_b8(vmfne_v_f_f64m8_b8(acc, 0.0)) >= cols) {\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    const int lane_size = vlenb_e32;\n    const int num_lanes = vsetvl_e32(cols, RVV_EFF_WIDTH);\n    \n    double* result = (double*)malloc(num_lanes * sizeof(double));\n    \n    for (size_t i = 0; i < rows; i += num_lanes) {\n        vfloat64m8_t acc = vfmv_v_f_f64m8(0.0, VL1);\n        \n        for (size_t j = 0; j < num_lanes; j++) {\n            vfloat64m8_t mat_row = vle64_v_f64m8(&matrix[(i + j) * cols], VL1);\n            vfloat64m8_t vec = vle64_v_f64m8(vector, VL1);\n            vbool8_t mask = vmsne_vv_u_b8_f64m8(mat_row, vec, VL1);\n            acc = vfmerge_vfm_f64m8(acc, mask, acc, VL1);\n        }\n        \n        vse64_v_f64m8(&result[0], acc, VL1);\n        \n        for (size_t j = 0; j < num_lanes; j++) {\n            if (result[j] == 0.0) {\n                free(result);\n                return true;\n            }\n        }\n    }\n    \n    free(result);\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e64m1(0);\n    const int col_chunks = cols / lane_count;\n    \n    double* result = (double*)malloc(sizeof(double) * lane_count);\n    \n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = 0; j < col_chunks; j++) {\n            vfloat64m1_t vmatrix = vle64_v_f64m1(&matrix[i * cols + j * lane_count], lane_count);\n            vfloat64m1_t vvector = vle64_v_f64m1(vector, lane_count);\n            vbool64_t mask = vmsne_vv_f64m1_b64(vmatrix, vvector, lane_count);\n            vse64_v_f64m1(result, mask, lane_count);\n            \n            int all_zeros = 1;\n            for (int k = 0; k < lane_count; k++) {\n                if (result[k] != 0) {\n                    all_zeros = 0;\n                    break;\n                }\n            }\n            \n            if (all_zeros) {\n                free(result);\n                return true;\n            }\n        }\n    }\n    \n    free(result);\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    const int lane_count = vsetvl_e64m1(0);\n    const int vector_length = cols / lane_count;\n    \n    double* result = (double*)malloc(sizeof(double) * lane_count);\n    \n    for (size_t i = 0; i < rows; i++) {\n        bool match = true;\n        \n        for (size_t j = 0; j < vector_length; j++) {\n            vfloat64m1_t matrix_v = vle64_v_f64m1(&matrix[i * cols + j * lane_count]);\n            vfloat64m1_t vector_v = vle64_v_f64m1(&vector[j * lane_count]);\n            vbool64_t cmp = vfmv_v_f_f64m1_b(&matrix_v, vector_v);\n            vse64_v_f64m1(&result[j], cmp);\n        }\n        \n        for (size_t k = 0; k < lane_count; k++) {\n            if (result[k] == 0) {\n                match = false;\n                break;\n            }\n        }\n        \n        if (match) {\n            free(result);\n            return true;\n        }\n    }\n    \n    free(result);\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t equal_count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t remaining_elements = total_elements;\n\n    size_t i = 0;\n    while (remaining_elements >= 64) {\n        vuint8m8_t va = vle8_v_u8m8(&A[i]);\n        vuint8m8_t vb = vle8_v_u8m8(&B[i]);\n        vbool8_t mask = vmsne_vv_u8m8_b8(va, vb);\n        equal_count += 64 - vpopc_m_b8(mask);\n        remaining_elements -= 64;\n        i += 64;\n    }\n\n    if (remaining_elements > 0) {\n        vuint8m8_t va = vle8_v_u8m8(&A[i]);\n        vuint8m8_t vb = vle8_v_u8m8(&B[i]);\n        vbool8_t mask = vmsne_vv_u8m8_b8(va, vb);\n        equal_count += remaining_elements - vpopc_m_b8(mask);\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t remaining_elements = total_elements;\n\n    size_t i = 0;\n    while (remaining_elements >= 64) {\n        vuint8m8_t va = vlse8_v_u8m8(&A[i], 64);\n        vuint8m8_t vb = vlse8_v_u8m8(&B[i], 64);\n        vbool8_t mask = vmsne_vv_u8m8_b8(va, vb, vzero_u8m8());\n        count += vpopc_m_b8(mask, vmand_mm_b8(mask, vmsne_vv_u8m8_b8(va, vb, vzero_u8m8())));\n        remaining_elements -= 64;\n        i += 64;\n    }\n\n    while (remaining_elements > 0) {\n        vuint8m1_t va = vlse8_v_u8m1(&A[i], remaining_elements);\n        vuint8m1_t vb = vlse8_v_u8m1(&B[i], remaining_elements);\n        vbool8_t mask = vmsne_vv_u8m1_b8(va, vb, vzero_u8m1());\n        count += vpopc_m_b8(mask, vmand_mm_b8(mask, vmsne_vv_u8m1_b8(va, vb, vzero_u8m1())));\n        remaining_elements = 0;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t equal_count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t remaining_elements = total_elements;\n\n    size_t i = 0;\n    while (remaining_elements >= 64) {\n        vuint8m8_t va = vlse8_v_u8m8(&A[i], 64);\n        vuint8m8_t vb = vlse8_v_u8m8(&B[i], 64);\n        vbool8_t mask = vmsne_vx_u8m8_b8(va, vb, 0);\n        equal_count += vpopc_m_b8(mask, 0);\n\n        i += 64;\n        remaining_elements -= 64;\n    }\n\n    for (size_t j = i; j < total_elements; j++) {\n        if (A[j] == B[j]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t equal_count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t remaining_elements = total_elements;\n\n    size_t i = 0;\n    while (remaining_elements >= 64) {\n        vuint8m8_t va = vlse8_v_u8m8(&A[i], 64);\n        vuint8m8_t vb = vlse8_v_u8m8(&B[i], 64);\n        vbool8_t mask = vmsne_vx_u8m8_b8(va, vb, 0);\n        equal_count += vpopc_m_b8(mask, 0);\n        remaining_elements -= 64;\n        i += 64;\n    }\n\n    while (remaining_elements > 0) {\n        vuint8m1_t va = vlse8_v_u8m1(&A[i], remaining_elements);\n        vuint8m1_t vb = vlse8_v_u8m1(&B[i], remaining_elements);\n        vbool8_t mask = vmsne_vx_u8m1_b8(va, vb, 0);\n        equal_count += vpopc_m_b8(mask, 0);\n        remaining_elements = 0;\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t equal_count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t remaining_elements = total_elements;\n\n    size_t i = 0;\n    while (remaining_elements >= 64) {\n        vuint8m8_t va = vlse8_v_u8m8(&A[i], 64);\n        vuint8m8_t vb = vlse8_v_u8m8(&B[i], 64);\n        vbool8_t mask = vmsne_vv_u8m8_b8(va, vb, va);\n\n        equal_count += vpopc_m_b8(mask, mask);\n\n        i += 64;\n        remaining_elements -= 64;\n    }\n\n    for (size_t j = i; j < total_elements; ++j) {\n        if (A[j] == B[j]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    int64_t step = vsetvl_e64m1(0);\n    size_t vl = (length + step - 1) / step;\n    vint64m1_t va = vmv_s_x_i64m1(a, step);\n    for (size_t i = 0; i < vl; i += step) {\n        vint64m1_t vx = vle64_v_i64m1(&x[i], step);\n        vint64m1_t vy = vwmul_vx_i64m1(vx, va, step);\n        vy = vabs_v_i64m1(vy, step);\n        vse64_v_i64m1(&y[i], vy, step);\n    }\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    const int64_t a_v = a;\n    const int64_t *x_ptr = x;\n    int64_t *y_ptr = y;\n    \n    size_t vl = vsetvl_e64m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint64m1_t vx = vle64_v_i64m1(x_ptr + i, vl);\n        vint64m1_t vy = vwmul_vx_i64m1(vx, a_v, vl);\n        vy = vabs_vx_i64m1(vy, vl);\n        vse64_v_i64m1(y_ptr + i, vy, vl);\n    }\n}\n```  "}
{"task_id": "SimdBench_34_RVV", "completion": "```\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    const int64_t a_v = a;\n    const int64_t *x_ptr = x;\n    int64_t *y_ptr = y;\n    size_t vl = vsetvl_e64m1(length);\n\n    for (; length >= vl; length -= vl) {\n        vint64m1_t vx = vle64_v_i64m1(x_ptr, vl);\n        vint64m1_t vy = vwmul_vx_i64m1(vx, a_v, vl);\n        vy = vabs_vx_i64m1(vy, vl);\n        vse64_v_i64m1(y_ptr, vy, vl);\n        x_ptr += vl;\n        y_ptr += vl;\n    }\n}\n```  "}
{"task_id": "SimdBench_34_RVV", "completion": "```\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n\n    vint64m1_t vec_x, vec_y;\n    vec_x = vle64_v_i64m1(x, vl);\n\n    vint64m1_t vec_a = vmv_s_x_i64m1(a, vl);\n\n    vec_y = vwmul_vx_i64m1(vec_a, vec_x, vl);\n    vec_y = vabs_v_i64m1(vec_y, vl);\n\n    vse64_v_i64m1(y, vec_y, vl);\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    int64_t step = vsetvl_e64m1(length);\n    vint64m1_t va = vmv_s_x_i64m1(a, step);\n    for (size_t i = 0; i < length; i += VL) {\n        size_t vl = vsetvl_e64m1(length - i);\n        vint64m1_t vx = vle64_v_i64m1(&x[i], vl);\n        vint64m1_t vy = vwmul_vx_i64m1(va, vx, vl);\n        vy = vabs_v_i64m1(vy, vl);\n        vse64_v_i64m1(&y[i], vy, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    const int vl = vsetvli(0, RVV_E16, RVV_M1);\n    vint16m1_t v_data = vle16_v_i16m1(vec, vl);\n\n    int16_t min_val = INT16_MAX;\n    int16_t max_val = INT16_MIN;\n\n    for (size_t i = 0; i < length; i += VL) {\n        v_data = vle16_v_i16m1(&vec[i], vl);\n        min_val = vminu_vx_i16(min_val, vfirst_m_i16m1(v_data, vl));\n        max_val = vmaxu_vx_i16(max_val, vfirst_m_i16m1(v_data, vl));\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    const int16_t* ptr = vec;\n    size_t remaining = length;\n\n    vint16m4_t v_min, v_max;\n    v_min = v_max = vmv_v_x_i16m4(0, VL1);\n\n    while (remaining >= VL) {\n        vint16m4_t v_data = vle16_v_i16m4(ptr, VL);\n        v_min = vmin_vx_i16m4(v_min, v_data);\n        v_max = vmax_vx_i16m4(v_max, v_data);\n        ptr += VL;\n        remaining -= VL;\n    }\n\n    int16_t min_num[VL], max_num[VL];\n    vse16_v_i16m4(min_num, v_min, VL);\n    vse16_v_i16m4(max_num, v_max, VL);\n\n    MinMaxPair result;\n    result.min_num = min_num[0];\n    result.max_num = max_num[0];\n\n    for (int i = 1; i < VL; i++) {\n        if (min_num[i] < result.min_num) {\n            result.min_num = min_num[i];\n        }\n        if (max_num[i] > result.max_num) {\n            result.max_num = max_num[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v_input = vle16_v_i16m8(vec, length);\n\n    int16_t min_val = vmin_vx_i16m8(v_input);\n    int16_t max_val = vmax_vx_i16m8(v_input);\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v_input = vle16_v_i16m8(vec, length);\n\n    int16_t min_val = vmin_vx_i16m8(v_input);\n    int16_t max_val = vmax_vx_i16m8(v_input);\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v_input = vle16_v_i16m8(vec, length);\n\n    vint16m8_t v_min = vfirst_m_i16m8(v_input);\n    vint16m8_t v_max = vfirst_m_i16m8(vinput);\n\n    for (size_t i = VL; i < length; i += VL) {\n        vint16m8_t v_temp = vle16_v_i16m8(&vec[i], length - i);\n        v_min = vmin_vx_i16m8(v_min, v_temp);\n        v_max = vmax_vx_i16m8(v_max, v_temp);\n    }\n\n    MinMaxPair result;\n    result.min_num = vslide1down_vx_i16m8_i16(v_min, 8);\n    result.max_num = vslide1down_vx_i16m8_i16(v_max, 8);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const int lane_count = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        vint32m1_t vec_row = vle32_v_i32m1(row, lane_count);\n\n        for (size_t j = 0; j < cols - 1; j++) {\n            vint32m1_t vec_curr = vle32_v_i32m1(row + j, lane_count);\n            vint32m1_t vec_next = vle32_v_i32m1(row + j + 1, lane_count);\n            vbool32_t cmp_result = vmslt_vx_i32m1_b32(vec_next, vec_curr, lane_count);\n\n            if (vmpopc_m_b32(cmp_result, lane_count) != lane_count) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const int lane_count = vsetvl_e32m1();\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n        vint32m1_t vec_row = vle32_v_i32m1(row, lane_count);\n\n        for (size_t j = 0; j < cols - 1; j++) {\n            vint32m1_t vec_curr = vle32_v_i32m1(row + j, lane_count);\n            vint32m1_t vec_next = vle32_v_i32m1(row + j + 1, lane_count);\n            vbool32_t cmp_result = vmsltu_vx_i32m1_b32(vec_next, vec_curr, lane_count);\n\n            if (vmpopc_m_b32(cmp_result, lane_count) != lane_count) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```  "}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const int lane_size = vsetvl_e32m1(0);\n    const int num_lanes = (cols + lane_size - 1) / lane_size;\n    const int* ptr = matrix;\n\n    for (size_t i = 0; i < rows; i++) {\n        vint32m1_t prev_vec = vmv_v_x_i32m1(*ptr++, 0);\n        for (int j = 1; j < num_lanes; j++) {\n            vint32m1_t curr_vec = vle32_v_i32m1(ptr + j * lane_size, lane_size);\n            vbool32_t mask = vmslt_vx_i32m1_b32(curr_vec, prev_vec, 0);\n            if (vmpopc_m_b32(mask) != 0) {\n                return false;\n            }\n            prev_vec = vslide1down_vx_i32m1(curr_vec, prev_vec, 0);\n        }\n        ptr += num_lanes * lane_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const int lane_count = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < rows; i++) {\n        vint32m1_t prev_vec = vmv_v_x_i32m1(*(matrix + i * cols), lane_count);\n        for (size_t j = 1; j < cols; j += VL) {\n            vint32m1_t current_vec = vle32_v_i32m1(matrix + i * cols + j, lane_count);\n            vbool32_t cmp_result = vmsltu_vx_i32m1_b32(current_vec, prev_vec, lane_count);\n            if (!vmpopc_m_b32(cmp_result, lane_count)) {\n                return false;\n            }\n            prev_vec = current_vec;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const int lane_size = vsetvl_e32m1(0);\n    const int num_lanes = (cols + lane_size - 1) / lane_size;\n    const int* ptr = matrix;\n\n    for (size_t i = 0; i < rows; i++) {\n        vint32m1_t prev = vle32_v_i32m1(ptr, num_lanes * lane_size);\n        ptr += num_lanes * lane_size;\n\n        for (size_t j = 1; j < cols; j += lane_size) {\n            vint32m1_t current = vle32_v_i32m1(ptr, num_lanes * lane_size);\n            vbool32_t cmp = vmslt_vv_i32m1_b32(prev, current, num_lanes * lane_size);\n            if (!vmfirst_m_b32(cmp, num_lanes * lane_size)) {\n                return false;\n            }\n            prev = current;\n            ptr += num_lanes * lane_size;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const int block_elements = block_size * block_size;\n    const int blocks_in_row = cols / block_size;\n    const int total_blocks = (rows / block_size) * blocks_in_row;\n\n    for (int i = 1; i < total_blocks; i++) {\n        int* block1 = matrix + i * block_elements;\n        int* block2 = matrix;\n\n        int equal = 1;\n        for (int j = 0; j < block_elements; j += 32) {\n            vint32m8_t v1 = vle32_v_i32m8(block1 + j);\n            vint32m8_t v2 = vle32_v_i32m8(block2 + j);\n            equal = vmand_mm_i32m8(equal, vmsne_vv_i32m8_b32(v1, v2));\n        }\n\n        if (vmfirst_m_i1(equal) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const int block_elements = block_size * block_size;\n    const int blocks_in_row = cols / block_size;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* current_block = &matrix[i * cols + j];\n\n            for (size_t k = 0; k < block_size; ++k) {\n                for (size_t l = 0; l < block_size; l += RVV_LEN) {\n                    vint32m4_t first_block = vlh_v_i32m4(current_block, 0);\n                    vint32m4_t current_vector = vlh_v_i32m4(current_block + l, 0);\n                    vbool8_t mask = vmseq_vx_i32m4_b8(first_block, current_vector, 0);\n                    if (!vmfirst_m_b8(mask)) {\n                        return false;\n                    }\n                }\n                current_block += cols;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const int block_elements = block_size * block_size;\n    const int blocks_in_row = cols / block_size;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* block_start = matrix + i * cols + j;\n            int32_t first_element = *block_start;\n\n            for (size_t k = 0; k < block_size; ++k) {\n                for (size_t l = 0; l < block_size; l += VL) {\n                    vint32m1_t block_elements_v = vle32_v_i32m1(block_start + k * cols + l);\n                    vint32m1_t first_element_v = vmv_s_x_i32m1(first_element);\n\n                    vbool32_t equal = vmsne_vx_i32m1_b32(block_elements_v, first_element_v);\n                    if (vmfirst_m_b32(equal) != -1) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const int block_elements = block_size * block_size;\n    const int blocks_in_row = cols / block_size;\n\n    vint32m4_t first_block = vlh_v_i32m4(matrix, block_elements, 0);\n    \n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            vint32m4_t current_block = vlh_v_i32m4(matrix + i * cols + j, block_elements, 0);\n            if (!vmsne_vx_i32m4_b4(current_block, first_block)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    const int block_elements = block_size * block_size;\n    const int blocks_in_row = cols / block_size;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* block_start = matrix + i * cols + j;\n            int32_t first_element = *block_start;\n\n            for (size_t k = 0; k < block_size; ++k) {\n                for (size_t l = 0; l < block_size; l += VL) {\n                    vint32m1_t block_elements_v = vle32_v_i32m1(block_start + k * cols + l);\n                    vint32m1_t first_element_v = vmv_s_x_i32m1(first_element);\n\n                    vbool32_t equal_mask = vmsne_vx_i32m1_b32(block_elements_v, first_element_v);\n                    if (vmfirst_m_b32(equal_mask) != -1) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false;\n    }\n\n    const size_t block_count = length / block_size;\n    const size_t vec_length = block_count * block_size;\n\n    vfloat64m1_t first_block = vle64_v_f64m1(&vec[0]);\n    vfloat64m1_t tolerance_v = vmv_v_f_f64m1(tolerance);\n\n    for (size_t i = 1; i < block_count; i++) {\n        vfloat64m1_t current_block = vle64_v_f64m1(&vec[i * block_size]);\n        vbool64_t cmp_result = vfmv_v_f_f64m1_b(current_block, first_block, tolerance_v);\n        if (!vmsbf_v_i64m1_b(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false;\n    }\n\n    const size_t num_blocks = length / block_size;\n    const size_t block_size_v = vsetvl_e64m1(num_blocks);\n\n    double* first_block = (double*)vec;\n    double* current_block = (double*)(vec + block_size);\n\n    vfloat64m1_t v_first_block = vle64_v_f64m1(first_block, block_size_v);\n    vfloat64m1_t v_tolerance = vfmv_v_f_f64m1(tolerance);\n\n    for (size_t i = 1; i < num_blocks; i++) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(current_block, block_size_v);\n        vbool64_t mask = vmsne_vv_u_b64m1_b64(v_first_block, v_current_block, block_size_v);\n        if (vmpopc_m_b64(mask) > 0) {\n            return false;\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false;\n    }\n\n    const int block_count = length / block_size;\n    const int vec_length = block_count * block_size;\n\n    vfloat64m1_t first_block = vle64_v_f64m1(&vec[0]);\n    vfloat64m1_t tolerance_v = vfmv_v_f_f64m1(tolerance);\n\n    for (int i = 1; i < block_count; i++) {\n        vfloat64m1_t current_block = vle64_v_f64m1(&vec[i * block_size]);\n\n        vbool64_t mask = vmsne_vv_u_b64m1_b64(current_block, first_block);\n        if (vmpopc_m_b64(mask) > 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    size_t block_elements = block_size / 8;\n\n    vfloat64m8_t first_block = vlse64_v_f64m8(&vec[0], block_elements);\n    vfloat64m8_t current_block;\n\n    for (size_t i = 1; i < num_blocks; i++) {\n        current_block = vlse64_v_f64m8(&vec[i * block_elements], block_elements);\n        vbool8_t mask = vmsne_vv_u_b8_f64m8(vfsub_vv_f64m8(current_block, first_block), 0.0, block_elements);\n        if (vmpopc_m_b8(mask) > 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false;\n    }\n\n    const size_t block_count = length / block_size;\n    const size_t vec_length = block_count * block_size;\n\n    vfloat64m1_t first_block = vle64_v_f64m1(&vec[0]);\n    vfloat64m1_t tolerance_v = vfmv_v_f_f64m1(tolerance);\n\n    for (size_t i = 1; i < block_count; i++) {\n        vfloat64m1_t current_block = vle64_v_f64m1(&vec[i * block_size]);\n\n        vbool64_t cmp_result = vmsne_vv_u_b64m1_b64(current_block, first_block);\n        if (vmpopc_m_b64(cmp_result) > 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    for (i = 0; i < rows; i += VL) {\n        vfloat64m8_t norm = vfmv_v_f_f64m8(0.0);\n        vfloat64m8_t threshold = vle64_v_f64m8(thresholds + i, vl);\n        for (j = 0; j < cols; j += VL) {\n            vfloat64m8_t data = vle64_v_f64m8(matrix + i * cols + j, vl);\n            norm = vfadd_vv_f64m8(norm, vfmul_vv_f64m8(data, data, vl), vl);\n        }\n        vbool8_t cmp = vfgt_vv_f64m8_b8(norm, threshold, vl);\n        vse8_v_i8m1(result + i, vmfirst_m_b8(cmp), vl);\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    for (i = 0; i < rows; i += VL) {\n        vfloat64m8_t norm = vfmv_v_f_f64m8(0.0);\n        vfloat64m8_t threshold = vle64_v_f64m8(thresholds + i, vl);\n        for (j = 0; j < cols; j += VL) {\n            vfloat64m8_t data = vle64_v_f64m8(matrix + i * cols + j, vl);\n            vfloat64m8_t diff = vfsub_vv_f64m8(data, threshold, vl);\n            vfloat64m8_t square = vfmul_vv_f64m8(diff, diff, vl);\n            norm = vfadd_vv_f64m8(norm, square, vl);\n        }\n        vbool8_t cmp = vfgt_vv_f64m8_b8(norm, threshold, vl);\n        vsseg8_v_i1(result + i, cmp, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    for (i = 0; i < rows; i += VL) {\n        vfloat64m8_t norm = vmv_v_f_f64m8(0.0);\n        vfloat64m8_t threshold = vle64_v_f64m8(thresholds + i);\n        for (j = 0; j < cols; j += VL) {\n            vfloat64m8_t data = vle64_v_f64m8(matrix + i * cols + j);\n            norm = vfadd_vv_f64m8(norm, vfmul_vv_f64m8(data, data));\n        }\n        vbool8_t cmp = vfgt_vv_f64m8_b8(vfsqrt_v_f64m8(norm), threshold);\n        vse8_v_i8m1_m(cmp, result + i, 1);\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    vfloat64m8_t vec_thresholds, vec_matrix, vec_norm;\n    vbool8_t mask;\n    \n    for (i = 0; i < rows; i++) {\n        vec_thresholds = vle64_v_f64m8(thresholds + i, cols);\n        vec_norm = vfmv_v_f_f64m8(0.0, 0);\n        \n        for (j = 0; j < cols; j += VL * 8) {\n            vec_matrix = vle64_v_f64m8(matrix + i * cols + j, cols - j < VL * 8 ? cols - j : VL * 8);\n            vec_norm = vfadd_vv_f64m8_f64m8_m(mask, vec_norm, vfncvt_f_f_w_f64m8(vfwmul_vv_f64m8(vec_matrix, vec_matrix, mask), 0, mask), mask);\n        }\n        \n        mask = vfgt_vv_f64m8_b8(vec_norm, vec_thresholds, 0);\n        vse8_v_i8m1(result + i, vmsne_vv_i8m1_b8(mask, vle8_v_i8m1(result + i, 0), 0), 0);\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    for (i = 0; i < rows; i += VL) {\n        vfloat64m8_t norm = vfmv_v_f_f64m8(0.0);\n        vbool8_t cmp_result = vmslt_vv_f64m8_b8(norm, vle64_v_f64m8(thresholds + i, VL));\n        for (j = 0; j < cols; j += VL) {\n            vfloat64m8_t vec1 = vle64_v_f64m8(matrix + i * cols + j, VL);\n            norm = vfadd_vv_f64m8(norm, vfncvt_f_x_v_f64m8(vec1));\n        }\n        vbool8_t mask = vmand_mm_b8(cmp_result, vmslt_vv_f64m8_b8(norm, vle64_v_f64m8(thresholds + i, VL)));\n        vsseg8_v_i8m1_m(mask, result + i, vmand_mm_b8(mask, vmslt_vx_f64m8_b8(norm, 0.0)));\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i], vl);\n        vuint32m1_t vb = vle32_v_u32m1(&B[i], vl);\n        vuint32m1_t result = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n    for (; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i], vl);\n        vuint32m1_t vb = vle32_v_u32m1(&B[i], vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i], vl);\n        vuint32m1_t vb = vle32_v_u32m1(&B[i], vl);\n        vuint32m1_t result = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vuint32m1_t vb = vle32_v_u32m1(&B[i]);\n        vuint32m1_t result = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(&C[i], result);\n    }\n}\n```  "}
{"task_id": "SimdBench_40_RVV", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i], vl);\n        vuint32m1_t vb = vle32_v_u32m1(&B[i], vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t result = 0;\n    int8x8_t a, b, a_sq, b_sq, a_mul_b;\n    int64x8_t sum_sq = {0}, sum_mul = {0};\n\n    for (size_t i = 0; i < length; i += 8) {\n        a = vle8_v_i8m1(&A[i]);\n        b = vle8_v_i8m1(&B[i]);\n\n        a_sq = vmul_vv_i8m1(a, a);\n        b_sq = vmul_vv_i8m1(b, b);\n        a_mul_b = vmul_vv_i8m1(a, b);\n\n        sum_sq = vadd_vx_i64m1(sum_sq, vdotusub_vv_i8m1(a_sq, a_sq, sum_sq));\n        sum_mul = vadd_vx_i64m1(sum_mul, vdotusub_vv_i8m1(a_mul_b, a_mul_b, sum_mul));\n    }\n\n    result = vmsub_vx_i64m1(sum_sq, sum_mul, result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t result = 0;\n    int8x8_t a, b;\n    int16x8_t a_sq, b_sq, a_mul_b;\n    int64x8_t sum_a_sq = (int64x8_t){0};\n    int64x8_t sum_a_mul_b = (int64x8_t){0};\n\n    for (size_t i = 0; i < length; i += 8) {\n        a = vlse8_v_i8m1(&A[i], 0);\n        b = vlse8_v_i8m1(&B[i], 0);\n\n        a_sq = vmulh_vx_i16m1(vwadd_vx_i16m1(vwmulu_vv_i16m1(a, a, 0), 0), 0);\n        b_sq = vmulh_vx_i16m1(vwadd_vx_i16m1(vwmulu_vv_i16m1(b, b, 0), 0), 0);\n        a_mul_b = vmulh_vx_i16m1(vwadd_vx_i16m1(vwmulu_vv_i16m1(a, b, 0), 0), 0);\n\n        sum_a_sq = vwredsum_vsx_i64m1(sum_a_sq, vwmulu_vv_i16m1(a_sq, (int16x8_t){1}, 0), 0);\n        sum_a_mul_b = vwredsum_vsx_i64m1(sum_a_mul_b, vwmulu_vv_i16m1(a_mul_b, (int16x8_t){1}, 0), 0);\n    }\n\n    for (int i = 0; i < 8; i++) {\n        result += sum_a_sq[i] - sum_a_mul_b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t result = 0;\n    int8x8_t a, b;\n    int16x8_t a_sq, b_sq, a_mul_b;\n    int32x4_t sum_a_sq, sum_b_sq, sum_a_mul_b;\n    \n    sum_a_sq = vwmul_vv_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_vx_i32(vwadd_v"}
{"task_id": "SimdBench_41_RVV", "completion": "```\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t result = 0;\n    int8x8_t a, b;\n    int16x8_t a_sq, b_sq, a_mul_b;\n    int64x8_t sum_a_sq = (int64x8_t){0};\n    int64x8_t sum_b_sq = (int64x8_t){0};\n    int64x8_t sum_a_mul_b = (int64x8_t){0};\n\n    for (size_t i = 0; i < length; i += 8) {\n        a = vlse8_v_i8m1(&A[i], 0);\n        b = vlse8_v_i8m1(&B[i], 0);\n\n        a_sq = vmulh_vx_i16(a, a, 0);\n        b_sq = vmulh_vx_i16(b, b, 0);\n        a_mul_b = vmulh_vx_i16(a, b, 0);\n\n        sum_a_sq = vwredsum_vsx_i16(sum_a_sq, a_sq, sum_a_sq, 0);\n        sum_b_sq = vwredsum_vsx_i16(sum_b_sq, b_sq, sum_b_sq, 0);\n        sum_a_mul_b = vwredsum_vsx_i16(sum_a_mul_b, a_mul_b, sum_a_mul_b, 0);\n    }\n\n    result = vadd_vx_i64(sum_a_sq, sum_b_sq, 0);\n    result = vsub_vx_i64(result, sum_a_mul_b, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    int64_t result = 0;\n    int8x8_t va, vb;\n    int16x8_t va_sq, vb_sq, va_vb;\n    int32x4_t va_sq_lo, va_sq_hi, vb_sq_lo, vb_sq_hi, va_vb_lo, va_vb_hi;\n    int64x2_t sum_sq, sum_prod, diff;\n\n    for (size_t i = 0; i < length; i += 8) {\n        va = vlse8_v_i8m1(A, 1);\n        vb = vlse8_v_i8m1(B, 1);\n\n        va_sq = vmulh_vx_i16m1(va, va);\n        vb_sq = vmulh_vx_i16m1(vb, vb);\n        va_vb = vmulh_vx_i16m1(va, vb);\n\n        va_sq_lo = vslide1down_vx_i32m4(vwadd_vv_i32m4(va_sq, va_sq));\n        va_sq_hi = vslide1down_vx_i32m4(vwadd_vv_i32m4(vwadd_vv_i32m4(va_sq_lo, va_sq_lo), va_sq_lo));\n\n        vb_sq_lo = vslide1down_vx_i32m4(vwadd_vv_i32m4(vwadd_vv_i32m4(vb_sq, vb_sq), vb_sq));\n        vb_sq_hi = vslide1down_vx_i32m4(vwadd_vv_i32m4(vwadd_vv_i32m4(vb_sq_lo, vb_sq_lo), vb_sq_lo));\n\n        va_vb_lo = vslide1down_vx_i32m4(vwadd_vv_i32m4(va_vb, va_vb));\n        va_vb_hi = vslide1down_vx_i32m4(vwadd_vv_i32m4(vwadd_vv_i32m4(va_vb_lo, va_vb_lo), va_vb_lo));\n\n        sum_sq = vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i64m2(vwadd_vv_i"}
{"task_id": "SimdBench_42_RVV", "completion": "```\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint64m1_t va = vle64_v_u64m1(&A[i], vl);\n        vuint64m1_t vb = vle64_v_u64m1(&B[i], vl);\n        vuint64m1_t result = vand_vv_u64m1(va, vnot_v_u64m1(vb));\n        vse64_v_u64m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint64m1_t va = vle64_v_u64m1(&A[i], vl);\n        vuint64m1_t vb = vle64_v_u64m1(&B[i], vl);\n        vuint64m1_t result = vand_vv_u64m1(va, vnot_v_u64m1(vb));\n        vse64_v_u64m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n    \n    for (; i < length; i += vl) {\n        vuint64m1_t va = vle64_v_u64m1(&A[i], vl);\n        vuint64m1_t vb = vle64_v_u64m1(&B[i], vl);\n        vuint64m1_t result = vand_vv_u64m1(va, vnot_v_u64m1(vb));\n        vse64_v_u64m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vuint64m1_t va = vle64_v_u64m1(&A[i], vl);\n        vuint64m1_t vb = vle64_v_u64m1(&B[i], vl);\n        vuint64m1_t result = vand_vv_u64m1(va, vnot_v_u64m1(vb));\n        vse64_v_u64m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vuint64m1_t va = vle64_v_u64m1(&A[i], vl);\n        vuint64m1_t vb = vle64_v_u64m1(&B[i], vl);\n        vuint64m1_t result = vand_vv_u64m1(va, vmv_x_s_u64m1(vnot_v_u64m1(vb)));\n        vse64_v_u64m1(&C[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(256);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint8m1_t va = vle8_v_i8m1(&A[i], vl);\n        vint8m1_t vb = vle8_v_i8m1(&B[i], vl);\n\n        vint8m1_t vpos = vmseq_vx_i8m1(va, 0, vl);\n        vint8m1_t vneg = vmseq_vx_i8m1(va, -1, vl);\n\n        vint8m1_t vand_pos = vand_vv_i8m1(vpos, vmseq_vx_i8m1(vb, 0, vl));\n        vint8m1_t vand_neg = vand_vv_i8m1(vneg, vmseq_vx_i8m1(vb, -1, vl));\n\n        vint8m1_t vresult = vadd_vv_i8m1(vand_pos, vand_neg);\n\n        vse8_v_i8m1(&C[i], vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(256); // Set the vector length\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint8m1_t va = vle8_v_i8m1(&A[i], vl); // Load vector A\n        vint8m1_t vb = vle8_v_i8m1(&B[i], vl); // Load vector B\n\n        vint8m1_t positive_mask = vand_vv_i8m1(va, vb); // Positive mask\n        vint8m1_t negative_mask = vand_vx_i8m1(vnot_v_i8m1(va), vnot_v_i8m1(vb)); // Negative mask\n\n        vint8m1_t result = vmerge_vvm_i8m1(positive_mask, negative_mask, vmsne_vv_i8m1(va, vb)); // Merge masks\n\n        vse8_v_i8m1(&C[i], result, vl); // Store the result\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(256);\n    size_t i = 0;\n\n    for (; i < length; i += vl) {\n        vint8m1_t va = vle8_v_i8m1(&A[i], vl);\n        vint8m1_t vb = vle8_v_i8m1(&B[i], vl);\n\n        vint8m1_t vpos_mask = vand_vv_i8m1(va, vb);\n        vint8m1_t vneg_mask = vand_vv_i8m1(vnot_v_i8m1(va), vnot_v_i8m1(vb));\n\n        vint8m1_t vresult = vmerge_vvm_i8m1(vpos_mask, vneg_mask, vpos_mask);\n\n        vse8_v_i8m1(&C[i], vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(0); // Set the vector length\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint8m1_t va = vle8_v_i8m1(&A[i], vl); // Load vector A\n        vint8m1_t vb = vle8_v_i8m1(&B[i], vl); // Load vector B\n\n        vint8m1_t vpos = vmseq_vx_i8m1(va, 0, vl); // Check if elements in A are positive\n        vint8m1_t vneg = vmseq_vx_i8m1(va, -1, vl); // Check if elements in A are negative\n\n        vint8m1_t vpos_b = vmseq_vx_i8m1(vb, 0, vl); // Check if elements in B are positive\n        vint8m1_t vneg_b = vmseq_vx_i8m1(vb, -1, vl); // Check if elements in B are negative\n\n        vint8m1_t vand_pos = vand_vv_i8m1(vpos, vpos_b, vl); // Both positive\n        vint8m1_t vand_neg = vand_vv_i8m1(vneg, vneg_b, vl); // Both negative\n\n        vint8m1_t vresult = vmerge_vvm_i8m1(vand_pos, vand_neg, vmv_x_s_i8m1(0, vl), vl); // Merge results\n\n        vse8_v_i8m1(&C[i], vresult, vl); // Store the result in C\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(256);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint8m1_t va = vle8_v_i8m1(&A[i], vl);\n        vint8m1_t vb = vle8_v_i8m1(&B[i], vl);\n\n        vint8m1_t vpos = vmseq_vx_i8m1(va, 0, vl);\n        vint8m1_t vneg = vmslt_vx_i8m1(va, 0, vl);\n\n        vint8m1_t vpos_b = vmseq_vx_i8m1(vb, 0, vl);\n        vint8m1_t vneg_b = vmslt_vx_i8m1(vb, 0, vl);\n\n        vint8m1_t vpos_and_pos_b = vand_vv_i8m1(vpos, vpos_b, vl);\n        vint8m1_t vneg_and_neg_b = vand_vv_i8m1(vneg, vneg_b, vl);\n\n        vint8m1_t vresult = vsub_vv_i8m1(vpos_and_pos_b, vneg_and_neg_b, vl);\n\n        vse8_v_i8m1(&C[i], vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t vl = vsetvl_e16m1(length);\n\n    vuint16m1_t va, vb, vc;\n    va = vle16_v_u16m1(A, vl);\n    vb = vle16_v_u16m1(B, vl);\n\n    vc = vand_vv_u16m1(va, vb, vl);\n    vse16_v_u16m1(C, vc, vl);\n\n    if (mid > 0) {\n        va = vle16_v_u16m1(A, mid);\n        vb = vle16_v_u16m1(B, mid);\n\n        vc = vxor_vv_u16m1(va, vb, mid);\n        vse16_v_u16m1(C, vc, mid);\n    }\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    const int VL = vsetvl_e16m1(0);\n    const int mid = length / 2;\n\n    for (size_t i = 0; i < mid; i += VL) {\n        vuint16m1_t va = vle16_v_u16m1(&A[i]);\n        vuint16m1_t vb = vle16_v_u16m1(&B[i]);\n        vuint16m1_t result = vxor_vv_u16m1(va, vb);\n        vse16_v_u16m1(&C[i], result);\n    }\n\n    if (mid < length) {\n        vuint16m1_t va = vle16_v_u16m1(&A[mid]);\n        vuint16m1_t vb = vle16_v_u16m1(&B[mid]);\n        vuint16m1_t result = va;\n        vse16_v_u16m1(&C[mid], result);\n    }\n\n    for (size_t i = mid + 1; i < length; i += VL) {\n        vuint16m1_t va = vle16_v_u16m1(&A[i]);\n        vuint16m1_t vb = vle16_v_u16m1(&B[i]);\n        vuint16m1_t result = vand_vv_u16m1(va, vb);\n        vse16_v_u16m1(&C[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    const int vl = vsetvl_e16m1(0);\n    const int mid = length / 2;\n\n    for (size_t i = 0; i < mid; i += vl) {\n        vuint16m1_t va = vle16_v_u16m1(&A[i], vl);\n        vuint16m1_t vb = vle16_v_u16m1(&B[i], vl);\n        vuint16m1_t result = vand_vv_u16m1(va, vb);\n        vse16_v_u16m1(&C[i], result, vl);\n    }\n\n    if (length % 2 != 0) {\n        C[mid] = A[mid];\n    }\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t vl = vsetvl_e16m1(length);\n\n    vuint16m1_t va, vb, vc;\n    va = vle16_v_u16m1(A, vl);\n    vb = vle16_v_u16m1(B, vl);\n\n    vc = vand_vv_u16m1(va, vb, vl);\n    vse16_v_u16m1(C, vc, vl);\n\n    if (mid > 0) {\n        va = vle16_v_u16m1(A, mid);\n        vb = vle16_v_u16m1(B, mid);\n\n        vc = vxor_vv_u16m1(va, vb, mid);\n        vse16_v_u16m1(C, vc, mid);\n    }\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    const size_t vl = vsetvl_e16m1(0);\n    const size_t mid = length / 2;\n\n    for (size_t i = 0; i < mid; i += vl) {\n        vuint16m1_t va = vle16_v_u16m1(&A[i]);\n        vuint16m1_t vb = vle16_v_u16m1(&B[i]);\n\n        vuint16m1_t xor_result = vxor_vv_u16m1(va, vb);\n        vuint16m1_t and_result = vand_vv_u16m1(va, vb);\n\n        vse16_v_u16m1(&C[i], xor_result);\n        vse16_v_u16m1(&C[i + vl / 2], va);\n        vse16_v_u16m1(&C[mid + i], and_result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vfloat32m1_t vc = vle32_v_f32m1(&C[i], vl);\n        vfloat32m1_t vd = vle32_v_f32m1(&D[i], vl);\n        vbool32_t vcond1 = vle32_v_u32m1(&cond1[i], vl);\n        vbool32_t vcond2 = vle32_v_u32m1(&cond2[i], vl);\n\n        vfloat32m1_t vout = vmv_v_f_f32m1(vd, vl); // Initialize with D values\n\n        vout = vmerge_vfm_f32m1(vout, va, vcond1, vl); // Select A values where cond1 is true\n        vout = vmerge_vfm_f32m1(vout, vb, vcond1 ^ 1, vl); // Select B values where cond1 is false\n        vout = vmerge_vfm_f32m1(vout, vc, vcond2, vl); // Select C values where cond2 is true\n        vout = vmerge_vfm_f32m1(vout, vd, vcond2 ^ 1, vl); // Select D values where cond2 is false\n\n        vse32_v_f32m1(&out[i], vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vfloat32m1_t vc = vle32_v_f32m1(&C[i], vl);\n        vfloat32m1_t vd = vle32_v_f32m1(&D[i], vl);\n\n        vbool32_t vcond1 = vle32_v_u32m1(&cond1[i], vl);\n        vbool32_t vcond2 = vle32_v_u32m1(&cond2[i], vl);\n\n        vfloat32m1_t vout = vmv_v_f_f32m1(vd, vl); // Initialize with D\n\n        vout = vmerge_vfm_f32m1(vout, va, vcond1, vl); // Select A where cond1 is true\n        vout = vmerge_vfm_f32m1(vout, vb, vcond1 ^ 1, vl); // Select B where cond1 is false\n        vout = vmerge_vfm_f32m1(vout, vc, vcond2, vl); // Select C where cond2 is true\n        vout = vmerge_vfm_f32m1(vout, vd, vcond2 ^ 1, vl); // Select D where cond2 is false\n\n        vse32_v_f32m1(&out[i], vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vfloat32m1_t vc = vle32_v_f32m1(&C[i], vl);\n        vfloat32m1_t vd = vle32_v_f32m1(&D[i], vl);\n        vbool32_t vcond1 = vle32_v_u32m1(&cond1[i], vl);\n        vbool32_t vcond2 = vle32_v_u32m1(&cond2[i], vl);\n\n        vfloat32m1_t vout = vmv_v_f_f32m1(vd, vl);\n        vout = vmerge_vfm_f32m1(vout, va, vcond1, vl);\n        vout = vmerge_vfm_f32m1(vout, vb, vcond2, vl);\n        vout = vmerge_vfm_f32m1(vout, vc, vand_vv_b32m1(vnot_v_b32m1(vcond1), vnot_v_b32m1(vcond2), vl), vl);\n\n        vse32_v_f32m1(&out[i], vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vfloat32m1_t vc = vle32_v_f32m1(&C[i], vl);\n        vfloat32m1_t vd = vle32_v_f32m1(&D[i], vl);\n        vbool32_t vcond1 = vle32_v_u32m1(&cond1[i], vl);\n        vbool32_t vcond2 = vle32_v_u32m1(&cond2[i], vl);\n\n        vfloat32m1_t vout = vmv_v_f_f32m1(vd, vl); // Initialize with D\n\n        vout = vmerge_vfm_f32m1(vout, va, vcond1, vl); // Select A where cond1 is true\n        vout = vmerge_vfm_f32m1(vout, vb, vcond1 ^ 1, vl); // Select B where cond1 is false\n        vout = vmerge_vfm_f32m1(vout, vc, vcond2, vl); // Select C where cond2 is true\n        vout = vmerge_vfm_f32m1(vout, vd, vcond2 ^ 1, vl); // Select D where cond2 is false\n\n        vse32_v_f32m1(&out[i], vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vfloat32m1_t vc = vle32_v_f32m1(&C[i], vl);\n        vfloat32m1_t vd = vle32_v_f32m1(&D[i], vl);\n\n        vbool32_t vcond1 = vle32_v_u32m1(&cond1[i], vl);\n        vbool32_t vcond2 = vle32_v_u32m1(&cond2[i], vl);\n\n        vfloat32m1_t vout = vmv_v_f_f32m1(vd, vl); // Initialize with D\n\n        vout = vmerge_vfm_f32m1(vout, va, vcond1 & vcond2, vl); // Update with A where cond1 and cond2 are true\n        vout = vmerge_vfm_f32m1(vout, vb, vcond1 & ~vcond2, vl); // Update with B where cond1 is true and cond2 is false\n        vout = vmerge_vfm_f32m1(vout, vc, ~vcond1 & vcond2, vl); // Update with C where cond1 is false and cond2 is true\n\n        vse32_v_f32m1(&out[i], vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t remaining = total_size % vsetvl_e32m1();\n\n    for (size_t i = 0; i < total_size - remaining; i += vsetvl_e32m1()) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vuint8m1_t popcnt = vpopc_vvm_u8m1(va);\n        vse8_v_u8m1(&out[i], popcnt);\n    }\n\n    if (remaining > 0) {\n        vuint32m1_t va = vle32_v_u32m1(&A[total_size - remaining]);\n        vuint8m1_t popcnt = vpopc_vvm_u8m1(va);\n        vse8_v_u8m1(&out[total_size - remaining], popcnt);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```cpp\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n    vuint32m4_t va;\n    vuint8m1_t vout;\n    for (i = 0; i < total_size; i += VL * 4) {\n        va = vle32_v_u32m4(&A[i]);\n        vout = vmseq_v_u8m1_i(va, 1);\n        vse8_v_u8m1(&out[i], vpopc_m(vout, vout));\n    }\n}\n```  "}
{"task_id": "SimdBench_46_RVV", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t remaining = total_size % vsetvl_e32m1();\n\n    for (size_t i = 0; i < total_size - remaining; i += vsetvl_e32m1()) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vuint8m1_t popcnt = vpopc_v_u32m1(va);\n        vse8_v_u8m1(&out[i], popcnt);\n    }\n\n    if (remaining > 0) {\n        vuint32m1_t va = vle32_v_u32m1(&A[total_size - remaining]);\n        vuint8m1_t popcnt = vpopc_v_u32m1(va);\n        vse8_v_u8m1(&out[total_size - remaining], popcnt);\n    }\n}\n```  "}
{"task_id": "SimdBench_46_RVV", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t remaining = total_size % vsetvl_e32m1();\n\n    vuint32m1_t va;\n    vuint8m1_t vout;\n    vuint8m1_t vcount = vmv_x_s(0, 8);\n\n    for (size_t i = 0; i < total_size - remaining; i += vsetvl_e32m1()) {\n        va = vlse32_v_u32m1(&A[i], vsetvl_e32m1());\n        vout = vpopc_vvm_u8m1(va);\n        vse8_v_u8m1(&out[i], vout, vsetvl_e32m1());\n    }\n\n    if (remaining > 0) {\n        va = vlse32_v_u32m1(&A[total_size - remaining], remaining);\n        vout = vpopc_vvm_u8m1(va);\n        vse8_v_u8m1(&out[total_size - remaining], vout, remaining);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vuint8m1_t result = vmseq_vx_u8m1(va, 1);\n        uint8_t* temp_out = &out[i];\n        vse8_v_u8m1(temp_out, result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t vl = vsetvl_e32m1(size);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m1_t va = vle32_v_i32m1(&A[i]);\n        vbool32_t mask = vremu_vx_i32m1_b32(va, 3);\n        vint32m1_t result = vmerge_vvm_i32m1(mask, vabs_vx_i32m1(va), va);\n        vse32_v_i32m1(&B[i], result);\n    }\n}\n```  "}
{"task_id": "SimdBench_47_RVV", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t vl = vsetvl_e32m1(size);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m1_t va = vle32_v_i32m1(&A[i], vl);\n        vbool32_t mask = vmseq_vx_i32m1_b32(va, 0, vl);\n        vint32m1_t vb = vabs_v_i32m1(va, vl);\n        vint32m1_t vc = vmerge_vvm_i32m1(mask, vb, va, vl);\n        vse32_v_i32m1(&B[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_size = rows * cols;\n    size_t vl = vsetvl_e32m1(total_size);\n\n    for (size_t i = 0; i < total_size; i += vl) {\n        vint32m1_t va = vle32_v_i32m1(&A[i]);\n        vbool32_t mask = vremu_vx_i32m1_b32(va, 3);\n        vint32m1_t result = vmerge_vvm_i32m1(mask, vabs_v_i32m1(va), va);\n        vse32_v_i32m1(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t vl = vsetvl_e32m1(size);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m1_t va = vle32_v_i32m1(&A[i]);\n        vbool32_t mask = vmv_x_s_b32m1(vremu_vx_i32m1(va, 3));\n        vint32m1_t result = vmerge_vvm_i32m1(mask, vabs_v_i32m1(va), va);\n        vse32_v_i32m1(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const int vl = vsetvl_e32m1(0); // Set the vector length\n    const int* ptrA = A;\n    int* ptrB = B;\n\n    for (size_t i = 0; i < rows * cols; i += vl) {\n        vint32m1_t va = vle32_v_i32m1(ptrA, vl); // Load vector A\n        vbool32_t mask = vremu_vx_i32m1_b32(va, 3); // Check if elements are divisible by 3\n        vint32m1_t vb = vabs_v_i32m1(va); // Calculate absolute value of A\n        vint32m1_t result = vmerge_vvm_i32m1(mask, vb, va); // Merge based on condition\n        vse32_v_i32m1(ptrB, result, vl); // Store result in B\n        ptrA += vl;\n        ptrB += vl;\n    }\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    vuint32m4_t v_src, v_dst, v_mask;\n    vuint32m4_t v_ones = vmv_x_s_u32(0xFFFFFFFF);\n    vint32m4_t v_rotate_bits = vmv_s_x_i32(rotate_bits);\n    \n    for(size_t i = 0; i < length; i += VL * 4){\n        v_src = vle32_v_u32m4(&src[i]);\n        v_mask = vand_vv_u32m4(v_src, v_ones);\n        \n        v_dst = vrgather_vx_u32m4(v_mask, &src[i], v_rotate_bits);\n        v_dst = vmerge_vxm_u32m4(v_mask, vnot_v_u32m4(v_src), v_dst);\n        \n        vse32_v_u32m4(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t vl = vsetvli(0, RVV_E32, RVV_M8);\n    vuint32m8_t src_v = vle32_v_u32m8(src, vl);\n    vbool4_t mask = VM4(0xF, 0xF, 0xF, 0xF);\n    vuint32m8_t rotated = vrgather_vx_u32m8(src_v, rotate_bits, vl);\n    vuint32m8_t negated = vmv_x_s_u32m8(0xFFFFFFFF, vl) ^ src_v;\n    vuint32m8_t result = vmerge_vvm_u32m8(mask, rotated, negated, vl);\n    vse32_v_u32m8(dst, result, vl);\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    int32_t vl = vsetvli(0, RVV_E32, RVV_M8);\n    vuint32m8_t v_src, v_dst, v_mask;\n    vint32m8_t v_rotated;\n\n    for (size_t i = 0; i < length; i += 8) {\n        v_src = vle32_v_u32m8(&src[i]);\n        v_mask = vmsne_v_u32m8_b8(vand_vx_u32m8(v_src, 0x0000000Fu), 0x0000000Fu);\n        v_rotated = vrgather_vx_i32m8(v_src, rotate_bits, v_mask);\n        v_dst = vmv_v_x_u32m8(vnot_v_u32m8(v_src), v_rotated, v_mask);\n        vse32_v_u32m8(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    vuint32m4_t v_src, v_dst, v_mask;\n    vint32m4_t v_negate;\n    v_src = vle32_v_u32m4(src, length);\n    v_mask = vmsne_v_u32m4_b4m1(vsrc, 0xFFFFFFFF, length);\n    v_dst = vrgather_vx_u32m4(v_mask, src, 0, length);\n    v_negate = vmseq_v_u32m4(v_mask, 0, length);\n    v_dst = vrgather_vx_u32m4(v_negate, src, 0, length);\n    vse32_v_u32m4(dst, v_dst, length);\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    int32_t vl = vsetvl_e32m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vbool32_t mask = vmseq_vx_u32m1_b32(vsrc, 0xFFFFFFFF, vl);\n    vuint32m1_t v_rotated = vrgather_vx_u32m1(vsrc, rotate_bits, vl);\n    vuint32m1_t v_negated = vnot_v_u32m1(vsrc, vl);\n    vuint32m1_t v_result = vmerge_vvm_u32m1(mask, v_rotated, v_negated, vl);\n    vse32_v_u32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, vb, vl);\n        vfloat32m1_t abs_diff = vfsgnjx_vf_f32m1(vfsub_vv_f32m1(va, vb, vl), 0.0, vl);\n        vfloat32m1_t abs_epsilon = vfmv_v_f_f32m1(epsilon, vl);\n\n        vmask_t cmp_gt = vmslt_vf_f32m1_b32(abs_diff, abs_epsilon, vl);\n        vmask_t cmp_lt = vmsgt_vf_f32m1_b32(abs_diff, abs_epsilon, vl);\n\n        vint8m1_t result = vmerge_vvm_i8m1(cmp_gt, 1, cmp_lt, -1, 0, vl);\n\n        vse8_v_i8m1(&out[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, vb, vl);\n        vfloat32m1_t abs_diff = vfsgnjx_vv_f32m1(diff, diff, vl);\n\n        vfloat32m1_t ve = vfmv_v_f_f32m1(epsilon, vl);\n        vbool32_t cmp_gt = vfgt_vf_f32m1_b32(abs_diff, ve, vl);\n        vbool32_t cmp_lt = vflt_vf_f32m1_b32(abs_diff, vfneg_v_f_f32m1(ve, vl), vl);\n\n        vint8m1_t res = vmerge_vvm_i8m1(cmp_gt, vmerge_vvm_i8m1(cmp_lt, vzero_i8m1(), -1, vl), 1, vl);\n\n        vse8_v_i8m1(&out[i], res, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, vb, vl);\n        vfloat32m1_t abs_diff = vfsgnjx_vv_f32m1(diff, diff, vl);\n\n        vfloat32m1_t ve = vfmv_v_f_f32m1(epsilon, vl);\n        vfloat32m1_t abs_epsilon = vfsgnjx_vv_f32m1(ve, ve, vl);\n\n        vbool32_t mask_gt = vfgt_vv_u32m1_b32(abs_diff, abs_epsilon, vl);\n        vbool32_t mask_lt = vfle_vv_u32m1_b32(abs_diff, abs_epsilon, vl);\n\n        vint8m1_t result = vmerge_vvm_i8m1(mask_gt, 1, mask_lt, -1, 0, vl);\n\n        vse8_v_i8m1(&out[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, vb, vl);\n        vfloat32m1_t abs_diff = vfmv_v_f_f32m1(epsilon, vl);\n        abs_diff = vfmv_v_f_f32m1(fabsf(epsilon), vl);\n\n        vmask_t mask_gt = vfgt_vf_f32m1_b1(diff, abs_diff, vl);\n        vmask_t mask_lt = vfle_vf_f32m1_b1(diff, vfneg_v_f_f32m1(abs_diff, vl), vl);\n\n        vint8m1_t result = vmv_x_s_i8m1(0, vl);\n        result = vmerge_vvm_i8m1(mask_gt, result, 1, vl);\n        result = vmerge_vvm_i8m1(mask_lt, result, -1, vl);\n\n        vse8_v_i8m1(&out[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, vb, vl);\n        vfloat32m1_t abs_diff = vfsgnjx_vf_f32m1(vfsub_vf_f32m1(vfmax_vf_f32m1(diff, epsilon, vl), epsilon, vl), epsilon, vl);\n\n        vint8m1_t result = vmslt_vx_i8m1_b(vfsub_vf_f32m1(va, vb, vl), 0, vl);\n        result = vmerge_vvm_i8m1(result, vmslt_vx_i8m1_b(vfsub_vf_f32m1(vb, va, vl), 0, vl), vmslt_vx_i8m1_b(abs_diff, 0, vl), vl);\n\n        vse8_v_i8m1(&out[i], result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    size_t i = 0;\n    for (; i < size / VL_E32M1; i++) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i * VL_E32M1], vl);\n        vint32m1_t vb = vfcvt_x_f_v_i32m1(va, 0, vl);\n        vse32_v_i32m1(&B[i * VL_E32M1], vb, vl);\n    }\n    size_t remaining = size % VL_E32M1;\n    if (remaining > 0) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i * VL_E32M1], remaining);\n        vint32m1_t vb = vfcvt_x_f_v_i32m1(va, 0, remaining);\n        vse32_v_i32m1(&B[i * VL_E32M1], vb, remaining);\n    }\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    size_t i = 0;\n\n    for (; i < size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i]);\n        vint32m1_t vb = vfcvt_x_f_v_i32m1(va, 0, VV_FNONE, &i);\n        vse32_v_i32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    size_t i = 0;\n\n    for (; i < size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i]);\n        vint32m1_t vb = vfcvt_x_f_v_i32m1(va, 0, 0);\n        vb = vmax_vx_i32m1(vb, INT32_MIN);\n        vb = vmin_vx_i32m1(vb, INT32_MAX);\n        vse32_v_i32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    int32_t INT32_MAX_VEC = vsetvlmax_e32m1();\n    int32_t INT32_MIN_VEC = vsetvlmax_e32m1();\n    vint32m1_t a, b, int32_max, int32_min;\n    vfloat32m1_t input;\n\n    int32_max = vmv_v_x_i32m1(INT32_MAX_VEC, INT32_MAX);\n    int32_min = vmv_v_x_i32m1(INT32_MIN_VEC, INT32_MIN);\n\n    for (size_t i = 0; i < size; i += VL) {\n        size_t vl = (size - i) < VL ? (size - i) : VL;\n        input = vle32_v_f32m1(&A[i], vl);\n        a = vfncvt_x_f_v_i32m1_int32m1(input);\n        b = vssmax_vv_i32m1(a, int32_min);\n        b = vssmin_vv_i32m1(b, int32_max);\n        vse32_v_i32m1(&B[i], b, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va;\n    vint32m1_t vb;\n    vint32m1_t vmax = vmv_v_x_i32m1(INT32_MAX);\n    vint32m1_t vmin = vmv_v_x_i32m1(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_f32m1(&A[i], vl);\n        vb = vfcvt_x_f_v_i32m1(va, 0, vl);\n        vb = vmin_vx_i32m1(vb, vmin, vl);\n        vb = vmax_vx_i32m1(vb, vmax, vl);\n        vse32_v_i32m1(&B[i], vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    int32_t* A_ptr = (int32_t*)A;\n    float* B_ptr = (float*)B;\n    int32_t step = vsetvl_e32m1(0);\n\n    int32_t v_scale = vmv_s_x_i32(scale);\n    float32_t v_scale_f = vfwcvt_f_x_v_f32m1(v_scale, 0);\n\n    for (size_t i = 0; i < size; i += step) {\n        step = vsetvl_e32m1(size - i);\n\n        vint32m1_t vin = vlse32_v_i32m1(A_ptr, step);\n        vfloat32m1_t vout = vfncvt_f_x_v_f32m1(vin, 0);\n\n        vfloat32m1_t scaled_vout = vfmul_vf_f32m1(vout, v_scale_f, 0);\n\n        vse32_v_f32m1(B_ptr, scaled_vout, step);\n\n        A_ptr += step;\n        B_ptr += step;\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int vl = vsetvl_e32m1(vl);\n\n    int32_t* A_ptr = (int32_t*) A;\n    float* B_ptr = (float*) B;\n\n    int32_tv0 A_vec;\n    float32v0 B_vec;\n\n    int32_t scale_vec = (int32_t) scale;\n\n    for (size_t i = 0; i < size; i += VL) {\n        size_t remaining = size - i;\n        size_t current_vl = (remaining < VL) ? remaining : VL;\n\n        A_vec = vle32_v_i32m1(A_ptr, current_vl);\n        B_vec = vfncvt_f_w_f32m1_f32m1(A_vec, 0, current_vl);\n        B_vec = vfmul_vf_f32m1(B_vec, (float32_t) scale_vec, current_vl);\n        vse32_v_f32m1(B_ptr, B_vec, current_vl);\n\n        A_ptr += current_vl;\n        B_ptr += current_vl;\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int vl = vsetvl_e32m1(vl);\n    const int32_t* ptrA = A;\n    float32_t* ptrB = (float32_t*)B;\n    int32_tv inputA;\n    float32_t scaleF = (float32_t)scale;\n\n    for (size_t i = 0; i < size; i += VL) {\n        inputA = vle32_v_i32m1(ptrA, vl);\n        vse32_v_f32m1(ptrB, vfncvt_f_f_w_f32m1(inputA, vl), vl);\n        ptrA += VL;\n        ptrB += VL;\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    int32_t* A_ptr = (int32_t*)A;\n    float* B_ptr = (float*)B;\n    int32_t vl = vsetvl_e32m1(size);\n\n    int32_t scale_vec = (int32_t)scale;\n    vint32m1_t vin;\n    vfloat32m1_t vout;\n\n    for (size_t i = 0; i < size; i += vl) {\n        vin = vle32_v_i32m1(A_ptr + i, vl);\n        vout = vfncvt_f_x_v_f32m1(vin, vl);\n        vout = vfmul_vf_f32m1(vout, (float)scale_vec, vl);\n        vse32_v_f32m1(B_ptr + i, vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const int vl = vsetvl_e32m1(vl);\n    const int32_t* ptrA = A;\n    float* ptrB = B;\n    int32_tv1_t vecA;\n    float32v1_t vecB;\n\n    int32_t scale_vec = (int32_t)scale;\n\n    for (size_t i = 0; i < size; i += VL) {\n        size_t chunk_size = size - i < VL ? size - i : VL;\n        vecA = vlseg2e32_v_i32m1(ptrA, vl);\n        vecB = vfncvt_f_x_v_f32m1(vecA, vl);\n        vecB = vfmul_vf_f32m1_m(vfslide1up_vf_f32m1(vecB, 1, vl), (float)scale_vec, vecB, vl);\n        vse32_v_f32m1(ptrB, vecB, vl);\n        ptrA += VL;\n        ptrB += VL;\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvli(size, RVV_E32, RVV_M8);\n    vfloat32m8_t va, vb;\n    vint32m8_t vc;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_f32m8(&A[i]);\n        vb = vle32_v_f32m8(&B[i]);\n        vc = vfcvt_x_f_v_i32m8(va / vb);\n        vse32_v_i32m8(&C[i], vc);\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va, vb;\n    vint32m1_t vc;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_f32m1(&A[i]);\n        vb = vle32_v_f32m1(&B[i]);\n        vc = vfcvt_x_f_v_i32m1(va / vb);\n        vse32_v_i32m1(&C[i], vc);\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        \n        vint32m1_t vc = vfcvt_x_f_v_i32m1(va / vb, 0, vl);\n        \n        vse32_v_i32m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vfloat32m1_t vb = vle32_v_f32m1(&B[i], vl);\n        vint32m1_t vc = vfcvt_x_f_v_i32m1(vfdiv_vv_f32m1(va, vb, vl), vl);\n        vse32_v_i32m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va, vb;\n    vint32m1_t vc;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_f32m1(&A[i], vl);\n        vb = vle32_v_f32m1(&B[i], vl);\n        vc = vfcvt_x_f_v_i32m1(va / vb, 0, vl);\n        vse32_v_i32m1(&C[i], vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n    double* current_A = A;\n    float* current_B = B;\n    \n    for (; i < size; i += vl) {\n        vl = vsetvl_e64m1(size - i);\n        vfloat64m1_t vA = vle64_v_f64m1(current_A, vl);\n        vfloat32m1_t vB = vfncvt_x_f_v_f32m1(vA, vl);\n        vfloat32m1_t vMax = vfmv_f_s_f32m1(FLT_MAX, vl);\n        vfloat32m1_t vMin = vfmv_f_s_f32m1(-FLT_MAX, vl);\n        vB = vfmax_vv_f32m1(vB, vMin, vl);\n        vB = vfmin_vv_f32m1(vB, vMax, vl);\n        vse32_v_f32m1(current_B, vB, vl);\n        current_A += vl;\n        current_B += vl;\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvli(size, RVV_FLOAT64);\n    size_t i = 0;\n    for (; i < size / vl * vl; i += vl) {\n        vfloat64m1_t va = vle64_v_f64m1(&A[i]);\n        vfloat32m1_t vb = vfncvt_f_f_w_f32m1(va);\n        vfloat32m1_t vc = vfmax_f_f_f32m1(vb, vfmv_f_s_f32m1(FLT_MIN));\n        vfloat32m1_t vd = vfmin_f_f_f32m1(vc, vfmv_f_s_f32m1(FLT_MAX));\n        vse32_v_f32m1(&B[i], vd);\n    }\n    for (; i < size; i++) {\n        B[i] = (float)A[i] > FLT_MAX ? FLT_MAX : ((float)A[i] < -FLT_MAX ? -FLT_MAX : (float)A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n\n    double* currentA = (double*) A;\n    float* currentB = (float*) B;\n\n    for (; i < size; i+=vl) {\n        double* chunkA = currentA + i;\n        float* chunkB = currentB + i;\n\n        vfloat64m1_t vA = vle64_v_f64m1(chunkA, vl);\n        vfloat32m1_t vB = vfncvt_f_f_w_f32m1(vA);\n\n        vfloat32m1_t vMax = vfmv_f_s_f32m1(FLT_MAX);\n        vfloat32m1_t vNegMax = vfmv_f_s_f32m1(-FLT_MAX);\n\n        vbool32_t maskGreaterThanMax = vfgt_vf_f32m1_b32(vB, vMax, vl);\n        vbool32_t maskLessThanNegMax = vflt_vf_f32m1_b32(vB, vNegMax, vl);\n\n        vB = vmerge_vfm_f32m1(maskGreaterThanMax, vMax, vB, vl);\n        vB = vmerge_vfm_f32m1(maskLessThanNegMax, vNegMax, vB, vl);\n\n        vse32_v_f32m1(chunkB, vB, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n    double* current_A = A;\n    float* current_B = B;\n    \n    for (; i < size; i += vl) {\n        vl = vsetvl_e64m1(size - i);\n        vfloat64m1_t vA = vle64_v_f64m1(current_A, vl);\n        vfloat32m1_t vB = vfncvt_rod_f_f_v_f32m1(vA, vl);\n        vfloat32m1_t vMax = vfmv_v_f_f32m1(FLT_MAX, vl);\n        vfloat32m1_t vMin = vfmv_v_f_f32m1(-FLT_MAX, vl);\n        vB = vfmax_vv_f32m1(vB, vMin, vl);\n        vB = vfmin_vv_f32m1(vB, vMax, vl);\n        vse32_v_f32m1(current_B, vB, vl);\n        \n        current_A += vl;\n        current_B += vl;\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e64m1(0);\n\n    for (size_t i = 0; i < size; i += vl) {\n        size_t cur_vl = vl;\n        if (i + vl > size) {\n            cur_vl = size - i;\n        }\n\n        vfloat64m1_t vA = vle64_v_f64m1(&A[i], cur_vl);\n        vfloat32m1_t vB = vfncvt_f_f_w_f32m1(vA);\n\n        vfloat32m1_t vMax = vfmv_f_s_f32m1(FLT_MAX);\n        vfloat32m1_t vMin = vfmv_f_s_f32m1(-FLT_MAX);\n\n        vbool32_t mask_gt_max = vfgt_vf_f32m1_b32(vB, vMax, cur_vl);\n        vbool32_t mask_lt_min = vfle_vf_f32m1_b32(vB, vMin, cur_vl);\n\n        vB = vmerge_vfm_f32m1(mask_gt_max, vMax, vB, cur_vl);\n        vB = vmerge_vfm_f32m1(mask_lt_min, vMin, vB, cur_vl);\n\n        vse32_v_f32m1(&B[i], vB, cur_vl);\n    }\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    vint16m1_t va;\n    vint32m1_t vb;\n    \n    for (size_t i = 0; i < size; i += vl) {\n        size_t cur_vl = (i + vl > size) ? size - i : vl;\n        va = vle16_v_i16m1(&A[i], cur_vl);\n        vb = vwmul_vv_i32m1(va, 1, cur_vl);\n        vse32_v_i32m1(&B[i], vb, cur_vl);\n    }\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    int16_t* ptr_A = (int16_t*) A;\n    int32_t* ptr_B = (int32_t*) B;\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint16m1_t vec_A = vle16_v_i16m1(ptr_A + i, vl);\n        vint32m1_t vec_B = vsext_vf2_i32m1(vec_A, vl);\n        vse32_v_i32m1(ptr_B + i, vec_B, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    size_t i = 0;\n\n    for (; i < size; i += vl) {\n        vint16m8_t va = vle16_v_i16m8(&A[i], vl);\n        vint32m4_t vb = vsext_vx_i32m4(va, 0, vl);\n        vse32_v_i32m4(&B[i], vb, vl);\n    }\n\n    vint16m8_t va = vle16_v_i16m8(&A[i], size - i);\n    vint32m4_t vb = vsext_vx_i32m4(va, 0, size - i);\n    vse32_v_i32m4(&B[i], vb, size - i);\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    size_t i = 0;\n\n    for (; i < size; i += vl) {\n        vint16m8_t va = vle16_v_i16m8(&A[i]);\n        vint32m4_t vb = vsext_vf2_i32m4(va);\n        vse32_v_i32m4(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint16m8_t va = vle16_v_i16m8(&A[i]);\n        vint32m4_t vb = vwmul_vx_i32m4(va, 1);\n        vse32_v_i32m4(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    int i;\n    int32_t mask_gt_zero;\n    vfloat32m8_t va, vb, vmin_val, vmax_val, vresult;\n\n    vmin_val = vmv_v_f_f32(min_val);\n    vmax_val = vmv_v_f_f32(max_val);\n\n    for (i = 0; i < size; i += VL * sizeof(float)) {\n        mask_gt_zero = vmslt_vx_i32m1_b8(control[i], 0);\n\n        va = vlseg8_v_f32m8(&A[i]);\n        vb = vlseg8_v_f32m8(&B[i]);\n\n        vresult = vfmul_vf_f32m8(va, 1.0f);\n        vresult = vfsub_vf_f32m8(vresult, vmin_val);\n        vresult = vfdiv_vv_f32m8(vresult, vmax_val - vmin_val);\n        vresult = vfredosum_vs_f32m8_f32m1(vresult, vresult, 0);\n        vresult = vfmv_v_f_f32m8(mask_gt_zero, vb, vresult);\n\n        vse8_v_f32m8(&B[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    int i;\n    vfloat32m8_t va, vb, vmin_val, vmax_val, vzero, vone;\n    vbool4_t mask;\n    \n    vmin_val = vmv_v_f_f32(min_val);\n    vmax_val = vmv_v_f_f32(max_val);\n    vzero = vmv_v_f_f32(0.0f);\n    vone = vmv_v_f_f32(1.0f);\n    \n    for (i = 0; i < size; i += VL) {\n        va = vlseg2e32_v_f32m8(&A[i]);\n        vb = vlseg2e32_v_f32m8(&B[i]);\n        \n        mask = vmslt_vx_i32m1_b4(vlseg2e32_v_i32m1(&control[i]), 0);\n        \n        vfloat32m8_t normalized = vfmul_vf_f32m8(va, vmin_val);\n        normalized = vfsub_vv_f32m8(normalized, vmin_val);\n        normalized = vfmul_vf_f32m8(normalized, vmax_val);\n        normalized = vfdiv_vv_f32m8(normalized, vmax_val);\n        \n        vb = vmerge_vvm_f32m8(mask, vb, normalized);\n        \n        vse32_v_f32m8(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    int i;\n    const int VL = vsetvl_e32m1(0);\n\n    for (i = 0; i < size; i += VL) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i]);\n        vint32m1_t vc = vle32_v_i32m1(&control[i]);\n        vfloat32m1_t vb = vfmv_v_f_f32m1(0.0f);\n\n        vbool32_t mask = vmslt_vx_i32m1_b32(vc, 0);\n        vfloat32m1_t normalized = vfmv_v_f_f32m1(0.0f);\n        \n        vfloat32m1_t min_val_v = vfmv_v_f_f32m1(min_val);\n        vfloat32m1_t max_val_v = vfmv_v_f_f32m1(max_val);\n\n        vfloat32m1_t diff = vfsub_vv_f32m1(va, min_val_v);\n        vfloat32m1_t range = vfsub_vv_f32m1(max_val_v, min_val_v);\n        normalized = vfmul_vv_f32m1(diff, vfredsum_vs_f32m1_f32m1(range, normalized, 0, mask));\n\n        vfloat32m1_t zero = vfmv_v_f_f32m1(0.0f);\n        vfloat32m1_t one = vfmv_v_f_f32m1(1.0f);\n        normalized = vfmv_v_f_f32m1(0.0f);\n        normalized = vfredsum_vs_f32m1_f32m1(vfredsum_vs_f32m1_f32m1(normalized, one, 0, vmsgt_vx_f32m1_b32(normalized, 1.0f)), zero, 0, vmslt_vx_f32m1_b32(normalized, 0.0f));\n\n        vfloat32m1_t result = vmerge_vvm_f32m1(normalized, va, mask);\n        vse32_v_f32m1(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vfloat32m1_t va = vle32_v_f32m1(&A[i], vl);\n        vint32m1_t vc = vle32_v_i32m1(&control[i], vl);\n        vfloat32m1_t vb = vfmul_vf_f32m1(va, 1.0f); // Initialize vb with va\n\n        vbool32_t mask = vmsltu_vx_i32m1_b32(vc, 1); // Mask for control <= 0\n\n        vfloat32m1_t vmin = vfmv_v_f_f32m1(min_val);\n        vfloat32m1_t vmax = vfmv_v_f_f32m1(max_val);\n\n        vfloat32m1_t vdiff = vfsub_vv_f32m1(vmax, vmin);\n        vfloat32m1_t vresult = vfdiv_vv_f32m1(vfsub_vv_f32m1(va, vmin), vdiff);\n        vresult = vmerge_vfm_f32m1(mask, vfmv_v_f_f32m1(0.0f), vresult); // Set values < 0 to 0\n        vresult = vmerge_vfm_f32m1(mask, va, vresult); // Copy values where control <= 0\n\n        vfloat32m1_t vone = vfmv_v_f_f32m1(1.0f);\n        vresult = vmerge_vfm_f32m1(vmsgtu_vf_f32m1(vresult, 1.0f), vone, vresult); // Set values > 1 to 1\n\n        vse32_v_f32m1(&B[i], vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    int i;\n    vfloat32m8_t va, vb, vmin_val, vmax_val, vzero, vone;\n    vint32m4_t vcontrol;\n    \n    vmin_val = vmv_v_f_f32(min_val);\n    vmax_val = vmv_v_f_f32(max_val);\n    vzero = vmv_v_f_f32(0.0f);\n    vone = vmv_v_f_f32(1.0f);\n    \n    for (i = 0; i < size; i += VL) {\n        va = vlseg8e32_v_f32m8(&A[i]);\n        vb = vfmv_v_f_f32m8(0.0f);\n        vcontrol = vle32_v_i32m4(&control[i]);\n        \n        vbool4_t mask = vmslt_vx_i32m4_b4(vcontrol, 1); // control <= 0\n        \n        vfloat32m8_t normalized = vfdiv_vv_f32m8(va - vmin_val, vmax_val - vmin_val);\n        normalized = vfredosum_vs_f32m8_f32m1(normalized, normalized, vzero);\n        normalized = vfredosum_vs_f32m8_f32m1(normalized, normalized, vzero);\n        \n        vb = vmerge_vvm_f32m8(mask, vb, normalized);\n        \n        vse8_v_f32m8(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t vin;\n    vfloat32m1_t vout;\n\n    for (size_t i = 0; i < size; i += vl) {\n        vin = vle32_v_u32m1(&A[i]);\n        vout = vfcvt_f_xu_f32m1(vin, vl);\n        vout = vfsgnj_f_f32m1(vout, vout, vl); // Clear the sign bit\n        vout = vsqrt_f32m1(vout, vl);\n        vse32_v_f32m1(&B[i], vout);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vfloat32m1_t vb = vfcvt_f_xu_f32m1(va, vl);\n        vfloat32m1_t vc = vfsgnjx_vf_f32m1(vb, vb, vl);\n        vfloat32m1_t vd = vsqrt_v_f32m1(vc, vl);\n        vse32_v_f32m1(&B[i], vd);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t va;\n    vfloat32m1_t vb;\n    \n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_u32m1(&A[i]);\n        vb = vfcvt_f_xu_f32m1(va, vl);\n        vb = vsqrt_v_f32m1(vb, vl);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t va;\n    vfloat32m1_t vb;\n    \n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_u32m1(&A[i]);\n        vb = vfcvt_f_xu_f32m1(va, vl);\n        vb = vfsgnj_f_f32m1(vb, vb, vl);\n        vb = vsqrt_f_f32m1(vb, vl);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t va;\n    vfloat32m1_t vb;\n    \n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_u32m1(&A[i]);\n        vb = vfcvt_f_xu_f32m1(va, vl);\n        vb = vfmv_v_f_f32m1(vb);\n        vb = vfsgnj_vf_f32m1(vb, vb, 0, vl);\n        vb = vsqrt_v_f32m1(vb, vl);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    for (size_t i = 0; i < size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vfloat32m1_t vb = vreinterpret_v_f32m1_u32m1(va);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t va;\n    vfloat32m1_t vb;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_u32m1(&A[i]);\n        vb = vreinterpret_v_f32m1_u32m1(va);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    for (size_t i = 0; i < size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vfloat32m1_t vb = vreinterpret_v_f32m1_u32m1(va);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vfloat32m1_t vb = vreinterpretu_v_f32m1(va);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(0);\n    for (size_t i = 0; i < size; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(&A[i]);\n        vfloat32m1_t vb = vreinterpret_v_f32m1_u32m1(va);\n        vse32_v_f32m1(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m4_t va, vprev, vdiff;\n    vfloat32m4_t vdiff_float;\n    vbool8_t vcond;\n\n    va = vle32_v_i32m4(A, size);\n    vprev = vslide1down_vx_i32m4(va, 0);\n    vcond = vle8_v_b(size, cond);\n    vdiff = vsub_vv_i32m4(va, vprev);\n    vdiff_float = vfwcvt_f_f_v_f32m4(vdiff);\n    vfloat32m4_t vzero = vfmv_v_f_f32m4(0.0f);\n    vfloat32m4_t vresult = vmerge_vvm_f32m4(vcond, vdiff_float, vzero);\n    vse32_v_f32m4(diff, vresult, size);\n}\n```  "}
{"task_id": "SimdBench_58_RVV", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m4_t va, vdiff;\n    vbool4_t vcond;\n    vfloat32m4_t vdiff_float;\n\n    va = vle32_v_i32m4(A, size);\n    vcond = vle8_v_u_i8m1(cond, size);\n\n    vint32m4_t va_shifted = vsll_vx_i32m4(va, 1);\n    vint32m4_t va_shifted_prev = vext_vv_i32m4(va_shifted, va, 1);\n\n    vint32m4_t diff_int = vsub_vv_i32m4(va_shifted, va_shifted_prev);\n    vdiff = vmerge_vvm_i32m4(vcond, vdiff, vint32m4_t());\n\n    vdiff_float = vfwcvt_f_x_v_f32m4(diff_int);\n    vse32_v_f32m4(diff, vdiff_float, size);\n}\n```  "}
{"task_id": "SimdBench_58_RVV", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m4_t va, vprev, vdiff;\n    vfloat32m4_t vdiff_float;\n    vbool8_t vcond;\n\n    va = vle32_v_i32m4(A, size);\n    vprev = vmv_v_x_i32m4(0, size); // Initialize previous element to 0\n    vcond = vle8_v_u8m1(cond, size);\n\n    for (size_t i = 0; i < size; i += VL * 4) {\n        vint32m4_t va_next = vslide1down_vx_i32m4(va, vprev, 1);\n        vbool8_t vcond_next = vslide1down_vx_u8m1(vcond, vcond, 1);\n\n        vdiff = vsub_vv_i32m4(va, va_next);\n        vdiff_float = vfwcvt_f_x_v_f32m4(vdiff, 0);\n        vfloat32m4_t vzero = vfmv_v_f_f32m4(0.0f);\n\n        vfloat32m4_t vresult = vmerge_vvm_f32m4(vzero, vdiff_float, vcond_next);\n        vse32_v_f32m4(diff, vresult);\n\n        va = va_next;\n        vprev = va_next;\n        vcond = vcond_next;\n        diff += VL;\n    }\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m4_t va, vdiff;\n    vbool4_t vcond;\n    vfloat32m4_t vdiff_float;\n\n    va = vle32_v_i32m4(A, size);\n    vcond = vle8_v_u_i8m1(cond, size);\n\n    vdiff = vsub_vv_i32m4(va, vsll_vx_i32m4(va, 1));\n    vdiff_float = vfncvt_f_w_f32m4(vdiff);\n\n    vfloat32m4_t vzero = vfmv_v_f_f32m4(0.0f);\n    vdiff_float = vmerge_vvm_f32m4(vzero, vdiff_float, vcond);\n\n    vse32_v_f32m4(diff, vdiff_float, size);\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m4_t va, vdiff;\n    vbool4_t vcond;\n    vfloat32m4_t vdiff_float;\n\n    va = vle32_v_i32m4(A, size);\n    vcond = vle8_v_u_i8m1(cond, size);\n\n    vdiff = vsub_vv_i32m4(va, vslide1down_vx_i32m4(va, 1, size));\n    vdiff_float = vfncvt_f_x_v_f32m4(vdiff);\n\n    vfloat32m4_t vzero = vfmv_v_f_f32m4(0.0f);\n    vdiff_float = vmerge_vvm_f32m4(vcond, vdiff_float, vzero);\n\n    vse32_v_f32m4(diff, vdiff_float, size);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvli(length, RVV_E64, RVV_M8);\n    vuint32m8_t v_src = vle32_v_u32m8(src, vl);\n    vuint64m8_t v_dst = vmerge_vvm_u64m8(vslide1down_vx_u64m8(v_src, 32), v_src);\n    vse64_v_u64m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    vuint32m1_t v_src;\n    vuint64m1_t v_dst;\n\n    for (size_t i = 0; i < length; i += vl) {\n        v_src = vle32_v_u32m1(&src[i]);\n        v_dst = vmerge_vvm_u64m1(vslide1down_vx_u64m1(vzext_vf2_u64m1(v_src), 32), vsrc1u_v_u64m1(1), vl);\n        vse64_v_u64m1(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n    for (; i < length / vl * vl; i += vl) {\n        vuint32m1_t v_src = vle32_v_u32m1(&src[i]);\n        vuint64m1_t v_dst = vmerge_vxm_u64m1(v_src, 0xFFFFFFFF);\n        vse64_v_u64m1(&dst[i], v_dst);\n    }\n    if (i < length) {\n        size_t remaining = length - i;\n        vuint32m1_t v_src = vle32_v_u32m1(&src[i]);\n        vuint64m1_t v_dst = vmerge_vxm_u64m1(v_src, 0xFFFFFFFF);\n        vse64_v_u64m1(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    size_t i = 0;\n    for (; i < length / (vl * 2); i++) {\n        vuint32m1_t vsrc1 = vle32_v_u32m1(&src[i * vl * 2]);\n        vuint32m1_t vsrc2 = vle32_v_u32m1(&src[i * vl * 2 + vl]);\n        vuint64m1_t vdst = vmerge_vvm_u64m1(vsrc1, vsrc2);\n        vse64_v_u64m1(&dst[i * vl], vdst);\n    }\n    size_t remaining = length % (vl * 2);\n    if (remaining > 0) {\n        vuint32m1_t vsrc1 = vle32_v_u32m1(&src[i * vl * 2]);\n        vuint32m1_t vsrc2 = vle32_v_u32m1(&src[i * vl * 2 + vl]);\n        vuint64m1_t vdst = vmerge_vvm_u64m1(vsrc1, vsrc2);\n        vse64_v_u64m1(&dst[i * vl], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(0);\n    vuint32m1_t v_src;\n    vuint64m1_t v_dst;\n\n    for (size_t i = 0; i < length; i += vl) {\n        v_src = vle32_v_u32m1(&src[i]);\n        v_dst = vwmulu_vx_u64m1(v_src, 0x0000000100000001U);\n        vse64_v_u64m1(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    vfloat64m1_t v_sum = vfmv_v_f_f64m1(0.0); // Initialize sum vector with zeros\n    for (size_t i = 0; i < length; i += VL) {\n        vint16m1_t v_index = vle16_v_i16m1(&index[i]); // Load indices vector\n        vfloat64m1_t v_vec = vle64_v_f64m1(&vec[0]); // Load elements vector\n        vfloat64m1_t v_selected = vslide1down_vx_f64m1(v_vec, v_index); // Select elements based on indices\n        v_sum = vfadd_vv_f64m1(v_sum, v_selected); // Add selected elements to sum vector\n    }\n    double sum = vfredsum_vs_f64m1_f64(v_sum, v_sum, 0.0); // Reduce sum vector to a scalar\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    vfloat64m1_t v_vec;\n    vint16m1_t v_index;\n    vfloat64m1_t v_sum = vmv_v_f_f64m1(0.0);\n\n    for (size_t i = 0; i < length; i += VL) {\n        size_t vl = (i + VL <= length) ? VL : (length - i);\n        v_index = vle16_v_i16m1(&index[i], vl);\n        v_vec = vle64_v_f64m1(&vec[0], vl);\n        vfloat64m1_t v_temp = vfwadd_vf_f64m1(v_vec, 0.0, v_index);\n        v_sum = vfadd_vv_f64m1(v_sum, v_temp);\n    }\n\n    double sum = vfredsum_vs_f64m1_f64(v_sum, v_sum, 0.0);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    const int vl = vsetvli(0, RVV_E64, RVV_M8);\n    double result = 0.0;\n    vfloat64m8_t v_sum = vfmv_v_f_f64m8(0.0);\n\n    for (size_t i = 0; i < length; i += VL) {\n        vint16m1_t v_index = vle16_v_i16m1(&index[i]);\n        vfloat64m8_t v_vec = vle64_v_f64m8(&vec[0]);\n        vfloat64m8_t v_selected = vslide1down_vx_f64m8(v_vec, v_index, vl);\n        v_sum = vfadd_vv_f64m8(v_sum, v_selected);\n    }\n\n    result = vfredsum_vs_f64m8_f64m1(result, v_sum, 0.0, vl);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    vfloat64m1_t v_src;\n    vint16m1_t v_idx;\n    vfloat64m1_t v_result = vmv_v_f_f64m1(0.0);\n\n    for (size_t i = 0; i < length; i += VL) {\n        v_idx = vle16_v_i16m1(&index[i]);\n        v_src = vle64_v_f64m1(&vec[0], v_idx);\n        v_result = vfadd_vv_f64m1(v_result, v_src);\n    }\n\n    double result;\n    vse64_v_f64m1(&result, v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    vfloat64m1_t v_sum = vfmv_v_f_f64m1(0.0); // Initialize sum vector to 0.0\n    for (size_t i = 0; i < length; i += VL) {\n        vint16m1_t v_index = vle16_v_i16m1(&index[i]); // Load indices vector\n        vfloat64m1_t v_vec = vle64_v_f64m1(&vec[0]); // Load elements vector\n        vfloat64m1_t v_selected = vslide1down_vx_f64m1(v_vec, v_index); // Select elements based on indices\n        v_sum = vfadd_vv_f64m1(v_sum, v_selected); // Add selected elements to sum vector\n    }\n    double sum = vfmv_f_s_f64_f64m1_f64(v_sum); // Move sum vector to scalar\n    return sum;\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t output_elements = output_size * output_size;\n    \n    for (size_t i = 0; i < output_elements; i += VL) {\n        vfloat64m8_t acc = vfmv_v_f_f64m8(0.0); // Initialize accumulator to 0.0\n        size_t input_idx = i / output_size * input_size + i % output_size;\n        \n        for (size_t j = 0; j < kernel_size; j++) {\n            size_t kernel_idx = j * kernel_size;\n            vfloat64m8_t input_vec = vle64_v_f64m8(&input[input_idx + j * input_size], VL);\n            vfloat64m8_t kernel_vec = vle64_v_f64m8(&kernel[kernel_idx], VL);\n            acc = vfma_vv_f64m8(acc, input_vec, kernel_vec);\n        }\n        \n        vfloat64m8_t zero = vfmv_v_f_f64m8(0.0);\n        vfloat64m8_t result = vmax_vv_f64m8(acc, zero); // ReLU activation\n        vse64_v_f64m8(&output[i], result, VL);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t output_elements = output_size * output_size;\n\n    for (size_t i = 0; i < output_elements; i += VL) {\n        vfloat64m8_t acc = vmv_v_f_f64m8(0.0);\n        size_t input_idx = i / output_size * input_size + i % output_size;\n        for (size_t j = 0; j < kernel_size; j++) {\n            vfloat64m8_t input_vec = vle64_v_f64m8(&input[input_idx + j * input_size], VL);\n            vfloat64m8_t kernel_vec = vle64_v_f64m8(&kernel[j * kernel_size], VL);\n            acc = vfwmacc_vf_f64m8(acc, input_vec, kernel_vec, VL);\n        }\n        vfloat64m8_t zero = vmv_v_f_f64m8(0.0);\n        acc = vmax_vf_f64m8(acc, zero, VL);\n        vse64_v_f64m8(&output[i], acc, VL);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    int output_size = input_size - kernel_size + 1;\n    int output_elements = output_size * output_size;\n\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vfloat64m8_t v_accumulator = vfmv_v_f_f64m8(0.0, VLMAX);\n\n    for (int i = 0; i < output_elements; i += VLMAX) {\n        v_output = vle64_v_f64m8(&output[i], VLMAX);\n\n        for (int j = 0; j < kernel_size; j++) {\n            for (int k = 0; k < kernel_size; k++) {\n                int input_index = i + j * output_size + k;\n                v_input = vle64_v_f64m8(&input[input_index], VLMAX);\n                v_kernel = vle64_v_f64m8(&kernel[j * kernel_size + k], VLMAX);\n\n                v_accumulator = vfwmacc_vv_f64m8(v_accumulator, v_input, v_kernel, VLMAX);\n            }\n        }\n\n        v_output = vfredsum_vs_f64m8_f64m8(v_accumulator, vfmv_s_f_f64m8(0.0, VLMAX), v_output, VLMAX);\n        vse64_v_f64m8(&output[i], v_output, VLMAX);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    int output_size = input_size - kernel_size + 1;\n    int output_elements = output_size * output_size;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vfloat64m8_t v_accumulator = vfmv_v_f_f64m8(0.0, VLMAX);\n    \n    for (int i = 0; i < output_elements; i += VLMAX) {\n        v_output = vle64_v_f64m8(&output[i], VLMAX);\n        v_accumulator = vfmv_v_f_f64m8(0.0, VLMAX);\n        \n        for (int j = 0; j < kernel_size; j++) {\n            for (int k = 0; k < kernel_size; k++) {\n                int input_index = i + j * output_size + k;\n                v_input = vle64_v_f64m8(&input[input_index], VLMAX);\n                v_kernel = vle64_v_f64m8(&kernel[j * kernel_size + k], VLMAX);\n                v_accumulator = vfwmacc_vv_f64m8(v_accumulator, v_input, v_kernel, VLMAX);\n            }\n        }\n        \n        vfloat64m8_t zero_mask = vmsne_vx_i64m8_b8(v_accumulator, 0, VLMAX);\n        v_accumulator = vfredsum_vs_f64m8_f64m8(v_accumulator, v_accumulator, v_accumulator, zero_mask, VLMAX);\n        vse64_v_f64m8(&output[i], v_accumulator, VLMAX);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    int output_size = input_size - kernel_size + 1;\n    int output_elements = output_size * output_size;\n    int input_elements = input_size * input_size;\n    int kernel_elements = kernel_size * kernel_size;\n\n    for (int i = 0; i < output_elements; i += VL) {\n        vfloat64m8_t acc = vfmv_v_f_f64m8(0.0);\n        for (int j = 0; j < kernel_elements; j++) {\n            int input_idx = i + j / kernel_size * input_size + j % kernel_size;\n            vfloat64m8_t input_vec = vle64_v_f64m8(&input[input_idx]);\n            vfloat64m8_t kernel_vec = vfmv_v_f_f64m8(kernel[j]);\n            acc = vfwmacc_vf_f64m8(acc, input_vec, kernel_vec);\n        }\n        vfloat64m8_t zero = vfmv_v_f_f64m8(0.0);\n        acc = vmax_vf_f64m8(acc, zero);\n        vse64_v_f64m8(&output[i], acc);\n    }\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vector_size = vlenb_vf * 2;\n    const int num_vectors = numbers.size() / vector_size;\n    \n    vfloat64m2_t v_threshold = vfmv_v_f_f64m2(threshold);\n    bool result = false;\n\n    for (int i = 0; i < num_vectors; i++) {\n        vfloat64m2_t va = vle64_v_f64m2(&numbers[i * vector_size]);\n        vfloat64m2_t vb = vle64_v_f64m2(&numbers[i * vector_size + vector_size]);\n\n        vfloat64m2_t diff = vfsub_vv_f64m2(va, vb);\n        vbool64_t cmp = vmslt_vx_f64m2_b(diff, v_threshold, 0);\n\n        if (vmpopc_m_b64(cmp) > 0) {\n            result = true;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int size = numbers.size();\n    const int vlen = vsetvli(0, RVV_EEW, RVV_M1);\n    \n    double *data = numbers.data();\n    vfloat64m1_t v_data = vle64_v_f64m1(data);\n    vfloat64m1_t v_threshold = vmv_v_f_f64m1(threshold);\n    \n    for (int i = 0; i < size; i += vlen) {\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_data, v_data, v_data);\n        vbool64_t v_mask = vmslt_vx_f64m1_b64(v_diff, v_threshold, 0);\n        \n        if (vmfirst_m_b64(v_mask) < vlen) {\n            return true;\n        }\n        \n        v_data = vslide1down_vx_f64m1(v_data, 1);\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vlen = vsetvli(0, RVV_EEW, RVV_M1);\n    const int num_lanes = vsetvli(0, RVV_EEW, RVV_M8);\n\n    int num_elements = numbers.size();\n    int num_chunks = num_elements / num_lanes;\n\n    double* data_ptr = numbers.data();\n    vfloat64m1_t v_threshold = vfmv_v_f_f64m1(threshold);\n\n    for (int i = 0; i < num_chunks; i++) {\n        vfloat64m1_t va = vlseg2e64_v_f64m1(data_ptr + i * num_lanes * 2, num_lanes);\n        vfloat64m1_t vb = vlseg2e64_v_f64m1(data_ptr + i * num_lanes * 2 + num_lanes, num_lanes);\n        vfloat64m1_t diff = vfsub_vv_f64m1(va, vb);\n        vbool64_t mask = vmslt_vx_f64m1_b64(diff, v_threshold, 0);\n        if (vmfirst_m_b64(mask) < num_lanes) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elements = numbers.size();\n    const int num_vectors = (num_elements + vector_size - 1) / vector_size;\n\n    double *input_ptr = const_cast<double*>(numbers.data());\n    vfloat64m1_t v_threshold = vfmv_v_f_f64m1(threshold);\n    vbool64_t result = vfalse_b64;\n\n    for (int i = 0; i < num_vectors; i++) {\n        int elements_in_vector = (i == num_vectors - 1) ? num_elements - i * vector_size : vector_size;\n        vfloat64m1_t v_numbers = vle64_v_f64m1(input_ptr + i * vector_size, elements_in_vector);\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_numbers, v_numbers);\n        vbool64_t v_close = vmslt_vx_f64m1_b64(v_diff, v_threshold, 0);\n        result = vor_mm_b64(result, v_close);\n    }\n\n    return vmfirst_m_b64(result);\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vector_size = vlenb_vf / sizeof(double);\n    const int num_vectors = numbers.size() / vector_size;\n    \n    vfloat64m1_t v_threshold = vmv_v_f_f64m1(threshold);\n    vbool64_t result = vfalse_v_b64m1();\n    \n    for (int i = 0; i < num_vectors; i++) {\n        vfloat64m1_t va = vle64_v_f64m1(&numbers[i * vector_size]);\n        \n        for (int j = i + 1; j < num_vectors; j++) {\n            vfloat64m1_t vb = vle64_v_f64m1(&numbers[j * vector_size]);\n            vfloat64m1_t diff = vsub_vv_f64m1(va, vb);\n            vbool64_t close_elements = vmsltu_vx_i64m1_b64(diff, v_threshold, 1);\n            result = vor_vb_b64(result, close_elements);\n        }\n    }\n    \n    return vmfirst_m_b64(result);\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    vint32m1_t balance = vmv_v_x_i32m1(0); // Initialize balance to zero\n    vbool32_t result = vmslt_vx_i32m1_b32(balance, 0); // Check if balance is less than zero\n\n    for (size_t i = 0; i < operations.size(); i += VL) {\n        vint32m1_t op = vle32_v_i32m1(&operations[i]); // Load vector of operations\n        balance = vadd_vv_i32m1(balance, op); // Update balance with operations\n        vbool32_t below_zero = vmslt_vx_i32m1_b32(balance, 0); // Check if balance is less than zero after operations\n        result = vor_vb_vb_b32(result, below_zero); // Update result vector\n    }\n\n    bool final_result = vmfirst_m_b32(result); // Find first true value in result vector\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    vint32m1_t balance = vmv_v_x_i32m1(0); // Initialize balance to zero\n    vbool32_t result = vmslt_vx_i32m1_b32(balance, 0); // Check if balance is less than zero\n\n    for (size_t i = 0; i < operations.size(); i += VL) {\n        vint32m1_t op = vle32_v_i32m1(&operations[i]); // Load vector of operations\n        balance = vadd_vv_i32m1(balance, op); // Update balance\n        result = vor_vv_b32(result, vmslt_vx_i32m1_b32(balance, 0)); // Update result\n    }\n\n    return vmfirst_m_b32(result); // Return true if any element in result is true\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    vint32m1_t balance = VMV_VI_I(0, 1);\n    vbool32_t result = VMVV_VI_I(0, 0);\n\n    for (size_t i = 0; i < operations.size(); i += VL) {\n        vint32m1_t op = VLE32_V_I(&operations[i], vl1);\n        balance = VADD_VV_I(balance, op);\n\n        vbool32_t below_zero = VSLT_VX_I(balance, 0, vl1);\n        result = VOR_VV_B(result, below_zero, vl1);\n\n        if (VMFIRSTM(result))\n            return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    vint32m1_t balance = vmv_v_x_i32m1(0); // Initialize balance to zero\n    vbool32_t below_zero = vmslt_vx_i32m1_b32(balance, 0); // Check if balance is below zero\n\n    for (size_t i = 0; i < operations.size(); i += VL) {\n        vint32m1_t op = vle32_v_i32m1(&operations[i]); // Load vector of operations\n        balance = vadd_vv_i32m1(balance, op); // Update balance with operations\n        vbool32_t below_zero_op = vmslt_vx_i32m1_b32(balance, 0); // Check if balance is below zero after operations\n        below_zero = vor_vb_vb_vb(below_zero, below_zero_op); // Update below_zero flag\n    }\n\n    return vmfirst_m_b32(below_zero); // Return true if any element in below_zero is true\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_zero_simd(std::vector<int> operations){\n    const int vlen = vsetvlmax();\n    int vl = vsetvl(vlen);\n\n    vint32m1_t balance = vmv_s_x_i32m1(0, vl);\n\n    for (size_t i = 0; i < operations.size(); i += vl) {\n        vint32m1_t op = vle32_v_i32m1(&operations[i], vl);\n\n        balance = vadd_vv_i32m1(balance, op, vl);\n\n        vbool32_t mask = vmslt_vx_i32m1_b32(balance, 0, vl);\n        if (vmfirst(mask) < vl) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int size = numbers.size();\n    const int lane_size = vsetvl_e64m1();\n    const int full_lanes = size / lane_size;\n    \n    double sum = 0.0;\n    double mean = 0.0;\n    \n    // Calculate mean\n    for (int i = 0; i < full_lanes * lane_size; i += lane_size) {\n        vfloat64m1_t vec_numbers = vle64_v_f64m1(&numbers[i]);\n        sum += vfadd_vf_f64m1(vfredsum_vs_f64m1_f64m1(vfmv_v_f_f64m1(0.0), vec_numbers), 0.0);\n    }\n    mean = sum / size;\n    \n    // Calculate MAD\n    sum = 0.0;\n    for (int i = 0; i < full_lanes * lane_size; i += lane_size) {\n        vfloat64m1_t vec_numbers = vle64_v_f64m1(&numbers[i]);\n        vfloat64m1_t diff = vfnmsub_vf_f64m1(vec_numbers, mean, vec_numbers);\n        vfloat64m1_t abs_diff = vfsgnj_vf_f64m1(diff, diff, 1);\n        sum += vfadd_vf_f64m1(vfredsum_vs_f64m1_f64m1(vfmv_v_f_f64m1(0.0), abs_diff), 0.0);\n    }\n    \n    return sum / size;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    int num_elements = numbers.size();\n    int num_vectors = num_elements / vector_size;\n    \n    double sum = 0.0;\n    double mean = 0.0;\n    \n    // Calculate the mean of the dataset\n    for (int i = 0; i < num_vectors; i++) {\n        vfloat64m1_t v_numbers = vle64_v_f64m1(&numbers[i * vector_size]);\n        sum += vfadd_vf_f64m1(vfadd_vf_f64m1(vfmv_v_f_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f64m1(vfadd_vf_f"}
{"task_id": "SimdBench_64_RVV", "completion": "```\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int size = numbers.size();\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    double sum = 0.0;\n    double mean = 0.0;\n\n    // Calculate the mean of the dataset\n    for (int i = 0; i < size; i += vector_size) {\n        vfloat64m1_t v_numbers = vle64_v_f64m1(&numbers[i]);\n        vfloat64m1_t v_sum = vfredsum_vs_f64m1_f64m1(vfmv_f_s_f64m1(vfsub_vv_f64m1(v_numbers, vfmv_v_f_f64m1(mean, v_numbers)), v_numbers), vfmv_s_f_f64m1(sum, v_numbers), v_numbers);\n        sum = vfmv_f_s_f64m1(v_sum, v_numbers);\n    }\n    mean = sum / size;\n\n    // Calculate the mean absolute deviation\n    sum = 0.0;\n    for (int i = 0; i < size; i += vector_size) {\n        vfloat64m1_t v_numbers = vle64_v_f64m1(&numbers[i]);\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_numbers, vfmv_v_f_f64m1(mean, v_numbers));\n        vfloat64m1_t v_abs_diff = vfabs_v_f64m1(v_diff);\n        vfloat64m1_t v_sum = vfredsum_vs_f64m1_f64m1(v_abs_diff, vfmv_s_f_f64m1(sum, v_abs_diff), v_abs_diff);\n        sum = vfmv_f_s_f64m1(v_sum, v_abs_diff);\n    }\n\n    return sum / size;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int size = numbers.size();\n    const int vlen = vsetvli(0, RVV_EEW, RVV_M);\n    \n    double sum = 0.0;\n    double mean = 0.0;\n    \n    // Calculate the mean\n    for (int i = 0; i < size; i += vlen) {\n        vfloat64m1_t vn = vle64_v_f64m1(&numbers[i]);\n        sum += vfredsum_vs_f64m1_f64m1(vfadd_vv_f64m1(vfmv_v_f_f64m1(0.0), vn), 0.0, vn);\n    }\n    mean = sum / size;\n    \n    sum = 0.0;\n    \n    // Calculate the mean absolute deviation\n    for (int i = 0; i < size; i += vlen) {\n        vfloat64m1_t vn = vle64_v_f64m1(&numbers[i]);\n        vfloat64m1_t diff = vfsub_vv_f64m1(vn, vfmv_v_f_f64m1(mean));\n        vfloat64m1_t abs_diff = vfsgnjn_vv_f64m1(diff, diff);\n        sum += vfredsum_vs_f64m1_f64m1(vfadd_vv_f64m1(vfmv_v_f_f64m1(0.0), abs_diff), 0.0, abs_diff);\n    }\n    \n    return sum / size;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int size = numbers.size();\n    const int vlen = vsetvli(0, RVV_EEW, RVV_M);\n    double sum = 0.0;\n    double mean = 0.0;\n    \n    vfloat64m1_t v_input = vle64_v_f64m1(&numbers[0], vlen);\n    \n    // Calculate the mean\n    vfloat64m1_t v_sum = vfredsum_vs_f64m1_f64m1(vfmv_f_s_f64m1(0.0), v_input, vfmv_s_f_f64m1(0.0), vlen);\n    vfloat64m1_t v_mean = vfdiv_vf_f64m1(v_sum, vset_f64m1(size));\n    vse64_v_f64m1(&mean, v_mean, vlen);\n    \n    // Calculate the absolute differences\n    vfloat64m1_t v_diff = vfsub_vv_f64m1(v_input, v_mean, vlen);\n    vfloat64m1_t v_abs_diff = vfsgnjx_vv_f64m1(vdiff, vdiff, vlen);\n    \n    // Calculate the sum of absolute differences\n    vfloat64m1_t v_sum_abs_diff = vfredsum_vs_f64m1_f64m1(vfmv_f_s_f64m1(0.0), v_abs_diff, vfmv_s_f_f64m1(0.0), vlen);\n    vse64_v_f64m1(&sum, v_sum_abs_diff, vlen);\n    \n    return sum / size;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int vlen = vsetvl_e32m1();\n    int size = numbers.size();\n    int output_size = size * 2 - 1;\n    std::vector<int> result(output_size);\n\n    vint32m1_t v_numbers = VLOAD(&numbers[0], vlen);\n    vint32m1_t v_delimeter = VMV(splats(delimeter), vlen);\n\n    int i = 0;\n    for (; i < size - 1; i++) {\n        result[i * 2] = VGET(v_numbers, i);\n        result[i * 2 + 1] = delimeter;\n    }\n    result[i * 2] = VGET(v_numbers, i);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int vlen = vsetvl_e32m1();\n    int size = numbers.size();\n    int output_size = size * 2 - 1;\n    std::vector<int> result(output_size);\n\n    vint32m1_t v_input = VLOAD(&numbers[0], vlen);\n    vint32m1_t v_delimeter = VMV_VI(delimeter, vlen);\n\n    for (int i = 0; i < size - 1; i++) {\n        int index = i * 2;\n        result[index] = VGET(v_input, i);\n        result[index + 1] = delimeter;\n    }\n    result[output_size - 1] = VGET(v_input, size - 1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int vlen = vsetvl_e32m1();\n    int size = numbers.size();\n    int output_size = size + (size - 1);\n    std::vector<int> result(output_size);\n\n    vint32m1_t v_input = {0};\n    vint32m1_t v_delimeter = vsplati_e32m1(delimeter);\n    vint32m1_t v_output = {0};\n\n    for (int i = 0; i < size; i += vlen) {\n        v_input = vle32_v_i32m1(&numbers[i]);\n        v_output = vslide1down_vx_i32m1(v_input, 1);\n        vse32_v_i32m1(&result[i * 2], v_input);\n        vse32_v_i32m1(&result[i * 2 + 1], v_delimeter);\n    }\n\n    return result;\n}\n```  "}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int vlen = vsetvl_e32m1();\n    int num_elements = numbers.size();\n    int result_size = num_elements + (num_elements - 1);\n    std::vector<int> result(result_size);\n\n    vint32m1_t v_numbers = {0};\n    vint32m1_t v_delimeter = vsplati_e32m1(delimeter);\n    vint32m1_t v_result = {0};\n\n    for (int i = 0; i < num_elements; i += vlen) {\n        v_numbers = vlseg2ei32_v_i32m1(&numbers[i], vlen);\n        v_result = vslide1down_vx_i32m1(v_numbers, 0);\n        vse32_v_i32m1(&result[i * 2], v_result, vlen);\n        if (i + vlen < num_elements) {\n            vse32_v_i32m1(&result[i * 2 + vlen], v_delimeter, vlen);\n        }\n    }\n\n    return result;\n}\n```  "}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int size = numbers.size();\n    const int new_size = size * 2 - 1;\n    std::vector<int> result(new_size);\n\n    vint32m4_t vec_numbers = vle32_v_i32m4(numbers.data());\n    vint32m4_t vec_result = vcreate_i32m4(0);\n\n    vint32m4_t vec_delimeter = vmv_v_x_i32m4(delimeter);\n\n    for (int i = 0; i < size; i++) {\n        vse32_v_i32m4(&result[i * 2], vec_numbers);\n        vec_result = vmerge_vvm_i32m4(vec_result, vec_delimeter, vec_numbers);\n    }\n\n    vse32_v_i32m4(&result[size * 2 - 1], vec_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int size = numbers.size();\n    const int lane_size = vsetvl_e16m1_b1();\n    const int full_lanes = size / lane_size;\n    \n    vint16m1_t v_numbers = {0};\n    vint32m2_t v_sum = {0};\n    vint32m2_t v_product = {1};\n\n    for (int i = 0; i < full_lanes * lane_size; i += lane_size) {\n        v_numbers = vlse16_v_i16m1(&numbers[i], lane_size);\n        v_sum = vwadd_vv_i32m2(v_sum, vnsrl_wx_i32m2(vwadd_vx_i32m2(v_sum, vwmulu_vx_i32m2(v_product, vle16_v_i16m1(&numbers[i], lane_size)), 16), 16));\n        v_product = vwmulu_vv_i32m2(v_product, vle16_v_i16m1(&numbers[i], lane_size));\n    }\n\n    int64_t sum = vmv_x_s_i64(vredsum_vs_i32m2_i64(v_sum, 0));\n    int64_t product = vmv_x_s_i64(vredsum_vs_i32m2_i64(v_product, 0));\n\n    for (int i = full_lanes * lane_size; i < size; i++) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    vint16m4_t vec = VLE16_v_i16m4(&numbers[0]);\n\n    vint32m8_t sum = VMV_VX_i32m8(0);\n    vint32m8_t product = VMV_VX_i32m8(1);\n\n    vint32m8_t sum_acc = VMV_VX_i32m8(0);\n    vint32m8_t product_acc = VMV_VX_i32m8(1);\n\n    for (size_t i = 0; i < numbers.size(); i += VLSEWMAX) {\n        vint16m4_t sub_vec = VLE16_v_i16m4(&numbers[i]);\n\n        vint32m8_t mul_res = VMUL_vv_i32m8(sub_vec, product_acc);\n        product = VMUL_vv_i32m8(product, mul_res);\n\n        vint32m8_t add_res = VADD_vv_i32m8(sub_vec, sum_acc);\n        sum = VADD_vv_i32m8(sum, add_res);\n\n        product_acc = VMV_x_s_i32m8(product);\n        sum_acc = VMV_x_s_i32m8(sum);\n    }\n\n    std::vector<int64_t> result = {VSSRA_vx_i32m8_i64(sum, 0), VSSRA_vx_i32m8_i64(product, 0)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    vint16m4_t vec = VVSETVL(0, RVV_EEW_16);\n    vint16m4_t input = VLOAD(&numbers[0], vec);\n\n    vint32m8_t sum = VMV_VI(0, vec);\n    vint32m8_t product = VMV_VI(1, vec);\n\n    for (size_t i = 0; i < numbers.size(); i += VLSEW(vec)) {\n        vec = VSETVL(numbers.size() - i, vec);\n        vint16m4_t chunk = VLE16_VI(&numbers[i], vec);\n\n        sum = VADD_VV(sum, VWMUL_VV(chunk, vec, vec), vec);\n        product = VMUL_VV(product, chunk, vec);\n    }\n\n    int64_t sum_result = VREDUCE_ADD_EPI32(sum, vec);\n    int64_t product_result = VREDUCE_MUL_EPI32(product, vec);\n\n    return {sum_result, product_result};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vl = vsetvl_e16m1(0);\n\n    vint16m1_t v_sum = vmv_v_x_i16m1(0, vl);\n    vint16m1_t v_prod = vmv_v_x_i16m1(1, vl);\n\n    for (size_t i = 0; i < numbers.size(); i += vl) {\n        vint16m1_t v_input = vle16_v_i16m1(&numbers[i], vl);\n\n        v_sum = vadd_vv_i16m1(v_sum, v_input, vl);\n        v_prod = vmul_vv_i16m1(v_prod, v_input, vl);\n    }\n\n    int64_t sum_result = vredsum_vs_i16m1_i64(v_sum, vmv_x_s_i64(0, vl), vl);\n    int64_t prod_result = vredsum_vs_i16m1_i64(v_prod, vmv_x_s_i64(0, vl), vl);\n\n    return {sum_result, prod_result};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    vint16m4_t vec = VMS_4SE16_v_i16m4(numbers.data(), numbers.size());\n    \n    vint32m8_t sum = VMV_V_XI_v_i32m8(0);\n    vint32m8_t product = VMV_V_XI_v_i32m8(1);\n\n    vint32m8_t sum_acc = VMV_V_XI_v_i32m8(0);\n    vint32m8_t product_acc = VMV_V_XI_v_i32m8(1);\n\n    vint32m8_t sum_tmp, product_tmp;\n\n    for (size_t i = 0; i < numbers.size(); i += VL * 8) {\n        sum_tmp = VREDUCE_ADD_v_i32m8_m(mask, sum, vec);\n        product_tmp = VREDUCE_MUL_v_i32m8_m(mask, product, vec);\n\n        sum_acc = VADD_vv_i32m8(sum_acc, sum_tmp);\n        product_acc = VMUL_vv_i32m8(product_acc, product_tmp);\n    }\n\n    int64_t sum_result = VMV_X_S_i64(sum_acc);\n    int64_t product_result = VMV_X_S_i64(product_acc);\n\n    return {sum_result, product_result};\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int size = numbers.size();\n    std::vector<int> result(size);\n    vint32m1_t v_numbers = vle32_v_i32m1(numbers.data());\n    vint32m1_t v_max = vsub_vv_i32m1(v_numbers, v_numbers);\n    \n    for (int i = 0; i < size; i++) {\n        vint32m1_t v_current = vle32_v_i32m1(numbers.data() + i);\n        v_max = vmax_vv_i32m1(v_max, v_current);\n        vse32_v_i32m1(result.data() + i, v_max);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int size = numbers.size();\n    std::vector<int> result(size);\n    vint32m1_t v_numbers = vle32_v_i32m1(numbers.data());\n    vint32m1_t v_max = vslide1down_vx_i32m1(v_numbers, 1);\n    vint32m1_t v_rolling_max = vmax_vv_i32m1(v_numbers, v_max);\n    vse32_v_i32m1(result.data(), v_rolling_max);\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int vlen = vsetvl_e32m1();\n    int size = numbers.size();\n    std::vector<int> result(size);\n    \n    vint32m1_t v_numbers = vle32_v_i32m1(&numbers[0], vlen);\n    vint32m1_t v_max = vdup_v_i32m1(numbers[0], vlen);\n    \n    for(int i = 0; i < size; i += vlen){\n        vint32m1_t v_curr = vle32_v_i32m1(&numbers[i], vlen);\n        v_max = vmax_vv_i32m1(v_max, v_curr, vlen);\n        vse32_v_i32m1(&result[i], v_max, vlen);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int vlen = vsetvl_e32m1();\n    const int size = numbers.size();\n    std::vector<int> result(size);\n\n    vint32m1_t v_numbers = {0};\n    vint32m1_t v_max = {0};\n\n    for (int i = 0; i < size; i += vlen) {\n        v_numbers = vle32_v_i32m1(&numbers[i]);\n        v_max = vmax_vv_i32m1(v_max, v_numbers);\n        vse32_v_i32m1(&result[i], v_max);\n    }\n\n    return result;\n}\n```  "}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int size = numbers.size();\n    std::vector<int> result(size);\n    vint32m1_t v_numbers = vle32_v_i32m1(numbers.data(), size);\n    vint32m1_t v_max = vslide1down_vx_i32m1(v_numbers, 1);\n    vint32m1_t v_rolling_max = vmax_vv_i32m1(v_max, v_numbers);\n    vse32_v_i32m1(result.data(), v_rolling_max, size);\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = a.size();\n    std::string result(len, '0');\n    vint8_t va, vb, vresult;\n    \n    for (int i = 0; i < len; i += 64) {\n        va = vrgather_vx_i8(&a[i], 0);\n        vb = vrgather_vx_i8(&b[i], 0);\n        vresult = vxor_vv_i8(va, vb);\n        vscatter_vx_i8(&result[i], vresult, 0);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = a.size();\n    std::string result(len, '0');\n    vint8_t va, vb, vr;\n    \n    for (int i = 0; i < len; i += 64) {\n        va = vrgather_vx_i8(&a[i], vzero_v(), 1);\n        vb = vrgather_vx_i8(&b[i], vzero_v(), 1);\n        vr = vxor(va, vb);\n        vscatter_vx_i8(&result[i], vr, vzero_v(), 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    const int N = a.size();\n    vint8_t va, vb, result;\n    std::string res(N, '0');\n\n    for (int i = 0; i < N; i += 64) {\n        va = vrgather_vx_i8(&a[i], vslide1up_vx_i8(vzero_8(), 1));\n        vb = vrgather_vx_i8(&b[i], vslide1up_vx_i8(vzero_8(), 1));\n        result = vxor_vv_i8(va, vb);\n        vscatter_vx_i8(&res[i], result, vslide1up_vx_i8(vzero_8(), 1));\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    const int N = a.size();\n    vint8_t result = vmv_v_x_i8(0);\n    for (int i = 0; i < N; i += 64) {\n        vint8_t va = vrgather_vx_i8(&a[i], vcreate_i64(0));\n        vint8_t vb = vrgather_vx_i8(&b[i], vcreate_i64(0));\n        vint8_t res = vxor_vv_i8(va, vb);\n        vscatter_vx_i8(&result[i], vcreate_i64(0), res);\n    }\n    std::string res_str(N, '0');\n    vse8_v_i8m1(&res_str[0], result);\n    return res_str;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    const int N = a.size();\n    vint8_t va, vb, result;\n    std::string res(N, '0');\n\n    for (int i = 0; i < N; i += 64) {\n        va = vrgather_vx_u8(&a[i], va, 0);\n        vb = vrgather_vx_u8(&b[i], vb, 0);\n        result = va ^ vb;\n        vscatter_vx_u8(&res[i], result, 0);\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"None\";\n\n    const int N = strings.size();\n    const int VL = vsetvl_e32m1(0);\n\n    int max_length = 0;\n    int max_index = 0;\n\n    for(int i = 0; i < N; i += VL){\n        vint32m1_t lengths = vmv_v_x_i32m1(0, 0);\n        vbool32_t mask = vmv_v_x_u32(0);\n\n        for(int j = 0; j < VL && i + j < N; j++){\n            int len = strings[i + j].length();\n            lengths = vsadd_vx_i32m1(lengths, len, mask);\n            mask = vmseq_vx_i32m1_b32(mask, 0, 1);\n        }\n\n        vint32m1_t max_lengths = vmax_vv_i32m1(lengths, lengths, mask);\n        vbool32_t cmp = vmsltu_vx_i32m1_b32(lengths, max_lengths, 1);\n        max_index = vmfirst_m_b32(cmp, 0);\n\n        if(i == 0 || strings[max_index].length() > max_length){\n            max_length = strings[max_index].length();\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"None\";\n\n    size_t max_length = 0;\n    std::string longest_string = strings[0];\n\n    vlenb_e32m1_t vlen = VLENB_E32M1;\n    size_t num_strings = strings.size();\n    size_t num_lanes = vsetvli(num_strings, vlen);\n\n    vuint32m1_t max_lengths = vmv_x_u32m1(0);\n    vbool32_t mask = vmsne_vx_u32m1_b32(num_strings, 0);\n\n    for(size_t i = 0; i < strings.size(); i += num_lanes){\n        vuint32m1_t lengths = vmv_x_u32m1(0);\n        for(size_t j = 0; j < num_lanes; j++){\n            if(i + j < strings.size()){\n                lengths = vsaddu_vx_u32m1(lengths, strings[i + j].size());\n            }\n        }\n\n        mask = vmand_mm_b32(mask, vmsleu_vv_u32m1_b32(lengths, max_lengths));\n        max_lengths = vmerge_vxm_u32m1(max_lengths, lengths, mask);\n\n        for(size_t j = 0; j < num_lanes; j++){\n            if(i + j < strings.size() && vmsne_vx_u32m1_b32(lengths, 0)){\n                mask = vmsleu_vv_u32m1_b32(lengths, max_lengths);\n                longest_string = vmerge_vxm_u32m1(longest_string, strings[i + j], mask);\n            }\n        }\n    }\n\n    return longest_string;\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"None\";\n\n    const int N = strings.size();\n    const int M = 32; // Maximum length of a string\n\n    vint8_t max_lengths = vmv_v_i8(0);\n    vint8_t max_indices = vmv_v_i8(0);\n\n    for(int i = 0; i < N; i += M){\n        vint8_t lengths = vmv_v_i8(0);\n        vint8_t indices = vmv_v_i8(0);\n\n        for(int j = 0; j < M; j++){\n            if(i + j < N){\n                int len = strings[i + j].length();\n                lengths = vsseg_v_i8_v_i32(lengths, len, j);\n                indices = vsseg_v_i8_v_i32(indices, i + j, j);\n            }\n        }\n\n        vbool8_t mask = vmsltu_vx_i8_vx_i8_v_i8(lengths, max_lengths);\n        max_lengths = vmerge_vvm_i8(mask, lengths, max_lengths);\n        max_indices = vmerge_vvm_i8(mask, indices, max_indices);\n    }\n\n    int max_index = vmfirst_m_i8(vmadc_vx_i8(max_lengths, 0, 0));\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) {\n        return \"None\";\n    }\n\n    const int N = strings.size();\n    const int M = 64; // Maximum length of a string\n\n    // Initialize variables to store the longest string and its length\n    std::string longest = strings[0];\n    int longest_length = strings[0].length();\n\n    // Vector to store the lengths of strings\n    vint64m1_t lengths = vmv_s_x_i64m1(0, 0);\n\n    // Iterate over the strings in parallel\n    for(int i = 0; i < N; i += 8) {\n        vint64m1_t indices = vmv_s_x_i64m1(0, i);\n        vbool8_t mask = vmsne_vx_i8m1_b8(indices, 0, 0);\n\n        // Load strings into vectors\n        vuint8m1_t str[M];\n        for(int j = 0; j < 8; j++) {\n            if(i + j < N) {\n                const char* s = strings[i + j].c_str();\n                for(int k = 0; k < M; k++) {\n                    str[k] = vmerge_vvm_u8m1(mask, str[k], vle8_v_u8m1(s + k, mask));\n                }\n            }\n        }\n\n        // Calculate lengths of strings\n        vint64m1_t len = vmsb_vv_i64m1(str[0], 0, mask);\n        for(int k = 1; k < M; k++) {\n            len = vmerge_vvm_i64m1(mask, len, vmsb_vv_i64m1(str[k], k, mask));\n        }\n\n        // Update longest string and its length\n        vbool64_t cmp = vmsltu_vv_u64m1_b64(len, lengths);\n        lengths = vmerge_vvm_i64m1(cmp, lengths, len);\n        for(int j = 0; j < 8; j++) {\n            if(i + j < N) {\n                if(vmfirst_m_b64(cmp) == j) {\n                    longest = strings[i + j];\n                    longest_length = vslide1down_vx_i64m1_i64(lengths, j, 0);\n                }\n            }\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"None\";\n\n    const int N = strings.size();\n    const int VL = vsetvl_e32m1(0);\n\n    int max_length = 0;\n    int max_index = 0;\n\n    for(int i = 0; i < N; i += VL){\n        vint32m1_t lengths = vmv_v_x_i32m1(0, 0);\n        vbool32_t mask = vmv_v_x_u32(0);\n\n        for(int j = 0; j < VL && i+j < N; j++){\n            int len = strings[i+j].length();\n            lengths = vsadd_vx_i32m1_i32m1(lengths, len, mask);\n            mask = vmseq_vx_i32m1_b32(lengths, len, mask);\n        }\n\n        vint32m1_t max_lengths = vmax_vv_i32m1(lengths, max_length, mask);\n        vbool32_t max_mask = vmseq_vv_i32m1_b32(max_lengths, lengths, mask);\n        max_length = vslide1down_vx_i32(max_length, max_lengths, max_mask);\n        max_index = vmerge_vvm_i32(max_index, i, max_mask);\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    vuint64m1_t va = vmv_v_x_u64m1(a);\n    vuint64m1_t vb = vmv_v_x_u64m1(b);\n    \n    while(va != vb){\n        vbool64_t cmp = vmsne_vv_u64m1_b64(va, vb);\n        va = vmerge_vvm_u64m1(cmp, vb, va);\n        vb = vmerge_vvm_u64m1(cmp, va, vb);\n        vbool64_t cmp2 = vmsne_vx_u64m1_b64(va, 0);\n        va = vmerge_vvm_u64m1(cmp2, va - vb, va);\n        vb = vmerge_vvm_u64m1(cmp2, vb, vb);\n    }\n    \n    return vslide1down_vx_u64m1_i(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    vuint64m1_t va = vmv_v_x_u64m1(a);\n    vuint64m1_t vb = vmv_v_x_u64m1(b);\n    \n    while (vmsne_vv_u64m1_b1(va, vb)) {\n        vbool1_t cmp = vmsgt_vv_u64m1_b1(va, vb);\n        va = vmerge_vvm_u64m1(cmp, va-vb, va);\n        vb = vmerge_vvm_u64m1(cmp, vb, vb-va);\n    }\n    \n    return vslide1down_vx_u64m1(vb, 0);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    vuint64m1_t va = vmv_v_x_u64m1(a);\n    vuint64m1_t vb = vmv_v_x_u64m1(b);\n    \n    while(va != vb){\n        vbool64_t cmp = vmsne_vv_u64m1_b64(va, vb);\n        va = vmerge_vvm_u64m1(cmp, vb, va);\n        vb = vmerge_vvm_u64m1(cmp, va, vb);\n        vbool64_t cmp2 = vmsne_vx_u64m1_b64(va, 0);\n        va = vdivu_vx_u64m1(va, vmand_mm_b64(cmp2, va));\n    }\n    \n    return vslide1down_vx_u64m1(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    vuint64m1_t va = vmv_v_x_u64m1(a);\n    vuint64m1_t vb = vmv_v_x_u64m1(b);\n    \n    while (va != vb) {\n        vbool64_t cmp = vmsne_vv_u64m1_b64(va, vb);\n        va = vmerge_vvm_u64m1(cmp, vb, va);\n        vb = vmerge_vvm_u64m1(cmp, va, vb);\n        vuint64m1_t diff = vsub_vv_u64m1(va, vb);\n        va = vmerge_vvm_u64m1(cmp, diff, va);\n        vb = vmerge_vvm_u64m1(cmp, vb, vb);\n    }\n    \n    return vslide1down_vx_u64m1(vb, 0);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    vint64m1_t va = vmv_s_x_i64m1(a);\n    vint64m1_t vb = vmv_s_x_i64m1(b);\n    \n    while (vmsne_vx_i64m1_b1(va, 0) && vmsne_vx_i64m1_b1(vb, 0)) {\n        vint64m1_t temp = vremu_vv_i64m1(va, vb);\n        va = vb;\n        vb = temp;\n    }\n    \n    return vmv_x_s_i64m1(va);\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vlen = vsetvl_e32m1();\n    int num_elements = numbers.size();\n    int remainder = num_elements % vlen;\n    int num_vectors = num_elements / vlen + (remainder != 0);\n\n    vfloat32m1_t v_input, v_next, v_diff, v_min_diff;\n    vfloat32m1_t v_result = vfmv_v_f_f32m1(0.0);\n    vbool32_t mask = vmseq_vx_i32m1_b32(num_elements % vlen, 0);\n\n    for (int i = 0; i < num_vectors; i++) {\n        int start = i * vlen;\n        v_input = vlse32_v_f32m1(&numbers[start], vlen);\n\n        v_next = vslide1down_vx_f32m1(v_input, 1);\n        v_diff = vfsub_vv_f32m1(v_input, v_next);\n        v_min_diff = vmin_vv_f32m1(v_diff, vslide1up_vx_f32m1(v_diff, 1));\n\n        mask = vmslt_vv_i32m1_b32(v_min_diff, v_result, mask);\n        v_result = vmerge_vvm_f32m1(mask, v_min_diff, v_result);\n    }\n\n    float result[vlen];\n    vse32_v_f32m1(result, v_result);\n\n    float min_diff = result[0];\n    for (int i = 1; i < vlen; i++) {\n        if (result[i] < min_diff) {\n            min_diff = result[i];\n        }\n    }\n\n    float closest_elements[2];\n    for (int i = 0; i < num_elements - 1; i++) {\n        if (numbers[i + 1] - numbers[i] == min_diff) {\n            closest_elements[0] = numbers[i];\n            closest_elements[1] = numbers[i + 1];\n            break;\n        }\n    }\n\n    return {closest_elements[0], closest_elements[1]};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vlen = vsetvl_e32m1();\n    int num_elements = numbers.size();\n    int remainder = num_elements % vlen;\n    int full_chunks = num_elements - remainder;\n\n    vfloat32m1_t v_input, v_input_next, v_diff, v_min_diff;\n    vfloat32m1_t v_closest1 = vfmv_v_f_f32m1(0.0);\n    vfloat32m1_t v_closest2 = vfmv_v_f_f32m1(0.0);\n    vbool32_t mask = VMSEQ_VI_F32m1_b32(vlen);\n\n    for (int i = 0; i < full_chunks; i += vlen) {\n        v_input = vlseg2e32_v_f32m1(&numbers[i]);\n        v_input_next = vlseg2e32_v_f32m1(&numbers[i + vlen]);\n\n        v_diff = vfsub_vv_f32m1(v_input, v_input_next);\n        v_min_diff = vmin_vx_f32m1(v_diff, 0.0f);\n\n        v_closest1 = vmerge_vvm_f32m1(v_closest1, v_input, vmsif_v_f32m1(mask, v_input_next, v_min_diff));\n        v_closest2 = vmerge_vvm_f32m1(v_closest2, v_input_next, vmsif_v_f32m1(mask, v_input, v_min_diff));\n    }\n\n    float closest1 = vslide1down_vx_f32m1_f32(v_closest1, 0.0f);\n    float closest2 = vslide1down_vx_f32m1_f32(v_closest2, 0.0f);\n\n    std::vector<float> result = {closest1, closest2};\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int VL = vsetvl_e32m1();\n    int num_elements = numbers.size();\n    int remainder = num_elements % VL;\n    int num_vectors = num_elements / VL + (remainder != 0);\n\n    vfloat32m1_t v_input, v_next, v_diff, v_min_diff;\n    vfloat32m1_t v_result = vfmv_v_f_f32m1(0.0);\n    float min_diff = FLT_MAX;\n    int min_idx1 = 0, min_idx2 = 0;\n\n    for (int i = 0; i < num_vectors; i++) {\n        int start = i * VL;\n        int end = (i == num_vectors - 1) ? num_elements : start + VL;\n        int vector_size = end - start;\n\n        v_input = vle32_v_f32m1(&numbers[start]);\n\n        for (int j = i + 1; j < num_vectors; j++) {\n            int start_next = j * VL;\n            int end_next = (j == num_vectors - 1) ? num_elements : start_next + VL;\n            int vector_size_next = end_next - start_next;\n\n            v_next = vle32_v_f32m1(&numbers[start_next]);\n\n            v_diff = vfsub_vv_f32m1(v_input, v_next);\n            v_diff = vfabs_v_f32m1(v_diff);\n\n            v_min_diff = vmin_vv_f32m1(v_min_diff, v_diff);\n\n            vbool32_t mask = vmsltu_vx_i32m1_b32(vminu_vx_i32m1(vmsltu_vx_i32m1_b32(v_input, v_next), 1), 1);\n            v_result = vsel_vf_f32m1(mask, v_input, v_result);\n\n            vbool32_t mask2 = vmsltu_vx_i32m1_b32(vminu_vx_i32m1(vmsltu_vx_i32m1_b32(v_input, v_next), 1), 0);\n            v_result = vsel_vf_f32m1(mask2, v_next, v_result);\n        }\n    }\n\n    vse32_v_f32m1(&result[0], v_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vlen = vsetvl_e32m1();\n    int num_elements = numbers.size();\n    int remainder = num_elements % vlen;\n    int full_chunks = num_elements - remainder;\n\n    vfloat32m1_t v_input, v_next, v_diff, v_min_diff;\n    vfloat32m1_t v_closest1 = vfmv_v_f_f32m1(0.0);\n    vfloat32m1_t v_closest2 = vfmv_v_f_f32m1(0.0);\n    vbool32_t mask = VMSEQ_VI_F32m1_b32(vlen);\n\n    for (int i = 0; i < full_chunks; i += vlen) {\n        v_input = vlse32_v_f32m1(&numbers[i], vlen);\n        v_next = vlse32_v_f32m1(&numbers[i + 1], vlen);\n        v_diff = vfsub_vv_f32m1(v_input, v_next);\n        v_min_diff = vmin_vv_f32m1(v_diff, vslide1down_vx_f32m1(v_diff, 1));\n        v_closest1 = vmerge_vvm_f32m1(v_closest1, v_input, vmsif_vx_f32m1(mask, v_input, v_min_diff));\n        v_closest2 = vmerge_vvm_f32m1(v_closest2, v_next, vmsif_vx_f32m1(mask, v_next, v_min_diff));\n    }\n\n    float closest1 = vfmv_f_s_f32_f32m1_f32(v_closest1);\n    float closest2 = vfmv_f_s_f32_f32m1_f32(v_closest2);\n\n    for (int i = full_chunks; i < num_elements - 1; i++) {\n        float diff = numbers[i] - numbers[i + 1];\n        if (diff < closest2 - closest1) {\n            closest1 = numbers[i];\n            closest2 = numbers[i + 1];\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vector_length = vsetvlmax_e32m1();\n    const int num_vectors = numbers.size() / vector_length;\n\n    vfloat32m1_t v_input, v_next, v_diff, v_min_diff;\n    vfloat32m1_t v_result = {0.0, 0.0};\n    float min_diff = 999999.0;\n\n    for (int i = 0; i < num_vectors; i++) {\n        v_input = vle32_v_f32m1(&numbers[i * vector_length]);\n\n        v_next = vle32_v_f32m1(&numbers[i * vector_length + 1]);\n\n        v_diff = vfsub_vv_f32m1(v_input, v_next);\n\n        v_min_diff = vmin_vv_f32m1(v_diff, v_min_diff);\n\n        vbool32_t mask = vmslt_vx_i32m1_b32(v_min_diff, 0.0);\n\n        v_result = vmerge_vvm_f32m1(mask, v_input, v_next);\n    }\n\n    float result[2];\n    vse32_v_f32m1(result, v_result);\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int size = numbers.size();\n    vfloat32m1_t vec_numbers = vle32_v_f32m1(&numbers[0]);\n\n    float min_val = vfmv_f_s_f32m1_f32(vec_numbers);\n    float max_val = vfmv_f_s_f32m1_f32(vfredmax_vs_f32m1_f32m1(vec_numbers, vec_numbers, vfmv_f_s_f32m1_f32(vec_numbers)));\n\n    vfloat32m1_t scaled_numbers = vfsub_vf_f32m1(vec_numbers, min_val);\n    scaled_numbers = vfmul_vf_f32m1(scaled_numbers, 1.0 / (max_val - min_val));\n\n    std::vector<float> result(size);\n    vse32_v_f32m1(&result[0], scaled_numbers);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int size = numbers.size();\n    vfloat32m1_t vec_numbers = vle32_v_f32m1(&numbers[0]);\n\n    float min_val = vfmv_f_s_f32m1_f32(vec_numbers);\n    float max_val = vfmv_f_s_f32m1_f32(vec_numbers);\n\n    for(int i = 0; i < size; i += VL) {\n        vfloat32m1_t chunk = vle32_v_f32m1(&numbers[i]);\n        vfloat32m1_t scaled = vfmul_vf_f32m1(chunk - min_val, 1.0 / (max_val - min_val));\n        vse32_v_f32m1(&numbers[i], scaled);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int num_elems = numbers.size();\n    vfloat32m1_t vec_nums = vle32_v_f32m1(&numbers[0]);\n\n    float min_num = vfmv_f_s_f32_f32(vec_nums);\n    float max_num = vfmv_f_s_f32_f32(vfredmax_vs_f32_f32m1_f32(vec_nums, vec_nums, vfmv_f_s_f32_f32(vec_nums)));\n\n    vfloat32m1_t scaled_nums = vfsub_vf_f32m1(vec_nums, min_num);\n    scaled_nums = vfmul_vf_f32m1(scaled_nums, 1.0f / (max_num - min_num));\n\n    std::vector<float> result(num_elems);\n    vse32_v_f32m1(&result[0], scaled_nums);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int size = numbers.size();\n    vfloat32m1_t vec_in = vle32_v_f32m1(&numbers[0]);\n    \n    float min_val = vmin_v_f32m1_f32(vec_in);\n    float max_val = vmax_v_f32m1_f32(vec_in);\n    \n    vfloat32m1_t vec_out = vfmul_vf_f32m1(vec_in, 1.0 / (max_val - min_val));\n    vec_out = vfsub_vf_f32m1(vec_out, min_val / (max_val - min_val));\n    \n    std::vector<float> result(size);\n    vse32_v_f32m1(&result[0], vec_out);\n    \n    return result;\n}\n```  "}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int vlen = vsetvlmax_e32m1();\n    int num_elems = numbers.size();\n    int num_vecs = num_elems / vlen;\n    \n    vfloat32m1_t vmax = vfmv_f_s_f32m1(0.0);\n    vfloat32m1_t vmin = vfmv_f_s_f32m1(FLT_MAX);\n    \n    for(int i = 0; i < num_vecs; i++){\n        vfloat32m1_t vdata = vle32_v_f32m1(&numbers[i * vlen]);\n        vmax = vfmax_vf_f32m1(vmax, vdata);\n        vmin = vfmin_vf_f32m1(vmin, vdata);\n    }\n    \n    float min_val, max_val;\n    vssmaxu_vx_f32m1(vmin, 0, &min_val);\n    vssmaxu_vx_f32m1(vmax, 0, &max_val);\n    \n    vfloat32m1_t vscale = vfmv_v_f_f32m1(1.0 / (max_val - min_val));\n    vfloat32m1_t voffset = vfmv_v_f_f32m1(-min_val / (max_val - min_val));\n    \n    std::vector<float> result(num_elems);\n    \n    for(int i = 0; i < num_vecs; i++){\n        vfloat32m1_t vdata = vle32_v_f32m1(&numbers[i * vlen]);\n        vfloat32m1_t vrescaled = vfmul_vf_f32m1(vfsub_vf_f32m1(vdata, vmin), vscale);\n        vse32_v_f32m1(&result[i * vlen], vrescaled);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    vuint64m1_t v_n = vmv_v_x_u64m1(n);\n    vuint64m1_t v_divisors = vdivu_vv_u64m1(v_n, v_n);\n    vuint64m1_t v_result = vsubu_vx_u64m1(v_divisors, 1);\n    uint64_t result;\n    vse64_v_u64m1(&result, v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    vuint64m1_t v_n = vmv_v_x_u64m1(n);\n    vuint64m1_t v_divisors = vdivu_vv_u64m1(v_n, v_n);\n    vuint64m1_t v_result = vsubu_vx_u64m1(v_divisors, 1);\n    uint64_t result;\n    vse64_v_u64m1(&result, v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    vuint64m1_t v_n = vmv_v_x_u64m1(n);\n    vuint64m1_t v_divisors = vsub_vx_u64m1(v_n, 1);\n    vbool64_t mask = vmsne_vx_u64m1_b64(v_n, 0);\n    vuint64m1_t v_result = vdivu_vv_u64m1(v_n, v_divisors);\n    v_result = vmerge_vvm_u64m1(mask, v_result, v_n);\n    return vslide1down_vx_u64(v_result, 0);\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    vuint64m1_t v_n = vmv_v_x_u64m1(n);\n    vuint64m1_t v_divisors = vmv_v_x_u64m1(1);\n    vbool64_t mask = vmsne_vx_u64m1_b64(v_n, 1);\n    \n    for (int i = 2; i <= n/2; i++) {\n        vuint64m1_t v_i = vmv_v_x_u64m1(i);\n        vbool64_t divisor_mask = vmand_mm_b64(mask, vmsne_vx_u64m1_b64(v_n, i));\n        v_divisors = vsel_v_u64m1_b64(divisor_mask, v_i, v_divisors);\n    }\n    \n    vuint64m1_t result = vslide1down_vx_u64m1(v_divisors, 1);\n    return vslide1down_vx_u64m1_m(mask, result, 1);\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    vuint64m1_t v_n = vmv_v_x_u64m1(n);\n    vuint64m1_t v_divisors = vsub_vx_u64m1(v_n, 1);\n    vbool64_t mask = vmsne_vx_u64m1_b64(v_divisors, 0);\n    v_divisors = vmerge_vvm_u64m1(mask, v_divisors, vmv_v_x_u64m1(0));\n    vbool64_t mask_divisible = vmand_mm_b64(v_n, v_divisors);\n    vuint64m1_t v_largest_divisor = vrgather_vx_u64m1(v_divisors, vmsbf_m_b64(mask_divisible));\n    uint64_t largest_divisor;\n    vse64_v_u64m1(&largest_divisor, v_largest_divisor);\n    return largest_divisor;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> factorize_simd(int n){\n    vint32m1_t factors = vmv_v_x_i32m1(n);\n    vint32m1_t divisor = vmv_v_x_i32m1(2);\n    vint32m1_t one = vmv_v_x_i32m1(1);\n    std::vector<int> result;\n\n    while (vmsne_vx_i32m1_b1(divisor, n)) {\n        vint32m1_t quotient = vdiv_vv_i32m1(factors, divisor);\n        vbool32_t is_divisible = vmand_mm_b32(vmsne_vv_i32m1_b32(quotient, one), vseq_vx_i32m1_b32(vmul_vv_i32m1(divisor, quotient), factors));\n\n        if (vmpopc_m_b32(is_divisible) > 0) {\n            result.push_back(vsadd_vx_i32m1_i32(vmv_x_s_i32m1(divisor), vmpopc_m_b32(is_divisible)));\n            factors = vdiv_vv_i32m1(factors, vssrl_vx_i32m1(divisor, vpopc_m_b32(is_divisible)));\n        } else {\n            divisor = vsadd_vx_i32m1_i32(divisor, 1);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> factorize_simd(int n){\n    vint32m1_t factors = vmv_v_i32m1(1); // Initialize vector to store factors\n    int factor = 2;\n    while (n > 1) {\n        if (n % factor == 0) {\n            vint32m1_t factor_v = vmv_v_i32m1(factor);\n            factors = vmerge_vvm_i32m1(factors, factor_v, n / factor); // Merge factor into factors vector n/factor times\n            n /= factor;\n        } else {\n            factor++;\n        }\n    }\n    return vfncvt_x_f_v_i32m1_i32(factors); // Convert vector to std::vector<int>\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> factorize_simd(int n){\n    vint32m1_t factors = vmv_v_x_i32m1(n);\n    vint32m1_t divisor = vmv_v_x_i32m1(2);\n    vint32m1_t one = vmv_v_x_i32m1(1);\n    vbool32_t mask = vmseq_vx_i32m1_b32(factors, 1);\n    std::vector<int> result;\n\n    while(!vmsltu_vx_i32m1_b32(factors, 2)){\n        if(vmsne_vv_i32m1_b32(vremu_vv_i32m1(divisor, factors), 0)){\n            divisor = vadd_vx_i32m1(divisor, 1);\n        } else {\n            factors = vdiv_vv_i32m1(factors, divisor);\n            result.push_back(vmv_x_s_i32m1(divisor));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> factorize_simd(int n){\n    vint32m1_t factors = vmv_v_i32m1(n);\n    vint32m1_t divisor = vmv_v_i32m1(2);\n    std::vector<int> result;\n\n    while (vmsne_vx_i32m1_b1(divisor, n)) {\n        vbool32_t is_factor = vmand_mm_b32(vmsne_vx_i32m1_b32(divisor, 0), vseq_vx_i32m1_b32(vdiv_vx_i32m1_b32(n, divisor), 0));\n        vint32m1_t divided = vdiv_vx_i32m1_b32(n, divisor);\n        vbool32_t is_divisible = vmand_mm_b32(vmsne_vx_i32m1_b32(divided, 0), vseq_vx_i32m1_b32(vmul_vx_i32m1_b32(divisor, divided), n));\n\n        factors = vsadd_vx_i32m1(factors, vsll_vx_i32m1(vmv_x_s_i32m1(vfirst_m_i32m1(is_factor), 1), 1));\n        n = vmerge_vvm_i32m1(is_divisible, n, divided);\n        divisor = vsadd_vx_i32m1(divisor, 1);\n    }\n\n    for (int i = 2; i <= n; i++) {\n        while (n % i == 0) {\n            result.push_back(i);\n            n /= i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> factorize_simd(int n){\n    vint32m1_t factors = vmv_v_x_i32m1(n);\n    vint32m1_t divisor = vmv_v_x_i32m1(2);\n    vint32m1_t one = vmv_v_x_i32m1(1);\n    vbool32_t mask = vmseq_vx_i32m1_b32(factors, 1);\n    std::vector<int> result;\n\n    while (!vmsne_vx_i32m1_b32(factors, 1)) {\n        vbool32_t is_factor = vmand_mm_b32(vmsne_vv_i32m1_b32(vremu_vv_i32m1_b32(factors, divisor), 0), mask);\n        int count = vmpopc_m_b32(is_factor);\n        if (count > 0) {\n            result.push_back(vmv_x_s_i32m1_i32(vslide1down_vx_i32m1(factors, 0)));\n            factors = vdiv_vv_i32m1(factors, vmerge_vvm_i32m1(is_factor, one, mask));\n        } else {\n            divisor = vadd_vx_i32m1(divisor, 1);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result;\n    \n    for (int i = 0; i < size; i += vl) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vbool32_t mask = vmfgt_vf_f32m1_b32(vec_l, 0.0f, vl);\n        vfloat32m1_t vec_result = vcompress_vm_f32m1(mask, vec_l, vl);\n        \n        for (int j = 0; j < vpopc_m_b32(mask); j++) {\n            float value;\n            vssd_v_f32m1(&value, vec_result, j);\n            result.push_back(value);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result;\n    \n    for (int i = 0; i < size; i += vl) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vbool32_t mask = vmfgt_vf_f32m1_b32(vec_l, 0.0f, vl);\n        vfloat32m1_t vec_result = vmerge_vfm_f32m1(mask, 0.0f, vec_l, vl);\n        \n        for (int j = 0; j < vl; j++) {\n            if (mask[j]) {\n                float value;\n                vse32_v_f32m1(&value, vec_result, j);\n                result.push_back(value);\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result;\n    \n    for (int i = 0; i < size; i += vl) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vbool32_t mask = vmfgt_vf_f32m1_b32(vec_l, 0.0f, vl);\n        vfloat32m1_t vec_result = vmerge_vfm_f32m1(mask, 0.0f, vec_l, vl);\n        \n        for (int j = 0; j < vl; j++) {\n            if (mask[j]) {\n                float val;\n                vse32_v_f32m1(&val, vec_result, j);\n                result.push_back(val);\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int vlen = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result;\n    \n    for (int i = 0; i < size; i += vlen) {\n        vfloat32m1_t vin = vle32_v_f32m1(&l[i]);\n        vbool32_t mask = vmfgt_vf_f32m1_b32(vin, 0.0f, vlen);\n        vfloat32m1_t vout = vcompress_vm_f32m1(mask, vin, vlen);\n        \n        for (int j = 0; j < vout.vl; j++) {\n            float val;\n            vse32_v_f32m1(&val, vout, j);\n            result.push_back(val);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result;\n    \n    for (int i = 0; i < size; i += vl) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vbool32_t mask = vmfgt_vf_f32m1_b32(vec_l, 0.0f, vl);\n        vfloat32m1_t vec_result = vslide1down_vx_f32m1(vec_l, 1, vl);\n        vse32_v_f32m1(&result[i], vec_result, mask);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    vint64m1_t v_n = vmv_s_x_i64m1(n);\n    vint64m1_t v_i = vmv_x_s_i64m1(5);\n    vbool64_t prime_mask = vmsne_vx_i64m1_b64(v_n, 2) & vmsne_vx_i64m1_b64(v_n, 3);\n\n    while (v_i < v_n) {\n        vbool64_t divisible = vseq_vx_i64m1_b64(v_n, 0) | vseq_vx_i64m1_b64(v_n, 1);\n        divisible |= vredsum_vs_i64m1_b64(v_n % v_i, divisible);\n        prime_mask &= vmsne_vx_i64m1_b64(divisible, 0);\n        v_i += 6;\n    }\n\n    return vmand_vv_b64m1_m(mask, vmsne_vx_i64m1_b64(v_n, 0));\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n){\n    if(n <= 1) return false;\n    if(n <= 3) return true;\n    \n    vint64m1_t v_n = vmv_s_x_i64m1(n);\n    vbool64_t prime_mask = vmsne_vx_i64m1_b64(v_n, 2);\n    \n    for(int64_t i = 2; i*i <= n; i++){\n        vint64m1_t v_i = vmv_s_x_i64m1(i);\n        vbool64_t div_mask = vmand_mm_b64(vmsne_vv_i64m1_b64(v_n, v_i), vmsne_vx_i64m1_b64(v_n, i+1));\n        prime_mask = vmand_mm_b64(prime_mask, div_mask);\n    }\n    \n    return vmand_vv_i64m1_b64(prime_mask, vmsne_vx_i64m1_b64(v_n, n));\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n){\n    if(n <= 1){\n        return false;\n    }\n    \n    if(n <= 3){\n        return true;\n    }\n    \n    vint64m1_t v_n = vmv_v_x_i64m1(n);\n    vbool64_t is_prime = vmsne_vx_i64m1_b(v_n, 2);\n    \n    for(int64_t i = 2; i*i <= n; i++){\n        vint64m1_t v_i = vmv_v_x_i64m1(i);\n        vbool64_t is_divisible = vmand_mm_b(vmsne_vx_i64m1_b(v_n, i), vmeq_vx_i64m1_b(vn, i));\n        is_prime = vmand_mm_b(is_prime, vmornot_mm_b(is_divisible));\n    }\n    \n    return vmfirst_m_b(is_prime);\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) {\n        return false;\n    }\n    \n    if (n <= 3) {\n        return true;\n    }\n    \n    vint64m1_t v_n = vmv_s_x_i64m1(n);\n    vbool64_t is_prime = vmsne_vx_i64m1_b64(v_n, 2);\n    \n    for (int64_t i = 2; i * i <= n; i++) {\n        vint64m1_t v_i = vmv_s_x_i64m1(i);\n        vbool64_t is_divisible = vmand_mm_b64(vmeq_vv_i64m1_b64(v_n, v_i), vmsne_vx_i64m1_b64(v_n, i));\n        is_prime = vmand_mm_b64(is_prime, is_divisible);\n    }\n    \n    return vmand_mm_b64(is_prime, vmsne_vx_i64m1_b64(v_n, 0));\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) {\n        return false;\n    }\n    \n    if (n <= 3) {\n        return true;\n    }\n    \n    if (n % 2 == 0 || n % 3 == 0) {\n        return false;\n    }\n    \n    int64_t i = 5;\n    int64_t step = 2;\n    \n    vint64m1_t v_n = vmv_s_x_i64m1(n);\n    vint64m1_t v_i = vmv_s_x_i64m1(i);\n    vint64m1_t v_step = vmv_s_x_i64m1(step);\n    \n    vbool64m1_t prime_mask = vmsne_vx_i64m1_b64(v_n, 0);\n    \n    while (i * i <= n) {\n        vbool64m1_t divisible_mask = vseq_vx_i64m1_b64(v_n, 0);\n        divisible_mask = vmand_mm_b64(divisible_mask, vmsne_vx_i64m1_b64(v_n, i));\n        divisible_mask = vmand_mm_b64(divisible_mask, vmsne_vx_i64m1_b64(v_n, i + 2));\n        \n        if (vmpopc_m_b64(divisible_mask) > 0) {\n            return false;\n        }\n        \n        i += step;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```\nstd::string solve_simd(const std::string & s){\n    const int N = s.size();\n    vint8 v_s = *reinterpret_cast<const vint8*>(s.data());\n    vint8 v_upper = vand(v_s, vmv_x_sint8(' '));\n    vint8 v_lower = vadd(v_upper, vmv_x_sint8(' '));\n\n    vint8 v_is_letter = vand(vclt(v_upper, vmv_x_sint8('A')), vcgt(v_upper, vmv_x_sint8('Z')));\n    v_is_letter = vor(v_is_letter, vand(vclt(v_upper, vmv_x_sint8('a')), vcgt(v_upper, vmv_x_sint8('z'))));\n\n    vint8 v_result = vselect(vxor(v_s, v_upper), v_s, v_is_letter);\n\n    vint8 v_no_letters = vnot(v_is_letter);\n    vint8 v_reversed = vslide1(v_result, 1);\n    v_reversed = vselect(v_reversed, v_result, vand(v_no_letters, vmsb(v_reversed)));\n\n    char result[N];\n    vse8_v_i(result, v_reversed);\n    return std::string(result, N);\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```\nstd::string solve_simd(const std::string & s){\n    const int N = s.size();\n    vint8 v_s = *reinterpret_cast<const vint8*>(s.data());\n    vint8 mask_upper = vmseq_vx_i8(v_s, 'A');\n    vint8 mask_lower = vmseq_vx_i8(v_s, 'a');\n    vint8 mask_letter = vor_vv_i8(mask_upper, mask_lower);\n    vint8 mask_non_letter = vmseq_vx_i8(v_s, 'A' - 1);\n    \n    vint8 diff_case = vsub_vx_i8(v_s, 'a' - 'A');\n    vint8 is_upper = vand_vv_i8(mask_upper, v_s);\n    vint8 is_lower = vand_vv_i8(mask_lower, v_s);\n    vint8 result = vadd_vv_i8(vxor_vv_vv_i8(is_upper, diff_case), is_lower);\n    \n    vint8 reversed = vrgather_vx_i8(result, reinterpret_cast<const int8_t*>(s.data()), vnot_v_i8(mask_letter));\n    vint8 final_result = vmerge_vvm_i8(reversed, result, mask_letter);\n    \n    std::string res(N, ' ');\n    vse8_v_i8m1(reinterpret_cast<int8_t*>(res.data()), final_result, N);\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string solve_simd(const std::string & s){\n    const int N = s.size();\n    vint8 v_s = *reinterpret_cast<const vint8*>(s.c_str());\n    vint8 mask_upper = vmsgeu_vx_vx_i8(v_s, 'A'-1) & vmseq_vx_i8(v_s, 'Z');\n    vint8 mask_lower = vmsgeu_vx_vx_i8(v_s, 'a'-1) & vmseq_vx_i8(v_s, 'z');\n    vint8 mask_letter = mask_upper | mask_lower;\n    vint8 diff = vmv_v_x_i8('a' - 'A');\n    \n    vint8 result = v_s ^ (mask_upper | mask_lower);\n    result = vmerge_vvm_vv_i8(mask_upper, result ^ diff, result);\n    result = vmerge_vvm_vv_i8(mask_lower, result ^ diff, result);\n    \n    vint8 mask_non_letter = vmseq_vx_i8(v_s, 'A'-1) | vmseq_vx_i8(v_s, 'Z') | vmseq_vx_i8(v_s, 'a'-1) | vmseq_vx_i8(v_s, 'z');\n    vint8 reversed = vrgather_vx_i8(v_s, N-1) ^ mask_non_letter;\n    \n    return std::string(reinterpret_cast<const char*>(&reversed), N);\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```\nstd::string solve_simd(const std::string & s) {\n    const int N = s.size();\n    vint8 v_s = *reinterpret_cast<const vint8*>(s.data());\n    vint8 v_upper = vand(v_s, vmv_x_sint8(' '));\n    vint8 v_lower = vadd(v_upper, vmv_x_sint8(' ' - 'a'));\n    vint8 v_is_letter = vand(vclt(v_upper, vmv_x_sint8(26)), vcgt(v_upper, vmv_x_sint8(-27)));\n    vint8 v_is_not_letter = vnot(v_is_letter);\n    vint8 v_result = vmerge(vxor(v_s, v_upper), vmerge(vxor(v_s, v_lower), v_s, v_is_not_letter), v_is_letter);\n    vint8 v_reversed = vslide1(v_result, 1);\n    vint8 v_mask = vmsne(v_reversed, vmv_x_sint8(0));\n    vint8 v_final = vmerge(vslide1(v_reversed, 1), v_reversed, v_mask);\n    char result[N];\n    vse8_v_i8(result, v_final);\n    return std::string(result, N);\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string solve_simd(const std::string & s){\n    const int N = s.size();\n    vint8 v_s = *reinterpret_cast<const vint8*>(s.data());\n    vint8 v_upper = vmsgeu_vx_i8(v_s, 'A'-1) & vmsleu_vx_i8(v_s, 'Z');\n    vint8 v_lower = vmsgeu_vx_i8(v_s, 'a'-1) & vmsleu_vx_i8(v_s, 'z');\n    vint8 v_case_diff = v_upper | v_lower;\n    vint8 v_result = v_s ^ (v_case_diff << 5);\n    vint8 v_non_letter = vmseq_vx_i8(v_s, 'A'-1) | vmseq_vx_i8(v_s, 'Z') | vmseq_vx_i8(v_s, 'a'-1) | vmseq_vx_i8(v_s, 'z');\n    vint8 v_all_letters = vmandnot_vv_i8(v_case_diff, v_non_letter);\n    vint8 v_reversed = vrgather_vx_i8(v_result, vint8(N-1) - vrseq_i8(N));\n    vint8 v_final_result = vmerge_vvm_i8(v_reversed, v_result, v_all_letters);\n    std::string result(N, ' ');\n    vse8_v_i8m1(result.data(), v_final_result, N);\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    const int remainder = size % vl;\n    const int full_vectors = size / vl;\n    \n    vint32m1_t v_l = vle32_v_i32m1(&l[0], vl * full_vectors);\n    \n    vint32m1_t v_indices = vmv_v_x_i32m1(0);\n    vint32m1_t v_mask = vmsne_vx_i32m1(v_indices, 0, vl);\n    \n    vint32m1_t v_l_divisible_by_three = vand_vvm_i32m1(v_mask, v_l);\n    vint32m1_t v_l_not_divisible_by_three = vandn_vvm_i32m1(v_mask, v_l);\n    \n    vint32m1_t v_l_divisible_by_three_sorted = vslideup_vx_i32m1(v_l_divisible_by_three, 1, vl);\n    v_l_divisible_by_three_sorted = vsort_v_i32m1(v_l_divisible_by_three_sorted);\n    \n    vint32m1_t v_result = vmerge_vvm_i32m1(v_l_not_divisible_by_three, v_l_divisible_by_three_sorted, v_mask);\n    \n    std::vector<int> result(size);\n    vse32_v_i32m1(&result[0], v_result, vl * full_vectors);\n    \n    for (int i = vl * full_vectors; i < size; ++i) {\n        result[i] = l[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int VL = vsetvlmax_e32m1();\n    int vlen = l.size();\n    int* l_ptr = l.data();\n    \n    // Load input vector\n    vint32m1_t vin = vle32_v_i32m1(l_ptr, VL);\n    \n    // Create masks for elements divisible by three\n    vbool32m1_t mask_divisible_by_three = vmseq_vx_i32m1(vin, 0, VL);\n    \n    // Sort elements divisible by three\n    vint32m1_t sorted_divisible_by_three = vsort_v_i32m1(vin, mask_divisible_by_three, VL);\n    \n    // Merge sorted elements back to the original vector\n    vint32m1_t merged = vmerge_vvm_i32m1(sorted_divisible_by_three, vin, mask_divisible_by_three, VL);\n    \n    // Store the result back to a vector\n    std::vector<int> result(vlen);\n    vse32_v_i32m1(result.data(), merged, VL);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int VL = vsetvl_e32m1();\n    int size = l.size();\n    int remainder = size % VL;\n    int full_chunks = size - remainder;\n\n    vint32m1_t v_l = vle32_v_i32m1(&l[0], VL);\n\n    vint32m1_t v_indices = vmv_v_x_i32m1(0);\n    vint32m1_t v_mask = vmsne_vx_i32m1(v_indices, 0, VL);\n\n    for(int i = 0; i < full_chunks; i += VL){\n        vint32m1_t v_chunk = vle32_v_i32m1(&l[i], VL);\n        vint32m1_t v_sorted_chunk = vslideup_vx_i32m1(v_chunk, 1, VL);\n        vint32m1_t v_result = vmerge_vvm_i32m1(v_sorted_chunk, v_chunk, v_mask, VL);\n        vse32_v_i32m1(&l[i], v_result, VL);\n    }\n\n    if(remainder > 0){\n        vint32m1_t v_chunk = vle32_v_i32m1(&l[full_chunks], remainder);\n        vint32m1_t v_sorted_chunk = vslideup_vx_i32m1(v_chunk, 1, remainder);\n        vint32m1_t v_result = vmerge_vvm_i32m1(v_sorted_chunk, v_chunk, v_mask, remainder);\n        vse32_v_i32m1(&l[full_chunks], v_result, remainder);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    int* data = l.data();\n    \n    for(int i = 0; i < size; i += vl){\n        vint32m1_t v_l = vle32_v_i32m1(&data[i]);\n        vint32m1_t v_mask = vmseq_vx_i32m1(v_l, 0, vl);\n        vint32m1_t v_sorted = vslideup_vx_i32m1(v_l, 1, vl);\n        vint32m1_t v_result = vmerge_vvm_i32m1(v_mask, v_l, v_sorted, vl);\n        vse32_v_i32m1(&data[i], v_result);\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int vector_size = vsetvlmax_e32m1();\n    int num_elements = l.size();\n    int num_vectors = (num_elements + vector_size - 1) / vector_size;\n\n    // Load input vector\n    vint32m1_t input = vle32_v_i32m1(l.data());\n\n    // Initialize masks for elements divisible by three\n    vbool32m1_t mask_divisible_by_three = vmseq_vx_i32m1_b32(input, 0, VL1);\n\n    // Create a sorted copy of elements divisible by three\n    vint32m1_t sorted_divisible_by_three = vslideup_vx_i32m1(vrgather_vx_i32m1(input, 0, VL1), 1, VL1);\n    sorted_divisible_by_three = vsort_v_i32m1(sorted_divisible_by_three);\n\n    // Merge original vector and sorted elements divisible by three\n    vint32m1_t result = vmerge_vvm_i32m1(mask_divisible_by_three, input, sorted_divisible_by_three, VL1);\n\n    // Store the result vector\n    std::vector<int> result_vector(num_elements);\n    vse32_v_i32m1(result_vector.data(), result);\n\n    return result_vector;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nfloat max_element_simd(std::vector<float> l){\n    vfloat32m1_t vec = vle32_v_f32m1(l.data());\n    vfloat32m1_t max_val = vfredmax_vs_f32m1_f32m1(vec, vec, vfmv_f_s_f32m1(0.0), l.size());\n    float result;\n    vse32_v_f32m1(&result, max_val);\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\nfloat max_element_simd(std::vector<float> l){\n    const int lane_count = vsetvlmax_e32m1();\n    int remaining = l.size();\n    float max_val = std::numeric_limits<float>::min();\n\n    while (remaining > 0) {\n        int vl = std::min(remaining, lane_count);\n        vfloat32m1_t v_l = vle32_v_f32m1(&l[l.size() - remaining], vl);\n\n        float max_in_lane = vfmv_f_s_f32m1_f32(vfredmax_vs_f32m1_f32m1(vfmv_f_f32m1_f32(v_l), v_l, vl));\n\n        max_val = std::max(max_val, max_in_lane);\n\n        remaining -= vl;\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\nfloat max_element_simd(std::vector<float> l){\n    const int lane_count = vsetvlmax_e32m1();\n    int remaining = l.size();\n    float max_val = std::numeric_limits<float>::min();\n    \n    while (remaining > 0) {\n        const int vl = std::min(remaining, lane_count);\n        vfloat32m1_t v_l = vle32_v_f32m1(&l[l.size() - remaining], vl);\n        \n        float max_in_lane = vfmv_f_s_f32m1_f32(v_l);\n        max_val = std::max(max_val, max_in_lane);\n        \n        remaining -= vl;\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nfloat max_element_simd(std::vector<float> l){\n    vfloat32m1_t vec = vle32_v_f32m1(l.data());\n    vfloat32m1_t max_val = vfredmax_vs_f32m1_f32m1(vec, vec, vfmv_f_s_f32m1(0.0), l.size());\n    float result;\n    vse32_v_f32m1(&result, max_val);\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\nfloat max_element_simd(std::vector<float> l){\n    const int lane_count = vsetvlmax_e32m1();\n    int remaining = l.size();\n    float max_val = std::numeric_limits<float>::min();\n    \n    while (remaining > 0) {\n        int vl = std::min(remaining, lane_count);\n        vfloat32m1_t v_l = vle32_v_f32m1(&l[l.size() - remaining], vl);\n        \n        float max_in_lane = vfmv_f_s_f32m1_f32(v_l);\n        max_val = std::max(max_val, max_in_lane);\n        \n        remaining -= vl;\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n){\n    vint8m1_t v_count = vmv_v_x_i8m1(0);\n    vint8m1_t v_seven = vmv_v_x_i8m1(7);\n    vint8m1_t v_eleven = vmv_v_x_i8m1(11);\n    vint8m1_t v_thirteen = vmv_v_x_i8m1(13);\n\n    for (int i = 0; i < n; i += 8) {\n        vint8m1_t v_i = vmv_v_x_i8m1(i);\n        vbool8_t mask = vmsne_vx_i8m1_b8(vremu_vv_i8m1_b8(v_i, v_eleven), 0) | vmsne_vx_i8m1_b8(vremu_vv_i8m1_b8(v_i, v_thirteen), 0);\n        v_count += vmand_mm_i8m1(vmsne_vv_i8m1_b8(v_i, v_seven), mask);\n    }\n\n    int count = vredsum_vs_i8_i32m1(v_count);\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n){\n    vint8m1_t v_count = vmv_s_x_i8m1(0);\n    vint8m1_t v_seven = vmv_s_x_i8m1(7);\n    vint8m1_t v_eleven = vmv_s_x_i8m1(11);\n    vint8m1_t v_thirteen = vmv_s_x_i8m1(13);\n    \n    for(int i = 0; i < n; i += 8){\n        vint8m1_t v_i = vmv_x_s_i8m1(i);\n        vbool8_t mask = vmsne_vx_i8m1_b8(vremu_vx_i8m1(v_i, v_eleven), 0) | vmsne_vx_i8m1_b8(vremu_vx_i8m1(v_i, v_thirteen), 0);\n        v_count += vmand_mm_b8(mask, vredsum_vs_i8m1_i8m1(vmsne_vx_i8m1_b8(v_i, v_seven)));\n    }\n    \n    int count = vslide1down_vx_i8m1_i8(vredsum_vs_i8m1_i8m1(v_count), 0);\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n){\n    vint8m1_t v_count = vmv_s_x_i8m1(0);\n    vint8m1_t v_seven = vmv_s_x_i8m1(7);\n    vint8m1_t v_eleven = vmv_s_x_i8m1(11);\n    vint8m1_t v_thirteen = vmv_s_x_i8m1(13);\n    \n    vint8m1_t v_i = vmv_s_x_i8m1(0);\n    vbool8_t mask = vmseq_vx_i8m1_b8(v_i, 0);\n    \n    for(int i=1; i<n; i++){\n        v_i = vadd_vx_i8m1(v_i, 1);\n        mask = vmsne_vx_i8m1_b8(v_i, 0);\n        \n        vint8m1_t v_mod_eleven = vrem_vx_i8m1(v_i, 11);\n        vint8m1_t v_mod_thirteen = vrem_vx_i8m1(v_i, 13);\n        \n        vbool8_t mask_eleven = vmseq_vx_i8m1_b8(v_mod_eleven, 0);\n        vbool8_t mask_thirteen = vmseq_vx_i8m1_b8(v_mod_thirteen, 0);\n        \n        vbool8_t mask_divisible = vor_vv_b8(mask_eleven, mask_thirteen);\n        \n        v_count = vadd_vv_i8m1(v_count, vand_vv_i8m1(mask, vand_vv_i8m1(mask_divisible, vand_vv_i8m1(vsext_vf2_i8m1(mask_divisible), v_seven))));\n    }\n    \n    int count = vmv_x_s_i8m1_i32(vredsum_vs_i8m1_i32(v_count));\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n){\n    vint32m1_t v_count = vmv_v_x_i32m1(0);\n    vint32m1_t v_seven = vmv_v_x_i32m1(7);\n    vint32m1_t v_eleven = vmv_v_x_i32m1(11);\n    vint32m1_t v_thirteen = vmv_v_x_i32m1(13);\n\n    for (int i = 0; i < n; i += VL) {\n        vint32m1_t v_i = vmv_v_x_i32m1(i);\n        vbool32_t mask = vmsne_vx_i32m1_b32(vremu_vx_i32m1(v_i, v_eleven), 0) | vmsne_vx_i32m1_b32(vremu_vx_i32m1(v_i, v_thirteen), 0);\n        v_count += vmand_mm_i32m1(mask, vredsum_vs_i32m1_i32m1(vmsne_vv_i32m1_b32(v_i, v_seven)));\n    }\n\n    int count;\n    vse32_v_i32m1(&count, v_count);\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n){\n    vint8m1_t v_count = vmv_s_x_i8m1(0);\n    vint8m1_t v_seven = vmv_s_x_i8m1(7);\n    vint8m1_t v_eleven = vmv_s_x_i8m1(11);\n    vint8m1_t v_thirteen = vmv_s_x_i8m1(13);\n\n    for (int i = 0; i < n; i += 8) {\n        vint8m1_t v_i = vmv_x_s_i8m1(i);\n        vbool8_t mask = vmsne_vx_i8m1_b8(v_i, n);\n        vint8m1_t v_divisible_by_eleven = vrem_vx_i8m1(v_i, v_eleven);\n        vint8m1_t v_divisible_by_thirteen = vrem_vx_i8m1(v_i, v_thirteen);\n        vbool8_t mask_eleven = vmand_mm_b8(vmsne_vx_i8m1_b8(v_divisible_by_eleven, 0), mask);\n        vbool8_t mask_thirteen = vmand_mm_b8(vmsne_vx_i8m1_b8(v_divisible_by_thirteen, 0), mask);\n        vint8m1_t v_result_eleven = vand_mm_i8m1(vmsne_vx_i8m1_b8(v_divisible_by_eleven, 0), v_seven);\n        vint8m1_t v_result_thirteen = vand_mm_i8m1(vmsne_vx_i8m1_b8(v_divisible_by_thirteen, 0), v_seven);\n        v_count = vadd_vv_i8m1(v_count, vmerge_vvm_i8m1(mask_eleven, v_result_eleven, v_result_thirteen));\n    }\n\n    int count = vslide1down_vx_i8m1_i8(v_count, 4);\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    const int vl = vsetvl_e32m1();\n    const int size = l.size();\n    std::vector<float> result(size);\n\n    for (int i = 0; i < size; i += vl) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vfloat32m1_t vec_l_sorted = vle32_v_f32m1(&l[i]);\n        vsort_v_f32m1(vec_l_sorted);\n        vse32_v_f32m1(&result[i], vec_l_sorted);\n        for (int j = 0; j < vl; j += 2) {\n            vse32_v_f32m1(&result[i + j], vec_l);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    const int VL = vsetvl_e32m1();\n    int size = l.size();\n    std::vector<float> result(size);\n\n    for (int i = 0; i < size; i += VL) {\n        vfloat32m1_t vec_l = vle32_v_f32m1(&l[i]);\n        vint32m1_t mask = vmseq_vx_i32m1(vmv_v_x_u_i32m1(vec_l, 0), 0);\n        vfloat32m1_t sorted = vsort_v_f32m1(vcompress_vm_f32m1(mask, vec_l));\n        vse32_v_f32m1(&result[i], vmerge_vfm_f32m1(sorted, vec_l, mask));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    const int VL = vsetvl_e32m1();\n\n    int size = l.size();\n    int remainder = size % VL;\n    int full_vectors = size / VL;\n\n    // Load input vector\n    vfloat32m1_t vin = vle32_v_f32m1(&l[0], size);\n\n    // Create mask for even indices\n    vbool32_t mask = VM_SEQ(VL) < size;\n\n    // Extract even indices\n    vfloat32m1_t even = vcompress_vm_f32m1(mask, vin);\n\n    // Sort even indices\n    vfloat32m1_t sorted_even = vsort_v_f32m1(even);\n\n    // Merge sorted even indices with original odd indices\n    vfloat32m1_t merged = vmerge_vvm_f32m1(mask, sorted_even, vin);\n\n    // Store the result\n    std::vector<float> result(size);\n    vse32_v_f32m1(&result[0], merged, size);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    const int size = l.size();\n    const int vl = vsetvl_e32m1();\n\n    if(size % vl != 0){\n        l.resize(((size + vl - 1) / vl) * vl);\n    }\n\n    float* data = l.data();\n    vfloat32m1_t vec_l, vec_sorted;\n    vbool32_t mask = VMSEQ_VI_F32M1(vl);\n\n    for(int i = 0; i < size; i += vl){\n        vec_l = vle32_v_f32m1(&data[i]);\n        vec_sorted = vslideup_vx_f32m1(vec_l, 1);\n        vec_sorted = vrgather_vx_f32m1(vec_sorted, &data[i], vl);\n        vse32_v_f32m1(&data[i], vec_sorted, mask);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    const int VL = vsetvl_e32m1();\n    int size = l.size();\n    std::vector<float> result(size);\n\n    float32x4_t vec_l, vec_sorted;\n    for (int i = 0; i < size; i += VL) {\n        vec_l = vle32_v_f32m1(&l[i]);\n        vec_sorted = vslideup_vx_f32m1(vec_l, 1);\n        vec_sorted = vrgather_vx_f32m1(vec_sorted, &l[i]);\n        vse32_v_f32m1(&result[i], vec_sorted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vlen = vsetvl_e32m1();\n    const int size = l.size();\n    const int rem = size % vlen;\n    const int bound = size - 2;\n    \n    vint32m1_t va, vb, vc, vd;\n    vbool32_t mask = VMASK_E32M1;\n    \n    for (int i = 0; i < bound; i += vlen) {\n        va = vlse32_v_i32m1(&l[i], vlen);\n        for (int j = i + 1; j < bound; j += vlen) {\n            vb = vlse32_v_i32m1(&l[j], vlen);\n            for (int k = j + 1; k < size; k += vlen) {\n                vc = vlse32_v_i32m1(&l[k], vlen);\n                vd = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc));\n                if (vmsne_vx_i32m1_b32(vd, 0, mask)) {\n                    return true;\n                }\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vlen = vsetvl_e32m1();\n    const int size = l.size();\n    const int rem = size % vlen;\n    const int bound = size - 2;\n\n    vint32m1_t va, vb, vc, vd;\n    vbool32_t mask = VMASK_E32M1;\n\n    for (int i = 0; i < bound; i += vlen) {\n        va = vlse32_v_i32m1(&l[i], vlen);\n        for (int j = i + 1; j < bound; j += vlen) {\n            vb = vlse32_v_i32m1(&l[j], vlen);\n            for (int k = j + 1; k < size; k += vlen) {\n                vc = vlse32_v_i32m1(&l[k], vlen);\n                vd = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc));\n                if (vmsne_vx_i32m1_b32(vd, 0, mask)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    if (rem > 0) {\n        va = vlse32_v_i32m1(&l[bound], rem);\n        for (int j = bound + 1; j < size; j++) {\n            vb = vlse32_v_i32m1(&l[j], 1);\n            for (int k = j + 1; k < size; k++) {\n                vc = vlse32_v_i32m1(&l[k], 1);\n                vd = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc));\n                if (vmsne_vx_i32m1_b32(vd, 0, mask)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vlen = vsetvl_e32m1();\n    const int size = l.size();\n    const int rem = size % vlen;\n    const int bound = size - 2;\n\n    vint32m1_t va, vb, vc, vd;\n    vbool32_t mask = VMASK_E32M1;\n\n    for (int i = 0; i < bound; i += vlen) {\n        va = vlse32_v_i32m1(&l[i], vlen);\n        for (int j = i + 1; j < bound; j += vlen) {\n            vb = vlse32_v_i32m1(&l[j], vlen);\n            for (int k = j + 1; k < size; k += vlen) {\n                vc = vlse32_v_i32m1(&l[k], vlen);\n                vd = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc));\n                if (vmand_mm_b32m1(vmsne_vx_i32m1(vd, 0), mask)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    if (rem) {\n        va = vlse32_v_i32m1(&l[bound], rem);\n        for (int j = bound + 1; j < size; j++) {\n            vb = vlse32_v_i32m1(&l[j], 1);\n            for (int k = j + 1; k < size; k++) {\n                vc = vlse32_v_i32m1(&l[k], 1);\n                vd = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc));\n                if (vmand_mm_b32m1(vmsne_vx_i32m1(vd, 0), mask)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vlen = vsetvl_e32m1();\n    const int size = l.size();\n    \n    for (int i = 0; i < size - 2; i++) {\n        vint32m1_t va = vle32_v_i32m1(&l[i], vlen);\n        for (int j = i + 1; j < size - 1; j++) {\n            vint32m1_t vb = vle32_v_i32m1(&l[j], vlen);\n            for (int k = j + 1; k < size; k += vlen) {\n                vint32m1_t vc = vle32_v_i32m1(&l[k], vlen);\n                vint32m1_t sum = vadd_vv_i32m1(va, vadd_vv_i32m1(vb, vc, vlen), vlen);\n                vbool32_t mask = vmseq_vx_i32m1_b32(sum, 0, vlen);\n                if (vmfirst_m_b32(mask, vlen) < vlen) {\n                    return true;\n                }\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vlen = vsetvl_e32m1();\n    int32_t* data = l.data();\n    int32x4_t v_l, v_m, v_n, v_sum;\n    \n    for (size_t i = 0; i < l.size(); i += vlen) {\n        v_l = vlse32_v_i32m1(data + i, vlen);\n        for (size_t j = i + 1; j < l.size(); j += vlen) {\n            v_m = vlse32_v_i32m1(data + j, vlen);\n            for (size_t k = j + 1; k < l.size(); k += vlen) {\n                v_n = vlse32_v_i32m1(data + k, vlen);\n                v_sum = vadd_vv_i32m1(vadd_vv_i32m1(v_l, v_m), v_n);\n                uint32_t mask = vmseq_vx_i32m1_b32(v_sum, 0);\n                if (vmpopc_m_b32(mask) > 3) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    \n    vint32m1_t v_input, v_output;\n    std::vector<int> result(l.size());\n    \n    for (int i = 0; i < num_lanes; i++) {\n        int mask = (i == num_lanes - 1) ? (1 << (l.size() % lane_size)) - 1 : -1;\n        \n        v_input = vlse32_v_i32m1(&l[i * lane_size], sizeof(int) * lane_size);\n        v_output = vadd_vx_i32m1(v_input, 1);\n        \n        vse32_v_i32m1(&result[i * lane_size], v_output, mask);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    std::vector<int> result(l.size());\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        v_l = vadd_vx_i32m1(v_l, 1);\n        vse32_v_i32m1(&result[i * lane_size], v_l);\n    }\n\n    return result;\n}\n```  "}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    std::vector<int> result(l.size());\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        v_l = vadd_vx_i32m1(v_l, 1);\n        vse32_v_i32m1(&result[i * lane_size], v_l);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n\n    vint32m1_t v_input, v_output;\n    v_input = vle32_v_i32m1(&l[0], l.size());\n\n    for (int i = 0; i < num_lanes; i++) {\n        v_output = vadd_vx_i32m1(v_input, 1);\n        vsse32_v_i32m1(&l[0], v_output, l.size());\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    std::vector<int> result(l.size());\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        v_l = vadd_vx_i32m1(v_l, 1);\n        vse32_v_i32m1(&result[i * lane_size], v_l);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    \n    vint32m1_t vec_l = {0};\n    vbool32_t result = vfalse_v_b32m1();\n    \n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t vec_lanes = vle32_v_i32m1(&l[i * lane_size]);\n        vint32m1_t vec_neg = vsub_vx_i32m1(vec_lanes, 0);\n        \n        for (int j = i + 1; j < num_lanes; j++) {\n            vint32m1_t vec_lanes2 = vle32_v_i32m1(&l[j * lane_size]);\n            vbool32_t mask = vmsne_vv_i32m1_b32(vec_lanes, vec_lanes2);\n            vbool32_t mask2 = vmand_mm_b32(mask, vmand_mm_b32(vmsle_vx_i32m1_b32(vec_lanes2, 0), vmsle_vx_i32m1_b32(vec_neg, vec_lanes2)));\n            result = vor_mm_b32(result, mask2);\n        }\n    }\n    \n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    \n    vint32m1_t vec_l;\n    vbool32_t result = vfalse32m1();\n\n    for (int i = 0; i < num_lanes; i++) {\n        int start = i * lane_size;\n        int end = start + lane_size > l.size() ? l.size() : start + lane_size;\n        \n        vec_l = vle32_v_i32m1(&l[start]);\n        \n        for (int j = i + 1; j < num_lanes; j++) {\n            vint32m1_t vec_l2 = vle32_v_i32m1(&l[j * lane_size]);\n            vint32m1_t sum = vadd_vv_i32m1(vec_l, vec_l2);\n            vbool32_t mask = vmsne_vx_i32m1_b32(sum, 0);\n            result = vor_vb_vb_vb32m1(result, mask);\n        }\n    }\n\n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    \n    vint32m1_t vec_l = {0};\n    vbool32_t result = vfalse_v_b32m1();\n    \n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t vec_lanes = vle32_v_i32m1(&l[i * lane_size]);\n        vint32m1_t vec_neg = vsub_vx_i32m1(vec_lanes, 0);\n        \n        for (int j = i + 1; j < num_lanes; j++) {\n            vint32m1_t vec_lanes2 = vle32_v_i32m1(&l[j * lane_size]);\n            vbool32_t mask = vmsne_vv_i32m1_b32(vec_lanes, vec_lanes2);\n            vbool32_t mask2 = vmand_mm_b32(mask, vmand_mm_b32(vmsle_vx_i32m1_b32(vec_lanes2, 0), vmsle_vx_i32m1_b32(vec_neg, vec_lanes2)));\n            result = vor_mm_b32(result, mask2);\n        }\n    }\n    \n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vector_size = vsetvli(0, RVV_E8, RVV_M8);\n    const int num_elements = l.size();\n    const int num_vectors = (num_elements + vector_size - 1) / vector_size;\n\n    vint8_t vec_l = {0};\n    for (int i = 0; i < num_vectors; i++) {\n        vbool8_t mask = vmseq_vx_b(vector_size, i * vector_size);\n        vint8_t vec_l_part = vlseg8_v_i8m1(l.data() + i * vector_size, mask);\n\n        for (int j = i + 1; j < num_vectors; j++) {\n            vbool8_t mask2 = vmseq_vx_b(vector_size, j * vector_size);\n            vint8_t vec_l_part2 = vlseg8_v_i8m1(l.data() + j * vector_size, mask2);\n\n            vint8_t vec_sum = vadd_vv_i8m1(vec_l_part, vec_l_part2);\n            vbool8_t mask_zero = vmseq_vx_i8m1(vec_sum, 0);\n            if (vmpopc_m(mask_zero) > 0) {\n                return true;\n            }\n        }\n        vec_l = vmerge_vvm_i8m1(vec_l, vec_l_part, mask);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = l.size() / lane_size + (l.size() % lane_size != 0);\n    \n    vint32m1_t vec_l = {0};\n    vint32m1_t vec_zero = vmv_v_x_i32m1(0, 0);\n    \n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t vec_elements = vle32_v_i32m1(&l[i * lane_size]);\n        vint32m1_t vec_neg_elements = vsub_vx_i32m1(vec_zero, vec_elements);\n        \n        for (int j = i + 1; j < num_lanes; j++) {\n            vint32m1_t vec_next_elements = vle32_v_i32m1(&l[j * lane_size]);\n            vbool32_t mask = vmsne_vv_i32m1_b32(vec_elements, vec_next_elements);\n            vint32m1_t vec_sum = vadd_vv_i32m1(vec_elements, vec_next_elements);\n            vbool32_t result = vmand_mm_b32(mask, vmsle_vx_i32m1_b32(vec_sum, vec_zero));\n            \n            if (vmpopc_m_b32(result) > 0) {\n                return true;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    vint8_t result = vmv_s_x_i8(x);\n    vint8_t base_v = vmv_s_x_i8(base);\n    vint8_t quotient, remainder;\n    vint8_t zero = vmv_s_i8(0);\n    vint8_t ten = vmv_s_i8(10);\n    vint8_t mask = vmv_s_i8(15);\n\n    std::string res = \"\";\n    while (vnclipu_wx_i8(quotient, result, base_v)) {\n        remainder = vremu_vv_i8(result, base_v);\n        result = vsrl_vv_i8(result, base_v);\n        vbool8_t mask_gt_9 = vmsltu_vx_i8(remainder, mask);\n        vint8_t add_val = vselect_vv_i8(mask_gt_9, vadd_vv_i8(remainder, vmv_s_i8('A'-10)), vadd_vv_i8(remainder, vmv_s_i8('0')));\n        res = std::to_string(vslide1down_vx_i8(res, add_val));\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    vint8_t result = vcreate_i8(0);\n    vint8_t input = vcreate_i8(x);\n    vint8_t divisor = vcreate_i8(base);\n\n    while (vmaxu_vx_i8(input, vdup_v_i8(0))) {\n        vint8_t remainder = vremu_vv_i8(input, divisor);\n        result = vsll_vv_i8(result, 1);\n        result = vor_vv_i8(result, remainder);\n        input = vdivu_vv_i8(input, divisor);\n    }\n\n    int8_t output[64];\n    vse8_v_i8(output, result);\n\n    int i = 0;\n    while (output[i] == 0) {\n        i++;\n    }\n\n    std::string res = \"\";\n    for (; i < 64; i++) {\n        res += std::to_string(output[i]);\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    vint8_t result = vmv_s_x_i8(x);\n    vint8_t base_v = vmv_s_x_i8(base);\n    vint8_t quotient, remainder;\n\n    vint8_t zero = vmv_x_s_i8(0);\n    vint8_t ten = vmv_x_s_i8(10);\n    vint8_t ascii_offset = vmv_x_s_i8(48);\n\n    std::string res = \"\";\n    \n    while(vmaxu_vx_i8(result, zero) != zero){\n        vdivu_vv_i8(quotient, remainder, result, base_v);\n        result = quotient;\n        vint8_t ascii_char = vadd_vv_i8(remainder, ascii_offset);\n        res = std::to_string(vslide1down_v_i8(ascii_char, 0)) + res;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    const int vl = vsetvl_e64m1();\n    const int64_t num_elements = vmsne_vx_i64m1_b8(x, 0, vl);\n    int64_t input[num_elements];\n    for (int i = 0; i < num_elements; i++) {\n        input[i] = x % base;\n        x /= base;\n    }\n    int64_t output[num_elements];\n    for (int i = 0; i < num_elements; i++) {\n        output[i] = input[num_elements - 1 - i];\n    }\n    char result[num_elements + 1];\n    for (int i = 0; i < num_elements; i++) {\n        result[i] = output[i] + '0';\n    }\n    result[num_elements] = '\\0';\n    return std::string(result);\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    vint8_t result = vmv_s_x_i8(x, 0);\n    vint8_t base_v = vmv_s_x_i8(base, 0);\n    vint8_t quotient, remainder;\n    vint8_t zero = vmv_s_x_i8(0, 0);\n    vint8_t one = vmv_s_x_i8(1, 0);\n    vint8_t ten = vmv_s_x_i8(10, 0);\n    vint8_t ascii_offset = vmv_s_x_i8(48, 0);\n    vint8_t ascii_result = vsub_vv_i8(result, vmul_vx_i8(vdiv_vv_i8(result, base_v), base_v));\n    vint8_t carry = vdiv_vv_i8(result, base_v);\n    vbool8_t mask = vmsne_vv_i8(result, zero);\n    vint8_t result_final = vand_vv_vb_i8(ascii_result, mask);\n    \n    while(vmpopc_m(mask) > 0){\n        result = vand_vv_vb_i8(carry, mask);\n        ascii_result = vsub_vv_i8(result, vmul_vx_i8(vdiv_vv_i8(result, base_v), base_v));\n        carry = vdiv_vv_i8(result, base_v);\n        mask = vmsne_vv_i8(result, zero);\n        result_final = vsll_vx_i8(result_final, 1);\n        result_final = vor_vv_vb_i8(result_final, vand_vv_vb_i8(ascii_result, mask));\n    }\n    \n    vint8_t result_str = vadd_vv_i8(result_final, ascii_offset);\n    int8_t result_arr[8];\n    vse8_v_i8(result_str, result_arr);\n    \n    std::string result_string;\n    for(int i=0; i<8; i++){\n        if(result_arr[i] != 0){\n            result_string += char(result_arr[i]);\n        }\n    }\n    \n    return result_string;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    int vec_len = len / 2;\n    vint8 v_text = *reinterpret_cast<vint8*>(text.data());\n    \n    for (int i = 0; i < vec_len; i++) {\n        vint8 v_front = vsseg8e8_v_i32m1(v_text, i);\n        vint8 v_back = vsseg8e8_v_i32m1(v_text, len - i - 1);\n        vbool8 mask = vmseq_vx_i8m1_b8(v_front, v_back);\n        if (!vmand_vx_i8m1(mask)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    int vec_len = len / 2;\n    vint8 v_text = *reinterpret_cast<vint8*>(text.data());\n    \n    for (int i = 0; i < vec_len; i++) {\n        vint8 v_first = vsseg8_v_i8(&v_text, i);\n        vint8 v_last = vsseg8_v_i8(&v_text, len - i - 1);\n        if (vfirst != vlast) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    int vec_len = len / 2;\n    vint8 v_text = *reinterpret_cast<vint8*>(text.data());\n    \n    for (int i = 0; i < vec_len; i++) {\n        vint8 v_first = vsseg8e8_v_i8(v_text, i);\n        vint8 v_last = vsseg8e8_v_i8(v_text, len - i - 1);\n        if (vmsne_vv_i8(v_first, v_last)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    int vec_len = len / 2;\n    vint8 v_text = create_vint8(text.c_str());\n    \n    for (int i = 0; i < vec_len; i += 8) {\n        vint8 v_left = vlh_v_i8(&v_text, i);\n        vint8 v_right = vlh_v_i8(&v_text, len - i - 8);\n        if (!vmseq_v_i8(v_left, v_right)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    int vec_len = len / 2;\n    vint8 v_text = *reinterpret_cast<vint8*>(text.data());\n    \n    for (int i = 0; i < vec_len; i++) {\n        vint8 v_front = vsseg8_v_i8(&v_text, i);\n        vint8 v_back = vsseg8_v_i8(&v_text, len - i - 1);\n        if (vmsne_vv_i8(v_front, v_back)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```\nstd::string decode_shift_simd(std::string s){\n    const int vec_len = vlenb_v_i8m1;\n    const int block_size = vec_len * 2;\n    \n    std::string out;\n    int i = 0;\n    \n    while (i < s.length()) {\n        vint8m1_t input = vle8_v_i8m1(&s[i]);\n        vint8m1_t shifted = vsub_vx_i8m1(input, 'a');\n        shifted = vadd_vx_i8m1(shifted, 21); // 26 - 5 = 21\n        shifted = vremu_vx_i8m1(shifted, 26);\n        shifted = vadd_vx_i8m1(shifted, 'a');\n        \n        vse8_v_i8m1(&out[i], shifted);\n        \n        i += block_size;\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string decode_shift_simd(std::string s){\n    const int vector_length = vlenb_v_i32;\n    const int total_length = s.length();\n    const int num_vectors = (total_length + vector_length - 1) / vector_length;\n    \n    vint32m1_t shift_amount = vmv_v_x_i32m1('a' - 5);\n    vint32m1_t modulo_value = vmv_v_x_i32m1(26);\n    vint32m1_t char_mask = vmv_v_x_i32m1(0xFF);\n    \n    std::string out;\n    out.resize(total_length);\n    \n    for (int i = 0; i < num_vectors; i++) {\n        vint32m1_t input_chars = vle32_v_i32m1(&s[i * vector_length]);\n        vint32m1_t shifted_chars = vadd_vx_i32m1(input_chars, 5);\n        shifted_chars = vsub_vv_i32m1(shifted_chars, shift_amount);\n        shifted_chars = vremu_vv_i32m1(shifted_chars, modulo_value);\n        shifted_chars = vadd_vv_i32m1(shifted_chars, shift_amount);\n        vse8_v_i32m1(&out[i * vector_length], vand_vx_i32m1(shifted_chars, char_mask));\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string decode_shift_simd(std::string s){\n    const int vector_length = vlenb_v_i32;\n    int i = 0;\n    std::string out;\n\n    while (i < s.length()) {\n        vint32m4_t str_v = vle32_v_i32m4(&s[i]);\n        vint32m4_t a_v = vmv_v_x_i32m4('a');\n        vint32m4_t shifted_v = vsub_vx_i32m4(str_v, 'a');\n        shifted_v = vadd_vx_i32m4(shifted_v, 21); // 26 - 5 = 21\n        shifted_v = vremu_vx_i32m4(shifted_v, 26);\n        shifted_v = vadd_vv_i32m4(shifted_v, a_v);\n\n        vse32_v_i32m4(&out[i], shifted_v);\n\n        i += vector_length;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```\nstd::string decode_shift_simd(std::string s){\n    const int8_t shift = -5;\n    const int8_t a_val = 'a';\n    const int8_t z_val = 'z';\n    const int8_t alphabet_size = z_val - a_val + 1;\n\n    const int8_t* input = reinterpret_cast<const int8_t*>(s.c_str());\n    size_t input_size = s.length();\n\n    std::string out;\n    out.resize(input_size);\n\n    const int8x16_t shift_v = vmv_s_x_i8(shift);\n    const int8x16_t a_val_v = vmv_s_x_i8(a_val);\n    const int8x16_t alphabet_size_v = vmv_s_x_i8(alphabet_size);\n\n    for (size_t i = 0; i < input_size; i += 16) {\n        int8x16_t input_v = vlse16_v_i8(input + i, 1);\n        int8x16_t shifted_v = vadd_vx_i8(input_v, shift_v);\n        int8x16_t mod_v = vremu_vv_i8(shifted_v, alphabet_size_v);\n        int8x16_t result_v = vadd_vv_i8(mod_v, a_val_v);\n        vse16_v_i8(out.data() + i, result_v, 1);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```\nstd::string decode_shift_simd(std::string s){\n    const int vector_length = vsetvl_e32m1(0);\n    int i = 0;\n    std::string out = \"\";\n    int8_t* input = (int8_t*)s.c_str();\n    int8_t* output = new int8_t[s.length()];\n\n    while (i < s.length()) {\n        int8x16_t input_vec = vlxb_v_i8m1(input + i, vector_length);\n        int8x16_t shifted_vec = vssub_vx_i8m1(input_vec, 'a', vector_length);\n        shifted_vec = vsadd_vx_i8m1(shifted_vec, 21, vector_length); // 5 in ASCII\n        shifted_vec = vremu_vx_i8m1(shifted_vec, 26, vector_length);\n        shifted_vec = vsadd_vx_i8m1(shifted_vec, 'a', vector_length);\n\n        vse8_v_i8m1(output + i, shifted_vec, vector_length);\n        i += vector_length;\n    }\n\n    out = std::string(output, s.length());\n    delete[] output;\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\nbool below_threshold_simd(std::vector<int> l, int t){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = (l.size() + lane_size - 1) / lane_size;\n    vint32m1_t v_l, v_t;\n    vbool32_t result = vmsltu_vx_u32m1_b(lane_size, v_l, v_t, t);\n    for (int i = 0; i < num_lanes; i++) {\n        v_l = vlse32_v_i32m1(&l[i * lane_size], lane_size);\n        v_t = vmv_v_x_i32m1(t);\n        result = vmand_mm_b32(result, vmsltu_vx_u32m1_b(lane_size, v_l, v_t, t));\n    }\n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_threshold_simd(std::vector<int> l, int t){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = (l.size() + lane_size - 1) / lane_size;\n    vint32m1_t v_threshold = vmv_s_x_i32m1(t);\n    bool result = true;\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        vbool32_t mask = vmsltu_vx_i32m1_b32(v_l, v_threshold);\n        if (!vmfirst(mask)) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_threshold_simd(std::vector<int> l, int t){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = (l.size() + lane_size - 1) / lane_size;\n    vint32m1_t v_threshold = vmv_s_x_i32m1(t);\n    bool result = true;\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        vbool32_t mask = vmsltu_vx_i32m1_b32(v_l, v_threshold);\n        result = result && vmfirst_m_b32(mask);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_threshold_simd(std::vector<int> l, int t){\n    const int lane_size = vsetvlmax_e32m1();\n    const int num_lanes = (l.size() + lane_size - 1) / lane_size;\n    vint32m1_t v_threshold = vmv_s_x_i32m1(t);\n    bool result = true;\n\n    for (int i = 0; i < num_lanes; i++) {\n        vint32m1_t v_l = vle32_v_i32m1(&l[i * lane_size]);\n        vbool32_t mask = vmsltu_vx_i32m1_b32(v_l, v_threshold);\n        if (!vmfirst(mask)) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool below_threshold_simd(std::vector<int> l, int t){\n    const int lane_size = vsetvlmax_e32m1();\n    int vl = l.size();\n    int* result = (int*)malloc(vl * sizeof(int));\n    vint32m1_t v_l = vle32_v_i32m1(&l[0], vl * lane_size);\n    vint32m1_t v_t = vmv_s_i32m1(t);\n    vbool32_t mask = vmsltu_vx_i32m1_b32(v_l, v_t);\n    vsseg1e32_v_i32m1(&result[0], mask, v_l, vl * lane_size);\n    bool res = true;\n    for(int i=0; i<vl; i++){\n        if(result[i] == 0){\n            res = false;\n            break;\n        }\n    }\n    free(result);\n    return res;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int size = brackets.size();\n    if(size % 2 != 0) return false;\n\n    vbool64_t stack = vmseq_v_i64(0, 0);\n    int count = 0;\n\n    for(int i = 0; i < size; i++){\n        if(brackets[i] == '<'){\n            stack = vmfirst_m(stack, count);\n        }\n        else if(brackets[i] == '>'){\n            stack = vmfirst_m(stack, count);\n            if(vmsle_vx_i64_b(stack, count, 0)) return false;\n        }\n        count++;\n    }\n\n    return vmand_vv_b(stack, vmseq_v_i64(0, 0));\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int n = brackets.size();\n    int count = 0;\n    vint8 v_count = vmv_s_x(0);\n    \n    for (int i = 0; i < n; i += 8) {\n        vuint8 v_input = vlseg8h_v_u8(&brackets[i]);\n        v_count = vadd_vx_i8(v_count, vmsleu_vx_u8(v_input, '<'));\n        v_count = vsub_vx_i8(v_count, vmsleu_vx_u8(v_input, '>'));\n    }\n    \n    int result = vredsum_vs_i8(v_count);\n    return result == 0;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    int n = brackets.size();\n    int count = 0;\n    vint8 v_count = vmv_s_x(0);\n    \n    for (int i = 0; i < n; i += 64) {\n        vint8 v_open = vsub_vx_i8(vmsle_vx_i8(vle8_v_i8(brackets.data() + i, 0), 0), 1);\n        v_count = vadd_vv_i8(v_count, v_open);\n        \n        vint8 v_close = vsub_vx_i8(vmsle_vx_i8(vle8_v_i8(brackets.data() + i, 0), 1), 1);\n        v_count = vadd_vv_i8(v_count, v_close);\n    }\n    \n    int result = vmsleu_vx_i8(v_count, 0);\n    return vmfirst_m_i8(result) == 64;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    vbool32_t result = vcreate_mask(0);\n    int count = 0;\n\n    for (int i = 0; i < N; i += VL) {\n        vint32m1_t vbrackets = vle32_v_i32m1(&brackets[i], VL);\n\n        vbool32_t isOpeningBracket = vmsne_vx_i32m1_b32(vbrackets, '>', VL);\n        vbool32_t isClosingBracket = vmsne_vx_i32m1_b32(vbrackets, '<', VL);\n\n        count += vpopc_m_b32(isOpeningBracket);\n        count -= vpopc_m_b32(isClosingBracket);\n\n        result = vand_m_b32(result, count >= 0);\n    }\n\n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    vbool32_t result = vmsne_vx_u32(vmv_v_x_u32(0, VL), vmv_v_x_u32(0, VL), VL);\n\n    for (int i = 0; i < N; i += VL) {\n        vuint32m1_t open = vmsleu_vx_u32_vb(vleu_v_u32m1(&brackets[i], VL), '<', result, VL);\n        vuint32m1_t close = vmsleu_vx_u32_vb(vleu_v_u32m1(&brackets[i], VL), '>', result, VL);\n        result = vmand_mm_b32(result, vmsleu_vv_u32_b32(open, close, result, VL), VL);\n    }\n\n    return vmfirst_m_b32(result, VL);\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    vbool32_t mask = VMSET_VL(VL);\n    int count = 0;\n\n    for (int i = 0; i < N; i += VL) {\n        vint32m1_t vbrackets = VLE32_VL(&brackets[i], VL);\n\n        vbool32_t isOpeningBracket = VMSEQ_VI_I32M1(mask, vbrackets, '(');\n        vbool32_t isClosingBracket = VMSEQ_VI_I32M1(mask, vbrackets, ')');\n\n        count += vpopc_m(mask, isOpeningBracket) - vpopc_m(mask, isClosingBracket);\n    }\n\n    return count == 0;\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    vbool32_t mask = VMSET_VL(VL);\n    int count = 0;\n\n    for (int i = 0; i < N; i += VL) {\n        vint32m1_t vbrackets = VLE32_VL(&brackets[i], VL);\n\n        vbool32_t isOpeningBracket = VMSEQ_VI_I32M1(vbrackets, '(');\n        vbool32_t isClosingBracket = VMSEQ_VI_I32M1(vbrackets, ')');\n\n        count += vpopc_m(mask, isOpeningBracket) - vpopc_m(mask, isClosingBracket);\n    }\n\n    return count == 0;\n}\n```  "}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    const int block_size = vlenb / 8;\n    const int num_blocks = (n + block_size - 1) / block_size;\n\n    vbool64_t result = vcreate_i64(0);\n\n    for (int i = 0; i < num_blocks; i++) {\n        int start = i * block_size;\n        int end = start + block_size;\n        if (end > n) end = n;\n\n        vint8m1_t bracket_block = vle8_v_i8m1((int8_t*)&brackets[start], vlenb);\n\n        vint8m1_t open_brackets = vand_vx_i8m1(bracket_block, 1);\n        vint8m1_t close_brackets = vand_vx_i8m1(bracket_block, 2);\n\n        vint8m1_t open_count = vredsum_vs_i8m1_i8m1(open_brackets, vcreate_i8(0));\n        vint8m1_t close_count = vredsum_vs_i8m1_i8m1(close_brackets, vcreate_i8(0));\n\n        result = vor_vb_i64(result, vmsne_vv_i8m1_b64(open_count, close_count));\n    }\n\n    return vmfirst_m_b64(result);\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    int remaining = 0;\n    for (int i = 0; i < N; i += VL) {\n        vuint32m1_t open = vmv_x_s(0, VL);\n        vuint32m1_t close = vmv_x_s(0, VL);\n\n        vbool32_t mask = vmv_x_s(0, VL);\n\n        for (int j = 0; j < VL; j++) {\n            if (i + j < N) {\n                if (brackets[i + j] == '(') {\n                    open = vsaddu_vx_u32m1(open, 1, mask);\n                } else if (brackets[i + j] == ')') {\n                    close = vsaddu_vx_u32m1(close, 1, mask);\n                }\n            }\n        }\n\n        remaining += vmsleu_vv_u32m1_b32(open, close, mask);\n    }\n\n    return remaining == 0;\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int N = brackets.size();\n    const int VL = vsetvl_e32m1(0);\n\n    vbool32_t mask = VMSEQ_VI_E32M1(vmv_x_sx_e32m1((const int32_t*)brackets.data(), VL), '(', VL);\n    int count = 0;\n\n    for (int i = 0; i < N; i += VL) {\n        vbool32_t mask_chunk = VMBSET_VI_E32M1(mask, i + VL > N ? N - i : VL, VL);\n        vint32m1_t bracket_chunk = vle32_v_i32m1((const int32_t*)brackets.data() + i, mask_chunk, VL);\n        count += vmsumu_vx_i32m1_i32(mask_chunk, bracket_chunk, VL);\n    }\n\n    return count == 0;\n}\n```  "}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int size = xs.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vfloat32m1_t input = vle32_v_f32m1(xs.data(), lane_size);\n\n    vfloat32m1_t indices = vmv_v_x_f32m1((float)1, lane_size);\n    vfloat32m1_t powers = vfmul_vf_f32m1(indices, input, lane_size);\n\n    for (int i = 1; i < size - 1; i++) {\n        indices = vfmv_v_f_f32m1((float)i + 1, lane_size);\n        vfloat32m1_t temp = vfmul_vf_f32m1(indices, vslide1down_vx_f32m1(powers, 0, lane_size), lane_size);\n        powers = vfmul_vf_f32m1(indices, input, lane_size);\n        powers = vfadd_vv_f32m1(powers, temp, lane_size);\n    }\n\n    vfloat32m1_t result = vslide1down_vx_f32m1(powers, 0, lane_size);\n\n    std::vector<float> derivative(size - 1);\n    vse32_v_f32m1(derivative.data(), result, lane_size);\n\n    return derivative;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int size = xs.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vfloat32m1_t vec_xs = vle32_v_f32m1(xs.data(), size);\n\n    vfloat32m1_t vec_indices = vmv_v_x_f32m1(1); // Start derivative from x^1\n    vfloat32m1_t vec_result = vfmul_vf_f32m1(vec_xs, vec_indices);\n\n    for (int i = 2; i < size; i++) {\n        vec_indices = vfmv_v_f_f32m1(i);\n        vfloat32m1_t temp = vfmul_vf_f32m1(vec_xs, vec_indices);\n        vec_result = vfadd_vv_f32m1(vec_result, temp);\n    }\n\n    std::vector<float> result(size - 1);\n    vse32_v_f32m1(result.data(), vec_result, size - 1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int size = xs.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vfloat32m1_t input = vle32_v_f32m1(xs.data(), lane_size);\n\n    vfloat32m1_t indices = vmv_v_x_f32m1(1, lane_size); // Create a vector with values 1, 2, 3, ...\n    vfloat32m1_t result = vfmul_vf_f32m1(input, indices, lane_size); // Multiply input by indices\n\n    vfloat32m1_t mask = vmsne_vx_f32m1_b1(input, 0, lane_size); // Create a mask to exclude the constant term\n    result = vmerge_vfm_f32m1(mask, 0, result, 0, lane_size); // Apply the mask to exclude the constant term\n\n    return std::vector<float>(result.vstart, result.vstart + size - 1);\n}\n```  "}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int size = xs.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vfloat32m1_t vec_xs = vle32_v_f32m1(xs.data(), size);\n\n    vfloat32m1_t vec_indices = vmv_v_x_f32m1((float)1, lane_size);\n\n    vfloat32m1_t vec_result = vfmul_vf_f32m1(vec_xs, vec_indices, lane_size);\n\n    for (int i = 2; i < size; i++) {\n        vec_indices = vmv_v_x_f32m1((float)i, lane_size);\n        vfloat32m1_t vec_temp = vfmul_vf_f32m1(vec_xs, vec_indices, lane_size);\n        vec_result = vfadd_vv_f32m1(vec_result, vec_temp, lane_size);\n    }\n\n    std::vector<float> result(size - 1);\n    vse32_v_f32m1(result.data(), vec_result, size - 1);\n\n    return result;\n}\n```  "}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int size = xs.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vfloat32m1_t input = vle32_v_f32m1(xs.data(), lane_size);\n\n    vfloat32m1_t indices = vmv_v_x_f32m1((float)1, lane_size);\n    vfloat32m1_t powers = vfmul_vf_f32m1(indices, input, lane_size);\n\n    for(int i = 1; i < size - 1; i++){\n        indices = vfmv_v_f_f32m1((float)i+1, lane_size);\n        vfloat32m1_t temp = vfmul_vf_f32m1(indices, vslide1down_vx_f32m1(powers, 0, lane_size), lane_size);\n        powers = vfmacc_vv_f32m1(powers, input, temp, lane_size);\n    }\n\n    vfloat32m1_t result = vslide1down_vx_f32m1(powers, 0, lane_size);\n\n    std::vector<float> derivative(size - 1);\n    vse32_v_f32m1(derivative.data(), result, lane_size);\n\n    return derivative;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint vowels_count_simd(std::string s){\n    vint8 v_vowels = vmv_v_x_i8('a') | vmv_v_x_i8('e') | vmv_v_x_i8('i') | vmv_v_x_i8('o') | vmv_v_x_i8('u');\n    vint8 v_word = vrgather_vx_i8(s.c_str(), vsext_vf2_i8(vmv_v_x_i8(0), vmv_v_x_i8(1)));\n    vint8 v_y = vmv_v_x_i8('y');\n    vint8 v_end = vslide1down_vx_i8(v_word, 1);\n    vint8 v_vowels_end = vand_vv_i8(v_word, v_y);\n    vint8 v_vowels_count = vpopc_m_i8(v_word == v_vowels) + vpopc_m_i8(v_end == v_vowels_end);\n    return vfirst_m_i8(v_vowels_count);\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint vowels_count_simd(std::string s){\n    vint8 v_vowels = vmv_v_x_i8('a') | vmv_v_x_i8('e') | vmv_v_x_i8('i') | vmv_v_x_i8('o') | vmv_v_x_i8('u');\n    vint8 v_input = vsadd_vx_i8(vle8_v_i8(s.c_str()), 32); // Convert input characters to lowercase\n\n    vint8 v_result = vmsle_vv_i8(v_input, v_vowels); // Mask of vowels\n    vint8 v_y = vmseq_vx_i8(v_input, 'y'); // Mask of 'y' characters\n\n    vint8 v_last_char = vslide1down_vx_i8(v_input, ' '); // Shift input to get the last character\n    vint8 v_y_at_end = vmand_mm(v_y, vmsle_vx_i8(v_last_char, 'a')); // Mask of 'y' at the end\n\n    vint8 v_vowels_at_end = vmand_mm(v_y_at_end, vmsle_vx_i8(v_last_char, 'a')); // Mask of vowels at the end\n\n    vint8 v_total_vowels = vmand_mm(v_result, vmseq_vx_i8(v_y, 0)); // Exclude 'y' from the middle\n    v_total_vowels = vmand_mm(v_total_vowels, vmseq_vx_i8(v_vowels_at_end, 0)); // Exclude 'y' at the end\n\n    return vpopc_m_i8(v_total_vowels);\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint vowels_count_simd(std::string s){\n    vint8 v_vowels = vmv_v_x_i8('a') | vmv_v_x_i8('e') | vmv_v_x_i8('i') | vmv_v_x_i8('o') | vmv_v_x_i8('u');\n    vint8 v_word = vrgather_vx_i8(&s[0], vset_vl_e8(s.size()));\n    vint8 v_word_lower = vand_vv_i8(v_word, vmv_v_x_i8(0xDF)); // Convert uppercase to lowercase\n    vint8 v_vowels_end = vand_vv_i8(v_word_lower, vmv_v_x_i8('y'));\n    vint8 v_vowels_count = vredsum_vs_i8(v_word_lower == v_vowels, v_vowels_end == v_vowels);\n    return vslide1down_vx_i8(v_vowels_count, 1)[0];\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint vowels_count_simd(std::string s){\n    vint8 v_vowels = vmv_v_x_i8('a') | vmv_v_x_i8('e') | vmv_v_x_i8('i') | vmv_v_x_i8('o') | vmv_v_x_i8('u');\n    vint8 v_word = vrgather_vx_i8(s.c_str(), vslideup_vx_i8(vzero_v_i8(), 1));\n    vint8 v_word_end = vslideup_vx_i8(v_word, -1);\n    vint8 v_y = vmv_v_x_i8('y');\n    vint8 v_mask = vmsle_vv_i8(v_word_end, v_y) & vmsle_vv_i8(v_word, v_vowels);\n    return vpopc_m_i8(v_mask);\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint vowels_count_simd(std::string s){\n    vint8 v_vowels = vmv_v_x_i8('a') | vmv_v_x_i8('e') | vmv_v_x_i8('i') | vmv_v_x_i8('o') | vmv_v_x_i8('u');\n    vint8 v_input = vsadd_vx_i8(vle8_v_i8(s.data()), 32); // Convert input characters to lowercase\n\n    vint8 v_result = vmsle_vv_i8(v_input, v_vowels); // Mask of vowels\n    vint8 v_y = vmsle_vx_i8(v_input, 'y'); // Mask of 'y' characters\n\n    vbool8 v_end_y = vmand_mm_b8(v_y, vmsle_vx_i8(v_input, 0)); // Mask of 'y' characters at the end of the word\n    vresult = vmand_mm_b8(vresult, v_end_y); // Include 'y' characters at the end\n\n    return vpopc_m_b8(vresult); // Count the number of vowels\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```\n#include <riscv_vector.h>\n\nint digitSum_simd(std::string s){\n    vint8m1_t ascii_codes = vmv_v_x_i8m1(0); // Initialize accumulator vector\n    vint8m1_t upper_mask = vmv_v_x_i8m1(0); // Initialize mask vector for upper characters\n\n    for (int i = 0; i < s.size(); i++) {\n        vint8m1_t input_char = vmv_v_x_i8m1(s[i]); // Load input character\n        vbool8_t is_upper = vmsge_vx_i8m1_b8(input_char, 'A') && vmsle_vx_i8m1_b8(input_char, 'Z'); // Check if character is uppercase\n        ascii_codes = vadd_vv_i8m1(ascii_codes, vmerge_vvm_i8m1(is_upper, vmv_v_x_i8m1(0), input_char)); // Add ASCII code to accumulator if character is uppercase\n    }\n\n    vint8m1_t sum = vredsum_vs_i8m1_i8m1(ascii_codes, vmv_x_s_i8m1(0)); // Calculate sum of ASCII codes\n    int result;\n    vse8_v_i8m1(&result, sum); // Store result in scalar variable\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```\n#include <riscv_vector.h>\n\nint digitSum_simd(std::string s){\n    vint8m1_t ascii_codes = vmv_v_x_i8m1(0); // Initialize accumulator vector\n    vint8m1_t upper_mask = vmsge_vx_i8m1_v_i8m1(vmv_v_x_i8m1(0), vmv_v_x_i8m1('a')); // Mask for upper characters\n\n    for (size_t i = 0; i < s.size(); i += VL * sizeof(int8_t)) {\n        vint8m1_t str_chunk = vle8_v_i8m1(&s[i]); // Load chunk of string\n        vbool8_t upper_chars = vmand_mm_b8(vmsltu_vv_i8m1_b8(str_chunk, vmv_v_x_i8m1('a')), vmsltu_vv_i8m1_b8(vmv_v_x_i8m1('Z'), str_chunk)); // Check for upper characters\n        vint8m1_t upper_values = vmerge_vvm_i8m1(ascii_codes, str_chunk, upper_chars); // Merge accumulator and chunk based on upper characters\n        ascii_codes = vwadd_vv_i8m1(ascii_codes, vmand_mm_i8m1(upper_values, upper_mask)); // Add ASCII codes of upper characters to accumulator\n    }\n\n    int sum = vredsum_vs_i32_i8m1_i32(ascii_codes); // Calculate sum of ASCII codes\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint digitSum_simd(std::string s){\n    vint8 v_sum = vmv_v_x_i8(0);\n    vint8 v_upper = vmv_v_x_i8('A');\n    \n    for (size_t i = 0; i < s.size(); i += VL*2) {\n        vint8 v_str1 = vrgather_vx_i8(&s[i], v_upper, 1);\n        vint8 v_str2 = vrgather_vx_i8(&s[i+VL], v_upper, 1);\n        \n        vint8 v_mask1 = vmsltu_vx_i8(v_str1, 'Z');\n        vint8 v_mask2 = vmsltu_vx_i8(v_str2, 'Z');\n        \n        vint8 v_filtered1 = vand_vv_vv_i8(v_str1, v_mask1);\n        vint8 v_filtered2 = vand_vv_vv_i8(v_str2, v_mask2);\n        \n        v_sum = vadd_vv_vv_i8(v_sum, vfiltered1);\n        v_sum = vadd_vv_vv_i8(v_sum, vfiltered2);\n    }\n    \n    int sum = vslide1down_vx_i8(v_sum, 4);\n    sum = vdotus_vv_i8_i16(v_sum, v_sum, 0);\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```\n#include <riscv_vector.h>\n\nint digitSum_simd(std::string s){\n    vint8m1_t ascii_codes = vmv_v_x_i8m1(0);\n    vint8m1_t upper_mask = vmv_v_x_i8m1(0);\n\n    for (size_t i = 0; i < s.size(); i += VL * sizeof(int8_t)) {\n        vint8m1_t str_chunk = vle8_v_i8m1(&s[i]);\n\n        upper_mask = vmsge_vx_i8m1_b(upper_mask, str_chunk, 'A');\n        upper_mask = vmand_mm_b8(upper_mask, vmsleu_vx_i8m1_b(str_chunk, 'Z'));\n\n        ascii_codes = vmerge_vvm_i8m1(ascii_codes, vadd_vx_i8m1(ascii_codes, str_chunk), upper_mask);\n    }\n\n    int sum = vredsum_vs_i8_i32m1_i32(vadd_vx_i8m1(ascii_codes, 0));\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```\n#include <riscv_vector.h>\n\nint digitSum_simd(std::string s){\n    vint8 v_sum = vmv_v_x_i8(0);\n    vint8 v_upper = vmv_v_x_i8('A');\n    \n    for (size_t i = 0; i < s.size(); i += VL*2) {\n        vint8 v_str1 = vlh_v_i8(&s[i], VL);\n        vint8 v_str2 = vlh_v_i8(&s[i+VL], VL);\n        \n        vbool8 v_mask1 = vmand_mm_b8(vmsgeu_vx_i8(v_str1, 'A'), vmsleu_vx_i8(v_str1, 'Z'));\n        vbool8 v_mask2 = vmand_mm_b8(vmsgeu_vx_i8(v_str2, 'A'), vmsleu_vx_i8(v_str2, 'Z'));\n        \n        vint8 v_upper1 = vand_vv_i8(v_str1, v_mask1);\n        vint8 v_upper2 = vand_vv_i8(v_str2, v_mask2);\n        \n        v_sum = vadd_vv_i8(v_sum, vredsum_vs_i8(vadd_vv_i8(v_upper1, v_upper2), v_sum));\n    }\n    \n    return vslide1down_vx_i8(v_sum, 8);\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    vbool32_t mask = vmand_mm_b32(vmsne_vx_i32m1_b32(vec, 0), vmseq_vx_i32m1_b32(vremu_vx_i32m1_b32(vec, 2), 0));\n\n    vint32m1_t min_vals = vcompress_vm_i32m1(mask, vec);\n    vint32m1_t min_indices = vcompress_vm_i32m1(mask, vid_v_i32m1());\n\n    int32_t min_val;\n    int32_t min_index;\n\n    vssort_v_i32m1(min_vals, min_indices);\n\n    vse32_v_i32m1(&min_val, min_vals);\n    vse32_v_i32m1(&min_index, min_indices);\n\n    return {min_val, min_index};\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vl = vsetvl_e32m1();\n\n    vint32m1_t vec = {0};\n    vint32m1_t min_val = {INT_MAX};\n    vint32m1_t min_idx = {INT_MAX};\n\n    for(size_t i = 0; i < arr.size(); i += vl){\n        vint32m1_t values = vle32_v_i32m1(&arr[i]);\n\n        vbool32_t is_even = vand_vx_i32m1_b32(values, 1) == 0;\n        vbool32_t is_smaller = vle_v_i32m1_b32(values, min_val);\n        vbool32_t is_smaller_or_equal = vle_v_i32m1_b32(values, min_val) || vseq_v_i32m1_b32(values, min_val);\n\n        min_val = vsel_v_i32m1_b32(is_even & is_smaller, values, min_val);\n        min_idx = vsel_v_i32m1_b32(is_even & is_smaller_or_equal, vint32m1_t(i), min_idx);\n    }\n\n    int result[2] = {INT_MAX, INT_MAX};\n    vse32_v_i32m1(result, min_val);\n    vse32_v_i32m1(result + 1, min_idx);\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    vbool32_t mask = vmslt_vx_i32m1_b32(vec, 0);\n    vbool32_t even_mask = vmand_mm_b32(mask, 0x1);\n\n    vint32m1_t even_values = vmerge_vvm_i32m1(mask, vec, 0);\n    vint32m1_t min_even = vfirst_m_i32m1(even_mask, even_values);\n\n    vint32m1_t indices = vid_v_i32m1();\n    vint32m1_t min_index = vfirst_m_i32m1(vmand_mm_b32(vmslt_vv_i32m1_b32(even_values, min_even), even_mask), indices);\n\n    std::vector<int> result = {vslide1down_vx_i32m1_i32(min_even, 1)[0], vslide1down_vx_i32m1_i32(min_index, 1)[0]};\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    vbool32_t mask = vmslt_vx_i32m1_b32(vec, 0);\n    vbool32_t even_mask = vmand_mm_b32(mask, vmseq_vx_i32m1_b32(vand_vx_i32m1(mask, vec), 2));\n\n    int32_t min_val = 2147483647;\n    int32_t min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += VLMAX){\n        vint32m1_t sub_vec = vle32_v_i32m1(&arr[i]);\n        vbool32_t sub_mask = vmslt_vx_i32m1_b32(sub_vec, 0);\n        vbool32_t sub_even_mask = vmand_mm_b32(sub_mask, vmseq_vx_i32m1_b32(vand_vx_i32m1(sub_mask, sub_vec), 2));\n\n        vbool32_t cmp_mask = vmsltu_vx_i32m1_b32(vfirst_m_i32m1(sub_even_mask), min_val);\n        min_val = vmerge_vxm_i32m1(min_val, vfirst_v_i32m1(sub_even_mask), cmp_mask);\n        min_index = vmerge_vxm_i32m1(min_index, i + vfirst_m_i32m1(sub_even_mask), cmp_mask);\n    }\n\n    if(min_val == 2147483647) return {};\n\n    std::vector<int> result = {min_val, min_index};\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), arr.size());\n    vint32m1_t even_mask = vand_vx_i32m1(vsetvl_e32m1(arr.size()), vec, 1);\n    vint32m1_t even_values = vcompress_vm_i32m1(even_mask, vec);\n    \n    if(vmv_x_s_i32m1(even_values) == 0) return {};\n\n    vint32m1_t min_even = vmin_vx_i32m1(even_values, vmv_x_s_i32m1(even_values));\n    vint32m1_t min_even_mask = vmsne_vv_i32m1_b32(min_even, even_values, vsetvl_e32m1(arr.size()));\n    vint32m1_t min_even_indices = vid_v_i32m1(vsetvl_e32m1(arr.size()));\n    vint32m1_t min_even_index = vmerge_vvm_i32m1(min_even_mask, min_even_indices, vint32m1_t());\n    \n    int32_t result[2];\n    vse32_v_i32m1(result, min_even, 0);\n    vse32_v_i32m1(result + 1, min_even_index, 0);\n    \n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint search_simd(std::vector<int> lst){\n    const int VL = vsetvl_e32m1();\n    vint32m1_t vec = vsub_vx_i32m1(vmv_v_x_i32m1(0, VL), 1);\n    vint32m1_t freq = vsub_vx_i32m1(vmv_v_x_i32m1(0, VL), 1);\n    vint32m1_t result = vsub_vx_i32m1(vmv_v_x_i32m1(0, VL), 1);\n    \n    for (size_t i = 0; i < lst.size(); i += VL) {\n        vbool32_t mask = vmseq_vx_i32m1_b32(vle32_v_i32m1(&lst[i], VL), 0, VL);\n        vint32m1_t values = vle32_v_i32m1(&lst[i], VL);\n        \n        vint32m1_t new_freq = vsub_vx_i32m1(vmv_v_x_i32m1(0, VL), 1);\n        vint32m1_t new_result = vsub_vx_i32m1(vmv_v_x_i32m1(0, VL), 1);\n        \n        for (int j = 0; j < VL; j++) {\n            vbool32_t cmp = vmsltu_vx_i32m1_b32(values, j+1, VL);\n            vbool32_t cmp_freq = vmseq_vx_i32m1_b32(freq, j+1, VL);\n            vbool32_t cmp_result = vmseq_vx_i32m1_b32(result, j+1, VL);\n            \n            vint32m1_t count = vmand_mm_b32(cmp, mask);\n            new_freq = vmerge_vvm_i32m1(new_freq, count, cmp_freq);\n            new_result = vmerge_vvm_i32m1(new_result, values, cmp_result);\n        }\n        \n        freq = vmax_vv_i32m1(freq, new_freq);\n        result = vmax_vv_i32m1(result, new_result);\n    }\n    \n    vbool32_t mask_result = vmseq_vx_i32m1_b32(result, -1, VL);\n    int res = vfirst_m_i32(mask_result);\n    \n    return res == VL ? -1 : vslide1down_vx_i32(result, 0, VL);\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```\nint search_simd(std::vector<int> lst){\n    const int size = lst.size();\n    const int lane_size = vsetvl_e32m1();\n    int max_val = -1;\n    \n    for (int i = 0; i < size; i += lane_size) {\n        vint32m1_t vec = vle32_v_i32m1(&lst[i]);\n        vbool32_t mask = vmseq_vx_i32m1_b32(vec, 0);\n        vint32m1_t unique_vals = vcompress_vm_b32(mask, vec);\n        \n        for (int j = 0; j < vfirstb32(mask); j++) {\n            int val = vslide1down_vx_i32m1_i32(unique_vals, j, 0);\n            vint32m1_t freq = vpopc_m_b32(mask, vmseq_vx_i32m1_b32(vec, val));\n            \n            if (val > 0 && val <= freq) {\n                max_val = vmax(max_val, val);\n            }\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint search_simd(std::vector<int> lst){\n    const int VL = vsetvl_e32m1();\n    vint32m1_t vec_lst = vle32_v_i32m1(&lst[0], VL);\n\n    vint32m1_t unique_values = vcpopcnt_v_i32m1(vec_lst, VL);\n    vbool32_t mask = vmsgeu_vx_i32m1_b32(unique_values, vec_lst, VL);\n\n    vint32m1_t result = vmerge_vvm_i32m1(mask, vec_lst, vsub_vx_i32m1(unique_values, 1, VL), VL);\n\n    int max_value = vmax_vx_i32m1(result, 0, VL);\n    if (max_value == 0) {\n        return -1;\n    } else {\n        return max_value;\n    }\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```\nint search_simd(std::vector<int> lst){\n    const int VL = vsetvl_e32m1();\n    vint32m1_t vec = vle32_v_i32m1(&lst[0], VL);\n\n    vint32m1_t unique_values, unique_counts;\n    vcompress_vm_v_i1m1(vmsle_vx_i32m1_b1(vec, 0, VL), &unique_values, vec, VL);\n    vcompress_vm_v_i1m1(vmsle_vx_i32m1_b1(vec, 0, VL), &unique_counts, vpopc_m_b1(vec, VL), VL);\n\n    for (int i = 0; i < VL; i++) {\n        if (vslide1down_vx_i32m1(vfirst_m_i1(vec, VL), 1, VL) == unique_values[i] && unique_counts[i] >= unique_values[i]) {\n            return unique_values[i];\n        }\n        vec = vslide1down_vx_i32m1(vec, 1, VL);\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint search_simd(std::vector<int> lst){\n    const int VL = vsetvl_e32m1();\n    vint32m1_t vec_lst = vle32_v_i32m1(&lst[0], VL);\n\n    vint32m1_t unique_values, unique_counts;\n    vcompress_vm_v_i1m1(vmsle_vx_i32m1(vec_lst, 0, VL), vec_lst);\n\n    vint32m1_t sorted_lst = vslide1down_vx_i32m1(vec_lst, 1, VL);\n    vint32m1_t diff = vsub_vv_i32m1(vec_lst, sorted_lst, VL);\n    vint32m1_t mask = vmsle_vx_i32m1(diff, 0, VL);\n    vint32m1_t masked_lst = vmerge_vxm_i32m1(vec_lst, 0, mask, VL);\n\n    vint32m1_t counts = vcpop_m_i32m1(mask, VL);\n    vcompress_vm_v_i1m1(mask, masked_lst);\n\n    vint32m1_t max_value = -1;\n    for (int i = 0; i < VL; i++) {\n        if (vslide1down_vx_i32m1(counts, i, VL) >= vslide1down_vx_i32m1(masked_lst, i, VL)) {\n            max_value = vmax_vx_i32m1(max_value, vslide1down_vx_i32m1(masked_lst, i, VL), VL);\n        }\n    }\n\n    return vssra_vx_i32m1_i32(max_value, 31, VL);\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int vlen = vsetvl_e32m1();\n    int rem = q.size() % vlen;\n    int rounds = q.size() / vlen;\n    \n    vint32m1_t vq = {0};\n    vint32m1_t vsum = {0};\n    \n    for (int i = 0; i < rounds; i++) {\n        vq = vlse32_v_i32m1(&q[i * vlen], vlen * sizeof(int), vq);\n        vsum = vadd_vv_i32m1(vsum, vq);\n    }\n    \n    if (rem > 0) {\n        vq = vlse32_v_i32m1(&q[rounds * vlen], rem * sizeof(int), vq);\n        vsum = vadd_vv_i32m1(vsum, vq);\n    }\n    \n    int sum = vmv_x_s_i32m1_i32(vredsum_vs_i32m1_i32m1(vsum, vsum));\n    \n    bool balanced = true;\n    for (int i = 0; i < vlen / 2; i++) {\n        if (vslide1down_v_i32m1(vq, i) != vslide1up_v_i32m1(vq, i)) {\n            balanced = false;\n            break;\n        }\n    }\n    \n    return balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int size = q.size();\n    const int lane_size = vsetvl_e32m1(0);\n\n    vint32m1_t vec_q = vle32_v_i32m1(&q[0], size * sizeof(int));\n\n    vint32m1_t vec_sum = vnclip_wx_i32m1(vredsum_vs_i32m1_i32m1(vec_q, vcreate_i32m1(0), lane_size), 31);\n\n    vbool32_t is_balanced = vmsne_vx_i32m1_b32(vec_sum, 0, lane_size);\n\n    vbool32_t is_less_than_w = vmsleu_vx_i32m1_b32(vec_sum, w, lane_size);\n\n    vbool32_t result = vmand_mm_b32(is_balanced, is_less_than_w, lane_size);\n\n    return vmfirst_m_b32(result, lane_size);\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int size = q.size();\n    const int lane_size = vsetvl_e32m1(0);\n\n    int32_t* data = q.data();\n    int32_t sum = 0;\n\n    vint32m1_t v_sum = vmv_v_x_i32m1(0, 0);\n\n    for (int i = 0; i < size; i += lane_size) {\n        const int remaining = size - i;\n        const int current_size = remaining < lane_size ? remaining : lane_size;\n\n        vint32m1_t v_data = vle32_v_i32m1(&data[i], current_size);\n\n        v_sum = vadd_vv_i32m1(v_sum, v_data);\n    }\n\n    sum = vredsum_vs_i32_i32m1_i32(v_sum, sum, v_sum);\n\n    if (sum > w) {\n        return false;\n    }\n\n    for (int i = 0; i < size / 2; i++) {\n        if (q[i] != q[size - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int vlen = vsetvl_e32m1();\n    int rem = q.size() % vlen;\n    \n    vint32m1_t vq = {0};\n    vint32m1_t vsum = {0};\n    \n    for (size_t i = 0; i < q.size() - rem; i += vlen) {\n        vq = vlse32_v_i32m1(&q[i], vlen);\n        vsum = vadd_vv_i32m1(vsum, vq);\n    }\n    \n    int sum = vmv_x_s_i32m1_i32(vredsum_vs_i32m1_i32(vsum, vsum, vlen));\n    \n    if (rem > 0) {\n        vq = vlse32_v_i32m1(&q[q.size() - rem], rem);\n        vsum = vadd_vv_i32m1(vsum, vq);\n        sum += vmv_x_s_i32m1_i32(vredsum_vs_i32m1_i32(vsum, vsum, rem));\n    }\n    \n    bool balanced = true;\n    for (size_t i = 0; i < q.size() / 2; i++) {\n        if (q[i] != q[q.size() - i - 1]) {\n            balanced = false;\n            break;\n        }\n    }\n    \n    return balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int vlen = vsetvli(0, RVV_EEW, RVV_M1);\n    int vl = vsetvli(0, RVV_EEW, RVV_M1);\n    \n    vint32m1_t vec_q = vle32_v_i32m1(&q[0], vl);\n    \n    vbool32_t mask = vmseq_vx_i32m1_b32(vec_q, 0, vl);\n    int sum = vredsum_vs_i32m1_i32(vec_q, 0, mask, vl);\n    \n    bool is_balanced = true;\n    for (int i = 0; i < q.size() / 2; i++) {\n        if (q[i] != q[q.size() - 1 - i]) {\n            is_balanced = false;\n            break;\n        }\n    }\n    \n    return is_balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint smallest_change_simd(std::vector<int> arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1(0);\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    int changes = 0;\n    for (int i = 0; i < size / 2; i += lane_size) {\n        vint32m1_t left = vle32_v_i32m1(&arr[i]);\n        vint32m1_t right = vle32_v_i32m1(&arr[size - 1 - i]);\n        vbool32_t mask = vmsne_vx_i32m1_b32(left, right);\n\n        changes += vpopc_m_b32(mask, 0);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint smallest_change_simd(std::vector<int> arr){\n    const int vlen = vsetvl_e32m1();\n    const int size = arr.size();\n    const int rem = size % vlen;\n    const int full_vlens = size - rem;\n\n    vint32m1_t va, vb, mask, result;\n    va = vle32_v_i32m1(&arr[0], vlen);\n\n    int changes = 0;\n    for (int i = 0; i < full_vlens; i += vlen) {\n        vb = vle32_v_i32m1(&arr[i], vlen);\n        mask = vmsne_vx_i32m1_b(vb, va, vlen);\n        changes += vpopc_m_b(mask, vlen);\n        va = vslide1down_vx_i32m1(va, 1, vlen);\n    }\n\n    if (rem > 0) {\n        vint32m1_t mask_rem = vmseq_vx_i32m1_b(vl, 0, rem);\n        vb = vle32_v_i32m1(&arr[full_vlens], rem);\n        mask = vmsne_vx_i32m1_b(vb, va, rem);\n        changes += vpopc_m_b(mask & mask_rem, rem);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint smallest_change_simd(std::vector<int> arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1();\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    int changes = 0;\n    for (int i = 0; i < size / 2; i += lane_size) {\n        vint32m1_t rev_vec = vrgather_vx_i32m1(vec, &arr[size - i - lane_size], 0);\n        vbool32_t mask = vmsne_vx_i32m1_b32(vec, rev_vec, 0);\n\n        changes += vpopc_m_b32(mask, 0);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint smallest_change_simd(std::vector<int> arr){\n    const int vlen = vsetvl_e32m1();\n    const int size = arr.size();\n    int changes = 0;\n    \n    vint32m1_t va = vle32_v_i32m1(&arr[0], vlen);\n    vint32m1_t vb = vslide1down_vx_i32m1(va, 1, vlen);\n    vbool32_t mask = vmsne_vx_i32m1_b32(va, vb, vlen);\n    \n    changes += vpopc_m_b32(mask, vlen);\n    \n    for (int i = 2; i <= size / 2; i++) {\n        va = vle32_v_i32m1(&arr[0], vlen);\n        vb = vslide1down_vx_i32m1(va, i, vlen);\n        mask = vmsne_vx_i32m1_b32(va, vb, vlen);\n        changes += vpopc_m_b32(mask, vlen);\n    }\n    \n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint smallest_change_simd(std::vector<int> arr){\n    const int size = arr.size();\n    const int vl = vsetvl_e32m1();\n\n    vint32m1_t vec = vle32_v_i32m1(&arr[0], vl);\n\n    int changes = 0;\n    for (int i = 0; i < size / 2; i++) {\n        vint32m1_t left = vslide1down_vx_i32m1(vec, i, vl);\n        vint32m1_t right = vslide1up_vx_i32m1(vec, i, vl);\n        vbool32_t cmp = vmsne_vx_i32m1_b32(left, right, vl);\n        changes += vpopc_m_b32(cmp, vl);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(0, lst1.size()), lst1.size());\n    vint32m1_t len1 = vfredsum_vs_i32m1_i32m1(mask, vpopc_m_b32(mask, vmsne_vv_i32m1_b32(vmv_v_x_i32m1(0, lst1.size()), 0, lst1.size())), 0);\n    mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(0, lst2.size()), lst2.size());\n    vint32m1_t len2 = vfredsum_vs_i32m1_i32m1(mask, vpopc_m_b32(mask, vmsne_vv_i32m1_b32(vmv_v_x_i32m1(0, lst2.size()), 0, lst2.size())), 0);\n    \n    return len1 < len2 ? lst1 : lst2;\n}\n```  "}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    vint8m1_t mask = vmseq_vx_i8m1_bv_i8m1(vmv_v_x_i8m1(vmv_x_s_i8m1(lst1.size())), 0);\n    vint8m1_t count1 = vzero_v_i8m1();\n    vint8m1_t count2 = vzero_v_i8m1();\n\n    for (size_t i = 0; i < lst1.size(); i += VL) {\n        vint8m1_t str1 = vle8_v_i8m1(&lst1[i]);\n        vint8m1_t str2 = vle8_v_i8m1(&lst2[i]);\n        count1 = vwaddu_vv_i8m1(count1, vmseq_vv_i8m1_bv_i8m1(str1, 0));\n        count2 = vwaddu_vv_i8m1(count2, vmseq_vv_i8m1_bv_i8m1(str2, 0));\n    }\n\n    vint8m1_t diff = vsub_vv_i8m1(count1, count2);\n    vint8m1_t mask_gt = vmsltu_vv_i8m1_bv_i8m1(count1, count2);\n    vint8m1_t mask_eq = vmand_mm_b_i8m1(mask, vmsne_vv_i8m1_bv_i8m1(count1, count2));\n    vint8m1_t result_mask = vor_mm_b_i8m1(mask_gt, mask_eq);\n\n    std::vector<std::string> result;\n    for (size_t i = 0; i < lst1.size(); i += VL) {\n        vint8m1_t str1 = vle8_v_i8m1(&lst1[i]);\n        vint8m1_t str2 = vle8_v_i8m1(&lst2[i]);\n        vbool1_t pred = vmsleu_vv_i8m1_b(str1, str2);\n        vint8m1_t selected = vsel_vv_i8m1(pred, str1, str2);\n        vbool1_t mask_result = vmand_mm_b_i8m1(result_mask, pred);\n        vse8_v_i8m1(&result, selected, mask_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    vint32m4_t total_chars1 = vmv_v_x_i32m4(0, 0);\n    vint32m4_t total_chars2 = vmv_v_x_i32m4(0, 0);\n\n    for (size_t i = 0; i < lst1.size(); i += VL) {\n        vint32m4_t str_len1 = vslide1down_vx_i32m4(vmv_v_x_i32m4(0, 0), 0);\n        vint32m4_t str_len2 = vslide1down_vx_i32m4(vmv_v_x_i32m4(0, 0), 0);\n\n        vbool8_t mask1 = vmseq_vx_i32m4_b8(vmv_v_x_i32m4(0, 0), 0);\n        vbool8_t mask2 = vmseq_vx_i32m4_b8(vmv_v_x_i32m4(0, 0), 0);\n\n        for (size_t j = 0; j < VL; j++) {\n            if (i + j < lst1.size()) {\n                vint32m4_t str_len = vmv_x_s_i32m4(lst1[i + j].size());\n                str_len1 = vmerge_vvm_i32m4(str_len1, str_len, mask1);\n            }\n            if (i + j < lst2.size()) {\n                vint32m4_t str_len = vmv_x_s_i32m4(lst2[i + j].size());\n                str_len2 = vmerge_vvm_i32m4(str_len2, str_len, mask2);\n            }\n        }\n\n        total_chars1 = vadd_vv_i32m4(total_chars1, str_len1);\n        total_chars2 = vadd_vv_i32m4(total_chars2, str_len2);\n    }\n\n    vbool32_t cmp = vmsltu_vv_i32m4_b32(total_chars1, total_chars2);\n    vbool32_t eq = vmeq_vv_i32m4_b32(total_chars1, total_chars2);\n\n    std::vector<std::string> result;\n    for (size_t i = 0; i < VL; i++) {\n        if (vbool32_t(cmp) & (1 << i)) {\n            result.push_back(lst1[i]);\n        } else if (vbool32_t(eq) & (1 << i)) {\n            result.push_back(lst1[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    vint32m4_t total_chars1 = vmv_v_x_i32m4(0, 0);\n    vint32m4_t total_chars2 = vmv_v_x_i32m4(0, 0);\n\n    for (size_t i = 0; i < lst1.size(); i += VL) {\n        vint32m4_t str_lens1 = vfredsum_vs_i32m4_i32m4(total_chars1, vle32_v_i32m4(vwmul_vx_i32m4(vmv_v_x_i32m4(0, 0), 1), (int32_t*)lst1.data() + i), total_chars1);\n        vint32m4_t str_lens2 = vfredsum_vs_i32m4_i32m4(total_chars2, vle32_v_i32m4(vwmul_vx_i32m4(vmv_v_x_i32m4(0, 0), 1), (int32_t*)lst2.data() + i), total_chars2);\n    }\n\n    vbool8_t mask = vmsltu_vx_i8m1_b8(vmv_v_x_i8m1(0, 0), total_chars1, total_chars2);\n    std::vector<std::string> result;\n    for (size_t i = 0; i < VL; i++) {\n        if (mask[i]) {\n            result.push_back(lst1[i]);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(0, lst1.size()), lst1.size());\n    vint32m1_t len1 = vfredsum_vs_i32m1_i32m1(mask, vpopc_v_vm_b32(vmsltu_vv_i32m1_b32(vmv_x_s_i32m1(lst1.data()), vmv_x_s_i32m1(0), lst1.size())), 0);\n    vint32m1_t len2 = vfredsum_vs_i32m1_i32m1(mask, vpopc_v_vm_b32(vmsltu_vv_i32m1_b32(vmv_x_s_i32m1(lst2.data()), vmv_x_s_i32m1(0), lst2.size())), 0);\n    return len1 < len2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    vuint64m1_t vx = vmv_s_x_u_i64(x, 0);\n    vuint64m1_t vn = vmv_s_x_u_i64(n, 0);\n    \n    vbool64_t mask = vmsne_vx_u64m1_b64(vx, 0);\n    \n    vbool64_t result = vmand_mm_b64(mask, vmsleu_vx_u64m1_b64(vx, vn));\n    \n    return vmfirst_m_b64(result);\n}\n```  "}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    vuint64m1_t vx = vmv_x_u_v_u64m1(x);\n    vuint64m1_t vn = vmv_x_u_v_u64m1(n);\n    \n    vbool64m1_t result = vmsne_vx_u_b64m1(vx, 0);\n    vbool64m1_t is_power = vmand_mm_b64m1(vmsne_vx_u_b64m1(vx, 1), vmsleu_vx_u_b64m1(vx, vn));\n    \n    return vmfirst_m_b64(result) && vmfirst_m_b64(is_power);\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    vuint64m1_t vx = vmv_s_x_u_i64(x, 0);\n    vuint64m1_t vn = vmv_s_x_u_i64(n, 0);\n    \n    vbool64_t result = vmsne_vx_u64m1_b64(vx, 0);\n    vbool64_t mask = vmsleu_vx_u64m1_b64(vx, 1);\n    \n    vint64m1_t exp = vdivu_vv_u64m1(vx, vn);\n    vbool64_t is_simple_power = vmand_mm_b64(mask, vmsleu_vx_u64m1_b64(vmul_vv_u64m1(exp, vn), vx));\n    \n    return vmfirst_m_b64(is_simple_power);\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    vuint64m1_t vx = vmv_s_x_u_i64(x, 0);\n    vuint64m1_t vn = vmv_s_x_u_i64(n, 0);\n    \n    vbool64_t result = vmsne_vx_u64m1_b64(vx, 0);\n    vbool64_t mask = vmand_mm_b64(result, vmsleu_vx_u64m1_b64(vx, 1));\n    \n    vuint64m1_t vres = vdivu_vv_u64m1(vx, vn);\n    vbool64_t is_power = vmand_mm_b64(vmsne_vv_u64m1_b64(vres, 0), vmeq_vv_u64m1_b64(vmul_vv_u64m1(vres, vn), vx));\n    \n    vbool64_t final_result = vmand_mm_b64(mask, is_power);\n    \n    return vmfirst_m_b64(final_result);\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    vuint64m1_t vx = vmv_v_x_u64m1(x);\n    vuint64m1_t vn = vmv_v_x_u64m1(n);\n    \n    vbool64m1_t result = vmsne_vv_u64m1_b64(vx, vn);\n    result = vmand_mm_b64(result, vmsne_vx_u64m1_b64(vx, 1));\n    \n    return !vmand_mm_b64(result, vmand_mm_b64(vmsne_vx_u64m1_b64(vn, 1), vmsne_vx_u64m1_b64(vn, 0)));\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint hex_key_simd(const std::string & num){\n    const int8x16_t primes = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53};\n    const int8x16_t hex_digits = {48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70};\n    int count = 0;\n    \n    for (size_t i = 0; i < num.size(); i += 16) {\n        int8x16_t input = *(int8x16_t*)(num.c_str() + i);\n        uint8x16_t is_hex_digit = vmseq_vx_i8(input, hex_digits);\n        uint8x16_t is_prime = vmand_vv_u8(is_hex_digit, vmseq_vx_i8(input, primes));\n        count += vpopc_m(is_prime);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint hex_key_simd(const std::string & num){\n    const int8x8_t primes = {2, 3, 5, 7, 11, 13, 17, 19};\n    const int8x8_t hex_values = {10, 11, 12, 13, 14, 15, 16, 17};\n    int count = 0;\n    \n    for (size_t i = 0; i < num.size(); i += 8) {\n        int8x8_t input = *(int8x8_t*)(num.data() + i);\n        int8x8_t cmp_primes = vand_vx_i8(input, 0x1F);\n        int8x8_t cmp_hex_values = vsub_vv_i8(cmp_primes, hex_values);\n        int8x8_t cmp_result = vcpopcnt_vv_i8(cmp_hex_values);\n        count += vfirst_m_i8(cmp_result);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint hex_key_simd(const std::string & num){\n    const int8x8_t primes = {2, 3, 5, 7, 11, 13, 17, 0}; // 17 is just a placeholder\n    const int8x8_t hex_values = {10, 11, 12, 13, 14, 15, 16, 17}; // A=10, B=11, C=12, D=13, E=14, F=15, 17 is just a placeholder\n    int count = 0;\n    \n    for (size_t i = 0; i < num.size(); i += 8) {\n        int8x8_t input = *(int8x8_t*)(num.c_str() + i);\n        uint8x8_t mask = vmsleu_vx_i8(input, 10); // Mask for digits 0-9\n        int8x8_t masked_input = vmul_vx_i8(input, mask);\n        int8x8_t primes_mask = vmsgtu_vv_i8(masked_input, primes);\n        int8x8_t hex_mask = vmsleu_vv_i8(masked_input, hex_values);\n        int8x8_t final_mask = vand_vv_i8(primes_mask, hex_mask);\n        count += vpopc_m(mask, final_mask);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint hex_key_simd(const std::string & num){\n    const int8x8_t primes = {2, 3, 5, 7, 11, 13, 17, 0};\n    const int8x8_t hex_values = {10, 11, 12, 13, 14, 15, 16, 0};\n    int count = 0;\n    \n    for (size_t i = 0; i < num.size(); i += 8) {\n        int8x8_t input = vmv_b_e8(num.data() + i);\n        int8x8_t is_prime = vand_vv_i8(input == primes, input != 0);\n        int8x8_t is_hex = vand_vv_i8(input >= 10, input <= 15);\n        int8x8_t is_valid = vand_vv_i8(is_prime, is_hex);\n        count += vmsumub_vx_i8(is_valid);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint hex_key_simd(const std::string & num){\n    const int8x16_t primes = {2, 3, 5, 7, 11, 13, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    const int8x16_t hex_digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F'};\n    \n    int count = 0;\n    for (size_t i = 0; i < num.size(); i += 16) {\n        int8x16_t input = *(int8x16_t*)(num.c_str() + i);\n        uint8x16_t mask = vmsgtu_vx_i8(input, hex_digits);\n        int8x16_t selected = vmerge_vvm_i8(mask, primes, input);\n        count += vmsbf_m1(selected);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```\nbool is_happy_simd(const std::string & s){\n    if(s.size() < 3) return false;\n    \n    const int n = s.size();\n    const int simd_width = vsetvlmax_e32m1();\n    \n    for(int i = 0; i <= n - 3; i += simd_width){\n        vuint8m1_t v1 = vle8_v_u8m1(&s[i], simd_width);\n        vuint8m1_t v2 = vle8_v_u8m1(&s[i+1], simd_width);\n        vuint8m1_t v3 = vle8_v_u8m1(&s[i+2], simd_width);\n        \n        vbool8_t mask1 = vmsne_vv_u8m1_b8(v1, v2, simd_width);\n        vbool8_t mask2 = vmsne_vv_u8m1_b8(v1, v3, simd_width);\n        vbool8_t mask3 = vmsne_vv_u8m1_b8(v2, v3, simd_width);\n        \n        vbool8_t result_mask = mand3(mask1, mask2, mask3, simd_width);\n        \n        if(!vmfirst(result_mask)) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```\nbool is_happy_simd(const std::string & s){\n    if(s.size() < 3) return false;\n    \n    const int n = s.size();\n    const int simd_width = vsetvl_e32m1(0);\n    \n    for(int i = 0; i <= n - 3; i += simd_width){\n        vuint8m1_t v1 = vlsext_v_u8m1_b8(&s[i], simd_width);\n        vuint8m1_t v2 = vlsext_v_u8m1_b8(&s[i + 1], simd_width);\n        vuint8m1_t v3 = vlsext_v_u8m1_b8(&s[i + 2], simd_width);\n        \n        vbool8_t distinct = vmseq_vx_u8m1_b8(v1, v2) & vmseq_vx_u8m1_b8(v2, v3) & vmseq_vx_u8m1_b8(v1, v3);\n        \n        if(!vmand_mm_b8(vmand_mm_b8(distinct, vmsne_vx_u8m1_b8(v1, 0)), vmsne_vx_u8m1_b8(v2, 0))) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```\nbool is_happy_simd(const std::string & s){\n    if(s.size() < 3){\n        return false;\n    }\n\n    const int n = s.size();\n    const int simd_width = vsetvlmax_e32m1();\n\n    for(int i = 0; i <= n - 3; i += simd_width){\n        vuint8m1_t v1 = vle8_v_u8m1(&s[i], simd_width);\n\n        vbool8_t mask = vmseq_vx_u8m1_b8(v1, s[i], simd_width);\n        vbool8_t mask2 = vmseq_vx_u8m1_b8(v1, s[i + 1], simd_width);\n        vbool8_t mask3 = vmseq_vx_u8m1_b8(v1, s[i + 2], simd_width);\n\n        vbool8_t final_mask = vmand_mm_b8(mask, mask2);\n        final_mask = vmand_mm_b8(final_mask, mask3);\n\n        if(vmsum_vx_i8m1_b8(final_mask, simd_width) != 0){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```\nbool is_happy_simd(const std::string & s){\n    if(s.size() < 3) return false;\n    \n    const int n = s.size();\n    const int simd_width = vlenb() / 8;\n    \n    for(int i = 0; i <= n - 3; i += simd_width){\n        vuint8m1_t v1 = vlh_v_u8m1(&s[i], simd_width);\n        vuint8m1_t v2 = vlh_v_u8m1(&s[i + 1], simd_width);\n        vuint8m1_t v3 = vlh_v_u8m1(&s[i + 2], simd_width);\n        \n        vbool8_t distinct = vmand_mm_b8(vmv_v_x_u_i8m1(v1) != vmv_v_x_u_i8m1(v2), vmv_v_x_u_i8m1(v2) != vmv_v_x_u_i8m1(v3));\n        \n        if(!vmand_mm_b8(vmsne_v_b8(distinct, vbool8_t(true), vbool8_t(false))){\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```\nbool is_happy_simd(const std::string & s){\n    if(s.size() < 3) return false;\n    \n    const int n = s.size();\n    const int simd_width = vlenb() / 8;\n    \n    for(int i = 0; i <= n - 3; i += simd_width){\n        vuint8m1_t v1 = vlh_v_u8m1(&s[i], simd_width);\n        vuint8m1_t v2 = vlh_v_u8m1(&s[i + 1], simd_width);\n        vuint8m1_t v3 = vlh_v_u8m1(&s[i + 2], simd_width);\n        \n        vbool8_t distinct = vmseq_vx_u8m1_b8(v1, v2, 0) & vmseq_vx_u8m1_b8(v2, v3, 0) & vmseq_vx_u8m1_b8(v1, v3, 0);\n        \n        if(!vmfirst_m_b8(distinct)) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if(length <= 1){\n        return false;\n    }\n    \n    vint8_t mask = vmseq_v_i8(length, 0);\n    mask = vmseq_v_i8(length, 1) & mask;\n    for(int i=2; i*i<=length; i++){\n        if(length % i == 0){\n            mask = vmseq_v_i8(length, 0) & mask;\n            break;\n        }\n    }\n    \n    int result = vmfirst_m(mask);\n    return result == 0;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if(length <= 1){\n        return false;\n    }\n    \n    vint8_t mask = vmseq_v_i8(length, 2);\n    for(int i = 2; i*i <= length; i++){\n        mask = vmand_mm_i8(mask, vmseq_v_i8(length, i));\n    }\n    \n    return vmand_mm_i8(mask, vmseq_v_i8(length, length)) == 1;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length <= 1) {\n        return false;\n    }\n    \n    vint8_t mask = vmseq_v_i8(length, 0);\n    vint8_t divisors = vmsne_v_i8(vmv_v_x_i8(length), 1);\n    for (int i = 2; i * i <= length; i++) {\n        if (length % i == 0) {\n            vint8_t divisor_mask = vmseq_v_i8(length, i);\n            divisors = vmandnot_vv_i8(divisors, divisor_mask);\n            divisors = vmandnot_vv_i8(divisors, vmseq_v_i8(length, length / i));\n        }\n    }\n    \n    vint8_t prime_mask = vmand_vv_i8(mask, divisors);\n    int result = vmandreduce_vs_i8(prime_mask);\n    \n    return result == 1;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length <= 1) {\n        return false;\n    }\n    \n    vint8_t mask = vmseq_v_i8(length, 0);\n    mask = vmand_vx_i8(mask, 1);\n    mask = vmand_vx_i8(mask, 2);\n    \n    int result = vmandreduce_max_i8(mask);\n    \n    return (result == 1);\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length <= 1) {\n        return false;\n    }\n    \n    vint8_t mask = vmseq_v_i8(length, 0);\n    mask = vmseq_v_i8(length, 1) & mask;\n    \n    for (int i = 2; i * i <= length; i++) {\n        if (length % i == 0) {\n            mask = vmseq_v_i8(length, 0);\n            break;\n        }\n    }\n    \n    return vmand_mm_b(mask);\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```\nstd::string solve_simd(uint64_t N){\n    vint64m1_t vec_N = vmv_s_x_i64(N, VL1);\n    vint64m1_t vec_ones = vmv_s_i_i64(1, VL1);\n    vint64m1_t vec_zeros = vmv_s_i_i64(0, VL1);\n    vint64m1_t vec_sum = vec_zeros;\n\n    while(vmv_x_s_i64(vec_N, VL1) > 0){\n        vint64m1_t digit = vremu_vx_i64(vec_N, 2, VL1);\n        vec_sum = vadd_vv_i64(vec_sum, digit, VL1);\n        vec_N = vdivu_vx_i64(vec_N, 2, VL1);\n    }\n\n    vint64m1_t result = vcompress_vm_i1(vec_sum, VL1);\n    int64_t res = vmv_x_s_i64(result, VL1);\n    std::string binary_str = std::bitset<64>(res).to_string();\n    return binary_str.substr(binary_str.find('1'));\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```\nstd::string solve_simd(uint64_t N){\n    vint64m1_t vec_N = vmv_s_x_i64(N, VL1);\n    vint64m1_t vec_digits = vpopc_m_b1_i64(vec_N, VL1);\n    int sum = vslide1down_vx_i64m1_i64(vec_digits, vec_digits, 32, VL1);\n    std::string result = std::bitset<64>(sum).to_string();\n    return result.substr(result.find('1'));\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```\nstd::string solve_simd(uint64_t N){\n    vint64m1_t v_N = vmv_s_x_i64(N, VL1);\n    vint64m1_t v_sum = vmv_s_i_i64(0, VL1);\n    \n    vint64m1_t v_ones = vmv_s_i_i64(1, VL1);\n    vint64m1_t v_zeros = vmv_s_i_i64(0, VL1);\n    \n    vint64m1_t v_mask = vmsne_vx_i64m1_b(v_N, 0, VL1);\n    \n    while(vmask){\n        vint64m1_t v_digit = vredsum_vs_i64m1_i64(v_mask, v_N, v_zeros, VL1);\n        v_sum = vadd_vv_i64m1(v_sum, v_digit, VL1);\n        \n        v_N = vdivu_vx_i64m1(v_N, 10, VL1);\n        v_mask = vmsne_vx_i64m1_b(v_N, 0, VL1);\n    }\n    \n    vint64m1_t v_result = vcompress_vm_i64m1(v_sum, v_mask, VL1);\n    \n    int64_t result = vslide1down_vx_i64(v_result, 0, VL1);\n    \n    std::string binary_result = std::bitset<64>(result).to_string();\n    \n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string solve_simd(uint64_t N){\n    vint64m1_t vec_N = vmv_s_x_i64m1(N);\n    vint64m1_t sum = vzero_v_i64m1();\n    \n    while(vmv_x_s_i64m1(vec_N) > 0){\n        vint64m1_t digit = vremu_vx_i64m1(vec_N, 2);\n        sum = vadd_vv_i64m1(sum, digit);\n        vec_N = vdivu_vx_i64m1(vec_N, 2);\n    }\n    \n    vint64m1_t result = vcompress_vm_i64m1(vmsne_vx_i64m1(sum, 0));\n    vint64m1_t reversed_result = vslideup_vx_i64m1(result, 1, result);\n    \n    std::string binary_str;\n    for(int i=0; i<64; i++){\n        binary_str += std::to_string(vmfeq_vx_i64m1(reversed_result, i));\n    }\n    \n    return binary_str;\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string solve_simd(uint64_t N){\n    vint64m1_t v_N = vmv_s_x_i64(N);\n    vint64m1_t v_sum = vmv_s_i_i64(0);\n    vint64m1_t v_ones = vmv_s_i_i64(1);\n    vint64m1_t v_zeros = vmv_s_i_i64(0);\n    \n    while(v_N){\n        vint64m1_t v_digit = vremu_vx_i64m1(v_N, 10);\n        v_sum = vadd_vv_i64m1(v_sum, v_digit);\n        v_N = vdivu_vx_i64m1(v_N, 10);\n    }\n    \n    vint64m1_t v_result = vcompress_vm_i64m1(v_sum);\n    vbool64_t mask = vmsne_vx_i64m1_b64(v_result, 0);\n    vint64m1_t v_final_result = vmerge_vvm_i64m1(mask, v_ones, v_zeros);\n    \n    int64_t result_scalar = vslide1down_vx_i64(v_final_result, 0);\n    return std::bitset<64>(result_scalar).to_string();\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_simd(std::vector<int> lst){\n    vint32m4_t vec = vle32_v_i32m4(&lst[0]);\n    vint32m4_t mask = vmseq_vx_i32m4(vec, 0);\n    vint32m4_t odd_indices = vmerge_vvm_i32m4(mask, vec, 0);\n    vint32m4_t even_elements = vslide1down_vx_i32m4(odd_indices, 0);\n    vint32m4_t result = vadd_vv_i32m4(even_elements, odd_indices);\n    int sum = vslide1up_vx_i32m4(result, 0)[0];\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_simd(std::vector<int> lst){\n    vint32m1_t vec = vle32_v_i32m1(&lst[1]); // Load vector starting from index 1\n    vint32m1_t mask = vmseq_vx_i32m1_b1(vec, 0); // Create a mask for even elements\n    vint32m1_t even_elements = vmerge_vvm_i32m1(mask, vec, 0); // Set odd elements to 0\n    vint32m1_t sum = vredsum_vs_i32m1_i32m1(vadd_vv_i32m1(even_elements, even_elements), 0); // Sum even elements\n    int result;\n    vse32_v_i32m1(&result, sum); // Store the result\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_simd(std::vector<int> lst){\n    vint32m1_t vec = vle32_v_i32m1(&lst[1]);\n    vint32m1_t mask = vmseq_vx_i32m1_b1(vec, 0);\n    vint32m1_t result = vmerge_vvm_i32m1(mask, vcreate_i32m1(0), vec);\n    return vslide1down_vx_i32m1(result, 0);\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_simd(std::vector<int> lst){\n    vint32m1_t vec = vle32_v_i32m1(&lst[1]); // Load elements at odd indices\n    vint32m1_t even_mask = vmseq_vx_i32m1_b1(vec, 0); // Create a mask for even elements\n    vint32m1_t even_elements = vmerge_vvm_i32m1(even_mask, 0, vec); // Extract even elements\n    vint32m1_t sum = vredsum_vs_i32m1_i32m1(even_elements); // Calculate the sum of even elements\n    int result;\n    vse32_v_i32m1(&result, sum); // Store the result\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    vint32m1_t vec = vle32_v_i32m1(&lst[1]); // Load vector starting from index 1\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vec, 0); // Create a mask for even elements\n    vint32m1_t even_elements = vmerge_vxm_i32m1(mask, vec, 0); // Extract even elements\n    vint32m1_t sum = vredsum_vs_i32m1_i32m1(even_elements); // Calculate the sum of even elements\n    return vslide1down_vx_i32m1(sum, 0); // Return the sum at odd indices\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```\nstd::string encrypt_simd(const std::string & s){\n    const int N = s.size();\n    const int VL = vsetvl_e32m1(0);\n\n    // Define the rotation amount\n    const int rotate_amount = 2 * 2;\n\n    // Initialize the result string\n    std::string result(N, ' ');\n\n    // Load input string into RVV vectors\n    vuint8m1_t input_v = vle8_v_u8m1((const uint8_t*)s.data(), VL);\n\n    // Define the rotation mask\n    vuint8m1_t rotate_mask = vmv_v_x_u8m1(rotate_amount, VL);\n\n    // Perform the rotation\n    vuint8m1_t rotated_v = vsll_vv_u8m1(input_v, rotate_mask, VL);\n\n    // Store the result back to a string\n    vse8_v_u8m1((uint8_t*)&result[0], rotated_v, VL);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encrypt_simd(const std::string & s){\n    const int N = s.size();\n    const int M = (N + 7) / 8;\n    \n    vint8_t shift = vmv_s_x(2);\n    vint8_t ascii_a = vmv_s_x('a');\n    vint8_t ascii_z = vmv_s_x('z');\n    vint8_t ascii_A = vmv_s_x('A');\n    vint8_t ascii_Z = vmv_s_x('Z');\n    \n    std::string result(N, ' ');\n    \n    for (int i = 0; i < M; i++) {\n        vint8_t input = vrgather_vx_u8(&s[i * 8], vset_s_x(1));\n        \n        vbool8_t is_lower = vand_vv_b8(vclt_vv_u8(input, ascii_a), vcgt_vv_u8(input, ascii_z));\n        vbool8_t is_upper = vand_vv_b8(vclt_vv_u8(input, ascii_A), vcgt_vv_u8(input, ascii_Z));\n        \n        vint8_t rotated_lower = vadd_vv_i8(vremu_vx_i8(vadd_vx_i8(input, shift), 26), ascii_a);\n        vint8_t rotated_upper = vadd_vv_i8(vremu_vx_i8(vadd_vx_i8(input, shift), 26), ascii_A);\n        \n        vint8_t result_char = vsel_vv_i8(is_lower, rotated_lower, input);\n        result_char = vsel_vv_i8(is_upper, rotated_upper, result_char);\n        \n        vscatter_vx_u8(&result[i * 8], vset_s_x(1), result_char);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encrypt_simd(const std::string & s){\n    const int N = s.size();\n    const int M = (N + 7) / 8;\n    \n    vint8_t shift = vmv_s_x(2);\n    vint8_t mask = vmv_s_x(0x1F);\n    \n    vint8_t result = vzero_8();\n    for (int i = 0; i < M; i++) {\n        vint8_t input = vrgather_vx_u8(&s[i * 8], 0);\n        vint8_t shifted = vand_8(vsll_8(input, shift), mask);\n        result = vor_8(result, shifted);\n    }\n    \n    char encrypted[N];\n    vse8_v_i8m1(encrypted, result);\n    \n    return std::string(encrypted, N);\n}\n```  "}
{"task_id": "SimdBench_105_RVV", "completion": "```\nstd::string encrypt_simd(const std::string & s){\n    const int N = s.size();\n    const int VL = vsetvl_e32m1(0);\n\n    // Define the rotation amount\n    const int shift = 2 * 2;\n\n    // Initialize result string\n    std::string result(N, ' ');\n\n    // Load input string into RVV vector\n    vuint8m1_t input = vle8_v_u8m1((const uint8_t*)s.data(), VL);\n\n    // Define the rotation mask\n    vuint8m1_t rot_mask = vmv_v_x_u8m1(shift, VL);\n\n    // Rotate the alphabet\n    vuint8m1_t rotated = vrgather_vx_u8m1(input, rot_mask, VL);\n\n    // Store the result back to the string\n    vse8_v_u8m1((uint8_t*)result.data(), rotated, VL);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```\nstd::string encrypt_simd(const std::string & s){\n    const int N = s.size();\n    const int M = (N + 7) / 8;\n    \n    vint8_t shift = vmv_s_x(2);\n    vint8_t mask = vmv_s_x(31);\n    \n    vint8_t result = vzero_8();\n    \n    for (int i = 0; i < M; i++) {\n        vint8_t input = vrgather_vx_i8(&s[i * 8], mask);\n        vint8_t shifted = vadd_vx_i8(input, shift);\n        vint8_t rotated = vremu_vx_i8(shifted, mask);\n        vscatter_vx_i8(&result[i * 8], rotated, mask);\n    }\n    \n    std::string encrypted(N, ' ');\n    vse8_v_i8m1(&encrypted[0], result, mask);\n    \n    return encrypted;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encode_simd(const std::string & message){\n    const int8x64_t vowels = vmv_v_x_i8('A' == 'A' || 'E' == 'E' || 'I' == 'I' || 'O' == 'O' || 'U' == 'U');\n    const int8x64_t case_mask = vmv_v_x_i8(0x20);\n    const int8x64_t vowel_offset = vmv_v_x_i8(2);\n\n    int length = message.length();\n    int remaining = length % 64;\n    int i = 0;\n\n    for (; i < length - remaining; i += 64) {\n        int8x64_t input = vlseg8_v_i8(&message[i]);\n        int8x64_t is_upper = vmand_vx_i8(input, vmv_v_x_i8(0x20));\n        int8x64_t is_lower = vmseq_vx_i8(input, vmv_v_x_i8(0x20));\n        int8x64_t is_letter = vor_vv_i8(is_upper, is_lower);\n        int8x64_t is_vowel = vand_vv_i8(is_letter, vowels);\n        int8x64_t vowel_index = vsub_vv_i8(is_vowel, vmv_v_x_i8('A'));\n        int8x64_t new_vowel = vadd_vv_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(vvadd_vx_i8(v"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encode_simd(const std::string & message){\n    const int8x8_t vowels = vmv_v_x_i8(0, 0x0AEAEAEAEAEAEAEA); // Vowels in ASCII: A=65, E=69, I=73, O=79, U=85\n    const int8x8_t case_mask = vmv_v_x_i8(0, 0x2020202020202020); // Mask to toggle case\n\n    std::string result = message;\n    int size = message.size();\n    for (int i = 0; i < size; i += 8) {\n        int8x8_t input = vlseg8_v_i8(&message[i]);\n        \n        // Swap case\n        int8x8_t swapped_case = vxor_vv_i8(input, case_mask);\n        \n        // Replace vowels\n        int8x8_t is_vowel = vand_vv_i8(vseq_v_i8(input), vowels);\n        int8x8_t replaced_vowels = vadd_vx_i8(is_vowel, 2);\n        int8x8_t result_vowels = vmerge_vvm_i8(replaced_vowels, input, is_vowel);\n        \n        int8x8_t final_result = vmerge_vvm_i8(swapped_case, result_vowels, is_vowel);\n        \n        vstseg8_v_i8(&result[i], final_result);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encode_simd(const std::string & message){\n    const int8x64_t vowels = vmv_v_i8('A' == 'A' || 'E' == 'E' || 'I' == 'I' || 'O' == 'O' || 'U' == 'U');\n    const int8x64_t vowel_replacement = vmv_v_i8('C' == 'C' || 'G' == 'G' || 'K' == 'K' || 'P' == 'P' || 'T' == 'T');\n\n    int8x64_t input = vrgather_vx_i8(vle8_v_i8(message.data()), vcreate_i64(0, 1, 2, 3, 4, 5, 6, 7));\n    \n    int8x64_t upper_case_mask = vmsle_vx_i8(input, 'a');\n    int8x64_t lower_case_mask = vmand_vv_i8(vmsle_vx_i8(input, 'A'), vmseq_vx_i8(input, 'Z'));\n    \n    int8x64_t swapped_case = vmerge_vvm_i8(upper_case_mask, vsub_vv_i8(input, 'a' - 'A'), lower_case_mask, vadd_vv_i8(input, 'a' - 'A'));\n    \n    int8x64_t vowels_mask = vmand_vv_i8(vgt_vv_i8(input, 'A' - 1), vlt_vv_i8(input, 'Z' + 1));\n    int8x64_t vowels_index = vsub_vv_i8(input, 'A');\n    int8x64_t vowels_replaced = vrgather_vv_i8(vowel_replacement, vowels_index);\n    \n    int8x64_t result = vmerge_vvm_i8(vowels_mask, vowels_replaced, vnot_v_i8(vowels_mask), swapped_case);\n    \n    char output[64];\n    vse8_v_i8(output, result);\n    \n    return std::string(output);\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encode_simd(const std::string & message){\n    const int8x16_t vowels = vmv_v_i8('A', 'E', 'I', 'O', 'U', 'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0);\n    const int8x16_t vowel_replacements = vmv_v_i8('C', 'D', 'K', 'P', 'V', 'c', 'd', 'k', 'p', 'v', 0, 0, 0, 0, 0, 0);\n    \n    int length = message.length();\n    int remaining = length % 16;\n    int i = 0;\n    \n    for (; i < length - remaining; i += 16) {\n        int8x16_t input = vlseg8_v_i8(&message[i]);\n        int8x16_t is_vowel = vmsgt_vv_i8(input, vowels);\n        int8x16_t vowel_indices = vand_vx_i8(is_vowel, 0x0F);\n        int8x16_t replacements = vrgather_vx_i8(vowel_replacements, vowel_indices);\n        int8x16_t swapped_case = vsub_vv_i8(input, 32);\n        int8x16_t result = vmerge_vvm_i8(swapped_case, input, is_vowel);\n        result = vmerge_vvm_i8(replacements, result, is_vowel);\n        vst8_v_i8(&message[i], result);\n    }\n    \n    return message;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string encode_simd(const std::string & message){\n    const int8_t vowels[8] = {'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I'};\n    const int8_t replacements[8] = {'c', 'g', 'm', 's', 'y', 'C', 'G', 'M'};\n    \n    vint8 v_message = *reinterpret_cast<const vint8*>(message.c_str());\n    vint8 v_vowels = *reinterpret_cast<const vint8*>(vowels);\n    vint8 v_replacements = *reinterpret_cast<const vint8*>(replacements);\n    \n    vint8 v_lower_mask = vmsle_vx_i8(v_message, 'Z');\n    vint8 v_upper_mask = vmsle_vx_i8(v_message, 'z');\n    \n    vint8 v_case_diff = vmv_v_x_i8('a' - 'A');\n    vint8 v_case_mask = vmand_mm_i8(v_lower_mask, v_case_diff);\n    \n    vint8 v_swapped_case = vmerge_vvm_i8(v_case_mask, v_case_diff, v_message);\n    \n    vint8 v_vowel_mask = vmseq_vv_i8(v_message, v_vowels);\n    vint8 v_replacement = vadd_vv_i8(v_message, v_replacements);\n    vint8 v_encoded = vmerge_vvm_i8(v_vowel_mask, v_replacement, v_swapped_case);\n    \n    char encoded_message[16];\n    vse8_v_i8(encoded_message, v_encoded);\n    encoded_message[16] = '\\0';\n    \n    return std::string(encoded_message);\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n    \n    vbool8_t all_lower = vmsne_vx_b8_vb8(vmv_x_s_b8(dict.keys(), 0x20), 0);\n    vbool8_t all_upper = vmsne_vx_b8_vb8(vmv_x_s_b8(dict.keys(), -0x20), 0);\n    \n    return vmand_mm_b8(all_lower, vmand_mm_b8(vmand_mm_b8(all_lower, all_upper), vmand_mm_b8(vmsne_vx_b8_vb8(dict.keys(), 0), vmsne_vx_b8_vb8(dict.keys(), -0x20))));\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n    \n    vbool64_t result = vsetvl_e64m1_b64(true);\n    vint8m1_t lowercase_mask = vmseq_vx_i8m1_b64(vmv_x_s_i8m1('a'), vmv_x_s_i8m1('z'), result);\n    vint8m1_t uppercase_mask = vmseq_vx_i8m1_b64(vmv_x_s_i8m1('A'), vmv_x_s_i8m1('Z'), result);\n    \n    bool all_lowercase = true;\n    bool all_uppercase = true;\n    \n    for(auto it = dict.begin(); it != dict.end(); ++it){\n        std::string key = it->first;\n        if(key.empty()) continue;\n        \n        int8_t first_char = key[0];\n        vint8m1_t key_char = vcreate_i8m1(first_char);\n        \n        vbool64_t is_lowercase = vand_mm_b64(vmsltu_vx_i8m1_b64(key_char, vmv_x_s_i8m1('a'), result),\n                                             vmsltu_vx_i8m1_b64(key_char, vmv_x_s_i8m1('z'), result), result);\n        \n        vbool64_t is_uppercase = vand_mm_b64(vmsltu_vx_i8m1_b64(key_char, vmv_x_s_i8m1('A'), result),\n                                             vmsltu_vx_i8m1_b64(key_char, vmv_x_s_i8m1('Z'), result), result);\n        \n        all_lowercase = all_lowercase && vmand_mm_b64(is_lowercase, lowercase_mask, result);\n        all_uppercase = all_uppercase && vmand_mm_b64(is_uppercase, uppercase_mask, result);\n        \n        if(!all_lowercase && !all_uppercase) return false;\n    }\n    \n    return all_lowercase || all_uppercase;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    vbool8_t all_lower = true;\n    vbool8_t all_upper = true;\n\n    for(auto it = dict.begin(); it != dict.end(); ++it){\n        std::string key = it->first;\n        vuint8m1_t key_v = vmv_x_u_v_u8m1((const uint8_t*)key.c_str());\n        vbool8_t is_lower = vmsgeu_vx_u_b8m1(key_v, 'a') && vmsleu_vx_u_b8m1(key_v, 'z');\n        vbool8_t is_upper = vmsgeu_vx_u_b8m1(key_v, 'A') && vmsleu_vx_u_b8m1(key_v, 'Z');\n\n        all_lower = all_lower && is_lower;\n        all_upper = all_upper && is_upper;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n    \n    vbool8_t all_lower = vmsle_vx_u8_b8(vle8_v_u8m1(dict.keys().data(), dict.size()), 'z');\n    vbool8_t all_upper = vmsge_vx_u8_b8(vle8_v_u8m1(dict.keys().data(), dict.size()), 'A');\n    \n    return vmand_mm_b8(all_lower, vmand_mm_b8(vmsle_vx_u8_b8(vle8_v_u8m1(dict.keys().data(), dict.size()), 'a'), vmsge_vx_u8_b8(vle8_v_u8m1(dict.keys().data(), dict.size()), 'Z'))) ||\n           vmand_mm_b8(all_upper, vmsge_vx_u8_b8(vle8_v_u8m1(dict.keys().data(), dict.size()), 'a'));\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n    \n    vbool8_t all_lower = vmsle_vx_u8_b8(vmv_v_x_u8(dict.keys()), 'z');\n    vbool8_t all_upper = vmsle_vx_u8_b8('A', vmv_v_x_u8(dict.keys()));\n    \n    return vmand_mm_b8(all_lower, vmand_mm_b8(vmsle_vx_u8_b8(vmv_v_x_u8(dict.keys()), 'a'), vmsle_vx_u8_b8('Z', vmv_v_x_u8(dict.keys())))) ||\n           vmand_mm_b8(all_upper, vmand_mm_b8(vmsle_vx_u8_b8('A', vmv_v_x_u8(dict.keys())), vmsle_vx_u8_b8(vmv_v_x_u8(dict.keys()), 'a')));\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> count_up_to_simd(int n){\n    vint32m1_t mask = vmseq_vx_i32m1(vmv_v_x_i32m1(n), 0);\n    vint32m1_t primes = vcompress_vm_i32m1(mask, vcreate_v_i32m1(2));\n    for (int i = 3; i < n; i += 2) {\n        vint32m1_t is_prime = vmseq_vx_i32m1(vmv_v_x_i32m1(i), 1);\n        for (int j = 3; j * j <= i; j += 2) {\n            vint32m1_t is_divisible = vmseq_vx_i32m1(vremu_vx_i32m1(vmv_v_x_i32m1(i), j), 0);\n            is_prime = vand_vv_i32m1(is_prime, vmseq_vx_i32m1(vmv_v_x_i32m1(j), 0));\n            is_prime = vand_vv_i32m1(is_prime, vmseq_vx_i32m1(is_divisible, 0));\n        }\n        primes = vmerge_vvm_i32m1(primes, is_prime, primes);\n    }\n    return vslideup_vx_i32(primes, 1);\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> count_up_to_simd(int n){\n    vint32m1_t mask = VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n);\n    vint32m1_t primes = VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n);\n    vbool32_t is_prime = VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n);\n    \n    for (int i = 2; i*i < n; i++) {\n        if (is_prime[i]) {\n            vint32m1_t multiples = VMVX_VI_I32M1(VMUL_VX_I32M1(viota_v_i32m1(), i));\n            is_prime = VMAND_MM_B32(is_prime, VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n), VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n));\n            is_prime = VMBNE_VV_B32(is_prime, multiples, VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n));\n        }\n    }\n    \n    vint32m1_t result = VMSGT_VX_I32M1(primes, 1);\n    result = VMAND_MM_I32(result, mask, is_prime);\n    \n    std::vector<int> output;\n    for (int i = 0; i < n; i++) {\n        if (VMEX_VI_I32M1(result, i)) {\n            output.push_back(i);\n        }\n    }\n    \n    return output;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> count_up_to_simd(int n){\n    vint32m1_t mask = VMSEQ_VI_I32M1(viota_v_i32m1(), 0, n);\n    vint32m1_t primes = VDIVU_VV_I32M1(mask, VMV_V_X_I32M1(2));\n    vint32m1_t one = VMV_V_X_I32M1(1);\n    vint32m1_t zero = VMV_V_X_I32M1(0);\n    vbool32_t is_prime = VMSEQ_VI_I32M1(primes, 2, n);\n    vint32m1_t divisors = VADD_VX_I32M1(VDIVU_VV_I32M1(mask, primes), one);\n    vbool32_t has_divisor = VMSEQ_VI_I32M1(divisors, 2, n);\n    vbool32_t is_not_prime = VMOR_MM_B32(is_prime, has_divisor);\n    vint32m1_t result = VPOPC_MN_I32M1(zero, is_not_prime);\n    return result;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> count_up_to_simd(int n){\n    vint32m1_t primes = vmv_v_i32m1(0); // Initialize vector to store prime numbers\n    vint32m1_t mask = vmslt_vx_i32m1_b(n, 1); // Mask to filter out numbers less than 2\n    vint32m1_t numbers = vmseq_vx_i32m1(vmv_v_x_i32m1(n), 0); // Generate vector of numbers from 0 to n-1\n\n    vbool32_t pred = vmand_mm_b32(mask, vmv_x_s_i32m1(1)); // Predicate to filter out numbers less than 2\n    vbool32_t pred_primes = vmseq_vx_i32m1_b(primes, 0); // Predicate to filter out non-prime numbers\n\n    for (int i = 2; i < n; i++) {\n        vint32m1_t divisor = vmv_v_x_i32m1(i);\n        vbool32_t pred_divisible = vmand_mm_b32(pred, vmsne_vv_i32m1_b(numbers, divisor)); // Predicate to filter out divisible numbers\n        vbool32_t pred_update = vmand_mm_b32(pred_divisible, pred_primes); // Predicate to update prime numbers vector\n        primes = vsadd_vx_i32m1_vb(primes, i, pred_update); // Update prime numbers vector\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\nstd::vector<int> count_up_to_simd(int n){\n    vint32m1_t mask = vmseq_v_i32m1(vi32m1_t(), 0, n);\n    vbool32_t is_prime = vmsne_vx_i32m1_b32(mask, 0);\n    vint32m1_t primes = vcompress_vm_i32m1(mask, is_prime);\n    std::vector<int> result(vmv_x_s_i32(primes));\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\nint count_upper_simd(const std::string & s){\n    const int8x8_t vowels_mask = vmv_v_x_i8(0x55); // Mask for 'AEIOU' in ASCII\n    int count = 0;\n    const int n = s.size();\n    for (int i = 0; i < n; i += 8) {\n        int8x8_t str_chunk = vlseg8b_v_i8(s.data() + i);\n        int8x8_t upper_vowels = vand_vv_i8(str_chunk, vowels_mask);\n        int8x8_t even_indices = vmv_v_x_i8(0x55); // Mask for even indices\n        int8x8_t even_upper_vowels = vand_vv_i8(upper_vowels, even_indices);\n        count += vmsb_vx_i8(even_upper_vowels, 0); // Count number of set bits\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\nint count_upper_simd(const std::string & s){\n    const int8x8_t vowels_mask = vmv_v_x_i8(0x55); // Mask for 'AEIOU' in ASCII\n    const int8x8_t even_indices_mask = vmv_v_x_i8(0x55); // Mask for even indices\n\n    int count = 0;\n    for (size_t i = 0; i < s.size(); i += 8) {\n        int8x8_t str_chars = vlseg8_v_i8m1(&s[i]); // Load 8 characters at a time\n        int8x8_t uppercase_chars = vand_vv_i8(str_chars, vowels_mask); // Check for uppercase vowels\n        int8x8_t even_indices_chars = vand_vv_i8(str_chars, even_indices_mask); // Check for even indices\n        int8x8_t result = vand_vv_i8(uppercase_chars, even_indices_chars); // Combine both conditions\n        count += vpopc_m_e8m1_b8(result); // Count the number of set bits\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_upper_simd(const std::string & s){\n    const int8x8_t vowels = vmv_s_b(0x15); // 'A', 'E', 'I', 'O', 'U' in ASCII\n    const int8x8_t mask = vmv_s_b(0x55); // Mask to extract even indices\n    int count = 0;\n\n    for (size_t i = 0; i < s.size(); i += 8) {\n        int8x8_t str = vlseg8b_v_i8(s.data() + i);\n        int8x8_t even_indices = vand_vx_i8(str, mask);\n        int8x8_t uppercase_vowels = vand_vv_i8(even_indices, vowels);\n        count += vpopc_m(mask, uppercase_vowels);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\nint count_upper_simd(const std::string & s){\n    const int8x8_t vowels = vmv_v_i8('A', 'E', 'I', 'O', 'U', 0, 0, 0);\n    const int8x8_t mask = vmv_v_i8(1, 0, 1, 0, 1, 0, 1, 0);\n    int count = 0;\n    \n    for (size_t i = 0; i < s.size(); i += 8) {\n        int8x8_t str = vlseg8_v_i8(&s[i]);\n        int8x8_t upper = vand_vv_i8(str, vmv_x_s8(0x20));\n        int8x8_t is_vowel = vseq_vv_i8(str, vowels);\n        int8x8_t even_indices = vand_vv_i8(vmv_v_x_u8(0, 1, 2, 3, 4, 5, 6, 7), mask);\n        int8x8_t result = vand_vv_i8(vand_vv_i8(upper, is_vowel), even_indices);\n        count += vmsumu_vx_i8(result);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_upper_simd(const std::string & s){\n    const int8x8_t vowels = vmv_v_i8('A', 'E', 'I', 'O', 'U', 0, 0, 0);\n    const int8x8_t mask = vmv_v_i8(1, 0, 1, 0, 1, 0, 1, 0);\n    int count = 0;\n    \n    for (size_t i = 0; i < s.size(); i += 8) {\n        int8x8_t str = vlseg8_v_i8(s.data() + i);\n        int8x8_t upper = vand_vv_i8(str, vmv_x_s8(0x20));\n        int8x8_t is_vowel = vand_vv_i8(str, vowels);\n        int8x8_t result = vand_vv_i8(vand_vv_i8(vseq_v_i8(0, 1), mask), is_vowel);\n        count += vmsumu_vx_i8(result);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```\nstd::vector<int> make_a_pile_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(n);\n    vint32m1_t increment = vmv_v_x_i32m1(1);\n    vbool32_t mask = vmsne_vx_i32m1_b32(result, 0);\n    \n    for(int i=0; i<n-1; i++){\n        result = vadd_vx_i32m1(result, 2*vincrement, mask);\n        increment = vadd_vx_i32m1(increment, 1, mask);\n        mask = vmsne_vx_i32m1_b32(result, 0);\n    }\n    \n    std::vector<int> stones(n);\n    vse32_v_i32m1(&stones[0], result);\n    \n    return stones;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\nstd::vector<int> make_a_pile_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(n);\n    vint32m1_t odd = vsub_vx_i32m1(result, 1);\n    vint32m1_t even = vadd_vx_i32m1(result, 1);\n    \n    vbool32_t mask = vmsne_vx_i32m1_b32(result, 0);\n    vint32m1_t next_stones = vmerge_vvm_i32m1(mask, odd, even);\n    \n    std::vector<int> stones(n);\n    vse32_v_i32m1(&stones[0], next_stones);\n    \n    return stones;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\nstd::vector<int> make_a_pile_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(n, VL1);\n    vint32m1_t indices = vid_v_i32m1();\n    vbool32_t mask = vmseq_vx_i32m1_b32(indices, 0, VL1);\n    \n    for(int i=0; i<n-1; i++){\n        vint32m1_t next_stones = vadd_vx_i32m1(result, 2, VL1);\n        vbool32_t is_even = vmand_mm_b32(vmsne_vx_i32m1_b32(result, 0, VL1), vmseq_vx_i32m1_b32(result, 0, VL1), VL1);\n        result = vmerge_vvm_i32m1(result, next_stones, is_even, VL1);\n    }\n    \n    std::vector<int> stones(n);\n    vse32_v_i32m1(&stones[0], result, VL1);\n    \n    return stones;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\nstd::vector<int> make_a_pile_simd(int n){\n    const int lane_size = vsetvl_e32m1();\n    const int num_lanes = (n + lane_size - 1) / lane_size;\n    vint32m1_t result = vmv_v_x_i32m1(0, 0);\n    vint32m1_t stones = vmv_v_x_i32m1(n, 0);\n\n    for (int i = 0; i < num_lanes; i++) {\n        vbool32_t mask = vmseq_vx_i32m1_b32(stones, 0);\n        vint32m1_t next_stones = vadd_vx_i32m1(stones, lane_size);\n        vint32m1_t next_values = vmerge_vvm_v_i32m1(mask, vadd_vx_i32m1(stones, 1), vadd_vx_i32m1(stones, 2));\n        result = vsseg2e32_v_i32m1(result, next_values, 0);\n        stones = next_stones;\n    }\n\n    std::vector<int> res(n);\n    vse32_v_i32m1(&res[0], result);\n    return res;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```\nstd::vector<int> make_a_pile_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(n, VL1);\n    vint32m1_t levels = vsub_vx_i32m1(result, 1, VL1);\n    vbool32_t is_odd = vmsne_vx_i32m1_b32(result, 0, VL1);\n    vint32m1_t odd_increment = vmerge_vvm_i32m1(is_odd, vsub_vx_i32m1(result, 1, VL1), vadd_vx_i32m1(result, 1, VL1), VL1);\n    vint32m1_t even_increment = vmerge_vvm_i32m1(is_odd, vadd_vx_i32m1(result, 1, VL1), vsub_vx_i32m1(result, 1, VL1), VL1);\n    vint32m1_t stones = vadd_vv_i32m1(levels, vmerge_vvm_i32m1(is_odd, odd_increment, even_increment, VL1), VL1);\n    return stones;\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n    \n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n    \n    vint64m1_t v_n = vmv_s_x_i64(n, VL1);\n    vint64m1_t v_m = vmv_s_x_i64(m, VL1);\n    vint64m1_t v_sum = vmv_s_x_i64(0, VL1);\n    \n    vint64m1_t v_indices = vmseq_v_i64m1(v_n, VL1);\n    vint64m1_t v_values = vadd_vv_i64m1(v_n, v_indices, VL1);\n    \n    vbool64_t mask = vmseq_v_i64m1(v_m, VL1);\n    v_values = vmerge_vvm_i64m1(mask, v_values, v_n, VL1);\n    \n    v_sum = vredsum_vs_i64m1_i64m1(v_sum, v_values, v_sum, VL1);\n    \n    int64_t avg = vslide1down_vx_i64m1(v_sum, 1, VL1);\n    avg = avg / count;\n    \n    std::string result = std::bitset<64>(avg).to_string();\n    return result.substr(64 - count);\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m) {\n        return \"-1\";\n    }\n    \n    vint64m1_t vec_n = vmv_s_x_i64(n, VL1);\n    vint64m1_t vec_m = vmv_s_x_i64(m, VL1);\n    \n    vint64m1_t vec_sum = vadd_vv_i64m1(vec_n, vec_m);\n    vint64m1_t vec_avg = vdiv_vx_i64m1(vec_sum, 2);\n    \n    vint64m1_t vec_rounded_avg = vnsrl_wx_i64m1(vec_avg, 1);\n    \n    int64_t rounded_avg = vmv_x_s_i64(vec_rounded_avg);\n    \n    std::string binary_avg = std::bitset<64>(rounded_avg).to_string();\n    \n    return binary_avg.substr(binary_avg.find('1'));\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m) {\n        return \"-1\";\n    }\n\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n    int64_t i = n;\n    vint64m1_t v_sum = vmv_s_x_i64m1(0);\n    vint64m1_t v_count = vmv_s_x_i64m1(count);\n\n    for(; i <= m - 8; i += 8) {\n        vint64m1_t v_i = vmv_s_x_i64m1(i);\n        v_sum = vadd_vx_i64m1(v_sum, v_i);\n    }\n\n    sum = vslide1down_vx_i64m1(v_sum, v_sum);\n    for(; i <= m; i++) {\n        sum += i;\n    }\n\n    int64_t avg = (sum + count / 2) / count;\n    std::string binary_avg = std::bitset<64>(avg).to_string();\n    return binary_avg.substr(64 - count);\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m) {\n        return \"-1\";\n    }\n\n    vint64m1_t v = vmv_v_i_i64(n);\n    vint64m1_t sum = v;\n    int64_t count = 1;\n\n    for(int64_t i = n + 1; i <= m; i++) {\n        vint64m1_t vi = vmv_v_i_i64(i);\n        sum = vadd_vv_i64m1(sum, vi);\n        count++;\n    }\n\n    vint64m1_t avg = vdiv_vx_i64m1(sum, count);\n    int64_t avg_scalar = vmv_x_s_i64(avg);\n\n    std::string binary_avg = std::bitset<64>(avg_scalar).to_string();\n    return binary_avg.substr(0, 64);\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m) {\n        return \"-1\";\n    }\n\n    vint64m1_t v_n = vmv_s_x_i64(n);\n    vint64m1_t v_m = vmv_s_x_i64(m);\n    vint64m1_t v_sum = vadd_vv_i64m1(v_n, v_m);\n    vint64m1_t v_avg = vdiv_vx_i64m1(v_sum, 2);\n    vint64m1_t v_rounded_avg = vssra_vx_i64m1(v_avg, 1);\n    int64_t rounded_avg = vslide1down_vx_i64m1_i64(v_rounded_avg, 0);\n    \n    std::string result = std::bitset<64>(rounded_avg).to_string();\n    return result.substr(result.find('1'));\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```\nstd::vector<int> func_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n    vint32m1_t factorial = vmv_v_x_i32m1(0, 1); // Initialize factorial vector\n    vint32m1_t sum = vmv_v_x_i32m1(0, 0); // Initialize sum vector\n\n    for (int i = 1; i <= n; i++) {\n        factorial = vwmul_vx_i32m1(factorial, i); // Calculate factorial\n        sum = vwadd_vx_i32m1(sum, i); // Calculate sum\n\n        vbool32_t mask = vmseq_vx_i32m1_b32(i, 0); // Create mask for even indices\n        result = vmerge_vvm_i32m1(mask, result, factorial); // Store factorial if even index\n        result = vmerge_vvm_i32m1(!mask, result, sum); // Store sum if odd index\n    }\n\n    std::vector<int> output(n);\n    vse32_v_i32m1(output.data(), result);\n    return output;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```\nstd::vector<int> func_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n    vint32m1_t factorial = vmv_v_x_i32m1(0, 1); // Initialize factorial vector\n    vint32m1_t sum = vmv_v_x_i32m1(0, 0); // Initialize sum vector\n\n    for (int i = 1; i <= n; i++) {\n        factorial = vmul_vx_i32m1(factorial, i); // Calculate factorial\n        sum = vadd_vx_i32m1(sum, i); // Calculate sum\n\n        vbool32_t mask = vmseq_vx_i32m1_b32(i, 1); // Check if i is odd\n        result = vmerge_vvm_i32m1(mask, result, sum, factorial); // Merge sum and factorial based on the mask\n    }\n\n    std::vector<int> output(n);\n    vse32_v_i32m1(output.data(), result); // Store the result vector in output\n\n    return output;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n    vint32m1_t factorial = vmv_v_x_i32m1(1, 0); // Initialize factorial vector\n    vint32m1_t sum = vmv_v_x_i32m1(0, 0); // Initialize sum vector\n\n    for (int i = 1; i <= n; i++) {\n        factorial = vmul_vx_i32m1(factorial, i); // Calculate factorial\n\n        vbool32_t is_even = vmv_x_s_i32m1(i % 2 == 0 ? 0xFFFFFFFF : 0, 0); // Check if i is even\n        vint32m1_t value = vmerge_vvm_i32m1(is_even, factorial % 10000, vadd_vx_i32m1(sum, i)); // Calculate value based on condition\n\n        result = vslide1down_vx_i32m1(result, value); // Shift result vector and insert value at the beginning\n        sum = vadd_vv_i32m1(sum, i); // Update sum vector\n    }\n\n    std::vector<int> output(n);\n    vse32_v_i32m1(output.data(), result);\n    return output;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```\nstd::vector<int> func_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n    vint32m1_t factorial = vmv_v_x_i32m1(1, 0); // Initialize factorial vector\n    vint32m1_t sum = vmv_v_x_i32m1(0, 0); // Initialize sum vector\n\n    for (int i = 1; i <= n; i++) {\n        factorial = vwmul_vx_i32m1(factorial, i); // Calculate factorial\n\n        vbool32_t mask = vmseq_vx_i32m1_b32(i, 1); // Check if i is odd\n        sum = vsadd_vx_i32m1(sum, i); // Calculate sum\n\n        result = vsel_vv_i32m1_b32(mask, sum, factorial); // Select between sum and factorial based on mask\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```\nstd::vector<int> func_simd(int n){\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n    vint32m1_t factorial = vmv_v_x_i32m1(0, 1); // Initialize factorial vector\n    vint32m1_t sum = vmv_v_x_i32m1(0, 0); // Initialize sum vector\n\n    for (int i = 1; i <= n; i++) {\n        factorial = vwmul_vx_i32m1(factorial, i); // Calculate factorial\n\n        vbool32_t is_even = vmseq_vx_i32m1_b32(factorial, 0); // Check if i is even\n        vint32m1_t factorial_mod = vremu_vx_i32m1(factorial, 10000); // Calculate factorial modulo 10,000\n        vint32m1_t sum_temp = vsadd_vx_i32m1(sum, i); // Calculate sum\n\n        result = vsel_vv_i32m1_b32(is_even, factorial_mod, sum_temp); // Select between factorial_mod and sum based on is_even\n        sum = sum_temp; // Update sum for the next iteration\n    }\n\n    std::vector<int> output(n);\n    vse32_v_i32m1(output.data(), result);\n    return output;\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    vint32m1_t even_count = vmv_v_x_i32m1(0);\n    vint32m1_t odd_count = vmv_v_x_i32m1(0);\n\n    vbool32_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(1), n);\n\n    vint32m1_t num = vmv_v_x_i32m1(1);\n    vint32m1_t rev_num = vmv_v_x_i32m1(0);\n\n    do {\n        vint32m1_t temp = vadd_vx_i32m1(num, 0);\n        vint32m1_t digit = vmv_x_s_i32m1(vredsum_vs_i32m1_i32m1(vxor_vv_i32m1(temp, rev_num), 1));\n\n        if (vmsne_vx_i32m1_b32(digit, 0)) {\n            vint32m1_t new_num = vadd_vx_i32m1(num, 1);\n            vint32m1_t new_rev_num = vmv_x_s_i32m1(vredsum_vs_i32m1_i32m1(vxor_vv_i32m1(new_num, rev_num), 1));\n\n            if (vmsne_vv_i32m1_b32(num, rev_num) && vmsne_vv_i32m1_b32(num, new_rev_num) && vmsne_vv_i32m1_b32(new_num, new_rev_num)) {\n                if (vmand_mm_b32(mask, vmsltu_vv_i32m1_b32(new_num, n))) {\n                    vint32m1_t is_even = vand_vx_i32m1(num, 1);\n                    even_count = vmerge_vvm_i32m1(even_count, vadd_vx_i32m1(even_count, vis_true_b32_v_i32m1(is_even)), vis_true_b32_v_i32m1(mask));\n                    odd_count = vmerge_vvm_i32m1(odd_count, vadd_vx_i32m1(odd_count, vis_true_b32_v_i32m1(vmxor_mm_b32(is_even, 1))), vis_true_b32_v_i32m1(mask));\n                }\n            }\n        }\n\n        rev_num = vadd_vx_i32m1(vredsum_vs_i32m1_i32m1(vxor_vv_i32m1(num, rev_num), 1), 0);\n        num = vadd_vx_i32m1(num, 1);\n    } while (vmsltu_vx_i32m1_b32(num, n));\n\n    std::vector<int> result = {vslide1down_vx_i32m1(vadd_vx_i32m1(even_count, 0), 1), vslide1down_vx_i32m1(vadd_vx_i32m1(odd_count, 0), 1)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    vint32m1_t even_count = vmv_v_x_i32m1(0);\n    vint32m1_t odd_count = vmv_v_x_i32m1(0);\n    \n    vbool32_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(1), n);\n    \n    vint32m1_t num = vseq_v_i32m1(vmv_v_x_i32m1(1), vmv_x_s_i32m1(n));\n    \n    vint32m1_t reversed = vrev_v_i32m1(num);\n    \n    vbool32_t is_palindrome = vmsne_vv_i32m1_b32(num, reversed);\n    \n    vint32m1_t is_even = vand_vx_i32m1(is_palindrome, 1);\n    \n    vint32m1_t is_odd = vand_vx_i32m1(is_palindrome, 0);\n    \n    even_count = vredsum_vs_i32m1_i32(even_count, vis_true_b32_v_i32m1(is_even), even_count);\n    odd_count = vredsum_vs_i32m1_i32(odd_count, vis_true_b32_v_i32m1(is_odd), odd_count);\n    \n    std::vector<int> result = {vslide1down_vx_i32m1(even_count, 0), vslide1down_vx_i32m1(odd_count, 0)};\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    vint32m1_t v_even = vmv_v_x_i32m1(0, VL1);\n    vint32m1_t v_odd = vmv_v_x_i32m1(0, VL1);\n\n    vint32m1_t v_n = vmv_v_x_i32m1(n, VL1);\n    vbool32_t mask = vmseq_vx_i32m1_b32(v_n, 0);\n\n    vint32m1_t v_num = vmv_v_x_i32m1(1, VL1);\n    vint32m1_t v_rev = vmv_v_x_i32m1(0, VL1);\n\n    do {\n        vint32m1_t v_temp = v_num;\n        vint32m1_t v_digit = vremu_vx_i32m1(v_temp, 10);\n        v_rev = vadd_vx_i32m1(v_rev, vwmulu_vx_i32m1(v_digit, 10));\n        v_num = vdivu_vx_i32m1(v_num, 10);\n    } while(vnum_vx_i32m1(v_num) > 0);\n\n    vbool32_t is_palindrome = vmand_mm_b32(v_num, vrev);\n\n    vint32m1_t v_is_even = vmseq_vx_i32m1_b32(vremu_vx_i32m1(v_num, 2), 0);\n    v_even = vadd_vv_i32m1(v_even, vmand_mm_i32m1(is_palindrome, v_is_even));\n    v_odd = vadd_vv_i32m1(v_odd, vmand_mm_i32m1(is_palindrome, vmseq_vx_i32m1_b32(v_is_even, 0)));\n\n    return {v_even[0], v_odd[0]};\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    vint32m1_t even_count = vmv_v_x_i32m1(0);\n    vint32m1_t odd_count = vmv_v_x_i32m1(0);\n\n    vbool32_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(1), n);\n\n    vint32m1_t num = vmv_v_x_i32m1(1);\n    vint32m1_t rev_num, temp, digit;\n    vint32m1_t even_mask, odd_mask;\n\n    while (vmsleu_vx_i32m1_b32(num, n)) {\n        rev_num = vmv_v_x_i32m1(0);\n        temp = num;\n        while (vmsne_vx_i32m1_b32(temp, vmv_v_x_i32m1(0))) {\n            digit = vremu_vx_i32m1(temp, vmv_v_x_i32m1(10));\n            rev_num = vadd_vx_i32m1(rev_num, digit);\n            temp = vdivu_vx_i32m1(temp, vmv_v_x_i32m1(10));\n            if (vmsltu_vx_i32m1_b32(rev_num, temp)) {\n                rev_num = vadd_vx_i32m1(rev_num, digit);\n            }\n            if (vmsltu_vx_i32m1_b32(rev_num, temp)) {\n                rev_num = vadd_vx_i32m1(rev_num, digit);\n            }\n            if (vmsltu_vx_i32m1_b32(rev_num, temp)) {\n                rev_num = vadd_vx_i32m1(rev_num, digit);\n            }\n        }\n\n        even_mask = vmand_mm_b32(mask, vmand_mm_b32(vmsltu_vx_i32m1_b32(num, rev_num), vmseq_vx_i32m1_b32(vmul_vx_i32m1(rev_num, vmv_v_x_i32m1(2)), num)));\n        odd_mask = vmand_mm_b32(mask, vmand_mm_b32(vmsltu_vx_i32m1_b32(rev_num, num), vmseq_vx_i32m1_b32(vmul_vx_i32m1(rev_num, vmv_v_x_i32m1(2)), num)));\n\n        even_count = vadd_vv_i32m1(even_count, vpopc_m_b32(even_mask));\n        odd_count = vadd_vv_i32m1(odd_count, vpopc_m_b32(odd_mask));\n\n        num = vadd_vx_i32m1(num, vmv_v_x_i32m1(1));\n    }\n\n    std::vector<int> result = {vslide1down_vx_i32m1(vgetfirst_m_i32m1(even_count), 0), vslide1down_vx_i32m1(vgetfirst_m_i32m1(odd_count), 0)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    vint32m1_t v_even = vmv_v_x_i32m1(0, VL1);\n    vint32m1_t v_odd = vmv_v_x_i32m1(0, VL1);\n    \n    for (int i = 1; i <= n; i++) {\n        int rev = 0, temp = i;\n        while (temp > 0) {\n            rev = rev * 10 + temp % 10;\n            temp /= 10;\n        }\n        if (rev == i) {\n            if (i % 2 == 0) {\n                v_even = vadd_vx_i32m1(v_even, 1, VL1);\n            } else {\n                v_odd = vadd_vx_i32m1(v_odd, 1, VL1);\n            }\n        }\n    }\n    \n    std::vector<int> result = {v_even[0], v_odd[0]};\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_nums_simd(std::vector<int> n){\n    vint32m1_t vec_n = vle32_v_i32m1(&n[0]);\n\n    vint32m1_t abs_n = vabs_v_i32m1(vec_n);\n    vint32m1_t sum_digits = vwmulsu_vx_i32m1(abs_n, 1, 1);\n\n    vbool32_t mask = vmsne_vx_i32m1_b32(sum_digits, 0);\n    int count = vmpopc_m_b32(mask);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_nums_simd(std::vector<int> n){\n    vint32m1_t vec_n = vle32_v_i32m1(&n[0]);\n\n    vint32m1_t abs_n = vabs_v_i32m1(vec_n);\n    vint32m1_t sum_digits = vwmulsu_vv_i32m1(abs_n, 1, 1);\n\n    vbool32_t mask = vmsne_vx_i32m1_b32(sum_digits, 0);\n\n    int count = vmpopc_m_b32(mask);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```\nint count_nums_simd(std::vector<int> n){\n    vint32m4_t vec = VLE32_V_I(n.data(), n.size());    \n    vint32m4_t abs_vec = VABS_VX_i32m4(vec);\n    vint32m4_t sum_digits = VMV_V_X_i32m4(VREDUCE_ADD_i32m4(VADD_i32m4(VREDUCE_ADD_i32m4(abs_vec), VREDUCE_ADD_i32m4(VDIVU_VX_i32m4(abs_vec, 10))));\n    vbool8_t mask = VMSEQ_VI_i8m1_b8(sum_digits, 0);\n    int count = VMCOUNT_m_b8(mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_nums_simd(std::vector<int> n){\n    vint32m1_t vec_n = vle32_v_i32m1(&n[0]);\n    vint32m1_t sum_digits = vwmulsu_vx_i32m1(vec_n, 1, VL1);\n    vint32m1_t sum_digits_abs = vabs_v_i32m1(sum_digits);\n    vint32m1_t sum_digits_abs_digits = vdivu_vx_i32m1(sum_digits_abs, 10, VL1);\n    vint32m1_t sum_digits_abs_digits_sum = vredsum_vs_i32m1_i32m1(sum_digits_abs_digits, 0, VL1);\n    vbool32_t mask = vmsne_vx_i32m1_b32(sum_digits_abs_digits_sum, 0, VL1);\n    int count = vmpopc_m_b32(mask, VL1);\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint count_nums_simd(std::vector<int> n){\n    vint32m1_t vec_n = vle32_v_i32m1(&n[0]);\n    vint32m1_t sum_digits = vwmulsu_vx_i32m1(vec_n, 1, VL1);\n    vint32m1_t sum_digits_abs = vabs_v_i32m1(sum_digits);\n    vint32m1_t sum_digits_abs_digits = vdivu_vx_i32m1(sum_digits_abs, 10, VL1);\n    vint32m1_t sum_digits_abs_digits_sum = vredsum_vs_i32m1_i32m1(sum_digits_abs_digits, 0, VL1);\n    vbool32_t mask = vmsne_vx_i32m1_b32(sum_digits_abs_digits_sum, 0, VL1);\n    int count = vmpopc_m_b32(mask, VL1);\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    if(N <= 1) return true;\n    \n    vint32m1_t vec = vmv_v_x_i32m1(0, 0); // Initialize a vector\n    \n    for(int i = 0; i < N; i++){\n        vec = vmerge_vim_i32m1(vec, arr[i], i); // Merge the vector with the current element\n    }\n    \n    vbool32_t sorted = vmsle_vx_i32m1_b32(vec, vslide1down_vx_i32m1(vec, 0), 0); // Check if vector is sorted\n    \n    return vmfirst_m_b32(sorted); // Return true if vector is sorted, false otherwise\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    if(N == 0) return true;\n\n    vint32m1_t vec = vmv_v_x_i32m1(0, 0); // Initialize a vector with zeros\n    vbool32_t sorted = vmslt_vx_i32m1_b(N, 0); // Initialize a mask vector with all true values\n\n    for(int i = 0; i < N; i++){\n        vec = vslide1down_vx_i32m1(vec, arr[i]); // Shift elements down by one position\n        sorted = vmand_mm_b32(sorted, vmsle_vx_i32m1_b(vec, arr[i])); // Update mask based on sorted condition\n    }\n\n    return vmfirst_m_b32(sorted); // Check if all elements are in non-decreasing order\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    vint32m1_t vec = vmv_v_x_i32m1(arr.data(), N, vl1);\n    vbool32_t mask = vmslt_vx_i32m1_b32(vec, vslide1down_vx_i32m1(vec, 1, vl1), vl1);\n    bool result = vmfirst_m_b32(mask, vl1) < N;\n    return result;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    if(N == 0){\n        return true;\n    }\n    \n    vint32m1_t vec = vmv_v_x_i32m1(arr.data(), N);\n    vint32m1_t sorted_vec = vslide1down_vx_i32m1(vec, N-1);\n    \n    vbool32_t cmp_result = vmsltu_vv_i32m1_b32(sorted_vec, vec);\n    \n    return vmsne_vx_i32m1_b32(cmp_result, 0);\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    if(N <= 1) return true;\n    \n    vint32m1_t vec = vle32_v_i32m1(&arr[0], N); // Load input vector\n    \n    vint32m1_t sorted_vec = vslide1down_vx_i32m1(vec, N-1); // Perform right shift operation\n    \n    vbool32_t cmp_result = vmsltu_vv_i32m1_b32(sorted_vec, vec, N); // Compare sorted vector with original vector\n    \n    bool result = vmand_mm_b32(cmp_result); // Check if all elements are in non-decreasing order\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    \n    int* ptr_lst1 = lst1.data();\n    int* ptr_lst2 = lst2.data();\n    \n    int mask_all_ones = -1;\n    vint32m1_t v_mask_all_ones = vmv_v_x_i32m1(mask_all_ones);\n    \n    int flag = 0;\n    \n    for (size_t i = 0; i < lst1.size(); i += vector_size) {\n        vint32m1_t v_lst1 = vle32_v_i32m1(ptr_lst1 + i, vector_size);\n        vint32m1_t v_lst2 = vle32_v_i32m1(ptr_lst2 + i, vector_size);\n        \n        vbool32_t v_mask_odd = vmand_mm_b32(vmand_mm_b32(v_lst1 % 2 == 1, v_lst2 % 2 == 0), v_mask_all_ones);\n        vbool32_t v_mask_even = vmand_mm_b32(vmand_mm_b32(v_lst1 % 2 == 0, v_lst2 % 2 == 1), v_mask_all_ones);\n        \n        if (vmsne_vx_i32m1_b32(vmsumu_vx_i32m1_i32m1(v_mask_odd), 0)) {\n            flag = 1;\n            break;\n        }\n        \n        if (vmsne_vx_i32m1_b32(vmsumu_vx_i32m1_i32m1(v_mask_even), 0)) {\n            flag = 1;\n            break;\n        }\n    }\n    \n    if (flag == 1) {\n        return \"NO\";\n    } else {\n        return \"YES\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vector_size = vsetvl_e32m1(0);\n\n    int* ptr_lst1 = const_cast<int*>(lst1.data());\n    int* ptr_lst2 = const_cast<int*>(lst2.data());\n\n    int* mask = (int*)malloc(sizeof(int) * vector_size);\n    int* result = (int*)malloc(sizeof(int) * vector_size);\n\n    vint32m1_t v_lst1 = vle32_v_i32m1(ptr_lst1, vector_size);\n    vint32m1_t v_lst2 = vle32_v_i32m1(ptr_lst2, vector_size);\n\n    vbool32_t mask_even = vmseq_vx_i32m1_b32(v_lst1, 0, vector_size);\n    vbool32_t mask_odd = vmseq_vx_i32m1_b32(v_lst1, 1, vector_size);\n\n    vbool32_t mask_lst2_even = vmseq_vx_i32m1_b32(v_lst2, 0, vector_size);\n    vbool32_t mask_lst2_odd = vmseq_vx_i32m1_b32(v_lst2, 1, vector_size);\n\n    vbool32_t mask_exchange = vmand_mm_b32(mask_even, mask_lst2_odd, vector_size);\n\n    vse32_v_i32m1(ptr_lst1, vmerge_vvm_i32m1(mask_exchange, v_lst2, v_lst1, vector_size), vector_size);\n\n    for (int i = 0; i < vector_size; i++) {\n        mask[i] = vmsne_vx_i32m1_b32(v_lst1[i], 0);\n    }\n\n    vbool32_t final_mask = vmand_mm_b32(mask, mask_odd, vector_size);\n    vsadd_vx_i32m1(result, final_mask, 1, vector_size);\n\n    int sum = 0;\n    for (int i = 0; i < vector_size; i++) {\n        sum += result[i];\n    }\n\n    free(mask);\n    free(result);\n\n    if (sum > 0) {\n        return \"NO\";\n    } else {\n        return \"YES\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vector_size = vsetvlmax_e32m1();\n    const int num_elements = lst1.size();\n\n    int* result = (int*)malloc(sizeof(int) * num_elements);\n    vint32m1_t v_lst1, v_lst2, v_mask;\n    vbool32_t v_pred = VMXT::vmslt_vx_u32m1_b32(num_elements, 0);\n    vint32m1_t v_zeros = VMXT::vzero_v_i32m1();\n    vint32m1_t v_ones = VMXT::vsub_vx_i32m1(v_zeros, 1);\n\n    for (int i = 0; i < num_elements; i += vector_size) {\n        int remaining = num_elements - i;\n        int current_size = (remaining >= vector_size) ? vector_size : remaining;\n\n        v_lst1 = VLSE::vlse32_v_i32m1(&lst1[i], current_size);\n        v_lst2 = VLSE::vlse32_v_i32m1(&lst2[i], current_size);\n\n        v_mask = VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32(VMXT::vmand_mm_b32("}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vector_size = vsetvl_e32m1();\n    const int num_elements = lst1.size();\n\n    int* ptr_lst1 = const_cast<int*>(lst1.data());\n    int* ptr_lst2 = const_cast<int*>(lst2.data());\n\n    int* even_mask = (int*)malloc(sizeof(int) * vector_size);\n    int* all_even = (int*)malloc(sizeof(int) * vector_size);\n    for (int i = 0; i < vector_size; i++) {\n        all_even[i] = 0;\n    }\n\n    for (int i = 0; i < num_elements; i += vector_size) {\n        vint32m1_t v_lst1 = vlse32_v_i32m1(ptr_lst1, vector_size);\n        vint32m1_t v_lst2 = vlse32_v_i32m1(ptr_lst2, vector_size);\n\n        vbool32_t mask = vmsne_vx_i32m1_b32(v_lst1, 0);\n        vbool32_t even_mask_v = vmand_mm_b32(mask, vmseq_vx_i32m1_b32(v_lst1, 0));\n\n        vint32m1_t swapped_lst1 = vmerge_vvm_i32m1(even_mask_v, v_lst2, v_lst1);\n\n        vint32m1_t even_elements = vand_vv_i32m1(swapped_lst1, 1);\n        vbool32_t all_even_mask = vmseq_vx_i32m1_b32(vredsum_vs_i32m1_i32m1(vnot_v_i32m1(even_elements), 0, 0), 0);\n\n        vbool32_t all_even_mask_final = vmand_mm_b32(all_even_mask, even_mask_v);\n\n        vsse32_v_i32m1(ptr_lst1, vector_size, swapped_lst1);\n\n        vbool32_t all_even_mask_store = vmand_mm_b32(all_even_mask_final, vmseq_vx_i32m1_b32(vlse32_v_i32m1(all_even, vector_size), 0));\n        vbool32_t all_even_mask_load = vmand_mm_b32(all_even_mask_final, vmseq_vx_i32m1_b32(vlse32_v_i32m1(ptr_lst1, vector_size), 0));\n\n        if (vmsne_vv_i32m1_b32(all_even_mask_store, all_even_mask_load)) {\n            free(even_mask);\n            free(all_even);\n            return \"NO\";\n        }\n    }\n\n    free(even_mask);\n    free(all_even);\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elements = lst1.size();\n\n    vint32m1_t va = vle32_v_i32m1(&lst1[0], vector_size*num_elements);\n    vint32m1_t vb = vle32_v_i32m1(&lst2[0], vector_size*num_elements);\n\n    vbool32_t mask = vmseq_vx_i32m1_b32(va, 0); // Mask for elements equal to 0 (even numbers)\n\n    vbool32_t result = vmand_mm_b32(mask, vmseq_vx_i32m1_b32(vb, 0)); // Check if corresponding elements in lst2 are also even\n\n    int all_true = vmfirst_m_b32(result); // Check if all elements are true\n\n    if(all_true == -1){\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto &str : lst) {\n        vint8 v_count = 0;\n        for (size_t i = 0; i < str.size(); ++i) {\n            vint8 v_digit = str[i] - '0';\n            vint8 v_is_odd = v_digit % 2;\n            v_count += v_is_odd;\n        }\n        int count[8];\n        vse8_v_i32(count, v_count);\n        std::string res_str = \"the number of odd elements \";\n        for (int i = 0; i < 8; ++i) {\n            res_str += std::to_string(count[i]);\n            res_str += \"n the str\";\n            res_str += std::to_string(i+1);\n            res_str += \"ng \";\n            res_str += std::to_string(i+1);\n            res_str += \" of the \";\n            res_str += std::to_string(i+1);\n            res_str += \"nput.\";\n            result.push_back(res_str);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto &str : lst) {\n        vint8 v_count = 0;\n        for (size_t i = 0; i < str.size(); i++) {\n            vint8 v_digit = str[i] - '0';\n            v_count += v_digit & 1;\n        }\n        result.push_back(\"the number of odd elements \" + std::to_string(v_count.sum()) + \"n the str\" + std::to_string(v_count.sum()) + \"ng \" + str + \" of the \" + std::to_string(v_count.sum()) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto &str : lst) {\n        vint8 v_odd_count = vmv_v_x_i8(0);\n        for (size_t i = 0; i < str.size(); i++) {\n            vint8 v_digit = vmv_v_x_i8(str[i] - '0');\n            v_odd_count = v_odd_count + vmand_vv_i8(v_digit, vmv_v_x_i8(1));\n        }\n        int8_t odd_count[8];\n        vse8_v_i8m1(odd_count, v_odd_count);\n        std::string res_str = \"the number of odd elements \";\n        for (int i = 0; i < 8; i++) {\n            res_str += std::to_string(odd_count[i]);\n            res_str += \"n \";\n        }\n        res_str += \"the str\" + std::to_string(result.size() + 1) + \"ng \" + std::to_string(result.size() + 1) + \" of the \" + std::to_string(result.size() + 1) + \"nput.\";\n        result.push_back(res_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result(lst.size());\n    vint8_t odd_count = 0;\n    \n    for (size_t i = 0; i < lst.size(); i++) {\n        vint8_t odd_mask = 0;\n        for (size_t j = 0; j < lst[i].size(); j++) {\n            odd_mask |= (lst[i][j] - '0') % 2;\n        }\n        odd_count = vadd_vx_i8(odd_count, odd_mask);\n        result[i] = \"the number of odd elements \" + std::to_string(vpopc_m1_i8(odd_mask)) + \"n the str\" + std::to_string(i+1) + \"ng \" + std::to_string(i+1) + \" of the \" + std::to_string(i+1) + \"nput.\";\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    const int vector_length = vsetvli(0, RVV_E8, RVV_M8);\n    \n    for (const std::string& str : lst) {\n        int odd_count = 0;\n        for (size_t i = 0; i < str.size(); i += vector_length) {\n            vuint8m8_t input = vle8_v_u8m8(&str[i]);\n            vbool8_t odd_mask = vmsne_vx_u8m8_b8(input, 0, VL1);\n            odd_count += vpopc_m_b8(odd_mask, VL1);\n        }\n        result.push_back(\"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" + std::to_string(result.size()) + \" of the \" + std::to_string(result.size()) + \"nput.\");\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int vlen = vsetvl_e64m1();\n    int64_t min_sum = nums[0];\n    int64_t current_sum = 0;\n\n    vint64m1_t v_sum = vmv_s_x_i64m1(0);\n    vint64m1_t v_min_sum = vmv_s_x_i64m1(min_sum);\n\n    for (size_t i = 0; i < nums.size(); i += vlen) {\n        vint64m1_t v_nums = vle64_v_i64m1(&nums[i]);\n\n        v_sum = vadd_vv_i64m1(v_sum, v_nums);\n        v_min_sum = vmin_vx_i64m1(v_min_sum, v_sum);\n\n        vint64m1_t v_sub_sum = vsub_vx_i64m1(v_sum, v_min_sum);\n        v_min_sum = vmin_vv_i64m1(v_min_sum, v_sum);\n\n        current_sum = vslide1down_vx_i64m1(v_sub_sum, 0);\n        min_sum = vmin(current_sum, min_sum);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int lane_size = vsetvli(0, RVV_E8, RVV_M8);\n    const int num_lanes = vsetvli(0, RVV_E8, RVV_M8) / VLMAX;\n    \n    int64_t min_sum = INT64_MAX;\n    int64_t current_sum = 0;\n    \n    vint64m8_t acc = vmv_v_x_i64m8(0, 0);\n    \n    for (size_t i = 0; i < nums.size(); i += num_lanes) {\n        vint64m8_t vec_nums = vle64_v_i64m8(&nums[i]);\n        acc = vadd_vv_i64m8(acc, vec_nums);\n        \n        vbool8_t mask = vmseq_vx_i64m8_b8(acc, 0, VLMAX);\n        vint64m8_t masked = vmerge_vxm_i64m8(vec_nums, 0, mask, VLMAX);\n        \n        vint64m8_t prefix_sum = vslide1down_vx_i64m8(acc, 0, VLMAX);\n        vint64m8_t subarray_sum = vsub_vv_i64m8(acc, prefix_sum);\n        \n        vint64m8_t min_subarray_sum = vmin_vv_i64m8(subarray_sum, masked);\n        min_sum = vminu_vx_i64(min_sum, vfirst_m_i64m8(min_subarray_sum, VLMAX));\n    }\n    \n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int size = nums.size();\n    vint64m1_t v_nums = {0};\n    vint64m1_t v_min_sum = {INT64_MAX};\n    \n    for (int i = 0; i < size; i += VLMAX * 8) {\n        v_nums = vlse64_v_i64m1(&nums[i], v_nums, 8);\n        vint64m1_t v_prefix_sum = vslide1down_vx_i64m1(vadd_vv_i64m1(vnums, vext_vv_i64m1(vnums, vnums, 1)), 1);\n        v_min_sum = vmin_vv_i64m1(v_min_sum, v_nums);\n        v_min_sum = vmin_vv_i64m1(v_min_sum, v_prefix_sum);\n    }\n    \n    int64_t result;\n    vse64_v_i64m1(&result, v_min_sum, 8);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int lane_size = vsetvlmax_e64m1();\n    const int num_lanes = nums.size() / lane_size + (nums.size() % lane_size != 0);\n    vint64m1_t v_min_sum = vmv_s_x_i64m1(0);\n    \n    for (int i = 0; i < num_lanes; i++) {\n        vint64m1_t v_sum = vmv_s_x_i64m1(0);\n        vint64m1_t v_min = vmv_s_x_i64m1(0);\n        \n        for (int j = 0; j < lane_size; j++) {\n            int idx = i * lane_size + j;\n            if (idx < nums.size()) {\n                vint64m1_t v_num = vle64_v_i64m1(&nums[idx]);\n                v_sum = vadd_vv_i64m1(v_sum, v_num);\n                v_min = vmin_vx_i64m1(v_min, v_sum);\n            }\n        }\n        \n        v_min_sum = vmin_vv_i64m1(v_min_sum, v_min);\n    }\n    \n    int64_t result;\n    vse64_v_i64m1(&result, v_min_sum);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int VL = vsetvl_e64m1(0);\n\n    int64_t min_sum = nums[0];\n    int64_t current_sum = 0;\n\n    vint64m1_t v_current_sum = vmv_s_x_i64m1(0, 0);\n    vint64m1_t v_min_sum = vmv_s_x_i64m1(0, min_sum);\n\n    for (size_t i = 0; i < nums.size(); i += VL) {\n        vint64m1_t v_nums = vle64_v_i64m1(&nums[i]);\n\n        v_current_sum = vadd_vv_i64m1(v_current_sum, v_nums);\n        v_min_sum = vmin_vx_i64m1(v_min_sum, v_current_sum);\n\n        vbool64_t mask = vmseq_vx_i64m1_b64(v_current_sum, 0);\n        v_current_sum = vmerge_vvm_i64m1(mask, v_current_sum, vmv_s_x_i64m1(0, 0));\n    }\n\n    v_min_sum = vslide1down_vx_i64m1(v_min_sum, 1);\n    min_sum = vslide1down_vx_i64m1_i64(v_min_sum, 0);\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int buckets = 0;\n    \n    vint8_t v_capacity = vmv_s_x(capacity);\n    vint8_t v_zeros = vmv_s_x(0);\n    vint8_t v_ones = vmv_s_x(1);\n    \n    for (int i = 0; i < rows; i += 8) {\n        vint8_t v_sum = vzeros8();\n        for (int j = 0; j < cols; j++) {\n            vint8_t v_wells = vle8_v_i(&grid[i][j], cols - j < 8 ? cols - j : 8);\n            v_sum = vadd_vv_i(v_sum, v_wells);\n            vbool8_t mask = vmsle_vx_i(v_sum, v_capacity);\n            vint8_t v_extracted = vand_vv_v(v_capacity, mask);\n            v_sum = vsub_vv_v(v_sum, v_extracted);\n            buckets += vpopc_m(mask);\n        }\n    }\n    \n    return buckets;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int buckets = 0;\n    int total_ops = 0;\n    \n    for (int i = 0; i < rows; i += VL) {\n        vint32m1_t v_sum = vmv_v_x_i32m1(0, VL);\n        \n        for (int j = 0; j < cols; j++) {\n            vint32m1_t v_wells = vle32_v_i32m1(&grid[i][j], VL);\n            v_sum = vadd_vv_i32m1(v_sum, v_wells);\n            \n            if ((j + 1) % capacity == 0 || j == cols - 1) {\n                vint32m1_t v_buckets = vdiv_vx_i32m1(v_sum, capacity);\n                v_buckets = vwaddu_vx_i32m1(v_buckets, 1);\n                v_buckets = vredsum_vs_i32m1_i32m1(v_buckets, v_buckets, 0);\n                buckets += v_buckets[0];\n                total_ops += 1;\n                v_sum = vmv_v_x_i32m1(0, VL);\n            }\n        }\n    }\n    \n    return total_ops * buckets;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int buckets = 0;\n    \n    vint32m4_t v_capacity = vmv_v_x_i32m4(capacity, VL);\n    vint32m4_t v_buckets = vmv_v_x_i32m4(0, VL);\n    \n    for (int i = 0; i < rows; i += VL) {\n        vint32m4_t v_sum = vmv_v_x_i32m4(0, VL);\n        \n        for (int j = 0; j < cols; j++) {\n            vbool8_t mask = vmseq_vx_i32m4_b8(vle32_v_i32m4(vrgather_vx_i32m4_v_i32m4(&grid[i][j], v_capacity, VL), 0, VL), 1, VL);\n            v_sum = vadd_vv_i32m4(v_sum, vand_vx_i32m4(v_capacity, mask, VL), VL);\n        }\n        \n        v_buckets = vadd_vv_i32m4(v_buckets, v_sum, VL);\n    }\n    \n    buckets = vredsum_vs_i32m4_i32(v_buckets, vmv_x_s_i32m4(0, VL), VL);\n    \n    return buckets;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total = 0;\n    int remaining = 0;\n    int buckets = 0;\n    \n    vint8_t v_capacity = {capacity, capacity, capacity, capacity, capacity, capacity, capacity, capacity};\n    \n    for (int i = 0; i < rows; i += 8) {\n        vint8_t v_total = {0, 0, 0, 0, 0, 0, 0, 0};\n        vint8_t v_remaining = {0, 0, 0, 0, 0, 0, 0, 0};\n        \n        for (int j = 0; j < cols; j++) {\n            vint8_t v_wells = *(vint8_t*)&grid[i + 0][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 1][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 2][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 3][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 4][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 5][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 6][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n            \n            v_wells = *(vint8_t*)&grid[i + 7][j];\n            v_total = vadd(v_total, v_wells);\n            v_remaining = vmin(vadd(v_remaining, v_wells), v_capacity);\n        }\n        \n        vint8_t v_buckets = vdivu(v_total, v_capacity);\n        vint8_t v_diff = vsub(v_buckets, v_remaining);\n        vint8_t v_zeros = vand(v_diff, vdiff(v_diff, vdiff(v_diff, v_diff)));\n        buckets += vpopc(v_zeros);\n    }\n    \n    return buckets;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int count = 0;\n    \n    vint8_t v_capacity = vmv_s_x(capacity);\n    vint8_t v_count = vmv_s_x(0);\n    \n    for (int i = 0; i < rows; i += VL) {\n        vint8_t v_sum = vmv_s_x(0);\n        for (int j = 0; j < cols; j++) {\n            vint1_t v_wells = vle8_v_i(&grid[i][j], VL);\n            v_sum = vadd_vv_i(v_sum, v_wells);\n        }\n        vint8_t v_diff = vsub_vv_i(v_capacity, v_sum);\n        vint8_t v_zeros = vle8_v_i(&grid[i][0], VL);\n        vint8_t v_ones = vsub_vv_i(v_capacity, v_zeros);\n        vint8_t v_result = vsub_vv_i(v_ones, v_diff);\n        v_count = vadd_vv_i(v_count, v_result);\n    }\n    \n    int result[VL];\n    vse8_v_i(result, v_count);\n    \n    int total_count = 0;\n    for (int i = 0; i < VL; i++) {\n        total_count += result[i];\n    }\n    \n    return total_count;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    vint16m4_t vec = vle16_v_i16m4(&lst[0]);\n\n    vbool8_t mask = vmsne_vx_i16m4_b8(vec, 0);\n    vbool8_t mask_even = vmseq_vx_i16m4_b8(vmv_v_x_i16m4(vec, 1), 0);\n\n    vint16m4_t odd_elements = vmerge_vvm_i16m4(vec, 0, mask);\n    vint16m4_t odd_elements_even_pos = vmerge_vvm_i16m4(odd_elements, 0, mask_even);\n\n    vint32m8_t sum = vwmulu_vv_i32m8(odd_elements_even_pos, 1);\n    vint64m1_t final_sum = vredsum_vs_i32m8_i64m1(sum, 0);\n\n    return vslide1down_vx_i64(final_sum, 0);\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    vint16m4_t vec = vle16_v_i16m4(&lst[0]);\n    vint16m4_t mask = vmseq_vx_i16m4(vec, 1);\n    vint16m4_t even_positions = vand_vx_i16m4(vmv_v_x_i16m4(mask), 0xAAAAAAAA);\n    vint16m4_t odd_elements = vand_vx_i16m4(vec, 0x55555555);\n    vint16m4_t result = vredsum_vs_i16m4_i16m1(even_positions, vmand_mm_i16m4(mask, odd_elements));\n    return vslide1down_vx_i16(result, 0);\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    const int vl = vsetvl_e16m1();\n    vint16m1_t vec = vle16_v_i16m1(&lst[0], vl);\n\n    vbool2_t mask = VM_SEQ_2_E16M1(vl);\n    vint16m1_t odd_elements = vslide1down_vx_i16m1(vcompress_vm_v_i16m1(mask, vec), 1);\n\n    vint16m1_t even_positions = vslide1down_vx_i16m1(vec, 1);\n    vint16m1_t odd_elements_in_even_positions = vand_vv_i16m1(odd_elements, even_positions);\n\n    vint16m1_t odd_elements_in_even_positions_masked = vand_vx_i16m1(odd_elements_in_even_positions, 0x5555);\n\n    vint16m1_t sum = vredsum_vs_i16m1_i16m1(odd_elements_in_even_positions_masked, 0, vl);\n\n    return vslide1down_vx_i16m1(sum, 1)[0];\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    vint16m4_t vec = vle16_v_i16m4(&lst[0]);\n    vbool8_t mask = VMSEQ_VI_I16M4(vec, 1);\n    vint16m4_t odd_elements = VMSGT_VX_I16M4(vec, 0) & mask;\n    vint16m4_t even_positions = VMSEQ_VI_I16M4(vec, 0) & mask;\n    vint16m4_t result = vslide1down_vx_i16m4(odd_elements, 0) & even_positions;\n    return vredsum_vs_i16m4_i64(result);\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    const int16_t mask_odd = 0x5555; // 0101 0101 0101 0101\n    const int16_t mask_even = 0xaaaa; // 1010 1010 1010 1010\n    int64_t sum = 0;\n\n    vint16m4_t v_lst = *reinterpret_cast<const vint16m4_t*>(lst.data());\n    vint16m4_t v_odd_elements = vslide1down_vx_i16m4(v_lst, 1) & mask_odd;\n\n    vint16m4_t v_sum = vmv_v_x_i16m4(0);\n\n    for (size_t i = 0; i < v_odd_elements.vlen; i++) {\n        v_sum = vadd_vv_i16m4(v_sum, v_odd_elements);\n        v_odd_elements = vslide1down_vx_i16m4(v_odd_elements, v_odd_elements.vlen);\n    }\n\n    sum = vredsum_vs_i16_i64m1_i64(sum, v_sum);\n\n    return sum;\n}\n```  "}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    vint32m4_t v_arr = vle32_v_i32m4(arr.data());\n    vint32m4_t v_sum = vmv_v_x_i32m4(0);\n\n    for (int i = 0; i < k; i += VL) {\n        vbool8_t mask = vmsge_vx_i32m4_b8(vslideup_vx_i32m4(vmsltu_vx_i32m4_b8(v_arr, 100), 1), i);\n        vint32m4_t v_filtered = vand_vv_i32m4(v_arr, mask);\n        v_sum = vadd_vv_i32m4(v_sum, vredsum_vs_i32m4_i32m1(v_filtered, v_sum));\n    }\n\n    int sum = vredsum_vs_i32m4_i32m1(v_sum, v_sum);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    vint32m4_t v_arr = vle32_v_i32m4(arr.data());\n    vint32m4_t v_sum = vmv_v_x_i32m4(0);\n\n    for (int i = 0; i < k; i += 4) {\n        vbool8_t mask = vmseq_vx_i32m4_b8(vslide1down_vx_i32m4(v_arr, i, 0), 0);\n        vint32m4_t v_filtered = vmerge_vvm_i32m4(mask, v_arr, 0);\n        v_sum = vadd_vv_i32m4(v_sum, vredsum_vs_i32m4_i32m1(vfiltered, vfiltered, 0));\n    }\n\n    int result;\n    vse32_v_i32m4(&result, v_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    vint32m4_t v_arr = vle32_v_i32m4(arr.data());\n    vint32m4_t v_mask = vmsne_vx_i32m4_b4(v_arr, 0);\n    vint32m4_t v_digits = vsetvl_e32m4(k);\n    vint32m4_t v_sum = vmv_v_x_i32m4(0);\n\n    vint32m4_t v_ones = vsub_vx_i32m4(v_mask, 1);\n    vint32m4_t v_tens = vsub_vx_i32m4(v_mask, 10);\n    vint32m4_t v_under_100 = vand_vv_i32m4(v_ones, v_tens);\n\n    vbool8_t v_pred = vmsltu_vx_i32m4_b8(v_under_100, 100);\n    v_sum = vfredsum_vs_i32m4_i32m1(v_sum, v_under_100, v_sum, v_pred);\n\n    return vslide1down_vx_i32m4(v_sum, 1);\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    vint32m4_t v_arr = vle32_v_i32m4(arr.data());\n    vint32m4_t v_sum = vmv_v_x_i32m4(0);\n\n    for (int i = 0; i < k; i += 4) {\n        vbool8_t mask = vmsge_vx_i32m4_b8(v_arr, 10);\n        vint32m4_t v_filtered = vmerge_vim_i32m4(mask, v_arr, 0);\n        v_sum = vadd_vv_i32m4(v_sum, v_filtered);\n        v_arr = vslide1down_vx_i32m4(v_arr, 1);\n    }\n\n    int sum = vmv_x_s_i32(vredsum_vs_i32m4_i32(v_sum, 0));\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    vint32m4_t v_arr = vle32_v_i32m4(arr.data());\n    vint32m4_t v_sum = vmv_v_x_i32m4(0);\n\n    for (int i = 0; i < k; i += VL) {\n        vbool8_t mask = vmseq_vx_i32m4_b8(vslideup_vx_i32m4(vmslt_vx_i32m4_b8(v_arr, 99), 1), 0);\n        vint32m4_t v_filtered = vmerge_vvm_i32m4(mask, v_arr, 0);\n        v_sum = vadd_vv_i32m4(v_sum, vredsum_vs_i32m4_i32m1(vfiltered, v_sum, 0));\n        v_arr = vslide1down_vx_i32m4(v_arr, 1);\n    }\n\n    int result;\n    vse32_v_i32m4(&result, v_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvlmax_e32m1();\n    int remaining = lst.size();\n    int i = 0;\n    \n    while (remaining >= vector_size) {\n        vint32m1_t vec = vle32_v_i32m1(&lst[i]);\n        vbool32_t mask = vmseq_vx_i32m1_b32(vec, 0);\n        vbool32_t sorted = vmsle_vx_i32m1_b32(vec, 0);\n        if (!vmand_mm_b32(mask, sorted)) {\n            return false;\n        }\n        i += vector_size;\n        remaining -= vector_size;\n    }\n    \n    if (remaining > 1) {\n        vint32m1_t vec = vle32_v_i32m1(&lst[i]);\n        vbool32_t mask = vmseq_vx_i32m1_b32(vec, 0);\n        vbool32_t sorted = vmsle_vx_i32m1_b32(vec, 0);\n        if (!vmand_mm_b32(mask, sorted)) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvl_e32m1();\n    int remaining = lst.size();\n    int i = 0;\n    \n    while (remaining >= vector_size) {\n        vint32m1_t vec = vle32_v_i32m1(&lst[i]);\n        vint32m1_t next_vec = vle32_v_i32m1(&lst[i + vector_size]);\n        vbool32_t cmp = vmsltu_vx_i32m1_b32(vec, next_vec);\n        \n        if (!vmfirst_m_b32(cmp)) {\n            return false;\n        }\n        \n        i += vector_size;\n        remaining -= vector_size;\n    }\n    \n    for (; i < lst.size() - 1; i++) {\n        if (lst[i] >= lst[i + 1]) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvl_e32m1();\n    int num_elements = lst.size();\n    int num_vectors = (num_elements + vector_size - 1) / vector_size;\n    \n    vint32m1_t input, next_input;\n    vbool32_t result = vmsltu_vx_i32m1_b32(vmv_v_x_i32m1(lst.data(), num_elements), 0, num_elements);\n    \n    for (int i = 0; i < num_vectors - 1; i++) {\n        input = vle32_v_i32m1(&lst[i * vector_size]);\n        next_input = vle32_v_i32m1(&lst[(i + 1) * vector_size]);\n        result = vmand_mm_b32(result, vmsleu_vv_i32m1_b32(input, next_input, vector_size));\n    }\n    \n    int remaining_elements = num_elements % vector_size;\n    if (remaining_elements > 0 && num_vectors > 1) {\n        input = vle32_v_i32m1(&lst[(num_vectors - 1) * vector_size]);\n        next_input = vle32_v_i32m1(&lst[num_elements - remaining_elements]);\n        result = vmand_mm_b32(result, vmsleu_vv_i32m1_b32(input, next_input, remaining_elements));\n    }\n    \n    return vmfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int size = lst.size();\n    const int lane_size = vsetvl_e32m1();\n    const int full_lanes = size / lane_size;\n    \n    vint32m1_t vec = vle32_v_i32m1(&lst[0]);\n    vbool32_t result = vmsle_vx_i32m1_b32(vec, vslideup_vx_i32m1(vec, 1, 0), 1);\n    \n    for (int i = 1; i < full_lanes; i++) {\n        vint32m1_t vec1 = vle32_v_i32m1(&lst[i * lane_size]);\n        vint32m1_t vec2 = vle32_v_i32m1(&lst[i * lane_size + 1]);\n        vbool32_t cmp = vmsle_vx_i32m1_b32(vec1, vec2, 1);\n        result = vmand_mm_b32(result, cmp);\n    }\n    \n    int remaining = size % lane_size;\n    if (remaining > 1) {\n        vint32m1_t vec1 = vle32_v_i32m1(&lst[full_lanes * lane_size]);\n        vint32m1_t vec2 = vle32_v_i32m1(&lst[full_lanes * lane_size + 1]);\n        vbool32_t cmp = vmsle_vx_i32m1_b32(vec1, vec2, 1);\n        result = vmand_mm_b32(result, cmp);\n    }\n    \n    bool final_result = vmfirst_m_b32(result);\n    \n    return final_result;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int size = lst.size();\n    if(size <= 1) return true;\n\n    vint32m1_t vec = vle32_v_i32m1(&lst[0], size);\n\n    vbool32_t result = vmsltu_vx_i32m1_b32(vec, vslide1down_vx_i32m1(vec, 1), 1);\n\n    bool sorted = vmfirst_m_b32(result) == -1;\n\n    return sorted;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    const int VL = vsetvl_e32m1();\n    vint32m1_t vec_sum = vmv_v_x_i32m1(0, VL);\n    vint32m1_t vec_sign_prod = vmv_v_x_i32m1(1, VL);\n\n    for(size_t i = 0; i < arr.size(); i += VL){\n        vint32m1_t vec_data = vle32_v_i32m1(&arr[i], VL);\n\n        vint32m1_t vec_abs = vabs_vx_i32m1(vec_data);\n        vint32m1_t vec_sign = vdiv_vx_i32m1(vec_data, vec_abs);\n\n        vec_sum = vwmulu_vv_i32m1(vec_sum, vec_abs);\n        vec_sign_prod = vwmulu_vv_i32m1(vec_sign_prod, vec_sign);\n    }\n\n    int sum = vmv_x_s_i32_i32m1_i32(vec_sum);\n    int sign_prod = vmv_x_s_i32_i32m1_i32(vec_sign_prod);\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    const int vl = vsetvl_e32m1();\n\n    vint32m1_t vec_sum = vmv_v_x_i32m1(0, vl);\n    vint32m1_t vec_signs = vmv_v_x_i32m1(1, vl);\n\n    for(size_t i = 0; i < arr.size(); i += VL) {\n        vint32m1_t vec_data = vle32_v_i32m1(&arr[i], vl);\n        vint32m1_t vec_abs = vabs_v_i32m1(vec_data, vl);\n        vint32m1_t vec_mul = vmul_vv_i32m1(vec_abs, vec_signs, vl);\n        vec_sum = vredsum_vs_i32m1_i32m1(vec_sum, vec_mul, vec_sum, vl);\n        vec_signs = vwmul_vv_i32m1(vec_signs, vec_data, vl);\n    }\n\n    int result;\n    vse32_v_i32m1(&result, vec_sum, vl);\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    const int vl = vsetvl_e32m1();\n\n    vint32m1_t vec_sum = vmv_v_x_i32m1(0, vl);\n    vint32m1_t vec_sign_product = vmv_v_x_i32m1(1, vl);\n\n    for(size_t i = 0; i < arr.size(); i += VL) {\n        const int vl_actual = std::min(static_cast<int>(arr.size() - i), vl);\n\n        vint32m1_t vec_data = vle32_v_i32m1(&arr[i], vl_actual);\n\n        vint32m1_t vec_sign = vmslt_vx_i32m1_b(vl_actual, vec_data, 0, vl_actual);\n        vec_sign_product = vmul_vv_i32m1(vec_sign_product, vec_sign, vl_actual);\n\n        vint32m1_t vec_abs = vabs_v_i32m1(vec_data, vl_actual);\n        vec_sum = vwmacc_vv_i32m1(vec_sum, vec_abs, vec_sign, vl_actual);\n    }\n\n    int result = vredsum_vs_i32_i32m1_i32(vec_sum, 0, vl);\n    result *= vredsum_vs_i32_i32m1_i32(vec_sign_product, 0, vl);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    const int VL = vsetvl_e32m1();\n\n    vint32m1_t vec_sum = vmv_v_x_i32m1(0, VL);\n    vint32m1_t vec_sign_prod = vmv_v_x_i32m1(1, VL);\n\n    for(size_t i = 0; i < arr.size(); i += VL){\n        vint32m1_t vec_data = vle32_v_i32m1(&arr[i], VL);\n\n        vint32m1_t vec_sign = vmslt_vx_i32m1_b(vec_data, 0, VL);\n        vec_sign_prod = vwmul_vv_i32m1(vec_sign_prod, vec_sign, VL);\n\n        vint32m1_t vec_abs = vabs_v_i32m1(vec_data, VL);\n        vec_sum = vwadd_vv_i32m1(vec_sum, vec_abs, VL);\n    }\n\n    int result = vslide1down_vx_i32(vec_sum);\n    result *= vslide1down_vx_i32(vec_sign_prod);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    const int lane_size = vsetvl_e32m1();\n\n    vint32m1_t sum = vmv_v_x_i32m1(0, lane_size);\n    vint32m1_t signs_product = vmv_v_x_i32m1(1, lane_size);\n\n    for(size_t i = 0; i < arr.size(); i += VL) {\n        vint32m1_t vec = vle32_v_i32m1(&arr[i], lane_size);\n\n        vbool32_t is_positive = vmslt_vx_i32m1_b32(vec, 0, lane_size);\n        vbool32_t is_negative = vmsgt_vx_i32m1_b32(vec, 0, lane_size);\n\n        vint32m1_t abs_vec = vabs_v_i32m1(vec, lane_size);\n        vint32m1_t signs = vmerge_vvm_i32m1(is_positive, is_negative, 0, lane_size);\n\n        sum = vwmul_vv_i32m1(sum, abs_vec, lane_size);\n        signs_product = vwmul_vv_i32m1(signs_product, signs, lane_size);\n    }\n\n    int result = vredsum_vs_i32(sum, vmv_x_s_i32(0, lane_size), lane_size) * vredsum_vs_i32(signs_product, vmv_x_s_i32(0, lane_size), lane_size);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    const int N = grid.size();\n    vint32m1_t path = vmv_v_x_i32m1(0, 0); // Initialize path vector\n    vint32m1_t indices = vmv_v_x_i32m1(0, 0); // Initialize indices vector\n\n    for(int i = 0; i < k; i++){\n        vint32m1_t min_val = vmv_v_x_i32m1(N * N + 1, 0); // Initialize min_val vector\n        vint32m1_t min_idx = vmv_v_x_i32m1(0, 0); // Initialize min_idx vector\n\n        for(int r = 0; r < N; r++){\n            for(int c = 0; c < N; c++){\n                vint32m1_t current_val = vmv_v_x_i32m1(grid[r][c], 0); // Get current value\n                vbool32_t mask = vmslt_vx_i32m1_b32(current_val, min_val, 0); // Compare current value with min_val\n\n                min_val = vmerge_vvm_v_i32m1(mask, current_val, min_val); // Update min_val\n                min_idx = vmerge_vvm_v_i32m1(mask, vmv_v_x_i32m1(r * N + c + 1, 0), min_idx); // Update min_idx\n            }\n        }\n\n        path = vmerge_vvm_v_i32m1(vmv_x_s_i32m1(0, 0), min_val, path); // Update path\n        indices = vmerge_vvm_v_i32m1(vmv_x_s_i32m1(0, 0), min_idx, indices); // Update indices\n    }\n\n    std::vector<int> result(k);\n    vse32_v_i32m1(&result[0], path); // Store path values in result vector\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    const int total_cells = N * N;\n\n    vint32m1_t path = vmv_v_x_i32m1(0, 0); // Initialize path vector\n    vint32m1_t indices = vmv_v_x_i32m1(0, 0); // Initialize indices vector\n\n    for (int i = 0; i < k; i++) {\n        vint32m1_t values = vle32_v_i32m1(&grid[0][0] + i, total_cells - i); // Load values from grid\n        vbool32_t mask = vmsltu_vx_i32m1_b32(values, total_cells - i); // Create mask for valid values\n        vint32m1_t selected_values = vcompress_vvm_i32m1(mask, values); // Compress valid values\n        vint32m1_t selected_indices = vcompress_vvm_i32m1(mask, indices); // Compress corresponding indices\n\n        vint32m1_t min_value = vfirst_vv_i32m1(selected_values, selected_indices); // Find minimum value\n        path = vmerge_vvm_i32m1(mask, min_value, path); // Update path vector\n        indices = vadd_vx_i32m1(indices, 1); // Increment indices\n    }\n\n    std::vector<int> result(k);\n    vse32_v_i32m1(&result[0], path, k); // Store result vector\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    const int N = grid.size();\n    vint32m1_t path = vmv_v_x_i32m1(0, 0); // Initialize path vector\n    vint32m1_t indices = vmv_v_x_i32m1(0, 0); // Initialize indices vector\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n\n    for (int i = 0; i < k; i++) {\n        vint32m1_t min_val = vmv_v_x_i32m1(N * N, 0); // Initialize min_val vector\n        vint32m1_t min_idx = vmv_v_x_i32m1(0, 0); // Initialize min_idx vector\n\n        for (int r = 0; r < N; r++) {\n            for (int c = 0; c < N; c++) {\n                vint32m1_t current_val = vmv_v_x_i32m1(grid[r][c], 0); // Get current value\n                vbool32_t mask = vmslt_vx_i32m1_b32(current_val, min_val, 0); // Compare current value with min_val\n\n                min_val = vmerge_vvm_v_i32m1(mask, current_val, min_val); // Update min_val\n                min_idx = vmerge_vvm_v_i32m1(mask, vmv_v_x_i32m1(r * N + c + 1, 0), min_idx); // Update min_idx\n            }\n        }\n\n        path = vmerge_vvm_v_i32m1(vmv_v_x_i32m1(0, 0), min_val, path); // Update path vector\n        indices = vmerge_vvm_v_i32m1(vmv_v_x_i32m1(0, 0), min_idx, indices); // Update indices vector\n\n        vint32m1_t mask = vmsne_vx_i32m1_b32(min_val, vmv_v_x_i32m1(N * N, 0), 0); // Check if min_val is not N*N\n        if (!vmand_mm_b32(mask, vmand_mm_b32(vmsltu_vx_i32m1_b32(indices, path, 0), vmsltu_vx_i32m1_b32(indices, vmv_v_x_i32m1(N * N, 0), 0))) {\n            break; // Break if condition is met\n        }\n    }\n\n    for (int i = 0; i < k; i++) {\n        result = vmerge_vvm_v_i32m1(vmsltu_vx_i32m1_b32(indices, vmv_v_x_i32m1(N * N, 0), 0), path, result); // Update result vector\n        path = vslide1down_vx_i32m1(path, 0); // Shift path vector down\n        indices = vslide1down_vx_i32m1(indices, 0); // Shift indices vector down\n    }\n\n    std::vector<int> res(k);\n    vse32_v_i32m1(&res[0], result);\n    return res;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    vint32m1_t path = vmv_v_x_i32m1(0, 0); // Initialize path vector\n    vint32m1_t indices = vmv_v_x_i32m1(0, 0); // Initialize indices vector\n    vint32m1_t result = vmv_v_x_i32m1(0, 0); // Initialize result vector\n\n    for (int i = 0; i < k; i++) {\n        vint32m1_t values = vle32_v_i32m1(&grid[0][0] + i, N * N); // Load values from grid\n        vbool32_t mask = vmsltu_vx_i32m1_b32(values, vmv_x_s_i32m1(0, N * N)); // Mask to filter out-of-bounds values\n        vint32m1_t filtered_values = vmerge_vvm_i32m1(mask, values, vmv_v_x_i32m1(0, 0)); // Apply mask to values\n\n        vint32m1_t min_value = vslide1down_vx_i32m1(filtered_values, 1); // Shift values to find minimum\n        vbool32_t min_mask = vmsltu_vv_i32m1_b32(filtered_values, min_value); // Mask for minimum values\n        vint32m1_t min_indices = vid_v_i32m1(N * N); // Create vector of indices\n        vint32m1_t min_indices_shifted = vslide1down_vx_i32m1(min_indices, 1); // Shift indices\n        vint32m1_t result_indices = vmerge_vvm_i32m1(min_mask, min_indices_shifted, min_indices); // Apply mask to indices\n\n        path = vmerge_vvm_i32m1(min_mask, filtered_values, path); // Update path with minimum values\n        indices = vmerge_vvm_i32m1(min_mask, result_indices, indices); // Update indices with minimum indices\n    }\n\n    vse32_v_i32m1(&result[0], path, N); // Store final path in result vector\n    return std::vector<int>(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    const int N = grid.size();\n    vint32m8_t vgrid[N];\n    for (int i = 0; i < N; i++) {\n        vgrid[i] = vle32_v_i32m8(&grid[i][0], N);\n    }\n\n    vint32m8_t result = vgrid[0];\n    for (int i = 1; i < k; i++) {\n        vint32m8_t neighbors[4];\n        neighbors[0] = vslide1down_vx_i32m8(result, 1);\n        neighbors[1] = vslide1up_vx_i32m8(result, 1);\n        neighbors[2] = vslide1left_vx_i32m8(result, 1);\n        neighbors[3] = vslide1right_vx_i32m8(result, 1);\n\n        vint32m8_t min_neighbor = vmin_vv_i32m8(neighbors[0], neighbors[1]);\n        min_neighbor = vmin_vv_i32m8(min_neighbor, neighbors[2]);\n        min_neighbor = vmin_vv_i32m8(min_neighbor, neighbors[3]);\n\n        result = min_neighbor;\n    }\n\n    std::vector<int> output(k);\n    vse32_v_i32m8(&output[0], result, k);\n    return output;\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    vuint8m1_t digits = vmv_s_x_u8m1(n);\n    vuint8m1_t odd_digits = vmand_u8_b(digits, vmv_x_s_u8m1(0x55));\n    vuint8m1_t result = vredsumu_vs_u8m1_u64m1(vmand_u8_b(odd_digits, vmv_x_s_u8m1(0x0F)));\n    return vslide1down_u64(result);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    vuint8m1_t digits = vmv_x_s_u8m1(n);\n    vuint8m1_t odd_digits = vmsne_vx_u8m1(digits, 0);\n    vuint8m1_t odd_digits_filtered = vmand_vv_u8m1(odd_digits, vmv_x_s_u8m1(0x5555555555555555));\n    vuint64m1_t result = vwmul_vv_u64m1_v_u8m1(odd_digits_filtered, digits);\n    return vmsumu_vx_u64m1(result);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    vuint8m1_t digits = vmv_x_s_u8m1(n);\n    vuint8m1_t odd_digits = vmv_s_x_u8m1(1);\n    vbool8_t odd_mask = vmsne_vx_u8m1_b8(digits, 0);\n    vuint8m1_t odd_digits_masked = vmerge_vxm_u8m1(odd_digits, 0, odd_mask);\n    vuint8m1_t result = vredsum_vs_u8m1_u8m1(odd_digits_masked);\n    return vslide1down_vx_u8(result);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    vuint8m1_t digits = vmv_x_s_u8m1(n);\n    vuint8m1_t odd_digits = vmsne_vx_u8m1(digits, 0);\n    vuint8m1_t odd_digits_filtered = vmand_vv_u8m1(odd_digits, vmv_x_s_u8m1(0x5555555555555555));\n    vuint64m1_t product = vwmul_vv_u64m1_v_u8m1(1, odd_digits_filtered);\n    return vmsumu_vx_u64m1_u64(product);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    vuint8m1_t digits = vmv_x_s_u8m1(n);\n    vuint8m1_t odd_digits = vmsne_vx_u8m1(digits, 0);\n    vuint8m1_t odd_digits_filtered = vmand_vv_u8m1(odd_digits, vmv_x_s_u8m1(0x5555555555555555));\n    vuint64m1_t result = vwmulu_vv_u64m1_v_u8m1(odd_digits_filtered, odd_digits_filtered);\n    return vmsumu_vx_u64m1_u64m1(result);\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_E32, RVV_M8);\n    const int num_elements = lst.size();\n    int remainder = num_elements % vector_size;\n    \n    vfloat32m8_t vec_input;\n    vint32m8_t vec_rounded;\n    vint32m8_t vec_squared;\n    vint64m8_t vec_sum = vmv_v_x_i64m8(0, 0, vector_size);\n    \n    uint64_t sum = 0;\n    \n    for (int i = 0; i < num_elements - remainder; i += vector_size) {\n        vec_input = vle32_v_f32m8(&lst[i]);\n        vec_rounded = vfceil_vf_v_i32m8_b8m1(vec_input);\n        vec_squared = vwadd_vv_i32m8(vec_rounded, vec_rounded);\n        vec_sum = vwadd_vv_i64m8(vec_sum, vwmulu_vv_i64m8(vec_squared, vec_squared));\n    }\n    \n    int64_t sum_array[vector_size];\n    vse64_v_i64m8(sum_array, vec_sum);\n    \n    for (int i = 0; i < vector_size; i++) {\n        sum += sum_array[i];\n    }\n    \n    for (int i = num_elements - remainder; i < num_elements; i++) {\n        sum += ceil(lst[i]) * ceil(lst[i]);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_E32, RVV_M8);\n    const int num_elements = lst.size();\n    const int num_chunks = (num_elements + vector_size - 1) / vector_size;\n    \n    float* rounded_values = (float*)malloc(vector_size * sizeof(float));\n    uint64_t sum = 0;\n    \n    for (int i = 0; i < num_chunks; i++) {\n        int start = i * vector_size;\n        int end = start + vector_size;\n        vfloat32m8_t v_input = vle32_v_f32m8(&lst[start]);\n        \n        v_input = vfceil_vf_f32m8(v_input);\n        vse32_v_f32m8(rounded_values, v_input);\n        \n        for (int j = 0; j < vector_size && start + j < num_elements; j++) {\n            sum += (uint64_t)(rounded_values[j] * rounded_values[j]);\n        }\n    }\n    \n    free(rounded_values);\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_E32, RVV_M8);\n    const int num_elements = lst.size();\n    const int num_vectors = (num_elements + vector_size - 1) / vector_size;\n\n    vfloat32m8_t vec_input;\n    vint32m8_t vec_input_rounded;\n    vint64m8_t vec_squared;\n    vint64m8_t vec_sum = vmv_v_x_i64m8(0, 0);\n\n    for (int i = 0; i < num_vectors; i++) {\n        int elements_in_vector = (i == num_vectors - 1) ? num_elements - i * vector_size : vector_size;\n\n        vec_input = vle32_v_f32m8(&lst[i * vector_size], elements_in_vector);\n        vec_input_rounded = vfceil_vf_v_i32m8(vec_input);\n        vec_squared = vwmul_vv_i64m8(vwadd_vv_i32m8(vec_input_rounded, vec_input_rounded), vwadd_vv_i32m8(vec_input_rounded, vec_input_rounded));\n        vec_sum = vwadd_vv_i64m8(vec_sum, vec_squared);\n    }\n\n    uint64_t sum = 0;\n    for (int i = 0; i < num_elements; i++) {\n        sum += vslide1down_vx_i64m8_i64(vec_sum, i * vector_size);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_E32, RVV_M8);\n    const int num_elements = lst.size();\n    const int num_vectors = num_elements / vector_size + (num_elements % vector_size == 0 ? 0 : 1);\n\n    vfloat32m8_t vec_input;\n    vint32m8_t vec_rounded;\n    vint32m8_t vec_squared;\n    vint64m8_t vec_sum = vmv_v_x_i64m8(0, 0);\n\n    for (int i = 0; i < num_vectors; i++) {\n        int elements_in_this_vector = (i == num_vectors - 1) ? num_elements % vector_size : vector_size;\n\n        vec_input = vle32_v_f32m8(&lst[i * vector_size], elements_in_this_vector);\n\n        vec_rounded = vfceil_vf_f32m8_i32m8(vec_input);\n        vec_squared = vwadd_vv_i32m8(vec_rounded, vec_rounded);\n        vec_squared = vwadd_vv_i32m8(vec_squared, vec_rounded); // Squaring by adding rounded number twice\n\n        vec_sum = vwredsum_vs_i32m8_i64m8(vec_sum, vec_squared, vec_squared);\n\n    }\n\n    uint64_t result;\n    vse64_v_i64m8(&result, vec_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_E32);\n    const int num_elements = lst.size();\n    const int num_vectors = (num_elements + vector_size - 1) / vector_size;\n\n    vfloat32m1_t vec_input;\n    vint32m1_t vec_rounded;\n    vint32m1_t vec_squared;\n    vint64m1_t vec_sum = vmv_v_x_i64m1(0, 0);\n\n    for (int i = 0; i < num_vectors; i++) {\n        int elements_in_vector = (i == num_vectors - 1) ? num_elements - i * vector_size : vector_size;\n\n        vec_input = vfmv_v_f_f32m1_m(mask, &lst[i * vector_size], elements_in_vector);\n        vec_rounded = vfceil_vf_f32m1_i32m1(vec_input);\n        vec_squared = vmul_vv_i32m1(vec_rounded, vec_rounded);\n        vec_sum = vredsum_vs_i32m1_i64m1(vec_sum, vec_squared, vec_squared);\n    }\n\n    uint64_t result;\n    vse64_v_i64m1(&result, vec_sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```\nint can_arrange_simd(const std::vector<int> & arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1();\n\n    if(size <= 1){\n        return -1;\n    }\n\n    int max_index = -1;\n    vint32m1_t vec = vle32_v_i32m1(&arr[0], size);\n\n    for(int i = 1; i < size; i += lane_size){\n        vint32m1_t prev = vslide1down_vx_i32m1(vec, 1);\n        vbool32_t mask = vmslt_vx_i32m1_b32(prev, vec, 0);\n        vint32m1_t indices = vid_v_i32m1();\n        vint32m1_t result = vmfirst_v_i32m1(mask, indices, 0);\n\n        int index = vfirst_m(mask);\n        if(index != -1){\n            max_index = i + index - 1;\n        }\n\n        vec = vle32_v_i32m1(&arr[i], size - i);\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1();\n\n    if(size <= 1){\n        return -1;\n    }\n\n    int max_index = -1;\n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n\n    for(int i = 1; i < size; i += lane_size){\n        const int remaining = size - i;\n        const int current_lanes = remaining < lane_size ? remaining : lane_size;\n\n        vint32m1_t next_vec = vle32_v_i32m1(&arr[i]);\n\n        vbool32_t cmp_result = vmsltu_vx_i32m1_b32(vec, next_vec, 1);\n        size_t index = vfirst_m_i32m1(cmp_result);\n\n        if(index < current_lanes){\n            max_index = i + index - 1;\n        }\n\n        vec = next_vec;\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```\nint can_arrange_simd(const std::vector<int> & arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1();\n    const int full_lanes = size / lane_size;\n    \n    vint32m1_t vec = {0};\n    vint32m1_t prev_vec = {0};\n    vbool32_t mask = VMSEQ_VI_I32M1(0, 0, lane_size);\n    int result = -1;\n\n    for (int i = 0; i < full_lanes; i++) {\n        vec = VLSEV_INT32M1(&arr[i * lane_size], 0, mask, vec);\n        vbool32_t cmp_mask = VMSEQ_VX_I32M1(vec, prev_vec);\n        vint32m1_t indices = VMV_V_XI_I32M1(i * lane_size, 0, mask);\n        vint32m1_t max_indices = VREDMAXU_VX_I32M1(indices, -1, cmp_mask);\n        result = VREDMAXU_VX_I32(result, -1, max_indices);\n        prev_vec = vec;\n    }\n\n    if (full_lanes * lane_size < size) {\n        vbool32_t last_mask = VMSEQ_VI_I32M1(0, 0, size - full_lanes * lane_size);\n        vec = VLSEV_INT32M1(&arr[full_lanes * lane_size], 0, last_mask, vec);\n        vbool32_t cmp_mask = VMSEQ_VX_I32M1(vec, prev_vec);\n        vint32m1_t indices = VMV_V_XI_I32M1(full_lanes * lane_size, 0, last_mask);\n        vint32m1_t max_indices = VREDMAXU_VX_I32M1(indices, -1, cmp_mask);\n        result = VREDMAXU_VX_I32(result, -1, max_indices);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```\nint can_arrange_simd(const std::vector<int> & arr){\n    const int vector_size = vsetvli(0, RVV_E32, RVV_M8);\n    const int num_elements = arr.size();\n    const int num_vectors = (num_elements + vector_size - 1) / vector_size;\n    \n    vint32m8_t vec_prev, vec_curr;\n    vint32m1_t mask;\n    int result = -1;\n    \n    for (int i = 0; i < num_vectors; i++) {\n        int start = i * vector_size;\n        int end = start + vector_size < num_elements ? start + vector_size : num_elements;\n        \n        vec_curr = vle32_v_i32m8(&arr[start]);\n        \n        if (i > 0) {\n            mask = vmslt_vx_i32m1_b8(vslide1down_vx_i32m8(vec_curr, 1, 0), vec_prev, 0);\n            int index = vfirst_m_i32m1(mask);\n            if (index >= 0) {\n                result = start + index - 1;\n                break;\n            }\n        }\n        \n        vec_prev = vec_curr;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    const int size = arr.size();\n    const int lane_size = vsetvl_e32m1();\n    const int full_lanes = size / lane_size;\n    const int remainder = size % lane_size;\n    \n    vint32m1_t vec = vle32_v_i32m1(&arr[0]);\n    vbool32_t mask = vmseq_vx_i32m1_b32(vec, 0, VL1);\n    \n    int result = -1;\n    for (int i = 0; i < full_lanes; i++) {\n        vint32m1_t prev = vslide1down_vx_i32m1(vec, 1, VL1);\n        vbool32_t cmp = vmsltu_vv_i32m1_b32(prev, vec, VL1);\n        vbool32_t masked_cmp = vmand_mm_b32(mask, cmp, VL1);\n        int index = vfirstb8(masked_cmp);\n        if (index != -1) {\n            result = i * lane_size + index;\n            break;\n        }\n        vec = vle32_v_i32m1(&arr[(i + 1) * lane_size]);\n    }\n    \n    if (remainder > 0 && result == -1) {\n        vint32m1_t prev = vslide1down_vx_i32m1(vec, 1, VL1);\n        vbool32_t cmp = vmsltu_vv_i32m1_b32(prev, vec, VL1);\n        vbool32_t masked_cmp = vmand_mm_b32(mask, cmp, VL1);\n        int index = vfirstb8(masked_cmp);\n        if (index != -1) {\n            result = full_lanes * lane_size + index;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()) return {0, 0};\n\n    vint32m4_t vec = vle32_v_i32m4(&lst[0]);\n\n    vint32m4_t neg_mask = vsetvl_e32m4(0);\n    vint32m4_t pos_mask = vsetvl_e32m4(0);\n\n    vint32m4_t neg_max = vfmv_v_f_i32m4(0, 0);\n    vint32m4_t pos_min = vfmv_v_f_i32m4(0, 0);\n\n    vbool8_t mask = vmseq_vx_i32m4_b8(vec, 0, neg_mask);\n    vint32m4_t neg_vec = vmerge_vvm_i32m4(mask, vec, vfmv_v_f_i32m4(0, 0));\n\n    mask = vmseq_vx_i32m4_b8(vec, 0, pos_mask);\n    vint32m4_t pos_vec = vmerge_vvm_i32m4(mask, vec, vfmv_v_f_i32m4(0, 0));\n\n    neg_max = vredmax_vs_i32m4_i32m4(neg_max, neg_vec, neg_mask);\n    pos_min = vredmin_vs_i32m4_i32m4(pos_min, pos_vec, pos_mask);\n\n    int result[2];\n    vse32_v_i32m4(result, neg_max);\n    vse32_v_i32m4(result + 1, pos_min);\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()){\n        return {0, 0};\n    }\n\n    vint32m1_t vec = vle32_v_i32m1(&lst[0]);\n\n    vint32m1_t neg_mask = vsetvl_e32m1(lst.size());\n    vint32m1_t pos_mask = vsetvl_e32m1(lst.size());\n\n    vint32m1_t neg_max = vssmax_vx_i32m1(vec, 0, neg_mask);\n    vint32m1_t pos_min = vsmin_vx_i32m1(vec, 0, pos_mask);\n\n    int32_t result[2];\n    vse32_v_i32m1(result, neg_max, 0);\n    vse32_v_i32m1(result + 1, pos_min, 0);\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()) {\n        return {0, 0};\n    }\n\n    vint32m4_t vec = vle32_v_i32m4(&lst[0]);\n\n    vint32m4_t neg_mask = vsetvl_e32m4(0);\n    vint32m4_t pos_mask = vsetvl_e32m4(0);\n    vint32m4_t zero_mask = vsetvl_e32m4(0);\n\n    vint32m4_t neg_max = vfmv_v_f_i32m4(0, 0, neg_mask);\n    vint32m4_t pos_min = vfmv_v_f_i32m4(0, 0, pos_mask);\n\n    vbool8_t mask = vmslt_vx_i32m4_b8(vec, 0, zero_mask);\n\n    vint32m4_t neg_values = vcompress_vm_i32m4(mask, vec);\n    vint32m4_t pos_values = vcompress_vm_i32m4(!mask, vec);\n\n    if(vfirst_m_i32m4(mask) != -1){\n        neg_max = vslide1down_vx_i32m4(neg_max, vfirst_m_i32m4(mask));\n    }\n\n    if(vfirst_m_i32m4(!mask) != -1){\n        pos_min = vslide1up_vx_i32m4(pos_min, vfirst_m_i32m4(!mask));\n    }\n\n    int32_t result[2];\n    vse32_v_i32m4(result, neg_max);\n    vse32_v_i32m4(result + 1, pos_min);\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()) return {0, 0};\n\n    vint32m4_t vec = vle32_v_i32m4(&lst[0]);\n\n    vint32m4_t neg_mask = vsetvl_e32m4(lst.size());\n    vint32m4_t pos_mask = vsetvl_e32m4(lst.size());\n\n    vint32m4_t neg = vrgather_vx_i32m4(vec, neg_mask, vmslt_vx_i32m4(vec, 0, neg_mask));\n    vint32m4_t pos = vrgather_vx_i32m4(vec, pos_mask, vmsgt_vx_i32m4(vec, 0, pos_mask));\n\n    int32_t max_neg = vfirst_m_i32m4(neg_mask, vredmax_vs_i32m4_i32m4(neg, 0, neg_mask));\n    int32_t min_pos = vfirst_m_i32m4(pos_mask, vredmin_vs_i32m4_i32m4(pos, 0, pos_mask));\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()){\n        return {0, 0};\n    }\n\n    vint32m4_t vec = vle32_v_i32m4(lst.data());\n\n    vint32m4_t neg_mask = vmslt_vx_i32m4_b(vec, 0, VL1);\n    vint32m4_t pos_mask = vmsgt_vx_i32m4_b(vec, 0, VL1);\n\n    vint32m4_t neg_vals = vmerge_vvm_i32m4(neg_mask, vec, 0, VL1);\n    vint32m4_t pos_vals = vmerge_vvm_i32m4(pos_mask, vec, 0, VL1);\n\n    vint32m1_t max_neg = vredmax_vs_i32m4_i32m1(neg_vals, 0, VL1);\n    vint32m1_t min_pos = vredminu_vs_i32m4_i32m1(pos_vals, 0, VL1);\n\n    int max_neg_scalar = vslide1down_vx_i32(vfmv_v_f_i32(vredmax_vs_i32m1_i32(max_neg)), 1);\n    int min_pos_scalar = vslide1down_vx_i32(vfmv_v_f_i32(vredminu_vs_i32m1_i32(min_pos)), 1);\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```\nuint64_t special_factorial_simd(uint64_t n){\n    vuint64m1_t acc = vmv_v_x_u64m1(1);\n    vuint64m1_t result = vmv_v_x_u64m1(1);\n    \n    for (uint64_t i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vmv_v_x_u64m1(i);\n        acc = vmul_vv_u64m1(acc, vec_i);\n        result = vmul_vv_u64m1(result, acc);\n    }\n    \n    uint64_t final_result[8];\n    vse64_v_u64m1(final_result, result);\n    \n    return final_result[0] * final_result[1] * final_result[2] * final_result[3] * final_result[4] * final_result[5] * final_result[6] * final_result[7];\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```\nuint64_t special_factorial_simd(uint64_t n){\n    vuint64m1_t acc = vmv_v_x_u64m1(1);\n    vuint64m1_t result = vmv_v_x_u64m1(1);\n    \n    for (uint64_t i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vmv_v_x_u64m1(i);\n        acc = vmul_vv_u64m1(acc, vec_i);\n        result = vmul_vv_u64m1(result, acc);\n    }\n    \n    uint64_t final_result;\n    vse64_v_u64m1(&final_result, result);\n    \n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```\nuint64_t special_factorial_simd(uint64_t n){\n    vuint64m1_t acc = vmv_v_x_u64m1(1);\n    vuint64m1_t result = vmv_v_x_u64m1(1);\n    \n    for(uint64_t i = 1; i <= n; i++){\n        vuint64m1_t vec_i = vmv_v_x_u64m1(i);\n        acc = vmul_vv_u64m1(acc, vec_i);\n        result = vmul_vv_u64m1(result, acc);\n    }\n    \n    uint64_t output;\n    vse64_v_u64m1(&output, result);\n    \n    return output;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```\nuint64_t special_factorial_simd(uint64_t n){\n    vuint64m1_t acc = vmv_v_x_u64m1(1);\n    vuint64m1_t result = vmv_v_x_u64m1(1);\n    \n    for (uint64_t i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vmv_v_x_u64m1(i);\n        acc = vmul_vv_u64m1(acc, vec_i);\n        result = vmul_vv_u64m1(result, acc);\n    }\n    \n    uint64_t final_result[8];\n    vse64_v_u64m1(final_result, result);\n    \n    return final_result[0] * final_result[1] * final_result[2] * final_result[3] * final_result[4] * final_result[5] * final_result[6] * final_result[7];\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```\nuint64_t special_factorial_simd(uint64_t n){\n    vuint64m1_t acc = vmv_v_x_u64m1(1);\n    vuint64m1_t result = vmv_v_x_u64m1(1);\n    \n    for (uint64_t i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vmv_v_x_u64m1(i);\n        acc = vwmulu_vv_u64m1(acc, vec_i);\n        result = vwmulu_vv_u64m1(result, acc);\n    }\n    \n    uint64_t final_result;\n    vse64_v_u64m1(&final_result, result);\n    \n    return final_result;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```\nint sum_squares_simd(const std::vector<int> & lst){\n    int len = lst.size();\n    int sum = 0;\n    vint32m4_t vec;\n    vint32m4_t result = vmv_v_x_i32m4(0, 0, 0, 0);\n    \n    for (int i = 0; i < len; i += 4) {\n        vec = vle32_v_i32m4(&lst[i]);\n        \n        vbool8_t mask = vmsne_vx_i32m4_b8(vec, 0);\n        \n        vint32m4_t square = vwmulu_vx_i32m4(vec, vec, mask);\n        vint32m4_t cube = vwmulu_vx_i32m4(vec, vec, mask);\n        cube = vwmulu_vx_i32m4(cube, vec, mask);\n        \n        vbool8_t cond3 = vmseq_vx_i32m4_b8(i % 3, 0);\n        vbool8_t cond4 = vmseq_vx_i32m4_b8(i % 4, 0) & !cond3;\n        \n        result = vmerge_vvm_i32m4(result, square, cond3);\n        result = vmerge_vvm_i32m4(result, cube, cond4);\n    }\n    \n    sum = vredsum_vs_i32m4_i32(result, sum, vbool32_t(true));\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```\nint sum_squares_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvli(0, RVV_E8, RVV_M8);\n    int sum = 0;\n    vint32m8_t vec = vle32_v_i32m8(&lst[0]);\n    \n    for (size_t i = 0; i < lst.size(); i += vector_size) {\n        vbool4_t mask = vmseq_vx_i32m8_b4(vec, 0);\n        vint32m8_t squared = vwmul_vx_i32m8(vec, vec);\n        vint32m8_t cubed = vwmul_vx_i32m8(vec, vmul_vx_i32m8(vec, vec));\n        \n        vbool4_t is_multiple_of_3 = vmseq_vx_i32m8_b4(vmv_v_x_i32m8(vec, i), 3);\n        vbool4_t is_multiple_of_4 = vmseq_vx_i32m8_b4(vmv_v_x_i32m8(vec, i), 4);\n        \n        vint32m8_t result = vmerge_vvm_i32m8(mask, vec, vmerge_vvm_i32m8(is_multiple_of_3, squared, vmerge_vvm_i32m8(is_multiple_of_4, cubed, vec)));\n        \n        sum += vredsum_vs_i32m8_i32(result, 0);\n        \n        vec = vle32_v_i32m8(&lst[i + vector_size]);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```\nint sum_squares_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvli(0, RVV_E8, RVV_M8);\n    int result = 0;\n    vint32m8_t vec = vle32_v_i32m8(&lst[0]);\n\n    for (size_t i = 0; i < lst.size(); i += vector_size) {\n        vbool4_t mask = vmseq_vx_i32m8_b4(vec, 0);\n        vint32m8_t squared = vwmulu_vx_i32m8(vec, vec);\n        vint32m8_t cubed = vwmulu_vx_i32m8(vec, vec) * vec;\n\n        vbool4_t cond1 = vmseq_vx_i32m8_b4(i % 3, 0);\n        vbool4_t cond2 = vmseq_vx_i32m8_b4(i % 4, 0) && !cond1;\n\n        vint32m8_t result_vec = vmerge_vvm_i32m8(mask, vec, cond1, squared, cond2, cubed);\n\n        result += vredsum_vs_i32m8_i32(result_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```\nint sum_squares_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvli(0, RVV_E8, RVV_M8);\n    int sum = 0;\n    vint32m8_t vec = vle32_v_i32m8(&lst[0]);\n\n    for (size_t i = 0; i < lst.size(); i += vector_size) {\n        vbool4_t mask = VM4;\n        vint32m8_t result = vsub_vv_i32m8(vec, vec, vec); // Initialize result vector with zeros\n\n        for (int j = 0; j < vector_size; j++) {\n            if ((i + j) % 3 == 0) {\n                result = vam_vv_i32m8(result, vwmul_vx_i32m8(vslideup_vx_i32m8(vec, 1, 0), vec, 0), result); // Square the integer entry\n            } else if ((i + j) % 4 == 0) {\n                result = vam_vv_i32m8(result, vmv_v_x_i32m8(vwmul_vx_i32m8(vslideup_vx_i32m8(vec, 1, 0), vec, 0)), result); // Cube the integer entry\n            }\n        }\n\n        sum += vredsum_vs_i32m8_i32(result, 0, mask);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```\nint sum_squares_simd(const std::vector<int> & lst){\n    const int vector_size = vsetvlmax_e32m1();\n    int result = 0;\n    vint32m1_t vec_input = {0};\n    vint32m1_t vec_output = {0};\n    \n    for (size_t i = 0; i < lst.size(); i += vector_size) {\n        size_t vl = std::min(vector_size, lst.size() - i);\n        vbool32_t mask = vmseq_vx_i32m1_b32(lst.data() + i, 0, vl);\n        vec_input = vle32_v_i32m1(lst.data() + i, vl);\n        \n        vint32m1_t vec_index = vmv_v_x_i32m1(i, vl);\n        vint32m1_t vec_square = vwmulu_vv_i32m1(vec_input, vec_input, vl);\n        vint32m1_t vec_cube = vwmulu_vv_i32m1(vec_input, vec_square, vl);\n        \n        vbool32_t cond_square = vmand_mm_b32(mask, vmand_mm_b32(vmadc_vx_i32m1_b32(vec_index, 1, 0, vl), vmseq_vx_i32m1_b32(3, 0, vl), vl), vl);\n        vbool32_t cond_cube = vmand_mm_b32(mask, vmand_mm_b32(vmadc_vx_i32m1_b32(vec_index, 1, 0, vl), vmseq_vx_i32m1_b32(4, 0, vl), vl), vl);\n        \n        vec_output = vsel_vv_i32m1(vec_square, vec_cube, cond_cube, vl);\n        vec_output = vsel_vv_i32m1(vec_input, vec_output, cond_square, vl);\n        \n        result += vredsum_vs_i32m1_i32(vec_output, 0, vl);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```\nint specialFilter_simd(const std::vector<int> & nums){\n    const int vectorSize = vsetvli(0, RVV_E8, RVV_M8);\n    int count = 0;\n    vint8_t v_count = vmv_v_x_i8(0);\n    vint8_t v_odd_digits = vmv_v_x_i8(0x11111111); // Mask for odd digits (1, 3, 5, 7, 9)\n    vint8_t v_ten = vmv_v_x_i8(10);\n\n    for (size_t i = 0; i < nums.size(); i += vectorSize) {\n        vint8_t v_nums = VLOAD(&nums[i]);\n        vint8_t v_first_digit = vand_vv_i8(v_nums, vmv_v_x_i8(0xF0)); // Mask to extract first digit\n        vint8_t v_last_digit = vand_vv_i8(v_nums, vmv_v_x_i8(0x0F)); // Mask to extract last digit\n        vint8_t v_is_odd_first = vand_vv_vb_i8(v_first_digit, v_odd_digits);\n        vint8_t v_is_odd_last = vand_vv_vb_i8(v_last_digit, v_odd_digits);\n        vint8_t v_is_greater_than_ten = vsgt_vv_i8(v_nums, v_ten);\n        vint8_t v_is_special = vand_vv_vb_i8(v_is_odd_first, v_is_odd_last);\n        v_is_special = vand_vv_vb_i8(v_is_special, v_is_greater_than_ten);\n        v_count = vadd_vv_i8(v_count, v_is_special);\n    }\n\n    count = vredsum_vs_i8_i32(v_count);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```\nint specialFilter_simd(const std::vector<int> & nums){\n    const int vlen = vsetvl_e32m1();\n    int count = 0;\n    vint32m1_t v_count = vmv_v_x_i32m1(0, vlen);\n\n    for (size_t i = 0; i < nums.size(); i += vlen) {\n        vint32m1_t v_nums = vle32_v_i32m1(&nums[i]);\n\n        vbool32_t mask = vmseq_vx_i32m1_b32(vand_vx_i32m1(vand_vx_i32m1(vnums, 0x1), 0x1), 1) & \n                        vmseq_vx_i32m1_b32(vand_vx_i32m1(vnums, 0xF), 0x5) &\n                        vmseq_vx_i32m1_b32(vand_vx_i32m1(vnums, 0xF0000000), 0x50000000) &\n                        vmsgt_vx_i32m1_b32(vnums, 10);\n\n        v_count = vmerge_vvm_i32m1(v_count, mask, v_count + 1, v_count);\n\n    }\n\n    count = vslide1down_vx_i32m1(vmsumu_vx_i32m1_i32(v_count, 0, vlen), 1, vlen);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```\nint specialFilter_simd(const std::vector<int> & nums){\n    const int vlen = vsetvl_e32m1();\n    int result = 0;\n    vint32m1_t v_nums = {0};\n    vint32m1_t v_first_digit = {0};\n    vint32m1_t v_last_digit = {0};\n    \n    for (size_t i = 0; i < nums.size(); i += vlen) {\n        v_nums = vlse32_v_i32m1(&nums[i], vlen);\n        v_first_digit = vand_vx_i32m1(v_nums, 0xF); // Extract first digit\n        v_last_digit = vnums = vdivu_vx_i32m1(v_nums, 10); // Extract last digit\n        \n        vbool32_t mask = vand_mm_b32(vand_mm_b32(vfirst_digit, 1), vand_mm_b32(vlast_digit, 1)); // Check if both first and last digits are odd\n        mask = vand_mm_b32(v_nums > 10, mask); // Check if number is greater than 10\n        \n        result += vpopc_m_b32(mask);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```\nint specialFilter_simd(const std::vector<int> & nums){\n    const int vlen = vsetvl_e32m1();\n    int count = 0;\n    vint32m1_t v_count = vmv_v_x_i32m1(0, vlen);\n\n    for (size_t i = 0; i < nums.size(); i += vlen) {\n        vint32m1_t v_nums = vle32_v_i32m1(&nums[i]);\n\n        vbool32_t mask = vmseq_vx_i32m1_b32(vredand_vs_u32_b32(vmsb_v_i32m1(v_nums), vmsb_v_i32m1(vadd_vx_i32m1(vdiv_vx_i32m1(v_nums, 10), 10))), 0, vlen);\n        mask = vmand_mm_b32(mask, vmsne_vx_i32m1_b32(vnums, 0, vlen));\n\n        v_count = vmerge_vvm_i32m1(v_count, vadd_vx_i32m1(vpopc_m_b32(mask), v_count), mask);\n    }\n\n    count = vslide1down_vx_i32m1(v_count, 1);\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```\nint specialFilter_simd(const std::vector<int> & nums){\n    const int vectorSize = vsetvli(0, RVV_EEW, RVV_M);\n    int count = 0;\n    vint32m1_t v_input = VLOAD(&nums[0], vectorSize);\n    vint32m1_t v_first_digit = vand_vx_i32m1(v_input, 0xF);\n    vint32m1_t v_last_digit = vand_vx_i32m1(v_input, 0xF0000000);\n    vint32m1_t v_odd_first_digit = vand_vx_i32m1(v_first_digit, 0x55555555);\n    vint32m1_t v_odd_last_digit = vand_vx_i32m1(v_last_digit, 0x55555555);\n    vbool32_t mask = vand_mm_b32(vmslt_vx_i32m1_b32(v_input, 10), vmand_mm_b32(vmadc_vxm_b32(v_odd_first_digit, 0, 0), vmadc_vxm_b32(v_odd_last_digit, 0, 0)));\n    count = vpopc_m_b32(mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    vint32m4_t a = vmv_v_x_i32m4(0, 0); // Initialize vector a with zeros\n    vint32m4_t indices = vid_v_i32m4(); // Vector of indices from 0 to n-1\n    vint32m4_t i = vsub_vx_i32m4(indices, 1); // i = indices - 1\n    vint32m4_t i_squared = vmul_vv_i32m4(i, i); // i_squared = i * i\n    vint32m4_t a_i = vsub_vv_i32m4(i_squared, i); // a_i = i_squared - i\n    vint32m4_t a_i_plus_1 = vadd_vx_i32m4(a_i, 1); // a_i_plus_1 = a_i + 1\n    vint32m4_t a_values = vadd_vv_i32m4(a_i_plus_1, indices); // a_values = a_i_plus_1 + indices\n\n    uint64_t count = 0;\n    for (int i = 0; i < n - 2; i++) {\n        vint32m4_t a_i = vslide1down_vx_i32m4(a_values, i); // Shift vector a_values by i elements\n        vint32m4_t a_ij_sum = vadd_vv_i32m4(a_values, a_i); // Calculate sum of a[i] + a[j]\n        vint32m4_t a_ijk_sum = vslide1down_vx_i32m4(a_ij_sum, 1); // Shift vector a_ij_sum by 1 element to get a[i] + a[j] + a[k]\n\n        vbool8_t mask = vmsne_vx_i32m4_b8(a_ijk_sum, 0); // Mask elements where a[i] + a[j] + a[k] is not 0\n        count += vpopc_m_b8(mask); // Count number of valid triples\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    vint32m4_t a = vmv_v_x_i32m4(0, 0); // Initialize vector a with zeros\n    vint32m4_t indices = vmv_v_x_i32m4(0, 0); // Initialize vector of indices with zeros\n    vint32m4_t increment = vmv_v_x_i32m4(1, 0); // Initialize vector of increments with 1\n\n    vint32m4_t i = vsub_vx_i32m4(indices, 1); // Initialize vector i with -1\n    vint32m4_t i_squared = vmul_vv_i32m4(i, i); // Calculate i * i\n    vint32m4_t i_minus_i = vsub_vv_i32m4(i, increment); // Calculate i - 1\n    vint32m4_t a_i = vadd_vv_i32m4(vadd_vv_i32m4(i_squared, i_minus_i), increment); // Calculate a[i] = i * i - i + 1\n\n    vint32m4_t sum = vadd_vv_i32m4(vadd_vv_i32m4(a_i, a_i), a_i); // Calculate a[i] + a[i] + a[i]\n\n    uint64_t count = 0;\n    for (int i = 0; i < n; i += 4) {\n        vbool8_t mask = vmsne_vx_i32m4_b8(sum, 0); // Check if sum is not equal to 0\n        vint32m4_t result = vmand_mm_i32m4(mask, sum); // Mask the sum vector\n        count += vpopc_m_b8(mask); // Count the number of set bits in the mask\n\n        indices = vadd_vv_i32m4(indices, increment); // Increment indices vector\n        i = vsub_vx_i32m4(indices, 1); // Update vector i\n        i_squared = vmul_vv_i32m4(i, i); // Calculate i * i\n        i_minus_i = vsub_vv_i32m4(i, increment); // Calculate i - 1\n        a_i = vadd_vv_i32m4(vadd_vv_i32m4(i_squared, i_minus_i), increment); // Calculate a[i] = i * i - i + 1\n        a = vslideup_vx_i32m4(a, a_i, 1); // Shift elements in vector a and insert new a_i\n        sum = vadd_vv_i32m4(vadd_vv_i32m4(a, a), a); // Calculate a[i] + a[j] + a[k]\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    vint32m4_t a = vmv_v_x_i32m4(0, 0); // Initialize a vector of length n\n    vint32m4_t indices = vmv_v_x_i32m4(0, 0); // Initialize a vector of indices\n    vint32m4_t ones = vmv_v_x_i32m4(1, 0); // Initialize a vector of ones\n    vint32m4_t threes = vmv_v_x_i32m4(3, 0); // Initialize a vector of threes\n    vint32m4_t sum = vmv_v_x_i32m4(0, 0); // Initialize a vector to store the sum of triples\n    uint64_t count = 0; // Initialize count of valid triples\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        vint32m4_t i_vec = vmv_v_x_i32m4(i, 0); // Create a vector of current indices\n        vint32m4_t i_squared = vwmul_vv_i32m4(i_vec, i_vec); // Calculate i * i\n        vint32m4_t a_i = vwmul_vx_i32m4(i_vec, i) - i_vec + ones; // Calculate a[i] = i * i - i + 1\n        vint32m4_t a_j = vwmul_vx_i32m4(i_vec + ones, i + 1) - (i_vec + ones) + ones; // Calculate a[j] = (i+1) * (i+1) - (i+1) + 1\n        vint32m4_t a_k = vwmul_vx_i32m4(i_vec + ones + ones, i + 2) - (i_vec + ones + ones) + ones; // Calculate a[k] = (i+2) * (i+2) - (i+2) + 1\n        a = vslideup_vx_i32m4(a, 4, a_i); // Slide up the vector a and insert a_i\n        indices = vslideup_vx_i32m4(indices, 4, i_vec); // Slide up the vector indices and insert current indices\n\n        sum = a + vslideup_vx_i32m4(a, 1, a_j) + vslideup_vx_i32m4(a, 2, a_k); // Calculate the sum of triples\n        vbool8_t mask = vmsne_vx_i32m4_b8(sum % threes, 0, 0); // Check if the sum is a multiple of 3\n        count += vpopc_m_b8(mask); // Count the number of valid triples\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    vint32m4_t a = vmv_v_x_i32m4(0, 0); // Initialize vector a with zeros\n    vint32m4_t indices = vmv_v_x_i32m4(0, 0); // Initialize vector of indices with zeros\n    vint32m4_t increment = vmv_v_x_i32m4(1, 0); // Initialize vector of increments with ones\n    vint32m4_t multiples_of_three = vmv_v_x_i32m4(3, 0); // Initialize vector of multiples of three\n\n    vint32m4_t i = vwmacc_vx_i32m4(indices, increment, 1); // Calculate vector of indices i\n    vint32m4_t i_squared = vwmul_vx_i32m4(i, i); // Calculate vector of i squared\n    vint32m4_t a_i = vwmul_vx_i32m4(i_squared, i); // Calculate vector a[i] = i * i - i + 1\n\n    vint32m4_t j = vwmacc_vx_i32m4(i, increment, 1); // Calculate vector of indices j\n    vint32m4_t j_squared = vwmul_vx_i32m4(j, j); // Calculate vector of j squared\n    vint32m4_t a_j = vwmul_vx_i32m4(j_squared, j); // Calculate vector a[j] = j * j - j + 1\n\n    vint32m4_t k = vwmacc_vx_i32m4(j, increment, 1); // Calculate vector of indices k\n    vint32m4_t k_squared = vwmul_vx_i32m4(k, k); // Calculate vector of k squared\n    vint32m4_t a_k = vwmul_vx_i32m4(k_squared, k); // Calculate vector a[k] = k * k - k + 1\n\n    vint32m4_t sum = vadd_vv_i32m4(vadd_vv_i32m4(a_i, a_j), a_k); // Calculate vector of sums a[i] + a[j] + a[k]\n    vbool8_t mask = vmsne_vx_i32m4_b8(sum, 0); // Create mask for non-zero sums\n    vint32m4_t valid_sums = vand_vv_i32m4(vremu_vx_i32m4(sum, multiples_of_three), mask); // Filter valid sums that are multiples of three\n\n    uint64_t count = vmpopc_m_b8(valid_sums); // Count the number of valid triples\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    vint32m4_t a = vmv_v_x_i32m4(0, 0); // Initialize vector a with zeros\n    vint32m4_t indices = vmv_v_x_i32m4(0, 0); // Initialize vector of indices with zeros\n    vint32m4_t increment = vmv_v_x_i32m4(1, 0); // Initialize vector of increments with 1\n\n    vint32m4_t i = vsub_vv_i32m4(indices, increment); // i = indices - increment\n    vint32m4_t i_squared = vmul_vv_i32m4(i, i); // i_squared = i * i\n    vint32m4_t a_i = vsub_vv_i32m4(i_squared, i); // a_i = i_squared - i\n    a = vslideup_vx_i32m4(a, vfirst_v_i32m4(a_i), 1); // Shift elements in vector a and insert a_i at the beginning\n\n    for (uint64_t j = 1; j < n; j++) {\n        vint32m4_t j_vec = vmv_v_x_i32m4(j, 0); // Create vector of j values\n        vint32m4_t j_squared = vmul_vv_i32m4(j_vec, j_vec); // j_squared = j * j\n        vint32m4_t a_j = vsub_vv_i32m4(j_squared, j_vec); // a_j = j_squared - j\n        a = vslideup_vx_i32m4(a, vfirst_v_i32m4(a_j), 1); // Shift elements in vector a and insert a_j at the beginning\n\n        for (uint64_t k = 2; k < n; k++) {\n            vint32m4_t k_vec = vmv_v_x_i32m4(k, 0); // Create vector of k values\n            vint32m4_t a_k = vsub_vv_i32m4(vmacc_vv_i32m4(a_i, a_j, k_vec), k_vec); // a_k = a_i + a_j - k\n            vint32m4_t sum = vadd_vv_i32m4(a_i, vadd_vv_i32m4(a_j, a_k)); // sum = a_i + a_j + a_k\n            vint32m4_t mod3 = vremu_vx_i32m4(sum, 3); // mod3 = sum % 3\n            vbool32_t mask = vmsne_vx_i32m4_b32(mod3, 0); // Create mask for elements where mod3 is not equal to 0\n            uint64_t count = vmpopc_m_b32(mask); // Count number of true elements in the mask\n            // Do something with the count (e.g., accumulate it)\n        }\n\n        a_i = a_j; // Update a_i with a_j for the next iteration\n    }\n\n    // Return the final result (e.g., accumulated count)\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(n), 1);\n    vint32m1_t result = vmerge_vvm_i32m1(mask, vcreate_v_i32m1(y), vcreate_v_i32m1(x));\n    return vslide1down_vx_i32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(n), 0); // Create a mask where elements are 1 if equal to n, 0 otherwise\n    vint32m1_t result = vmerge_vvm_v_i32m1(mask, vcreate_v_i32m1(y), vcreate_v_i32m1(x)); // Merge x and y based on the mask\n    return vslide1down_vx_i32(result, 0); // Return the first element of the result vector\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(n), 0);\n    vint32m1_t result = vmerge_vvm_v_i32m1(mask, vmv_v_x_i32m1(y), vmv_v_x_i32m1(x));\n    return vslide1down_vx_i32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y){\n    vint32m1_t mask = vmseq_vx_i32m1_b32(vmv_v_x_i32m1(n), 0);\n    vint32m1_t result = vmerge_vvm_i32m1(mask, vcreate_v_i32m1(y), vcreate_v_i32m1(x));\n    return vslide1down_vx_i32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y){\n    vint32m1_t mask = vmseq_v_i32m1_b32(n, 2); // Check if n is prime\n    vint32m1_t result = vmerge_vvm_i32m1(mask, vcreate_v_i32m1(y), vcreate_v_i32m1(x));\n    return vslide1down_vx_i32m1_i32(result, 0);\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elements = lst.size();\n    \n    if(num_elements == 0){\n        return 0;\n    }\n    \n    vfloat32m1_t vec_input = vle32_v_f32m1(&lst[0], num_elements, vector_size);\n    vint32m1_t vec_input_int = vf2in32_v_i32m1(vec_input, vector_size);\n    \n    vbool32_t mask_odd = vmand_mm_b32(vmsne_vx_i32m1_b32(vec_input_int, 0, vector_size), vmsne_vx_i32m1_b32(vec_input_int, -1, vector_size));\n    vbool32_t mask_positive = vmsle_vx_i32m1_b32(vec_input_int, 0, vector_size);\n    \n    vint32m1_t vec_squared = vwmul_vv_i32m1(vec_input_int, vec_input_int, vector_size);\n    vint32m1_t vec_odd_squared = vmerge_vxm_i32m1(mask_odd, vec_squared, 0, vector_size);\n    vint32m1_t vec_positive_odd_squared = vmerge_vxm_i32m1(mask_positive, vec_odd_squared, 0, vector_size);\n    \n    vint64m1_t vec_sum = vredsum_vs_i64m1_i32m1(vec_positive_odd_squared, 0, vector_size);\n    \n    return vslide1down_vx_i64m1_i64m1(vec_sum, 0, vector_size);\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elements = lst.size();\n    const int num_lanes = vsetvli(0, RVV_EEW, RVV_M);\n    \n    if(num_elements == 0){\n        return 0;\n    }\n    \n    vfloat32m1_t vec_input = vfmv_v_f_f32m1(lst.data(), num_elements);\n    vbool32_t mask_odd = vmseq_vx_f32m1_b32(vec_input, 0) && vmv_x_s_f32m1_b32(vec_input) % 2 != 0;\n    vfloat32m1_t vec_odd = vcompress_vm_f32m1(mask_odd, vec_input);\n    vfloat32m1_t vec_squared = vfmul_vv_f32m1(vec_odd, vec_odd);\n    vfloat32m1_t vec_sum = vfredsum_vs_f32m1_f32m1(vec_squared, vec_squared, vfmv_s_f_f32m1(0), num_lanes);\n    \n    return vfmv_f_s_f32_f64(vec_sum);\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elems = lst.size();\n    \n    if (num_elems == 0) {\n        return 0;\n    }\n    \n    vfloat32m1_t vec_input = vle32_v_f32m1(&lst[0], num_elems, vector_size);\n    vint32m1_t vec_input_int = vfncvt_x_f_v_i32m1_b32(vec_input, 0, vector_size);\n    vint32m1_t vec_input_abs = vabs_vx_i32m1(vec_input_int, vector_size);\n    vbool32_t mask_odd = vmseq_vx_i32m1_b32(vec_input_abs, 1, vector_size);\n    vbool32_t mask_positive = vmseq_vx_i32m1_b32(vec_input_int, 0, vector_size);\n    vbool32_t mask_valid = vand_mm_b32(mask_odd, mask_positive, vector_size);\n    \n    vint32m1_t vec_input_odd = vmerge_vvm_i32m1(mask_valid, vec_input_int, 0, vector_size);\n    vint32m1_t vec_input_odd_squared = vmul_vv_i32m1(vec_input_odd, vec_input_odd, vector_size);\n    \n    vint64m1_t vec_input_odd_squared_wide = vwmul_vv_i64m1(vec_input_odd_squared, vec_input_odd_squared, vector_size);\n    vint64m1_t vec_sum = vredsum_vs_i64m1_i64m1(vec_input_odd_squared_wide, vec_input_odd_squared_wide, 0, vector_size);\n    \n    return vslide1down_vx_i64m1_i64(vec_sum, 0, vector_size);\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M);\n    const int num_elems = lst.size();\n    \n    if(num_elems == 0){\n        return 0;\n    }\n    \n    vfloat32m1_t vec_input = vle32_v_f32m1(&lst[0], num_elems, vector_size);\n    vint32m1_t vec_input_int = vfncvt_x_f_v_i32m1(vec_input, VM);\n    vbool32_t mask_odd = vmand_mm_b32(vmsne_vx_i32m1_b32(vec_input_int, 0, vector_size), vmsne_vx_i32m1_b32(vmand_vx_i32m1(vec_input_int, 1, vector_size), 0, vector_size), vector_size);\n    vint32m1_t vec_input_odd = vmerge_vvm_i32m1(mask_odd, vec_input_int, 0, vector_size);\n    vint32m1_t vec_input_odd_squared = vmul_vv_i32m1(vec_input_odd, vec_input_odd, vector_size);\n    vint64m1_t vec_input_odd_squared_doubled = vwmul_vv_i64m1(vec_input_odd_squared, vec_input_odd_squared, vector_size);\n    vint64m1_t sum = vredsum_vs_i64m1_i64m1(vec_input_odd_squared_doubled, 0, vector_size);\n    \n    return sum[0];\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    const int vector_size = vsetvli(0, RVV_EEW, RVV_M1);\n    const int num_elems = lst.size();\n    \n    if (num_elems == 0) {\n        return 0;\n    }\n    \n    vfloat32m1_t vec_input = vle32_v_f32m1(&lst[0]);\n    vint32m1_t is_odd = vfmv_v_f_f32m1_b32(vec_input, 1.0f) & 1;\n    vint32m1_t is_positive = vmfgt_vx_i32m1_b32(vec_input, 0);\n    vint32m1_t is_integer = vfcvt_x_f_v_i32m1_b32(vec_input) == vec_input;\n    vint32m1_t valid_elements = is_odd & is_positive & is_integer;\n    \n    vint32m1_t squared_elements = vwmul_vv_i32m1(valid_elements, valid_elements);\n    vint32m1_t sum_squared_elements = vredsum_vs_i32m1_i32m1(squared_elements, squared_elements, vector_size);\n    \n    return vslide1down_vx_i32(sum_squared_elements, 0);\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    const int size = game.size();\n    std::vector<int> result(size);\n    \n    vint32m1_t v_game, v_guess, v_result;\n    \n    for (int i = 0; i < size; i += VL) {\n        v_game = vle32_v_i32m1(&game[i]);\n        v_guess = vle32_v_i32m1(&guess[i]);\n        \n        v_result = vsub_vv_i32m1(v_game, v_guess);\n        v_result = vabs_v_i32m1(v_result);\n        \n        vse32_v_i32m1(&result[i], v_result);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    const int size = game.size();\n    std::vector<int> result(size);\n\n    const int lane_size = vsetvl_e32m1();\n    const int remainder = size % lane_size;\n    const int full_chunks = size - remainder;\n\n    for (int i = 0; i < full_chunks; i += lane_size) {\n        vint32m1_t v_game = vle32_v_i32m1(&game[i]);\n        vint32m1_t v_guess = vle32_v_i32m1(&guess[i]);\n        vint32m1_t v_diff = vsub_vv_i32m1(v_game, v_guess);\n        vint32m1_t v_abs_diff = vabs_v_i32m1(v_diff);\n        vse32_v_i32m1(&result[i], v_abs_diff);\n    }\n\n    if (remainder > 0) {\n        vint32m1_t v_game = vle32_v_i32m1(&game[full_chunks]);\n        vint32m1_t v_guess = vle32_v_i32m1(&guess[full_chunks]);\n        vint32m1_t v_diff = vsub_vv_i32m1(v_game, v_guess);\n        vint32m1_t v_abs_diff = vabs_v_i32m1(v_diff);\n        int32_t temp[VLMAX];\n        vse32_v_i32m1(temp, v_abs_diff);\n        for (int i = 0; i < remainder; ++i) {\n            result[full_chunks + i] = temp[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    const int size = game.size();\n    std::vector<int> result(size);\n\n    const int lane_size = vsetvl_e32m1();\n    const int remainder = size % lane_size;\n    const int full_chunks = size - remainder;\n\n    for (int i = 0; i < full_chunks; i += lane_size) {\n        vint32m1_t vg = vle32_v_i32m1(&guess[i]);\n        vint32m1_t vs = vle32_v_i32m1(&game[i]);\n        vint32m1_t diff = vsub_vv_i32m1(vs, vg);\n        vint32m1_t abs_diff = vabs_v_i32m1(diff);\n        vse32_v_i32m1(&result[i], abs_diff);\n    }\n\n    if (remainder > 0) {\n        vint32m1_t vg = vle32_v_i32m1(&guess[full_chunks]);\n        vint32m1_t vs = vle32_v_i32m1(&game[full_chunks]);\n        vint32m1_t diff = vsub_vv_i32m1(vs, vg);\n        vint32m1_t abs_diff = vabs_v_i32m1(diff);\n        int32_t temp[lane_size];\n        vse32_v_i32m1(temp, abs_diff);\n        for (int i = 0; i < remainder; ++i) {\n            result[full_chunks + i] = temp[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    const int size = game.size();\n    std::vector<int> result(size);\n\n    // Load vectors into RVV registers\n    vint32m1_t v_game = vle32_v_i32m1(&game[0]);\n    vint32m1_t v_guess = vle32_v_i32m1(&guess[0]);\n\n    // Calculate absolute difference between guess and game\n    vint32m1_t diff = vsub_vv_i32m1(v_guess, v_game);\n    diff = vabs_v_i32m1(diff);\n\n    // Store result back to vector\n    vse32_v_i32m1(&result[0], diff);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    const int size = game.size();\n    std::vector<int> result(size);\n\n    const int lane_size = vsetvl_e32m1();\n    const int full_lanes = size / lane_size;\n    const int remainder = size % lane_size;\n\n    for (int i = 0; i < full_lanes; i++) {\n        vint32m1_t v_game = vle32_v_i32m1(&game[i * lane_size]);\n        vint32m1_t v_guess = vle32_v_i32m1(&guess[i * lane_size]);\n        vint32m1_t v_diff = vsub_vv_i32m1(v_game, v_guess);\n        vint32m1_t v_abs_diff = vabs_v_i32m1(v_diff);\n        vse32_v_i32m1(&result[i * lane_size], v_abs_diff);\n    }\n\n    if (remainder > 0) {\n        vint32m1_t v_game = vle32_v_i32m1(&game[full_lanes * lane_size]);\n        vint32m1_t v_guess = vle32_v_i32m1(&guess[full_lanes * lane_size]);\n        vint32m1_t v_diff = vsub_vv_i32m1(v_game, v_guess);\n        vint32m1_t v_abs_diff = vabs_v_i32m1(v_diff);\n        int32_t temp[VLMAX];\n        vse32_v_i32m1(temp, v_abs_diff);\n        for (int i = 0; i < remainder; i++) {\n            result[full_lanes * lane_size + i] = temp[i];\n        }\n    }\n\n    return result;\n}\n```"}

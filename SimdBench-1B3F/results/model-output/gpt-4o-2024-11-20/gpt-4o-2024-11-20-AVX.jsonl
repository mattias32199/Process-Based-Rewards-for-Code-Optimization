{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers per vector\n\n    // Process elements in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 4 elements from src and mask\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i *)(mask + i));\n\n        // Convert mask (boolean) to 64-bit integers (0 or -1)\n        __m256i mask_int_vec = _mm256_cmpgt_epi64(mask_vec, _mm256_setzero_si256());\n\n        // Blend src and dst based on the mask\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)(dst + i));\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_int_vec);\n\n        // Store the result back to dst\n        _mm256_storeu_si256((__m256i *)(dst + i), result_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from src and mask\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask_vec = _mm256_loadu_si256((__m256i *)&mask[i]);\n\n        // Apply mask\n        __m256i result_vec = _mm256_blendv_epi8(zero, src_vec, mask_vec);\n\n        // Store result\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = mask[i] ? src[i] : 0;\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from src and mask\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const *)(mask + i));\n\n        // Apply mask\n        __m256i result = _mm256_blendv_epi8(zero, src_vec, mask_vec);\n\n        // Store result to dst\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from src and mask\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i *)(mask + i));\n\n        // Apply mask\n        __m256i result = _mm256_blendv_epi8(zero, src_vec, mask_vec);\n\n        // Store result to dst\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from src and mask\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const *)(mask + i));\n\n        // Apply mask and conditional move\n        __m256i result = _mm256_blendv_epi8(zero, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from src\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        // Calculate element * scale\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n\n        // Calculate tmp1 = (element * scale) >> 3\n        __m256i v_tmp1 = _mm256_srai_epi32(v_scaled, 3);\n\n        // Calculate tmp2 = (element * scale) << 3\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        // Calculate tmp1 * tmp2\n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Calculate final result: (element * scale + tmp1 * tmp2) / 7\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm256_div_epi32(v_result, v_seven);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        __m256i v_tmp1 = _mm256_srai_epi32(_mm256_mullo_epi32(v_src, v_scale), 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(_mm256_mullo_epi32(v_src, v_scale), 3);\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_result = _mm256_add_epi32(_mm256_mullo_epi32(v_src, v_scale), v_product);\n        v_result = _mm256_div_epi32(v_result, v_seven);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        int tmp1 = (src[i] * scale) >> 3;\n        int tmp2 = (src[i] * scale) << 3;\n        dst[i] = (src[i] * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from src\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        // Compute element * scale\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n\n        // Compute tmp1 = (element * scale) >> 3\n        __m256i v_tmp1 = _mm256_srai_epi32(v_scaled, 3);\n\n        // Compute tmp2 = (element * scale) << 3\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        // Compute tmp1 * tmp2\n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Compute final result: (element * scale + tmp1 * tmp2) / 7\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm256_div_epi32(v_result, v_seven);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n        __m256i v_tmp1 = _mm256_srai_epi32(v_scaled, 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm256_div_epi32(v_result, v_seven);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_seven = _mm256_set1_epi32(7);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        __m256i v_tmp1 = _mm256_srai_epi32(_mm256_mullo_epi32(v_src, v_scale), 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(_mm256_mullo_epi32(v_src, v_scale), 3);\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_result = _mm256_div_epi32(_mm256_add_epi32(_mm256_mullo_epi32(v_src, v_scale), v_product), v_seven);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        int tmp1 = (src[i] * scale) >> 3;\n        int tmp2 = (src[i] * scale) << 3;\n        dst[i] = (src[i] * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process rows in chunks of 4 using AVX2\n    for (; i + 4 <= rows; i += 4) {\n        // Calculate the indices for the strided load\n        size_t idx0 = i * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Gather the values from the src array using AVX2\n        __m256d gathered = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the gathered values into the dst array\n        _mm256_storeu_pd(&dst[i], gathered);\n    }\n\n    // Handle remaining rows (if rows is not a multiple of 4)\n    for (; i < rows; i++) {\n        size_t idx = i * cols + stride;\n        dst[i] = src[idx];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process in chunks of 4 rows using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Calculate the indices for the 4 rows\n        size_t idx0 = i * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Load the 4 elements from the src array\n        __m256d src_vals = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the 4 elements into the dst array\n        _mm256_storeu_pd(&dst[i], src_vals);\n    }\n\n    // Handle any remaining rows\n    for (; i < rows; i++) {\n        size_t idx = i * cols + stride;\n        dst[i] = src[idx];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process rows in chunks of 4 using AVX2\n    for (; i + 4 <= rows; i += 4) {\n        // Calculate the indices for the strided elements\n        size_t idx0 = i * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Load the strided elements from src using scalar loads\n        __m256d vec = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the vector into the dst array\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Handle remaining rows (if rows is not a multiple of 4)\n    for (; i < rows; i++) {\n        size_t idx = i * cols + stride;\n        dst[i] = src[idx];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process rows in chunks of 4 using AVX2\n    for (; i + 4 <= rows; i += 4) {\n        // Calculate the indices for the strided elements\n        size_t idx0 = i * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Load the strided elements into an AVX2 register\n        __m256d vec = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the vector into the destination array\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Handle remaining rows that couldn't be processed in chunks of 4\n    for (; i < rows; ++i) {\n        size_t idx = i * cols + stride;\n        dst[i] = src[idx];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process in chunks of 4 rows using AVX (4 double-precision floats per 256-bit register)\n    for (; i + 3 < rows; i += 4) {\n        // Calculate the indices for the strided elements\n        size_t idx0 = i * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Gather the 4 elements from the src array\n        __m256d gathered = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the gathered elements into the dst array\n        _mm256_storeu_pd(&dst[i], gathered);\n    }\n\n    // Handle remaining rows (if rows is not a multiple of 4)\n    for (; i < rows; i++) {\n        size_t idx = i * cols + stride;\n        dst[i] = src[idx];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements (AVX2 register size for floats)\n    for (; i + 8 <= length; i += 8) {\n        // Gather operation: Load values from `src` using `indices` into a vector\n        __m256 gathered = _mm256_i32gather_ps(src, _mm256_loadu_si256((__m256i *)(indices + i)), 4);\n\n        // Store the gathered values into `dst`\n        _mm256_storeu_ps(dst + i, gathered);\n\n        // Scatter operation: Write values from `src` to `dst` using `indices` as write locations\n        for (int j = 0; j < 8; ++j) {\n            dst[indices[i + j]] = src[i + j];\n        }\n    }\n\n    // Handle remaining elements (tail case)\n    for (; i < length; ++i) {\n        // Gather operation\n        dst[i] = src[indices[i]];\n\n        // Scatter operation\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements (AVX2 register size)\n    for (; i + 8 <= length; i += 8) {\n        // Gather operation: Load values from src using indices\n        __m256 gathered = _mm256_i32gather_ps(src, _mm256_loadu_si256((__m256i*)(indices + i)), 4);\n\n        // Store gathered values into dst\n        _mm256_storeu_ps(dst + i, gathered);\n    }\n\n    // Scatter operation: Write values from src to dst using indices\n    for (size_t j = 0; j < length; ++j) {\n        dst[indices[j]] = src[j];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements (AVX2 can handle 8 floats at a time)\n    for (; i + 8 <= length; i += 8) {\n        // Gather operation: Load values from `src` using indices into `dst`\n        __m256 gathered = _mm256_i32gather_ps(src, _mm256_loadu_si256((__m256i*)&indices[i]), 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n\n        // Scatter operation: Store values from `src` into `dst` using indices\n        for (int j = 0; j < 8; ++j) {\n            dst[indices[i + j]] = src[i + j];\n        }\n    }\n\n    // Handle remaining elements (if length is not a multiple of 8)\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // Gather\n        dst[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements (AVX2 register size)\n    for (; i + 8 <= length; i += 8) {\n        // Gather operation: Load values from src using indices\n        __m256 gathered = _mm256_i32gather_ps(src, _mm256_loadu_si256((__m256i*)&indices[i]), 4);\n\n        // Store gathered values into dst\n        _mm256_storeu_ps(&dst[i], gathered);\n\n        // Scatter operation: Write values from src to dst using indices\n        for (int j = 0; j < 8; ++j) {\n            dst[indices[i + j]] = src[i + j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // Gather\n        dst[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements (AVX2 registers can hold 8 floats)\n    for (; i + 8 <= length; i += 8) {\n        // Gather operation: Load values from `src` using `indices` into a SIMD register\n        __m256 gathered = _mm256_i32gather_ps(src, _mm256_loadu_si256((__m256i*)(indices + i)), 4);\n\n        // Store gathered values into `dst`\n        _mm256_storeu_ps(dst + i, gathered);\n\n        // Scatter operation: Write values from `src` into `dst` using `indices` as write locations\n        for (int j = 0; j < 8; ++j) {\n            dst[indices[i + j]] = src[i + j];\n        }\n    }\n\n    // Handle remaining elements (tail case)\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // Gather operation for remaining elements\n        dst[indices[i]] = src[i]; // Scatter operation for remaining elements\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (right - left + 1 >= 16) {\n        // Load 16 elements from the left and right\n        __m256i left_vec = _mm256_loadu_si256((__m256i *)&arr[left]);\n        __m256i right_vec = _mm256_loadu_si256((__m256i *)&arr[right - 15]);\n\n        // Reverse the right vector\n        right_vec = _mm256_permute4x64_epi64(right_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        right_vec = _mm256_shuffle_epi8(right_vec, _mm256_setr_epi8(\n            30, 31, 28, 29, 26, 27, 24, 25,\n            22, 23, 20, 21, 18, 19, 16, 17,\n            14, 15, 12, 13, 10, 11, 8, 9,\n            6, 7, 4, 5, 2, 3, 0, 1\n        ));\n\n        // Store the reversed vectors\n        _mm256_storeu_si256((__m256i *)&arr[left], right_vec);\n        _mm256_storeu_si256((__m256i *)&arr[right - 15], left_vec);\n\n        // Move the pointers\n        left += 16;\n        right -= 16;\n    }\n\n    // Handle the remaining elements with a scalar loop\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (left < right) {\n        // Load 16 elements from the left and right sides using AVX2\n        __m256i left_vec = _mm256_loadu_si256((__m256i *)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i *)(arr + right - 15));\n\n        // Reverse the elements in the vectors\n        left_vec = _mm256_permute4x64_epi64(left_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        left_vec = _mm256_shuffle_epi8(left_vec, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        ));\n\n        // Store reversed vectors back to their respective positions\n        _mm256_storeu_si256((__m256i *)(arr + left), right_vec);\n        _mm256_storeu_si256((__m256i *)(arr + right - 15), left_vec);\n\n        left += 16;\n        right -= 16;\n    }\n\n    // Handle remaining elements if the range is not a multiple of 16\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (left < right) {\n        // Load 16 elements from the left and right sides using AVX2\n        __m256i left_vec = _mm256_loadu_si256((__m256i *)(arr + left));\n        __m256i right_vec = _mm256_loadu_si256((__m256i *)(arr + right - 15));\n\n        // Reverse the elements in the vectors\n        left_vec = _mm256_permute4x64_epi64(left_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        left_vec = _mm256_shuffle_epi8(left_vec, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        ));\n\n        // Store reversed vectors back to their positions\n        _mm256_storeu_si256((__m256i *)(arr + left), right_vec);\n        _mm256_storeu_si256((__m256i *)(arr + right - 15), left_vec);\n\n        // Update indices\n        left += 16;\n        right -= 16;\n    }\n\n    // Handle remaining elements if the range is not a multiple of 16\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t range_length = end - start + 1;\n    uint64_t simd_width = 16; // AVX2 processes 16 16-bit integers at a time\n    uint64_t simd_chunks = range_length / simd_width;\n\n    for (uint64_t i = 0; i < simd_chunks; ++i) {\n        uint64_t left_index = start + i * simd_width;\n        uint64_t right_index = end - i * simd_width - simd_width + 1;\n\n        __m256i left_vec = _mm256_loadu_si256((__m256i *)(arr + left_index));\n        __m256i right_vec = _mm256_loadu_si256((__m256i *)(arr + right_index));\n\n        // Reverse the vectors\n        left_vec = _mm256_permute4x64_epi64(left_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        left_vec = _mm256_shuffle_epi8(left_vec, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        ));\n        right_vec = _mm256_permute4x64_epi64(right_vec, _MM_SHUFFLE(0, 1, 2, 3));\n        right_vec = _mm256_shuffle_epi8(right_vec, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        ));\n\n        _mm256_storeu_si256((__m256i *)(arr + left_index), right_vec);\n        _mm256_storeu_si256((__m256i *)(arr + right_index), left_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (i + 15 < j) {\n        __m256i vec_i = _mm256_loadu_si256((__m256i *)(arr + i));\n        __m256i vec_j = _mm256_loadu_si256((__m256i *)(arr + j - 15));\n\n        vec_j = _mm256_permute4x64_epi64(vec_j, _MM_SHUFFLE(0, 1, 2, 3));\n        vec_j = _mm256_shuffle_epi8(vec_j, _mm256_setr_epi8(\n            14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1,\n            14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1\n        ));\n\n        vec_i = _mm256_permute4x64_epi64(vec_i, _MM_SHUFFLE(0, 1, 2, 3));\n        vec_i = _mm256_shuffle_epi8(vec_i, _mm256_setr_epi8(\n            14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1,\n            14, 15, 12, 13, 10, 11, 8, 9, 6, 7, 4, 5, 2, 3, 0, 1\n        ));\n\n        _mm256_storeu_si256((__m256i *)(arr + i), vec_j);\n        _mm256_storeu_si256((__m256i *)(arr + j - 15), vec_i);\n\n        i += 16;\n        j -= 16;\n    }\n\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset to the start of the slice in the flattened tensor\n    size_t total_elements = dim1 * dim2;          // Total elements in the 2D slice\n    size_t i = 0;\n\n    // Use AVX2 to copy 32 bytes (256 bits) at a time\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i));\n        _mm256_storeu_si256((__m256i *)(slice + i), data);\n    }\n\n    // Handle the remaining elements that do not fit into 32-byte chunks\n    for (; i < total_elements; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Calculate the starting offset of the slice in the tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 32 <= slice_size; i += 32) {\n        // Load 32 bytes from the tensor using AVX\n        __m256i data = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i));\n        // Store 32 bytes into the slice\n        _mm256_storeu_si256((__m256i *)(slice + i), data);\n    }\n\n    // Handle remaining elements that don't fit into a 32-byte chunk\n    for (; i < slice_size; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    size_t i = 0;\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i const *)(tensor + slice_offset + i));\n        _mm256_storeu_si256((__m256i *)(slice + i), data);\n    }\n\n    for (; i < total_elements; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t slice_size = dim1 * dim2;\n\n    size_t i = 0;\n    for (; i + 32 <= slice_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i));\n        _mm256_storeu_si256((__m256i *)(slice + i), data);\n    }\n\n    for (; i < slice_size; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset to the start of the slice in the 1D tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 32 <= slice_size; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i)); // Load 32 bytes from the tensor\n        _mm256_storeu_si256((__m256i *)(slice + i), data);                        // Store 32 bytes to the slice\n    }\n\n    // Handle remaining elements that don't fit into 32-byte chunks\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 16) {\n                    size_t remaining_cols = (bj + 16 > block_col_end) ? block_col_end - bj : 16;\n\n                    __m256i vec = _mm256_loadu_si256((__m256i *)&src[bi * cols + bj]);\n                    if (remaining_cols < 16) {\n                        int16_t temp[16] = {0};\n                        for (size_t k = 0; k < remaining_cols; k++) {\n                            temp[k] = src[bi * cols + bj + k];\n                        }\n                        vec = _mm256_loadu_si256((__m256i *)temp);\n                    }\n\n                    for (size_t k = 0; k < remaining_cols; k++) {\n                        dst[(bj + k) * rows + bi] = ((int16_t *)&vec)[k];\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 16) {\n                    size_t remaining_cols = block_col_end - bj;\n                    if (remaining_cols >= 16) {\n                        __m256i vec = _mm256_loadu_si256((__m256i *)&src[bi * cols + bj]);\n                        _mm256_storeu_si256((__m256i *)&dst[bj * rows + bi], vec);\n                    } else {\n                        for (size_t k = 0; k < remaining_cols; k++) {\n                            dst[(bj + k) * rows + bi] = src[bi * cols + bj + k];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 16) {\n                    size_t remaining_cols = block_col_end - bj;\n                    size_t simd_width = remaining_cols >= 16 ? 16 : remaining_cols;\n\n                    __m256i vec = _mm256_loadu_si256((__m256i *)(src + bi * cols + bj));\n                    _mm256_storeu_si256((__m256i *)(dst + bj * rows + bi), vec);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 16) {\n                    size_t remaining_cols = block_col_end - bj;\n                    if (remaining_cols >= 16) {\n                        __m256i vec = _mm256_loadu_si256((__m256i *)(src + bi * cols + bj));\n                        _mm256_storeu_si256((__m256i *)(dst + bj * rows + bi), vec);\n                    } else {\n                        for (size_t k = 0; k < remaining_cols; k++) {\n                            dst[(bj + k) * rows + bi] = src[bi * cols + bj + k];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 16) {\n                    size_t remaining = (bj + 16 > block_col_end) ? block_col_end - bj : 16;\n\n                    __m256i vec = _mm256_loadu_si256((__m256i *)&src[bi * cols + bj]);\n                    if (remaining < 16) {\n                        int16_t temp[16] = {0};\n                        _mm256_storeu_si256((__m256i *)temp, vec);\n                        for (size_t k = 0; k < remaining; k++) {\n                            dst[bj + k + bi * cols] = temp[k];\n                        }\n                    } else {\n                        _mm256_storeu_si256((__m256i *)&dst[bj + bi * cols], vec);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    size_t i = 0;\n    __m256 sum_vec = _mm256_setzero_ps(); // Initialize sum vector to zero\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + 8 <= diagonal_count; i += 8) {\n        // Calculate indices for diagonal elements\n        size_t indices[8] = {\n            i * dim * dim + i * dim + i,\n            (i + 1) * dim * dim + (i + 1) * dim + (i + 1),\n            (i + 2) * dim * dim + (i + 2) * dim + (i + 2),\n            (i + 3) * dim * dim + (i + 3) * dim + (i + 3),\n            (i + 4) * dim * dim + (i + 4) * dim + (i + 4),\n            (i + 5) * dim * dim + (i + 5) * dim + (i + 5),\n            (i + 6) * dim * dim + (i + 6) * dim + (i + 6),\n            (i + 7) * dim * dim + (i + 7) * dim + (i + 7)\n        };\n\n        // Load diagonal elements into an AVX register\n        __m256 diag_vec = _mm256_set_ps(\n            array[indices[7]],\n            array[indices[6]],\n            array[indices[5]],\n            array[indices[4]],\n            array[indices[3]],\n            array[indices[2]],\n            array[indices[1]],\n            array[indices[0]]\n        );\n\n        // Accumulate the sum\n        sum_vec = _mm256_add_ps(sum_vec, diag_vec);\n    }\n\n    // Horizontal sum of the AVX register\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Process remaining elements\n    for (; i < diagonal_count; i++) {\n        size_t index = i * dim * dim + i * dim + i;\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    size_t simd_width = 8; // AVX works with 8 floats at a time\n    size_t simd_iterations = diagonal_count / simd_width; // Number of full SIMD iterations\n    size_t remainder = diagonal_count % simd_width; // Remaining elements after SIMD processing\n\n    __m256 sum_vec = _mm256_setzero_ps(); // Initialize SIMD vector for summation\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        size_t base_index = i * simd_width;\n        __m256 indices_vec = _mm256_set_ps(\n            base_index + 7, base_index + 6, base_index + 5, base_index + 4,\n            base_index + 3, base_index + 2, base_index + 1, base_index + 0\n        );\n\n        __m256 values_vec = _mm256_i32gather_ps(array, _mm256_cvtps_epi32(indices_vec), sizeof(float));\n        sum_vec = _mm256_add_ps(sum_vec, values_vec);\n    }\n\n    float sum = 0.0f;\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < diagonal_count; ++i) {\n        sum += array[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    size_t simd_width = 8;       // AVX processes 8 floats at a time\n    size_t simd_iterations = diagonal_count / simd_width;\n    size_t remainder = diagonal_count % simd_width;\n\n    __m256 sum_vec = _mm256_setzero_ps(); // Initialize SIMD sum vector to zero\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        size_t base_index = i * simd_width * (dim * dim + dim + 1);\n        __m256 diag_elements = _mm256_set_ps(\n            array[base_index + 7 * (dim * dim + dim + 1)],\n            array[base_index + 6 * (dim * dim + dim + 1)],\n            array[base_index + 5 * (dim * dim + dim + 1)],\n            array[base_index + 4 * (dim * dim + dim + 1)],\n            array[base_index + 3 * (dim * dim + dim + 1)],\n            array[base_index + 2 * (dim * dim + dim + 1)],\n            array[base_index + 1 * (dim * dim + dim + 1)],\n            array[base_index + 0 * (dim * dim + dim + 1)]\n        );\n        sum_vec = _mm256_add_ps(sum_vec, diag_elements);\n    }\n\n    // Horizontal sum of the SIMD vector\n    float sum_array[8];\n    _mm256_storeu_ps(sum_array, sum_vec);\n    float total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                      sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n    // Handle remaining elements\n    for (size_t i = simd_iterations * simd_width; i < diagonal_count; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        total_sum += array[index];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_iterations = diagonal_count / simd_width;\n    size_t remainder = diagonal_count % simd_width;\n\n    __m256 sum_vec = _mm256_setzero_ps(); // Initialize SIMD vector for sum\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        size_t base_index = i * simd_width * (dim * dim + dim + 1);\n        __m256 diag_elements = _mm256_loadu_ps(&array[base_index]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_elements);\n    }\n\n    // Horizontal sum of SIMD vector\n    float sum_array[simd_width];\n    _mm256_storeu_ps(sum_array, sum_vec);\n    float total_sum = 0.0f;\n    for (size_t i = 0; i < simd_width; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Handle remainder elements\n    for (size_t i = simd_iterations * simd_width; i < diagonal_count; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        total_sum += array[index];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_iterations = diagonal_count / simd_width;\n    size_t remainder = diagonal_count % simd_width;\n\n    __m256 sum_vec = _mm256_setzero_ps(); // Initialize SIMD vector for accumulation\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        size_t base_index = i * simd_width * (dim * dim + dim + 1);\n        __m256 diag_elements = _mm256_loadu_ps(&array[base_index]);\n        sum_vec = _mm256_add_ps(sum_vec, diag_elements);\n    }\n\n    // Horizontal sum of the SIMD vector\n    float sum[simd_width];\n    _mm256_storeu_ps(sum, sum_vec);\n    float total_sum = 0.0f;\n    for (size_t i = 0; i < simd_width; ++i) {\n        total_sum += sum[i];\n    }\n\n    // Handle remainder elements\n    for (size_t i = simd_iterations * simd_width; i < diagonal_count; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        total_sum += array[index];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t aligned_elements = total_elements / simd_width * simd_width;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < aligned_elements; i += simd_width) {\n        __m256d src_vec = _mm256_loadu_pd(&src[i]);\n        __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n        __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n        __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d src_vec = _mm256_loadu_pd(&src[i]);\n        __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n        __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n        __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < total_elements; i++) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256d src_vec = _mm256_loadu_pd(src + i * simd_width);\n        __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n        __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n        __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n        _mm256_storeu_pd(dst + i * simd_width, result_vec);\n    }\n\n    // Handle remainder elements\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    // Load threshold and scale into AVX registers\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    // Process 4 elements at a time\n    for (; i + 3 < total_elements; i += 4) {\n        // Load 4 double values from the source array\n        __m256d src_vec = _mm256_loadu_pd(&src[i]);\n\n        // Compare each value with the threshold\n        __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n\n        // Multiply values greater than the threshold by the scale factor\n        __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n\n        // Blend the scaled and original values based on the mask\n        __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n\n        // Store the result in the destination array\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    // Load threshold and scale into AVX registers\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n\n    // Process 4 elements at a time\n    for (; i + 3 < total_elements; i += 4) {\n        // Load 4 elements from the source array\n        __m256d src_vec = _mm256_loadu_pd(&src[i]);\n\n        // Compare elements with the threshold\n        __m256d mask = _mm256_cmp_pd(src_vec, threshold_vec, _CMP_GT_OQ);\n\n        // Scale the elements that are greater than the threshold\n        __m256d scaled_vec = _mm256_mul_pd(src_vec, scale_vec);\n\n        // Blend the scaled and original values based on the mask\n        __m256d result_vec = _mm256_blendv_pd(src_vec, scaled_vec, mask);\n\n        // Store the result in the destination array\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t row_size = cols * sizeof(double); // Size of a row in bytes\n    size_t simd_width = 4; // Number of doubles processed in one AVX2 register (256 bits)\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i]; // Get the source row index\n        const double *src_row = src + src_row_idx * cols; // Pointer to the source row\n        double *dst_row = dst + i * cols; // Pointer to the destination row\n\n        size_t j = 0;\n        // Process columns in chunks of 4 doubles using AVX2\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d src_vec = _mm256_loadu_pd(src_row + j); // Load 4 doubles from the source row\n            _mm256_storeu_pd(dst_row + j, src_vec); // Store 4 doubles to the destination row\n        }\n\n        // Process remaining columns that don't fit into a SIMD register\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t num_elements_per_row = cols;\n    size_t simd_width = 4; // AVX processes 4 doubles per instruction\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * num_elements_per_row;\n        double *dst_row = dst + i * num_elements_per_row;\n\n        size_t j = 0;\n        for (; j + simd_width <= num_elements_per_row; j += simd_width) {\n            __m256d src_vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, src_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < num_elements_per_row; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t row_size = cols; // Number of elements in a row\n    size_t simd_width = 4;  // AVX processes 4 doubles at a time\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * row_size;\n        double *dst_row = dst + i * row_size;\n\n        size_t j = 0;\n        // Process elements in chunks of 4 using AVX\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d src_vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, src_vec);\n        }\n\n        // Process remaining elements (if cols is not a multiple of 4)\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_stride = cols; // Number of columns in the matrix\n    size_t simd_width = 4;    // AVX processes 4 doubles (64-bit) at a time\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i]; // Get the source row index from the indices array\n        const double *src_row_ptr = src + src_row_idx * col_stride;\n        double *dst_row_ptr = dst + i * col_stride;\n\n        size_t j = 0;\n        // Process columns in chunks of 4 using AVX\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d src_data = _mm256_loadu_pd(src_row_ptr + j); // Load 4 doubles from the source row\n            _mm256_storeu_pd(dst_row_ptr + j, src_data);         // Store 4 doubles into the destination row\n        }\n\n        // Handle remaining columns that don't fit into a full AVX register\n        for (; j < cols; ++j) {\n            dst_row_ptr[j] = src_row_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t row_size = cols * sizeof(double); // Size of a row in bytes\n    size_t simd_width = 4; // Number of doubles processed per AVX2 register (256 bits / 64 bits per double)\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i]; // Get the source row index from the indices array\n        const double *src_row = src + src_row_index * cols; // Pointer to the source row\n        double *dst_row = dst + i * cols; // Pointer to the destination row\n\n        size_t j = 0;\n        // Process columns in chunks of 4 doubles using AVX2\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d src_vec = _mm256_loadu_pd(src_row + j); // Load 4 doubles from the source row\n            _mm256_storeu_pd(dst_row + j, src_vec); // Store 4 doubles to the destination row\n        }\n\n        // Process remaining columns (if any) sequentially\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m256i divisor_vec = _mm256_set1_epi64x(2);    // Broadcast divisor (2) to all elements\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)(src1 + i)); // Load 4 elements from src1\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)(src2 + i)); // Load 4 elements from src2\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);               // Element-wise addition of src1 and src2\n        __m256i sum_with_scalar = _mm256_add_epi64(sum, scalar_vec); // Add scalar to each element\n        __m256i result = _mm256_div_epi64(sum_with_scalar, divisor_vec); // Divide by 2 (round-toward-zero)\n\n        _mm256_storeu_si256((__m256i *)(dst + i), result);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m256i divisor_vec = _mm256_set1_epi64x(2);    // Broadcast divisor (2) to all elements\n\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) { // Process 4 elements at a time\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)&src1[i]); // Load 4 elements from src1\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)&src2[i]); // Load 4 elements from src2\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);             // Element-wise addition of src1 and src2\n        __m256i sum_with_scalar = _mm256_add_epi64(sum, scalar_vec); // Add scalar to each element\n        __m256i result = _mm256_srai_epi64(sum_with_scalar, 1); // Divide by 2 (round-toward-zero)\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m256i divisor = _mm256_set1_epi64x(2);        // Broadcast divisor (2) to all elements\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i *)&src1[i]); // Load 4 elements from src1\n        __m256i vec2 = _mm256_loadu_si256((__m256i *)&src2[i]); // Load 4 elements from src2\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);             // Element-wise addition of src1 and src2\n        __m256i sum_with_scalar = _mm256_add_epi64(sum, scalar_vec); // Add scalar to each element\n        __m256i result = _mm256_srai_epi64(_mm256_add_epi64(sum_with_scalar, divisor), 1); // Divide by 2 (round-toward-zero)\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result);        // Store result in dst\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar); // Broadcast scalar to all elements of a 256-bit vector\n    __m256i divisor_vec = _mm256_set1_epi64x(2);    // Broadcast divisor (2) to all elements of a 256-bit vector\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]); // Load 4 elements from src1\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]); // Load 4 elements from src2\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);            // Element-wise addition of src1 and src2\n        __m256i sum_with_scalar = _mm256_add_epi64(sum, scalar_vec); // Add scalar to each element\n        __m256i result = _mm256_srai_epi64(_mm256_add_epi64(sum_with_scalar, divisor_vec), 1); // Divide by 2 (round-toward-zero)\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i divisor_vec = _mm256_set1_epi64x(2);\n    \n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const *)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i const *)(src2 + i));\n        \n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        __m256i sum_with_scalar = _mm256_add_epi64(sum, scalar_vec);\n        __m256i result = _mm256_srai_epi64(_mm256_add_epi64(sum_with_scalar, divisor_vec), 1);\n        \n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i] + scalar) / 2);\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX); // Initialize with max int value\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i)); // Load 8 integers\n        __m256i mask = _mm256_set_epi32(\n            (i + 7) % 2 == 0 ? -1 : 0,\n            (i + 6) % 2 == 0 ? -1 : 0,\n            (i + 5) % 2 == 0 ? -1 : 0,\n            (i + 4) % 2 == 0 ? -1 : 0,\n            (i + 3) % 2 == 0 ? -1 : 0,\n            (i + 2) % 2 == 0 ? -1 : 0,\n            (i + 1) % 2 == 0 ? -1 : 0,\n            (i + 0) % 2 == 0 ? -1 : 0\n        ); // Create mask for even indices\n        __m256i masked_data = _mm256_and_si256(data, mask); // Mask out odd indices\n        min_vec = _mm256_min_epi32(min_vec, masked_data); // Compute minimum\n    }\n\n    // Extract minimum value from min_vec\n    int min_values[8];\n    _mm256_storeu_si256((__m256i *)min_values, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (min_values[j] < min_value) {\n            min_value = min_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX); // Initialize with maximum integer value\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i)); // Load 8 integers\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1)); // Mask to check even numbers\n        __m256i even_mask = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256()); // Compare to zero (even numbers)\n\n        __m256i filtered_data = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, even_mask); // Replace odd numbers with INT_MAX\n        min_vec = _mm256_min_epi32(min_vec, filtered_data); // Find minimum\n    }\n\n    // Reduce the min_vec to a single minimum value\n    int min_values[8];\n    _mm256_storeu_si256((__m256i *)min_values, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (min_values[j] < min_value) {\n            min_value = min_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value == INT_MAX ? -1 : min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX); // Initialize with maximum integer value\n    __m256i even_mask = _mm256_set1_epi32(1);    // Mask to check even numbers\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i)); // Load 8 integers\n        __m256i is_even = _mm256_and_si256(data, even_mask);     // Check if numbers are even\n        __m256i cmp_mask = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256()); // Compare to zero (even numbers)\n\n        __m256i filtered = _mm256_or_si256(_mm256_and_si256(cmp_mask, data), _mm256_andnot_si256(cmp_mask, min_vec));\n        min_vec = _mm256_min_epi32(min_vec, filtered); // Update minimum\n    }\n\n    int min_val = INT_MAX;\n    int temp[8];\n    _mm256_storeu_si256((__m256i *)temp, min_vec); // Store the vector to a temporary array\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == INT_MAX ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i even_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n        __m256i filtered = _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), data, is_even);\n        min_vec = _mm256_min_epi32(min_vec, filtered);\n    }\n\n    // Reduce the minimum value in the vector\n    int min_values[8];\n    _mm256_storeu_si256((__m256i *)min_values, min_vec);\n    int min_result = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (min_values[j] < min_result) {\n            min_result = min_values[j];\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i += 2) {\n        if (src[i] < min_result) {\n            min_result = src[i];\n        }\n    }\n\n    return min_result == INT_MAX ? -1 : min_result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX); // Initialize min vector with INT_MAX\n    __m256i two_vec = _mm256_set1_epi32(2);      // Vector of 2 for modulus operation\n    __m256i zero_vec = _mm256_setzero_si256();   // Vector of zeros\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i)); // Load 8 integers\n        __m256i mod = _mm256_and_si256(data, _mm256_set1_epi32(1)); // Check if even (mod 2)\n        __m256i mask = _mm256_cmpeq_epi32(mod, zero_vec); // Mask for even numbers\n\n        __m256i filtered = _mm256_or_si256(_mm256_and_si256(mask, data), _mm256_andnot_si256(mask, _mm256_set1_epi32(INT_MAX))); // Replace odd numbers with INT_MAX\n        min_vec = _mm256_min_epi32(min_vec, filtered); // Update min vector\n    }\n\n    // Reduce the min_vec to find the minimum value\n    int min_array[8];\n    _mm256_storeu_si256((__m256i *)min_array, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (min_array[j] < min_value) {\n            min_value = min_array[j];\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value == INT_MAX ? -1 : min_value;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m256i v_arith_shift = _mm256_srai_epi32(v_src, shift);\n\n        // Perform logical right shift\n        __m256i v_logical_shift = _mm256_srlv_epi32(_mm256_and_si256(v_src, _mm256_set1_epi32(0xFFFFFFFF)), v_shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m256i v_result = _mm256_add_epi32(v_arith_shift, v_logical_shift);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift;\n        int logical_shift = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i)); // Load 8 integers from src\n\n        // Arithmetic right shift\n        __m256i v_arithmetic = _mm256_srai_epi32(v_src, shift);\n\n        // Logical right shift\n        __m256i v_unsigned = _mm256_srli_epi32(v_src, shift);\n\n        // Add results\n        __m256i v_result = _mm256_add_epi32(v_arithmetic, v_unsigned);\n\n        // Store result in dst\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arithmetic = src[i] >> shift; // Arithmetic right shift\n        unsigned int logical = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arithmetic + (int)logical; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32(shift);\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Perform arithmetic right shift\n        __m256i v_arith_shift = _mm256_srai_epi32(v_src, shift);\n\n        // Perform logical right shift\n        __m256i v_logical_shift = _mm256_srlv_epi32(_mm256_castsi256_si256(v_src), v_shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m256i v_result = _mm256_add_epi32(v_arith_shift, v_logical_shift);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift; // Arithmetic right shift\n        int logical_shift = (int)((unsigned int)src[i] >> shift); // Logical right shift\n        dst[i] = arith_shift + logical_shift; // Add the results\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m256i v_arithmetic = _mm256_srai_epi32(v_src, shift);\n\n        // Perform logical right shift\n        __m256i v_logical = _mm256_srlv_epi32(_mm256_castsi256_si256(v_src), v_shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m256i v_result = _mm256_add_epi32(v_arithmetic, v_logical);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arithmetic = src[i] >> shift; // Arithmetic right shift\n        unsigned int logical = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arithmetic + (int)logical; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i v_shift = _mm256_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Perform arithmetic right shift\n        __m256i v_arith_shift = _mm256_srai_epi32(v_src, shift);\n\n        // Perform logical right shift\n        __m256i v_unsigned_src = _mm256_and_si256(v_src, _mm256_set1_epi32(0xFFFFFFFF)); // Mask to treat as unsigned\n        __m256i v_logical_shift = _mm256_srlv_epi32(v_unsigned_src, v_shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m256i v_result = _mm256_add_epi32(v_arith_shift, v_logical_shift);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        int arith_shift = src[i] >> shift; // Arithmetic right shift\n        int logical_shift = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arith_shift + logical_shift; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector sum to zero\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(matrix + i)); // Load 8 integers\n        sum_vec = _mm256_add_epi32(sum_vec, vec); // Accumulate sum\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec); // Extract lower 128 bits\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1); // Extract higher 128 bits\n    __m128i sum128 = _mm_add_epi32(low, high); // Add lower and higher parts\n\n    // Further reduce sum128 to a single integer\n    sum128 = _mm_hadd_epi32(sum128, sum128); // Horizontal add\n    sum128 = _mm_hadd_epi32(sum128, sum128); // Horizontal add again\n\n    int partial_sum = _mm_cvtsi128_si32(sum128); // Extract the final sum\n\n    // Handle remaining elements\n    int64_t total_sum = partial_sum;\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector sum to zero\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)&matrix[i]); // Load 8 integers into vector\n        sum_vec = _mm256_add_epi32(sum_vec, vec); // Add vector to sum_vec\n    }\n\n    // Horizontal sum of sum_vec\n    int temp[simd_width];\n    _mm256_storeu_si256((__m256i *)temp, sum_vec);\n    int64_t sum = 0;\n    for (size_t j = 0; j < simd_width; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector sum to zero\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256i data_vec = _mm256_loadu_si256((__m256i *)&matrix[i]); // Load 8 integers\n        sum_vec = _mm256_add_epi32(sum_vec, data_vec); // Accumulate the sum\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec); // Lower 128 bits\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1); // Upper 128 bits\n    __m128i sum128 = _mm_add_epi32(low, high); // Add lower and upper parts\n\n    // Further horizontal sum within the 128-bit vector\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 8)); // Add pairs of 32-bit integers\n    sum128 = _mm_add_epi32(sum128, _mm_srli_si128(sum128, 4)); // Add remaining pairs\n\n    int partial_sum = _mm_cvtsi128_si32(sum128); // Extract the final sum from the 128-bit vector\n\n    // Handle remaining elements (if total_elements is not a multiple of simd_width)\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector sum to zero\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(matrix + i)); // Load 8 integers\n        sum_vec = _mm256_add_epi32(sum_vec, vec); // Accumulate sum\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec); // Lower 128 bits\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1); // Upper 128 bits\n    __m128i sum128 = _mm_add_epi32(low, high); // Add lower and upper parts\n\n    // Extract individual sums from 128-bit vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum128);\n    int64_t total_sum = (int64_t)temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector to zero\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)&matrix[i]); // Load 8 integers\n        sum_vec = _mm256_add_epi32(sum_vec, vec); // Accumulate sum in vector\n    }\n\n    // Horizontal sum of the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i *)temp, sum_vec);\n    int64_t sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_values = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_indices = _mm256_setzero_si256();\n    __m256i index_offset = _mm256_set_epi8(\n        31, 30, 29, 28, 27, 26, 25, 24,\n        23, 22, 21, 20, 19, 18, 17, 16,\n        15, 14, 13, 12, 11, 10, 9, 8,\n        7, 6, 5, 4, 3, 2, 1, 0\n    );\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    for (size_t i = 0; i < length; i += 32) {\n        size_t remaining = length - i;\n        __m256i current_values;\n\n        if (remaining >= 32) {\n            current_values = _mm256_loadu_si256((__m256i *)(src + i));\n        } else {\n            int8_t temp[32] = {0};\n            for (size_t j = 0; j < remaining; ++j) {\n                temp[j] = src[i + j];\n            }\n            current_values = _mm256_loadu_si256((__m256i *)temp);\n        }\n\n        __m256i cmp_mask = _mm256_cmpgt_epi8(current_values, max_values);\n        max_values = _mm256_max_epi8(max_values, current_values);\n        max_indices = _mm256_blendv_epi8(max_indices, index_offset, cmp_mask);\n\n        index_offset = _mm256_add_epi8(index_offset, _mm256_set1_epi8(32));\n    }\n\n    int8_t max_values_array[32];\n    int8_t max_indices_array[32];\n    _mm256_storeu_si256((__m256i *)max_values_array, max_values);\n    _mm256_storeu_si256((__m256i *)max_indices_array, max_indices);\n\n    for (int i = 0; i < 32; ++i) {\n        if (max_values_array[i] > max_value) {\n            max_value = max_values_array[i];\n            max_index = max_indices_array[i];\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_vals = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_indices = _mm256_setzero_si256();\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i cmp_mask = _mm256_cmpgt_epi8(data, max_vals);\n        max_vals = _mm256_max_epi8(max_vals, data);\n\n        __m256i indices = _mm256_add_epi8(_mm256_set1_epi8(i), _mm256_set_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24,\n            23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n        max_indices = _mm256_blendv_epi8(max_indices, indices, cmp_mask);\n    }\n\n    alignas(32) int8_t max_vals_array[32];\n    alignas(32) int8_t max_indices_array[32];\n    _mm256_store_si256((__m256i *)max_vals_array, max_vals);\n    _mm256_store_si256((__m256i *)max_indices_array, max_indices);\n\n    for (int j = 0; j < 32; ++j) {\n        if (max_vals_array[j] > max_value) {\n            max_value = max_vals_array[j];\n            max_index = max_indices_array[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_values = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_indices = _mm256_setzero_si256();\n    __m256i index_increment = _mm256_set1_epi8(1);\n    __m256i current_indices = _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24,\n                                              23, 22, 21, 20, 19, 18, 17, 16,\n                                              15, 14, 13, 12, 11, 10, 9, 8,\n                                              7, 6, 5, 4, 3, 2, 1, 0);\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    for (size_t i = 0; i < length; i += 32) {\n        size_t remaining = length - i;\n        __m256i values = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(values, max_values);\n        max_values = _mm256_blendv_epi8(max_values, values, mask);\n        max_indices = _mm256_blendv_epi8(max_indices, current_indices, mask);\n        current_indices = _mm256_add_epi8(current_indices, index_increment);\n\n        if (remaining < 32) {\n            break;\n        }\n    }\n\n    int8_t temp_values[32];\n    int8_t temp_indices[32];\n    _mm256_storeu_si256((__m256i *)temp_values, max_values);\n    _mm256_storeu_si256((__m256i *)temp_indices, max_indices);\n\n    for (int i = 0; i < 32; ++i) {\n        if (temp_values[i] > max_value) {\n            max_value = temp_values[i];\n            max_index = i + (length > 32 ? (length - 32) : 0);\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_values = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_indices = _mm256_setzero_si256();\n    __m256i indices = _mm256_setzero_si256();\n    __m256i increment = _mm256_set1_epi8(1);\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i current_values = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(current_values, max_values);\n        max_values = _mm256_blendv_epi8(max_values, current_values, mask);\n        max_indices = _mm256_blendv_epi8(max_indices, indices, mask);\n        indices = _mm256_add_epi8(indices, increment);\n    }\n\n    // Reduce max_values and max_indices to find the maximum value and its index\n    int8_t max_value = INT8_MIN;\n    size_t max_index = 0;\n    alignas(32) int8_t values[32];\n    alignas(32) int8_t indices_arr[32];\n    _mm256_store_si256((__m256i *)values, max_values);\n    _mm256_store_si256((__m256i *)indices_arr, max_indices);\n\n    for (int j = 0; j < 32; ++j) {\n        if (values[j] > max_value) {\n            max_value = values[j];\n            max_index = indices_arr[j];\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_values = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_indices = _mm256_setzero_si256();\n    __m256i indices = _mm256_setzero_si256();\n    __m256i increment = _mm256_set1_epi8(1);\n\n    size_t i = 0;\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i current_values = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i cmp_mask = _mm256_cmpgt_epi8(current_values, max_values);\n\n        max_values = _mm256_blendv_epi8(max_values, current_values, cmp_mask);\n        max_indices = _mm256_blendv_epi8(max_indices, indices, cmp_mask);\n\n        indices = _mm256_add_epi8(indices, increment);\n    }\n\n    int8_t temp_values[32];\n    int8_t temp_indices[32];\n    _mm256_storeu_si256((__m256i *)temp_values, max_values);\n    _mm256_storeu_si256((__m256i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 32; ++j) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j] + i - 32;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask); // Broadcast mask to all lanes\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_setr_epi32(\n            i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i indices_high = _mm256_setr_epi32(\n            i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n\n        __m256i src1_vec = _mm256_loadu_si256((const __m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((const __m256i*)(src2 + i));\n        __m256i blend = _mm256_blend_epi16\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi16((uint16_t)mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi16(\n            i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i\n        );\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i index_mask = _mm256_and_si256(indices, mask_vec);\n        __m256i blend_mask = _mm256_cmpeq_epi16(index_mask, _mm256_setzero_si256());\n\n        __m256i result = _mm256_blendv_epi8(src1_vec, src2_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask); // Broadcast mask to all lanes\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi32(i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8,\n                                           i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i)); // Load 16 elements from src1\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i)); // Load 16 elements from src2\n\n        __m256i index_mask = _mm256_and_si256(indices, mask_vec); // Perform bitwise AND with indices and mask\n        __m256i blend_mask = _mm256_cmpeq_epi32(index_mask, _mm256_setzero_si256()); // Compare result to zero\n\n        __m256i result = _mm256_blendv_epi8(src1_vec, src2_vec, blend_mask); // Blend based on mask\n        _mm256_storeu_si256((__m256i*)(dst + i), result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi16((uint16_t)mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi16(\n            i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i\n        );\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i index_mask = _mm256_and_si256(indices, mask_vec);\n        __m256i blend_mask = _mm256_cmpgt_epi16(index_mask, _mm256_setzero_si256());\n\n        __m256i result = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vector = _mm256_set1_epi16((uint16_t)mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi16(\n            i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i\n        );\n\n        __m256i src1_vector = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vector = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i index_mask = _mm256_and_si256(indices, mask_vector);\n        __m256i blend_mask = _mm256_cmpeq_epi16(index_mask, _mm256_setzero_si256());\n\n        __m256i result = _mm256_blendv_epi8(src1_vector, src2_vector, blend_mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask1 = _mm256_set1_epi32(0x55555555); // Mask for bits 0x01\n    __m256i mask2 = _mm256_set1_epi32(0x33333333); // Mask for bits 0x03\n    __m256i mask4 = _mm256_set1_epi32(0x0F0F0F0F); // Mask for bits 0x0F\n    __m256i mask8 = _mm256_set1_epi32(0x01010101); // Mask for final sum\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Step 1: Count bits in each 2-bit group\n        __m256i count = _mm256_sub_epi32(data, _mm256_and_si256(_mm256_srli_epi32(data, 1), mask1));\n\n        // Step 2: Count bits in each 4-bit group\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask2), _mm256_and_si256(_mm256_srli_epi32(count, 2), mask2));\n\n        // Step 3: Count bits in each 8-bit group\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask4), _mm256_and_si256(_mm256_srli_epi32(count, 4), mask4));\n\n        // Step 4: Sum up the bits in each 32-bit integer\n        count = _mm256_mullo_epi32(count, mask8);\n        count = _mm256_srli_epi32(count, 24);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), count);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 8; // AVX2 processes 8 uint32_t values at a time\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 uint32_t values\n        __m256i popcnt = _mm256_setzero_si256(); // Initialize population count vector\n\n        // Calculate population count using bitwise operations\n        for (int j = 0; j < 32; ++j) {\n            __m256i bit_mask = _mm256_set1_epi32(1 << j); // Create mask for the j-th bit\n            __m256i bit_set = _mm256_and_si256(vec, bit_mask); // Extract the j-th bit\n            __m256i bit_count = _mm256_cmpgt_epi32(bit_set, _mm256_setzero_si256()); // Count if bit is set\n            popcnt = _mm256_add_epi32(popcnt, bit_count); // Accumulate population count\n        }\n\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt); // Store the result\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; ++i) {\n        uint32_t value = src[i];\n        uint32_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 8; // AVX2 processes 8 uint32_t values at a time\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vec_src = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 uint32_t values\n        __m256i popcnt_vec = _mm256_setzero_si256();\n\n        // Calculate population count for each 32-bit integer\n        for (int j = 0; j < 32; j++) {\n            __m256i bit_mask = _mm256_set1_epi32(1 << j);\n            __m256i bit_set = _mm256_and_si256(vec_src, bit_mask);\n            __m256i bit_count = _mm256_cmpgt_epi32(bit_set, _mm256_setzero_si256());\n            popcnt_vec = _mm256_add_epi32(popcnt_vec, bit_count);\n        }\n\n        _mm256_storeu_si256((__m256i*)&dst[i], popcnt_vec); // Store the result\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        uint32_t value = src[i];\n        uint32_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 uint32_t elements from the source array\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Perform population count on each 32-bit integer\n        __m256i popcnt = _mm256_setzero_si256();\n        for (int shift = 0; shift < 32; shift++) {\n            __m256i bit = _mm256_and_si256(data, _mm256_set1_epi32(1));\n            popcnt = _mm256_add_epi32(popcnt, bit);\n            data = _mm256_srli_epi32(data, 1);\n        }\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), popcnt);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        uint32_t value = src[i];\n        uint32_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask1 = _mm256_set1_epi32(0x55555555); // Mask for bits 0x01\n    __m256i mask2 = _mm256_set1_epi32(0x33333333); // Mask for bits 0x03\n    __m256i mask4 = _mm256_set1_epi32(0x0F0F0F0F); // Mask for bits 0x0F\n    __m256i mask8 = _mm256_set1_epi32(0x01010101); // Mask for final sum\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Step 1: Count bits in each 2-bit group\n        __m256i count = _mm256_sub_epi32(data, _mm256_and_si256(_mm256_srli_epi32(data, 1), mask1));\n\n        // Step 2: Count bits in each 4-bit group\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask2), _mm256_and_si256(_mm256_srli_epi32(count, 2), mask2));\n\n        // Step 3: Count bits in each 8-bit group\n        count = _mm256_and_si256(_mm256_add_epi32(count, _mm256_srli_epi32(count, 4)), mask4);\n\n        // Step 4: Sum all bytes to get the population count\n        count = _mm256_mullo_epi32(count, mask8);\n        count = _mm256_srli_epi32(count, 24);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), count);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 16; // AVX2 processes 16 uint16_t elements at a time\n\n    // Process elements in chunks of 16 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 16 elements from src1 and src2 into AVX2 registers\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        // Perform saturating addition\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n\n        // Store the result back to the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Process remaining elements that don't fit into a full SIMD register\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= length; i += 16) {\n        // Load 16 uint16_t elements from src1 and src2\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        // Perform saturated addition\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 16; // Each __m256i can hold 16 uint16_t values\n\n    // Process in chunks of 16 elements using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 16 uint16_t elements from src1 and src2\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Perform saturating addition\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n\n        // Store the result back to dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Handle the remaining elements that don't fit into a full SIMD register\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 16; // AVX2 processes 16 uint16_t values at a time\n\n    // Process elements in chunks of 16 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 16 uint16_t values from src1 and src2\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        // Perform saturated addition\n        __m256i v_result = _mm256_adds_epu16(v_src1, v_src2);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 16; // AVX2 processes 16 uint16_t elements per iteration\n\n    // Process elements in chunks of 16 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 16 uint16_t elements from src1 and src2\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        // Perform saturating addition\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Process remaining elements that don't fit into a full SIMD register\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vals = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vals = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d a_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                __m256d b_mask = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                __m256d valid_mask = _mm256_and_pd(a_mask, b_mask);\n                __m256d product = _mm256_mul_pd(a_vals, b_vals);\n                product = _mm256_and_pd(product, valid_mask);\n\n                sum = _mm256_add_pd(sum, product);\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from A and B\n                __m256d a_vals = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vals = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check if A elements are within range [-100, 100]\n                __m256d a_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Check if B elements are within range [-100, 100]\n                __m256d b_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Combine range masks for A and B\n                __m256d valid_mask = _mm256_and_pd(a_in_range, b_in_range);\n\n                // Perform multiplication\n                __m256d product = _mm256_mul_pd(a_vals, b_vals);\n\n                // Mask invalid values (set them to zero)\n                product = _mm256_and_pd(product, valid_mask);\n\n                // Accumulate the sum\n                sum = _mm256_add_pd(sum, product);\n            }\n\n            // Horizontal sum of the 4 lanes in the AVX register\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from A and B\n                __m256d a_vals = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vals = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check if A elements are in range [-100, 100]\n                __m256d a_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Check if B elements are in range [-100, 100]\n                __m256d b_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Combine range masks for A and B\n                __m256d valid_mask = _mm256_and_pd(a_in_range, b_in_range);\n\n                // Multiply A and B\n                __m256d product = _mm256_mul_pd(a_vals, b_vals);\n\n                // Mask out invalid products\n                product = _mm256_and_pd(product, valid_mask);\n\n                // Accumulate the valid products\n                sum = _mm256_add_pd(sum, product);\n            }\n\n            // Horizontal sum of the accumulated values\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from A and B\n                __m256d a_vals = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vals = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check if elements of A are within range [-100, 100]\n                __m256d a_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(a_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Check if elements of B are within range [-100, 100]\n                __m256d b_in_range = _mm256_and_pd(\n                    _mm256_cmp_pd(b_vals, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vals, upper_bound, _CMP_LE_OQ)\n                );\n\n                // Mask out-of-range elements\n                __m256d a_masked = _mm256_and_pd(a_vals, a_in_range);\n                __m256d b_masked = _mm256_and_pd(b_vals, b_in_range);\n\n                // Perform multiplication\n                __m256d product = _mm256_mul_pd(a_masked, b_masked);\n\n                // Accumulate the result\n                sum = _mm256_add_pd(sum, product);\n            }\n\n            // Horizontal sum of the accumulated values\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from A and B\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check range for A\n                __m256d a_mask_lower = _mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OQ);\n                __m256d a_mask_upper = _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OQ);\n                __m256d a_mask = _mm256_and_pd(a_mask_lower, a_mask_upper);\n\n                // Check range for B\n                __m256d b_mask_lower = _mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OQ);\n                __m256d b_mask_upper = _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OQ);\n                __m256d b_mask = _mm256_and_pd(b_mask_lower, b_mask_upper);\n\n                // Combine masks\n                __m256d combined_mask = _mm256_and_pd(a_mask, b_mask);\n\n                // Perform multiplication and apply mask\n                __m256d mul_vec = _mm256_mul_pd(a_vec, b_vec);\n                __m256d masked_mul_vec = _mm256_and_pd(mul_vec, combined_mask);\n\n                // Accumulate results\n                sum_vec = _mm256_add_pd(sum_vec, masked_mul_vec);\n            }\n\n            // Horizontal sum of sum_vec\n            double temp[4];\n            _mm256_storeu_pd(temp, sum_vec);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // AVX2 can process 4 64-bit integers at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i * simd_width));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i * simd_width));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i * simd_width), vecC);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers at a time\n    size_t aligned_elements = total_elements - (total_elements % simd_width);\n\n    for (size_t i = 0; i < aligned_elements; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2 intrinsics\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle the remaining elements (if any)\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // AVX2 can process 4 64-bit integers at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i * simd_width));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i * simd_width));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i * simd_width), vecC);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);  // Load 8 floats from src1\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);  // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(vec1, vec2);   // Perform element-wise addition\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm256_storeu_ps(dst + i, rounded);       // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i); // Load 8 floats from src1\n        __m256 vec2 = _mm256_loadu_ps(src2 + i); // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(vec1, vec2);  // Element-wise addition\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm256_storeu_ps(dst + i, rounded);      // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i); // Load 8 floats from src1\n        __m256 vec2 = _mm256_loadu_ps(src2 + i); // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(vec1, vec2);  // Element-wise addition\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm256_storeu_ps(dst + i, rounded);      // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process in chunks of 8 floats\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]); // Load 8 floats from src1\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]); // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(vec1, vec2);  // Element-wise addition\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm256_storeu_ps(&dst[i], rounded); // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        float sum = src1[i] + src2[i];\n        dst[i] = roundf(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i); // Load 8 floats from src1\n        __m256 vec2 = _mm256_loadu_ps(src2 + i); // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(vec1, vec2);  // Element-wise addition\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm256_storeu_ps(dst + i, rounded);      // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n    __m256 vec_fifty = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        __m256 vec_diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 vec_abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), vec_diff);\n\n        // Check condition: abs(diff) <= 50\n        __m256 mask_diff = _mm256_cmp_ps(vec_abs_diff, vec_fifty, _CMP_LE_OQ);\n\n        // Check condition: index is a multiple of 2\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_indices = _mm256_and_si256(vec_indices, _mm256_set1_epi32(1));\n        __m256 mask_even = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mask_indices, _mm256_setzero_si256()));\n\n        // Combine masks\n        __m256 mask = _mm256_and_ps(mask_diff, mask_even);\n\n        // Perform multiplication\n        __m256 vec_mul = _mm256_mul_ps(vec_src1, vec_src2);\n\n        // Round up toward positive infinity\n        __m256 vec_ceil = _mm256_ceil_ps(vec_mul);\n\n        // Select results based on mask\n        __m256 vec_result = _mm256_blendv_ps(vec_neg_one, vec_ceil, mask);\n\n        // Store results\n        _mm256_storeu_ps(dst + i, vec_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50 && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n    __m256 vec_fifty = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 vec_diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 vec_abs_diff = _mm256_and_ps(vec_diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n\n        __m256 mask_within_50 = _mm256_cmp_ps(vec_abs_diff, vec_fifty, _CMP_LE_OS);\n\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_even_indices = _mm256_cmpeq_epi32(_mm256_and_si256(vec_indices, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n\n        __m256 mask_conditions = _mm256_and_ps(mask_within_50, _mm256_castsi256_ps(mask_even_indices));\n\n        __m256 vec_mul = _mm256_mul_ps(vec_src1, vec_src2);\n        __m256 vec_ceil = _mm256_ceil_ps(vec_mul);\n\n        __m256 vec_result = _mm256_blendv_ps(vec_neg_one, vec_ceil, mask_conditions);\n\n        _mm256_storeu_ps(dst + i, vec_result);\n    }\n\n    for (; i < length; i++) {\n        float diff = src1[i] - src2[i];\n        if (fabs(diff) <= 50 && (i % 2 == 0)) {\n            dst[i] = ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n    __m256 vec_fifty = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        __m256 vec_diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 vec_abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), vec_diff);\n\n        // Check condition: absolute difference <= 50\n        __m256 mask_diff = _mm256_cmp_ps(vec_abs_diff, vec_fifty, _CMP_LE_OQ);\n\n        // Check condition: index is a multiple of 2\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_indices = _mm256_and_si256(vec_indices, _mm256_set1_epi32(1));\n        __m256 mask_even = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mask_indices, _mm256_setzero_si256()));\n\n        // Combine masks\n        __m256 mask = _mm256_and_ps(mask_diff, mask_even);\n\n        // Perform multiplication\n        __m256 vec_mul = _mm256_mul_ps(vec_src1, vec_src2);\n\n        // Round up toward positive infinity\n        __m256 vec_ceil = _mm256_ceil_ps(vec_mul);\n\n        // Apply mask: if conditions are met, use vec_ceil; otherwise, use -1\n        __m256 vec_result = _mm256_blendv_ps(vec_neg_one, vec_ceil, mask);\n\n        // Store result\n        _mm256_storeu_ps(dst + i, vec_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50 && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n    __m256 vec_fifty = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 vec_diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 vec_abs_diff = _mm256_and_ps(vec_diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n\n        __m256 mask_within_50 = _mm256_cmp_ps(vec_abs_diff, vec_fifty, _CMP_LE_OQ);\n\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_even_indices = _mm256_cmpeq_epi32(_mm256_and_si256(vec_indices, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n\n        __m256 mask_conditions = _mm256_and_ps(mask_within_50, _mm256_castsi256_ps(mask_even_indices));\n\n        __m256 vec_mul = _mm256_mul_ps(vec_src1, vec_src2);\n        __m256 vec_ceil_mul = _mm256_ceil_ps(vec_mul);\n\n        __m256 vec_result = _mm256_blendv_ps(vec_neg_one, vec_ceil_mul, mask_conditions);\n\n        _mm256_storeu_ps(dst + i, vec_result);\n    }\n\n    for (; i < length; i++) {\n        float diff = src1[i] - src2[i];\n        if (fabsf(diff) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n    __m256 vec_fifty = _mm256_set1_ps(50.0f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 vec_diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 vec_abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), vec_diff);\n\n        __m256 mask_within_50 = _mm256_cmp_ps(vec_abs_diff, vec_fifty, _CMP_LE_OQ);\n\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mask_even_indices = _mm256_and_si256(vec_indices, _mm256_set1_epi32(1));\n        __m256 mask_even = _mm256_castsi256_ps(_mm256_cmpeq_epi32(mask_even_indices, _mm256_setzero_si256()));\n\n        __m256 mask_conditions = _mm256_and_ps(mask_within_50, mask_even);\n\n        __m256 vec_mul = _mm256_mul_ps(vec_src1, vec_src2);\n        __m256 vec_ceil_mul = _mm256_ceil_ps(vec_mul);\n\n        __m256 vec_result = _mm256_blendv_ps(vec_neg_one, vec_ceil_mul, mask_conditions);\n\n        _mm256_storeu_ps(dst + i, vec_result);\n    }\n\n    for (; i < length; i++) {\n        float diff = src1[i] - src2[i];\n        if (fabsf(diff) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load the scalar into an AVX register\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    // Process 4 elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 double values from the source matrix\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result in the destination matrix\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load the scalar into an AVX register\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    // Process elements in chunks of 4 (256 bits = 4 doubles)\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 double values from the source matrix\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result into the destination matrix\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load the scalar into an AVX register\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    // Process elements in chunks of 4 using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 double values from the source matrix into an AVX register\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result into the destination matrix\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load the scalar into an AVX register\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    // Process 4 elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 double values from the source matrix\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result in the destination matrix\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load the scalar into an AVX vector\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    // Process 4 elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 elements from the source matrix\n        __m256d mat_vec = _mm256_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m256d result_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result into the destination matrix\n        _mm256_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle the remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute the L^2 norm of the row\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_vec = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of the vector\n        float sum_array[8];\n        _mm256_storeu_ps(sum_array, sum_vec);\n        float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                    sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            sum += val * val;\n        }\n\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n\n        // Process 8 elements at a time\n        for (; j + 7 < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute L^2 norm of the row\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_vec = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n        }\n        float sum_array[8];\n        _mm256_storeu_ps(sum_array, sum_vec);\n        float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                    sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n\n        // Compute L^2 norm using AVX\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(vec, vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n\n        // Horizontal sum of sum_vec\n        float sum_array[8];\n        _mm256_storeu_ps(sum_array, sum_vec);\n        float l2_norm_squared = 0.0f;\n        for (int k = 0; k < 8; ++k) {\n            l2_norm_squared += sum_array[k];\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            l2_norm_squared += row[j] * row[j];\n        }\n\n        float l2_norm = sqrtf(l2_norm_squared);\n\n        // Normalize row using AVX\n        __m256 norm_vec = _mm256_set1_ps(l2_norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        // Compute the L^2 norm of the row\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_vec = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of the elements in sum_vec\n        float sum_array[8];\n        _mm256_storeu_ps(sum_array, sum_vec);\n        float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                    sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n        // Handle remaining elements\n        for (; j < n; j++) {\n            sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; j++) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute the L^2 norm of the row\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_vec = _mm256_mul_ps(row_vec, row_vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of sum_vec\n        float sum_array[8];\n        _mm256_storeu_ps(sum_array, sum_vec);\n        float row_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3] +\n                        sum_array[4] + sum_array[5] + sum_array[6] + sum_array[7];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float norm = sqrtf(row_sum);\n\n        // Normalize the row\n        __m256 norm_vec = _mm256_set1_ps(norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_vec = _mm256_div_ps(row_vec, norm_vec);\n            _mm256_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd(); // Initialize sum vector to zero\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from mat1 row\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                // Load 4 elements from mat2 column\n                __m256d mat2_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n                // Perform element-wise multiplication and accumulate\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(mat1_vec, mat2_vec));\n            }\n            // Horizontal sum of the 4 elements in the sum vector\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total = temp[0] + temp[1] + temp[2] + temp[3];\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = (double)((int)(total + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec1 = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d mat2_vec2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d mat2_vec3 = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d mat2_vec4 = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d prod_vec = _mm256_mul_pd(mat1_vec, mat2_vec);\n                sum_vec = _mm256_add_pd(sum_vec, prod_vec);\n            }\n\n            // Horizontal sum of the 4 lanes in the AVX register\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to the nearest integer and store in dst\n            dst[i * p + j] = (double)((int)(sum + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from mat1 row\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n\n                // Load 4 elements from mat2 column\n                __m256d mat2_vec = _mm256_set_pd(\n                    mat2[(k + 3) * p + j],\n                    mat2[(k + 2) * p + j],\n                    mat2[(k + 1) * p + j],\n                    mat2[k * p + j]\n                );\n\n                // Multiply and accumulate\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(mat1_vec, mat2_vec));\n            }\n\n            // Horizontal sum of the 4 lanes in the AVX register\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = round(total);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd(); // Initialize sum vector to zero\n            for (size_t k = 0; k < n; k += 4) {\n                // Load 4 elements from mat1 row\n                __m256d mat1_vals = _mm256_loadu_pd(&mat1[i * n + k]);\n                // Load 4 elements from mat2 column\n                __m256d mat2_vals = _mm256_loadu_pd(&mat2[k * p + j]);\n                // Multiply and accumulate\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(mat1_vals, mat2_vals));\n            }\n            // Horizontal sum of the 4 lanes in the sum vector\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = (double)((int)(total_sum + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Perform dot product for the row of mat1 and column of mat2 using AVX\n            for (; k + 4 <= n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_set_pd(mat2[(k + 3) * p + j], mat2[(k + 2) * p + j], mat2[(k + 1) * p + j], mat2[k * p + j]);\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            // Horizontal sum of the AVX register\n            __m128d low = _mm256_castpd256_pd128(sum_vec);\n            __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n            __m128d sum_128 = _mm_add_pd(low, high);\n            double sum = _mm_cvtsd_f64(sum_128) + _mm_cvtsd_f64(_mm_unpackhi_pd(sum_128, sum_128));\n\n            // Handle the remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to the nearest integer and store in dst\n            dst[i * p + j] = round(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 inv_quarter = _mm256_set1_ps(4.0f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j++) {\n            __m256 values = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(_mm256_mul_ps(values, inv_quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            rounded = _mm256_mul_ps(rounded, quarter);\n\n            for (size_t k = 0; k < 8; k++) {\n                dst[j * rows + i + k] = rounded[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 inv_quarter = _mm256_set1_ps(4.0f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 values = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(_mm256_mul_ps(values, inv_quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            rounded = _mm256_mul_ps(rounded, quarter);\n\n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < rows) {\n                    dst[j * rows + (i + k)] = ((float*)&rounded)[k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 values = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 divided = _mm256_div_ps(values, quarter);\n            __m256 rounded = _mm256_floor_ps(_mm256_add_ps(divided, half));\n            __m256 result = _mm256_mul_ps(rounded, quarter);\n\n            for (size_t k = 0; k < 8; ++k) {\n                dst[j * rows + (i + k)] = ((float*)&result)[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 inv_quarter = _mm256_set1_ps(4.0f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j++) {\n            __m256 values = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(_mm256_mul_ps(values, inv_quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            rounded = _mm256_mul_ps(rounded, quarter);\n            _mm256_storeu_ps(&dst[j * rows + i], rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 inv_quarter = _mm256_set1_ps(4.0f);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j++) {\n            __m256 values = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 rounded = _mm256_round_ps(_mm256_mul_ps(values, inv_quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            rounded = _mm256_mul_ps(rounded, quarter);\n\n            for (size_t k = 0; k < 8; k++) {\n                dst[j * rows + i + k] = rounded[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i); // Load 8 floats from tensor1\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i); // Load 8 floats from tensor2\n        __m256 sub_result = _mm256_sub_ps(vec1, vec2); // Perform element-wise subtraction\n        __m256 rounded_result = _mm256_round_ps(sub_result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm256_storeu_ps(dst + i, rounded_result); // Store the result in dst\n    }\n\n    // Process remaining elements (if total_elements is not a multiple of simd_width)\n    for (; i < total_elements; i++) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (sub_result > 0) ? floorf(sub_result) : ceilf(sub_result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);  // Load 8 floats from tensor1\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);  // Load 8 floats from tensor2\n        __m256 result = _mm256_sub_ps(vec1, vec2);   // Perform element-wise subtraction\n        __m256 rounded = _mm256_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm256_storeu_ps(dst + i, rounded);         // Store the result in dst\n    }\n\n    // Handle the remaining elements (if any)\n    for (; i < total_elements; i++) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? floor(result) : ceil(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_chunks = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_chunks; i++) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i * simd_width);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i * simd_width);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded_result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i * simd_width, rounded_result);\n    }\n\n    for (size_t i = simd_chunks * simd_width; i < total_elements; i++) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? floor(result) : ceil(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_end = total_elements - (total_elements % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i); // Load 8 floats from tensor1\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i); // Load 8 floats from tensor2\n        __m256 sub_result = _mm256_sub_ps(vec1, vec2); // Perform element-wise subtraction\n        __m256 rounded_result = _mm256_round_ps(sub_result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm256_storeu_ps(dst + i, rounded_result); // Store the result in dst\n    }\n\n    // Handle remaining elements that don't fit into SIMD width\n    for (size_t i = simd_end; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (sub_result > 0) ? floorf(sub_result) : ceilf(sub_result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i); // Load 8 floats from tensor1\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i); // Load 8 floats from tensor2\n        __m256 result = _mm256_sub_ps(vec1, vec2);  // Perform element-wise subtraction\n        __m256 rounded_result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm256_storeu_ps(dst + i, rounded_result);  // Store the result in dst\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 8)\n    for (; i < total_elements; i++) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? (float)((int)result) : (float)((int)result + (result < 0));\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations * simd_width; i += simd_width) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]);\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(&dst[i], result);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Process 4 double elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]); // Load 4 doubles from mat1\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]); // Load 4 doubles from mat2\n        __m256d result = _mm256_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm256_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle the remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]); // Load 4 double elements from mat1\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]); // Load 4 double elements from mat2\n        __m256d result = _mm256_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm256_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle any remaining elements\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Process 4 doubles at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]); // Load 4 doubles from mat1\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]); // Load 4 doubles from mat2\n        __m256d result = _mm256_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm256_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t aligned_elements = total_elements - (total_elements % simd_width);\n\n    for (size_t i = 0; i < aligned_elements; i += simd_width) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]);\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(&dst[i], result);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_values = _mm256_loadu_ps(&mat[i * n + j]); // Load 8 elements from the matrix row\n            __m256 vec_values = _mm256_loadu_ps(&vec[j]);         // Load 8 elements from the vector\n            __m256 product = _mm256_mul_ps(mat_values, vec_values); // Multiply element-wise\n            sum = _mm256_add_ps(sum, product);                    // Accumulate the sum\n        }\n\n        // Horizontal sum of the 8 elements in the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements (if n is not a multiple of 8)\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = row_sum; // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vals = _mm256_loadu_ps(&mat[i * n + j]); // Load 8 elements from the matrix row\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);         // Load 8 elements from the vector\n            __m256 prod = _mm256_mul_ps(mat_vals, vec_vals);    // Multiply element-wise\n            sum = _mm256_add_ps(sum, prod);                    // Accumulate the results\n        }\n\n        // Horizontal sum of the 8 elements in the sum vector\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Handle remaining elements (if n is not a multiple of 8)\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = row_sum; // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_col = _mm256_loadu_ps(&vec[j]);\n            __m256 prod = _mm256_mul_ps(mat_row, vec_col);\n            sum = _mm256_add_ps(sum, prod);\n        }\n\n        // Horizontal sum of the 8 elements in the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = row_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vals = _mm256_loadu_ps(&mat[i * n + j]); // Load 8 matrix elements\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);         // Load 8 vector elements\n            __m256 prod = _mm256_mul_ps(mat_vals, vec_vals);    // Multiply element-wise\n            sum = _mm256_add_ps(sum, prod);                     // Accumulate the sum\n        }\n\n        // Horizontal sum of the 8 elements in the sum register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements (if n is not a multiple of 8)\n        for (; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_vals = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_fmadd_ps(mat_vals, vec_vals, sum);\n        }\n\n        // Horizontal sum of the 8 elements in the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = row_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d sum_vec = _mm256_setzero_pd(); // Initialize vector to hold sum\n    size_t i = 0;\n\n    // Process elements in chunks of 4 doubles using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]); // Load 4 doubles into vector\n        sum_vec = _mm256_add_pd(sum_vec, vec); // Accumulate sum\n    }\n\n    // Horizontal sum of the vector\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    // Calculate and return the average\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process elements in chunks of 4 doubles using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX vector\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the vector\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d sum_vec = _mm256_setzero_pd(); // Initialize vector to hold sum\n    size_t i = 0;\n\n    // Process elements in chunks of 4 doubles using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]); // Load 4 doubles into vector\n        sum_vec = _mm256_add_pd(sum_vec, vec); // Add to sum vector\n    }\n\n    // Horizontal sum of the vector\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n    double total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Process remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        total_sum += mat[i];\n    }\n\n    // Calculate and return the average\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX\n    for (; i + 4 <= total_elements; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX register\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements (if any)\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    // Calculate and return the average\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            if (ascending) {\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        if (ascending) {\n            for (; j + 8 <= cols; j += 8) {\n                __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n            for (; j + 1 < cols; ++j) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            }\n        } else {\n            for (; j + 8 <= cols; j += 8) {\n                __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n            for (; j + 1 < cols; ++j) {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            if (ascending) {\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            if (ascending) {\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            if (ascending) {\n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Divide src by base (integer division)\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n\n        // Multiply back to get the nearest multiple <= src\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vmul);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base); // Broadcast base to all elements\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]); // Load 16 elements from src\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);         // Divide src by base\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);       // Multiply back to get nearest multiple\n        _mm256_storeu_si256((__m256i*)&dst[i], vmul);         // Store result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm256_set1_epi16(base); // Broadcast base to all elements\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i)); // Load 16 elements from src\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);     // Divide src by base\n        __m256i mul_vec = _mm256_mullo_epi16(div_vec, base_vec);   // Multiply quotient by base\n        _mm256_storeu_si256((__m256i*)(dst + i), mul_vec);         // Store result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base); // Broadcast base to all elements\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)(src + i)); // Load 16 elements from src\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);           // Divide src by base\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);         // Multiply quotient by base\n        _mm256_storeu_si256((__m256i*)(dst + i), vmul);         // Store result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i vbase = _mm256_set1_epi16(base);\n\n    for (; i + 16 <= length; i += 16) {\n        // Load 16 elements from src\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Divide src by base (integer division)\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n\n        // Multiply back to get the nearest multiple <= src\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vmul);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n        __m256d match = _mm256_set1_pd(1.0); // Initialize match to all ones (true)\n\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_vals = _mm256_loadu_pd(row + j);\n            __m256d vec_vals = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_vals, vec_vals, _CMP_EQ_OQ); // Compare row and vector elements\n            match = _mm256_and_pd(match, cmp); // Update match with logical AND\n        }\n\n        // Check if all elements in the match vector are true\n        if (_mm256_testc_pd(match, _mm256_set1_pd(-1.0))) {\n            // Handle remaining elements\n            for (; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    goto next_row;\n                }\n            }\n            return true;\n        }\n\n    next_row:\n        continue;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n        __m256d match = _mm256_set1_pd(1.0); // Initialize match vector to all ones\n\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_chunk = _mm256_loadu_pd(row + j);\n            __m256d vector_chunk = _mm256_loadu_pd(vector + j);\n            __m256d cmp = _mm256_cmp_pd(row_chunk, vector_chunk, _CMP_EQ_OQ);\n            match = _mm256_and_pd(match, cmp);\n        }\n\n        // Check if all elements in the match vector are true\n        if (_mm256_testc_pd(match, _mm256_set1_pd(-1.0))) {\n            // Check remaining elements\n            for (; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    goto next_row;\n                }\n            }\n            return true;\n        }\n\n    next_row:\n        continue;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Use AVX to compare columns in chunks of 4 doubles\n        for (; j + 4 <= cols; j += 4) {\n            __m256d row_chunk = _mm256_loadu_pd(row + j);\n            __m256d vector_chunk = _mm256_loadu_pd(vector + j);\n            __m256d cmp_result = _mm256_cmp_pd(row_chunk, vector_chunk, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_result);\n            if (mask != 0xF) {\n                goto next_row;\n            }\n        }\n\n        // Compare remaining elements (if cols is not a multiple of 4)\n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                goto next_row;\n            }\n        }\n\n        // If all elements match, return true\n        return true;\n\n    next_row:\n        continue;\n    }\n\n    // No matching row found\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t aligned_cols = cols - (cols % simd_width);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool row_matches = true;\n\n        for (size_t j = 0; j < aligned_cols; j += simd_width) {\n            __m256d row_chunk = _mm256_loadu_pd(row + j);\n            __m256d vector_chunk = _mm256_loadu_pd(vector + j);\n            __m256d cmp_result = _mm256_cmp_pd(row_chunk, vector_chunk, _CMP_EQ_OQ);\n\n            if (_mm256_movemask_pd(cmp_result) != 0xF) {\n                row_matches = false;\n                break;\n            }\n        }\n\n        if (row_matches) {\n            for (size_t j = aligned_cols; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    row_matches = false;\n                    break;\n                }\n            }\n        }\n\n        if (row_matches) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t simd_width = 4; // AVX processes 4 doubles (64-bit) at a time\n    size_t aligned_cols = cols - (cols % simd_width);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool row_matches = true;\n\n        for (size_t j = 0; j < aligned_cols; j += simd_width) {\n            __m256d row_chunk = _mm256_loadu_pd(row + j);\n            __m256d vector_chunk = _mm256_loadu_pd(vector + j);\n            __m256d cmp_result = _mm256_cmp_pd(row_chunk, vector_chunk, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_result);\n\n            if (mask != 0xF) { // Not all elements are equal\n                row_matches = false;\n                break;\n            }\n        }\n\n        if (row_matches && aligned_cols < cols) {\n            for (size_t j = aligned_cols; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    row_matches = false;\n                    break;\n                }\n            }\n        }\n\n        if (row_matches) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m256i v_equal_count = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i v_cmp = _mm256_cmpeq_epi8(vA, vB);\n        v_equal_count = _mm256_add_epi8(v_equal_count, v_cmp);\n    }\n\n    // Sum up the counts in the vector register\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, v_equal_count);\n    for (int j = 0; j < 32; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Handle the remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m256i count_vec = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n        count_vec = _mm256_add_epi8(count_vec, cmp);\n    }\n\n    // Sum up the counts in count_vec\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, count_vec);\n    for (int j = 0; j < 32; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m256i count_vec = _mm256_setzero_si256();\n\n    for (; i + 32 <= total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp_result = _mm256_cmpeq_epi8(vecA, vecB);\n        count_vec = _mm256_add_epi8(count_vec, cmp_result);\n    }\n\n    // Sum up the counts in the count_vec\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, count_vec);\n    for (int j = 0; j < 32; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m256i v_equal_count = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i v_cmp = _mm256_cmpeq_epi8(vA, vB);\n        v_equal_count = _mm256_add_epi8(v_equal_count, v_cmp);\n    }\n\n    // Sum up the counts in the vector register\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, v_equal_count);\n    for (int j = 0; j < 32; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m256i count_vec = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n        count_vec = _mm256_add_epi8(count_vec, cmp);\n    }\n\n    // Sum up the counts in count_vec\n    uint8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, count_vec);\n    for (int j = 0; j < 32; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n\n    // Load scalar 'a' into an AVX2 register\n    __m256i scalar_a = _mm256_set1_epi64x(a);\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x into an AVX2 register\n        __m256i vec_x = _mm256_loadu_si256((__m256i const *)(x + i));\n\n        // Perform scalar-vector multiplication\n        __m256i mul_result = _mm256_mullo_epi64(scalar_a, vec_x);\n\n        // Load 4 elements from y into an AVX2 register\n        __m256i vec_y = _mm256_loadu_si256((__m256i const *)(y + i));\n\n        // Compute absolute value of y\n        __m256i abs_y = _mm256_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m256i result = _mm256_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm256_storeu_si256((__m256i *)(y + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i scalar = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of the vector\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x and y\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vec_y = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        // Perform scalar-vector multiplication\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n\n        // Compute absolute value of y\n        __m256i abs_y = _mm256_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m256i result = _mm256_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm256_storeu_si256((__m256i*)&y[i], result);\n    }\n\n    // Handle remaining elements (tail case)\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = y[i] < 0 ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i scalar = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of a 256-bit vector\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)&x[i]); // Load 4 elements from x\n        __m256i vec_y = _mm256_loadu_si256((__m256i*)&y[i]); // Load 4 elements from y\n\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x); // Multiply scalar 'a' with vector x\n        __m256i abs_y = _mm256_abs_epi64(vec_y); // Compute absolute value of y\n        __m256i result = _mm256_sub_epi64(mul_result, abs_y); // Subtract abs(y) from the multiplication result\n\n        _mm256_storeu_si256((__m256i*)&y[i], result); // Store the result back to y\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        y[i] = (a * x[i]) - (y[i] < 0 ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n\n    // Load scalar 'a' into an AVX2 register\n    __m256i scalar_a = _mm256_set1_epi64x(a);\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x\n        __m256i vec_x = _mm256_loadu_si256((__m256i const *)(x + i));\n\n        // Perform scalar-vector multiplication\n        __m256i mul_result = _mm256_mullo_epi64(vec_x, scalar_a);\n\n        // Load 4 elements from y\n        __m256i vec_y = _mm256_loadu_si256((__m256i const *)(y + i));\n\n        // Compute absolute value of y\n        __m256i abs_y = _mm256_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m256i result = _mm256_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm256_storeu_si256((__m256i *)(y + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = y[i] < 0 ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i scalar = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of the vector\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x and y\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_y = _mm256_loadu_si256((__m256i *)&y[i]);\n\n        // Perform scalar-vector multiplication\n        __m256i mul_result = _mm256_mullo_epi64(scalar, vec_x);\n\n        // Compute absolute value of y\n        __m256i abs_y = _mm256_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m256i result = _mm256_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm256_storeu_si256((__m256i *)&y[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = y[i] < 0 ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    if (length == 0) {\n        result.min_num = INT16_MAX;\n        result.max_num = INT16_MIN;\n        return result;\n    }\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_vals[16], max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) min_num = min_vals[j];\n        if (max_vals[j] > max_num) max_num = max_vals[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    if (length == 0) {\n        result.min_num = 0;\n        result.max_num = 0;\n        return result;\n    }\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_values[16];\n    int16_t max_values[16];\n    _mm256_storeu_si256((__m256i*)min_values, min_vec);\n    _mm256_storeu_si256((__m256i*)max_values, max_vec);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (min_values[j] < min_num) min_num = min_values[j];\n        if (max_values[j] > max_num) max_num = max_values[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    if (length == 0) {\n        result.min_num = 0;\n        result.max_num = 0;\n        return result;\n    }\n\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_array[16], max_array[16];\n    _mm256_storeu_si256((__m256i*)min_array, min_vec);\n    _mm256_storeu_si256((__m256i*)max_array, max_vec);\n\n    int16_t min_val = INT16_MAX;\n    int16_t max_val = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (min_array[j] < min_val) min_val = min_array[j];\n        if (max_array[j] > max_val) max_val = max_array[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n\n    // Initialize min and max vectors with the first 16 elements or fewer\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(vec + i));\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    // Reduce min_vec and max_vec to scalar values\n    int16_t min_arr[16], max_arr[16];\n    _mm256_storeu_si256((__m256i*)min_arr, min_vec);\n    _mm256_storeu_si256((__m256i*)max_arr, max_vec);\n\n    int16_t min_val = INT16_MAX;\n    int16_t max_val = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (min_arr[j] < min_val) min_val = min_arr[j];\n        if (max_arr[j] > max_val) max_val = max_arr[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_vec = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_vec = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_vec = _mm256_min_epi16(min_vec, data);\n        max_vec = _mm256_max_epi16(max_vec, data);\n    }\n\n    int16_t min_vals[16], max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n    _mm256_storeu_si256((__m256i*)max_vals, max_vec);\n\n    int16_t min_result = INT16_MAX;\n    int16_t max_result = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_result) min_result = min_vals[j];\n        if (max_vals[j] > max_result) max_result = max_vals[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result = {min_result, max_result};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            // Load 8 consecutive elements from the row\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n\n            // Compare current elements with the next elements\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n\n            // Check if any element in cmp is non-zero (indicating a violation)\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n\n        // Handle the remaining elements in the row\n        for (; j + 1 < cols; j++) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n            int mask = _mm256_movemask_epi8(cmp);\n\n            if (mask != -1) { // Not all comparisons are true\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 8 < cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&row[j + 1]);\n\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t block_area = block_size * block_size;\n\n    // Load the first block into a reference buffer\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < rows / block_size; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            if (block_row == 0 && block_col == 0) {\n                continue; // Skip the first block (used as reference)\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* ref_row = first_block + i * cols;\n                const int* curr_row = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 8 <= block_size; j += 8) {\n                    __m256i ref_vec = _mm256_loadu_si256((__m256i*)(ref_row + j));\n                    __m256i curr_vec = _mm256_loadu_si256((__m256i*)(curr_row + j));\n                    __m256i cmp = _mm256_cmpeq_epi32(ref_vec, curr_vec);\n\n                    if (_mm256_movemask_epi8(cmp) != -1) {\n                        return false;\n                    }\n                }\n\n                for (; j < block_size; ++j) {\n                    if (ref_row[j] != curr_row[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t block_area = block_size * block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < rows / block_size; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            if (block_row == 0 && block_col == 0) {\n                continue; // Skip the first block\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* first_row = first_block + i * cols;\n                const int* current_row = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 8 <= block_size; j += 8) {\n                    __m256i first_vals = _mm256_loadu_si256((__m256i*)(first_row + j));\n                    __m256i current_vals = _mm256_loadu_si256((__m256i*)(current_row + j));\n                    __m256i cmp = _mm256_cmpeq_epi32(first_vals, current_vals);\n                    if (_mm256_movemask_epi8(cmp) != -1) {\n                        return false;\n                    }\n                }\n\n                for (; j < block_size; ++j) {\n                    if (first_row[j] != current_row[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t block_area = block_size * block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < rows / block_size; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            if (block_row == 0 && block_col == 0) {\n                continue; // Skip the first block as it is the reference\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* first_row = first_block + i * cols;\n                const int* current_row = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 8 <= block_size; j += 8) {\n                    __m256i first_vals = _mm256_loadu_si256((__m256i*)(first_row + j));\n                    __m256i current_vals = _mm256_loadu_si256((__m256i*)(current_row + j));\n                    __m256i cmp = _mm256_cmpeq_epi32(first_vals, current_vals);\n                    if (_mm256_movemask_epi8(cmp) != -1) {\n                        return false;\n                    }\n                }\n\n                for (; j < block_size; ++j) {\n                    if (first_row[j] != current_row[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into an AVX vector\n    const int* first_block = matrix;\n    __m256i first_block_vec[block_elements / 8];\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block_vec[i] = _mm256_loadu_si256((__m256i*)(first_block + i * 8));\n    }\n\n    // Iterate over all blocks\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            if (block_row == 0 && block_col == 0) {\n                continue; // Skip the first block\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n            for (size_t i = 0; i < block_elements / 8; ++i) {\n                __m256i current_vec = _mm256_loadu_si256((__m256i*)(current_block + i * 8));\n                __m256i cmp_result = _mm256_cmpeq_epi32(first_block_vec[i], current_vec);\n                if (_mm256_movemask_epi8(cmp_result) != -1) {\n                    return false; // Blocks are not equal\n                }\n            }\n        }\n    }\n\n    return true; // All blocks are equal\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into an AVX vector\n    const int* first_block = matrix;\n    __m256i first_block_vec[block_elements / 8];\n    for (size_t i = 0; i < block_elements / 8; i++) {\n        first_block_vec[i] = _mm256_loadu_si256((__m256i*)(first_block + i * 8));\n    }\n\n    // Iterate over all blocks\n    for (size_t block_row = 0; block_row < blocks_per_col; block_row++) {\n        for (size_t block_col = 0; block_col < blocks_per_row; block_col++) {\n            if (block_row == 0 && block_col == 0) {\n                continue; // Skip the first block\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n            for (size_t i = 0; i < block_elements / 8; i++) {\n                __m256i current_vec = _mm256_loadu_si256((__m256i*)(current_block + i * 8));\n                __m256i cmp_result = _mm256_cmpeq_epi32(first_block_vec[i], current_vec);\n                if (_mm256_movemask_epi8(cmp_result) != -1) {\n                    return false; // Blocks are not equal\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d block1 = _mm256_loadu_pd(&vec[j]);\n            __m256d block2 = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(block1, block2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    const double* first_block = vec;\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        const double* current_block = vec + block_idx * block_size;\n\n        for (size_t i = 0; i < block_size; i += 4) {\n            __m256d first_block_vec = _mm256_loadu_pd(first_block + i);\n            __m256d current_block_vec = _mm256_loadu_pd(current_block + i);\n\n            __m256d diff = _mm256_sub_pd(first_block_vec, current_block_vec);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false; // Invalid input\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false; // No complete blocks to compare\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block = _mm256_loadu_pd(&vec[j]);\n            __m256d current_block = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(first_block, current_block);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                return false; // Blocks are not equal within tolerance\n            }\n        }\n    }\n\n    return true; // All blocks are equal within tolerance\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false; // Invalid input or no complete blocks\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false; // No complete blocks to compare\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d block1 = _mm256_loadu_pd(&vec[j]);\n            __m256d block2 = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(block1, block2);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                return false; // Blocks are not equal within tolerance\n            }\n        }\n    }\n\n    return true; // All blocks are equal within tolerance\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false; // Invalid input\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false; // No complete blocks to compare\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block = _mm256_loadu_pd(&vec[j]);\n            __m256d current_block = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(first_block, current_block);\n            __m256d abs_diff = _mm256_max_pd(diff, _mm256_sub_pd(_mm256_setzero_pd(), diff));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0xF) {\n                return false; // Blocks are not equal within tolerance\n            }\n        }\n    }\n\n    return true; // All blocks are equal within tolerance\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd(); // Initialize sum vector to zero\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]); // Load 4 double values\n            sum_vec = _mm256_fmadd_pd(vec, vec, sum_vec); // Accumulate squared values\n        }\n\n        // Horizontal sum of the vector sum_vec\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        __m128d low = _mm256_castpd256_pd128(temp);\n        __m128d high = _mm256_extractf128_pd(temp, 1);\n        __m128d final_sum = _mm_add_pd(low, high);\n        double row_sum = _mm_cvtsd_f64(final_sum);\n\n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            row_sum += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        // Compute square root of the sum (Euclidean norm)\n        double norm = sqrt(row_sum);\n\n        // Compare with the threshold\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd(); // Initialize sum vector to zero\n\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]); // Load 4 double values\n            sum_vec = _mm256_fmadd_pd(vec, vec, sum_vec); // Accumulate squared values\n        }\n\n        // Horizontal sum of the 256-bit vector\n        __m128d low = _mm256_castpd256_pd128(sum_vec);\n        __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n        __m128d sum128 = _mm_add_pd(low, high);\n        double row_sum = _mm_cvtsd_f64(sum128) + _mm_cvtsd_f64(_mm_unpackhi_pd(sum128, sum128));\n\n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n\n        // Compute the square root of the row sum (Euclidean norm)\n        double norm = sqrt(row_sum);\n\n        // Compare the norm with the threshold\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec);\n        }\n\n        // Horizontal sum of the accumulated vector\n        __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n        __m128d low = _mm256_castpd256_pd128(temp);\n        __m128d high = _mm256_extractf128_pd(temp, 1);\n        double sum = _mm_cvtsd_f64(_mm_add_pd(low, high));\n\n        // Process remaining elements\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n\n        // Compute the Euclidean norm\n        double norm = sqrt(sum);\n\n        // Compare with the threshold\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec); // sum_vec += row_vec * row_vec\n        }\n\n        // Horizontal sum of the accumulated vector\n        double sum_array[4];\n        _mm256_storeu_pd(sum_array, sum_vec);\n        double norm_squared = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n        // Process remaining columns\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            norm_squared += val * val;\n        }\n\n        // Compare the Euclidean norm with the threshold\n        double norm = sqrt(norm_squared);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + 3 < cols; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec); // sum_vec += row_vec * row_vec\n        }\n\n        // Horizontal sum of the accumulated vector\n        double sum_array[4];\n        _mm256_storeu_pd(sum_array, sum_vec);\n        double row_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n        // Process remaining columns\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n\n        // Compute the Euclidean norm and compare with the threshold\n        double norm = sqrt(row_sum);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i)); // Load 8 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i)); // Load 8 elements from B\n        __m256i vecC = _mm256_or_si256(vecA, vecB);                 // Perform bitwise OR\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);              // Store the result in C\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 32-bit integers at a time\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 32-bit integers in parallel\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Process remaining elements that do not fit into a full SIMD register\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 32-bit integers at a time\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle remaining elements that don't fit in SIMD width\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 32-bit integers at a time\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i vecA16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 0));\n        __m256i vecB16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 0));\n        __m256i vecA16_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 1));\n        __m256i vecB16_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 1));\n\n        __m256i squares_lo = _mm256_mullo_epi16(vecA16, vecA16);\n        __m256i squares_hi = _mm256_mullo_epi16(vecA16_hi, vecA16_hi);\n        __m256i products_lo = _mm256_mullo_epi16(vecA16, vecB16);\n        __m256i products_hi = _mm256_mullo_epi16(vecA16_hi, vecB16_hi);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_lo, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_lo, 1))\n        ));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_hi, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_hi, 1))\n        ));\n\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_lo, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_lo, 1))\n        ));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_hi, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_hi, 1))\n        ));\n    }\n\n    int32_t squares[8], products[8];\n    _mm256_storeu_si256((__m256i *)squares, sum_of_squares);\n    _mm256_storeu_si256((__m256i *)products, sum_of_products);\n\n    int64_t total_squares = 0, total_products = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_squares += squares[j];\n        total_products += products[j];\n    }\n\n    for (; i < length; ++i) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += a * a;\n        total_products += a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i vecA16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 0));\n        __m256i vecB16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 0));\n        __m256i vecA16_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 1));\n        __m256i vecB16_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 1));\n\n        __m256i squares_lo = _mm256_mullo_epi16(vecA16, vecA16);\n        __m256i squares_hi = _mm256_mullo_epi16(vecA16_hi, vecA16_hi);\n        __m256i products_lo = _mm256_mullo_epi16(vecA16, vecB16);\n        __m256i products_hi = _mm256_mullo_epi16(vecA16_hi, vecB16_hi);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(_mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_lo, 0)), _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_lo, 1))));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(_mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_hi, 0)), _mm256_cvtepi16_epi32(_mm256_extracti128_si256(squares_hi, 1))));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_add_epi32(_mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_lo, 0)), _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_lo, 1))));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_add_epi32(_mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_hi, 0)), _mm256_cvtepi16_epi32(_mm256_extracti128_si256(products_hi, 1))));\n    }\n\n    int32_t squares[8], products[8];\n    _mm256_storeu_si256((__m256i*)squares, sum_squares);\n    _mm256_storeu_si256((__m256i*)products, sum_products);\n\n    int64_t total_squares = 0, total_products = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_squares += squares[j];\n        total_products += products[j];\n    }\n\n    for (; i < length; ++i) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += a * a;\n        total_products += a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i vecA16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 0));\n        __m256i vecB16 = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 0));\n        __m256i vecA16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 1));\n        __m256i vecB16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 1));\n\n        __m256i square_low = _mm256_mullo_epi16(vecA16, vecA16);\n        __m256i square_high = _mm256_mullo_epi16(vecA16_high, vecA16_high);\n        __m256i product_low = _mm256_mullo_epi16(vecA16, vecB16);\n        __m256i product_high = _mm256_mullo_epi16(vecA16_high, vecB16_high);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(square_low, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(square_low, 1))\n        ));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(square_high, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(square_high, 1))\n        ));\n\n        sum_products = _mm256_add_epi32(sum_products, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(product_low, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(product_low, 1))\n        ));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_add_epi32(\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(product_high, 0)),\n            _mm256_cvtepi16_epi32(_mm256_extracti128_si256(product_high, 1))\n        ));\n    }\n\n    int32_t squares[8], products[8];\n    _mm256_storeu_si256((__m256i *)squares, sum_squares);\n    _mm256_storeu_si256((__m256i *)products, sum_products);\n\n    int64_t total_squares = 0, total_products = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_squares += squares[j];\n        total_products += products[j];\n    }\n\n    for (; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        total_squares += a * a;\n        total_products += a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i vecA16 = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vecA));\n        __m256i vecB16 = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vecB));\n        __m256i vecA16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecA, 1));\n        __m256i vecB16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(vecB, 1));\n\n        __m256i square_low = _mm256_mullo_epi16(vecA16, vecA16);\n        __m256i square_high = _mm256_mullo_epi16(vecA16_high, vecA16_high);\n        __m256i product_low = _mm256_mullo_epi16(vecA16, vecB16);\n        __m256i product_high = _mm256_mullo_epi16(vecA16_high, vecB16_high);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(\n            _mm256_unpacklo_epi16(square_low, _mm256_setzero_si256()),\n            _mm256_unpackhi_epi16(square_low, _mm256_setzero_si256())));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(\n            _mm256_unpacklo_epi16(square_high, _mm256_setzero_si256()),\n            _mm256_unpackhi_epi16(square_high, _mm256_setzero_si256())));\n\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(\n            _mm256_unpacklo_epi16(product_low, _mm256_setzero_si256()),\n            _mm256_unpackhi_epi16(product_low, _mm256_setzero_si256())));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(\n            _mm256_unpacklo_epi16(product_high, _mm256_setzero_si256()),\n            _mm256_unpackhi_epi16(product_high, _mm256_setzero_si256())));\n    }\n\n    int32_t squares[8], products[8];\n    _mm256_storeu_si256((__m256i *)squares, sum_of_squares);\n    _mm256_storeu_si256((__m256i *)products, sum_of_products);\n\n    int64_t total_squares = 0, total_products = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_squares += squares[j];\n        total_products += products[j];\n    }\n\n    for (; i < length; ++i) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += a * a;\n        total_products += a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i vecA_lo = _mm256_unpacklo_epi8(vecA, _mm256_setzero_si256());\n        __m256i vecA_hi = _mm256_unpackhi_epi8(vecA, _mm256_setzero_si256());\n        __m256i vecB_lo = _mm256_unpacklo_epi8(vecB, _mm256_setzero_si256());\n        __m256i vecB_hi = _mm256_unpackhi_epi8(vecB, _mm256_setzero_si256());\n\n        __m256i squareA_lo = _mm256_mullo_epi16(vecA_lo, vecA_lo);\n        __m256i squareA_hi = _mm256_mullo_epi16(vecA_hi, vecA_hi);\n        __m256i squareB_lo = _mm256_mullo_epi16(vecB_lo, vecB_lo);\n        __m256i squareB_hi = _mm256_mullo_epi16(vecB_hi, vecB_hi);\n\n        __m256i product_lo = _mm256_mullo_epi16(vecA_lo, vecB_lo);\n        __m256i product_hi = _mm256_mullo_epi16(vecA_hi, vecB_hi);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(_mm256_unpacklo_epi16(squareA_lo, _mm256_setzero_si256()), _mm256_unpackhi_epi16(squareA_lo, _mm256_setzero_si256())));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(_mm256_unpacklo_epi16(squareA_hi, _mm256_setzero_si256()), _mm256_unpackhi_epi16(squareA_hi, _mm256_setzero_si256())));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(_mm256_unpacklo_epi16(squareB_lo, _mm256_setzero_si256()), _mm256_unpackhi_epi16(squareB_lo, _mm256_setzero_si256())));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_add_epi32(_mm256_unpacklo_epi16(squareB_hi, _mm256_setzero_si256()), _mm256_unpackhi_epi16(squareB_hi, _mm256_setzero_si256())));\n\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(_mm256_unpacklo_epi16(product_lo, _mm256_setzero_si256()), _mm256_unpackhi_epi16(product_lo, _mm256_setzero_si256())));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_add_epi32(_mm256_unpacklo_epi16(product_hi, _mm256_setzero_si256()), _mm256_unpackhi_epi16(product_hi, _mm256_setzero_si256())));\n    }\n\n    int64_t result_squares = 0, result_products = 0;\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i *)temp, sum_of_squares);\n    for (int j = 0; j < 8; ++j) result_squares += temp[j];\n    _mm256_storeu_si256((__m256i *)temp, sum_of_products);\n    for (int j = 0; j < 8; ++j) result_products += temp[j];\n\n    for (; i < length; ++i) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        result_squares += a * a + b * b;\n        result_products += a * b;\n    }\n\n    return result_squares - result_products;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers per vector\n\n    // Process in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecB_not = _mm256_andnot_si256(vecB, _mm256_set1_epi64x(-1)); // Bitwise NOT of B\n        __m256i vecC = _mm256_and_si256(vecA, vecB_not); // A AND (NOT B)\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers per vector\n\n    // Process elements in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i)); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i)); // Load 4 elements from B\n        __m256i vecB_not = _mm256_xor_si256(vecB, _mm256_set1_epi64x(-1)); // Bitwise NOT of B\n        __m256i vecC = _mm256_and_si256(vecA, vecB_not); // A & ~B\n        _mm256_storeu_si256((__m256i*)(C + i), vecC); // Store result in C\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // Each __m256i can hold 4 64-bit integers\n\n    // Process in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i)); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i)); // Load 4 elements from B\n        __m256i vecB_not = _mm256_andnot_si256(vecB, _mm256_set1_epi64x(-1)); // Bitwise NOT of B\n        __m256i vecC = _mm256_and_si256(vecA, vecB_not); // A & ~B\n        _mm256_storeu_si256((__m256i*)(C + i), vecC); // Store the result in C\n    }\n\n    // Process the remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers per vector\n\n    // Process in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i)); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i)); // Load 4 elements from B\n        __m256i vecB_not = _mm256_andnot_si256(vecB, _mm256_set1_epi64x(-1)); // Compute bitwise NOT of B\n        __m256i vecC = _mm256_and_si256(vecA, vecB_not); // Perform bitwise AND between A and NOT(B)\n        _mm256_storeu_si256((__m256i*)(C + i), vecC); // Store the result in C\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 64-bit integers per vector\n\n    // Process in chunks of 4 using AVX2\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]); // Load 4 elements from B\n        __m256i vecB_not = _mm256_andnot_si256(vecB, _mm256_set1_epi64x(-1)); // Bitwise NOT of B\n        __m256i vecC = _mm256_and_si256(vecA, vecB_not); // A & ~B\n        _mm256_storeu_si256((__m256i*)&C[i], vecC); // Store result in C\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 32; // AVX2 processes 32 8-bit integers at a time\n\n    for (; i + simd_width <= length; i += simd_width) {\n        // Load 32 elements from A and B\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        // Compare A > 0 and B > 0\n        __m256i mask_pos_A = _mm256_cmpgt_epi8(vecA, _mm256_setzero_si256());\n        __m256i mask_pos_B = _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256());\n        __m256i mask_pos = _mm256_and_si256(mask_pos_A, mask_pos_B);\n\n        // Compare A < 0 and B < 0\n        __m256i mask_neg_A = _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA);\n        __m256i mask_neg_B = _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB);\n        __m256i mask_neg = _mm256_and_si256(mask_neg_A, mask_neg_B);\n\n        // Set C[i] = 1 for positive mask\n        __m256i result_pos = _mm256_and_si256(mask_pos, _mm256_set1_epi8(1));\n\n        // Set C[i] = -1 for negative mask\n        __m256i result_neg = _mm256_and_si256(mask_neg, _mm256_set1_epi8(-1));\n\n        // Combine results\n        __m256i result = _mm256_or_si256(result_pos, result_neg);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 32; // AVX2 processes 32 8-bit integers at a time\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i pos_maskA = _mm256_cmpgt_epi8(vecA, zero);\n        __m256i pos_maskB = _mm256_cmpgt_epi8(vecB, zero);\n        __m256i neg_maskA = _mm256_cmpgt_epi8(zero, vecA);\n        __m256i neg_maskB = _mm256_cmpgt_epi8(zero, vecB);\n\n        __m256i both_positive = _mm256_and_si256(pos_maskA, pos_maskB);\n        __m256i both_negative = _mm256_and_si256(neg_maskA, neg_maskB);\n\n        __m256i result = _mm256_blendv_epi8(zero, one, both_positive);\n        result = _mm256_blendv_epi8(result, neg_one, both_negative);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 32; // AVX2 processes 32 8-bit integers in parallel\n\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i zero = _mm256_setzero_si256();\n\n        // Compare A > 0 and B > 0\n        __m256i mask_posA = _mm256_cmpgt_epi8(vecA, zero);\n        __m256i mask_posB = _mm256_cmpgt_epi8(vecB, zero);\n        __m256i mask_pos = _mm256_and_si256(mask_posA, mask_posB);\n\n        // Compare A < 0 and B < 0\n        __m256i mask_negA = _mm256_cmpgt_epi8(zero, vecA);\n        __m256i mask_negB = _mm256_cmpgt_epi8(zero, vecB);\n        __m256i mask_neg = _mm256_and_si256(mask_negA, mask_negB);\n\n        // Set C[i] = 1 where both A[i] > 0 and B[i] > 0\n        __m256i result_pos = _mm256_and_si256(mask_pos, _mm256_set1_epi8(1));\n\n        // Set C[i] = -1 where both A[i] < 0 and B[i] < 0\n        __m256i result_neg = _mm256_and_si256(mask_neg, _mm256_set1_epi8(-1));\n\n        // Combine results\n        __m256i result = _mm256_or_si256(result_pos, result_neg);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i zero = _mm256_setzero_si256();\n\n        // Compare A > 0 and B > 0\n        __m256i maskA_pos = _mm256_cmpgt_epi8(vecA, zero);\n        __m256i maskB_pos = _mm256_cmpgt_epi8(vecB, zero);\n        __m256i maskBothPos = _mm256_and_si256(maskA_pos, maskB_pos);\n\n        // Compare A < 0 and B < 0\n        __m256i maskA_neg = _mm256_cmpgt_epi8(zero, vecA);\n        __m256i maskB_neg = _mm256_cmpgt_epi8(zero, vecB);\n        __m256i maskBothNeg = _mm256_and_si256(maskA_neg, maskB_neg);\n\n        // Set result for positive case (1)\n        __m256i resultPos = _mm256_and_si256(maskBothPos, _mm256_set1_epi8(1));\n\n        // Set result for negative case (-1)\n        __m256i resultNeg = _mm256_and_si256(maskBothNeg, _mm256_set1_epi8(-1));\n\n        // Combine results\n        __m256i result = _mm256_or_si256(resultPos, resultNeg);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 32; // AVX2 processes 32 8-bit integers at a time\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i is_positive_A = _mm256_cmpgt_epi8(vecA, zero);\n        __m256i is_positive_B = _mm256_cmpgt_epi8(vecB, zero);\n        __m256i is_negative_A = _mm256_cmpgt_epi8(zero, vecA);\n        __m256i is_negative_B = _mm256_cmpgt_epi8(zero, vecB);\n\n        __m256i both_positive = _mm256_and_si256(is_positive_A, is_positive_B);\n        __m256i both_negative = _mm256_and_si256(is_negative_A, is_negative_B);\n\n        __m256i result = _mm256_blendv_epi8(zero, one, both_positive);\n        result = _mm256_blendv_epi8(result, neg_one, both_negative);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint with AND\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint with AND\n    for (; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using XOR\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    C[midpoint] = A[midpoint];\n    i = midpoint + 1;\n\n    // Process elements after the midpoint using AND\n    for (; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using XOR\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint using AND\n    for (; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecXOR = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecXOR);\n    }\n\n    // Process the midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecAND = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecAND);\n    }\n\n    // Handle remaining elements before the midpoint\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 8 floats from A, B, C, D\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean values from cond1 and cond2\n        __m256i vecCond1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i vecCond2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean values to masks (0xFFFFFFFF for true, 0x0 for false)\n        __m256 maskCond1 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond1, _mm256_set1_epi32(1)));\n        __m256 maskCond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond2, _mm256_set1_epi32(1)));\n\n        // Compute masks for each condition\n        __m256 maskA = _mm256_and_ps(maskCond1, maskCond2); // cond1 && cond2\n        __m256 maskB = _mm256_and_ps(maskCond1, _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // cond1 && !cond2\n        __m256 maskC = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), maskCond2); // !cond1 && cond2\n        __m256 maskD = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m256 result = _mm256_or_ps(\n            _mm256_or_ps(_mm256_and_ps(maskA, vecA), _mm256_and_ps(maskB, vecB)),\n            _mm256_or_ps(_mm256_and_ps(maskC, vecC), _mm256_and_ps(maskD, vecD))\n        );\n\n        // Store the result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 8 elements from A, B, C, D\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i vecCond1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i vecCond2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m256 maskCond1 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond1, _mm256_set1_epi32(1)));\n        __m256 maskCond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond2, _mm256_set1_epi32(1)));\n\n        // Compute intermediate masks\n        __m256 maskA = _mm256_and_ps(maskCond1, maskCond2); // cond1 && cond2\n        __m256 maskB = _mm256_and_ps(maskCond1, _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // cond1 && !cond2\n        __m256 maskC = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), maskCond2); // !cond1 && cond2\n        __m256 maskD = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m256 resultA = _mm256_and_ps(maskA, vecA);\n        __m256 resultB = _mm256_and_ps(maskB, vecB);\n        __m256 resultC = _mm256_and_ps(maskC, vecC);\n        __m256 resultD = _mm256_and_ps(maskD, vecD);\n\n        // Combine results\n        __m256 result = _mm256_or_ps(_mm256_or_ps(resultA, resultB), _mm256_or_ps(resultC, resultD));\n\n        // Store the result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_elements = total_elements - (total_elements % simd_width);\n\n    for (size_t i = 0; i < aligned_elements; i += simd_width) {\n        // Load 8 floats from A, B, C, D\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean values from cond1 and cond2\n        __m256i vecCond1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i vecCond2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean values to masks (0xFFFFFFFF for true, 0x0 for false)\n        __m256 maskCond1 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond1, _mm256_set1_epi32(1)));\n        __m256 maskCond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond2, _mm256_set1_epi32(1)));\n\n        // Compute masks for each condition\n        __m256 maskA = _mm256_and_ps(maskCond1, maskCond2); // cond1 && cond2\n        __m256 maskB = _mm256_and_ps(maskCond1, _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // cond1 && !cond2\n        __m256 maskC = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), maskCond2); // !cond1 && cond2\n        __m256 maskD = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m256 resultA = _mm256_and_ps(maskA, vecA);\n        __m256 resultB = _mm256_and_ps(maskB, vecB);\n        __m256 resultC = _mm256_and_ps(maskC, vecC);\n        __m256 resultD = _mm256_and_ps(maskD, vecD);\n\n        // Combine results\n        __m256 result = _mm256_or_ps(_mm256_or_ps(resultA, resultB), _mm256_or_ps(resultC, resultD));\n\n        // Store the result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_elements; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 8 floats from A, B, C, D\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean values from cond1 and cond2\n        __m256i vecCond1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i vecCond2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean values to masks (0xFFFFFFFF for true, 0x0 for false)\n        __m256 maskCond1 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond1, _mm256_set1_epi32(1)));\n        __m256 maskCond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(vecCond2, _mm256_set1_epi32(1)));\n\n        // Compute intermediate masks\n        __m256 maskA = _mm256_and_ps(maskCond1, maskCond2); // cond1 && cond2\n        __m256 maskB = _mm256_and_ps(maskCond1, _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // cond1 && !cond2\n        __m256 maskC = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), maskCond2); // !cond1 && cond2\n        __m256 maskD = _mm256_and_ps(_mm256_xor_ps(maskCond1, _mm256_set1_ps(-1)), _mm256_xor_ps(maskCond2, _mm256_set1_ps(-1))); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m256 resultA = _mm256_and_ps(maskA, vecA);\n        __m256 resultB = _mm256_and_ps(maskB, vecB);\n        __m256 resultC = _mm256_and_ps(maskC, vecC);\n        __m256 resultD = _mm256_and_ps(maskD, vecD);\n\n        // Combine results\n        __m256 result = _mm256_or_ps(_mm256_or_ps(resultA, resultB), _mm256_or_ps(resultC, resultD));\n\n        // Store the result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 8 elements from A, B, C, D\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks (0xFFFFFFFF for true, 0x00000000 for false)\n        __m256 cond1_mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond1_vec, _mm256_set1_epi32(1)));\n        __m256 cond2_mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond2_vec, _mm256_set1_epi32(1)));\n\n        // Compute intermediate masks\n        __m256 cond1_and_cond2 = _mm256_and_ps(cond1_mask, cond2_mask);\n        __m256 cond1_and_not_cond2 = _mm256_and_ps(cond1_mask, _mm256_xor_ps(cond2_mask, _mm256_set1_ps(-1)));\n        __m256 not_cond1_and_cond2 = _mm256_and_ps(_mm256_xor_ps(cond1_mask, _mm256_set1_ps(-1)), cond2_mask);\n        __m256 not_cond1_and_not_cond2 = _mm256_and_ps(_mm256_xor_ps(cond1_mask, _mm256_set1_ps(-1)), _mm256_xor_ps(cond2_mask, _mm256_set1_ps(-1)));\n\n        // Select values based on conditions\n        __m256 result = _mm256_or_ps(\n            _mm256_or_ps(\n                _mm256_and_ps(cond1_and_cond2, a),\n                _mm256_and_ps(cond1_and_not_cond2, b)\n            ),\n            _mm256_or_ps(\n                _mm256_and_ps(not_cond1_and_cond2, c),\n                _mm256_and_ps(not_cond1_and_not_cond2, d)\n            )\n        );\n\n        // Store the result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements (tail case)\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i mask1 = _mm256_set1_epi32(0x55555555); // Mask for 1-bit groups\n    __m256i mask2 = _mm256_set1_epi32(0x33333333); // Mask for 2-bit groups\n    __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F); // Mask for 4-bit groups\n    __m256i mask4 = _mm256_set1_epi32(0x01010101); // Mask for summing bytes\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Count bits in groups of 1\n        __m256i count = _mm256_sub_epi32(vec, _mm256_and_si256(_mm256_srli_epi32(vec, 1), mask1));\n\n        // Count bits in groups of 2\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask2), _mm256_and_si256(_mm256_srli_epi32(count, 2), mask2));\n\n        // Count bits in groups of 4\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask3), _mm256_and_si256(_mm256_srli_epi32(count, 4), mask3));\n\n        // Sum up the counts in each byte\n        count = _mm256_mullo_epi32(count, mask4);\n        __m256i result = _mm256_srli_epi32(count, 24);\n\n        // Store the result\n        __m128i low = _mm256_castsi256_si128(result);\n        __m128i high = _mm256_extracti128_si256(result, 1);\n        _mm_storel_epi64((__m128i*)&out[i], low);\n        _mm_storel_epi64((__m128i*)&out[i + 4], high);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val * 0x01010101) >> 24;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(A + i));\n\n        __m256i low_nibble = _mm256_and_si256(vec, _mm256_set1_epi8(0x0F));\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 4), _mm256_set1_epi8(0x0F));\n\n        __m256i low_bits = _mm256_shuffle_epi8(lookup, low_nibble);\n        __m256i high_bits = _mm256_shuffle_epi8(lookup, high_nibble);\n\n        __m256i sum = _mm256_add_epi8(low_bits, high_bits);\n\n        __m256i sum1 = _mm256_add_epi8(sum, _mm256_srli_epi32(vec, 8));\n        __m256i sum2 = _mm256_add_epi8(sum, _mm256_srli_epi32(vec, 8));\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i mask1 = _mm256_set1_epi32(0x55555555); // Mask for bits 0x01\n    __m256i mask2 = _mm256_set1_epi32(0x33333333); // Mask for bits 0x03\n    __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F); // Mask for bits 0x0F\n    __m256i mask4 = _mm256_set1_epi32(0x01010101); // Mask for summing bytes\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Count bits in each 32-bit integer\n        __m256i count = vec;\n        count = _mm256_sub_epi32(count, _mm256_and_si256(_mm256_srli_epi32(count, 1), mask1));\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask2), _mm256_and_si256(_mm256_srli_epi32(count, 2), mask2));\n        count = _mm256_add_epi32(_mm256_and_si256(count, mask3), _mm256_and_si256(_mm256_srli_epi32(count, 4), mask3));\n        count = _mm256_mullo_epi32(count, mask4);\n        count = _mm256_srli_epi32(count, 24);\n\n        // Store the result as 8-bit integers\n        __m128i low = _mm256_castsi256_si128(count);\n        __m128i high = _mm256_extracti128_si256(count, 1);\n        _mm_storel_epi64((__m128i*)&out[i], low);\n        _mm_storel_epi64((__m128i*)&out[i + 4], high);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val * 0x01010101) >> 24;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        __m256i low_nibble = _mm256_and_si256(vec, _mm256_set1_epi8(0x0F));\n        __m256i high_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 4), _mm256_set1_epi8(0x0F));\n\n        __m256i low_count = _mm256_shuffle_epi8(lookup, low_nibble);\n        __m256i high_count = _mm256_shuffle_epi8(lookup, high_nibble);\n\n        __m256i total_count = _mm256_add_epi8(low_count, high_count);\n\n        __m256i shifted1 = _mm256_srli_epi32(vec, 8);\n        low_nibble = _mm256_and_si256(shifted1, _mm256_set1_epi8(0x0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(shifted1, 4), _mm256_set1_epi8(0x0F));\n\n        low_count = _mm256_shuffle_epi8(lookup, low_nibble);\n        high_count = _mm256_shuffle_epi8(lookup, high_nibble);\n\n        total_count = _mm256_add_epi8(total_count, low_count);\n        total_count = _mm256_add_epi8(total_count, high_count);\n\n        __m256i shifted2 = _mm256_srli_epi32(vec, 16);\n        low_nibble = _mm256_and_si256(shifted2, _mm256_set1_epi8(0x0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(shifted2, 4), _mm256_set1_epi8(0x0F));\n\n        low_count = _mm256_shuffle_epi8(lookup, low_nibble);\n        high_count = _mm256_shuffle_epi8(lookup, high_nibble);\n\n        total_count = _mm256_add_epi8(total_count, low_count);\n        total_count = _mm256_add_epi8(total_count, high_count);\n\n        __m256i shifted3 = _mm256_srli_epi32(vec, 24);\n        low_nibble = _mm256_and_si256(shifted3, _mm256_set1_epi8(0x0F));\n        high_nibble = _mm256_and_si256(_mm256_srli_epi32(shifted3, 4), _mm256_set1_epi8(0x0F));\n\n        low_count = _mm256_shuffle_epi8(lookup, low_nibble);\n        high_count = _mm256_shuffle_epi8(lookup, high_nibble);\n\n        total_count = _mm256_add_epi8(total_count, low_count);\n        total_count = _mm256_add_epi8(total_count, high_count);\n\n        __m128i low = _mm256_castsi256_si128(total_count);\n        __m128i high = _mm256_extracti128_si256(total_count, 1);\n\n        _mm_storeu_si128((__m128i*)&out[i], low);\n        _mm_storeu_si128((__m128i*)&out[i + 4], high);\n    }\n\n    for (; i < total_elements; ++i) {\n        uint32_t value = A[i];\n        uint8_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i lookup = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(A + i));\n\n        __m256i lower_nibble = _mm256_and_si256(vec, _mm256_set1_epi8(0x0F));\n        __m256i upper_nibble = _mm256_and_si256(_mm256_srli_epi32(vec, 4), _mm256_set1_epi8(0x0F));\n\n        __m256i lower_bits = _mm256_shuffle_epi8(lookup, lower_nibble);\n        __m256i upper_bits = _mm256_shuffle_epi8(lookup, upper_nibble);\n\n        __m256i sum_bits = _mm256_add_epi8(lower_bits, upper_bits);\n\n        __m256i lower_sum = _mm256_and_si256(sum_bits, _mm256_set1_epi16(0x00FF));\n        __m256i upper_sum = _mm256_and_si256(_mm256_srli_epi16(sum_bits, 8), _mm256_set1_epi16(0x00FF));\n\n        __m256i total_bits = _mm256_add_epi8(lower_sum, upper_sum);\n\n        __m128i low = _mm256_castsi256_si128(total_bits);\n        __m128i high = _mm256_extracti128_si256(total_bits, 1);\n\n        _mm_storel_epi64((__m128i*)(out + i), low);\n        _mm_storel_epi64((__m128i*)(out + i + 4), high);\n    }\n\n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        out[i] = (uint8_t)__builtin_popcount(val);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256i vec_three = _mm256_set1_epi32(3);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_three);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero);\n\n        __m256i vec_abs = _mm256_abs_epi32(vec_a);\n        __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs, vec_cmp);\n\n        _mm256_storeu_si256((__m256i*)(B + i), vec_result);\n    }\n\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256i vec_three = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i vec_zero = _mm256_setzero_si256(); // Vector with all elements set to 0\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 integers from A\n        __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_three); // Compute A % 3\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero); // Compare (A % 3 == 0)\n\n        __m256i vec_abs = _mm256_abs_epi32(vec_a); // Compute absolute value of A\n        __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs, vec_cmp); // Select abs(A) if divisible by 3, else A\n\n        _mm256_storeu_si256((__m256i*)&B[i], vec_result); // Store result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256i vec_three = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i vec_zero = _mm256_setzero_si256(); // Vector with all elements set to 0\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 integers from A\n        __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_three); // Compute A % 3\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero); // Compare (A % 3 == 0)\n\n        __m256i vec_abs = _mm256_abs_epi32(vec_a); // Compute absolute values of A\n        __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs, vec_cmp); // Select abs(A) if condition is true, else A\n\n        _mm256_storeu_si256((__m256i*)&B[i], vec_result); // Store result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256i vec_three = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i vec_zero = _mm256_setzero_si256(); // Vector with all elements set to 0\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 integers from A\n        __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_three); // Compute A % 3\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero); // Compare (A % 3 == 0)\n\n        __m256i vec_abs = _mm256_abs_epi32(vec_a); // Compute absolute value of A\n        __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs, vec_cmp); // Select abs(A) if condition is true, else A\n\n        _mm256_storeu_si256((__m256i*)&B[i], vec_result); // Store result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m256i vec_three = _mm256_set1_epi32(3);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (; i + 8 <= total_elements; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_three); // Compute A % 3\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero); // Check if A % 3 == 0\n\n        __m256i vec_abs = _mm256_abs_epi32(vec_a); // Compute absolute value of A\n        __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs, vec_cmp); // Select based on condition\n\n        _mm256_storeu_si256((__m256i*)(B + i), vec_result);\n    }\n\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i mask_low_4 = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i all_ones = _mm256_set1_epi32(0xF);   // Value to compare low 4 bits\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits); // Rotate bits mask\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i)); // Load 8 elements from src\n        __m256i low_4_bits = _mm256_and_si256(src_vec, mask_low_4); // Extract low 4 bits\n        __m256i condition = _mm256_cmpeq_epi32(low_4_bits, all_ones); // Check if low 4 bits are all 1\n\n        // Rotate bits\n        __m256i rotated_left = _mm256_sllv_epi32(src_vec, rotate_mask); // Left shift\n        __m256i rotated_right = _mm256_srlv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask)); // Right shift\n        __m256i rotated = _mm256_or_si256(rotated_left, rotated_right); // Combine left and right shifts\n\n        // Bitwise negation\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF)); // Negate bits\n\n        // Select based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition); // Blend based on condition\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_4 = src[i] & 0xF;\n        if (low_4 == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i mask_low_4_bits = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i all_ones_low_4_bits = _mm256_set1_epi32(0xF); // Value to compare for all 1s in low 4 bits\n\n    while (i + 8 <= length) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i)); // Load 8 elements from src\n        __m256i low_4_bits = _mm256_and_si256(src_vec, mask_low_4_bits); // Extract low 4 bits\n        __m256i condition_mask = _mm256_cmpeq_epi32(low_4_bits, all_ones_low_4_bits); // Compare low 4 bits to all 1s\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits)), // Right shift\n            _mm256_sllv_epi32(src_vec, _mm256_set1_epi32(32 - rotate_bits)) // Left shift\n        ); // Perform rotation\n\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF)); // Bitwise negation\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition_mask); // Select based on condition\n        _mm256_storeu_si256((__m256i*)(dst + i), result); // Store result in dst\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        uint32_t low_4_bits = src[i] & 0xF;\n        if (low_4_bits == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i mask_low4 = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i all_ones = _mm256_set1_epi32(0xF); // Value to compare low 4 bits\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits); // Rotation bits mask\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 elements from src\n        __m256i low4_bits = _mm256_and_si256(src_vec, mask_low4); // Extract low 4 bits\n        __m256i condition = _mm256_cmpeq_epi32(low4_bits, all_ones); // Check if low 4 bits are all 1\n\n        // Perform rotation\n        __m256i rotated_left = _mm256_sllv_epi32(src_vec, rotate_mask); // Shift left\n        __m256i rotated_right = _mm256_srlv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask)); // Shift right\n        __m256i rotated = _mm256_or_si256(rotated_left, rotated_right); // Combine left and right shifts\n\n        // Perform negation\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF)); // Bitwise negation\n\n        // Select based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition); // Blend based on condition\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low4 = src[i] & 0xF;\n        if (low4 == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i mask_low_4 = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i all_ones = _mm256_set1_epi32(0xF);  // Value to compare for all 1s in low 4 bits\n\n    while (i + 8 <= length) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i)); // Load 8 elements from src\n\n        // Extract low 4 bits and compare with all 1s\n        __m256i low_4_bits = _mm256_and_si256(src_vec, mask_low_4);\n        __m256i condition_mask = _mm256_cmpeq_epi32(low_4_bits, all_ones);\n\n        // Perform bitwise rotation\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits)), // Shift right\n            _mm256_sllv_epi32(src_vec, _mm256_set1_epi32(32 - rotate_bits)) // Shift left\n        );\n\n        // Perform bitwise negation\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        // Select based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition_mask);\n\n        // Store result in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n\n        i += 8; // Process next 8 elements\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_4 = src[i] & 0xF;\n        if (low_4 == 0xF) {\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i = 0;\n    __m256i mask_low_4_bits = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i all_ones = _mm256_set1_epi32(0xF);        // All 1s in the low 4 bits\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits); // Rotate bits mask\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 elements from src\n        __m256i low_4_bits = _mm256_and_si256(src_vec, mask_low_4_bits); // Extract low 4 bits\n        __m256i condition = _mm256_cmpeq_epi32(low_4_bits, all_ones); // Check if low 4 bits are all 1\n\n        // Rotate bits toward low bits\n        __m256i rotated_left = _mm256_sllv_epi32(src_vec, rotate_mask);\n        __m256i rotated_right = _mm256_srlv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask));\n        __m256i rotated = _mm256_or_si256(rotated_left, rotated_right);\n\n        // Bitwise negation\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        // Select based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_4_bits = src[i] & 0xF;\n        if (low_4_bits == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 vec_epsilon = _mm256_set1_ps(epsilon);\n    __m256 vec_neg_epsilon = _mm256_set1_ps(-epsilon);\n    __m256 vec_zero = _mm256_set1_ps(0.0f);\n    __m256 vec_one = _mm256_set1_ps(1.0f);\n    __m256 vec_neg_one = _mm256_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_a = _mm256_loadu_ps(A + i);\n        __m256 vec_b = _mm256_loadu_ps(B + i);\n        __m256 vec_diff = _mm256_sub_ps(vec_a, vec_b);\n\n        __m256 mask_gt = _mm256_cmp_ps(vec_diff, vec_epsilon, _CMP_GT_OQ);\n        __m256 mask_lt = _mm256_cmp_ps(vec_diff, vec_neg_epsilon, _CMP_LT_OQ);\n        __m256 mask_eq = _mm256_andnot_ps(_mm256_or_ps(mask_gt, mask_lt), _mm256_set1_ps(1.0f));\n\n        __m256 result_gt = _mm256_and_ps(mask_gt, vec_one);\n        __m256 result_lt = _mm256_and_ps(mask_lt, vec_neg_one);\n        __m256 result_eq = _mm256_and_ps(mask_eq, vec_zero);\n\n        __m256 result = _mm256_or_ps(_mm256_or_ps(result_gt, result_lt), result_eq);\n\n        __m256i result_int = _mm256_cvtps_epi32(result);\n        _mm256_storeu_si256((__m256i*)(out + i), result_int);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 vec_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    __m256 vec_neg_epsilon = _mm256_set1_ps(-fabsf(epsilon));\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(A + i);\n        __m256 vec_B = _mm256_loadu_ps(B + i);\n        __m256 vec_diff = _mm256_sub_ps(vec_A, vec_B);\n\n        __m256 cmp_gt = _mm256_cmp_ps(vec_diff, vec_epsilon, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(vec_diff, vec_neg_epsilon, _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_andnot_ps(_mm256_or_ps(cmp_gt, cmp_lt), _mm256_set1_ps(1.0f));\n\n        __m256i result_gt = _mm256_castps_si256(cmp_gt);\n        __m256i result_lt = _mm256_castps_si256(cmp_lt);\n        __m256i result_eq = _mm256_castps_si256(cmp_eq);\n\n        __m256i vec_result = _mm256_add_epi8(\n            _mm256_add_epi8(result_gt, _mm256_set1_epi8(1)),\n            _mm256_sub_epi8(result_lt, _mm256_set1_epi8(1))\n        );\n\n        _mm256_storeu_si256((__m256i*)(out + i), vec_result);\n    }\n\n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    // Compute absolute value of epsilon\n    float abs_epsilon = fabsf(epsilon);\n    __m256 epsilon_vec = _mm256_set1_ps(abs_epsilon);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n    __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 elements from A and B\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n\n        // Compute A[i] - B[i]\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n\n        // Compute absolute difference\n        __m256 abs_diff_vec = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff_vec);\n\n        // Compare abs(A[i] - B[i]) <= abs(epsilon)\n        __m256 cmp_le_vec = _mm256_cmp_ps(abs_diff_vec, epsilon_vec, _CMP_LE_OS);\n\n        // Compare A[i] - B[i] > abs(epsilon)\n        __m256 cmp_gt_vec = _mm256_cmp_ps(diff_vec, epsilon_vec, _CMP_GT_OS);\n\n        // Compare A[i] - B[i] < -abs(epsilon)\n        __m256 cmp_lt_vec = _mm256_cmp_ps(diff_vec, _mm256_sub_ps(zero_vec, epsilon_vec), _CMP_LT_OS);\n\n        // Set results\n        __m256 result_vec = _mm256_blendv_ps(_mm256_blendv_ps(zero_vec, neg_one_vec, cmp_lt_vec), one_vec, cmp_gt_vec);\n\n        // Store results as int8_t\n        __m256i result_int_vec = _mm256_cvtps_epi32(result_vec);\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (int8_t)_mm256_extract_epi32(result_int_vec, j);\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > abs_epsilon) {\n            out[i] = 1;\n        } else if (diff < -abs_epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 vec_epsilon = _mm256_set1_ps(epsilon);\n    __m256 vec_abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(A + i);\n        __m256 vec_B = _mm256_loadu_ps(B + i);\n        __m256 vec_diff = _mm256_sub_ps(vec_A, vec_B);\n        __m256 vec_abs_diff = _mm256_and_ps(vec_diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n\n        __m256 cmp_gt = _mm256_cmp_ps(vec_diff, vec_abs_epsilon, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(vec_diff, _mm256_sub_ps(_mm256_setzero_ps(), vec_abs_epsilon), _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_cmp_ps(vec_abs_diff, vec_abs_epsilon, _CMP_LE_OQ);\n\n        __m256i result_gt = _mm256_castps_si256(cmp_gt);\n        __m256i result_lt = _mm256_castps_si256(cmp_lt);\n        __m256i result_eq = _mm256_castps_si256(cmp_eq);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(result_gt, _mm256_set1_epi8(1)),\n            _mm256_or_si256(\n                _mm256_and_si256(result_lt, _mm256_set1_epi8(-1)),\n                _mm256_and_si256(result_eq, _mm256_set1_epi8(0))\n            )\n        );\n\n        _mm256_storeu_si256((__m256i*)(out + i), result);\n    }\n\n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 vec_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    __m256 vec_neg_epsilon = _mm256_set1_ps(-fabsf(epsilon));\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(A + i);\n        __m256 vec_B = _mm256_loadu_ps(B + i);\n        __m256 vec_diff = _mm256_sub_ps(vec_A, vec_B);\n\n        __m256 cmp_gt = _mm256_cmp_ps(vec_diff, vec_epsilon, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(vec_diff, vec_neg_epsilon, _CMP_LT_OQ);\n        __m256 cmp_eq = _mm256_andnot_ps(_mm256_or_ps(cmp_gt, cmp_lt), _mm256_set1_ps(1.0f));\n\n        __m256i result_gt = _mm256_castps_si256(cmp_gt);\n        __m256i result_lt = _mm256_castps_si256(cmp_lt);\n        __m256i result_eq = _mm256_castps_si256(cmp_eq);\n\n        __m256i result = _mm256_sub_epi32(result_gt, result_lt);\n        result = _mm256_and_si256(result, _mm256_set1_epi32(1));\n        result = _mm256_or_si256(result, _mm256_and_si256(result_eq, _mm256_set1_epi32(0)));\n\n        int8_t temp[8];\n        _mm256_storeu_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = temp[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_int32 = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_int32 = _mm256_set1_ps((float)INT32_MIN);\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 values = _mm256_loadu_ps(&A[i]);\n        values = _mm256_min_ps(values, max_int32);\n        values = _mm256_max_ps(values, min_int32);\n        __m256i int_values = _mm256_cvtps_epi32(values);\n\n        // Saturate values manually\n        __m256i saturated = _mm256_max_epi32(int_values, int32_min);\n        saturated = _mm256_min_epi32(saturated, int32_max);\n\n        _mm256_storeu_si256((__m256i*)&B[i], saturated);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_int32 = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_int32 = _mm256_set1_ps((float)INT32_MIN);\n    const __m256i int32_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int32_min = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 src = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values to INT32_MIN and INT32_MAX\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(src, min_int32), max_int32);\n\n        // Convert to int32_t\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 float_vals = _mm256_loadu_ps(&A[i]);\n        float_vals = _mm256_min_ps(float_vals, max_val);\n        float_vals = _mm256_max_ps(float_vals, min_val);\n        __m256i int_vals = _mm256_cvtps_epi32(float_vals);\n        _mm256_storeu_si256((__m256i*)&B[i], int_vals);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_int32 = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_int32 = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 src = _mm256_loadu_ps(&A[i]);\n\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(src, min_int32), max_int32);\n\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 input = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(input, min_val), max_val);\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale); // Broadcast the scale factor to all elements\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 int32_t values\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);        // Convert int32_t to float\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm256_storeu_ps(&B[i], scaled_vec);                  // Store the scaled float values\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale); // Broadcast scale factor to all lanes\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)(A + i)); // Load 8 int32_t values\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);          // Convert int32_t to float\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm256_storeu_ps(B + i, scaled_vec);                    // Store the result in B\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale); // Broadcast scale factor to all lanes\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)(A + i)); // Load 8 int32_t values\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);          // Convert int32_t to float\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm256_storeu_ps(B + i, scaled_vec);                    // Store the scaled values in B\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale); // Broadcast scale factor to all elements\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 int32_t values\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);        // Convert int32_t to float\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm256_storeu_ps(&B[i], scaled_vec);                   // Store the results in B\n    }\n\n    // Handle remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale); // Broadcast scale factor to all elements\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)&A[i]); // Load 8 int32_t values\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);        // Convert int32_t to float\n        __m256 scaled_vec = _mm256_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm256_storeu_ps(&B[i], scaled_vec);                   // Store the result\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 floats from A and B\n        __m256 a_vals = _mm256_loadu_ps(&A[i]);\n        __m256 b_vals = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m256 div_vals = _mm256_div_ps(a_vals, b_vals);\n\n        // Round to nearest integer\n        __m256 rounded_vals = _mm256_round_ps(div_vals, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert to 32-bit integers\n        __m256i int_vals = _mm256_cvtps_epi32(rounded_vals);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)&C[i], int_vals);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 floats from A and B\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m256 vecDiv = _mm256_div_ps(vecA, vecB);\n\n        // Round to the nearest integer\n        __m256 vecRounded = _mm256_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert rounded floats to 32-bit integers\n        __m256i vecInt = _mm256_cvttps_epi32(vecRounded);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)&C[i], vecInt);\n    }\n\n    // Process remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256 vecA, vecB, vecDiv, vecRounded;\n    __m256i vecResult;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        vecA = _mm256_loadu_ps(&A[i]);\n        vecB = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        vecDiv = _mm256_div_ps(vecA, vecB);\n\n        // Round to the nearest integer\n        vecRounded = _mm256_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert to 32-bit integers\n        vecResult = _mm256_cvtps_epi32(vecRounded);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)&C[i], vecResult);\n    }\n\n    // Handle the remaining elements\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div > 0 ? div + 0.5f : div - 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 floats from A and B\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m256 vecDiv = _mm256_div_ps(vecA, vecB);\n\n        // Round the result to the nearest integer\n        __m256 vecRounded = _mm256_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the rounded floats to 32-bit integers\n        __m256i vecInt = _mm256_cvtps_epi32(vecRounded);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)&C[i], vecInt);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n\n    // Process in chunks of 8 floats\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256 vecA = _mm256_loadu_ps(&A[i]); // Load 8 floats from A\n        __m256 vecB = _mm256_loadu_ps(&B[i]); // Load 8 floats from B\n        __m256 vecDiv = _mm256_div_ps(vecA, vecB); // Perform element-wise division\n        __m256 vecRounded = _mm256_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        __m256i vecInt = _mm256_cvtps_epi32(vecRounded); // Convert to 32-bit integers\n        _mm256_storeu_si256((__m256i*)&C[i], vecInt); // Store the result in C\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div > 0 ? 0.5f : -0.5f)); // Manual rounding to nearest integer\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 double values from A\n        __m256d values = _mm256_loadu_pd(&A[i]);\n\n        // Convert double to float\n        __m128 converted = _mm256_cvtpd_ps(values);\n\n        // Saturate values to FLT_MAX and -FLT_MAX\n        __m256 saturated = _mm256_min_ps(_mm256_max_ps(_mm256_castps128_ps256(converted), min_float), max_float);\n\n        // Store the saturated float values into B\n        _mm_storeu_ps(&B[i], _mm256_castps256_ps128(saturated));\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 double values from A\n        __m256d vec_double = _mm256_loadu_pd(&A[i]);\n\n        // Convert double to float\n        __m128 vec_float = _mm256_cvtpd_ps(vec_double);\n\n        // Saturate values to FLT_MAX and -FLT_MAX\n        __m256 vec_float_extended = _mm256_castps128_ps256(vec_float);\n        vec_float_extended = _mm256_min_ps(vec_float_extended, max_float);\n        vec_float_extended = _mm256_max_ps(vec_float_extended, min_float);\n\n        // Store the saturated float values into B\n        _mm_storeu_ps(&B[i], _mm256_castps256_ps128(vec_float_extended));\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 double values from A\n        __m256d values = _mm256_loadu_pd(&A[i]);\n\n        // Convert double to float\n        __m128 float_values = _mm256_cvtpd_ps(values);\n\n        // Clamp the values to the range [-FLT_MAX, FLT_MAX]\n        __m256 float_values_256 = _mm256_cvtps_ph(float_values, 0);\n        __m256 clamped = _mm256_min_ps(max_float, _mm256_max_ps(min_float, float_values_256));\n\n        // Store the result in B\n        _mm256_storeu_ps(&B[i], clamped);\n    }\n\n    // Handle the remaining elements\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);  // FLT_MAX as 256-bit vector\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX); // -FLT_MAX as 256-bit vector\n\n    size_t i = 0;\n\n    // Process 4 double values (256 bits) at a time\n    for (; i + 3 < size; i += 4) {\n        // Load 4 double values from A\n        __m256d values = _mm256_loadu_pd(&A[i]);\n\n        // Convert double to float\n        __m128 floats = _mm256_cvtpd_ps(values);\n\n        // Saturate values to FLT_MAX and -FLT_MAX\n        __m256 floats_256 = _mm256_cvtps_ph(floats, 0); // Promote to 256-bit\n        floats_256 = _mm256_min_ps(floats_256, max_float);\n        floats_256 = _mm256_max_ps(floats_256, min_float);\n\n        // Store the saturated float values into B\n        _mm_storeu_ps(&B[i], floats_256);\n    }\n\n    // Handle the remaining elements\n    for (; i < size; i++) {\n        float value = (float)A[i];\n        if (value > FLT_MAX) {\n            value = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            value = -FLT_MAX;\n        }\n        B[i] = value;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 double values from A\n        __m256d vec_double = _mm256_loadu_pd(&A[i]);\n\n        // Convert double to float\n        __m128 vec_float = _mm256_cvtpd_ps(vec_double);\n\n        // Saturate values to FLT_MAX and -FLT_MAX\n        __m256 vec_float_extended = _mm256_castps128_ps256(vec_float);\n        vec_float_extended = _mm256_min_ps(vec_float_extended, max_float);\n        vec_float_extended = _mm256_max_ps(vec_float_extended, min_float);\n\n        // Store the saturated float values into B\n        _mm_storeu_ps(&B[i], _mm256_castps256_ps128(vec_float_extended));\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 16; // AVX2 can process 16 int16_t values at a time\n\n    // Process in chunks of 16 elements using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 16 int16_t values from A\n        __m256i vec_int16 = _mm256_loadu_si256((__m256i*)(A + i));\n        \n        // Convert int16_t to int32_t (sign-extend)\n        __m256i vec_int32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec_int16));\n        __m256i vec_int32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec_int16, 1));\n        \n        // Store the converted int32_t values into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec_int32_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec_int32_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_width = 16; // AVX2 processes 16 int16_t elements at a time\n\n    // Process elements in chunks of 16 using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 16 int16_t values from A\n        __m256i vec_int16 = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Convert int16_t values to int32_t values\n        __m256i vec_int32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec_int16));\n        __m256i vec_int32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec_int16, 1));\n\n        // Store the converted int32_t values into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec_int32_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec_int32_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec16 = _mm256_loadu_si256((__m256i*)(A + i));\n        \n        // Extract lower 8 int16_t values and convert to int32_t\n        __m256i vec32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec16));\n        // Extract upper 8 int16_t values and convert to int32_t\n        __m256i vec32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec16, 1));\n        \n        // Store the converted values into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec32_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec32_high);\n    }\n    \n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 16; // AVX2 processes 16 int16_t elements at a time\n\n    // Process elements in chunks of 16 using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 16 int16_t values from A into an AVX2 register\n        __m256i vec_int16 = _mm256_loadu_si256((__m256i*)(A + i));\n        \n        // Convert int16_t values to int32_t values\n        __m256i vec_int32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec_int16));\n        __m256i vec_int32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec_int16, 1));\n        \n        // Store the converted int32_t values into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec_int32_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec_int32_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec16 = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Extract lower 8 int16_t values and convert to int32_t\n        __m256i vec32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec16));\n\n        // Extract upper 8 int16_t values and convert to int32_t\n        __m256i vec32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec16, 1));\n\n        // Store the converted values into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec32_low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec32_high);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Compare control values with 0\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256()));\n\n        // Normalize values\n        __m256 normalized_vec = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized_vec = _mm256_max_ps(normalized_vec, zero_vec); // Clamp to [0, 1]\n        normalized_vec = _mm256_min_ps(normalized_vec, one_vec);\n\n        // Blend normalized values and original values based on control mask\n        __m256 result_vec = _mm256_blendv_ps(a_vec, normalized_vec, mask);\n\n        // Store result in B\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Compare control values to 0\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256()));\n\n        // Normalize values\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        // Blend normalized values with original values based on control mask\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n\n        // Store the result in B\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    __m256 zero_vec = _mm256_set1_ps(0.0f);\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Compare control > 0\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256()));\n\n        // Normalize: (A[i] - min_val) / (max_val - min_val)\n        __m256 normalized_vec = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n\n        // Clamp to [0, 1]\n        normalized_vec = _mm256_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm256_min_ps(normalized_vec, one_vec);\n\n        // Select normalized value or original value based on control\n        __m256 result_vec = _mm256_blendv_ps(a_vec, normalized_vec, mask);\n\n        // Store result in B\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_val_vec = _mm256_set1_ps(min_val);\n    const __m256 max_val_vec = _mm256_set1_ps(max_val);\n    const __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    const __m256 zero_vec = _mm256_set1_ps(0.0f);\n    const __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Compare control values with 0\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256()));\n\n        // Normalize values\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        // Blend normalized values and original values based on mask\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask);\n\n        // Store result in B\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n    __m256 zero_vec = _mm256_set1_ps(0.0f);\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Compare control > 0\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256()));\n\n        // Normalize: (A[i] - min_val) / (max_val - min_val)\n        __m256 normalized_vec = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), range_vec);\n\n        // Clamp to [0, 1]\n        normalized_vec = _mm256_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm256_min_ps(normalized_vec, one_vec);\n\n        // Blend normalized values and original values based on control mask\n        __m256 result_vec = _mm256_blendv_ps(a_vec, normalized_vec, mask);\n\n        // Store the result in B\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t value\n        __m256i high_bits = _mm256_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m256 float_values = _mm256_cvtepi32_ps(high_bits);\n\n        // Compute the square root of the float values\n        __m256 sqrt_values = _mm256_sqrt_ps(float_values);\n\n        // Store the results in B\n        _mm256_storeu_ps(&B[i], sqrt_values);\n    }\n\n    // Process remaining elements (if size is not a multiple of 8)\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float float_value = (float)high_bits;\n        B[i] = sqrtf(float_value);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t value\n        __m256i high16 = _mm256_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVals = _mm256_cvtepi32_ps(high16);\n\n        // Compute the square root of the float values\n        __m256 sqrtVals = _mm256_sqrt_ps(floatVals);\n\n        // Store the results in B\n        _mm256_storeu_ps(&B[i], sqrtVals);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        float floatVal = (float)high16;\n        B[i] = sqrtf(floatVal);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n\n    // Process in chunks of 8 elements using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Extract the high 16 bits of each uint32_t value\n        __m256i high16 = _mm256_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVals = _mm256_cvtepi32_ps(high16);\n\n        // Compute the square root of the float values\n        __m256 sqrtVals = _mm256_sqrt_ps(floatVals);\n\n        // Store the results in B\n        _mm256_storeu_ps(B + i, sqrtVals);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        float floatVal = (float)high16;\n        B[i] = sqrtf(floatVal);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Shift right by 16 bits to extract the high 16 bits\n        __m256i high16 = _mm256_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVals = _mm256_cvtepi32_ps(high16);\n\n        // Compute the square root of the float values\n        __m256 sqrtVals = _mm256_sqrt_ps(floatVals);\n\n        // Store the results in B\n        _mm256_storeu_ps(&B[i], sqrtVals);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        float floatVal = (float)high16;\n        B[i] = sqrtf(floatVal);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n\n    // Process in chunks of simd_width\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVals = _mm256_cvtepi32_ps(highBits);\n\n        // Compute the square root of the float values\n        __m256 sqrtVals = _mm256_sqrt_ps(floatVals);\n\n        // Store the results in B\n        _mm256_storeu_ps(&B[i], sqrtVals);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        uint32_t highBits = A[i] >> 16;\n        float floatVal = (float)highBits;\n        B[i] = sqrtf(floatVal);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A into an AVX2 register\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Cast the integer vector to a float vector without conversion\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting float vector into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        // Type-punning using direct reinterpretation\n        B[i] = *((float*)&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 elements at a time for 32-bit values\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 uint32_t values from A into an AVX2 register\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        \n        // Cast the integer vector to a floating-point vector (reinterpret cast)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        \n        // Store the resulting float vector into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i]; // Type-punning for remaining elements\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        // Reinterpret the bits as 8 float values\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        // Store the resulting float values in B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle the remaining elements (if size is not a multiple of 8)\n    for (; i < size; ++i) {\n        // Reinterpret the bits of the remaining uint32_t values as float\n        B[i] = *((float*)(A + i));\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 8; // AVX2 processes 8 elements at a time for 32-bit data\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 uint32_t values from A into an AVX2 register\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        \n        // Reinterpret the bits as 8 floats\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        \n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        // Reinterpret the bits directly for the remaining elements\n        B[i] = *((float*)&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process in chunks of 8 elements using AVX2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A into an AVX2 register\n        __m256i int_vec = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Cast the integer vector to a floating-point vector (type-punning)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting float vector into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        // Type-punning each uint32_t to float manually\n        B[i] = *((float*)&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1; // Start from the second element\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 integers from A[i] and A[i-1]\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute differences (A[i] - A[i-1])\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean conditions\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask for conditions (convert bool to 0xFFFFFFFF or 0x00000000)\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1)));\n\n        // Apply mask: set diff[i] to 0.0f where cond[i] is false\n        __m256 result = _mm256_and_ps(mask, diff_float);\n\n        // Store the result in the destination array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    size_t i = 1; // Start from the second element\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 integers from A[i] and A[i-1]\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute the difference (current - previous)\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert the difference to float\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean values from cond[i] and convert them to a mask\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1)));\n\n        // Apply the mask: set diff[i] to 0.0f where cond[i] is false\n        __m256 result = _mm256_and_ps(mask, diff_float);\n\n        // Store the result in the diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle the remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    size_t i = 1;\n\n    // Process in chunks of 8 (AVX2 processes 8 integers or floats at a time)\n    for (; i + 7 < size; i += 8) {\n        // Load 8 elements of A and cond\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Compute differences (current - previous)\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Create a mask for cond (convert boolean to 0xFFFFFFFF or 0x0)\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1)));\n\n        // Apply mask: set diff[i] to 0.0f where cond[i] is false\n        __m256 result = _mm256_and_ps(diff_float, mask);\n\n        // Store the result in diff\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle the remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1; // Start from the second element\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 integers from A\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute differences (A[i] - A[i-1])\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean values from cond\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask for cond[i] (true -> 0xFFFFFFFF, false -> 0x00000000)\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1)));\n\n        // Apply mask: diff[i] = diff_float if cond[i] is true, otherwise 0.0f\n        __m256 result = _mm256_and_ps(mask, diff_float);\n\n        // Store the result in the destination array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1; // Start from the second element\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 integers from A and A[i-1]\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i previous = _mm256_loadu_si256((__m256i*)(A + i - 1));\n\n        // Compute differences (A[i] - A[i-1])\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean conditions\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)(cond + i));\n\n        // Create a mask for the condition (convert boolean to 0xFFFFFFFF or 0x0)\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1)));\n\n        // Apply the mask: set diff[i] to 0.0f if cond[i] is false\n        __m256 result = _mm256_and_ps(mask, diff_float);\n\n        // Store the result in the destination array\n        _mm256_storeu_ps(diff + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL); // High 32 bits set to 1\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 32-bit integers from src\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n\n        // Convert 32-bit integers to 64-bit integers (low part)\n        __m256i low_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 0));\n        __m256i high_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n\n        // Add high bits to each 64-bit integer\n        low_part = _mm256_or_si256(low_part, high_bits);\n        high_part = _mm256_or_si256(high_part, high_bits);\n\n        // Store results in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), low_part);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high_part);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL); // High 32 bits set to 1\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 32-bit integers from src\n        __m256i src_data = _mm256_loadu_si256((__m256i*)(src + i));\n\n        // Convert 32-bit integers to 64-bit integers (low part)\n        __m256i src_low = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_data));\n        __m256i src_high = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_data, 1));\n\n        // Set high 32 bits to 1\n        src_low = _mm256_or_si256(src_low, high_bits);\n        src_high = _mm256_or_si256(src_high, high_bits);\n\n        // Store the results in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), src_low);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), src_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL);\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 32-bit integers from src\n        __m256i src_vals = _mm256_loadu_si256((__m256i*)(src + i));\n\n        // Convert 32-bit integers to 64-bit integers (low part)\n        __m256i src_vals_low = _mm256_unpacklo_epi32(src_vals, _mm256_setzero_si256());\n        __m256i src_vals_high = _mm256_unpackhi_epi32(src_vals, _mm256_setzero_si256());\n\n        // Add high bits to each 64-bit integer\n        __m256i result_low = _mm256_or_si256(src_vals_low, high_bits);\n        __m256i result_high = _mm256_or_si256(src_vals_high, high_bits);\n\n        // Store the results in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result_low);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), result_high);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL); // High 32 bits set to 1\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i)); // Load 8 32-bit integers\n        __m256i src_low = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 0)); // Convert lower 4 elements to 64-bit\n        __m256i src_high = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1)); // Convert upper 4 elements to 64-bit\n\n        __m256i dst_low = _mm256_or_si256(src_low, high_bits); // Set high 32 bits to 1\n        __m256i dst_high = _mm256_or_si256(src_high, high_bits); // Set high 32 bits to 1\n\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_low); // Store lower 4 results\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), dst_high); // Store upper 4 results\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL; // Set high 32 bits to 1\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    const __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000ULL); // High 32 bits set to 1\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 32-bit integers from src\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n\n        // Convert 32-bit integers to 64-bit integers (low part)\n        __m256i src_low = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_vec));\n        __m256i src_high = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n\n        // Add high bits to each 64-bit integer\n        src_low = _mm256_or_si256(src_low, high_bits);\n        src_high = _mm256_or_si256(src_high, high_bits);\n\n        // Store the results in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), src_low);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), src_high);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd(); // Initialize the sum vector to zero\n    size_t i = 0;\n\n    // Process in chunks of 4 indices using AVX2\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Convert 16-bit indices to 32-bit integers\n        __m256i indices_32 = _mm256_cvtepi16_epi32(indices);\n\n        // Gather the corresponding elements from the vec array\n        __m256d gathered_values = _mm256_i32gather_pd(vec, indices_32, 8);\n\n        // Accumulate the gathered values into the sum vector\n        sum_vec = _mm256_add_pd(sum_vec, gathered_values);\n    }\n\n    // Horizontal sum of the sum vector\n    double result[4];\n    _mm256_storeu_pd(result, sum_vec);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < length) {\n            total_sum += vec[index[i]];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd(); // Initialize sum vector to zero\n    size_t i = 0;\n\n    // Process in chunks of 4 indices using AVX2\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Convert 16-bit indices to 32-bit integers\n        __m256i indices_32 = _mm256_cvtepi16_epi32(indices);\n\n        // Gather 4 double values from vec using the indices\n        __m256d gathered = _mm256_i32gather_pd(vec, indices_32, 8);\n\n        // Accumulate the gathered values into the sum vector\n        sum = _mm256_add_pd(sum, gathered);\n    }\n\n    // Horizontal sum of the sum vector\n    double result[4];\n    _mm256_storeu_pd(result, sum);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < length) {\n            total_sum += vec[index[i]];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd(); // Initialize sum vector to zero\n    size_t i = 0;\n\n    // Process in chunks of 4 indices\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Extract the indices as 32-bit integers\n        __m128i indices_low = _mm_unpacklo_epi16(indices, _mm_setzero_si128());\n        __m128i indices_high = _mm_unpackhi_epi16(indices, _mm_setzero_si128());\n\n        // Gather 4 double values from vec using the indices\n        __m256d values_low = _mm256_i32gather_pd(vec, indices_low, 8);\n        __m256d values_high = _mm256_i32gather_pd(vec, indices_high, 8);\n\n        // Accumulate the values into the sum vector\n        sum = _mm256_add_pd(sum, values_low);\n        sum = _mm256_add_pd(sum, values_high);\n    }\n\n    // Horizontal sum of the accumulated vector\n    double result[4];\n    _mm256_storeu_pd(result, sum);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process 4 indices at a time\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Convert 16-bit indices to 32-bit integers\n        __m256i indices_32 = _mm256_cvtepi16_epi32(indices);\n\n        // Gather 4 double values from vec using the indices\n        __m256d gathered_values = _mm256_i32gather_pd(vec, indices_32, 8);\n\n        // Accumulate the gathered values into the sum vector\n        sum_vec = _mm256_add_pd(sum_vec, gathered_values);\n    }\n\n    // Horizontal sum of the accumulated values in sum_vec\n    __m128d low = _mm256_castpd256_pd128(sum_vec);\n    __m128d high = _mm256_extractf128_pd(sum_vec, 1);\n    __m128d sum_128 = _mm_add_pd(low, high);\n    double result[2];\n    _mm_storeu_pd(result, sum_128);\n    double total_sum = result[0] + result[1];\n\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < length) {\n            total_sum += vec[index[i]];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd(); // Initialize sum vector to zero\n    size_t i = 0;\n\n    // Process indices in chunks of 4 (since AVX can handle 4 doubles at a time)\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices from the index array\n        __m256i indices = _mm256_loadu_si256((__m256i*)(index + i));\n\n        // Convert 16-bit indices to 32-bit integers\n        __m256i indices_32 = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(indices));\n\n        // Gather 4 double values from the vec array using the indices\n        __m256d gathered_values = _mm256_i32gather_pd(vec, indices_32, 8);\n\n        // Accumulate the gathered values into the sum vector\n        sum_vec = _mm256_add_pd(sum_vec, gathered_values);\n    }\n\n    // Horizontal sum of the sum_vec vector\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        if (index[i] >= 0 && index[i] < length) {\n            result += vec[index[i]];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n                    __m256d mul = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, mul);\n                }\n            }\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n            output[i * output_size + j] = total_sum > 0 ? total_sum : 0; // ReLU activation\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, mul);\n                }\n            }\n\n            // Horizontal sum of the SIMD register\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = result > 0.0 ? result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, mul);\n                }\n            }\n\n            // Horizontal addition of the SIMD register\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double conv_result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = conv_result > 0.0 ? conv_result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_row = i + ki;\n                    size_t input_col = j + kj;\n\n                    size_t kernel_index = ki * kernel_size + kj;\n                    size_t input_index = input_row * input_size + input_col;\n\n                    __m256d input_vals = _mm256_loadu_pd(&input[input_index]);\n                    __m256d kernel_vals = _mm256_loadu_pd(&kernel[kernel_index]);\n\n                    __m256d mul = _mm256_mul_pd(input_vals, kernel_vals);\n                    sum = _mm256_add_pd(sum, mul);\n                }\n            }\n\n            // Horizontal sum of the SIMD register\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = total_sum > 0.0 ? total_sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, mul);\n                }\n            }\n\n            // Horizontal addition of the sum vector\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = total_sum > 0 ? total_sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m256d current_vec = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            size_t remaining = n - j;\n            if (remaining < 4) {\n                for (size_t k = 0; k < remaining; ++k) {\n                    if (std::abs(numbers[i] - numbers[j + k]) < threshold) {\n                        return true;\n                    }\n                }\n                break;\n            }\n\n            __m256d other_vec = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff_vec = _mm256_sub_pd(current_vec, other_vec);\n            __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec);\n            __m256d cmp_vec = _mm256_cmp_pd(abs_diff_vec, threshold_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp_vec)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m256d current = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j + 3 < n; j += 4) {\n            __m256d other = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(current, other);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        // Handle remaining elements (non-multiple of 4)\n        for (size_t j = (n / 4) * 4; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d current_val = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j + 3 < size; j += 4) {\n            __m256d other_vals = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(current_val, other_vals);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        for (size_t j = size - (size % 4); j < size; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m256d current = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            size_t remaining = n - j;\n            if (remaining < 4) {\n                for (size_t k = 0; k < remaining; ++k) {\n                    if (std::abs(numbers[i] - numbers[j + k]) < threshold) {\n                        return true;\n                    }\n                }\n                break;\n            }\n\n            __m256d next_vals = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(current, next_vals);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m256d current = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            size_t remaining = n - j;\n            if (remaining >= 4) {\n                __m256d next_vals = _mm256_loadu_pd(&numbers[j]);\n                __m256d diff = _mm256_sub_pd(current, next_vals);\n                __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n                __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n                if (_mm256_movemask_pd(cmp)) {\n                    return true;\n                }\n            } else {\n                for (size_t k = j; k < n; ++k) {\n                    if (std::abs(numbers[i] - numbers[k]) < threshold) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int size = operations.size();\n    __m256i balance = _mm256_setzero_si256(); // Initialize balance to zero\n    __m256i zero = _mm256_setzero_si256();   // Zero vector for comparison\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]); // Load 8 integers\n        balance = _mm256_add_epi32(balance, ops);                   // Update balance\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);            // Check if balance < 0\n        if (_mm256_movemask_epi8(cmp)) {                            // If any element < 0\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    __m256i cumulative_sum = _mm256_setzero_si256(); // Initialize cumulative sum to zero\n    __m256i zero = _mm256_setzero_si256();          // Zero vector for comparison\n\n    for (int i = 0; i < n; i += 8) {\n        // Load the next 8 integers into an AVX register\n        __m256i current = _mm256_loadu_si256((__m256i*)&operations[i]);\n\n        // Add the current values to the cumulative sum\n        cumulative_sum = _mm256_add_epi32(cumulative_sum, current);\n\n        // Check if any value in the cumulative sum is less than zero\n        __m256i cmp = _mm256_cmpgt_epi32(zero, cumulative_sum);\n\n        // If any element is less than zero, return true\n        if (_mm256_movemask_epi8(cmp)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int size = operations.size();\n    __m256i balance = _mm256_setzero_si256(); // Initialize balance to zero\n    __m256i zero = _mm256_setzero_si256();   // Zero vector for comparison\n\n    for (int i = 0; i < size; i += 8) {\n        // Load 8 integers into an AVX2 register\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n\n        // Update balance\n        balance = _mm256_add_epi32(balance, ops);\n\n        // Check if any element in the balance is below zero\n        __m256i mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int size = operations.size();\n    __m256i balance = _mm256_setzero_si256(); // Initialize balance to zero\n    __m256i zero = _mm256_setzero_si256();   // Zero vector for comparison\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]); // Load 8 integers\n        balance = _mm256_add_epi32(balance, ops);                   // Update balance\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);            // Compare balance < 0\n        if (_mm256_movemask_epi8(cmp)) {                            // Check if any element is negative\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    int balance = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    __m256i current_balance = _mm256_set1_epi32(0); // Initialize balance vector to zero\n    for (int i = 0; i < size; i += 8) {\n        // Load the next 8 integers into an AVX2 register\n        __m256i ops = _mm256_setzero_si256();\n        if (i + 8 <= size) {\n            ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        } else {\n            int temp[8] = {0};\n            for (int j = 0; j < size - i; ++j) {\n                temp[j] = operations[i + j];\n            }\n            ops = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        // Add the operations to the current balance\n        current_balance = _mm256_add_epi32(current_balance, ops);\n\n        // Check if any value in the current balance vector is below zero\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi32(zero, current_balance);\n        if (_mm256_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    // Check remaining operations if size is not a multiple of 8\n    for (int i = (size / 8) * 8; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of sum_vec\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Calculate Mean Absolute Deviation using SIMD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_vec = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff_vec = _mm256_max_pd(diff_vec, _mm256_sub_pd(_mm256_setzero_pd(), diff_vec));\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff_vec);\n    }\n\n    // Horizontal sum of mad_sum_vec\n    _mm256_storeu_pd(sum_array, mad_sum_vec);\n    double mad_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Calculate the mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Step 2: Calculate Mean Absolute Deviation using SIMD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_vec = _mm256_setzero_pd();\n    i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_vec = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec); // Absolute value\n        mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n    }\n\n    double mad[4];\n    _mm256_storeu_pd(mad, mad_vec);\n    double total_mad = mad[0] + mad[1] + mad[2] + mad[3];\n\n    for (; i < n; ++i) {\n        total_mad += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Compute the mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Step 2: Compute the Mean Absolute Deviation using SIMD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_vec = _mm256_setzero_pd();\n    i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_vec = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff_vec = _mm256_max_pd(diff_vec, _mm256_sub_pd(_mm256_setzero_pd(), diff_vec));\n        mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n    }\n\n    double mad_array[4];\n    _mm256_storeu_pd(mad_array, mad_vec);\n    double mad_sum = mad_array[0] + mad_array[1] + mad_array[2] + mad_array[3];\n\n    for (; i < n; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Calculate the mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Step 2: Calculate Mean Absolute Deviation using SIMD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_vec = _mm256_setzero_pd();\n    i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_vec = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec); // Absolute value\n        mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n    }\n\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_vec);\n    double total_mad = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n\n    for (; i < n; ++i) {\n        total_mad += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Compute the mean using SIMD\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double mean = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < n; ++i) {\n        mean += numbers[i];\n    }\n    mean /= n;\n\n    // Compute the Mean Absolute Deviation using SIMD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_vec = _mm256_setzero_pd();\n    i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_vec = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff_vec = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_vec); // Absolute value\n        mad_vec = _mm256_add_pd(mad_vec, abs_diff_vec);\n    }\n\n    double mad[4];\n    _mm256_storeu_pd(mad, mad_vec);\n    double mad_sum = mad[0] + mad[1] + mad[2] + mad[3];\n\n    for (; i < n; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    size_t result_size = 2 * n - (n > 0 ? 1 : 0); // Calculate the size of the result vector\n    std::vector<int> result(result_size, delimeter);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t i = 0;\n    __m256i delimeter_vec = _mm256_set1_epi32(delimeter);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i interleaved_vec = _mm256_unpacklo_epi32(input_vec, delimeter_vec);\n        _mm256_storeu_si256((__m256i*)&result[2 * i], interleaved_vec);\n    }\n\n    for (; i < n; ++i) {\n        result[2 * i] = numbers[i];\n        result[2 * i + 1] = delimeter;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    size_t result_size = 2 * n - (n > 0 ? 1 : 0);\n    std::vector<int> result(result_size, delimeter);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_chunks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i delimeter_vec = _mm256_set1_epi32(delimeter);\n\n    for (size_t i = 0; i < simd_chunks; ++i) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)&numbers[i * simd_width]);\n        __m256i interleaved_vec = _mm256_unpacklo_epi32(input_vec, delimeter_vec);\n        _mm256_storeu_si256((__m256i*)&result[i * simd_width * 2], interleaved_vec);\n\n        interleaved_vec = _mm256_unpackhi_epi32(input_vec, delimeter_vec);\n        _mm256_storeu_si256((__m256i*)&result[i * simd_width * 2 + simd_width], interleaved_vec);\n    }\n\n    size_t offset = simd_chunks * simd_width * 2;\n    for (size_t i = 0; i < remainder; ++i) {\n        result[offset + i * 2] = numbers[simd_chunks * simd_width + i];\n        result[offset + i * 2 + 1] = delimeter;\n    }\n\n    result[result_size - 1] = numbers[n - 1]; // Fix the last element\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    size_t result_size = 2 * n - (n > 0 ? 1 : 0);\n    std::vector<int> result(result_size, delimeter);\n\n    if (n == 0) {\n        return result;\n    }\n\n    __m256i delimeter_vec = _mm256_set1_epi32(delimeter);\n    size_t i = 0, j = 0;\n\n    for (; i + 8 <= n; i += 8, j += 16) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        _mm256_storeu_si256((__m256i*)&result[j], input_vec);\n        _mm256_storeu_si256((__m256i*)&result[j + 8], delimeter_vec);\n    }\n\n    for (; i < n; ++i, ++j) {\n        result[j] = numbers[i];\n        if (i < n - 1) {\n            result[j + 1] = delimeter;\n            ++j;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    size_t result_size = 2 * n - (n > 0 ? 1 : 0);\n    std::vector<int> result(result_size);\n\n    __m256i delimeter_vec = _mm256_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 8 <= n; i += 8, j += 16) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n\n        __m256i interleaved_low = _mm256_unpacklo_epi32(input_vec, delimeter_vec);\n        __m256i interleaved_high = _mm256_unpackhi_epi32(input_vec, delimeter_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[j], interleaved_low);\n        _mm256_storeu_si256((__m256i*)&result[j + 8], interleaved_high);\n    }\n\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        if (j + 1 < result_size) {\n            result[j + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m256i delimeter_vec = _mm256_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n    for (; i + 8 <= n; i += 8, j += 16) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n\n        // Store input elements in even indices\n        _mm256_storeu_si256((__m256i*)&result[j], input_vec);\n\n        // Store delimiter in odd indices\n        _mm256_storeu_si256((__m256i*)&result[j + 8], delimeter_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[j++] = numbers[i];\n        if (i < n - 1) {\n            result[j++] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_width = 16; // AVX2 processes 16 int16_t elements at a time\n    size_t simd_iterations = size / simd_width;\n    size_t remainder = size % simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i * simd_width]);\n        sum_vec = _mm256_add_epi16(sum_vec, data);\n        product_vec = _mm256_mullo_epi16(product_vec, data);\n    }\n\n    // Horizontal sum for sum_vec\n    alignas(32) int16_t sum_array[simd_width];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    for (size_t i = 0; i < simd_width; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for product_vec\n    alignas(32) int16_t product_array[simd_width];\n    _mm256_store_si256((__m256i*)product_array, product_vec);\n    for (size_t i = 0; i < simd_width; ++i) {\n        product *= product_array[i];\n    }\n\n    // Process remainder elements\n    for (size_t i = simd_iterations * simd_width; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_width = 16; // AVX2 processes 16 int16_t values at a time\n    size_t aligned_size = (size / simd_width) * simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum_vec = _mm256_add_epi16(sum_vec, data);\n        product_vec = _mm256_mullo_epi16(product_vec, data);\n    }\n\n    // Horizontal sum for sum_vec\n    int16_t sum_array[simd_width];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n    for (size_t i = 0; i < simd_width; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for product_vec\n    int16_t product_array[simd_width];\n    _mm256_storeu_si256((__m256i*)product_array, product_vec);\n    for (size_t i = 0; i < simd_width; ++i) {\n        product *= product_array[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_width = 16; // AVX2 processes 16 int16_t values at a time\n    size_t aligned_size = size - (size % simd_width);\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum_vec = _mm256_add_epi16(sum_vec, data);\n        product_vec = _mm256_mullo_epi16(product_vec, data);\n    }\n\n    // Horizontal sum for sum_vec\n    int16_t sum_array[16];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n    for (int i = 0; i < 16; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for product_vec\n    int16_t product_array[16];\n    _mm256_storeu_si256((__m256i*)product_array, product_vec);\n    for (int i = 0; i < 16; ++i) {\n        product *= product_array[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_width = 16; // AVX2 processes 16 16-bit integers at a time\n    size_t aligned_size = size - (size % simd_width);\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i product_vec = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum_vec = _mm256_add_epi16(sum_vec, data);\n        product_vec = _mm256_mullo_epi16(product_vec, data);\n    }\n\n    // Horizontal sum for the sum vector\n    int16_t sum_array[16];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n    for (int i = 0; i < 16; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for the product vector\n    int16_t product_array[16];\n    _mm256_storeu_si256((__m256i*)product_array, product_vec);\n    for (int i = 0; i < 16; ++i) {\n        product *= product_array[i];\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_width = 16; // AVX2 can process 16 int16_t values at a time\n    size_t simd_iterations = size / simd_width;\n    size_t remainder = size % simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&numbers[i * simd_width]);\n        sum_vec = _mm256_add_epi16(sum_vec, data);\n        prod_vec = _mm256_mullo_epi16(prod_vec, data);\n    }\n\n    // Horizontal reduction for sum\n    int16_t sum_array[16];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_vec);\n    int64_t total_sum = 0;\n    for (int i = 0; i < 16; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Horizontal reduction for product\n    int16_t prod_array[16];\n    _mm256_storeu_si256((__m256i*)prod_array, prod_vec);\n    int64_t total_product = 1;\n    for (int i = 0; i < 16; ++i) {\n        total_product *= prod_array[i];\n    }\n\n    // Process the remainder\n    for (size_t i = simd_iterations * simd_width; i < size; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 8; // AVX2 processes 8 integers (256 bits) at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m256i max_vector = _mm256_set1_epi32(numbers[0]);\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i current_vector = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_vector = _mm256_max_epi32(max_vector, current_vector);\n        _mm256_storeu_si256((__m256i*)&result[i], max_vector);\n    }\n\n    // Handle the remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) return result;\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n    }\n\n    // Handle the remaining elements\n    int rolling_max = result[aligned_size - 1];\n    for (size_t i = aligned_size; i < n; ++i) {\n        rolling_max = std::max(rolling_max, numbers[i]);\n        result[i] = rolling_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m256i max_vector = _mm256_set1_epi32(numbers[0]);\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i current_vector = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_vector = _mm256_max_epi32(max_vector, current_vector);\n        _mm256_storeu_si256((__m256i*)&result[i], max_vector);\n    }\n\n    // Handle the remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    size_t simd_end = (n / simd_width) * simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]); // Initialize with the first element\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n    }\n\n    // Handle the remaining elements\n    int rolling_max = result[simd_end - 1];\n    for (size_t i = simd_end; i < n; ++i) {\n        rolling_max = std::max(rolling_max, numbers[i]);\n        result[i] = rolling_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    size_t simd_chunks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]); // Initialize with the first element\n    for (size_t i = 0; i < simd_chunks; ++i) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&numbers[i * simd_width]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        _mm256_storeu_si256((__m256i*)&result[i * simd_width], max_vec);\n    }\n\n    // Handle the remainder elements\n    if (remainder > 0) {\n        size_t start_idx = simd_chunks * simd_width;\n        for (size_t i = start_idx; i < n; ++i) {\n            max_vec = _mm256_set1_epi32(result[i - 1]);\n            result[i] = std::max(numbers[i], result[i - 1]);\n        }\n    }\n\n    // Propagate the rolling max for the entire array\n    for (size_t i = simd_width; i < n; ++i) {\n        result[i] = std::max(result[i], result[i - 1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have the same length\");\n    }\n\n    size_t simd_width = 32; // AVX2 processes 256 bits (32 bytes) at a time\n    size_t chunks = len / simd_width;\n    size_t remainder = len % simd_width;\n\n    std::string result(len, '0');\n\n    for (size_t i = 0; i < chunks; ++i) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i * simd_width));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i * simd_width));\n\n        __m256i vec_a_bits = _mm256_sub_epi8(vec_a, _mm256_set1_epi8('0'));\n        __m256i vec_b_bits = _mm256_sub_epi8(vec_b, _mm256_set1_epi8('0'));\n\n        __m256i vec_xor = _mm256_xor_si256(vec_a_bits, vec_b_bits);\n        __m256i vec_result = _mm256_add_epi8(vec_xor, _mm256_set1_epi8('0'));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i * simd_width), vec_result);\n    }\n\n    for (size_t i = chunks * simd_width; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must be of equal length.\");\n    }\n\n    // Pad strings to make their length a multiple of 32 (AVX2 processes 256 bits = 32 bytes at a time)\n    size_t padded_len = ((len + 31) / 32) * 32;\n    a.resize(padded_len, '0');\n    b.resize(padded_len, '0');\n\n    std::string result(padded_len, '0');\n\n    for (size_t i = 0; i < padded_len; i += 32) {\n        // Load 32 bytes from each string into AVX2 registers\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n\n        // Convert ASCII '0' and '1' to binary 0 and 1\n        __m256i zero = _mm256_set1_epi8('0');\n        vec_a = _mm256_sub_epi8(vec_a, zero);\n        vec_b = _mm256_sub_epi8(vec_b, zero);\n\n        // Perform XOR operation\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n\n        // Convert binary 0 and 1 back to ASCII '0' and '1'\n        vec_result = _mm256_add_epi8(vec_result, zero);\n\n        // Store the result back to the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), vec_result);\n    }\n\n    // Trim the result to the original length\n    result.resize(len);\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have the same length\");\n    }\n\n    // Pad strings to be a multiple of 32 for AVX processing\n    size_t padded_len = (len + 31) & ~31;\n    a.resize(padded_len, '0');\n    b.resize(padded_len, '0');\n\n    std::string result(padded_len, '0');\n\n    for (size_t i = 0; i < padded_len; i += 32) {\n        // Load 32 bytes from each string into AVX registers\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n\n        // Convert '0' and '1' characters to binary values (0 and 1)\n        __m256i zero = _mm256_set1_epi8('0');\n        vec_a = _mm256_sub_epi8(vec_a, zero);\n        vec_b = _mm256_sub_epi8(vec_b, zero);\n\n        // Perform XOR operation\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n\n        // Convert binary values back to '0' and '1' characters\n        vec_result = _mm256_add_epi8(vec_result, zero);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), vec_result);\n    }\n\n    // Resize the result to the original length\n    result.resize(len);\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length.\");\n    }\n\n    size_t simd_width = 32; // AVX2 processes 256 bits = 32 bytes at a time\n    size_t simd_blocks = len / simd_width;\n    size_t remainder = len % simd_width;\n\n    std::string result(len, '0');\n\n    const char *a_ptr = a.data();\n    const char *b_ptr = b.data();\n    char *res_ptr = result.data();\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a_ptr + i * simd_width));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b_ptr + i * simd_width));\n\n        __m256i vec_a_bin = _mm256_sub_epi8(vec_a, _mm256_set1_epi8('0'));\n        __m256i vec_b_bin = _mm256_sub_epi8(vec_b, _mm256_set1_epi8('0'));\n\n        __m256i vec_xor = _mm256_xor_si256(vec_a_bin, vec_b_bin);\n        __m256i vec_result = _mm256_add_epi8(vec_xor, _mm256_set1_epi8('0'));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(res_ptr + i * simd_width), vec_result);\n    }\n\n    for (size_t i = simd_blocks * simd_width; i < len; ++i) {\n        res_ptr[i] = ((a_ptr[i] - '0') ^ (b_ptr[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length\");\n    }\n\n    std::string result(len, '0');\n\n    size_t simd_width = 32; // AVX2 processes 256 bits = 32 bytes at a time\n    size_t simd_chunks = len / simd_width;\n    size_t remainder = len % simd_width;\n\n    for (size_t i = 0; i < simd_chunks; ++i) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i * simd_width));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i * simd_width));\n\n        __m256i vec_a_bits = _mm256_sub_epi8(vec_a, _mm256_set1_epi8('0'));\n        __m256i vec_b_bits = _mm256_sub_epi8(vec_b, _mm256_set1_epi8('0'));\n\n        __m256i vec_xor = _mm256_xor_si256(vec_a_bits, vec_b_bits);\n        __m256i vec_result = _mm256_add_epi8(vec_xor, _mm256_set1_epi8('0'));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i * simd_width), vec_result);\n    }\n\n    for (size_t i = simd_chunks * simd_width; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"\"; // Return empty string for empty input\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n\n    // Compute lengths of all strings\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].size();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    // Use AVX2 intrinsics to find the maximum length\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_iterations = n / simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(0);\n    __m256i max_idx_vec = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i length_vec = _mm256_loadu_si256((__m256i*)&lengths[i * simd_width]);\n        __m256i index_vec = _mm256_setr_epi32(\n            i * simd_width, i * simd_width + 1, i * simd_width + 2, i * simd_width + 3,\n            i * simd_width + 4, i * simd_width + 5, i * simd_width + 6, i * simd_width + 7);\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(length_vec, max_vec);\n        max_vec = _mm256_blendv_epi8(max_vec, length_vec, cmp_mask);\n        max_idx_vec = _mm256_blendv_epi8(max_idx_vec, index_vec, cmp_mask);\n    }\n\n    // Extract max length and index from SIMD registers\n    int max_lengths[simd_width];\n    int max_indices[simd_width];\n    _mm256_storeu_si256((__m256i*)max_lengths, max_vec);\n    _mm256_storeu_si256((__m256i*)max_indices, max_idx_vec);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_lengths[i] > max_length) {\n            max_length = max_lengths[i];\n            max_index = max_indices[i];\n        }\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_iterations * simd_width; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n\n    // Compute lengths of strings\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].size();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    // Process lengths using AVX2 intrinsics\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_iterations = n / simd_width;\n\n    __m256i max_vec = _mm256_setzero_si256();\n    __m256i max_index_vec = _mm256_setzero_si256();\n    __m256i index_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i length_vec = _mm256_loadu_si256((__m256i *)&lengths[i * simd_width]);\n        __m256i mask = _mm256_cmpgt_epi32(length_vec, max_vec);\n\n        max_vec = _mm256_blendv_epi8(max_vec, length_vec, mask);\n        max_index_vec = _mm256_blendv_epi8(max_index_vec, index_vec, mask);\n\n        index_vec = _mm256_add_epi32(index_vec, _mm256_set1_epi32(simd_width));\n    }\n\n    // Extract max length and index from SIMD registers\n    alignas(32) int max_array[8];\n    alignas(32) int max_index_array[8];\n    _mm256_store_si256((__m256i *)max_array, max_vec);\n    _mm256_store_si256((__m256i *)max_index_array, max_index_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        if (max_array[i] > max_length) {\n            max_length = max_array[i];\n            max_index = max_index_array[i];\n        }\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_iterations * simd_width; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].size();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_iterations = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(0);\n    __m256i index_vec = _mm256_set1_epi32(0);\n    __m256i current_indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i length_vec = _mm256_loadu_si256((__m256i *)&lengths[i * simd_width]);\n        __m256i mask = _mm256_cmpgt_epi32(length_vec, max_vec);\n        max_vec = _mm256_blendv_epi8(max_vec, length_vec, mask);\n        index_vec = _mm256_blendv_epi8(index_vec, current_indices, mask);\n        current_indices = _mm256_add_epi32(current_indices, _mm256_set1_epi32(simd_width));\n    }\n\n    int max_values[simd_width];\n    int max_indices[simd_width];\n    _mm256_storeu_si256((__m256i *)max_values, max_vec);\n    _mm256_storeu_si256((__m256i *)max_indices, index_vec);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_values[i] > max_length) {\n            max_length = max_values[i];\n            max_index = max_indices[i];\n        }\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n\n    // Compute lengths of all strings\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].length();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    // Use AVX2 intrinsics to find the maximum length and its index\n    size_t i = 0;\n    __m256i max_vec = _mm256_set1_epi32(0);\n    __m256i max_idx_vec = _mm256_set1_epi32(0);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i len_vec = _mm256_loadu_si256((__m256i *)&lengths[i]);\n        __m256i idx_vec = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(len_vec, max_vec);\n        max_vec = _mm256_blendv_epi8(max_vec, len_vec, cmp_mask);\n        max_idx_vec = _mm256_blendv_epi8(max_idx_vec, idx_vec, cmp_mask);\n    }\n\n    // Extract max length and index from the vector\n    int max_lengths[8];\n    int max_indices[8];\n    _mm256_storeu_si256((__m256i *)max_lengths, max_vec);\n    _mm256_storeu_si256((__m256i *)max_indices, max_idx_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (max_lengths[j] > max_length) {\n            max_length = max_lengths[j];\n            max_index = max_indices[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].length();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_iterations = n / simd_width;\n\n    __m256i max_vec = _mm256_set1_epi32(0);\n    __m256i max_index_vec = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m256i length_vec = _mm256_loadu_si256((__m256i *)&lengths[i * simd_width]);\n        __m256i index_vec = _mm256_setr_epi32(\n            i * simd_width, i * simd_width + 1, i * simd_width + 2, i * simd_width + 3,\n            i * simd_width + 4, i * simd_width + 5, i * simd_width + 6, i * simd_width + 7);\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(length_vec, max_vec);\n        max_vec = _mm256_blendv_epi8(max_vec, length_vec, cmp_mask);\n        max_index_vec = _mm256_blendv_epi8(max_index_vec, index_vec, cmp_mask);\n    }\n\n    int max_lengths[simd_width];\n    int max_indices[simd_width];\n    _mm256_storeu_si256((__m256i *)max_lengths, max_vec);\n    _mm256_storeu_si256((__m256i *)max_indices, max_index_vec);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_lengths[i] > max_length) {\n            max_length = max_lengths[i];\n            max_index = max_indices[i];\n        }\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testz_si256(cmp, cmp)) {\n            break;\n        }\n\n        __m256i mod = _mm256_rem_epi64(va, vb);\n        va = vb;\n        vb = mod;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testc_si256(cmp, _mm256_set1_epi64x(-1))) {\n            __m256i result = va;\n            return _mm256_extract_epi64(result, 0);\n        }\n\n        __m256i rem = _mm256_rem_epi64(va, vb);\n        va = vb;\n        vb = rem;\n    }\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testc_si256(cmp, _mm256_set1_epi64x(-1))) {\n            __m256i result = va;\n            return _mm256_extract_epi64(result, 0);\n        }\n\n        __m256i mod = _mm256_rem_epi64(va, vb);\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testz_si256(cmp, cmp)) {\n            break;\n        }\n\n        __m256i rem = _mm256_rem_epi64(va, vb);\n        va = vb;\n        vb = rem;\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, va);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testz_si256(cmp, cmp)) {\n            break;\n        }\n\n        __m256i rem = _mm256_rem_epi64(va, vb);\n        va = vb;\n        vb = rem;\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, va);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty vector if there are less than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i++) {\n        __m256 current = _mm256_set1_ps(numbers[i]); // Broadcast the current number\n        size_t j = i + 1;\n\n        for (; j + 8 <= n; j += 8) {\n            __m256 other = _mm256_loadu_ps(&numbers[j]); // Load 8 numbers\n            __m256 diff = _mm256_sub_ps(current, other); // Compute differences\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Absolute differences\n\n            __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n            __m256 cmp_mask = _mm256_cmp_ps(abs_diff, min_diff_vec, _CMP_LT_OQ); // Compare with current min_diff\n            int mask = _mm256_movemask_ps(cmp_mask);\n\n            if (mask != 0) {\n                for (int k = 0; k < 8; ++k) {\n                    if (mask & (1 << k)) {\n                        float diff_val = std::fabs(numbers[i] - numbers[j + k]);\n                        if (diff_val < min_diff) {\n                            min_diff = diff_val;\n                            closest_a = numbers[i];\n                            closest_b = numbers[j + k];\n                        }\n                    }\n                }\n            }\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            float diff_val = std::fabs(numbers[i] - numbers[j]);\n            if (diff_val < min_diff) {\n                min_diff = diff_val;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty vector if there are less than 2 elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec_i = _mm256_loadu_ps(&numbers[i]); // Load 8 floats from numbers[i]\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec_j = _mm256_loadu_ps(&numbers[j]); // Load 8 floats from numbers[j]\n\n            __m256 diff = _mm256_sub_ps(vec_i, vec_j);  // Compute differences\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Absolute differences\n\n            alignas(32) float diffs[8];\n            _mm256_store_ps(diffs, abs_diff); // Store the absolute differences\n\n            for (size_t k = 0; k < 8; ++k) {\n                if (i + k < n && j + k < n && i + k != j + k) {\n                    if (diffs[k] < min_diff) {\n                        min_diff = diffs[k];\n                        closest_a = numbers[i + k];\n                        closest_b = numbers[j + k];\n                    }\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty vector if there are fewer than 2 elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec_i = _mm256_loadu_ps(&numbers[i]); // Load 8 floats from numbers[i]\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 vec_j = _mm256_loadu_ps(&numbers[j]); // Load 8 floats from numbers[j]\n            __m256 diff = _mm256_sub_ps(vec_i, vec_j);  // Compute differences\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Absolute value of differences\n\n            // Find the minimum difference in the current 8 elements\n            __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff_vec, _CMP_LT_OQ);\n            __m256 masked_diff = _mm256_blendv_ps(min_diff_vec, abs_diff, mask);\n\n            // Extract the minimum difference and corresponding elements\n            float temp[8];\n            _mm256_storeu_ps(temp, masked_diff);\n            for (int k = 0; k < 8; ++k) {\n                if (temp[k] < min_diff) {\n                    min_diff = temp[k];\n                    closest_a = numbers[i + k];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty if there are less than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 current = _mm256_loadu_ps(&numbers[i]); // Load 8 floats into a SIMD register\n\n        for (size_t j = i + 1; j < n; j += 8) {\n            __m256 other = _mm256_loadu_ps(&numbers[j]); // Load another 8 floats into a SIMD register\n\n            __m256 diff = _mm256_sub_ps(current, other);\n            __m256 abs_diff = _mm256.abs_ps(diff);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty if there are less than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec_i = _mm256_loadu_ps(&numbers[i]); // Load 8 floats from numbers[i]\n\n        for (size_t j = 0; j < n; j++) {\n            __m256 vec_j = _mm256_set1_ps(numbers[j]); // Broadcast numbers[j] to all lanes\n            __m256 diff = _mm256_sub_ps(vec_i, vec_j); // Compute differences\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Absolute value\n\n            // Find the minimum difference in the 8 lanes\n            __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff_vec, _CMP_LT_OQ);\n            if (_mm256_movemask_ps(mask)) {\n                float temp[8];\n                _mm256_storeu_ps(temp, abs_diff);\n                for (int k = 0; k < 8; ++k) {\n                    if (temp[k] < min_diff && i + k < n && i + k != j) {\n                        min_diff = temp[k];\n                        closest_a = numbers[i + k];\n                        closest_b = numbers[j];\n                    }\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t size = numbers.size();\n    if (size < 2) return numbers;\n\n    // Find the minimum and maximum values in the vector\n    float min_val = numbers[0];\n    float max_val = numbers[0];\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_min_ps(vec, _mm256_set1_ps(min_val));\n        __m256 max_vec = _mm256_max_ps(vec, _mm256_set1_ps(max_val));\n        min_val = std::min(min_val, _mm256_cvtss_f32(min_vec));\n        max_val = std::max(max_val, _mm256_cvtss_f32(max_vec));\n    }\n\n    // Compute the range\n    float range = max_val - min_val;\n    if (range == 0.0f) return std::vector<float>(size, 0.0f);\n\n    // Rescale the values to [0, 1]\n    std::vector<float> result(size);\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 range_vec = _mm256_set1_ps(range);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 rescaled = _mm256_div_ps(_mm256_sub_ps(vec, min_vec), range_vec);\n        _mm256_storeu_ps(&result[i], rescaled);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find min and max using SIMD\n    __m256 min_vec = _mm256_set1_ps(FLT_MAX);\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        min_vec = _mm256_min_ps(min_vec, data);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n\n    // Reduce min and max vectors\n    float min_vals[simd_width], max_vals[simd_width];\n    _mm256_storeu_ps(min_vals, min_vec);\n    _mm256_storeu_ps(max_vals, max_vec);\n\n    float min_val = *std::min_element(min_vals, min_vals + simd_width);\n    float max_val = *std::max_element(max_vals, max_vals + simd_width);\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    // Compute scale and offset\n    float range = max_val - min_val;\n    if (range == 0.0f) return std::vector<float>(n, 0.0f); // Avoid division by zero\n\n    __m256 min_vec_broadcast = _mm256_set1_ps(min_val);\n    __m256 range_vec_broadcast = _mm256_set1_ps(range);\n\n    std::vector<float> result(n);\n    i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_sub_ps(data, min_vec_broadcast);\n        normalized = _mm256_div_ps(normalized, range_vec_broadcast);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find the minimum and maximum values in the vector\n    __m256 min_val = _mm256_set1_ps(FLT_MAX);\n    __m256 max_val = _mm256_set1_ps(-FLT_MAX);\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    // Reduce min and max values across the lanes\n    float min_array[simd_width], max_array[simd_width];\n    _mm256_storeu_ps(min_array, min_val);\n    _mm256_storeu_ps(max_array, max_val);\n\n    float min_scalar = *std::min_element(min_array, min_array + simd_width);\n    float max_scalar = *std::max_element(max_array, max_array + simd_width);\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        min_scalar = std::min(min_scalar, numbers[i]);\n        max_scalar = std::max(max_scalar, numbers[i]);\n    }\n\n    // Compute the scale and offset\n    float range = max_scalar - min_scalar;\n    if (range == 0.0f) return std::vector<float>(n, 0.0f); // Avoid division by zero\n\n    float scale = 1.0f / range;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n    __m256 min_vec = _mm256_set1_ps(min_scalar);\n\n    std::vector<float> result(n);\n    i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 rescaled = _mm256_mul_ps(_mm256_sub_ps(vec, min_vec), scale_vec);\n        _mm256_storeu_ps(&result[i], rescaled);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_scalar) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find the minimum and maximum values using SIMD\n    __m256 min_vec = _mm256_set1_ps(FLT_MAX);\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        min_vec = _mm256_min_ps(min_vec, data);\n        max_vec = _mm256_max_ps(max_vec, data);\n    }\n\n    // Reduce min and max vectors to single values\n    float min_val = FLT_MAX;\n    float max_val = -FLT_MAX;\n    alignas(32) float min_arr[simd_width], max_arr[simd_width];\n    _mm256_store_ps(min_arr, min_vec);\n    _mm256_store_ps(max_arr, max_vec);\n\n    for (size_t j = 0; j < simd_width; ++j) {\n        min_val = std::min(min_val, min_arr[j]);\n        max_val = std::max(max_val, max_arr[j]);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    // Compute scale and offset\n    float range = max_val - min_val;\n    if (range == 0.0f) return std::vector<float>(n, 0.0f); // Avoid division by zero\n\n    __m256 range_vec = _mm256_set1_ps(range);\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n\n    // Rescale the numbers using SIMD\n    std::vector<float> result(n);\n    i = 0;\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 data = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_sub_ps(data, min_val_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find the minimum and maximum values in the vector\n    __m256 min_val = _mm256_set1_ps(FLT_MAX);\n    __m256 max_val = _mm256_set1_ps(-FLT_MAX);\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    // Reduce min and max values across the SIMD lanes\n    float min_array[simd_width], max_array[simd_width];\n    _mm256_storeu_ps(min_array, min_val);\n    _mm256_storeu_ps(max_array, max_val);\n\n    float min_scalar = *std::min_element(min_array, min_array + simd_width);\n    float max_scalar = *std::max_element(max_array, max_array + simd_width);\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        min_scalar = std::min(min_scalar, numbers[i]);\n        max_scalar = std::max(max_scalar, numbers[i]);\n    }\n\n    // Compute the range\n    float range = max_scalar - min_scalar;\n    if (range == 0.0f) return std::vector<float>(n, 0.0f);\n\n    __m256 min_vec = _mm256_set1_ps(min_scalar);\n    __m256 range_vec = _mm256_set1_ps(range);\n\n    std::vector<float> result(n);\n    i = 0;\n\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(vec, min_vec), range_vec);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_scalar) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t result = 1;\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    uint64_t remainder[4] = {0, 0, 0, 0};\n    uint64_t divisors[4] = {0, 0, 0, 0};\n\n    for (uint64_t i = sqrt_n; i > 0; i -= 4) {\n        __m256i v_i = _mm256_set_epi64x(i, i - 1, i - 2, i - 3);\n        __m256i v_n = _mm256_set1_epi64x(n);\n        __m256i v_remainder = _mm256_rem_epi64(v_n, v_i);\n\n        _mm256_storeu_si256((__m256i*)remainder, v_remainder);\n        _mm256_storeu_si256((__m256i*)divisors, v_i);\n\n        for (int j = 0; j < 4; ++j) {\n            if (remainder[j] == 0 && divisors[j] < n) {\n                return divisors[j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t max_divisor = 0;\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n\n    __m256i divisor_vec = _mm256_set_epi64x(sqrt_n - 3, sqrt_n - 2, sqrt_n - 1, sqrt_n);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    while (sqrt_n > 0) {\n        __m256i rem_vec = _mm256_rem_epi64(n_vec, divisor_vec);\n        __m256i cmp_vec = _mm256_cmpeq_epi64(rem_vec, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        if (mask) {\n            for (int i = 3; i >= 0; --i) {\n                if ((mask >> (i * 8)) & 1) {\n                    max_divisor = _mm256_extract_epi64(divisor_vec, i);\n                    return max_divisor;\n                }\n            }\n        }\n\n        divisor_vec = _mm256_sub_epi64(divisor_vec, _mm256_set1_epi64x(4));\n        sqrt_n -= 4;\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t limit = n / 2;\n    __m256i divisor_vec = _mm256_set_epi64x(limit - 3, limit - 2, limit - 1, limit);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    while (limit > 0) {\n        __m256i rem_vec = _mm256_rem_epi64(n_vec, divisor_vec);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp_vec = _mm256_cmpeq_epi64(rem_vec, zero_vec);\n\n        int mask = _mm256_movemask_epi8(cmp_vec);\n        if (mask) {\n            for (int i = 3; i >= 0; --i) {\n                if ((mask >> (i * 8)) & 1) {\n                    return _mm256_extract_epi64(divisor_vec, i);\n                }\n            }\n        }\n\n        limit -= 4;\n        divisor_vec = _mm256_set_epi64x(limit - 3, limit - 2, limit - 1, limit);\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t limit = n / 2;\n    uint64_t result = 1;\n\n    __m256i divisor_vec = _mm256_set_epi64x(limit - 3, limit - 2, limit - 1, limit);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    while (limit > 0) {\n        __m256i rem_vec = _mm256_rem_epi64(n_vec, divisor_vec);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp_vec = _mm256_cmpeq_epi64(rem_vec, zero_vec);\n\n        int mask = _mm256_movemask_epi8(cmp_vec);\n        if (mask) {\n            for (int i = 3; i >= 0; i--) {\n                if ((mask >> (i * 8)) & 1) {\n                    result = _mm256_extract_epi64(divisor_vec, i);\n                    return result;\n                }\n            }\n        }\n\n        divisor_vec = _mm256_sub_epi64(divisor_vec, _mm256_set1_epi64x(4));\n        limit -= 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t result = 1;\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    uint64_t i;\n\n    __m256i divisor_vec, n_vec, zero_vec, cmp_mask;\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i step_vec = _mm256_set1_epi64x(4);\n    n_vec = _mm256_set1_epi64x(n);\n    zero_vec = _mm256_setzero_si256();\n\n    for (i = sqrt_n; i > 0; i -= 4) {\n        divisor_vec = _mm256_set_epi64x(i, i - 1, i - 2, i - 3);\n        __m256i rem_vec = _mm256_rem_epi64(n_vec, divisor_vec);\n        cmp_mask = _mm256_cmpeq_epi64(rem_vec, zero_vec);\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(cmp_mask));\n\n        if (mask) {\n            for (int j = 3; j >= 0; --j) {\n                if (mask & (1 << j)) {\n                    result = i - (3 - j);\n                    return result;\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) return factors;\n\n    // Initialize an array of potential divisors\n    const int max_divisor = 100; // Arbitrary limit for divisors\n    int divisors[max_divisor];\n    for (int i = 0; i < max_divisor; ++i) {\n        divisors[i] = i + 2; // Start from 2\n    }\n\n    // Load divisors into an AVX2 register\n    __m256i divisor_vec = _mm256_loadu_si256((__m256i*)divisors);\n\n    while (n > 1) {\n        // Broadcast the current number `n` into an AVX2 register\n        __m256i n_vec = _mm256_set1_epi32(n);\n\n        // Perform modulo operation in parallel\n        __m256i mod_vec = _mm256_rem_epi32(n_vec, divisor_vec);\n\n        // Compare results to zero (check divisibility)\n        __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n\n        // Extract the mask of divisible numbers\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        // Check divisors in the mask\n        for (int i = 0; i < max_divisor; ++i) {\n            if (mask & (1 << (i * 4))) { // Check divisibility\n                while (n % divisors[i] == 0) {\n                    factors.push_back(divisors[i]);\n                    n /= divisors[i];\n                }\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize a vector of potential divisors\n    const int max_divisor = 100; // Arbitrary upper limit for divisors\n    alignas(32) int divisors[max_divisor];\n    for (int i = 0; i < max_divisor; ++i) {\n        divisors[i] = i + 2; // Start from 2\n    }\n\n    __m256i vec_divisors = _mm256_load_si256((__m256i*)divisors);\n\n    while (n > 1) {\n        alignas(32) int results[8];\n        bool found = false;\n\n        for (int i = 0; i < max_divisor; i += 8) {\n            vec_divisors = _mm256_load_si256((__m256i*)(divisors + i));\n            __m256i vec_n = _mm256_set1_epi32(n);\n\n            // Perform modulo operation\n            __m256i vec_mod = _mm256_rem_epi32(vec_n, vec_divisors);\n\n            // Compare results to zero\n            __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, _mm256_setzero_si256());\n\n            // Store comparison results\n            _mm256_store_si256((__m256i*)results, vec_cmp);\n\n            for (int j = 0; j < 8; ++j) {\n                if (results[j]) {\n                    int divisor = divisors[i + j];\n                    factors.push_back(divisor);\n                    n /= divisor;\n                    found = true;\n                    break;\n                }\n            }\n\n            if (found) break;\n        }\n\n        if (!found) {\n            factors.push_back(n);\n            break;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize a vector of potential divisors\n    const int max_divisor = 100; // Arbitrary upper limit for divisors\n    alignas(32) int divisors[max_divisor];\n    for (int i = 2; i <= max_divisor; ++i) {\n        divisors[i - 2] = i;\n    }\n\n    // Load divisors into an AVX vector\n    __m256i divisor_vec = _mm256_load_si256((__m256i*)divisors);\n\n    while (n > 1) {\n        // Broadcast the current number n to all lanes\n        __m256i n_vec = _mm256_set1_epi32(n);\n\n        // Perform modulo operation in parallel\n        __m256i mod_vec = _mm256_rem_epi32(n_vec, divisor_vec);\n\n        // Compare results to zero (mod == 0)\n        __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n\n        // Extract mask of matching lanes\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        // Check for divisors\n        for (int i = 0; i < max_divisor; ++i) {\n            if (mask & (1 << (i * 4))) { // Check if divisor is valid\n                int divisor = divisors[i];\n                factors.push_back(divisor);\n                n /= divisor;\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize an array of possible divisors\n    const int max_divisor = 100; // Arbitrary upper limit for divisors\n    alignas(32) int divisors[max_divisor];\n    for (int i = 2; i < max_divisor; ++i) {\n        divisors[i - 2] = i;\n    }\n\n    // Load divisors into AVX vector\n    __m256i divisor_vec = _mm256_load_si256((__m256i*)divisors);\n\n    while (n > 1) {\n        alignas(32) int results[8];\n        alignas(32) int masks[8];\n\n        // Broadcast the current number `n` into an AVX vector\n        __m256i n_vec = _mm256_set1_epi32(n);\n\n        // Perform modulo operation in parallel\n        __m256i mod_vec = _mm256_rem_epi32(n_vec, divisor_vec);\n\n        // Compare results to zero (check divisibility)\n        __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n\n        // Store comparison results into masks array\n        _mm256_store_si256((__m256i*)masks, cmp_vec);\n\n        // Find the smallest divisor\n        for (int i = 0; i < 8; ++i) {\n            if (masks[i]) {\n                int divisor = divisors[i];\n                factors.push_back(divisor);\n                n /= divisor;\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize an array of potential divisors\n    const int max_divisor = 1000; // Arbitrary limit for divisors\n    alignas(32) int divisors[max_divisor];\n    for (int i = 2; i < max_divisor; ++i) {\n        divisors[i - 2] = i;\n    }\n\n    // Load divisors into SIMD registers\n    __m256i divisor_vec;\n    for (int i = 0; i < max_divisor - 2; i += 8) {\n        divisor_vec = _mm256_load_si256((__m256i*)&divisors[i]);\n\n        while (true) {\n            // Broadcast the current number `n` across a SIMD register\n            __m256i n_vec = _mm256_set1_epi32(n);\n\n            // Perform modulo operation: n % divisors[i:i+7]\n            __m256i mod_vec = _mm256_rem_epi32(n_vec, divisor_vec);\n\n            // Compare results to zero (check divisibility)\n            __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n\n            // Extract mask of divisible elements\n            int mask = _mm256_movemask_epi8(cmp_vec);\n\n            // Process divisible elements\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (1 << (j * 4))) { // Check if divisible\n                    int divisor = divisors[i + j];\n                    factors.push_back(divisor);\n                    n /= divisor; // Reduce n\n                    break; // Restart loop for current divisor_vec\n                }\n            }\n\n            // If no divisors found, break out of the loop\n            if (mask == 0) break;\n        }\n\n        // If n becomes 1, factorization is complete\n        if (n == 1) break;\n    }\n\n    // If n is still greater than 1, it is a prime number\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    result.reserve(size);\n\n    // Process 8 floats at a time using AVX\n    __m256 zero = _mm256_setzero_ps();\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(data, zero, _CMP_GT_OQ);\n        int bitmask = _mm256_movemask_ps(mask);\n\n        for (int j = 0; j < 8; ++j) {\n            if (bitmask & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Reserve space for the result vector to avoid frequent reallocations\n    result.reserve(size);\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]); // Load 8 floats into an AVX register\n        __m256 zero = _mm256_setzero_ps();  // Create a zero vector\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ); // Compare elements with zero (greater than)\n        int bitmask = _mm256_movemask_ps(mask); // Extract the mask as a bitmask\n\n        for (int j = 0; j < simd_width; ++j) {\n            if (bitmask & (1 << j)) {\n                result.push_back(l[i + j]); // Add positive elements to the result\n            }\n        }\n    }\n\n    // Process remaining elements that don't fit into a full SIMD register\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    result.reserve(size);\n\n    __m256 zero = _mm256_setzero_ps(); // AVX register with all zeros\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&l[i]); // Load 8 floats from the input vector\n        __m256 mask = _mm256_cmp_ps(data, zero, _CMP_GT_OQ); // Compare if data > 0\n        int mask_bits = _mm256_movemask_ps(mask); // Get the mask as an integer\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask_bits & (1 << j)) { // Check if the j-th element is positive\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Reserve space for the result vector to avoid frequent reallocations\n    result.reserve(size);\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n\n        // Compare each element with 0.0f to create a mask\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_set1_ps(0.0f), _CMP_GT_OQ);\n\n        // Extract the mask as a bitfield\n        int mask_bits = _mm256_movemask_ps(mask);\n\n        // Store positive elements in the result vector\n        for (int j = 0; j < simd_width; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    // Process remaining elements that don't fit into a full SIMD register\n    for (; i < size; ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Reserve space for the result vector to avoid repeated allocations\n    result.reserve(size);\n\n    // Process elements in chunks of 8 using AVX\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256 data = _mm256_loadu_ps(&l[i]);\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask = _mm256_cmp_ps(data, zero, _CMP_GT_OQ); // Compare elements > 0\n        __m256 positive = _mm256_and_ps(data, mask);         // Mask out non-positive values\n\n        float temp[simd_width];\n        _mm256_storeu_ps(temp, positive);\n\n        for (size_t j = 0; j < simd_width; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t step = 8; // Number of elements processed in parallel\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n\n    for (int64_t i = 5; i <= limit; i += step) {\n        __m256i vec_i = _mm256_set_epi64x(i + 7, i + 5, i + 3, i + 1);\n        __m256i vec_mod = _mm256_rem_epi64(vec_n, vec_i);\n\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n\n    while (i <= limit) {\n        __m256i vec_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i vec_rem = _mm256_rem_epi64(vec_n, vec_i);\n\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vec_rem, vec_zero);\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n\n    while (i <= limit) {\n        __m256i vec_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i vec_rem = _mm256_rem_epi64(vec_n, vec_i);\n\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vec_rem, vec_zero);\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t step = 8; // Number of integers to test simultaneously\n    __m256i vec_n = _mm256_set1_epi64x(n);\n\n    for (int64_t i = 5; i <= limit; i += step) {\n        __m256i vec_i = _mm256_set_epi64x(i + 7, i + 5, i + 3, i + 1);\n        __m256i vec_rem = _mm256_rem_epi64(vec_n, vec_i);\n\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i cmp_result = _mm256_cmpeq_epi64(vec_rem, vec_zero);\n\n        if (_mm256_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n < 2) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t step = 8; // Number of integers processed in parallel\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    for (int64_t i = 5; i <= limit; i += step) {\n        __m256i vec_i = _mm256_set_epi64x(i + 7, i + 5, i + 3, i + 1);\n        __m256i vec_rem = _mm256_rem_epi64(vec_n, vec_i);\n\n        __m256i vec_zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi64(vec_rem, vec_zero);\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    for (size_t i = 0; i < n; i += 32) {\n        __m256i chunk = _mm256_setzero_si256();\n        size_t chunk_size = std::min(size_t(32), n - i);\n\n        // Load characters into SIMD register\n        for (size_t j = 0; j < chunk_size; ++j) {\n            chunk = _mm256_insert_epi8(chunk, s[i + j], j);\n        }\n\n        // Check for letters and reverse case\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n                                            _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('z' + 1)));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                            _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('Z' + 1)));\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        has_letters |= _mm256_movemask_epi8(is_letter) != 0;\n\n        __m256i flipped_case = _mm256_xor_si256(chunk, _mm256_and_si256(is_letter, _mm256_set1_epi8(0x20)));\n\n        // Store results back into the result string\n        for (size_t j = 0; j < chunk_size; ++j) {\n            result[i + j] = _mm256_extract_epi8(flipped_case, j);\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    __m256i lower_a = _mm256_set1_epi8('a');\n    __m256i lower_z = _mm256_set1_epi8('z');\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_z = _mm256_set1_epi8('Z');\n    __m256i case_mask = _mm256_set1_epi8(0x20);\n\n    size_t i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)(s.data() + i));\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, lower_a - _mm256_set1_epi8(1)),\n            _mm256_cmpgt_epi8(lower_z + _mm256_set1_epi8(1), chunk)\n        );\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, upper_a - _mm256_set1_epi8(1)),\n            _mm256_cmpgt_epi8(upper_z + _mm256_set1_epi8(1), chunk)\n        );\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        has_letters |= _mm256_movemask_epi8(is_letter) != 0;\n\n        __m256i toggled_case = _mm256_xor_si256(chunk, _mm256_and_si256(case_mask, is_letter));\n        _mm256_storeu_si256((__m256i *)(result.data() + i), toggled_case);\n    }\n\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            result[i] = c ^ 0x20;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string &s) {\n    size_t n = s.size();\n    std::string result = s;\n\n    // Check if the string contains any letters\n    bool has_letters = false;\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (!has_letters) {\n        // Reverse the string if no letters are present\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string using AVX2 intrinsics\n    size_t i = 0;\n    __m256i lower_a = _mm256_set1_epi8('a');\n    __m256i lower_z = _mm256_set1_epi8('z');\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_z = _mm256_set1_epi8('Z');\n    __m256i case_flip_mask = _mm256_set1_epi8(0x20);\n\n    for (; i + 31 < n; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)&s[i]);\n\n        // Check if characters are lowercase letters\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_a),\n            _mm256_cmpgt_epi8(lower_z, chars)\n        );\n\n        // Check if characters are uppercase letters\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, upper_a),\n            _mm256_cmpgt_epi8(upper_z, chars)\n        );\n\n        // Combine masks for letters\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n\n        // Flip case for letters\n        __m256i flipped = _mm256_xor_si256(chars, case_flip_mask);\n\n        // Select flipped or original characters\n        __m256i result_chars = _mm256_blendv_epi8(chars, flipped, is_letter);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i *)&result[i], result_chars);\n    }\n\n    // Handle remaining characters\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - 'a' + 'A';\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c - 'A' + 'a';\n        } else {\n            result[i] = c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::vector<char> result(n);\n\n    // Load the string into a vector of characters\n    const char* input = s.data();\n    char* output = result.data();\n\n    // Check if the string contains any letters\n    bool contains_letters = false;\n    for (size_t i = 0; i < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(input + i));\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n                                            _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk));\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        if (_mm256_movemask_epi8(is_letter)) {\n            contains_letters = true;\n            break;\n        }\n    }\n\n    if (contains_letters) {\n        for (size_t i = 0; i < n; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(input + i));\n            __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk));\n            __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk));\n            __m256i flip_case_lower = _mm256_sub_epi8(chunk, _mm256_set1_epi8('a' - 'A'));\n            __m256i flip_case_upper = _mm256_add_epi8(chunk, _mm256_set1_epi8('a' - 'A'));\n            __m256i result_chunk = _mm256_blendv_epi8(_mm256_blendv_epi8(chunk, flip_case_lower, is_lower),\n                                                      flip_case_upper, is_upper);\n            _mm256_storeu_si256((__m256i*)(output + i), result_chunk);\n        }\n    } else {\n        for (size_t i = 0; i < n; i += 32) {\n            size_t remaining = std::min(size_t(32), n - i);\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(input + n - i - remaining));\n            _mm256_storeu_si256((__m256i*)(output + i), chunk);\n        }\n    }\n\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20);\n\n    size_t i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(s.data() + i));\n\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_a),\n            _mm256_cmpgt_epi8(lower_z, chars)\n        );\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, upper_a),\n            _mm256_cmpgt_epi8(upper_z, chars)\n        );\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n\n        has_letters |= _mm256_movemask_epi8(is_letter) != 0;\n\n        __m256i toggled_case = _mm256_xor_si256(chars, _mm256_and_si256(is_letter, case_mask));\n        _mm256_storeu_si256((__m256i *)(result.data() + i), toggled_case);\n    }\n\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (std::isalpha(c)) {\n            has_letters = true;\n            result[i] = (c ^ 0x20);\n        } else {\n            result[i] = c;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Extract indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the values at indices divisible by 3\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace values in the original vector at indices divisible by 3\n    size_t sorted_idx = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_idx++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Extract indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the divisible_by_three vector\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace indices divisible by 3 in the result vector\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Collect indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the divisible_by_three values\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the values in the original vector at indices divisible by 3\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Extract indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the divisible_by_three vector\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace indices divisible by 3 in the result vector\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Collect indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the values at indices divisible by 3\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the sorted values back into the result vector\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t size = l.size();\n    if (size == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n\n    size_t i = 0;\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    // Extract the maximum value from the SIMD register\n    alignas(32) float max_values[simd_width];\n    _mm256_store_ps(max_values, max_vec);\n\n    float max_value = std::numeric_limits<float>::lowest();\n    for (size_t j = 0; j < simd_width; ++j) {\n        if (max_values[j] > max_value) {\n            max_value = max_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > max_value) {\n            max_value = l[i];\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_size = n - (n % simd_width);\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_array[simd_width];\n    _mm256_storeu_ps(max_array, max_vec);\n\n    float max_value = std::numeric_limits<float>::lowest();\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_array[i] > max_value) {\n            max_value = max_array[i];\n        }\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (l[i] > max_value) {\n            max_value = l[i];\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_size = n - (n % simd_width);\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_array[simd_width];\n    _mm256_storeu_ps(max_array, max_vec);\n\n    float max_value = std::numeric_limits<float>::lowest();\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_array[i] > max_value) {\n            max_value = max_array[i];\n        }\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (l[i] > max_value) {\n            max_value = l[i];\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_size = n - (n % simd_width);\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_array[simd_width];\n    _mm256_storeu_ps(max_array, max_vec);\n\n    float max_value = std::numeric_limits<float>::lowest();\n    for (size_t i = 0; i < simd_width; ++i) {\n        if (max_array[i] > max_value) {\n            max_value = max_array[i];\n        }\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (l[i] > max_value) {\n            max_value = l[i];\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n\n    size_t i = 0;\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256 data_vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, data_vec);\n    }\n\n    // Extract maximum value from max_vec\n    alignas(32) float max_array[simd_width];\n    _mm256_store_ps(max_array, max_vec);\n    float max_val = std::numeric_limits<float>::lowest();\n    for (size_t j = 0; j < simd_width; ++j) {\n        max_val = std::max(max_val, max_array[j]);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    int count = 0;\n    __m256i seven_mask = _mm256_set1_epi8('7');\n    __m256i eleven = _mm256_set1_epi32(11);\n    __m256i thirteen = _mm256_set1_epi32(13);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i mod_eleven = _mm256_rem_epi32(indices, eleven);\n        __m256i mod_thirteen = _mm256_rem_epi32(indices, thirteen);\n\n        __m256i divisible = _mm256_or_si256(_mm256_cmpeq_epi32(mod_eleven, zero), _mm256_cmpeq_epi32(mod_thirteen, zero));\n        int mask = _mm256_movemask_epi8(divisible);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask & (1 << (j * 4))) {\n                int num = i + j;\n                char buffer[12];\n                snprintf(buffer, sizeof(buffer), \"%d\", num);\n\n                __m256i chars = _mm256_loadu_si256((__m256i*)buffer);\n                __m256i cmp = _mm256_cmpeq_epi8(chars, seven_mask);\n                count += _mm256_popcnt_u32(_mm256_movemask_epi8(cmp));\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    __m256i count = _mm256_setzero_si256();\n    __m256i seven = _mm256_set1_epi8('7');\n    __m256i eleven = _mm256_set1_epi32(11);\n    __m256i thirteen = _mm256_set1_epi32(13);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i mod11 = _mm256_rem_epi32(indices, eleven);\n        __m256i mod13 = _mm256_rem_epi32(indices, thirteen);\n        __m256i mask11 = _mm256_cmpeq_epi32(mod11, zero);\n        __m256i mask13 = _mm256_cmpeq_epi32(mod13, zero);\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        __m256i filtered = _mm256_and_si256(indices, mask);\n\n        __m256i digits = _mm256_extract_epi8(filtered);\n        __m256i digitmask = _mm256_cmpeq_epi8(digits, seven);\n        count+=\n    }\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    __m256i count7 = _mm256_setzero_si256();\n    __m256i eleven = _mm256_set1_epi32(11);\n    __m256i thirteen = _mm256_set1_epi32(13);\n    __m256i seven = _mm256_set1_epi32(7);\n\n    for (int i = 0; i < n; i += simd_width) {\n        __m256i indices = _mm256_setr_epi32(\n            i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Mask out values greater than or equal to n\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), indices);\n\n        // Check divisibility by 11 or 13\n        __m256i mod11 = _mm256_rem_epi32(indices, eleven);\n        __m256i mod13 = _mm256_rem_epi32(indices, thirteen);\n        __m256i divisible = _mm256_or_si256(\n            _mm256_cmpeq_epi32(mod11, _mm256_setzero_si256()),\n            _mm256_cmpeq_epi32(mod13, _mm256_setzero_si256()));\n\n        // Apply mask to filter valid indices\n        divisible = _mm256_and_si256(divisible, mask);\n\n        // Count occurrences of digit 7 in valid numbers\n        for (int j = 0; j < simd_width; ++j) {\n            int value = i + j;\n            if (value < n && (value % 11 == 0 || value % 13 == 0)) {\n                int temp = value;\n                while (temp > 0) {\n                    if (temp % 10 == 7) {\n                        count7 = _mm256_add_epi32(count7, seven);\n                    }\n                    temp /= 10;\n                }\n            }\n        }\n    }\n\n    // Sum up the counts\n    int result[simd_width];\n    _mm256_storeu_si256((__m256i*)result, count7);\n    int total = 0;\n    for (int i = 0; i < simd_width; ++i) {\n        total += result[i];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    __m256i count = _mm256_setzero_si256();\n    __m256i sevens = _mm256_set1_epi32(7);\n    __m256i elevens = _mm256_set1_epi32(11);\n    __m256i thirteens = _mm256_set1_epi32(13);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mod11 = _mm256_rem_epi32(indices, elevens);\n        __m256i mod13 = _mm256_rem_epi32(indices, thirteens);\n\n        __m256i divisible11 = _mm256_cmpeq_epi32(mod11, zero);\n        __m256i divisible13 = _mm256_cmpeq_epi32(mod13, zero);\n        __m256i divisible = _mm256_or_si256(divisible11, divisible13);\n\n        __m256i digit7 = _mm256_setzero_si256();\n        for (int j = 0; j < 8; ++j) {\n            int num = i + j;\n            if (num < n && (num % 11 == 0 || num % 13 == 0)) {\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        digit7 = _mm256_add_epi32(digit7, sevens);\n                    }\n                    num /= 10;\n                }\n            }\n        }\n    }\n\n    return digit7;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    int count = 0;\n    __m256i seven_mask = _mm256_set1_epi8('7');\n    __m256i eleven_mask = _mm256_set1_epi32(11);\n    __m256i thirteen_mask = _mm256_set1_epi32(13);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i mod_eleven = _mm256_rem_epi32(indices, eleven_mask);\n        __m256i mod_thirteen = _mm256_rem_epi32(indices, thirteen_mask);\n\n        __m256i divisible = _mm256_or_si256(_mm256_cmpeq_epi32(mod_eleven, zero), _mm256_cmpeq_epi32(mod_thirteen, zero));\n        int mask = _mm256_movemask_epi8(divisible);\n\n        for (int j = 0; j < 32; ++j) {\n            if (mask & (1 << j)) {\n                int num = i + j;\n                char buffer[12];\n                snprintf(buffer, sizeof(buffer), \"%d\", num);\n                __m256i digits = _mm256_loadu_si256((__m256i*)buffer);\n                __m256i cmp = _mm256_cmpeq_epi8(digits, seven_mask);\n                count += _mm256_movemask_epi8(cmp);\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_values;\n    for (size_t i = 0; i < n; i += 2) {\n        even_values.push_back(l[i]);\n    }\n\n    // Sort the even indices using std::sort\n    std::sort(even_values.begin(), even_values.end());\n\n    // Prepare the result vector\n    std::vector<float> result(n);\n\n    // Use SIMD to copy odd indices and sorted even indices back to the result\n    size_t even_idx = 0;\n    __m256 vec_even, vec_result;\n    for (size_t i = 0; i < n; i += 8) {\n        float temp_even[8] = {0};\n        float temp_result[8] = {0};\n\n        for (size_t j = 0; j < 8 && (i + j) < n; ++j) {\n            if ((i + j) % 2 == 0) {\n                temp_even[j] = even_values[even_idx++];\n            } else {\n                temp_result[j] = l[i + j];\n            }\n        }\n\n        vec_even = _mm256_loadu_ps(temp_even);\n        vec_result = _mm256_loadu_ps(temp_result);\n\n        vec_result = _mm256_blend_ps(vec_result, vec_even, 0x55); // Blend even indices\n\n        _mm256_storeu_ps(temp_result, vec_result);\n\n        for (size_t j = 0; j < 8 && (i + j) < n; ++j) {\n            result[i + j] = temp_result[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_values;\n    for (size_t i = 0; i < n; i += 2) {\n        even_values.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(even_values.begin(), even_values.end());\n\n    // Use SIMD to efficiently copy sorted even indices and odd indices back to result\n    size_t even_idx = 0;\n    __m256 vec_even, vec_odd;\n    for (size_t i = 0; i < n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256 vec_input = _mm256_loadu_ps(l.data() + i);\n\n        // Extract even and odd indices\n        float temp_even[8], temp_odd[8];\n        for (size_t j = 0; j < 8; j++) {\n            if ((i + j) % 2 == 0) {\n                temp_even[j] = even_values[even_idx++];\n                temp_odd[j] = 0; // Placeholder for odd indices\n            } else {\n                temp_even[j] = 0; // Placeholder for even indices\n                temp_odd[j] = l[i + j];\n            }\n        }\n\n        // Load even and odd values into SIMD registers\n        vec_even = _mm256_loadu_ps(temp_even);\n        vec_odd = _mm256_loadu_ps(temp_odd);\n\n        // Combine even and odd values into the result vector\n        __m256 vec_result = _mm256_add_ps(vec_even, vec_odd);\n        _mm256_storeu_ps(result.data() + i, vec_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(even_indices.begin(), even_indices.end());\n\n    // Use SIMD to merge sorted even indices back into the original vector\n    size_t even_count = even_indices.size();\n    size_t simd_width = 8; // AVX2 processes 8 floats at a time\n    size_t i = 0;\n\n    for (; i + simd_width <= even_count; i += simd_width) {\n        __m256 sorted_even = _mm256_loadu_ps(even_indices.data() + i);\n        for (size_t j = 0; j < simd_width; ++j) {\n            l[i * 2 + j * 2] = ((float*)&sorted_even)[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < even_count; ++i) {\n        l[i * 2] = even_indices[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) {\n        return l; // No even indices to sort\n    }\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices using SIMD\n    size_t even_size = even_indices.size();\n    for (size_t i = 0; i < even_size; i += 8) {\n        size_t chunk_size = std::min(size_t(8), even_size - i);\n\n        // Load up to 8 elements into an AVX register\n        __m256 vec = _mm256_setzero_ps();\n        for (size_t j = 0; j < chunk_size; ++j) {\n            vec[j] = even_indices[i + j];\n        }\n\n        // Sort the chunk using a simple bubble sort (SIMD-friendly)\n        for (int pass = 0; pass < chunk_size - 1; ++pass) {\n            for (int k = 0; k < chunk_size - pass - 1; ++k) {\n                __m256 left = _mm256_set1_ps(vec[k]);\n                __m256 right = _mm256_set1_ps(vec[k + 1]);\n                __m256 mask = _mm256_cmp_ps(left, right, _CMP_GT_OQ);\n\n                float temp = vec[k];\n                vec[k] = mask[k] ? vec[k + 1] : vec[k];\n                vec[k + 1] = mask[k] ? temp : vec[k + 1];\n            }\n        }\n\n        // Store the sorted chunk back into the even_indices vector\n        for (size_t j = 0; j < chunk_size; ++j) {\n            even_indices[i + j] = vec[j];\n        }\n    }\n\n    // Merge the sorted even indices back into the original vector\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_indices[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    size_t even_size = even_indices.size();\n    for (size_t i = 0; i < even_size; i += 8) {\n        size_t remaining = std::min(even_size - i, size_t(8));\n        __m256 vec = _mm256_loadu_ps(even_indices.data() + i);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n        std::sort(temp, temp + remaining);\n        vec = _mm256_loadu_ps(temp);\n        _mm256_storeu_ps(even_indices.data() + i, vec);\n    }\n\n    // Merge sorted even indices back into the original vector\n    size_t even_idx = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_indices[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            __m256i vi = _mm256_set1_epi32(l[i]);\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i sum_ij = _mm256_add_epi32(vi, vj);\n\n            for (size_t k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(sum_ij, vk);\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; i++) {\n        for (int j = i + 1; j < n - 1; j++) {\n            __m256i vi = _mm256_set1_epi32(l[i]);\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i sum_ij = _mm256_add_epi32(vi, vj);\n\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(sum_ij, vk);\n\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vi = _mm256_set1_epi32(l[i] + l[j]); // Broadcast l[i] + l[j] to all lanes\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&l[k]); // Load 8 integers from l[k]\n                __m256i sum = _mm256_add_epi32(vi, vk); // Compute l[i] + l[j] + l[k]\n                __m256i zero = _mm256_setzero_si256(); // Set all lanes to zero\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare sum with zero\n                if (_mm256_movemask_epi8(cmp)) { // Check if any lane is zero\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            __m256i vi = _mm256_set1_epi32(l[i] + l[j]);\n            size_t k = j + 1;\n\n            for (; k + 8 <= n; k += 8) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(vi, vk);\n                __m256i cmp = _mm256_cmpeq_epi32(sum, _mm256_setzero_si256());\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i sum_ij = _mm256_set1_epi32(l[i] + l[j]);\n\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vec_k;\n                if (k + 8 <= n) {\n                    vec_k = _mm256_loadu_si256((__m256i*)&l[k]);\n                } else {\n                    int temp[8] = {0};\n                    for (int t = 0; t < n - k; ++t) {\n                        temp[t] = l[k + t];\n                    }\n                    vec_k = _mm256_loadu_si256((__m256i*)temp);\n                }\n\n                __m256i sum = _mm256_add_epi32(sum_ij, vec_k);\n                __m256i cmp = _mm256_cmpeq_epi32(sum, _mm256_setzero_si256());\n\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    size_t i = 0;\n\n    __m256i one = _mm256_set1_epi32(1); // Vector of 1s for addition\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers from the input vector\n        __m256i incremented = _mm256_add_epi32(vec, one);  // Increment each integer by 1\n        _mm256_storeu_si256((__m256i*)&result[i], incremented); // Store the result back to the output vector\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t i = 0;\n\n    __m256i one = _mm256_set1_epi32(1); // Vector of 1s\n\n    // Process elements in chunks of 8\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers\n        __m256i incremented = _mm256_add_epi32(vec, one); // Increment by 1\n        _mm256_storeu_si256((__m256i*)&result[i], incremented); // Store result\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t i = 0;\n\n    __m256i one = _mm256_set1_epi32(1); // Vector with all elements set to 1\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers\n        __m256i incremented = _mm256_add_epi32(vec, one); // Increment each by 1\n        _mm256_storeu_si256((__m256i*)&result[i], incremented); // Store the result\n    }\n\n    // Process remaining elements\n    for (; i < n; i++) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers\n        __m256i increment = _mm256_set1_epi32(1);         // Set all elements to 1\n        __m256i result_vec = _mm256_add_epi32(vec, increment); // Add 1 to each element\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec); // Store the result\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_end = n / simd_width * simd_width;\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i incremented = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&result[i], incremented);\n    }\n\n    for (size_t i = simd_end; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (size_t j = 0; j < n; j += 8) {\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i sum = _mm256_add_epi32(vec1, vec2);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                for (size_t k = 0; k < 8; ++k) {\n                    if (i + k < n && j + k < n && l[i + k] + l[j + k] == 0 && i + k != j + k) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers into vec1\n        for (size_t j = 0; j < n; j += 8) {\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&l[j]); // Load 8 integers into vec2\n\n            __m256i neg_vec2 = _mm256_sub_epi32(_mm256_setzero_si256(), vec2); // Compute -vec2\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, neg_vec2); // Compare vec1 with -vec2\n\n            int mask = _mm256_movemask_epi8(cmp); // Extract comparison results\n            if (mask != 0) {\n                return true; // Found a pair that sums to zero\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_set_epi32(\n            i + 7 < n ? l[i + 7] : 0,\n            i + 6 < n ? l[i + 6] : 0,\n            i + 5 < n ? l[i + 5] : 0,\n            i + 4 < n ? l[i + 4] : 0,\n            i + 3 < n ? l[i + 3] : 0,\n            i + 2 < n ? l[i + 2] : 0,\n            i + 1 < n ? l[i + 1] : 0,\n            l[i]\n        );\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256i vec_j = _mm256_set_epi32(\n                j + 7 < n ? l[j + 7] : 0,\n                j + 6 < n ? l[j + 6] : 0,\n                j + 5 < n ? l[j + 5] : 0,\n                j + 4 < n ? l[j + 4] : 0,\n                j + 3 < n ? l[j + 3] : 0,\n                j + 2 < n ? l[j + 2] : 0,\n                j + 1 < n ? l[j + 1] : 0,\n                l[j]\n            );\n\n            __m256i sum = _mm256_add_epi32(vec_i, vec_j);\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n\n            if (_mm256_movemask_epi8(cmp)) {\n                for (int k = 0; k < 8; ++k) {\n                    int idx_i = i + k;\n                    int idx_j = j + k;\n                    if (idx_i < n && idx_j < n && idx_i != idx_j && l[idx_i] + l[idx_j] == 0) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    for (size_t i = 0; i < n; i++) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast l[i] to all elements of a 256-bit register\n\n        for (size_t j = i + 1; j < n; j += 8) {\n            size_t remaining = n - j;\n            if (remaining < 8) {\n                // Handle remaining elements without AVX\n                for (size_t k = j; k < n; k++) {\n                    if (l[i] + l[k] == 0) return true;\n                }\n                break;\n            }\n\n            __m256i vj = _mm256_loadu_si256((__m256i*)&l[j]); // Load 8 integers from l[j]\n            __m256i sum = _mm256_add_epi32(vi, vj);           // Add vi and vj\n            __m256i zero = _mm256_setzero_si256();           // Set all elements to zero\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);     // Compare sum with zero\n\n            if (_mm256_movemask_epi8(cmp)) {                 // Check if any comparison result is true\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i vec_i = _mm256_loadu_si256((__m256i*)&l[i]);\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256i vec_j = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i sum = _mm256_add_epi32(vec_i, vec_j);\n\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                for (size_t k = 0; k < 8; ++k) {\n                    if (i + k < n && j + k < n && l[i + k] + l[j + k] == 0 && i + k != j + k) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    const int max_digits = 64; // Maximum digits for a 64-bit integer in base 2\n    char result[max_digits];\n    int index = max_digits - 1;\n\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i x_vec = _mm256_set1_epi64x(x);\n\n    while (_mm256_testz_si256(x_vec, x_vec) == 0) {\n        __m256i remainder_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i digit_vec = _mm256_add_epi64(remainder_vec, zero_vec);\n\n        int64_t remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, digit_vec);\n\n        for (int i = 0; i < 4 && index >= 0; ++i) {\n            result[index--] = '0' + remainder[i];\n        }\n\n        x_vec = _mm256_div_epi64(x_vec, base_vec);\n    }\n\n    return std::string(result + index + 1, result + max_digits);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9 || x < 0) {\n        return \"\"; // Invalid input\n    }\n\n    // Buffer to store the digits in reverse order\n    std::vector<char> buffer(64);\n    int index = 0;\n\n    // Process the number in chunks using AVX2\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        // Compute x % base and x / base for 4 lanes in parallel\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i rem_vec = _mm256_rem_epi64(x_vec, base_vec); // Remainder\n        __m256i div_vec = _mm256_div_epi64(x_vec, base_vec); // Quotient\n\n        // Extract the remainder values\n        int64_t rem[4];\n        _mm256_storeu_si256((__m256i*)rem, rem_vec);\n\n        // Store the digits in the buffer\n        for (int i = 0; i < 4 && x > 0; ++i) {\n            buffer[index++] = '0' + rem[i];\n            x /= base;\n        }\n    }\n\n    // Reverse the buffer to get the correct order\n    std::string result(buffer.rbegin(), buffer.rbegin() + index);\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    std::vector<int8_t> digits;\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while (x > 0) {\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i remainder_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i digit_vec = _mm256_max_epi64(remainder_vec, zero_vec);\n\n        int64_t remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, digit_vec);\n\n        digits.push_back(static_cast<int8_t>(remainder[0]));\n        x /= base;\n    }\n\n    std::string result;\n    for (auto it = digits.rbegin(); it != digits.rend(); ++it) {\n        result += std::to_string(*it);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    char buffer[64];\n    int index = 63;\n    buffer[index] = '\\0';\n\n    __m256i base_vec = _mm256_set1_epi64x(base);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n\n    while (_mm256_testz_si256(x_vec, x_vec) == 0) {\n        __m256i remainder_vec = _mm256_rem_epi64(x_vec, base_vec);\n        __m256i quotient_vec = _mm256_div_epi64(x_vec, base_vec);\n\n        int64_t remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, remainder_vec);\n\n        for (int i = 0; i < 4; ++i) {\n            if (index > 0 && x > 0) {\n                buffer[--index] = '0' + remainder[i];\n            }\n        }\n\n        x_vec = quotient_vec;\n    }\n\n    return std::string(&buffer[index]);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    const int max_digits = 64; // Maximum digits for a 64-bit integer in base 2\n    char result[max_digits];\n    int index = max_digits - 1;\n\n    __m256i vec_base = _mm256_set1_epi64x(base);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (_mm256_testz_si256(vec_x, vec_x) == 0) {\n        __m256i vec_remainder = _mm256_rem_epi64(vec_x, vec_base);\n        __m256i vec_digit = _mm256_add_epi64(vec_remainder, vec_zero);\n\n        int64_t digit[4];\n        _mm256_storeu_si256((__m256i*)digit, vec_digit);\n\n        result[index--] = '0' + digit[0]; // Only process the first lane for simplicity\n\n        vec_x = _mm256_div_epi64(vec_x, vec_base);\n    }\n\n    return std::string(result + index + 1, result + max_digits);\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) return true;\n\n    size_t half_len = len / 2;\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* start_ptr = text.data();\n    const char* end_ptr = text.data() + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i start_vec = _mm256_loadu_si256((__m256i*)(start_ptr + i * simd_width));\n        __m256i end_vec = _mm256_loadu_si256((__m256i*)(end_ptr - i * simd_width));\n\n        __m256i reversed_end_vec = _mm256_shuffle_epi8(end_vec, _mm256_setr_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24,\n            23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n\n        __m256i cmp = _mm256_cmpeq_epi8(start_vec, reversed_end_vec);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    size_t remaining = half_len % simd_width;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (text[i + simd_blocks * simd_width] != text[len - 1 - i - simd_blocks * simd_width]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) return true;\n\n    size_t half_len = len / 2;\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* str = text.c_str();\n    const char* rev_str = str + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i forward = _mm256_loadu_si256((__m256i*)(str + i * simd_width));\n        __m256i reverse = _mm256_loadu_si256((__m256i*)(rev_str - i * simd_width));\n\n        reverse = _mm256_shuffle_epi8(reverse, _mm256_setr_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24,\n            23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n\n        __m256i cmp = _mm256_cmpeq_epi8(forward, reverse);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    size_t remaining = half_len % simd_width;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (str[simd_blocks * simd_width + i] != text[len - 1 - (simd_blocks * simd_width + i)]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) return true;\n\n    size_t half_len = len / 2;\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* str = text.c_str();\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i left = _mm256_loadu_si256((__m256i*)(str + i * simd_width));\n        __m256i right = _mm256_loadu_si256((__m256i*)(str + len - simd_width * (i + 1)));\n\n        __m256i reversed_right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24,\n            23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, reversed_right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    size_t remaining = half_len % simd_width;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (str[simd_blocks * simd_width + i] != str[len - simd_blocks * simd_width - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* str = text.c_str();\n    const char* left = str;\n    const char* right = str + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i*)left);\n        __m256i right_vec = _mm256_loadu_si256((__m256i*)right);\n\n        right_vec = _mm256_shuffle_epi8(right_vec, _mm256_setr_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24,\n            23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left_vec, right_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n\n        left += simd_width;\n        right -= simd_width;\n    }\n\n    for (size_t i = simd_blocks * simd_width; i < half_len; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) return true;\n\n    size_t i = 0, j = len - 1;\n\n    while (i < j) {\n        size_t chunk_size = std::min<size_t>(32, j - i + 1);\n\n        __m256i left_chunk = _mm256_loadu_si256((__m256i*)(text.data() + i));\n        __m256i right_chunk = _mm256_loadu_si256((__m256i*)(text.data() + j - chunk_size + 1));\n\n        __m256i reversed_right_chunk = _mm256_permutevar8x32_epi32(right_chunk, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        reversed_right_chunk = _mm256_shuffle_epi8(reversed_right_chunk, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        ));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left_chunk, reversed_right_chunk);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != (1 << chunk_size) - 1) return false;\n\n        i += chunk_size;\n        j -= chunk_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    const int shift = 5;\n    const int alphabet_size = 26;\n    const char base_char = 'a';\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    size_t i = 0;\n    __m256i shift_vec = _mm256_set1_epi8(shift);\n    __m256i base_vec = _mm256_set1_epi8(base_char);\n    __m256i alphabet_size_vec = _mm256_set1_epi8(alphabet_size);\n\n    for (; i + 32 <= len; i += 32) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i base_adjusted = _mm256_sub_epi8(input_vec, base_vec);\n        __m256i shifted = _mm256_sub_epi8(base_adjusted, shift_vec);\n        __m256i wrapped = _mm256_add_epi8(shifted, alphabet_size_vec);\n        __m256i modded = _mm256_rem_epi8(wrapped, alphabet_size_vec);\n        __m256i decoded = _mm256_add_epi8(modded, base_vec);\n        _mm256_storeu_si256((__m256i*)(out.data() + i), decoded);\n    }\n\n    for (; i < len; i++) {\n        char decoded_char = (s[i] - base_char - shift + alphabet_size) % alphabet_size + base_char;\n        out[i] = decoded_char;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m256i shift = _mm256_set1_epi8(5); // Shift value\n    const __m256i a_val = _mm256_set1_epi8('a'); // ASCII value of 'a'\n    const __m256i z_val = _mm256_set1_epi8('z'); // ASCII value of 'z'\n    const __m256i alphabet_size = _mm256_set1_epi8(26); // Alphabet size\n\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]); // Load 32 characters\n        __m256i shifted = _mm256_sub_epi8(chars, shift); // Subtract shift value\n        __m256i mask = _mm256_cmpgt_epi8(a_val, shifted); // Check if shifted value is less than 'a'\n        __m256i adjusted = _mm256_add_epi8(shifted, alphabet_size); // Add 26 to wrap around\n        __m256i result = _mm256_blendv_epi8(shifted, adjusted, mask); // Blend values based on mask\n        _mm256_storeu_si256((__m256i*)&out[i], result); // Store result\n    }\n\n    for (; i < len; i++) {\n        char shifted = s[i] - 5;\n        if (shifted < 'a') {\n            shifted += 26;\n        }\n        out[i] = shifted;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m256i shift = _mm256_set1_epi8(5); // Shift value\n    const __m256i a_val = _mm256_set1_epi8('a'); // ASCII value of 'a'\n    const __m256i z_val = _mm256_set1_epi8('z'); // ASCII value of 'z'\n    const __m256i alphabet_size = _mm256_set1_epi8(26); // Alphabet size\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]); // Load 32 characters\n        __m256i shifted = _mm256_sub_epi8(chars, shift); // Subtract shift value\n\n        // Handle wrap-around for characters less than 'a'\n        __m256i mask = _mm256_cmpgt_epi8(a_val, shifted); // Check if shifted < 'a'\n        __m256i wrapped = _mm256_add_epi8(shifted, alphabet_size); // Wrap-around\n        shifted = _mm256_blendv_epi8(shifted, wrapped, mask); // Apply wrap-around conditionally\n\n        _mm256_storeu_si256((__m256i*)&out[i], shifted); // Store result\n    }\n\n    // Process remaining characters\n    for (; i < len; i++) {\n        char shifted = s[i] - 5;\n        if (shifted < 'a') {\n            shifted += 26;\n        }\n        out[i] = shifted;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m256i shift = _mm256_set1_epi8(5); // Shift value\n    const __m256i a_val = _mm256_set1_epi8('a'); // ASCII value of 'a'\n    const __m256i z_val = _mm256_set1_epi8('z'); // ASCII value of 'z'\n    const __m256i alphabet_size = _mm256_set1_epi8(26); // Alphabet size\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&s[i]); // Load 32 characters\n        __m256i shifted = _mm256_sub_epi8(chars, shift); // Subtract shift value\n        __m256i mask = _mm256_cmpgt_epi8(shifted, a_val); // Check if shifted value is less than 'a'\n        __m256i wrapped = _mm256_add_epi8(shifted, alphabet_size); // Wrap around\n        __m256i result = _mm256_blendv_epi8(wrapped, shifted, mask); // Blend wrapped and shifted values\n        _mm256_storeu_si256((__m256i*)&out[i], result); // Store result\n    }\n\n    for (; i < len; ++i) {\n        char shifted = s[i] - 5;\n        out[i] = (shifted < 'a') ? shifted + 26 : shifted;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m256i shift = _mm256_set1_epi8(5); // Shift value\n    const __m256i a_val = _mm256_set1_epi8('a'); // ASCII value of 'a'\n    const __m256i z_val = _mm256_set1_epi8('z'); // ASCII value of 'z'\n    const __m256i alphabet_size = _mm256_set1_epi8(26); // Alphabet size\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(s.data() + i)); // Load 32 characters\n        __m256i shifted = _mm256_sub_epi8(chars, shift); // Subtract shift value\n        __m256i mask = _mm256_cmpgt_epi8(shifted, a_val); // Check if shifted value is less than 'a'\n        __m256i wrapped = _mm256_add_epi8(shifted, alphabet_size); // Wrap around\n        __m256i result = _mm256_blendv_epi8(wrapped, shifted, mask); // Blend values based on mask\n        _mm256_storeu_si256((__m256i*)(out.data() + i), result); // Store result\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        char decoded = c - 5;\n        if (decoded < 'a') {\n            decoded += 26;\n        }\n        out[i] = decoded;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    __m256i threshold = _mm256_set1_epi32(t); // Broadcast threshold to all elements\n    size_t i = 0;\n\n    // Process 8 integers at a time\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold); // Compare data > threshold\n        if (!_mm256_testz_si256(cmp, cmp)) { // Check if any element is greater\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    if (size == 0) return true;\n\n    __m256i threshold = _mm256_set1_epi32(t); // Broadcast threshold to all elements\n    size_t i = 0;\n\n    // Process 8 integers at a time\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold); // Compare data > threshold\n        if (!_mm256_testz_si256(cmp, cmp)) { // Check if any element is greater\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    __m256i threshold_vec = _mm256_set1_epi32(t);\n\n    size_t i = 0;\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256i data_vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp_result = _mm256_cmpgt_epi32(data_vec, threshold_vec);\n        if (!_mm256_testz_si256(cmp_result, cmp_result)) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    if (size == 0) return true;\n\n    __m256i threshold = _mm256_set1_epi32(t); // Set threshold value across all lanes\n    size_t i = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&l[i]); // Load 8 integers from the vector\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold); // Compare each element with the threshold\n        if (!_mm256_testz_si256(cmp, cmp)) { // Check if any element is greater than the threshold\n            return false;\n        }\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    if (size == 0) return true;\n\n    __m256i threshold = _mm256_set1_epi32(t);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i opening_brackets = _mm256_set1_epi8('<');\n    __m256i closing_brackets = _mm256_set1_epi8('>');\n\n    int total_balance = 0;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i * simd_width));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening_brackets);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing_brackets);\n\n        __m256i opening_count = _mm256_sub_epi8(is_opening, is_closing);\n\n        // Accumulate the balance for this chunk\n        int8_t temp[simd_width];\n        _mm256_storeu_si256((__m256i*)temp, opening_count);\n\n        for (size_t j = 0; j < simd_width; ++j) {\n            total_balance += temp[j];\n            if (total_balance < 0) return false; // Early exit if balance goes negative\n        }\n    }\n\n    // Handle remainder\n    for (size_t i = simd_blocks * simd_width; i < n; ++i) {\n        if (brackets[i] == '<') {\n            total_balance++;\n        } else if (brackets[i] == '>') {\n            total_balance--;\n        }\n        if (total_balance < 0) return false; // Early exit if balance goes negative\n    }\n\n    return total_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i opening_bracket = _mm256_set1_epi8('<');\n    __m256i closing_bracket = _mm256_set1_epi8('>');\n    int balance = 0;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i * simd_width));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening_bracket);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing_bracket);\n\n        __m256i opening_count = _mm256_sub_epi8(is_opening, is_closing);\n\n        // Accumulate balance\n        for (int j = 0; j < simd_width; ++j) {\n            balance += ((char*)&opening_count)[j];\n            if (balance < 0) return false; // Early exit if balance goes negative\n        }\n    }\n\n    // Process remainder\n    for (size_t i = simd_blocks * simd_width; i < n; ++i) {\n        if (brackets[i] == '<') balance++;\n        else if (brackets[i] == '>') balance--;\n        if (balance < 0) return false; // Early exit if balance goes negative\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t simd_blocks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i opening_bracket = _mm256_set1_epi8('<');\n    __m256i closing_bracket = _mm256_set1_epi8('>');\n\n    int total_balance = 0;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i * simd_width));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening_bracket);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing_bracket);\n\n        __m256i opening_count = _mm256_sub_epi8(is_opening, is_closing);\n\n        // Accumulate the balance for this block\n        int block_balance = 0;\n        for (int j = 0; j < simd_width; ++j) {\n            block_balance += ((char*)&opening_count)[j];\n            if (block_balance < 0) return false; // Early exit if balance goes negative\n        }\n        total_balance += block_balance;\n    }\n\n    // Process remainder\n    for (size_t i = simd_blocks * simd_width; i < n; ++i) {\n        if (brackets[i] == '<') {\n            total_balance++;\n        } else if (brackets[i] == '>') {\n            total_balance--;\n        }\n        if (total_balance < 0) return false; // Early exit if balance goes negative\n    }\n\n    return total_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t chunks = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i opening_brackets = _mm256_set1_epi8('<');\n    __m256i closing_brackets = _mm256_set1_epi8('>');\n    int total_balance = 0;\n\n    for (size_t i = 0; i < chunks; ++i) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i * simd_width));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening_brackets);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing_brackets);\n\n        __m256i opening_count = _mm256_sub_epi8(is_opening, is_closing);\n\n        // Horizontal sum of opening_count\n        __m128i low = _mm256_castsi256_si128(opening_count);\n        __m128i high = _mm256_extracti128_si256(opening_count, 1);\n        __m128i sum128 = _mm_add_epi8(low, high);\n\n        sum128 = _mm_sad_epu8(sum128, _mm_setzero_si128());\n        total_balance += _mm_extract_epi16(sum128, 0) + _mm_extract_epi16(sum128, 4);\n\n        if (total_balance < 0) return false;\n    }\n\n    // Process remainder\n    for (size_t i = chunks * simd_width; i < n; ++i) {\n        if (brackets[i] == '<') {\n            total_balance++;\n        } else if (brackets[i] == '>') {\n            total_balance--;\n        }\n        if (total_balance < 0) return false;\n    }\n\n    return total_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    const char* data = brackets.data();\n    size_t i = 0;\n    __m256i opening_count = _mm256_setzero_si256();\n    __m256i closing_count = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    while (i + 32 <= n) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n\n        opening_count = _mm256_add_epi8(opening_count, is_opening);\n        closing_count = _mm256_add_epi8(closing_count, is_closing);\n\n        __m256i imbalance = _mm256_sub_epi8(opening_count, closing_count);\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi8(zero, imbalance)) != 0) {\n            return false;\n        }\n\n        i += 32;\n    }\n\n    int total_opening = 0, total_closing = 0;\n    alignas(32) char opening_buffer[32], closing_buffer[32];\n    _mm256_store_si256((__m256i*)opening_buffer, opening_count);\n    _mm256_store_si256((__m256i*)closing_buffer, closing_count);\n\n    for (int j = 0; j < 32; ++j) {\n        total_opening += opening_buffer[j];\n        total_closing += closing_buffer[j];\n    }\n\n    for (; i < n; ++i) {\n        if (data[i] == '<') total_opening++;\n        else if (data[i] == '>') total_closing++;\n\n        if (total_closing > total_opening) return false;\n    }\n\n    return total_opening == total_closing;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    // Ensure the input size is a multiple of 32 for AVX processing\n    size_t padded_size = (n + 31) & ~31;\n    std::string padded_brackets = brackets;\n    padded_brackets.resize(padded_size, ')'); // Pad with closing brackets\n\n    __m256i opening_mask = _mm256_set1_epi8('(');\n    __m256i closing_mask = _mm256_set1_epi8(')');\n    int balance = 0;\n\n    for (size_t i = 0; i < padded_size; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&padded_brackets[i]);\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening_mask);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing_mask);\n\n        __m256i opening_count = _mm256_sub_epi8(is_opening, is_closing);\n\n        alignas(32) char counts[32];\n        _mm256_store_si256((__m256i*)counts, opening_count);\n\n        for (int j = 0; j < 32; ++j) {\n            balance += counts[j];\n            if (balance < 0) return false; // Early exit if balance goes negative\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    // Ensure the input size is a multiple of 32 for AVX2 processing\n    size_t padded_size = ((n + 31) / 32) * 32;\n    std::string padded_brackets = brackets;\n    padded_brackets.resize(padded_size, ')'); // Pad with closing brackets\n\n    __m256i opening = _mm256_set1_epi8('(');\n    __m256i closing = _mm256_set1_epi8(')');\n    int balance = 0;\n\n    for (size_t i = 0; i < padded_size; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&padded_brackets[i]);\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing);\n\n        // Convert masks to counts\n        int opening_count = _mm256_movemask_epi8(is_opening);\n        int closing_count = _mm256_movemask_epi8(is_closing);\n\n        // Count the number of '(' and ')' in the chunk\n        opening_count = __builtin_popcount(opening_count);\n        closing_count = __builtin_popcount(closing_count);\n\n        balance += opening_count - closing_count;\n        if (balance < 0) return false; // Early exit if imbalance occurs\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    __m256i open_count = _mm256_setzero_si256();\n    __m256i close_count = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i is_open = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('('));\n        __m256i is_close = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8(')'));\n\n        open_count = _mm256_add_epi8(open_count, is_open);\n        close_count = _mm256_add_epi8(close_count, is_close);\n\n        __m256i balance = _mm256_sub_epi8(open_count, close_count);\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi8(zero, balance)) != 0) {\n            return false;\n        }\n    }\n\n    int open_sum = 0, close_sum = 0;\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') open_sum++;\n        else if (brackets[i] == ')') close_sum++;\n        if (close_sum > open_sum) return false;\n    }\n\n    alignas(32) char open_buffer[32], close_buffer[32];\n    _mm256_store_si256((__m256i*)open_buffer, open_count);\n    _mm256_store_si256((__m256i*)close_buffer, close_count);\n\n    for (int j = 0; j < 32; ++j) {\n        open_sum += open_buffer[j];\n        close_sum += close_buffer[j];\n    }\n\n    return open_sum == close_sum;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    __m256i opening = _mm256_set1_epi8('(');\n    __m256i closing = _mm256_set1_epi8(')');\n    int balance = 0;\n\n    for (int i = 0; i < n; i += 32) {\n        int chunk_size = (i + 32 <= n) ? 32 : n - i;\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n        __m256i is_opening = _mm256_cmpeq_epi8(chunk, opening);\n        __m256i is_closing = _mm256_cmpeq_epi8(chunk, closing);\n\n        __m256i delta = _mm256_sub_epi8(is_opening, is_closing);\n\n        alignas(32) char deltas[32];\n        _mm256_store_si256((__m256i*)deltas, delta);\n\n        for (int j = 0; j < chunk_size; ++j) {\n            balance += deltas[j];\n            if (balance < 0) return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    __m256i open_count = _mm256_setzero_si256();\n    __m256i close_count = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n\n    int i = 0;\n    for (; i + 31 < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i const*)(brackets.data() + i));\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n\n        open_count = _mm256_add_epi8(open_count, _mm256_and_si256(open_mask, ones));\n        close_count = _mm256_add_epi8(close_count, _mm256_and_si256(close_mask, ones));\n\n        __m256i balance = _mm256_sub_epi8(open_count, close_count);\n        __m256i negative_balance = _mm256_cmpgt_epi8(_mm256_setzero_si256(), balance);\n        if (_mm256_movemask_epi8(negative_balance)) {\n            return false;\n        }\n    }\n\n    int open_sum = 0, close_sum = 0;\n    for (; i < n; i++) {\n        if (brackets[i] == '(') open_sum++;\n        else if (brackets[i] == ')') close_sum++;\n        if (close_sum > open_sum) return false;\n    }\n\n    alignas(32) char open_buffer[32], close_buffer[32];\n    _mm256_store_si256((__m256i*)open_buffer, open_count);\n    _mm256_store_si256((__m256i*)close_buffer, close_count);\n\n    for (int j = 0; j < 32; j++) {\n        open_sum += open_buffer[j];\n        close_sum += close_buffer[j];\n    }\n\n    return open_sum == close_sum;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // No derivative for constant polynomial\n\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 1; i < n; i += 8) {\n        size_t remaining = std::min(size_t(8), n - i);\n\n        // Load coefficients into AVX register\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n\n        // Generate indices (1, 2, ..., 8) for multiplication\n        float indices_arr[8] = {float(i), float(i + 1), float(i + 2), float(i + 3),\n                                float(i + 4), float(i + 5), float(i + 6), float(i + 7)};\n        __m256 indices = _mm256_loadu_ps(indices_arr);\n\n        // Multiply coefficients by their respective indices\n        __m256 derivative = _mm256_mul_ps(coeffs, indices);\n\n        // Store the result back to the result vector\n        _mm256_storeu_ps(&result[i - 1], derivative);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // Derivative of a constant is zero.\n\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 1; i < n; i += 8) {\n        size_t remaining = n - i;\n        size_t count = remaining >= 8 ? 8 : remaining;\n\n        __m256 indices = _mm256_set_ps(\n            float(i + 7), float(i + 6), float(i + 5), float(i + 4),\n            float(i + 3), float(i + 2), float(i + 1), float(i)\n        );\n\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 product = _mm256_mul_ps(indices, coeffs);\n\n        if (count < 8) {\n            float temp[8];\n            _mm256_storeu_ps(temp, product);\n            for (size_t j = 0; j < count; ++j) {\n                result[i - 1 + j] = temp[j];\n            }\n        } else {\n            _mm256_storeu_ps(&result[i - 1], product);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // Derivative of a constant is zero.\n\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 1; i < n; i += 8) {\n        size_t remaining = std::min(n - i, size_t(8));\n\n        __m256 indices = _mm256_set_ps(\n            float(i + 7), float(i + 6), float(i + 5), float(i + 4),\n            float(i + 3), float(i + 2), float(i + 1), float(i)\n        );\n\n        __m256 coefficients = _mm256_loadu_ps(&xs[i]);\n        __m256 derivatives = _mm256_mul_ps(indices, coefficients);\n\n        if (remaining < 8) {\n            float temp[8];\n            _mm256_storeu_ps(temp, derivatives);\n            for (size_t j = 0; j < remaining; ++j) {\n                result[i - 1 + j] = temp[j];\n            }\n        } else {\n            _mm256_storeu_ps(&result[i - 1], derivatives);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // No derivative for constant or empty polynomial\n\n    std::vector<float> result(n - 1);\n\n    // Prepare coefficients for multiplication\n    std::vector<float> coefficients(n - 1);\n    for (size_t i = 1; i < n; ++i) {\n        coefficients[i - 1] = static_cast<float>(i);\n    }\n\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n\n    // Process in chunks of 8 using AVX\n    for (; i + simd_width <= n - 1; i += simd_width) {\n        __m256 coeffs = _mm256_loadu_ps(&coefficients[i]);\n        __m256 values = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 product = _mm256_mul_ps(coeffs, values);\n        _mm256_storeu_ps(&result[i], product);\n    }\n\n    // Handle remaining elements\n    for (; i < n - 1; ++i) {\n        result[i] = coefficients[i] * xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // Derivative of constant is empty.\n\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        // Load coefficients into AVX register\n        __m256 coeffs = _mm256_loadu_ps(xs.data() + i + 1);\n\n        // Generate indices (1, 2, 3, ..., 8) for multiplication\n        __m256 indices = _mm256_set_ps(\n            float(i + 8), float(i + 7), float(i + 6), float(i + 5),\n            float(i + 4), float(i + 3), float(i + 2), float(i + 1));\n\n        // Multiply coefficients by their respective indices\n        __m256 derivatives = _mm256_mul_ps(coeffs, indices);\n\n        // Store the result back to the result vector\n        _mm256_storeu_ps(result.data() + i, derivatives);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y[] = \"yY\";\n    int len = s.length();\n    int count = 0;\n\n    // Process 32 characters at a time using AVX2\n    __m256i vowel_mask = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0\n    );\n\n    for (int i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i match = _mm256_cmpeq_epi8(chunk, vowel_mask);\n        count += _mm256_movemask_epi8(match);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int len = s.length();\n    int count = 0;\n\n    // Process in chunks of 32 bytes using AVX2\n    __m256i vowel_mask = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowel_mask);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(cmp));\n    }\n\n    // Handle remaining characters\n    for (; i < len; i++) {\n        char c = s[i];\n        if (strchr(vowel_mask,c)!=0) count++;\n    }\n}\n"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int len = s.length();\n    int count = 0;\n\n    // Load vowels into an AVX2 register\n    __m256i vowel_mask = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    // Process the string in chunks of 32 characters\n    for (int i = 0; i < len; i += 32) {\n        int chunk_size = (i + 32 <= len) ? 32 : (len - i);\n        char chunk[32] = {0};\n        for (int j = 0; j < chunk_size; ++j) {\n            chunk[j] = s[i + j];\n        }\n\n        // Load the chunk into an AVX2 register\n        __m256i input_chunk = _mm256_loadu_si256((__m256i*)chunk);\n\n        // Compare the input chunk with the vowel mask\n        __m256i cmp_result = _mm256_cmpeq_epi8(input_chunk, vowel_mask);\n\n        // Count matches\n        int mask = _mm256_movemask_epi8(cmp_result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Check if 'y' or 'Y' is at the end of the string\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int length = s.size();\n    int count = 0;\n\n    // Process in chunks of 32 bytes using AVX2\n    __m256i vowel_mask = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    int i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowel_mask);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __popcnt(mask);\n    }\n\n    // Process remaining characters\n    for (; i < length; i++) {\n        char c = s[i];\n        if (strchr(vowels, c)) {\n            count++;\n        }\n    }\n\n    // Check if 'y' or 'Y' is at the end of the word\n    if (length > 0 && strchr(y_vowel, s[length - 1])) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_lower = 'y';\n    const char y_upper = 'Y';\n    int len = s.size();\n    int count = 0;\n\n    __m256i vowel_mask1 = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (int i = 0; i < len; i += 32) {\n        int remaining = len - i;\n        int process_len = remaining < 32 ? remaining : 32;\n\n        __m256i input_chunk = _mm256_setzero_si256();\n        for (int j = 0; j < process_len; ++j) {\n            ((char*)&input_chunk)[j] = s[i + j];\n        }\n\n        __m256i cmp_result = _mm256_cmpeq_epi8(input_chunk, vowel_mask1);\n        int mask = _mm256_movemask_epi8(cmp_result);\n        count += __popcnt(mask);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector to hold sum\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(str + i)); // Load 32 characters\n        __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n                                              _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z' + 1))); // Check if characters are uppercase\n        __m256i upper_chars = _mm256_and_si256(chars, upper_mask); // Mask out non-uppercase characters\n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_sad_epu8(upper_chars, _mm256_setzero_si256())); // Sum ASCII values\n    }\n\n    // Horizontal sum of the vector\n    int sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const size_t len = s.size();\n    const char* str = s.c_str();\n    int sum = 0;\n\n    // Process 32 characters at a time using AVX2\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    while (i + 32 <= len) {\n        // Load 32 characters into an AVX2 register\n        __m256i chars = _mm256_loadu_si256((__m256i*)(str + i));\n\n        // Compare if characters are uppercase ('A' <= c <= 'Z')\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chars)\n        );\n\n        // Mask out non-uppercase characters\n        __m256i masked_chars = _mm256_and_si256(chars, is_upper);\n\n        // Horizontally sum the ASCII values of uppercase characters\n        __m256i zero = _mm256_setzero_si256();\n        __m256i unpacked_low = _mm256_unpacklo_epi8(masked_chars, zero);\n        __m256i unpacked_high = _mm256_unpackhi_epi8(masked_chars, zero);\n        __m256i sum_low = _mm256_add_epi16(unpacked_low, unpacked_high);\n        sum_vec = _mm256_add_epi16(sum_vec, sum_low);\n\n        i += 32;\n    }\n\n    // Horizontally add the 16-bit integers in sum_vec\n    __m128i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i final_sum = _mm_add_epi16(sum_low, sum_high);\n    final_sum = _mm_hadd_epi16(final_sum, final_sum);\n    final_sum = _mm_hadd_epi16(final_sum, final_sum);\n    sum += _mm_extract_epi16(final_sum, 0);\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    int sum = 0;\n\n    __m256i upper_bound = _mm256_set1_epi8('Z'); // ASCII 'Z'\n    __m256i lower_bound = _mm256_set1_epi8('A'); // ASCII 'A'\n    __m256i total_sum = _mm256_setzero_si256(); // Accumulator for sum\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(str + i)); // Load 32 characters\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_bound), // chars > 'A'\n            _mm256_cmpgt_epi8(upper_bound, chars) // chars < 'Z'\n        );\n\n        __m256i masked_chars = _mm256_and_si256(chars, is_upper); // Mask non-uppercase chars\n        total_sum = _mm256_add_epi8(total_sum, masked_chars); // Accumulate ASCII values\n    }\n\n    // Horizontal sum of total_sum\n    alignas(32) char buffer[32];\n    _mm256_store_si256((__m256i*)buffer, total_sum);\n    for (int j = 0; j < 32; ++j) {\n        sum += buffer[j];\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i upper_bound = _mm256_set1_epi8('Z');\n    __m256i lower_bound = _mm256_set1_epi8('A');\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_bound),\n            _mm256_cmpgt_epi8(upper_bound, chars)\n        );\n        __m256i masked_chars = _mm256_and_si256(chars, is_upper);\n        sum_vec = _mm256_add_epi8(sum_vec, masked_chars);\n    }\n\n    alignas(32) char temp[32];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    int sum = 0;\n    for (int j = 0; j < 32; ++j) {\n        sum += (unsigned char)temp[j];\n    }\n\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const size_t len = s.size();\n    const char* str = s.c_str();\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize vector to hold the sum\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(str + i)); // Load 32 characters\n        __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n                                              _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z' + 1))); // Mask for uppercase letters\n        __m256i upper_chars = _mm256_and_si256(chars, upper_mask); // Extract uppercase letters\n        sum_vec = _mm256_add_epi8(sum_vec, upper_chars); // Add ASCII values of uppercase letters\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec);\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi8(low, high);\n\n    // Sum remaining characters\n    int sum = 0;\n    for (int j = 0; j < 16; ++j) {\n        sum += ((unsigned char*)&sum128)[j];\n    }\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    const int max_int = std::numeric_limits<int>::max();\n\n    __m256i min_values = _mm256_set1_epi32(max_int); // Initialize with max int\n    __m256i min_indices = _mm256_set1_epi32(max_int); // Initialize with max int\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7); // Indices for the first SIMD chunk\n\n    for (int i = 0; i < n; i += simd_width) {\n        // Load the next chunk of data\n        __m256i data;\n        if (i + simd_width <= n) {\n            data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        } else {\n            // Handle the tail case where the array size is not a multiple of simd_width\n            int temp[simd_width] = {max_int, max_int, max_int, max_int, max_int, max_int, max_int, max_int};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            data = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        // Check if values are even\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n\n        // Mask odd values to max_int\n        __m256i masked_data = _mm256_blendv_epi8(_mm256_set1_epi32(max_int), data, even_mask);\n\n        // Compare and update minimum values and indices\n        __m256i cmp_mask = _mm256_cmpgt_epi32(min_values, masked_data);\n        min_values = _mm256_blendv_epi8(min_values, masked_data, cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, indices, cmp_mask);\n\n        // Increment indices for the next chunk\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(simd_width));\n    }\n\n    // Extract the minimum value and index from the SIMD registers\n    int min_value = max_int;\n    int min_index = max_int;\n    alignas(32) int values[simd_width];\n    alignas(32) int indices_arr[simd_width];\n    _mm256_store_si256((__m256i*)values, min_values);\n    _mm256_store_si256((__m256i*)indices_arr, min_indices);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (values[i] < min_value) {\n            min_value = values[i];\n            min_index = indices_arr[i];\n        }\n    }\n\n    if (min_value == max_int) {\n        return {}; // No even values found\n    }\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    const int max_int = std::numeric_limits<int>::max();\n\n    __m256i min_values = _mm256_set1_epi32(max_int);\n    __m256i min_indices = _mm256_set1_epi32(max_int);\n\n    for (int i = 0; i < n; i += simd_width) {\n        int remaining = n - i;\n        __m256i values;\n\n        if (remaining >= simd_width) {\n            values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        } else {\n            int temp[simd_width] = {max_int, max_int, max_int, max_int, max_int, max_int, max_int, max_int};\n            for (int j = 0; j < remaining; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(values, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i filtered_values = _mm256_blendv_epi8(_mm256_set1_epi32(max_int), values, even_mask);\n        __m256i filtered_indices = _mm256_blendv_epi8(_mm256_set1_epi32(max_int), indices, even_mask);\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(min_values, filtered_values);\n        min_values = _mm256_blendv_epi8(min_values, filtered_values, cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, filtered_indices, cmp_mask);\n\n        __m256i eq_mask = _mm256_cmpeq_epi32(min_values, filtered_values);\n        __m256i index_cmp_mask = _mm256_cmpgt_epi32(min_indices, filtered_indices);\n        __m256i final_mask = _mm256_and_si256(eq_mask, index_cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, filtered_indices, final_mask);\n    }\n\n    alignas(32) int min_values_arr[simd_width];\n    alignas(32) int min_indices_arr[simd_width];\n    _mm256_store_si256((__m256i*)min_values_arr, min_values);\n    _mm256_store_si256((__m256i*)min_indices_arr, min_indices);\n\n    int smallest_value = max_int;\n    int smallest_index = max_int;\n    for (int i = 0; i < simd_width; ++i) {\n        if (min_values_arr[i] < smallest_value || (min_values_arr[i] == smallest_value && min_indices_arr[i] < smallest_index)) {\n            smallest_value = min_values_arr[i];\n            smallest_index = min_indices_arr[i];\n        }\n    }\n\n    if (smallest_value == max_int) {\n        return {};\n    }\n\n    return {smallest_value, smallest_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    const int max_int = std::numeric_limits<int>::max();\n\n    __m256i min_values = _mm256_set1_epi32(max_int); // Initialize to max int\n    __m256i min_indices = _mm256_set1_epi32(max_int); // Initialize indices to max int\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0); // Initial indices for SIMD lane\n    __m256i increment = _mm256_set1_epi32(simd_width); // Increment for indices\n\n    for (int i = 0; i < n; i += simd_width) {\n        int remaining = n - i;\n        __m256i values;\n        if (remaining >= simd_width) {\n            values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        } else {\n            int temp[simd_width] = {max_int, max_int, max_int, max_int, max_int, max_int, max_int, max_int};\n            for (int j = 0; j < remaining; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(values, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i filtered_values = _mm256_or_si256(_mm256_and_si256(even_mask, values), _mm256_andnot_si256(even_mask, _mm256_set1_epi32(max_int)));\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(min_values, filtered_values);\n        min_values = _mm256_blendv_epi8(min_values, filtered_values, cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, indices, cmp_mask);\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    // Extract the minimum value and index from SIMD registers\n    int min_value = max_int;\n    int min_index = max_int;\n    alignas(32) int values[simd_width];\n    alignas(32) int indices_arr[simd_width];\n    _mm256_store_si256((__m256i*)values, min_values);\n    _mm256_store_si256((__m256i*)indices_arr, min_indices);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (values[i] < min_value) {\n            min_value = values[i];\n            min_index = indices_arr[i];\n        }\n    }\n\n    if (min_value == max_int) {\n        return {};\n    }\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    const int max_int = std::numeric_limits<int>::max();\n\n    __m256i min_values = _mm256_set1_epi32(max_int);\n    __m256i min_indices = _mm256_set1_epi32(max_int);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (int i = 0; i < n; i += simd_width) {\n        // Load the next chunk of data\n        __m256i values = _mm256_set1_epi32(max_int);\n        if (i + simd_width <= n) {\n            values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        } else {\n            int temp[simd_width];\n            for (int j = 0; j < simd_width; ++j) {\n                temp[j] = (i + j < n) ? arr[i + j] : max_int;\n            }\n            values = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        // Check if values are even\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(values, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n\n        // Mask values that are not even\n        values = _mm256_blendv_epi8(_mm256_set1_epi32(max_int), values, even_mask);\n\n        // Compare and update minimum values and indices\n        __m256i cmp_mask = _mm256_cmpgt_epi32(min_values, values);\n        min_values = _mm256_blendv_epi8(min_values, values, cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, indices, cmp_mask);\n\n        // Increment indices for the next chunk\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(simd_width));\n    }\n\n    // Extract the minimum value and index from the SIMD registers\n    int min_value = max_int;\n    int min_index = max_int;\n    alignas(32) int values[simd_width];\n    alignas(32) int indices_arr[simd_width];\n    _mm256_store_si256((__m256i*)values, min_values);\n    _mm256_store_si256((__m256i*)indices_arr, min_indices);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (values[i] < min_value) {\n            min_value = values[i];\n            min_index = indices_arr[i];\n        }\n    }\n\n    if (min_value == max_int) {\n        return {};\n    }\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    const int max_value = std::numeric_limits<int>::max();\n\n    __m256i min_values = _mm256_set1_epi32(max_value); // Initialize to max value\n    __m256i min_indices = _mm256_set1_epi32(max_value); // Initialize indices to max value\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7); // Initialize indices for SIMD lanes\n\n    for (int i = 0; i < n; i += simd_width) {\n        // Load 8 integers from the array\n        __m256i values = _mm256_set1_epi32(max_value);\n        if (i + simd_width <= n) {\n            values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        } else {\n            int temp[simd_width] = {max_value, max_value, max_value, max_value, max_value, max_value, max_value, max_value};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm256_loadu_si256((__m256i*)temp);\n        }\n\n        // Mask for even values\n        __m256i even_mask = _mm256_and_si256(values, _mm256_set1_epi32(1)); // Check if values are even\n        even_mask = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256()); // Mask for even numbers\n\n        // Apply mask to values\n        __m256i masked_values = _mm256_blendv_epi8(_mm256_set1_epi32(max_value), values, even_mask);\n\n        // Compare and update minimum values\n        __m256i cmp_mask = _mm256_cmpgt_epi32(min_values, masked_values);\n        min_values = _mm256_blendv_epi8(min_values, masked_values, cmp_mask);\n        min_indices = _mm256_blendv_epi8(min_indices, indices, cmp_mask);\n\n        // Increment indices for next iteration\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(simd_width));\n    }\n\n    // Extract minimum value and index from SIMD registers\n    int min_value = max_value;\n    int min_index = max_value;\n    int values[simd_width], indices_arr[simd_width];\n    _mm256_storeu_si256((__m256i*)values, min_values);\n    _mm256_storeu_si256((__m256i*)indices_arr, min_indices);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (values[i] < min_value) {\n            min_value = values[i];\n            min_index = indices_arr[i];\n        }\n    }\n\n    if (min_value == max_value) {\n        return {};\n    }\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> frequency_map;\n    int max_value = *std::max_element(lst.begin(), lst.end());\n\n    // Count frequencies using SIMD\n    __m256i zero = _mm256_setzero_si256();\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_set_epi32(\n            i + 7 < lst.size() ? lst[i + 7] : 0,\n            i + 6 < lst.size() ? lst[i + 6] : 0,\n            i + 5 < lst.size() ? lst[i + 5] : 0,\n            i + 4 < lst.size() ? lst[i + 4] : 0,\n            i + 3 < lst.size() ? lst[i + 3] : 0,\n            i + 2 < lst.size() ? lst[i + 2] : 0,\n            i + 1 < lst.size() ? lst[i + 1] : 0,\n            lst[i]\n        );\n\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(vec, j);\n            if (val > 0) {\n                frequency_map[val]++;\n            }\n        }\n    }\n\n    // Find the greatest integer meeting the condition\n    int result = -1;\n    for (const auto& [key, freq] : frequency_map) {\n        if (freq >= key && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int size = keys.size();\n    int max_value = -1;\n\n    // Step 3: Use AVX2 intrinsics to find the maximum satisfying condition\n    __m256i max_vec = _mm256_set1_epi32(-1);\n\n    for (int i = 0; i < size; i += 8) {\n        // Load keys and values into AVX registers\n        __m256i keys_vec = _mm256_loadu_si256((__m256i*)&keys[i]);\n        __m256i values_vec = _mm256_loadu_si256((__m256i*)&values[i]);\n\n        // Compare values >= keys\n        __m256i cmp_mask = _mm256_cmpgt_epi32(values_vec, keys_vec);\n        cmp_mask = _mm256_or_si256(cmp_mask, _mm256_cmpeq_epi32(values_vec, keys_vec));\n\n        // Mask keys with the comparison result\n        __m256i masked_keys = _mm256_and_si256(keys_vec, cmp_mask);\n\n        // Find the maximum value in the masked keys\n        max_vec = _mm256_max_epi32(max_vec, masked_keys);\n    }\n\n    // Extract the maximum value from the AVX register\n    alignas(32) int max_array[8];\n    _mm256_store_si256((__m256i*)max_array, max_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        max_value = std::max(max_value, max_array[i]);\n    }\n\n    return max_value > 0 ? max_value : -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays for SIMD processing\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    int max_result = -1;\n\n    // Step 3: Process keys and values using AVX2 intrinsics\n    for (int i = 0; i < n; i += 8) {\n        // Load keys and values into SIMD registers\n        __m256i key_vec = _mm256_loadu_si256((__m256i*)&keys[i]);\n        __m256i value_vec = _mm256_loadu_si256((__m256i*)&values[i]);\n\n        // Compare values >= keys\n        __m256i cmp_mask = _mm256_cmpgt_epi32(value_vec, key_vec);\n        cmp_mask = _mm256_or_si256(cmp_mask, _mm256_cmpeq_epi32(value_vec, key_vec));\n\n        // Mask keys where condition is true\n        __m256i masked_keys = _mm256_and_si256(key_vec, cmp_mask);\n\n        // Find the maximum value in masked_keys\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, masked_keys);\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > max_result) {\n                max_result = temp[j];\n            }\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq_map;\n\n    // Calculate frequencies using SIMD\n    size_t size = lst.size();\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_end = size - (size % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < simd_width; ++j) {\n            int value = _mm256_extract_epi32(vec, j);\n            freq_map[value]++;\n        }\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_end; i < size; ++i) {\n        freq_map[lst[i]]++;\n    }\n\n    // Find the greatest integer satisfying the condition\n    int result = -1;\n    for (const auto& [key, freq] : freq_map) {\n        if (freq >= key && key > result) {\n            result = key;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays for SIMD processing\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    if (n == 0) return -1;\n\n    // Step 3: Use AVX2 intrinsics to find the maximum valid number\n    __m256i max_result = _mm256_set1_epi32(-1); // Initialize max_result to -1\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < n; i += 8) {\n        // Load keys and values into SIMD registers\n        __m256i key_vec = _mm256_loadu_si256((__m256i*)&keys[i]);\n        __m256i value_vec = _mm256_loadu_si256((__m256i*)&values[i]);\n\n        // Compare values >= keys\n        __m256i cmp_mask = _mm256_cmpgt_epi32(value_vec, key_vec);\n        cmp_mask = _mm256_or_si256(cmp_mask, _mm256_cmpeq_epi32(value_vec, key_vec));\n\n        // Mask out invalid keys (keys <= 0)\n        __m256i valid_mask = _mm256_cmpgt_epi32(key_vec, zero);\n        cmp_mask = _mm256_and_si256(cmp_mask, valid_mask);\n\n        // Update max_result\n        __m256i masked_keys = _mm256_and_si256(key_vec, cmp_mask);\n        max_result = _mm256_max_epi32(max_result, masked_keys);\n    }\n\n    // Step 4: Extract the maximum value from the SIMD register\n    alignas(32) int max_array[8];\n    _mm256_store_si256((__m256i*)max_array, max_result);\n\n    int max_value = -1;\n    for (int i = 0; i < 8; ++i) {\n        max_value = std::max(max_value, max_array[i]);\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <algorithm>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    bool is_palindromic = true;\n    int half_size = n / 2;\n\n    for (int i = 0; i < half_size; i += 8) {\n        int remaining = std::min(8, half_size - i);\n\n        __m256i left = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&q[n - i - remaining]);\n\n        // Reverse the right side\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n\n        // Compare left and reversed right\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != -1) {\n            is_palindromic = false;\n            break;\n        }\n    }\n\n    if (!is_palindromic) return false;\n\n    // Calculate the sum of the vector\n    __m256i sum_vec = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        int remaining = std::min(8, n - i);\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the sum_vec\n    __m128i low = _mm256_castsi256_si128(sum_vec);\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi32(low, high);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    int total_sum = _mm_cvtsi128_si32(sum128);\n\n    // Check if the sum is less than or equal to the maximum weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n#include <algorithm>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    size_t half = n / 2;\n    for (size_t i = 0; i < half; i++) {\n        if (q[i] != q[n - i - 1]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    // Process 8 integers at a time\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the 256-bit vector\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; j++) {\n        total_sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; i++) {\n        total_sum += q[i];\n    }\n\n    // Check if the sum is less than or equal to the maximum weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n#include <algorithm>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n\n    // Check if the vector is palindromic (balanced)\n    bool is_palindromic = true;\n    int half_size = n / 2;\n    for (int i = 0; i < half_size; i++) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindromic = false;\n            break;\n        }\n    }\n\n    if (!is_palindromic) {\n        return false;\n    }\n\n    // Compute the sum of the elements using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX2 vector\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    int total_sum = std::accumulate(temp, temp + 8, 0);\n\n    // Add remaining elements\n    for (; i < n; i++) {\n        total_sum += q[i];\n    }\n\n    // Check if the sum is less than or equal to the maximum weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    int half = n / 2;\n    for (int i = 0; i < half; i += 8) {\n        int remaining = half - i;\n        int chunk_size = remaining >= 8 ? 8 : remaining;\n\n        // Load elements from the start and end of the vector\n        __m256i start = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i end = _mm256_loadu_si256((__m256i*)&q[n - i - chunk_size]);\n\n        // Reverse the end chunk\n        __m256i reversed_end = _mm256_permutevar8x32_epi32(end, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n\n        // Compare start and reversed end\n        __m256i cmp = _mm256_cmpeq_epi32(start, reversed_end);\n        if (_mm256_movemask_epi8(cmp) != -1) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of the vector\n    __m256i sum_vec = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        int remaining = n - i;\n        int chunk_size = remaining >= 8 ? 8 : remaining;\n\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, chunk);\n    }\n\n    // Horizontal sum of the vector\n    alignas(32) int32_t sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Check if the sum is less than or equal to the maximum weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <numeric>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    int half = n / 2;\n    bool is_palindromic = true;\n\n    for (int i = 0; i < half; i += 8) {\n        int remaining = std::min(8, half - i);\n\n        __m256i left = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&q[n - i - remaining]);\n\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            is_palindromic = false;\n            break;\n        }\n    }\n\n    if (!is_palindromic) return false;\n\n    // Calculate the sum of the elements\n    __m256i sum_vec = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        int remaining = std::min(8, n - i);\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the vector\n    int sum[8];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n    int total_sum = std::accumulate(sum, sum + 8, 0);\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Process the vector in chunks of 8 using AVX2 intrinsics\n    int i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the right vector\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n\n        // Compare left and reversed right\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += 8 - __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements sequentially\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int half = n / 2;\n    int changes = 0;\n\n    // Process in chunks of 8 using AVX2\n    int i = 0;\n    for (; i + 8 <= half; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the right vector\n        right = _mm256_permutevar8x32_epi32(right, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare left and right\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += 8 - (__builtin_popcount(mask) / 4); // Each int comparison contributes 4 bits\n    }\n\n    // Handle remaining elements\n    for (; i < half; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Process the vector in chunks of 8 integers using AVX2\n    int i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the right vector\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n\n        // Compare left and reversed right\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += 8 - __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements sequentially\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Iterate over the first half of the vector\n    for (int i = 0; i < n / 2; i += 8) {\n        // Load 8 integers from the left half\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Load 8 integers from the right half (reversed order)\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n        right = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm256_shuffle_epi8(right, _mm256_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n    }\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Process the vector in chunks of 8 integers using AVX2\n    int i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the right vector\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n\n        // Compare left and reversed_right\n        __m256i cmp = _mm256_cmpeq_epi32(left, reversed_right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += 8 - __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements sequentially\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        size_t size = lst.size();\n        int total_chars = 0;\n\n        for (size_t i = 0; i < size; i += 8) {\n            __m256i char_counts = _mm256_setzero_si256();\n\n            for (size_t j = 0; j < 8 && (i + j) < size; ++j) {\n                int len = lst[i + j].size();\n                __m256i len_vec = _mm256_set1_epi32(len);\n                char_counts = _mm256_add_epi32(char_counts, len_vec);\n            }\n\n            alignas(32) int buffer[8];\n            _mm256_store_si256((__m256i*)buffer, char_counts);\n\n            for (int k = 0; k < 8 && (i + k) < size; ++k) {\n                total_chars += buffer[k];\n            }\n        }\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        size_t i = 0;\n\n        // Process strings in chunks using AVX2\n        __m256i sum = _mm256_setzero_si256();\n        for (; i + 8 <= size; i += 8) {\n            __m256i lengths = _mm256_set_epi32(\n                lst[i].size(), lst[i + 1].size(), lst[i + 2].size(), lst[i + 3].size(),\n                lst[i + 4].size(), lst[i + 5].size(), lst[i + 6].size(), lst[i + 7].size()\n            );\n            sum = _mm256_add_epi32(sum, lengths);\n        }\n\n        // Horizontal sum of the AVX2 register\n        int buffer[8];\n        _mm256_storeu_si256((__m256i*)buffer, sum);\n        for (int j = 0; j < 8; ++j) {\n            total_chars += buffer[j];\n        }\n\n        // Process remaining strings\n        for (; i < size; ++i) {\n            total_chars += lst[i].size();\n        }\n\n        return total_chars;\n    };\n\n    int total1 = calculate_total_chars_simd(lst1);\n    int total2 = calculate_total_chars_simd(lst2);\n\n    return (total1 <= total2) ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        size_t i = 0;\n\n        // Process strings in chunks using AVX2\n        for (; i + 8 <= size; i += 8) {\n            __m256i char_counts = _mm256_setzero_si256();\n\n            for (size_t j = 0; j < 8; ++j) {\n                size_t len = lst[i + j].size();\n                char_counts = _mm256_add_epi32(char_counts, _mm256_set1_epi32(len));\n            }\n\n            // Sum up the values in the AVX2 register\n            alignas(32) int buffer[8];\n            _mm256_store_si256((__m256i*)buffer, char_counts);\n            for (int k = 0; k < 8; ++k) {\n                total_chars += buffer[k];\n            }\n        }\n\n        // Process remaining strings\n        for (; i < size; ++i) {\n            total_chars += lst[i].size();\n        }\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        size_t i = 0;\n\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; i + 8 <= size; i += 8) {\n            __m256i lengths_vec = _mm256_set_epi32(\n                lst[i].size(), lst[i + 1].size(), lst[i + 2].size(), lst[i + 3].size(),\n                lst[i + 4].size(), lst[i + 5].size(), lst[i + 6].size(), lst[i + 7].size()\n            );\n            sum_vec = _mm256_add_epi32(sum_vec, lengths_vec);\n        }\n\n        int buffer[8];\n        _mm256_storeu_si256((__m256i*)buffer, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            total_chars += buffer[j];\n        }\n\n        for (; i < size; ++i) {\n            total_chars += lst[i].size();\n        }\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        size_t i = 0;\n\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (; i + 8 <= size; i += 8) {\n            __m256i lengths_vec = _mm256_set_epi32(\n                lst[i].size(), lst[i + 1].size(), lst[i + 2].size(), lst[i + 3].size(),\n                lst[i + 4].size(), lst[i + 5].size(), lst[i + 6].size(), lst[i + 7].size()\n            );\n            sum_vec = _mm256_add_epi32(sum_vec, lengths_vec);\n        }\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            total_chars += temp[j];\n        }\n\n        for (; i < size; ++i) {\n            total_chars += lst[i].size();\n        }\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n^0 = 1 for any n > 0\n    if (n < 2) return false; // n must be >= 2 for valid powers\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 64; i++) {\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n        __m256i cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(cmp)) {\n            return true;\n        }\n        // Break early if vec_result exceeds x\n        __m256i overflow_cmp = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(overflow_cmp)) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n**0 = 1 for any n > 0\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 64; i++) {\n        vec_result = _mm256_mullo_epi64(vec_result, vec_n);\n        __m256i cmp = _mm256_cmpeq_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(cmp)) {\n            return true;\n        }\n        __m256i overflow_check = _mm256_cmpgt_epi64(vec_result, vec_x);\n        if (_mm256_movemask_epi8(overflow_check)) {\n            return false;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n**0 = 1 for any n > 0\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n\n    uint64_t powers[4] = {1, 1, 1, 1};\n    while (true) {\n        vec_result = _mm256_mul_epu32(vec_result, vec_n);\n        _mm256_storeu_si256((__m256i *)powers, vec_result);\n\n        for (int i = 0; i < 4; i++) {\n            if (powers[i] == x) return true;\n            if (powers[i] > x) return false;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n^0 = 1\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m256i vec_n = _mm256_set1_epi64x(n); // Broadcast n to all lanes\n    __m256i vec_x = _mm256_set1_epi64x(x); // Broadcast x to all lanes\n    __m256i vec_pow = _mm256_set1_epi64x(1); // Initialize power to 1\n\n    while (true) {\n        vec_pow = _mm256_mul_epu32(vec_pow, vec_n); // Multiply current power by n\n\n        // Compare vec_pow with vec_x\n        __m256i cmp_eq = _mm256_cmpeq_epi64(vec_pow, vec_x);\n        __m256i cmp_gt = _mm256_cmpgt_epi64(vec_pow, vec_x);\n\n        // Check if any lane matches x\n        if (_mm256_testz_si256(cmp_eq, cmp_eq) == 0) {\n            return true; // Found a match\n        }\n\n        // Check if any lane exceeds x\n        if (_mm256_testz_si256(cmp_gt, cmp_gt) == 0) {\n            return false; // Exceeded x\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n**0 = 1\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_x = _mm256_set1_epi64x(x);\n    __m256i vec_pow = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 64; i++) {\n        vec_pow = _mm256_mul_epu32(vec_pow, vec_n); // Compute powers of n\n        __m256i cmp = _mm256_cmpeq_epi64(vec_pow, vec_x); // Compare with x\n        if (_mm256_movemask_epi8(cmp)) return true; // If match found, return true\n        if (_mm256_testz_si256(vec_pow, vec_x)) break; // Stop if vec_pow exceeds x\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    if (num.empty()) return 0;\n\n    const char prime_hex_digits[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(prime_hex_digits) / sizeof(prime_hex_digits[0]);\n\n    int count = 0;\n    size_t len = num.size();\n\n    // Load prime hex digits into an AVX2 register\n    __m256i prime_digits = _mm256_setr_epi8(\n        prime_hex_digits[0], prime_hex_digits[1], prime_hex_digits[2], prime_hex_digits[3],\n        prime_hex_digits[4], prime_hex_digits[5], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the input string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character in the input with the prime hex digits\n        __m256i result = _mm256_setzero_si256();\n        for (int j = 0; j < prime_count; ++j) {\n            __m256i prime_digit = _mm256_set1_epi8(prime_hex_digits[j]);\n            __m256i cmp = _mm256_cmpeq_epi8(input, prime_digit);\n            result = _mm256_or_si256(result, cmp);\n        }\n\n        // Count the number of matches\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        for (int j = 0; j < prime_count; ++j) {\n            if (c == prime_hex_digits[j]) {\n                count++;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(primes) / sizeof(primes[0]);\n\n    int count = 0;\n    size_t len = num.size();\n\n    // Load prime characters into an AVX2 register\n    __m256i prime_chars = _mm256_setr_epi8(\n        primes[0], primes[1], primes[2], primes[3], primes[4], primes[5],\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the input string\n        __m256i input_chars = _mm256_loadu_si256((__m256i *)(num.data() + i));\n\n        // Compare each character in input_chars with prime_chars\n        __m256i result = _mm256_setzero_si256();\n        for (int j = 0; j < prime_count; ++j) {\n            __m256i prime_char = _mm256_set1_epi8(primes[j]);\n            __m256i cmp = _mm256_cmpeq_epi8(input_chars, prime_char);\n            result = _mm256_or_si256(result, cmp);\n        }\n\n        // Count the number of matches\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = num[i];\n        for (int j = 0; j < prime_count; ++j) {\n            if (c == primes[j]) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    if (num.empty()) return 0;\n\n    const char prime_hex_digits[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = 6;\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    __m256i prime_digits = _mm256_setr_epi8(\n        '2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        '2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (; i + 31 < len; i += 32) {\n        __m256i input = _mm256_loadu_si256((__m256i *)(num.data() + i));\n        __m256i result = _mm256_setzero_si256();\n\n        for (int j = 0; j < prime_count; ++j) {\n            __m256i prime = _mm256_set1_epi8(prime_hex_digits[j]);\n            __m256i cmp = _mm256_cmpeq_epi8(input, prime);\n            result = _mm256_or_si256(result, cmp);\n        }\n\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        for (int j = 0; j < prime_count; ++j) {\n            if (c == prime_hex_digits[j]) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(primes) / sizeof(primes[0]);\n\n    int count = 0;\n    size_t len = num.size();\n\n    // Load prime characters into an AVX2 register\n    __m256i prime_chars = _mm256_setr_epi8(\n        primes[0], primes[1], primes[2], primes[3], primes[4], primes[5], 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i < len; i += 32) {\n        // Load up to 32 characters from the input string\n        __m256i input_chars;\n        if (i + 32 <= len) {\n            input_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        } else {\n            char temp[32] = {0};\n            size_t remaining = len - i;\n            for (size_t j = 0; j < remaining; ++j) {\n                temp[j] = num[i + j];\n            }\n            input_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp));\n        }\n\n        // Compare input characters with prime characters\n        __m256i result = _mm256_setzero_si256();\n        for (int j = 0; j < prime_count; ++j) {\n            __m256i prime_char = _mm256_set1_epi8(primes[j]);\n            __m256i cmp = _mm256_cmpeq_epi8(input_chars, prime_char);\n            result = _mm256_or_si256(result, cmp);\n        }\n\n        // Count matches\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    // Hexadecimal prime digits: 2, 3, 5, 7, B (11), D (13)\n    const char prime_digits[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(prime_digits) / sizeof(prime_digits[0]);\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Load prime digits into an AVX2 register\n    __m256i prime_vec = _mm256_setr_epi8(\n        prime_digits[0], prime_digits[1], prime_digits[2], prime_digits[3],\n        prime_digits[4], prime_digits[5], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    // Process 32 characters at a time\n    while (i + 32 <= len) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i *)(num.data() + i));\n        __m256i result = _mm256_setzero_si256();\n\n        for (int j = 0; j < prime_count; ++j) {\n            __m256i prime_char = _mm256_set1_epi8(prime_digits[j]);\n            __m256i cmp = _mm256_cmpeq_epi8(input_vec, prime_char);\n            result = _mm256_or_si256(result, cmp);\n        }\n\n        // Count matches\n        int mask = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n\n        i += 32;\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        for (int j = 0; j < prime_count; ++j) {\n            if (num[i] == prime_digits[j]) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 32) {\n        size_t remaining = std::min(n - i, (size_t)32);\n\n        __m256i chunk1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i chunk2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 1));\n        __m256i chunk3 = _mm256_loadu_si256((__m256i*)(s.data() + i + 2));\n\n        __m256i cmp12 = _mm256_cmpeq_epi8(chunk1, chunk2);\n        __m256i cmp13 = _mm256_cmpeq_epi8(chunk1, chunk3);\n        __m256i cmp23 = _mm256_cmpeq_epi8(chunk2, chunk3);\n\n        __m256i any_equal = _mm256_or_si256(_mm256_or_si256(cmp12, cmp13), cmp23);\n\n        if (_mm256_movemask_epi8(any_equal) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 32) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 32 ? 32 : remaining;\n\n        __m256i chars1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i chars2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 1));\n        __m256i chars3 = _mm256_loadu_si256((__m256i*)(s.data() + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(chars1, chars2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chars2, chars3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chars1, chars3);\n\n        __m256i or_cmp = _mm256_or_si256(cmp1, _mm256_or_si256(cmp2, cmp3));\n        int mask = _mm256_movemask_epi8(or_cmp);\n\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 32) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 32 ? 32 : remaining;\n\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 1));\n        __m256i v3 = _mm256_loadu_si256((__m256i*)(s.data() + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(v2, v3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(v1, v3);\n\n        __m256i combined = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        int mask = _mm256_movemask_epi8(combined);\n\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 32) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 32 ? 32 : remaining;\n\n        __m256i v1 = _mm256_loadu_si256((const __m256i*)(s.data() + i));\n        __m256i v2 = _mm256_loadu_si256((const __m256i*)(s.data() + i + 1));\n        __m256i v3 = _mm256_loadu_si256((const __m256i*)(s.data() + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(v2, v3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(v1, v3);\n\n        __m256i result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n\n        if (_mm256_movemask_epi8(result) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 32) {\n        size_t remaining = std::min(n - i, (size_t)32);\n        if (remaining < 3) break;\n\n        __m256i chars1 = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i chars2 = _mm256_loadu_si256((__m256i*)(s.data() + i + 1));\n        __m256i chars3 = _mm256_loadu_si256((__m256i*)(s.data() + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(chars1, chars2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(chars2, chars3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(chars1, chars3);\n\n        __m256i combined = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), cmp3);\n        int mask = _mm256_movemask_epi8(combined);\n\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.size();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m256i vec_length = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 8) {\n        __m256i vec_divisors = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec_remainders = _mm256_rem_epi32(vec_length, vec_divisors);\n        __m256i vec_zeros = _mm256_setzero_si256();\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vec_remainders, vec_zeros))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) {\n        return false; // Numbers less than 2 are not prime\n    }\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m256i vec_length = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 8) {\n        __m256i vec_divisors = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec_remainders = _mm256_rem_epi32(vec_length, vec_divisors);\n        __m256i vec_zeros = _mm256_setzero_si256();\n        __m256i cmp_result = _mm256_cmpeq_epi32(vec_remainders, vec_zeros);\n\n        if (_mm256_movemask_epi8(cmp_result)) {\n            return false; // Found a divisor\n        }\n    }\n\n    return true; // No divisors found, length is prime\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length)) + 1;\n\n    __m256i vec_length = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 8) {\n        __m256i vec_divisors = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec_remainders = _mm256_rem_epi32(vec_length, vec_divisors);\n\n        __m256i vec_zeros = _mm256_setzero_si256();\n        __m256i cmp_result = _mm256_cmpeq_epi32(vec_remainders, vec_zeros);\n\n        if (_mm256_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len < 2) return false;\n\n    int sqrt_len = static_cast<int>(std::sqrt(len));\n    __m256i vec_len = _mm256_set1_epi32(len);\n\n    for (int i = 2; i <= sqrt_len; i += 8) {\n        __m256i vec_divisors = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec_remainders = _mm256_rem_epi32(vec_len, vec_divisors);\n        __m256i vec_zeros = _mm256_setzero_si256();\n        __m256i cmp_result = _mm256_cmpeq_epi32(vec_remainders, vec_zeros);\n\n        if (_mm256_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.size();\n    if (length < 2) return false;\n\n    int sqrt_len = static_cast<int>(std::sqrt(length));\n    __m256i divisor_vec = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_len; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i remainders = _mm256_rem_epi32(divisor_vec, indices);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(remainders, zero_vec);\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract its digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate an array to store the digits\n    alignas(32) int digits[32] = {0};\n    for (size_t i = 0; i < len; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Use AVX2 to sum the digits\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    // Process 8 digits at a time using AVX2\n    for (; i + 8 <= len; i += 8) {\n        __m256i digit_vec = _mm256_load_si256((__m256i*)&digits[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n    }\n\n    // Horizontal sum of the vector\n    int sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n    }\n\n    // Process remaining digits\n    for (; i < len; ++i) {\n        total_sum += digits[i];\n    }\n\n    // Convert the sum to binary\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = (char)((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate an array to store the digits\n    alignas(32) int digits[32] = {0};\n    for (size_t i = 0; i < len; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Load digits into SIMD registers\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 8 <= len; i += 8) {\n        __m256i digit_vec = _mm256_load_si256((__m256i*)(digits + i));\n        sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n    }\n\n    // Horizontal sum of the SIMD register\n    alignas(32) int sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n    }\n\n    // Handle remaining digits\n    for (; i < len; ++i) {\n        total_sum += digits[i];\n    }\n\n    // Convert the sum to binary\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = char('0' + (total_sum % 2)) + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract individual digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate an array to store the digits\n    alignas(32) int digits[32] = {0};\n    for (size_t i = 0; i < len; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Load digits into AVX2 registers\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 8 <= len; i += 8) {\n        __m256i digit_vec = _mm256_load_si256((__m256i*)&digits[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n    }\n\n    // Horizontal sum of the vector\n    alignas(32) int sum_array[8];\n    _mm256_store_si256((__m256i*)sum_array, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n    }\n\n    // Handle remaining digits\n    for (; i < len; ++i) {\n        total_sum += digits[i];\n    }\n\n    // Convert the total sum to binary\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = (char)((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Load digits into an array of integers\n    std::vector<int> digits(len);\n    for (size_t i = 0; i < len; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Sum the digits using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    // Process 8 digits at a time\n    for (; i + 8 <= len; i += 8) {\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)&digits[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec);\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i sum128 = _mm_add_epi32(low, high);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    sum128 = _mm_hadd_epi32(sum128, sum128);\n    int total_sum = _mm_cvtsi128_si32(sum128);\n\n    // Process remaining digits\n    for (; i < len; ++i) {\n        total_sum += digits[i];\n    }\n\n    // Convert the sum to binary string\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = char((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate an array to store the digits\n    std::vector<int> digits(len);\n    for (size_t i = 0; i < len; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Sum the digits using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    // Process 8 digits at a time (AVX2 can handle 8 integers in parallel)\n    for (; i + 8 <= len; i += 8) {\n        __m256i digit_vec = _mm256_loadu_si256((__m256i*)&digits[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n    }\n\n    // Horizontal sum of the vector\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += temp[j];\n    }\n\n    // Process remaining digits\n    for (; i < len; ++i) {\n        total_sum += digits[i];\n    }\n\n    // Convert the total sum to binary\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = char((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(1); // Mask for checking even numbers\n    __m256i mask_odd_indices = _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0); // Mask for odd indices\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]); // Load 8 integers\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i); // Generate indices\n\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec, mask_even), _mm256_setzero_si256()); // Check even numbers\n        __m256i odd_index_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_odd_indices), mask_odd_indices); // Check odd indices\n\n        __m256i valid_mask = _mm256_and_si256(even_mask, odd_index_mask); // Combine masks\n        __m256i filtered = _mm256_and_si256(vec, valid_mask); // Apply mask to elements\n\n        vec_sum = _mm256_add_epi32(vec_sum, filtered); // Accumulate valid elements\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vec_sum); // Store result\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n\n    for (int i = 1; i < n; i += 16) {\n        __m256i indices = _mm256_set_epi32(i + 14, i + 12, i + 10, i + 8, i + 6, i + 4, i + 2, i);\n        __m256i values = _mm256_i32gather_epi32(lst.data(), indices, 4);\n\n        __m256i mask = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n        __m256i filtered_values = _mm256_and_si256(values, mask);\n        vec_sum = _mm256_add_epi32(vec_sum, filtered_values);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(1); // Mask for even numbers\n    __m256i mask_odd_indices = _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0); // Mask for odd indices\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]); // Load 8 integers\n        __m256i vec_indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i); // Indices\n\n        __m256i vec_even = _mm256_and_si256(vec, mask_even); // Check even numbers\n        __m256i vec_odd_mask = _mm256_and_si256(vec_indices, mask_odd_indices); // Check odd indices\n\n        __m256i vec_valid = _mm256_and_si256(vec_even, vec_odd_mask); // Combine masks\n        vec_sum = _mm256_add_epi32(vec_sum, vec_valid); // Add valid elements\n    }\n\n    // Horizontal sum of vec_sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vec_sum);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int size = lst.size();\n    if (size < 2) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even = _mm256_set1_epi32(1); // Mask for even numbers\n    __m256i mask_odd_indices = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1); // Mask for odd indices\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, mask_even), _mm256_setzero_si256());\n        __m256i odd_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_odd_indices), mask_odd_indices);\n\n        __m256i valid_mask = _mm256_and_si256(even_mask, odd_mask);\n        __m256i filtered_data = _mm256_and_si256(data, valid_mask);\n\n        sum_vec = _mm256_add_epi32(sum_vec, filtered_data);\n    }\n\n    int sum[8];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n\n    int total = 0;\n    for (int j = 0; j < 8; ++j) {\n        total += sum[j];\n    }\n\n    for (; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            total += lst[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int size = lst.size();\n    if (size < 2) return 0;\n\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n\n    for (int i = 1; i < size; i += 16) {\n        __m256i indices = _mm256_set_epi32(i + 14, i + 12, i + 10, i + 8, i + 6, i + 4, i + 2, i);\n        __m256i values = _mm256_i32gather_epi32(lst.data(), indices, 4);\n\n        __m256i mask = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n        for (int j = 0; j < 8; j++) {\n            mask[j] = (lst[j] %2==0 )"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const int block_size = 32; // AVX2 processes 32 bytes at a time\n    const __m256i shift = _mm256_set1_epi8(8); // Rotate by 2 * 2 = 8 places\n\n    size_t i = 0;\n    for (; i + block_size <= len; i += block_size) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), encrypted);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 8;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n\n    const int shift = 4; // 2 * 2 places\n    const __m256i shift_vec = _mm256_set1_epi8(shift);\n    const __m256i lower_bound = _mm256_set1_epi8('a');\n    const __m256i upper_bound = _mm256_set1_epi8('z');\n\n    size_t i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Check if characters are within 'a' to 'z'\n        __m256i is_alpha = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_bound),\n            _mm256_cmpgt_epi8(upper_bound, chars)\n        );\n\n        // Add shift to characters\n        __m256i shifted_chars = _mm256_add_epi8(chars, shift_vec);\n\n        // Wrap around if shifted_chars exceed 'z'\n        __m256i wrap_mask = _mm256_cmpgt_epi8(shifted_chars, upper_bound);\n        __m256i wrapped_chars = _mm256_sub_epi8(shifted_chars, _mm256_set1_epi8(26));\n\n        // Select wrapped or shifted characters based on wrap_mask\n        __m256i final_chars = _mm256_blendv_epi8(shifted_chars, wrapped_chars, wrap_mask);\n\n        // Apply mask to keep non-alphabet characters unchanged\n        final_chars = _mm256_blendv_epi8(chars, final_chars, is_alpha);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_chars);\n    }\n\n    // Process remaining characters\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            char shifted = c + shift;\n            if (shifted > 'z') {\n                shifted -= 26;\n            }\n            result[i] = shifted;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const int block_size = 32; // AVX2 processes 32 bytes at a time\n    const __m256i shift = _mm256_set1_epi8(8); // Rotate by 2 * 2 = 8 places\n\n    size_t i = 0;\n    for (; i + block_size <= len; i += block_size) {\n        // Load 32 bytes from the input string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n\n        // Add the shift value to each byte\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n\n        // Store the result in the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), encrypted);\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        result[i] = s[i] + 8;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const int block_size = 32; // AVX2 processes 32 bytes at a time\n    size_t i = 0;\n\n    __m256i shift = _mm256_set1_epi8(8); // Shift value (2 * 2 = 4 places down the alphabet)\n\n    for (; i + block_size <= len; i += block_size) {\n        // Load 32 characters from the input string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Add the shift value to each character\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n\n        // Store the result back to the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 8;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const int shift = 2 * 2; // Rotate alphabet by 4 places\n    const char lower_bound = 'a';\n    const char upper_bound = 'z';\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i lower_bound_vec = _mm256_set1_epi8(lower_bound);\n        __m256i upper_bound_vec = _mm256_set1_epi8(upper_bound);\n        __m256i shift_vec = _mm256_set1_epi8(shift);\n\n        __m256i mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, lower_bound_vec),\n            _mm256_cmpgt_epi8(upper_bound_vec, chars)\n        );\n\n        __m256i rotated = _mm256_add_epi8(chars, shift_vec);\n        rotated = _mm256_sub_epi8(rotated, _mm256_and_si256(\n            _mm256_cmpgt_epi8(rotated, upper_bound_vec),\n            _mm256_set1_epi8(26)\n        ));\n\n        __m256i result_vec = _mm256_blendv_epi8(chars, rotated, mask);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), result_vec);\n    }\n\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= lower_bound && c <= upper_bound) {\n            c += shift;\n            if (c > upper_bound) {\n                c -= 26;\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n    const __m256i vowel_mask = _mm256_set1_epi8(2);  // Offset for vowels\n\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Swap case\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, lower_a), _mm256_cmpgt_epi8(lower_z, chunk));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_a), _mm256_cmpgt_epi8(upper_z, chunk));\n        __m256i toggle_case = _mm256_or_si256(is_lower, is_upper);\n        __m256i swapped_case = _mm256_xor_si256(chunk, _mm256_and_si256(toggle_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(swapped_case, vowels_lower);\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(swapped_case, vowels_upper);\n        __m256i is_vowel = _mm256_or_si256(is_vowel_lower, is_vowel_upper);\n        __m256i vowel_replacement = _mm256_add_epi8(swapped_case, _mm256_and_si256(is_vowel, vowel_mask));\n\n        // Store result\n        _mm256_storeu_si256((__m256i*)&result[i], vowel_replacement);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20; // Swap case\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c += 2; // Replace vowel\n            }\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20; // Swap case\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c += 2; // Replace vowel\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n    const __m256i vowel_mask = _mm256_set1_epi8(2);   // Offset for vowels\n\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'a', 'e', 'i', 'o', 'u', 'a', 'e', 'i', 'o', 'u', 'a', 'e', 'i', 'o', 'u', 'a', 'e', 'i', 'o', 'u', 'a', 'e', 'i', 'o', 'u');\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U', 'A', 'E', 'I', 'O', 'U');\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)(message.data() + i));\n\n        // Swap case\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, lower_a), _mm256_cmpgt_epi8(lower_z, chunk));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_a), _mm256_cmpgt_epi8(upper_z, chunk));\n        __m256i toggle_case = _mm256_or_si256(is_lower, is_upper);\n        __m256i swapped_case = _mm256_xor_si256(chunk, _mm256_and_si256(toggle_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(swapped_case, vowels_lower);\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(swapped_case, vowels_upper);\n        __m256i is_vowel = _mm256_or_si256(is_vowel_lower, is_vowel_upper);\n        __m256i vowel_replacement = _mm256_add_epi8(swapped_case, _mm256_and_si256(is_vowel, vowel_mask));\n\n        // Store result\n        _mm256_storeu_si256((__m256i *)(result.data() + i), vowel_replacement);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        char swapped = (c >= 'a' && c <= 'z') ? c - 32 : (c >= 'A' && c <= 'Z') ? c + 32 : c;\n        if (swapped == 'a' || swapped == 'e' || swapped == 'i' || swapped == 'o' || swapped == 'u' ||\n            swapped == 'A' || swapped == 'E' || swapped == 'I' || swapped == 'O' || swapped == 'U') {\n            swapped += 2;\n        }\n        result[i] = swapped;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowel_shift = _mm256_set1_epi8(2); // Shift vowels by 2 places\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)&message[i]);\n\n        // Swap case\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, lower_a), _mm256_cmpgt_epi8(lower_z, chunk));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_a), _mm256_cmpgt_epi8(upper_z, chunk));\n        __m256i toggle_case = _mm256_or_si256(is_lower, is_upper);\n        __m256i swapped_case = _mm256_xor_si256(chunk, _mm256_and_si256(toggle_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i shifted_vowel = _mm256_add_epi8(swapped_case, _mm256_and_si256(is_vowel, vowel_shift));\n\n        // Store result\n        _mm256_storeu_si256((__m256i *)&result[i], shifted_vowel);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20; // Swap case\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20; // Swap case\n        }\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2; // Shift vowels\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n    const __m256i vowel_mask = _mm256_set1_epi8(2);  // Offset for vowels\n\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(message.data() + i));\n\n        // Swap case\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chars, lower_a), _mm256_cmpgt_epi8(lower_z, chars));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chars, upper_a), _mm256_cmpgt_epi8(upper_z, chars));\n        __m256i toggle_case = _mm256_or_si256(is_lower, is_upper);\n        __m256i swapped_case = _mm256_xor_si256(chars, _mm256_and_si256(toggle_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(swapped_case, vowels_lower);\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(swapped_case, vowels_upper);\n        __m256i is_vowel = _mm256_or_si256(is_vowel_lower, is_vowel_upper);\n        __m256i replaced_vowel = _mm256_add_epi8(swapped_case, _mm256_and_si256(is_vowel, vowel_mask));\n\n        // Store result\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced_vowel);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        char swapped = (c >= 'a' && c <= 'z') ? c - 32 : (c >= 'A' && c <= 'Z') ? c + 32 : c;\n        if (swapped == 'a' || swapped == 'e' || swapped == 'i' || swapped == 'o' || swapped == 'u' ||\n            swapped == 'A' || swapped == 'E' || swapped == 'I' || swapped == 'O' || swapped == 'U') {\n            swapped += 2;\n        }\n        result[i] = swapped;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_a = _mm256_set1_epi8('a');\n    const __m256i lower_z = _mm256_set1_epi8('z');\n    const __m256i upper_a = _mm256_set1_epi8('A');\n    const __m256i upper_z = _mm256_set1_epi8('Z');\n    const __m256i case_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n    const __m256i vowel_mask = _mm256_set1_epi8(2);   // Offset for vowels\n\n    const __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)(message.data() + i));\n\n        // Swap case\n        __m256i is_lower = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, lower_a),\n            _mm256_cmpgt_epi8(lower_z, chunk)\n        );\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chunk, upper_a),\n            _mm256_cmpgt_epi8(upper_z, chunk)\n        );\n        __m256i toggle_case = _mm256_or_si256(is_lower, is_upper);\n        chunk = _mm256_xor_si256(chunk, _mm256_and_si256(toggle_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        chunk = _mm256_add_epi8(chunk, _mm256_and_si256(is_vowel, vowel_mask));\n\n        _mm256_storeu_si256((__m256i *)(result.data() + i), chunk);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c ^= 0x20; // Toggle case\n        } else if (c >= 'A' && c <= 'Z') {\n            c ^= 0x20; // Toggle case\n        }\n\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            c += 2; // Replace vowel\n        }\n\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) {\n            return false;\n        }\n\n        size_t i = 0;\n        while (i + 32 <= len) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n\n            __m256i lower_bound = _mm256_set1_epi8('a');\n            __m256i upper_bound = _mm256_set1_epi8('z');\n            __m256i upper_lower_bound = _mm256_set1_epi8('A');\n            __m256i upper_upper_bound = _mm256_set1_epi8('Z');\n\n            __m256i is_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, lower_bound),\n                _mm256_cmpgt_epi8(upper_bound, chars)\n            );\n\n            __m256i is_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chars, upper_lower_bound),\n                _mm256_cmpgt_epi8(upper_upper_bound, chars)\n            );\n\n            if (_mm256_testz_si256(is_lower, is_lower) == 0) {\n                all_upper = false;\n            }\n\n            if (_mm256_testz_si256(is_upper, is_upper) == 0) {\n                all_lower = false;\n            }\n\n            i += 32;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (c >= 'a' && c <= 'z') {\n                all_upper = false;\n            } else if (c >= 'A' && c <= 'Z') {\n                all_lower = false;\n            } else {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) return false;\n\n        size_t i = 0;\n        while (i + 32 <= len) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(key.data() + i));\n            __m256i lower_bound = _mm256_set1_epi8('a');\n            __m256i upper_bound = _mm256_set1_epi8('z');\n            __m256i upper_lower_bound = _mm256_set1_epi8('A');\n            __m256i upper_upper_bound = _mm256_set1_epi8('Z');\n\n            __m256i is_lower = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chunk, lower_bound),\n                _mm256_cmpgt_epi8(upper_bound, chunk)\n            );\n\n            __m256i is_upper = _mm256_and_si256(\n                _mm256_cmpgt_epi8(chunk, upper_lower_bound),\n                _mm256_cmpgt_epi8(upper_upper_bound, chunk)\n            );\n\n            if (_mm256_testz_si256(is_lower, is_lower)) all_lower = false;\n            if (_mm256_testz_si256(is_upper, is_upper)) all_upper = false;\n\n            i += 32;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (c >= 'a' && c <= 'z') {\n                all_upper = false;\n            } else if (c >= 'A' && c <= 'Z') {\n                all_lower = false;\n            } else {\n                return false;\n            }\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        // Process the key in chunks of 32 bytes using AVX2\n        size_t i = 0;\n        while (i + 32 <= len) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n            \n            // Check for lowercase\n            __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('z')),\n                                                  _mm256_cmpgt_epi8(_mm256_set1_epi8('a' - 1), chunk));\n            if (_mm256_movemask_epi8(lower_mask) != 0) {\n                all_lower = false;\n            }\n\n            // Check for uppercase\n            __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z')),\n                                                  _mm256_cmpgt_epi8(_mm256_set1_epi8('A' - 1), chunk));\n            if (_mm256_movemask_epi8(upper_mask) != 0) {\n                all_upper = false;\n            }\n\n            i += 32;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!std::islower(c)) all_lower = false;\n            if (!std::isupper(c)) all_upper = false;\n        }\n\n        // If neither all_lower nor all_upper is true, return false\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) {\n            return false;\n        }\n\n        size_t i = 0;\n        while (i + 32 <= len) {\n            __m256i chars = _mm256_loadu_si256((const __m256i*)(key.data() + i));\n            __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('z')),\n                                                  _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), chars));\n            __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('Z')),\n                                                  _mm256_cmpgt_epi8(_mm256_set1_epi8('A'), chars));\n\n            if (!_mm256_testz_si256(lower_mask, lower_mask)) {\n                all_lower = false;\n            }\n            if (!_mm256_testz_si256(upper_mask, upper_mask)) {\n                all_upper = false;\n            }\n\n            i += 32;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (c < 'a' || c > 'z') {\n                all_lower = false;\n            }\n            if (c < 'A' || c > 'Z') {\n                all_upper = false;\n            }\n        }\n\n        if (!all_lower && !all_upper) {\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        // Process the key in chunks of 32 bytes using AVX2\n        size_t i = 0;\n        while (i + 32 <= len) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n            __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('a' - 1)),\n                                                  _mm256_cmplt_epi8(chars, _mm256_set1_epi8('z' + 1)));\n            __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n                                                  _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z' + 1)));\n\n            if (_mm256_testz_si256(lower_mask, lower_mask)) all_lower = false;\n            if (_mm256_testz_si256(upper_mask, upper_mask)) all_upper = false;\n\n            i += 32;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (!std::islower(c)) all_lower = false;\n            if (!std::isupper(c)) all_upper = false;\n        }\n\n        // If neither all_lower nor all_upper is true, return false\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Create a mask for numbers from 2 to n-1\n    int size = n - 1;\n    std::vector<int> is_prime(size, 1);\n    is_prime[0] = 0; // 1 is not prime\n\n    int sqrt_n = static_cast<int>(std::sqrt(n)) + 1;\n\n    for (int i = 2; i <= sqrt_n; ++i) {\n        if (is_prime[i - 1]) {\n            // Use SIMD to mark multiples of i as non-prime\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j - 1] = 0;\n            }\n        }\n    }\n\n    // Collect primes using SIMD\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i - 1]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    int limit = n;\n    int sieve_size = (limit + 31) / 32; // Number of 32-bit integers needed for the sieve\n    std::vector<uint32_t> sieve(sieve_size, 0xFFFFFFFF); // Initialize sieve with all bits set\n\n    sieve[0] &= ~0x3; // Mark 0 and 1 as non-prime\n\n    for (int i = 2; i * i < limit; ++i) {\n        if (sieve[i / 32] & (1U << (i % 32))) { // Check if i is prime\n            for (int j = i * i; j < limit; j += i) {\n                sieve[j / 32] &= ~(1U << (j % 32)); // Mark multiples of i as non-prime\n            }\n        }\n    }\n\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7); // Vector of indices\n    __m256i increment = _mm256_set1_epi32(8); // Increment vector\n    for (int i = 0; i < limit; i += 8) {\n        __m256i mask = _mm256_setzero_si256();\n        for (int j = 0; j < 8; ++j) {\n            int idx = i + j;\n            if (idx < limit && (sieve[idx / 32] & (1U << (idx % 32)))) {\n                mask = _mm256_insert_epi32(mask, idx, j);\n            }\n        }\n        for (int j = 0; j < 8; ++j) {\n            int prime_candidate = _mm256_extract_epi32(mask, j);\n            if (prime_candidate < limit && (sieve[prime_candidate / 32] & (1U << (prime_candidate % 32)))) {\n                primes.push_back(prime_candidate);\n            }\n        }\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Create a mask for numbers from 2 to n-1\n    int size = n - 2;\n    std::vector<int> is_prime(size, 1);\n\n    // Use SIMD to mark non-prime numbers\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i - 2]) {\n            __m256i vec_i = _mm256_set1_epi32(i);\n            for (int j = i * i; j < n; j += 8 * i) {\n                __m256i vec_j = _mm256_set_epi32(j + 7 * i, j + 6 * i, j + 5 * i, j + 4 * i, j + 3 * i, j + 2 * i, j + i, j);\n                __m256i vec_mod = _mm256_rem_epi32(vec_j, vec_i);\n                __m256i mask = _mm256_cmpeq_epi32(vec_mod, _mm256_setzero_si256());\n                _mm256_storeu_si256((__m256i*)&is_prime[j - 2], mask);\n            }\n        }\n    }\n\n    // Collect prime numbers\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i - 2]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    int limit = n;\n    int sieve_size = (limit + 31) / 32; // Number of 32-bit integers needed for the sieve\n    std::vector<uint32_t> sieve(sieve_size, 0xFFFFFFFF); // Initialize all bits to 1\n\n    sieve[0] &= ~0x3; // Mark 0 and 1 as non-prime\n\n    int sqrt_limit = static_cast<int>(sqrt(limit));\n    for (int i = 2; i <= sqrt_limit; ++i) {\n        if (sieve[i / 32] & (1U << (i % 32))) { // Check if i is prime\n            for (int j = i * i; j < limit; j += i) {\n                sieve[j / 32] &= ~(1U << (j % 32)); // Mark multiples of i as non-prime\n            }\n        }\n    }\n\n    __m256i vec_indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    for (int i = 0; i < sieve_size; ++i) {\n        uint32_t bits = sieve[i];\n        while (bits) {\n            __m256i vec_bits = _mm256_set1_epi32(bits);\n            __m256i vec_mask = _mm256_and_si256(vec_bits, vec_indices);\n            int mask = _mm256_movemask_epi8(vec_mask);\n\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (1 << j)) {\n                    primes.push_back(i * 32 + j);\n                }\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Create a mask for numbers from 2 to n-1\n    int limit = n - 1;\n    int size = (limit + 31) / 32; // Number of 32-bit integers needed\n    std::vector<uint32_t> sieve(size, 0xFFFFFFFF); // Initialize all bits to 1 (potential primes)\n\n    // Mark non-prime numbers using SIMD\n    for (int i = 2; i * i < n; ++i) {\n        if ((sieve[i / 32] & (1U << (i % 32))) != 0) { // Check if i is prime\n            __m256i vec_i = _mm256_set1_epi32(i); // Broadcast i\n            for (int j = i * i; j < n; j += i * 8) {\n                __m256i vec_j = _mm256_set_epi32(j + 7 * i, j + 6 * i, j + 5 * i, j + 4 * i,\n                                                 j + 3 * i, j + 2 * i, j + i, j);\n                __m256i vec_mask = _mm256_sllv_epi32(_mm256_set1_epi32(1), vec_j); // Create mask\n                sieve[j / 32] &= ~vec_mask; // Mark as non-prime\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if ((sieve[i / 32] & (1U << (i % 32))) != 0) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *str = s.c_str();\n    size_t len = s.size();\n    int count = 0;\n\n    __m256i uppercase_vowels = _mm256_setr_epi8(\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t remaining = len - i;\n        size_t chunk_size = remaining < 32 ? remaining : 32;\n\n        __m256i indices = _mm256_loadu_si256((__m256i *)(str + i));\n        __m256i mask_even = _mm256_setr_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n\n        __m256i masked_indices = _mm256_and_si256(indices, mask_even);\n        __m256i cmp_result = _mm256_cmpeq_epi8(masked_indices, uppercase_vowels);\n\n        int mask = _mm256_movemask_epi8(cmp_result);\n        for (size_t j = 0; j < chunk_size; j++) {\n            if (mask & (1 << j)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const size_t len = s.size();\n    const char *str = s.c_str();\n    const __m256i uppercase_vowels = _mm256_setr_epi8(\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    int count = 0;\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(str + i));\n        __m256i indices = _mm256_setr_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n            16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n        );\n\n        __m256i even_mask = _mm256_and_si256(indices, _mm256_set1_epi8(1));\n        __m256i even_indices = _mm256_cmpeq_epi8(even_mask, _mm256_setzero_si256());\n\n        __m256i is_uppercase = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z' + 1))\n        );\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(\n            _mm256_or_si256(\n                _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('A')),\n                _mm256_or_si256(\n                    _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('E')),\n                    _mm256_or_si256(\n                        _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('I')),\n                        _mm256_or_si256(\n                            _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('O')),\n                            _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('U'))\n                        )\n                    )\n                )\n            ),\n            _mm256_set1_epi8(-1)\n        );\n\n        __m256i valid_chars = _mm256_and_si256(even_indices, _mm256_and_si256(is_uppercase, is_vowel));\n        int mask = _mm256_movemask_epi8(valid_chars);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; i += 2) {\n        if (i % 2 == 0 && (str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *str = s.c_str();\n    size_t len = s.size();\n    int count = 0;\n\n    // Uppercase vowels: 'A', 'E', 'I', 'O', 'U'\n    __m256i uppercase_vowels = _mm256_setr_epi8(\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(str + i));\n        \n        // Mask for even indices\n        __m256i even_mask = _mm256_setr_epi8(\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00\n        );\n\n        // Apply even index mask\n        __m256i masked_chars = _mm256_and_si256(chars, even_mask);\n\n        // Compare with uppercase vowels\n        __m256i cmp_result = _mm256_cmpeq_epi8(masked_chars, uppercase_vowels);\n\n        // Count matches\n        int mask = _mm256_movemask_epi8(cmp_result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; i += 2) {\n        char c = str[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *str = s.c_str();\n    size_t len = s.size();\n    __m256i uppercase_vowels = _mm256_set_epi8(\n        'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U',\n        'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O'\n    );\n\n    int count = 0;\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(str + i));\n        __m256i indices = _mm256_set_epi8(\n            31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16,\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n        );\n\n        __m256i even_mask = _mm256_and_si256(indices, _mm256_set1_epi8(1));\n        __m256i is_even = _mm256_cmpeq_epi8(even_mask, _mm256_setzero_si256());\n\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1)),\n            _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z' + 1))\n        );\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(\n            _mm256_and_si256(chars, _mm256_set1_epi8(0xDF)), // Normalize to uppercase\n            uppercase_vowels\n        );\n\n        __m256i is_valid = _mm256_and_si256(is_even, _mm256_and_si256(is_upper, is_vowel));\n        int mask = _mm256_movemask_epi8(is_valid);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; i += 2) {\n        if (i % 2 == 0 && (str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    const char *str = s.c_str();\n    size_t len = s.size();\n    int count = 0;\n\n    // Uppercase vowels: 'A', 'E', 'I', 'O', 'U'\n    const __m256i uppercase_vowels = _mm256_setr_epi8(\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i *)(str + i));\n\n        // Mask for even indices\n        const __m256i even_mask = _mm256_setr_epi8(\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00\n        );\n\n        // Apply mask to keep only even indices\n        __m256i masked_chars = _mm256_and_si256(chars, even_mask);\n\n        // Compare masked characters with uppercase vowels\n        __m256i cmp_result = _mm256_cmpeq_epi8(masked_chars, uppercase_vowels);\n\n        // Count matches\n        int matches = _mm256_movemask_epi8(cmp_result);\n        count += __builtin_popcount(matches);\n    }\n\n    // Process remaining characters\n    for (size_t i = len - (len % 32); i < len; i += 2) {\n        char c = str[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    // Initialize the first level\n    int start = n;\n    int increment = (n % 2 == 0) ? 2 : 2; // Increment is always 2 for odd/even\n\n    // Use AVX2 intrinsics to compute the levels\n    __m256i vec_start = _mm256_set1_epi32(start);\n    __m256i vec_increment = _mm256_set1_epi32(increment);\n    __m256i vec_indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        // Compute the stone counts for 8 levels at a time\n        __m256i vec_levels = _mm256_add_epi32(vec_start, _mm256_mullo_epi32(vec_indices, vec_increment));\n\n        // Store the results back to the vector\n        _mm256_storeu_si256((__m256i*)&result[i], vec_levels);\n\n        // Update indices for the next batch\n        vec_indices = _mm256_add_epi32(vec_indices, _mm256_set1_epi32(8));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    // Initialize the first level\n    int start = n;\n\n    // Create an AVX vector with the starting value\n    __m256i current = _mm256_set1_epi32(start);\n\n    // Create an AVX vector for the increment (odd/even step)\n    __m256i increment = _mm256_set1_epi32((start % 2 == 0) ? 2 : 2);\n\n    for (int i = 0; i < n; i += 8) {\n        // Store the current values into the result vector\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n\n        // Increment the current values for the next levels\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    // Initialize the first level\n    int start = n;\n\n    // Create an AVX vector for the base values\n    __m256i base = _mm256_set1_epi32(start);\n\n    // Create an AVX vector for the step values (increment by 2)\n    __m256i step = _mm256_set1_epi32(2);\n\n    // Process in chunks of 8 using AVX\n    for (int i = 0; i < n; i += 8) {\n        // Generate indices for the current chunk\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        // Calculate the stone count for each level\n        __m256i stones = _mm256_add_epi32(base, _mm256_mullo_epi32(indices, step));\n\n        // Store the result back into the vector\n        _mm256_storeu_si256((__m256i*)&result[i], stones);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    // Initialize the first level\n    result[0] = n;\n\n    // Create an AVX vector for the base value\n    __m256i base = _mm256_set1_epi32(n);\n\n    // Create an AVX vector for the increment value\n    __m256i increment = _mm256_set1_epi32((n % 2 == 0) ? 2 : 1);\n\n    // Fill the levels using AVX intrinsics\n    for (int i = 1; i < n; i += 8) {\n        // Load the current indices\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n\n        // Compute the values for the current levels\n        __m256i values = _mm256_add_epi32(base, _mm256_mullo_epi32(increment, indices));\n\n        // Store the computed values into the result vector\n        _mm256_storeu_si256((__m256i*)&result[i], values);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    int start = n;\n    int increment = (n % 2 == 0) ? 2 : 2;\n\n    // Initialize the first level\n    result[0] = start;\n\n    // Use AVX2 intrinsics to compute the remaining levels\n    __m256i vec_increment = _mm256_set1_epi32(increment);\n    __m256i vec_start = _mm256_set1_epi32(start);\n    __m256i vec_result = _mm256_add_epi32(vec_start, vec_increment);\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t range = m - n + 1;\n    int64_t sum = 0;\n\n    // SIMD computation for summing integers from n to m\n    __m256i vec_sum = _mm256_setzero_si256();\n    int64_t step = 8; // AVX2 processes 8 integers at a time\n    int64_t i = n;\n\n    for (; i + step - 1 <= m; i += step) {\n        __m256i vec = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        vec_sum = _mm256_add_epi64(vec_sum, vec);\n    }\n\n    // Extract the sum from the SIMD register\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec_sum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining integers\n    for (; i <= m; ++i) {\n        sum += i;\n    }\n\n    // Compute the average and round to the nearest integer\n    int64_t avg = std::round(static_cast<double>(sum) / range);\n\n    // Convert the average to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Calculate the total number of integers in the range [n, m]\n    int64_t count = m - n + 1;\n\n    // Use AVX2 to compute the sum of integers from n to m\n    __m256i sum = _mm256_setzero_si256();\n    __m256i step = _mm256_set1_epi64x(1);\n    __m256i current = _mm256_set_epi64x(n + 3, n + 2, n + 1, n);\n\n    while (n <= m) {\n        __m256i mask = _mm256_cmpgt_epi64(_mm256_set1_epi64x(m + 1), current);\n        __m256i masked_current = _mm256_and_si256(current, mask);\n        sum = _mm256_add_epi64(sum, masked_current);\n        current = _mm256_add_epi64(current, step);\n        n += 4;\n    }\n\n    // Extract the sum from the AVX2 register\n    int64_t result[4];\n    _mm256_storeu_si256((__m256i *)result, sum);\n    int64_t total_sum = result[0] + result[1] + result[2] + result[3];\n\n    // Compute the average and round to the nearest integer\n    int64_t avg = std::llround((double)total_sum / count);\n\n    // Convert the average to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t range = m - n + 1;\n    int64_t sum = 0;\n\n    // SIMD processing\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_step = _mm256_set1_epi64x(1);\n    __m256i vec_end = _mm256_set1_epi64x(m);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        __m256i vec_i = _mm256_add_epi64(vec_n, _mm256_set_epi64x(3, 2, 1, 0));\n        vec_i = _mm256_min_epi64(vec_i, vec_end); // Ensure we don't exceed m\n        vec_sum = _mm256_add_epi64(vec_sum, vec_i);\n        vec_n = _mm256_add_epi64(vec_n, _mm256_set1_epi64x(4));\n    }\n\n    // Extract the sum from the SIMD register\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vec_sum);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    // Compute the average and round\n    int64_t avg = std::round((double)sum / range);\n\n    // Convert to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Calculate the number of elements in the range\n    int64_t count = m - n + 1;\n\n    // Use AVX2 to compute the sum of integers from n to m\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i step_vec = _mm256_set1_epi64x(4); // Step size for AVX2\n    __m256i current_vec = _mm256_set_epi64x(n + 3, n + 2, n + 1, n);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        // Load the current values into the vector\n        __m256i mask = _mm256_cmpgt_epi64(_mm256_set1_epi64x(m + 1), current_vec);\n        __m256i add_vec = _mm256_and_si256(current_vec, mask);\n        sum_vec = _mm256_add_epi64(sum_vec, add_vec);\n\n        // Increment the current vector\n        current_vec = _mm256_add_epi64(current_vec, step_vec);\n    }\n\n    // Extract the sum from the vector\n    int64_t sum[4];\n    _mm256_storeu_si256((__m256i *)sum, sum_vec);\n    int64_t total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    // Compute the average and round to the nearest integer\n    int64_t avg = std::llround(static_cast<double>(total_sum) / count);\n\n    // Convert the average to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    int64_t range = m - n + 1;\n    int64_t sum = 0;\n\n    // SIMD computation for summing integers from n to m\n    __m256i vec_sum = _mm256_setzero_si256();\n    for (int64_t i = n; i <= m; i += 8) {\n        __m256i vec = _mm256_set_epi64x(\n            std::min(i + 7, m), std::min(i + 6, m), \n            std::min(i + 5, m), std::min(i + 4, m)\n        );\n        __m256i vec2 = _mm256_set_epi64x(\n            std::min(i + 3, m), std::min(i + 2, m), \n            std-min(i + "}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i mod_val = _mm256_set1_epi32(10000); // Modulo value for factorial calculation\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i); // Indices\n        __m256i even_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1)); // Mask for even/odd\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256()); // Check if even\n\n        // Factorial calculation for even indices\n        __m256i factorial = _mm256_set1_epi32(1);\n        for (int j = 1; j <= 8; ++j) {\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorial = _mm256_mullo_epi32(factorial, multiplier);\n            factorial = _mm256_rem_epi32(factorial, mod_val); // Apply modulo\n        }\n\n        // Sum calculation for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, _mm256_set1_epi32(1)));\n        sum = _mm256_srli_epi32(sum, 1); // Sum = i * (i + 1) / 2\n\n        // Blend results based on even/odd mask\n        __m256i blended = _mm256_blendv_epi8(sum, factorial, is_even);\n\n        // Store results\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result[i + j] = _mm256_extract_epi32(blended, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i tens = _mm256_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_add_epi32(_mm256_set1_epi32(i), _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1));\n        __m256i is_even = _mm256_and_si256(indices, ones);\n\n        __m256i factorial = ones;\n        __m256i sum = ones;\n        __m256i next = indices;\n        __m256i mask_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        for (int j = 1; j <= 8; j++) {\n            __m256i next_factorial = _mm256_mullo_epi32(factorial, next);\n            factorial = _mm256_blendv_epi8(factorial, next_factorial, mask_even);\n            next = _mm256_sub_epi32(next, ones);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i mod_val = _mm256_set1_epi32(10000); // Modulo value\n    __m256i ones = _mm256_set1_epi32(1);       // Vector of ones\n\n    for (int i = 0; i < n; i += 8) {\n        // Load indices into vector\n        __m256i indices = _mm256_set_epi32(\n            i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n\n        // Calculate even mask\n        __m256i even_mask = _mm256_and_si256(indices, ones);\n        even_mask = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        // Compute factorial modulo 10000 for even indices\n        __m256i factorial = ones;\n        for (int j = 1; j <= 8; ++j) {\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorial = _mm256_mullo_epi32(factorial, multiplier);\n            factorial = _mm256_rem_epi32(factorial, mod_val);\n        }\n\n        // Compute sum for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, ones));\n        sum = _mm256_srli_epi32(sum, 1); // Divide by 2\n\n        // Blend results based on even_mask\n        __m256i blended = _mm256_blendv_epi8(sum, factorial, even_mask);\n\n        // Store results into the output vector\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, blended);\n        for (int j = 0; j < 8 && i + j < n; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i mod_val = _mm256_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256i is_even_mask = _mm256_and_si256(indices, _mm256_set1_epi32(1)); // Check if index is even\n        __m256i is_even = _mm256_cmpeq_epi32(is_even_mask, _mm256_setzero_si256());\n\n        // Compute factorial modulo 10000 for even indices\n        __m256i factorials = _mm256_set1_epi32(1);\n        for (int j = 1; j <= 8; ++j) {\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, multiplier);\n            factorials = _mm256_rem_epi32(factorials, mod_val); // Modulo operation\n        }\n\n        // Compute sum of numbers from 1 to i for odd indices\n        __m256i sums = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, _mm256_set1_epi32(1)));\n        sums = _mm256_srli_epi32(sums, 1); // Divide by 2\n\n        // Blend results based on even/odd mask\n        __m256i blended = _mm256_blendv_epi8(sums, factorials, is_even);\n\n        // Store results back to the vector\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result[i + j] = _mm256_extract_epi32(blended, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i mod_val = _mm256_set1_epi32(10000); // Set modulo value\n    __m256i one = _mm256_set1_epi32(1);        // Set constant 1\n\n    for (int i = 0; i < n; i += 8) {\n        // Load indices (i+1, i+2, ..., i+8)\n        __m256i indices = _mm256_set_epi32(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n\n        // Check if indices are even\n        __m256i even_mask = _mm256_and_si256(indices, one); // Mask for odd/even\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        // Compute factorial modulo 10000 for even indices\n        __m256i factorial = one;\n        __m256i current = one;\n        for (int j = 1; j <= 8; ++j) {\n            current = _mm256_set1_epi32(j);\n            factorial = _mm256_mullo_epi32(factorial, current);\n            factorial = _mm256_rem_epi32(factorial, mod_val); // Apply modulo\n        }\n\n        // Compute sum for odd indices\n        __m256i sum = _mm256_mullo_epi32(indices, _mm256_add_epi32(indices, one));\n        sum = _mm256_srli_epi32(sum, 1); // Divide by 2\n\n        // Select between factorial and sum based on even/odd\n        __m256i result_vec = _mm256_blendv_epi8(sum, factorial, is_even);\n\n        // Store results in the output vector\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, result_vec);\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nbool is_palindrome(int num) {\n    int reversed = 0, original = num;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return original == reversed;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks using AVX2\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i results = _mm256_setzero_si256();\n\n        // Check palindrome for each number in the vector\n        for (int j = 0; j < 8 && (i + j) <= n; ++j) {\n            int num = i + j;\n            if (is_palindrome(num)) {\n                results = _mm256_insert_epi32(results, num, j);\n            }\n        }\n\n        // Check even or odd for each palindrome\n        for (int j = 0; j < 8 && (i + j) <= n; ++j) {\n            int num = _mm256_extract_epi32(results, j);\n            if (num != 0) {\n                if (num % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\ninline bool is_palindrome(int num) {\n    std::string s = std::to_string(num);\n    int len = s.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (s[i] != s[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks using AVX2\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(\n            i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Mask to store results of palindrome checks\n        __m256i palindrome_mask = _mm256_setzero_si256();\n\n        // Check each number for palindrome property\n        for (int j = 0; j < 8; ++j) {\n            int num = i + j;\n            if (num <= n && is_palindrome(num)) {\n                palindrome_mask = _mm256_insert_epi32(palindrome_mask, 1, j);\n            }\n        }\n\n        // Extract even/odd information\n        __m256i even_mask = _mm256_and_si256(numbers, _mm256_set1_epi32(1));\n        __m256i odd_mask = _mm256_xor_si256(even_mask, _mm256_set1_epi32(1));\n\n        // Count even and odd palindromes\n        for (int j = 0; j < 8; ++j) {\n            int is_palindrome = _mm256_extract_epi32(palindrome_mask, j);\n            int is_even = _mm256_extract_epi32(even_mask, j);\n            if (is_palindrome) {\n                if (is_even == 0) {\n                    ++even_count;\n                } else {\n                    ++odd_count;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\ninline bool is_palindrome(int num) {\n    std::string s = std::to_string(num);\n    int len = s.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (s[i] != s[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks using AVX2\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(\n            i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Mask to track valid numbers within range\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n + 1), numbers);\n\n        // Check each number for palindrome property\n        for (int j = 0; j < 8; ++j) {\n            int num = ((int*)&numbers)[j];\n            if (num <= n && is_palindrome(num)) {\n                if (num % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\ninline bool is_palindrome(int num) {\n    std::string s = std::to_string(num);\n    int len = s.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (s[i] != s[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks using SIMD\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i evens_mask = _mm256_setzero_si256();\n        __m256i odds_mask = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int num = i + j;\n            if (num > n) break;\n            if (is_palindrome(num)) {\n                if (num % 2 == 0) {\n                    evens_mask = _mm256_add_epi32(evens_mask, _mm256_set1_epi32(1));\n                } else {\n                    odds_mask = _mm256_add_epi32(odds_mask, _mm256_set1_epi32(1));\n                }\n            }\n        }\n\n        // Sum up the counts\n        even_count += _mm256_extract_epi32(evens_mask, 0) + _mm256_extract_epi32(evens_mask, 1) +\n                      _mm256_extract_epi32(evens_mask, 2) + _mm256_extract_epi32(evens_mask, 3) +\n                      _mm256_extract_epi32(evens_mask, 4) + _mm256_extract_epi32(evens_mask, 5) +\n                      _mm256_extract_epi32(evens_mask, 6) + _mm256_extract_epi32(evens_mask, 7);\n\n        odd_count += _mm256_extract_epi32(odds_mask, 0) + _mm256_extract_epi32(odds_mask, 1) +\n                     _mm256_extract_epi32(odds_mask, 2) + _mm256_extract_epi32(odds_mask, 3) +\n                     _mm256_extract_epi32(odds_mask, 4) + _mm256_extract_epi32(odds_mask, 5) +\n                     _mm256_extract_epi32(odds_mask, 6) + _mm256_extract_epi32(odds_mask, 7);\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\ninline bool is_palindrome(int num) {\n    std::string s = std::to_string(num);\n    int len = s.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (s[i] != s[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks of 8 using AVX2 intrinsics\n    for (int i = 1; i <= n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(\n            i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Mask to track valid numbers within range\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(n + 1), numbers);\n\n        // Check each number for palindrome property\n        alignas(32) int nums[8];\n        _mm256_store_si256((__m256i*)nums, numbers);\n\n        for (int j = 0; j < 8; ++j) {\n            if (nums[j] <= n && is_palindrome(nums[j])) {\n                if (nums[j] % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nint count_nums_simd(std::vector<int> n) {\n    size_t size = n.size();\n    int count = 0;\n\n    // Process in chunks of 8 integers (256 bits / 32 bits per integer)\n    size_t simd_size = size / 8 * 8;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i result = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Extract digits and compute sum of digits\n        __m256i sum_digits = _mm256_setzero_si256();\n        while (!_mm256_testz_si256(abs_vec, abs_vec)) {\n            __m256i digit = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n            sum_digits = _mm256_add_epi32(sum_digits, digit);\n            abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n        }\n\n        // Check if sum of digits > 0\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, zero);\n        result = _mm256_add_epi32(result, mask);\n    }\n\n    // Sum up the results from the SIMD registers\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, result);\n    for (int i = 0; i < 8; ++i) {\n        count += temp[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        num = std::abs(num);\n        while (num > 0) {\n            sum += num % 10;\n            num /= 10;\n        }\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nint count_nums_simd(std::vector<int> n) {\n    size_t size = n.size();\n    if (size == 0) return 0;\n\n    int count = 0;\n    size_t i = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    __m256i zero = _mm256_setzero_si256();\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n\n        // Compute absolute values of the numbers\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Extract digits and compute sum of digits\n        __m256i sum_digits = zero;\n        while (!_mm256_testz_si256(abs_vec, abs_vec)) {\n            __m256i digit = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n            sum_digits = _mm256_add_epi32(sum_digits, digit);\n            abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n        }\n\n        // Check if sum of digits > 0\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, zero);\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(mask));\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (sum_digits > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int count = 0;\n    int remainder = size % simd_width;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i result = zero;\n\n    for (int i = 0; i < size - remainder; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        __m256i sum_digits = zero;\n        for (int j = 0; j < 10; ++j) {\n            __m256i mod = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n            sum_digits = _mm256_add_epi32(sum_digits, mod);\n            abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, zero);\n        result = _mm256_add_epi32(result, mask);\n    }\n\n    int temp[simd_width];\n    _mm256_storeu_si256((__m256i*)temp, result);\n    for (int i = 0; i < simd_width; ++i) {\n        count += temp[i];\n    }\n\n    for (int i = size - remainder; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (sum_digits > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int count = 0;\n    int remainder = size % 8; // Handle remainder for non-multiple of 8\n    int limit = size - remainder;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i result = _mm256_setzero_si256();\n\n    for (int i = 0; i < limit; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        __m256i sum_digits = zero;\n        for (int j = 0; j < 10; ++j) {\n            __m256i mod = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n            sum_digits = _mm256_add_epi32(sum_digits, mod);\n            abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, zero);\n        result = _mm256_add_epi32(result, mask);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, result);\n    for (int i = 0; i < 8; ++i) {\n        count += temp[i];\n    }\n\n    for (int i = limit; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (sum_digits > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    const int size = n.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int count = 0;\n\n    // Process in chunks of simd_width\n    int i = 0;\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 8 integers into an AVX2 register\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n\n        // Compute absolute values of the integers\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Extract digits and compute sum of digits\n        __m256i sum_vec = _mm256_setzero_si256();\n        while (true) {\n            __m256i digit_vec = _mm256_rem_epi32(abs_vec, _mm256_set1_epi32(10));\n            sum_vec = _mm256_add_epi32(sum_vec, digit_vec);\n            abs_vec = _mm256_div_epi32(abs_vec, _mm256_set1_epi32(10));\n            if (_mm256_testz_si256(abs_vec, abs_vec)) break;\n        }\n\n        // Check if sum of digits > 0\n        __m256i mask = _mm256_cmpgt_epi32(sum_vec, _mm256_setzero_si256());\n        count += _mm256_popcnt_epi32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n        if (sum_digits > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool is_sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n    if (is_sorted) return true;\n\n    // Perform right shifts and check for sorted order\n    for (int shift = 1; shift < n; ++shift) {\n        // Perform the right shift\n        std::rotate(arr.rbegin(), arr.rbegin() + 1, arr.rend());\n\n        // Use AVX2 intrinsics to check if the array is sorted\n        bool sorted = true;\n        int i = 0;\n\n        // Process in chunks of 8 integers using AVX2\n        for (; i + 8 <= n; i += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n        }\n\n        // Check remaining elements\n        for (; i < n - 1; ++i) {\n            if (arr[i] > arr[i + 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool is_sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n    if (is_sorted) return true;\n\n    // Perform the check for all possible rotations\n    for (int shift = 1; shift < n; ++shift) {\n        // Rotate the array using SIMD\n        __m256i prev = _mm256_setzero_si256();\n        bool sorted = true;\n\n        for (int i = 0; i < n; i += 8) {\n            __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n            sorted = sorted && prev.compare(curr);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            sorted = false;\n            break;\n        }\n    }\n    if (sorted) return true;\n\n    // Use AVX2 intrinsics to check for sorted order after each rotation\n    for (int shift = 1; shift < n; ++shift) {\n        // Perform the right shift\n        std::vector<int> rotated(n);\n        for (int i = 0; i < n; ++i) {\n            rotated[i] = arr[(i + shift) % n];\n        }\n\n        // Check if the rotated array is sorted using AVX2\n        bool is_sorted = true;\n        int i = 0;\n        for (; i + 8 <= n; i += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&rotated[i]);\n            __m256i next = _mm256_loadu_si256((__m256i*)&rotated[i + 1]);\n\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        // Check remaining elements\n        for (; i < n - 1; ++i) {\n            if (rotated[i] > rotated[i + 1]) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool is_sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n    if (is_sorted) return true;\n\n    // Perform the right shift operation and check for sorted order\n    for (int shift = 1; shift < n; ++shift) {\n        // Perform the right shift\n        std::vector<int> shifted_arr(n);\n        for (int i = 0; i < n; ++i) {\n            shifted_arr[i] = arr[(i - shift + n) % n];\n        }\n\n        // Use AVX2 intrinsics to check if the shifted array is sorted\n        __m256i prev = _mm256_set1_epi32(shifted_arr[0]);\n        bool sorted = true;\n\n        for (int i = 1; i < n; i += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)&shifted_arr[i]);\n            __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n            prev = current;\n        }\n\n        if (sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            sorted = false;\n            break;\n        }\n    }\n    if (sorted) return true;\n\n    // Perform the check for all possible rotations\n    for (int shift = 1; shift < n; ++shift) {\n        // Rotate the array by one position to the right\n        std::vector<int> rotated(n);\n        for (int i = 0; i < n; ++i) {\n            rotated[i] = arr[(i - shift + n) % n];\n        }\n\n        // Use AVX2 intrinsics to check if the rotated array is sorted\n        __m256i prev = _mm256_set1_epi32(rotated[0]);\n        bool is_sorted = true;\n\n        for (int i = 1; i < n; i += 8) {\n            int remaining = std::min(8, n - i);\n            __m256i current = _mm256_setzero_si256();\n\n            if (remaining == 8) {\n                current = _mm256_loadu_si256((__m256i*)&rotated[i]);\n            } else {\n                int temp[8] = {0};\n                for (int j = 0; j < remaining; ++j) {\n                    temp[j] = rotated[i + j];\n                }\n                current = _mm256_loadu_si256((__m256i*)temp);\n            }\n\n            __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                is_sorted = false;\n                break;\n            }\n            prev = current;\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Check if lst1 already contains only even numbers\n    for (size_t i = 0; i < size1; i++) {\n        if (lst1[i] % 2 != 0) {\n            goto check_exchange;\n        }\n    }\n    return \"YES\";\n\ncheck_exchange:\n    // Prepare masks for even numbers\n    __m256i even_mask = _mm256_set1_epi32(1);\n\n    // Check lst2 for even numbers\n    size_t i = 0;\n    for (; i + 8 <= size2; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst2[i]);\n        __m256i mod_result = _mm256_and_si256(vec, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(cmp_result) != 0xFFFFFFFF) {\n            goto check_lst1;\n        }\n    }\n\n    // Handle remaining elements in lst2\n    for (; i < size2; i++) {\n        if (lst2[i] % 2 == 0) {\n            goto check_lst1;\n        }\n    }\n    return \"NO\";\n\ncheck_lst1:\n    // Check lst1 for odd numbers\n    i = 0;\n    for (; i + 8 <= size1; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i mod_result = _mm256_and_si256(vec, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(cmp_result) != 0xFFFFFFFF) {\n            return \"YES\";\n        }\n    }\n\n    // Handle remaining elements in lst1\n    for (; i < size1; i++) {\n        if (lst1[i] % 2 != 0) {\n            return \"YES\";\n        }\n    }\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Check if lst1 already contains only even numbers\n    for (size_t i = 0; i < size1; i++) {\n        if (lst1[i] % 2 != 0) {\n            break;\n        }\n        if (i == size1 - 1) {\n            return \"YES\";\n        }\n    }\n\n    // Prepare masks for even numbers in lst2\n    __m256i mask_even = _mm256_set1_epi32(1); // Mask for odd/even check\n    __m256i zero = _mm256_setzero_si256();   // Zero vector for comparison\n\n    // Check lst2 for even numbers\n    size_t i = 0;\n    for (; i + 8 <= size2; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst2[i]);\n        __m256i mod = _mm256_and_si256(vec, mask_even);\n        __m256i cmp = _mm256_cmpeq_epi32(mod, zero);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return YES;\n    }\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Check if lst1 already contains only even numbers\n    for (size_t i = 0; i < size1; i++) {\n        if (lst1[i] % 2 != 0) {\n            goto check_exchange;\n        }\n    }\n    return \"YES\";\n\ncheck_exchange:\n    // Prepare masks for even numbers in lst1 and lst2\n    __m256i even_mask = _mm256_set1_epi32(1); // Mask for checking odd/even\n    __m256i zero_mask = _mm256_setzero_si256(); // Mask for comparison\n\n    // Check lst1 for odd numbers\n    std::vector<int> odd_lst1;\n    for (size_t i = 0; i < size1; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i mod_result = _mm256_and_si256(vec1, even_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, zero_mask);\n        int mask = _mm256_movemask_epi8(cmp_result);\n\n        if (mask != -1) {\n            odd_lst1.push_back(i);\n        }\n    }\n\n    // Check lst2 for odd numbers\n    std::vector<int> odd_lst2;\n    for (size2+="}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n1 = lst1.size();\n    size_t n2 = lst2.size();\n\n    // Check if lst1 already contains only even numbers\n    size_t i = 0;\n    for (; i + 8 <= n1; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i mod2 = _mm256_and_si256(vec1, _mm256_set1_epi32(1));\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(mod2, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n            break;\n        }\n    }\n    for (; i < n1; ++i) {\n        if (lst1[i] % 2 != 0) {\n            break;\n        }\n    }\n    if (i == n1) {\n        return \"YES\";\n    }\n\n    // Check if lst2 contains at least one even number\n    bool has_even_in_lst2 = false;\n    i = 0;\n    for (; i + 8 <= n2; i += 8) {\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n        __m256i mod2 = _mm256_and_si256(vec2, _mm256_set1_epi32(1));\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(mod2, _mm256_setzero_si256())) != 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n    for (; i < n2; ++i) {\n        if (lst2[i] % 2 == 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    return has_even_in_lst2 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Check if lst1 already contains only even numbers\n    for (size_t i = 0; i < size1; i++) {\n        if (lst1[i] % 2 != 0) {\n            goto check_exchange;\n        }\n    }\n    return \"YES\";\n\ncheck_exchange:\n    // Prepare SIMD vectors for lst2\n    size_t simd_size = 8; // AVX2 processes 8 integers at a time\n    size_t aligned_size2 = size2 - (size2 % simd_size);\n    __m256i even_mask = _mm256_set1_epi32(1); // Mask for checking even numbers\n\n    for (size_t i = 0; i < size1; i++) {\n        if (lst1[i] % 2 != 0) { // If lst1[i] is odd\n            bool found_even = false;\n\n            // Process lst2 in chunks of 8 using SIMD\n            for (size_t j = 0; j < aligned_size2; j += simd_size) {\n                __m256i vec = _mm256_loadu_si256((__m256i*)&lst2[j]);\n                __m256i mod_result = _mm256_and_si256(vec, even_mask); // Check even numbers\n                __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, _mm256_setzero_si256());\n\n                int mask = _mm256_movemask_epi8(cmp_result);\n                if (mask != 0) { // If any even number is found\n                    found_even = true;\n                    break;\n                }\n            }\n\n            // Check remaining elements in lst2\n            if (!found_even) {\n                for (size_t j = aligned_size2; j < size2; j++) {\n                    if (lst2[j] % 2 == 0) {\n                        found_even = true;\n                        break;\n                    }\n                }\n            }\n\n            if (!found_even) {\n                return \"NO\";\n            }\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (size_t i = 0; i < lst.size(); ++i) {\n        const std::string &str = lst[i];\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 32 characters using AVX2\n        size_t j = 0;\n        for (; j + 32 <= len; j += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + j));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert ASCII digits to integers\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1)); // Mask odd digits\n            __m256i odd_cmp = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1)); // Compare for odd digits\n            odd_count += _mm_popcnt_u32(_mm256_movemask_epi8(odd_cmp)); // Count odd digits\n        }\n\n        // Process remaining characters\n        for (; j < len; ++j) {\n            char c = str[j];\n            if ((c - '0') % 2 == 1) {\n                ++odd_count;\n            }\n        }\n\n        // Create the output string\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) + \n                             \"n the str\" + std::to_string(odd_count) + \n                             \"ng \" + std::to_string(odd_count) + \" of the \" + \n                             std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 32 bytes using AVX2\n        for (size_t i = 0; i < len; i += 32) {\n            size_t chunk_size = std::min(len - i, size_t(32));\n            char buffer[32] = {0};\n            std::memcpy(buffer, str.data() + i, chunk_size);\n\n            // Load 32 bytes into an AVX2 register\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(buffer));\n\n            // Subtract '0' to convert ASCII digits to integers\n            __m256i digits = _mm256_sub_epi8(data, _mm256_set1_epi8('0'));\n\n            // Mask to identify odd numbers\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n\n            // Compare the mask to 1 to identify odd digits\n            __m256i is_odd = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1));\n\n            // Count the number of odd digits in the chunk\n            int mask = _mm256_movemask_epi8(is_odd);\n            odd_count += __builtin_popcount(mask);\n        }\n\n        // Construct the output string\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << odd_count << \"n the str\" << odd_count\n            << \"ng \" << odd_count << \" of the \" << odd_count << \"nput.\";\n        result.push_back(oss.str());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (size_t i = 0; i < lst.size(); ++i) {\n        const std::string &str = lst[i];\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 32 bytes using AVX2\n        size_t j = 0;\n        for (; j + 32 <= len; j += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + j));\n            __m256i digits = _mm256_sub_epi8(chunk, _mm256_set1_epi8('0'));\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n            __m256i odd_digits = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1));\n            odd_count += _mm_popcnt_u32(_mm256_movemask_epi8(odd_digits));\n        }\n\n        // Process remaining characters\n        for (; j < len; ++j) {\n            char c = str[j];\n            if ((c - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        // Construct the output string\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << odd_count << \"n the str\" << odd_count << \"ng \" << odd_count << \" of the \" << odd_count << \"nput.\";\n        result.push_back(oss.str());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (size_t i = 0; i < lst.size(); ++i) {\n        const std::string &str = lst[i];\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 32 bytes using AVX2\n        size_t j = 0;\n        for (; j + 32 <= len; j += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + j));\n            __m256i digits = _mm256_sub_epi8(chunk, _mm256_set1_epi8('0')); // Convert ASCII to digit\n            __m256i odd_mask = _mm256_and_si256(digits, _mm256_set1_epi8(1)); // Check odd digits\n            __m256i odd_digits = _mm256_cmpeq_epi8(odd_mask, _mm256_set1_epi8(1)); // Mask for odd digits\n            odd_count += _mm_popcnt_u32(_mm256_movemask_epi8(odd_digits)); // Count odd digits\n        }\n\n        // Process remaining characters\n        for (; j < len; ++j) {\n            char c = str[j];\n            if ((c - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        // Format the result string\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << odd_count << \"n the str\" << odd_count << \"ng \" << odd_count << \" of the \" << odd_count << \"nput.\";\n        result.push_back(oss.str());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (size_t i = 0; i < lst.size(); ++i) {\n        const std::string &str = lst[i];\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 32 bytes using AVX2\n        size_t j = 0;\n        __m256i odd_mask = _mm256_set1_epi8(1); // Mask for odd digits\n        __m256i zero = _mm256_set1_epi8('0');   // ASCII value of '0'\n        __m256i nine = _mm256_set1_epi8('9');  // ASCII value of '9'\n\n        for (; j + 32 <= len; j += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(str.data() + j));\n            __m256i is_digit = _mm256_and_si256(_mm256_cmpgt_epi8(chars, zero), _mm256_cmpgt_epi8(nine, chars));\n            __m256i digit_values = _mm256_sub_epi8(chars, zero);\n            __m256i is_odd = _mm256_and_si256(digit_values, odd_mask);\n            __m256i odd_digits = _mm256_and_si256(is_digit, is_odd);\n\n            // Count the number of odd digits\n            odd_count += _mm_popcnt_u32(_mm256_movemask_epi8(odd_digits));\n        }\n\n        // Process remaining characters\n        for (; j < len; ++j) {\n            char c = str[j];\n            if (c >= '0' && c <= '9' && (c - '0') % 2 == 1) {\n                ++odd_count;\n            }\n        }\n\n        // Construct the result string\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << odd_count << \"n the str\" << odd_count << \"ng \" << odd_count << \" of the \" << odd_count << \"nput.\";\n        result.push_back(oss.str());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    const size_t n = nums.size();\n    if (n == 0) return 0;\n\n    const size_t simd_width = 4; // AVX2 processes 4 int64_t values at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m256i current_min = _mm256_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m256i current_sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        current_sum = _mm256_add_epi64(current_sum, vec);\n        current_min = _mm256_min_epi64(current_min, current_sum);\n        current_sum = _mm256_min_epi64(current_sum, _mm256_setzero_si256());\n    }\n\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    int64_t temp[simd_width];\n    _mm256_storeu_si256((__m256i*)temp, current_min);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        min_sum = std::min(min_sum, temp[i]);\n    }\n\n    int64_t scalar_sum = 0;\n    for (size_t i = aligned_size; i < n; ++i) {\n        scalar_sum += nums[i];\n        min_sum = std::min(min_sum, scalar_sum);\n        scalar_sum = std::min(scalar_sum, int64_t(0));\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    size_t n = nums.size();\n    const size_t simd_width = 4; // AVX2 processes 4 int64_t elements at a time\n    size_t simd_iterations = n / simd_width;\n    size_t remainder = n % simd_width;\n\n    __m256i current_min = _mm256_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m256i current_sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < simd_iterations * simd_width; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        current_sum = _mm256_add_epi64(current_sum, vec);\n        current_min = _mm256_min_epi64(current_min, current_sum);\n\n        // Reset current_sum to zero if it becomes positive (Kadane's algorithm logic)\n        __m256i mask = _mm256_cmpgt_epi64(current_sum, _mm256_setzero_si256());\n        current_sum = _mm256_blendv_epi8(current_sum, _mm256_setzero_si256(), mask);\n    }\n\n    // Reduce the SIMD registers to find the minimum value\n    int64_t min_values[simd_width];\n    _mm256_storeu_si256((__m256i*)min_values, current_min);\n    int64_t global_min = std::numeric_limits<int64_t>::max();\n    for (size_t i = 0; i < simd_width; ++i) {\n        global_min = std::min(global_min, min_values[i]);\n    }\n\n    // Process the remainder elements\n    int64_t remainder_sum = 0;\n    for (size_t i = simd_iterations * simd_width; i < n; ++i) {\n        remainder_sum += nums[i];\n        global_min = std::min(global_min, remainder_sum);\n        if (remainder_sum > 0) remainder_sum = 0;\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cstdint>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    const size_t n = nums.size();\n    const size_t simd_width = 4; // AVX2 processes 4 int64_t values at a time\n    const size_t simd_chunks = n / simd_width;\n    const size_t remainder = n % simd_width;\n\n    __m256i current_min = _mm256_set1_epi64x(0);\n    __m256i global_min = _mm256_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m256i current_sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < simd_chunks * simd_width; i += simd_width) {\n        __m256i nums_vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        current_sum = _mm256_add_epi64(current_sum, nums_vec);\n        current_min = _mm256_min_epi64(current_min, current_sum);\n        global_min = _mm256_min_epi64(global_min, current_min);\n\n        // Reset current_sum to 0 if it becomes positive\n        __m256i reset_mask = _mm256_cmpgt_epi64(current_sum, _mm256_setzero_si256());\n        current_sum = _mm256_blendv_epi8(current_sum, _mm256_setzero_si256(), reset_mask);\n    }\n\n    // Reduce the global minimum from SIMD registers\n    alignas(32) int64_t global_min_array[simd_width];\n    _mm256_store_si256((__m256i*)global_min_array, global_min);\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    for (size_t i = 0; i < simd_width; ++i) {\n        min_sum = std::min(min_sum, global_min_array[i]);\n    }\n\n    // Handle the remainder elements\n    int64_t current_sum_scalar = 0;\n    for (size_t i = simd_chunks * simd_width; i < n; ++i) {\n        current_sum_scalar += nums[i];\n        min_sum = std::min(min_sum, current_sum_scalar);\n        if (current_sum_scalar > 0) {\n            current_sum_scalar = 0;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    const size_t simd_width = 4; // AVX2 processes 4 int64_t values per register\n    size_t simd_end = n - (n % simd_width);\n\n    __m256i current_min = _mm256_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m256i current_sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        current_sum = _mm256_add_epi64(current_sum, vec);\n        current_min = _mm256_min_epi64(current_min, current_sum);\n        current_sum = _mm256_min_epi64(current_sum, _mm256_setzero_si256());\n    }\n\n    // Reduce the SIMD registers to find the minimum value\n    int64_t min_value = std::numeric_limits<int64_t>::max();\n    alignas(32) int64_t result[simd_width];\n    _mm256_store_si256((__m256i*)result, current_min);\n    for (size_t i = 0; i < simd_width; ++i) {\n        min_value = std::min(min_value, result[i]);\n    }\n\n    // Handle the remaining elements\n    int64_t scalar_sum = 0;\n    for (size_t i = simd_end; i < n; ++i) {\n        scalar_sum += nums[i];\n        min_value = std::min(min_value, scalar_sum);\n        scalar_sum = std::min(scalar_sum, int64_t(0));\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cstdint>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    const size_t n = nums.size();\n    const size_t simd_width = 4; // AVX2 processes 4 int64_t values at a time\n    const size_t aligned_size = n - (n % simd_width);\n\n    __m256i min_sum_vec = _mm256_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m256i current_sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256i nums_vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        current_sum_vec = _mm256_add_epi64(current_sum_vec, nums_vec);\n        min_sum_vec = _mm256_min_epi64(min_sum_vec, current_sum_vec);\n\n        // Reset current_sum_vec if it becomes positive\n        __m256i reset_mask = _mm256_cmpgt_epi64(current_sum_vec, _mm256_setzero_si256());\n        current_sum_vec = _mm256_blendv_epi8(current_sum_vec, _mm256_setzero_si256(), reset_mask);\n    }\n\n    // Reduce the SIMD vector to find the minimum value\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    alignas(32) int64_t min_sum_array[simd_width];\n    _mm256_store_si256((__m256i*)min_sum_array, min_sum_vec);\n\n    for (size_t i = 0; i < simd_width; ++i) {\n        min_sum = std::min(min_sum, min_sum_array[i]);\n    }\n\n    // Process remaining elements\n    int64_t current_sum = 0;\n    for (size_t i = aligned_size; i < n; ++i) {\n        current_sum += nums[i];\n        min_sum = std::min(min_sum, current_sum);\n        if (current_sum > 0) current_sum = 0;\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Use AVX2 intrinsics to sum the row\n        __m256i total_sum = _mm256_setzero_si256();\n        int i = 0;\n\n        // Process 8 elements at a time\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i *)&row[i]);\n            total_sum = _mm256_add_epi32(total_sum, data);\n        }\n\n        // Extract the sum from the SIMD register\n        alignas(32) int buffer[8];\n        _mm256_store_si256((__m256i *)buffer, total_sum);\n        for (int j = 0; j < 8; ++j) {\n            sum += buffer[j];\n        }\n\n        // Handle remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of operations needed for this row\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <numeric>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto& row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Use AVX2 intrinsics to sum the row\n        __m256i total = _mm256_setzero_si256();\n        int i = 0;\n\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i*)&row[i]);\n            total = _mm256_add_epi32(total, data);\n        }\n\n        // Horizontal sum of the 256-bit register\n        alignas(32) int buffer[8];\n        _mm256_store_si256((__m256i*)buffer, total);\n        sum = std::accumulate(buffer, buffer + 8, 0);\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of bucket operations for this row\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int row_sum = 0;\n\n        // Use AVX2 intrinsics to sum the row\n        __m256i sum_vec = _mm256_setzero_si256();\n        int i = 0;\n\n        // Process 8 elements at a time\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i *)&row[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal sum of the 256-bit vector\n        alignas(32) int buffer[8];\n        _mm256_store_si256((__m256i *)buffer, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            row_sum += buffer[j];\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            row_sum += row[i];\n        }\n\n        // Calculate buckets needed for this row\n        total_buckets += (row_sum + capacity - 1) / capacity; // Ceiling division\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto &row : grid) {\n        int row_sum = 0;\n        size_t n = row.size();\n        size_t simd_width = 8; // AVX2 processes 8 integers at a time\n        size_t simd_end = n - (n % simd_width);\n\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        for (size_t i = 0; i < simd_end; i += simd_width) {\n            __m256i data = _mm256_loadu_si256((__m256i *)&row[i]);\n            sum_vec = _mm256_add_epi32(sum_vec, data);\n        }\n\n        // Horizontal sum of the SIMD vector\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i *)temp, sum_vec);\n        for (int i = 0; i < 8; ++i) {\n            row_sum += temp[i];\n        }\n\n        // Process remaining elements\n        for (size_t i = simd_end; i < n; ++i) {\n            row_sum += row[i];\n        }\n\n        // Calculate the number of bucket operations for this row\n        total_operations += (row_sum + capacity - 1) / capacity; // Equivalent to ceil(row_sum / capacity)\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <numeric>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Use AVX2 intrinsics to sum the row\n        __m256i total = _mm256_setzero_si256();\n        int i = 0;\n\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i data = _mm256_loadu_si256((__m256i *)&row[i]);\n            total = _mm256_add_epi32(total, data);\n        }\n\n        // Extract the sum from the SIMD register\n        alignas(32) int buffer[8];\n        _mm256_store_si256((__m256i *)buffer, total);\n        sum = std::accumulate(buffer, buffer + 8, 0);\n\n        // Handle remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of buckets needed for this row\n        total_buckets += (sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const size_t simd_width = 16; // 16 elements for 16-bit integers in 256-bit registers\n    size_t simd_end = n / simd_width * simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even_positions = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i mask_odd_values = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_pos_masked = _mm256_and_si256(data, mask_even_positions);\n        __m256i odd_values_masked = _mm256_and_si256(data, mask_odd_values);\n        __m256i odd_in_even_positions = _mm256_and_si256(even_pos_masked, odd_values_masked);\n        sum_vec = _mm256_add_epi16(sum_vec, odd_in_even_positions);\n    }\n\n    int16_t temp[simd_width];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    int64_t sum = 0;\n    for (size_t i = 0; i < simd_width; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = simd_end; i < n; i += 2) {\n        if (lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    int64_t result = 0;\n    size_t simd_width = 16; // AVX2 processes 16 16-bit integers at a time\n    size_t simd_end = n / simd_width * simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i mask_even_positions = _mm256_set_epi16(0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, \n                                                   0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i odd_mask = _mm256_and_si256(data, _mm256_set1_epi16(1));\n        __m256i is_odd = _mm256_cmpeq_epi16(odd_mask, _mm256_set1_epi16(1));\n        __m256i even_pos_data = _mm256_and_si256(data, mask_even_positions);\n        __m256i filtered_data = _mm256_and_si256(even_pos_data, is_odd);\n        sum_vec = _mm256_add_epi16(sum_vec, filtered_data);\n    }\n\n    // Horizontal sum of sum_vec\n    alignas(32) int16_t temp[simd_width];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (size_t i = 0; i < simd_width; ++i) {\n        result += temp[i];\n    }\n\n    // Handle remaining elements\n    for (size_t i = simd_end; i < n; i += 2) {\n        if (lst[i] & 1) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    const int16_t* data = lst.data();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask_even_positions = _mm256_set_epi16(0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF);\n\n    size_t i = 0;\n    for (; i + 16 <= size; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i odd_mask = _mm256_and_si256(vec, _mm256_set1_epi16(1));\n        __m256i is_odd = _mm256_cmpeq_epi16(odd_mask, _mm256_set1_epi16(1));\n        __m256i even_pos_masked = _mm256_and_si256(mask_even_positions, is_odd);\n        __m256i filtered = _mm256_and_si256(vec, even_pos_masked);\n        sum = _mm256_add_epi16(sum, filtered);\n    }\n\n    int16_t temp[16];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    int64_t total_sum = 0;\n    for (int j = 0; j < 16; ++j) {\n        total_sum += temp[j];\n    }\n\n    for (; i < size; i += 2) {\n        if (data[i] & 1) {\n            total_sum += data[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const size_t simd_width = 16; // Number of 16-bit integers in a 256-bit AVX2 register\n    size_t simd_end = n / simd_width * simd_width; // End of the SIMD processing range\n    __m256i sum_vec = _mm256_setzero_si256(); // Accumulator for the sum\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]); // Load 16 elements\n        __m256i indices = _mm256_set_epi16(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); // Indices\n        __m256i even_mask = _mm256_cmpeq_epi16(_mm256_and_si256(indices, _mm256_set1_epi16(1)), _mm256_setzero_si256()); // Mask for even positions\n        __m256i odd_mask = _mm256_cmpeq_epi16(_mm256_and_si256(data, _mm256_set1_epi16(1)), _mm256_set1_epi16(1)); // Mask for odd elements\n        __m256i combined_mask = _mm256_and_si256(even_mask, odd_mask); // Combine masks\n        __m256i filtered = _mm256_and_si256(data, combined_mask); // Apply mask to data\n        sum_vec = _mm256_add_epi16(sum_vec, filtered); // Accumulate\n    }\n\n    // Horizontal sum of the accumulated vector\n    int16_t temp[simd_width];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    int64_t total_sum = 0;\n    for (size_t i = 0; i < simd_width; ++i) {\n        total_sum += temp[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_end; i < n; i += 2) {\n        if (lst[i] & 1) { // Check if odd\n            total_sum += lst[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const int16_t* data = lst.data();\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i odd_mask = _mm256_set1_epi16(1); // Mask for odd numbers\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]); // Load 16 elements\n        __m256i indices = _mm256_set_epi16(\n            i + 15, i + 14, i + 13, i + 12, i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i\n        );\n\n        __m256i even_pos_mask = _mm256_and_si256(indices, odd_mask); // Check even positions\n        even_pos_mask = _mm256_cmpeq_epi16(even_pos_mask, _mm256_setzero_si256());\n\n        __m256i odd_masked = _mm256_and_si256(vec, odd_mask); // Check odd numbers\n        __m256i odd_check = _mm256_cmpeq_epi16(odd_masked, odd_mask);\n\n        __m256i valid_mask = _mm256_and_si256(even_pos_mask, odd_check); // Combine masks\n        __m256i filtered = _mm256_and_si256(vec, valid_mask); // Apply mask to elements\n\n        sum_vec = _mm256_add_epi16(sum_vec, filtered); // Accumulate\n    }\n\n    int16_t temp[16];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    int64_t total_sum = 0;\n    for (int j = 0; j < 16; ++j) {\n        total_sum += temp[j];\n    }\n\n    for (; i < n; i += 2) {\n        if (data[i] & 1) { // Check if odd\n            total_sum += data[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int n = std::min((int)arr.size(), k);\n    int sum = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_two_digits = _mm256_set1_epi32(99);\n    __m256i min_two_digits = _mm256_set1_epi32(-99);\n\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask1 = _mm256_cmpgt_epi32(data, max_two_digits);\n        __m256i mask2 = _mm256_cmpgt_epi32(min_two_digits, data);\n        __m256i mask = _mm256_or_si256(mask1, mask2);\n        __m256i filtered = _mm256_andnot_si256(mask, data);\n        sum_vec = _mm256_add_epi32(sum_vec, filtered);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    const int n = arr.size();\n    if (k > n) k = n;\n\n    int sum = 0;\n    int i = 0;\n\n    // Process elements in chunks of 8 using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    for (; i + 8 <= k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Mask to check if elements are within two digits\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), abs_vec);\n\n        // Zero out elements that are not within two digits\n        __m256i filtered_vec = _mm256_and_si256(vec, mask);\n\n        // Accumulate filtered elements\n        sum_vec = _mm256_add_epi32(sum_vec, filtered_vec);\n    }\n\n    // Horizontal sum of the accumulated vector\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        if (std::abs(arr[i]) < 100) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int n = arr.size();\n    k = std::min(k, n); // Ensure k does not exceed the size of the array\n    int sum = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_ten = _mm256_set1_epi32(10);\n    __m256i vec_hundred = _mm256_set1_epi32(100);\n\n    int i = 0;\n    for (; i + 8 <= k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        // Take absolute value of the elements\n        __m256i vec_abs = _mm256_abs_epi32(vec);\n\n        // Check if elements are less than 100 (two digits)\n        __m256i mask = _mm256_and_si256(\n            _mm256_cmpgt_epi32(vec_hundred, vec_abs), // vec_abs < 100\n            _mm256_cmpgt_epi32(vec_abs, vec_ten)     // vec_abs >= 10\n        );\n\n        // Zero out elements that do not satisfy the condition\n        __m256i filtered = _mm256_and_si256(vec, mask);\n\n        // Accumulate the filtered values\n        vec_sum = _mm256_add_epi32(vec_sum, filtered);\n    }\n\n    // Horizontal sum of vec_sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vec_sum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle the remaining elements\n    for (; i < k; ++i) {\n        if (arr[i] >= 10 && arr[i] < 100) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    const int n = arr.size();\n    const int limit = std::min(k, n);\n    int sum = 0;\n\n    // Process elements in chunks of 8 using AVX2\n    int i = 0;\n    __m256i v_sum = _mm256_setzero_si256();\n    for (; i + 8 <= limit; i += 8) {\n        // Load 8 integers from the array\n        __m256i v_data = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compute absolute values of the integers\n        __m256i v_abs = _mm256_abs_epi32(v_data);\n\n        // Compare with 100 (two-digit limit)\n        __m256i v_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), v_abs);\n\n        // Mask the values that are <= 100\n        __m256i v_filtered = _mm256_and_si256(v_data, v_mask);\n\n        // Accumulate the filtered values\n        v_sum = _mm256_add_epi32(v_sum, v_filtered);\n    }\n\n    // Horizontal sum of v_sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, v_sum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < limit; ++i) {\n        if (std::abs(arr[i]) <= 100) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int n = arr.size();\n    int limit = (k < n) ? k : n;\n\n    // Process elements in chunks of 8 using AVX2\n    int i = 0;\n    __m256i v_sum = _mm256_setzero_si256(); // Initialize vector sum to zero\n    for (; i + 8 <= limit; i += 8) {\n        // Load 8 integers from the array\n        __m256i v_data = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compute absolute values (to handle negative numbers)\n        __m256i v_abs = _mm256_abs_epi32(v_data);\n\n        // Mask for numbers with at most two digits (<= 99)\n        __m256i v_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), v_abs);\n\n        // Zero out elements that don't satisfy the condition\n        __m256i v_filtered = _mm256_and_si256(v_data, v_mask);\n\n        // Accumulate the filtered values\n        v_sum = _mm256_add_epi32(v_sum, v_filtered);\n    }\n\n    // Horizontal sum of the vector sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, v_sum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements sequentially\n    for (; i < limit; ++i) {\n        if (abs(arr[i]) < 100) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    for (size_t i = 0; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) return false;\n\n        __m256i duplicates = _mm256_cmpeq_epi32(current, next);\n        if (!_mm256_testz_si256(duplicates, duplicates)) return false;\n    }\n\n    for (size_t i = n - n % 8; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1] || (i > 0 && lst[i] == lst[i + 1] && lst[i] == lst[i - 1])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    const int *data = lst.data();\n    size_t i = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    for (; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i *)(data + i));\n        __m256i next = _mm256_loadu_si256((__m256i *)(data + i + 1));\n\n        // Compare adjacent elements\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n\n        // Check for duplicates (more than 1 duplicate)\n        __m256i duplicates = _mm256_cmpeq_epi32(current, next);\n        __m256i shifted_duplicates = _mm256_srli_si256(duplicates, 4);\n        __m256i duplicate_check = _mm256_and_si256(duplicates, shifted_duplicates);\n        if (!_mm256_testz_si256(duplicate_check, duplicate_check)) {\n            return false;\n        }\n    }\n\n    // Process remaining elements sequentially\n    for (; i < n - 1; ++i) {\n        if (data[i] > data[i + 1]) return false;\n        if (i < n - 2 && data[i] == data[i + 1] && data[i] == data[i + 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        size_t remaining = n - i;\n        if (remaining < 8) {\n            for (size_t j = i; j < n - 1; ++j) {\n                if (lst[j] > lst[j + 1] || (j > 0 && lst[j] == lst[j - 1] && lst[j] == lst[j + 1])) {\n                    return false;\n                }\n            }\n            break;\n        }\n\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&lst[i + 1]);\n\n        __m256i greater_mask = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(greater_mask, greater_mask)) {\n            return false;\n        }\n\n        __m256i duplicates = _mm256_cmpeq_epi32(current, next);\n        __m256i shifted_duplicates = _mm256_slli_si256(duplicates, 4);\n        __m256i duplicate_check = _mm256_and_si256(duplicates, shifted_duplicates);\n        if (!_mm256_testz_si256(duplicate_check, duplicate_check)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256-bit registers)\n    size_t simd_end = n - (n % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&lst[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (!_mm256_testz_si256(cmp, cmp)) return false;\n\n        __m256i duplicates = _mm256_cmpeq_epi32(current, next);\n        if (!_mm256_testz_si256(duplicates, duplicates)) return false;\n    }\n\n    for (size_t i = simd_end; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1] || (i > 0 && lst[i] == lst[i - 1])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t size = lst.size();\n    if (size <= 1) return true;\n\n    const int *data = lst.data();\n    size_t i = 0;\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 8 <= size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&data[i + 1]);\n\n        // Compare adjacent elements\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n        if (_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n\n        // Check for duplicates (more than 1 duplicate of the same number)\n        __m256i duplicates = _mm256_cmpeq_epi32(current, next);\n        if (_mm256_movemask_epi8(duplicates) & _mm256_movemask_epi8(_mm256_srli_si256(duplicates, 4))) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size - 1; ++i) {\n        if (data[i] > data[i + 1]) return false;\n        if (i < size - 2 && data[i] == data[i + 1] && data[i] == data[i + 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_magnitudes = _mm256_setzero_si256();\n    __m256i product_signs = _mm256_set1_epi32(1);\n\n    size_t size = arr.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        \n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum_magnitudes = _mm256_add_epi32(sum_magnitudes, abs_vec);\n\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        product_signs = _mm256_mullo_epi32(product_signs, signs);\n    }\n\n    int sum = 0;\n    int prod = 1;\n\n    alignas(32) int magnitudes[8];\n    alignas(32) int signs[8];\n\n    _mm256_store_si256((__m256i*)magnitudes, sum_magnitudes);\n    _mm256_store_si256((__m256i*)signs, product_signs);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += magnitudes[j];\n        prod *= signs[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += abs(arr[i]);\n        prod *= (arr[i] > 0) ? 1 : (arr[i] < 0) ? -1 : 0;\n    }\n\n    return sum * prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    const size_t n = arr.size();\n    size_t i = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n\n        __m256i sign_vec = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, sign_vec);\n    }\n\n    int sum = 0;\n    int sign_prod = 1;\n\n    alignas(32) int sum_arr[8];\n    alignas(32) int sign_prod_arr[8];\n\n    _mm256_store_si256((__m256i*)sum_arr, sum_vec);\n    _mm256_store_si256((__m256i*)sign_prod_arr, sign_prod_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_arr[j];\n        sign_prod *= sign_prod_arr[j];\n    }\n\n    for (; i < n; ++i) {\n        sum += std::abs(arr[i]);\n        sign_prod *= (arr[i] > 0) - (arr[i] < 0);\n    }\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    const int n = arr.size();\n    __m256i sum_magnitudes = _mm256_setzero_si256();\n    __m256i prod_signs = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum_magnitudes = _mm256_add_epi32(sum_magnitudes, abs_vec);\n\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        prod_signs = _mm256_mullo_epi32(prod_signs, signs);\n    }\n\n    int sum = 0;\n    int product = 1;\n    alignas(32) int temp_sum[8];\n    alignas(32) int temp_prod[8];\n\n    _mm256_store_si256((__m256i*)temp_sum, sum_magnitudes);\n    _mm256_store_si256((__m256i*)temp_prod, prod_signs);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp_sum[j];\n        product *= temp_prod[j];\n    }\n\n    for (; i < n; ++i) {\n        sum += std::abs(arr[i]);\n        product *= (arr[i] > 0) ? 1 : (arr[i] < 0 ? -1 : 0);\n    }\n\n    return sum * product;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum_magnitudes = _mm256_setzero_si256();\n    int product_of_signs = 1;\n\n    size_t size = arr.size();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum_magnitudes = _mm256_add_epi32(sum_magnitudes, abs_vec);\n\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        int signs_array[8];\n        _mm256_storeu_si256((__m256i*)signs_array, signs);\n\n        for (int j = 0; j < 8; ++j) {\n            product_of_signs *= signs_array[j];\n        }\n    }\n\n    int sum = 0;\n    int sum_array[8];\n    _mm256_storeu_si256((__m256i*)sum_array, sum_magnitudes);\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_array[j];\n    }\n\n    for (; i < size; ++i) {\n        sum += abs(arr[i]);\n        product_of_signs *= (arr[i] > 0) - (arr[i] < 0);\n    }\n\n    return sum * product_of_signs;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint prod_signs_simd(const std::vector<int> &arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    const int n = arr.size();\n    __m256i sum_magnitudes = _mm256_setzero_si256();\n    __m256i product_signs = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n\n        sum_magnitudes = _mm256_add_epi32(sum_magnitudes, abs_vec);\n        product_signs = _mm256_mullo_epi32(product_signs, signs);\n    }\n\n    int sum_magnitudes_scalar = 0;\n    int product_signs_scalar = 1;\n\n    alignas(32) int temp_sum[8];\n    alignas(32) int temp_signs[8];\n    _mm256_store_si256((__m256i*)temp_sum, sum_magnitudes);\n    _mm256_store_si256((__m256i*)temp_signs, product_signs);\n\n    for (int j = 0; j < 8; ++j) {\n        sum_magnitudes_scalar += temp_sum[j];\n        product_signs_scalar *= temp_signs[j];\n    }\n\n    for (; i < n; ++i) {\n        sum_magnitudes_scalar += abs(arr[i]);\n        product_signs_scalar *= (arr[i] > 0 ? 1 : (arr[i] < 0 ? -1 : 0));\n    }\n\n    return sum_magnitudes_scalar * product_signs_scalar;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <limits>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, std::numeric_limits<int>::max())));\n    std::vector<std::vector<std::vector<std::vector<int>>>> paths(N, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(k + 1)));\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n            paths[i][j][1] = {grid[i][j]};\n        }\n    }\n\n    for (int step = 2; step <= k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_values = _mm256_set1_epi32(std::numeric_limits<int>::max());\n                std::vector<std::vector<int>> candidates;\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (std::abs(di) + std::abs(dj) != 1) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i current_values = _mm256_set1_epi32(dp[ni][nj][step - 1]);\n                            min_values = _mm256_min_epi32(min_values, current_values);\n                            candidates.push_back(paths[ni][nj][step - 1]);\n                        }\n                    }\n                }\n\n                int min_value = std::numeric_limits<int>::max();\n                alignas(32) int min_array[8];\n                _mm256_store_si256((__m256i *)min_array, min_values);\n                for (int idx = 0; idx < 8; ++idx) {\n                    min_value = std::min(min_value, min_array[idx]);\n                }\n\n                dp[i][j][step] = min_value;\n                for (const auto &candidate : candidates) {\n                    if (candidate.back() == min_value) {\n                        paths[i][j][step] = candidate;\n                        paths[i][j][step].push_back(grid[i][j]);\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    std::vector<int> result;\n    int min_value = std::numeric_limits<int>::max();\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp[i][j][k] < min_value) {\n                min_value = dp[i][j][k];\n                result = paths[i][j][k];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, std::numeric_limits<int>::max());\n    std::vector<std::vector<int>> dp(N, std::vector<int>(N, std::numeric_limits<int>::max()));\n\n    // Initialize dp with the grid values\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j] = grid[i][j];\n        }\n    }\n\n    // Temporary storage for SIMD processing\n    std::vector<int> temp(N * N, std::numeric_limits<int>::max());\n\n    for (int step = 1; step < k; ++step) {\n        std::vector<std::vector<int>> new_dp(N, std::vector<int>(N, std::numeric_limits<int>::max()));\n\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_values = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n                // Process neighbors using SIMD\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (std::abs(di) + std::abs(dj) != 1) continue; // Only adjacent cells\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i current = _mm256_set1_epi32(dp[ni][nj]);\n                            min_values = _mm256_min_epi32(min_values, current);\n                        }\n                    }\n                }\n\n                // Store the minimum value back\n                int min_value_array[8];\n                _mm256_storeu_si256((__m256i *)min_value_array, min_values);\n                int min_value = *std::min_element(min_value_array, min_value_array + 8);\n                new_dp[i][j] = min_value + grid[i][j];\n            }\n        }\n\n        dp = new_dp;\n    }\n\n    // Find the lexicographically smallest path\n    int min_value = std::numeric_limits<int>::max();\n    int start_i = 0, start_j = 0;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp[i][j] < min_value) {\n                min_value = dp[i][j];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    result[0] = grid[start_i][start_j];\n    for (int step = 1; step < k; ++step) {\n        int next_i = start_i, next_j = start_j;\n        min_value = std::numeric_limits<int>::max();\n\n        for (int di = -1; di <= 1; ++di) {\n            for (int dj = -1; dj <= 1; ++dj) {\n                if (std::abs(di) + std::abs(dj) != 1) continue; // Only adjacent cells\n                int ni = start_i + di, nj = start_j + dj;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    if (dp[ni][nj] < min_value) {\n                        min_value = dp[ni][nj];\n                        next_i = ni;\n                        next_j = nj;\n                    }\n                }\n            }\n        }\n\n        start_i = next_i;\n        start_j = next_j;\n        result[step] = grid[start_i][start_j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <vector>\n#include <immintrin.h>\n#include <limits>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    const int INF = std::numeric_limits<int>::max();\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, INF)));\n    std::vector<std::vector<std::vector<int>>> path(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, -1)));\n\n    // Initialize dp for paths of length 1\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n        }\n    }\n\n    // Compute dp for paths of length 2 to k using SIMD\n    for (int len = 2; len <= k; ++len) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_values = _mm256_set1_epi32(INF);\n                __m256i current_value = _mm256_set1_epi32(grid[i][j]);\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (di == 0 && dj == 0) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i neighbor_values = _mm256_set1_epi32(dp[ni][nj][len - 1]);\n                            __m256i combined_values = _mm256_add_epi32(current_value, neighbor_values);\n                            min_values = _mm256_min_epi32(min_values, combined_values);\n                        }\n                    }\n                }\n\n                int min_value_array[8];\n                _mm256_storeu_si256((__m256i *)min_value_array, min_values);\n                dp[i][j][len] = *std::min_element(min_value_array, min_value_array + 8);\n            }\n        }\n    }\n\n    // Find the minimum path\n    std::vector<int> result;\n    int min_value = INF, start_i = -1, start_j = -1;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp[i][j][k] < min_value) {\n                min_value = dp[i][j][k];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    // Reconstruct the path\n    int len = k, i = start_i, j = start_j;\n    while (len > 0) {\n        result.push_back(grid[i][j]);\n        int next_i = -1, next_j = -1;\n        for (int di = -1; di <= 1; ++di) {\n            for (int dj = -1; dj <= 1; ++dj) {\n                if (di == 0 && dj == 0) continue;\n                int ni = i + di, nj = j + dj;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N && dp[ni][nj][len - 1] + grid[i][j] == dp[i][j][len]) {\n                    next_i = ni;\n                    next_j = nj;\n                    break;\n                }\n            }\n            if (next_i != -1) break;\n        }\n        i = next_i;\n        j = next_j;\n        --len;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, std::numeric_limits<int>::max())));\n    std::vector<std::vector<std::vector<std::vector<int>>>> path(N, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(k + 1)));\n\n    // Initialize DP for paths of length 1\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n            path[i][j][1] = {grid[i][j]};\n        }\n    }\n\n    // Compute DP for paths of length 2 to k\n    for (int len = 2; len <= k; ++len) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_values = _mm256_set1_epi32(std::numeric_limits<int>::max());\n                std::vector<std::vector<int>> candidates;\n\n                // Check all neighbors\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (std::abs(di) + std::abs(dj) != 1) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i current_value = _mm256_set1_epi32(dp[ni][nj][len - 1]);\n                            min_values = _mm256_min_epi32(min_values, current_value);\n                            candidates.push_back(path[ni][nj][len - 1]);\n                        }\n                    }\n                }\n\n                // Extract minimum value and corresponding path\n                int min_value = std::numeric_limits<int>::max();\n                std::vector<int> min_path;\n                for (size_t idx = 0; idx < candidates.size(); ++idx) {\n                    if (dp[i][j][len - 1] < min_value) {\n                        min_value = dp[i][j][len - 1];\n                        min_path = candidates[idx];\n                    }\n                }\n\n                dp[i][j][len] = min_value + grid[i][j];\n                min_path.push_back(grid[i][j]);\n                path[i][j][len] = min_path;\n            }\n        }\n    }\n\n    // Find the minimum path of length k\n    std::vector<int> result;\n    int min_value = std::numeric_limits<int>::max();\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp[i][j][k] < min_value) {\n                min_value = dp[i][j][k];\n                result = path[i][j][k];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <queue>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k);\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, INT_MAX)));\n    std::vector<std::vector<std::vector<std::vector<int>>>> path(N, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(k + 1)));\n\n    __m256i inf_vec = _mm256_set1_epi32(INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n            path[i][j][1] = {grid[i][j]};\n        }\n    }\n\n    for (int step = 2; step <= k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_vec = inf_vec;\n                std::vector<int> best_path;\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) + abs(dj) != 1) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i dp_vec = _mm256_set1_epi32(dp[ni][nj][step - 1]);\n                            __m256i grid_vec = _mm256_set1_epi32(grid[i][j]);\n                            __m256i sum_vec = _mm256_add_epi32(dp_vec, grid_vec);\n\n                            if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(min_vec, sum_vec))) {\n                                min_vec = sum_vec;\n                                best_path = path[ni][nj][step - 1];\n                                best_path.push_back(grid[i][j]);\n                            }\n                        }\n                    }\n                }\n\n                dp[i][j][step] = _mm256_extract_epi32(min_vec, 0);\n                path[i][j][step] = best_path;\n            }\n        }\n    }\n\n    __m256i global_min_vec = inf_vec;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m256i dp_vec = _mm256_set1_epi32(dp[i][j][k]);\n            if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(global_min_vec, dp_vec))) {\n                global_min_vec = dp_vec;\n                result = path[i][j][k];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8];\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    for (int i = 0; i < 8; i++) {\n        digits[i] = n % 10;\n        n /= 10;\n        if (n == 0) break;\n    }\n\n    // Load digits into an AVX2 register\n    __m256i vec_digits = _mm256_loadu_si256((__m256i*)digits);\n\n    // Create a mask for odd digits\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_twos = _mm256_set1_epi64x(2);\n    __m256i vec_mod = _mm256_rem_epi64(vec_digits, vec_twos);\n    __m256i vec_mask = _mm256_cmpeq_epi64(vec_mod, vec_ones);\n\n    // Multiply odd digits\n    __m256i vec_result = _mm256_and_si256(vec_digits, vec_mask);\n    for (int i = 0; i < 8; i++) {\n        uint64_t val = ((uint64_t*)&vec_result)[i];\n        if (val > 0) {\n            product *= val;\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    int count = 0;\n\n    // Extract digits from the number\n    while (n > 0 && count < 8) {\n        digits[count++] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an AVX2 register\n    __m256i vec_digits = _mm256_loadu_si256((__m256i*)digits);\n\n    // Create a mask for odd digits\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_mask = _mm256_and_si256(vec_digits, vec_ones);\n\n    // Compare mask with 1 to identify odd digits\n    __m256i vec_is_odd = _mm256_cmpeq_epi64(vec_mask, vec_ones);\n\n    // Zero out even digits\n    __m256i vec_odds = _mm256_and_si256(vec_digits, vec_is_odd);\n\n    // Extract the odd digits from the vector\n    uint64_t odd_digits[8];\n    _mm256_storeu_si256((__m256i*)odd_digits, vec_odds);\n\n    // Compute the product of odd digits\n    uint64_t product = 1;\n    int has_odd = 0;\n    for (int i = 0; i < count; i++) {\n        if (odd_digits[i] != 0) {\n            product *= odd_digits[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    int count = 0;\n\n    // Extract digits from the number\n    while (n > 0 && count < 8) {\n        digits[count++] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an AVX2 register\n    __m256i vec_digits = _mm256_loadu_si256((__m256i*)digits);\n\n    // Mask for odd digits (1, 3, 5, 7, 9)\n    __m256i odd_mask = _mm256_set1_epi64x(1);\n\n    // Compute modulo 2 for each digit\n    __m256i mod_result = _mm256_and_si256(vec_digits, odd_mask);\n\n    // Compare modulo result to 1 (odd check)\n    __m256i is_odd = _mm256_cmpeq_epi64(mod_result, odd_mask);\n\n    // Mask out even digits\n    __m256i odd_digits = _mm256_and_si256(vec_digits, is_odd);\n\n    // Extract the odd digits back to an array\n    uint64_t odd_digits_array[8];\n    _mm256_storeu_si256((__m256i*)odd_digits_array, odd_digits);\n\n    // Compute the product of odd digits\n    uint64_t product = 1;\n    int has_odd = 0;\n    for (int i = 0; i < count; i++) {\n        if (odd_digits_array[i] != 0) {\n            product *= odd_digits_array[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    int count = 0;\n    while (n > 0 && count < 8) {\n        digits[count++] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an AVX2 register\n    __m256i vec_digits = _mm256_loadu_si256((__m256i*)digits);\n\n    // Create masks for odd digits\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_twos = _mm256_set1_epi64x(2);\n    __m256i vec_mod = _mm256_rem_epi64(vec_digits, vec_twos);\n    __m256i vec_mask = _mm256_cmpeq_epi64(vec_mod, vec_ones);\n\n    // Apply mask to extract odd digits\n    __m256i vec_odds = _mm256_and_si256(vec_digits, vec_mask);\n\n    // Extract results back to an array\n    uint64_t odd_digits[8];\n    _mm256_storeu_si256((__m256i*)odd_digits, vec_odds);\n\n    // Compute the product of odd digits\n    for (int i = 0; i < count; i++) {\n        if (odd_digits[i] != 0) {\n            product *= odd_digits[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8];\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    for (int i = 0; i < 8; i++) {\n        digits[i] = n % 10;\n        n /= 10;\n        if (n == 0) break;\n    }\n\n    // Load digits into a SIMD register\n    __m256i vec_digits = _mm256_set_epi64x(digits[3], digits[2], digits[1], digits[0]);\n\n    // Mask for odd digits\n    __m256i odd_mask = _mm256_set1_epi64x(1);\n\n    // Check if digits are odd\n    __m256i is_odd = _mm256_and_si256(vec_digits, odd_mask);\n\n    // Compare odd digits\n    __m256i cmp_result = _mm256_cmpeq_epi64(is_odd, odd_mask);\n\n    // Extract results and compute product\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i *)result, cmp_result);\n\n    for (int i = 0; i < 4; i++) {\n        if (result[i]) {\n            product *= digits[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_end = n / simd_width * simd_width;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256 input_vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(input_vec);\n        __m256 squared_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n    }\n\n    float sum_array[8];\n    _mm256_storeu_ps(sum_array, sum_vec);\n\n    float total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    for (size_t i = simd_end; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_size = n - (n % simd_width);\n\n    __m256 sum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 squared_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum = _mm256_add_ps(sum, squared_vec);\n    }\n\n    float result[simd_width];\n    _mm256_storeu_ps(result, sum);\n\n    float total_sum = 0;\n    for (size_t i = 0; i < simd_width; ++i) {\n        total_sum += result[i];\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t simd_end = n / simd_width * simd_width;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vals = _mm256_ceil_ps(input);\n        __m256 squared_vals = _mm256_mul_ps(ceil_vals, ceil_vals);\n        __m256i squared_ints = _mm256_cvtps_epi32(squared_vals);\n        sum_vec = _mm256_add_epi32(sum_vec, squared_ints);\n    }\n\n    // Horizontal sum of the SIMD register\n    alignas(32) int32_t temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    uint64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += temp[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_end; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_width = 8; // AVX registers can process 8 floats at a time\n    size_t aligned_size = n - (n % simd_width);\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 input_vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(input_vec);\n        __m256 squared_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum_vec = _mm256_add_ps(sum_vec, squared_vec);\n    }\n\n    float partial_sum[simd_width];\n    _mm256_storeu_ps(partial_sum, sum_vec);\n\n    float total_sum = 0.0f;\n    for (size_t i = 0; i < simd_width; ++i) {\n        total_sum += partial_sum[i];\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t size = lst.size();\n    size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t aligned_size = size - (size % simd_width);\n\n    __m256 sum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vals = _mm256_ceil_ps(input);\n        __m256 squared_vals = _mm256_mul_ps(ceil_vals, ceil_vals);\n        sum = _mm256_add_ps(sum, squared_vals);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    float total_sum = 0.0f;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int result = -1;\n\n    for (int i = 0; i <= n - simd_width; i += simd_width) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next); // Compare current > next\n        int mask = _mm256_movemask_epi8(cmp); // Create a mask from comparison results\n\n        if (mask != 0) { // If any comparison is true\n            for (int j = 0; j < simd_width; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements (if any) sequentially\n    for (int i = n - simd_width; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int result = -1;\n\n    for (int i = 0; i <= n - simd_width; i += simd_width) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next); // Compare current[i] > next[i]\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < simd_width; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    for (int i = n - n % simd_width; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int result = -1;\n\n    for (int i = 0; i <= n - simd_width; i += simd_width) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(current, next); // Compare current > next\n        int mask = _mm256_movemask_epi8(cmp); // Create a mask of comparison results\n\n        if (mask) {\n            for (int j = 0; j < simd_width - 1; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % simd_width; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process in chunks of 8 integers using AVX2\n    int i = 0;\n    __m256i prev, curr, mask;\n\n    for (; i + 8 <= n; i += 8) {\n        prev = _mm256_loadu_si256((__m256i*)&arr[i]);\n        curr = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        mask = _mm256_cmpgt_epi32(prev, curr); // Compare prev > curr\n        int result_mask = _mm256_movemask_epi8(mask);\n\n        if (result_mask != 0) {\n            // Find the first mismatch in the chunk\n            for (int j = 0; j < 8; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process elements in chunks of 8 (AVX2 supports 256-bit registers, 8 integers per register)\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 integers from the array\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        // Compare adjacent elements (next < current)\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n\n        // Check if any comparison result is true\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            // Find the first failing index in the chunk\n            for (int j = 0; j < 8; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements sequentially\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i largest_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i smallest_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i mask_neg = _mm256_cmpgt_epi32(zero, vec);\n        __m256i mask_pos = _mm256_cmpgt_epi32(vec, zero);\n\n        __m256i neg_values = _mm256_and_si256(vec, mask_neg);\n        __m256i pos_values = _mm256_and_si256(vec, mask_pos);\n\n        largest_neg = _mm256_max_epi32(largest_neg, neg_values);\n\n        __m256i pos_min_mask = _mm256_cmpgt_epi32(smallest_pos, pos_values);\n        smallest_pos = _mm256_blendv_epi8(smallest_pos, pos_values, pos_min_mask);\n    }\n\n    int largest_neg_scalar = std::numeric_limits<int>::min();\n    int smallest_pos_scalar = std::numeric_limits<int>::max();\n\n    alignas(32) int neg_array[8];\n    alignas(32) int pos_array[8];\n\n    _mm256_store_si256((__m256i*)neg_array, largest_neg);\n    _mm256_store_si256((__m256i*)pos_array, smallest_pos);\n\n    for (int j = 0; j < 8; ++j) {\n        largest_neg_scalar = std::max(largest_neg_scalar, neg_array[j]);\n        if (pos_array[j] > 0) {\n            smallest_pos_scalar = std::min(smallest_pos_scalar, pos_array[j]);\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0) {\n            largest_neg_scalar = std::max(largest_neg_scalar, lst[i]);\n        } else if (lst[i] > 0) {\n            smallest_pos_scalar = std::min(smallest_pos_scalar, lst[i]);\n        }\n    }\n\n    if (largest_neg_scalar == std::numeric_limits<int>::min()) {\n        largest_neg_scalar = 0;\n    }\n    if (smallest_pos_scalar == std::numeric_limits<int>::max()) {\n        smallest_pos_scalar = 0;\n    }\n\n    return {largest_neg_scalar, smallest_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n\n    size_t i = 0;\n    size_t size = lst.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i is_neg = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i is_pos = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_vals = _mm256_and_si256(vec, is_neg);\n        __m256i pos_vals = _mm256_and_si256(vec, is_pos);\n\n        max_neg = _mm256_max_epi32(max_neg, neg_vals);\n\n        __m256i pos_mask = _mm256_and_si256(pos_vals, is_pos);\n        min_pos = _mm256_min_epi32(min_pos, pos_mask);\n    }\n\n    int max_neg_arr[8];\n    int min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int largest_negative = std::numeric_limits<int>::min();\n    int smallest_positive = std::numeric_limits<int>::max();\n\n    for (int j = 0; j < 8; ++j) {\n        largest_negative = std::max(largest_negative, max_neg_arr[j]);\n        if (min_pos_arr[j] > 0) {\n            smallest_positive = std::min(smallest_positive, min_pos_arr[j]);\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] < 0) {\n            largest_negative = std::max(largest_negative, lst[i]);\n        } else if (lst[i] > 0) {\n            smallest_positive = std::min(smallest_positive, lst[i]);\n        }\n    }\n\n    if (largest_negative == std::numeric_limits<int>::min()) {\n        largest_negative = 0;\n    }\n    if (smallest_positive == std::numeric_limits<int>::max()) {\n        smallest_positive = 0;\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(data + i));\n\n        __m256i mask_neg = _mm256_cmpgt_epi32(zero, vec);\n        __m256i mask_pos = _mm256_cmpgt_epi32(vec, zero);\n\n        __m256i neg_values = _mm256_and_si256(vec, mask_neg);\n        __m256i pos_values = _mm256_and_si256(vec, mask_pos);\n\n        max_neg = _mm256_max_epi32(max_neg, neg_values);\n        min_pos = _mm256_min_epi32(min_pos, pos_values);\n    }\n\n    int max_neg_scalar = std::numeric_limits<int>::min();\n    int min_pos_scalar = std::numeric_limits<int>::max();\n\n    alignas(32) int max_neg_array[8];\n    alignas(32) int min_pos_array[8];\n\n    _mm256_store_si256((__m256i*)max_neg_array, max_neg);\n    _mm256_store_si256((__m256i*)min_pos_array, min_pos);\n\n    for (int j = 0; j < 8; ++j) {\n        max_neg_scalar = std::max(max_neg_scalar, max_neg_array[j]);\n        min_pos_scalar = std::min(min_pos_scalar, min_pos_array[j]);\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < 0) {\n            max_neg_scalar = std::max(max_neg_scalar, data[i]);\n        } else if (data[i] > 0) {\n            min_pos_scalar = std::min(min_pos_scalar, data[i]);\n        }\n    }\n\n    if (max_neg_scalar == std::numeric_limits<int>::min()) {\n        max_neg_scalar = 0;\n    }\n    if (min_pos_scalar == std::numeric_limits<int>::max()) {\n        min_pos_scalar = 0;\n    }\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(data + i));\n\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n        __m256i is_pos = _mm256_cmpgt_epi32(vec, zero);\n\n        __m256i neg_values = _mm256_and_si256(vec, is_neg);\n        __m256i pos_values = _mm256_and_si256(vec, is_pos);\n\n        max_neg = _mm256_max_epi32(max_neg, neg_values);\n        min_pos = _mm256_min_epi32(min_pos, pos_values);\n    }\n\n    int max_neg_arr[8], min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n\n    int largest_neg = std::numeric_limits<int>::min();\n    int smallest_pos = std::numeric_limits<int>::max();\n\n    for (int j = 0; j < 8; ++j) {\n        largest_neg = std::max(largest_neg, max_neg_arr[j]);\n        smallest_pos = std::min(smallest_pos, min_pos_arr[j]);\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < 0) {\n            largest_neg = std::max(largest_neg, data[i]);\n        } else if (data[i] > 0) {\n            smallest_pos = std::min(smallest_pos, data[i]);\n        }\n    }\n\n    if (largest_neg == std::numeric_limits<int>::min()) {\n        largest_neg = 0;\n    }\n    if (smallest_pos == std::numeric_limits<int>::max()) {\n        smallest_pos = 0;\n    }\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n\n    __m256i max_neg = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i min_pos = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(data + i));\n\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n        __m256i is_pos = _mm256_cmpgt_epi32(vec, zero);\n\n        __m256i neg_values = _mm256_and_si256(vec, is_neg);\n        __m256i pos_values = _mm256_and_si256(vec, is_pos);\n\n        max_neg = _mm256_max_epi32(max_neg, neg_values);\n        min_pos = _mm256_min_epi32(min_pos, pos_values);\n    }\n\n    int max_neg_scalar = std::numeric_limits<int>::min();\n    int min_pos_scalar = std::numeric_limits<int>::max();\n\n    alignas(32) int max_neg_array[8];\n    alignas(32) int min_pos_array[8];\n\n    _mm256_store_si256((__m256i*)max_neg_array, max_neg);\n    _mm256_store_si256((__m256i*)min_pos_array, min_pos);\n\n    for (int j = 0; j < 8; ++j) {\n        max_neg_scalar = std::max(max_neg_scalar, max_neg_array[j]);\n        min_pos_scalar = std::min(min_pos_scalar, min_pos_array[j]);\n    }\n\n    for (; i < size; ++i) {\n        if (data[i] < 0) {\n            max_neg_scalar = std::max(max_neg_scalar, data[i]);\n        } else if (data[i] > 0) {\n            min_pos_scalar = std::min(min_pos_scalar, data[i]);\n        }\n    }\n\n    if (max_neg_scalar == std::numeric_limits<int>::min()) {\n        max_neg_scalar = 0;\n    }\n    if (min_pos_scalar == std::numeric_limits<int>::max()) {\n        min_pos_scalar = 0;\n    }\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t factorial(uint64_t n) {\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        result *= i;\n    }\n    return result;\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t i = 1;\n\n    // Process in chunks of 4 using AVX2\n    __m256i vec_result = _mm256_set1_epi64x(1); // Initialize vector to hold results\n    for (; i + 3 <= n; i += 4) {\n        __m256i vec_i = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i vec_fact = _mm256_set_epi64x(factorial(i + 3), factorial(i + 2), factorial(i + 1), factorial(i));\n        vec_result = _mm256_mul_epu32(vec_result, vec_fact);\n    }\n\n    // Extract results from the vector and multiply them\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i *)temp, vec_result);\n    for (int j = 0; j < 4; j++) {\n        result *= temp[j];\n    }\n\n    // Handle remaining values\n    for (; i <= n; i++) {\n        result *= factorial(i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t factorial(uint64_t n) {\n    uint64_t result = 1;\n    for (uint64_t i = 2; i <= n; i++) {\n        result *= i;\n    }\n    return result;\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t temp[4];\n\n    for (uint64_t i = n; i > 0; i -= 4) {\n        __m256i vec = _mm256_set_epi64x(\n            i >= 4 ? factorial(i - 3) : 1,\n            i >= 3 ? factorial(i - 2) : 1,\n            i >= 2 ? factorial(i - 1) : 1,\n            factorial(i)\n        );\n\n        _mm256_storeu_si256((__m256i *)temp, vec);\n\n        result *= temp[0];\n        result *= temp[1];\n        result *= temp[2];\n        result *= temp[3];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n\n    for (uint64_t i = n; i > 0; i -= 4) {\n        __m256i factorials = _mm256_set1_epi64x(1);\n        __m256i values = _mm256_set_epi64x(\n            i >= 4 ? i - 3 : 1,\n            i >= 3 ? i - 2 : 1,\n            i >= 2 ? i - 1 : 1,\n            i\n        );\n\n        for (uint64_t j = 1; j <= i; ++j) {\n            __m256i multiplier = _mm256_set1_epi64x(j);\n            factorials = _mm256_mullo_epi64(factorials, multiplier);\n        }\n\n        __m256i temp_result = _mm256_mullo_epi64(factorials, values);\n        result *= ((uint64_t*)&temp_result)[0];\n        result *= ((uint64_t*)&temp_result)[1];\n        result *= ((uint64_t*)&temp_result)[2];\n        result *= ((uint64_t*)&temp_result)[3];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t factorial(uint64_t n) {\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        result *= i;\n    }\n    return result;\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t temp[4];\n    __m256i vec_result = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = n; i > 0; i -= 4) {\n        temp[0] = (i >= 1) ? factorial(i) : 1;\n        temp[1] = (i >= 2) ? factorial(i - 1) : 1;\n        temp[2] = (i >= 3) ? factorial(i - 2) : 1;\n        temp[3] = (i >= 4) ? factorial(i - 3) : 1;\n\n        __m256i vec_factorials = _mm256_loadu_si256((__m256i *)temp);\n        vec_result = _mm256_mul_epu32(vec_result, vec_factorials);\n    }\n\n    _mm256_storeu_si256((__m256i *)temp, vec_result);\n    result = temp[0] * temp[1] * temp[2] * temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t factorial(uint64_t n) {\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        result *= i;\n    }\n    return result;\n}\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t temp[4];\n    __m256i vec_result = _mm256_set1_epi64x(1);\n\n    while (n > 0) {\n        uint64_t batch[4] = {n, n > 1 ? n - 1 : 1, n > 2 ? n - 2 : 1, n > 3 ? n - 3 : 1};\n        for (int i = 0; i < 4; i++) {\n            temp[i] = factorial(batch[i]);\n        }\n\n        __m256i vec_factorials = _mm256_loadu_si256((__m256i *)temp);\n        vec_result = _mm256_mullo_epi64(vec_result, vec_factorials);\n\n        n -= 4;\n    }\n\n    _mm256_storeu_si256((__m256i *)temp, vec_result);\n    for (int i = 0; i < 4; i++) {\n        result *= temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const int* data = lst.data();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i idx = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n\n        __m256i mod3 = _mm256_rem_epi32(idx, three);\n        __m256i mod4 = _mm256_rem_epi32(idx, four);\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_cmpeq_epi32(mod4, _mm256_setzero_si256());\n        __m256i is_multiple_of_4_not_3 = _mm256_andnot_si256(is_multiple_of_3, is_multiple_of_4);\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(squared, values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, is_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, is_multiple_of_4_not_3);\n\n        sum = _mm256_add_epi32(sum, result);\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(8));\n    }\n\n    int final_sum = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    for (int j = 0; j < 8; ++j) {\n        final_sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        final_sum += value;\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const int *data = lst.data();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i *)(data + i));\n        __m256i idx = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n\n        __m256i mod3 = _mm256_rem_epi32(idx, _mm256_set1_epi32(3));\n        __m256i mod4 = _mm256_rem_epi32(idx, _mm256_set1_epi32(4));\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_andnot_si256(is_multiple_of_3, _mm256_cmpeq_epi32(mod4, _mm256_setzero_si256()));\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(squared, values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, is_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, is_multiple_of_4);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    alignas(32) int buffer[8];\n    _mm256_store_si256((__m256i *)buffer, sum);\n\n    int total = 0;\n    for (int i = 0; i < 8; ++i) {\n        total += buffer[i];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256(); // Initialize sum vector to zero\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_loadu_si256((__m256i const *)&lst[i]);\n\n        __m256i mod3 = _mm256_rem_epi32(indices, three);\n        __m256i mod4 = _mm256_rem_epi32(indices, four);\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod3, zero);\n        __m256i is_multiple_of_4 = _mm256_cmpeq_epi32(mod4, zero);\n        __m256i is_multiple_of_4_not_3 = _mm256_andnot_si256(is_multiple_of_3, is_multiple_of_4);\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(squared, values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, is_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, is_multiple_of_4_not_3);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    int sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i *)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    const int* data = lst.data();\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_loadu_si256((__m256i*)(data + i));\n\n        __m256i mod3 = _mm256_rem_epi32(indices, _mm256_set1_epi32(3));\n        __m256i mod4 = _mm256_rem_epi32(indices, _mm256_set1_epi32(4));\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_andnot_si256(is_multiple_of_3, _mm256_cmpeq_epi32(mod4, _mm256_setzero_si256()));\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(squared, values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, is_multiple_of_3);\n        result = _mm256_blendv_epi8(result, cubed, is_multiple_of_4);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    int sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        int value = data[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        sum += value;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_loadu_si256((__m256i const *)(&lst[i]));\n\n        __m256i mod3 = _mm256_rem_epi32(indices, _mm256_set1_epi32(3));\n        __m256i mod4 = _mm256_rem_epi32(indices, _mm256_set1_epi32(4));\n\n        __m256i is_multiple_of_3 = _mm256_cmpeq_epi32(mod3, _mm256_setzero_si256());\n        __m256i is_multiple_of_4 = _mm256_cmpeq_epi32(mod4, _mm256_setzero_si256());\n        __m256i is_not_multiple_of_3 = _mm256_xor_si256(is_multiple_of_3, _mm256_set1_epi32(-1));\n\n        __m256i cube_mask = _mm256_and_si256(is_multiple_of_4, is_not_multiple_of_3);\n        __m256i square_mask = is_multiple_of_3;\n\n        __m256i squared_values = _mm256_mullo_epi32(values, values);\n        __m256i cubed_values = _mm256_mullo_epi32(squared_values, values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared_values, square_mask);\n        result = _mm256_blendv_epi8(result, cubed_values, cube_mask);\n\n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i *)temp, sum_vec);\n\n    int sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        sum += value;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int n = nums.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int count = 0;\n\n    __m256i odd_mask = _mm256_set1_epi32(0xAAAAAAAA); // Mask for odd digits (binary representation)\n    __m256i ten = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i  = _mm256_set1_epi32(10); // Mask for odd digits (binary representation)\n    __m256i"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int n = nums.size();\n    int count = 0;\n    const __m256i odd_digits = _mm256_set_epi32(9, 7, 5, 3, 1, 9, 7, 5);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        //.."}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    int count = 0;\n    size_t size = nums.size();\n    size_t simdWidth = 8; // AVX2 processes 8 integers at a time\n    size_t simdIterations = size / simdWidth;\n    size_t remainder = size % simdWidth;\n\n    __m256i oddMask = _mm256_set1_epi32(0x55555555); // Mask for odd digits (1, 3, 5, 7, 9)\n    __m256i ten = _mm256_set1_epi32(10); // Threshold value\n\n    for (size_t i = 0; i < simdIterations; ++i) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i * simdWidth]);\n\n        // Check if numbers are greater than 10\n        __m256i greaterThanTen = _mm256_cmpgt_epi32(vec, ten);\n\n        // Compute absolute values\n        __m256i absVec = _mm256_abs_epi32(vec);\n\n        // Extract last digits\n        __m256i lastDigit = _mm256_rem_epi32(absVec, _mm256_set1_epi32(10));\n\n        // Check if last digits are odd\n        __m256i lastDigitOdd = _mm256_and_si256(lastDigit, oddMask);\n\n        // Extract first digits\n        __m256i firstDigit = absVec;\n        while (true) {\n            __m256i greaterThanNine = _mm256_cmpgt_epi32(firstDigit, _mm256_set1_epi32(9));\n            if (_mm256_testz_si256(greaterThanNine, greaterThanNine)) break;\n            firstDigit = _mm256_div_epi32(firstDigit, _mm256_set1_epi32(10));\n        }\n\n        // Check if first digits are odd\n        __m256i firstDigitOdd = _mm256_and_si256(firstDigit, oddMask);\n\n        // Combine all conditions\n        __m256i valid = _mm256_and_si256(greaterThanTen, _mm256_and_si256(lastDigitOdd, firstDigitOdd));\n\n        // Count valid elements\n        count += _mm256_popcnt_epi32(valid);\n    }\n\n    // Process remainder elements\n    for (size_t i = simdIterations * simdWidth; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int absNum = std::abs(num);\n            int lastDigit = absNum % 10;\n            int firstDigit = absNum;\n            while (firstDigit >= 10) {\n                firstDigit /= 10;\n            }\n            if ((lastDigit % 2 == 1) && (firstDigit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int n = nums.size();\n    int count = 0;\n\n    // Process in chunks of 8 integers using AVX2\n    int i = 0;\n    __m256i vec_count = _mm256_setzero_si256(); // Initialize count vector to zero\n\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec_nums = _mm256_loadu_si256((__m256i*)&nums[i]); // Load 8 integers\n        __m256i vec_gt10 = _mm256_cmpgt_epi32(vec_nums, _mm256_set1_epi32(10)); // Compare nums > 10\n\n        // Absolute values of numbers\n        __m256i vec_abs = _mm256_abs_epi32(vec_nums);\n\n        // Extract last digit: abs(nums) % 10\n        __m256i vec_last_digit = _mm256_rem_epi32(vec_abs, _mm256_set1_epi32(10));\n\n        // Extract first digit: abs(nums) / pow(10, floor(log10(abs(nums))))\n        __m256i vec_first_digit = vec_abs;\n        for (int j = 0; j < 9; ++j) {\n            __m256i mask = _mm256_cmpgt_epi32(vec_first_digit, _mm256_set1_epi32(9));\n            vec_first_digit = _mm256_blendv_epi8(vec_first_digit, _mm256_div_epi32(vec_first_digit, _mm256_set1_epi32(10)), mask);\n        }\n\n        // Check if first and last digits are odd\n        __m256i vec_first_odd = _mm256_and_si256(vec_first_digit, _mm256_set1_epi32(1));\n        __m256i vec_last_odd = _mm256_and_si256(vec_last_digit, _mm256_set1_epi32(1));\n        __m256i vec_odd_digits = _mm256_and_si256(vec_first_odd, vec_last_odd);\n\n        // Combine conditions: nums > 10 && first and last digits are odd\n        __m256i vec_result = _mm256_and_si256(vec_gt10, vec_odd_digits);\n\n        // Count the number of matches\n        vec_count = _mm256_add_epi32(vec_count, _mm256_and_si256(vec_result, _mm256_set1_epi32(1)));\n    }\n\n    // Sum up the counts from the vector\n    alignas(32) int counts[8];\n    _mm256_store_si256((__m256i*)counts, vec_count);\n    for (int j = 0; j < 8; ++j) {\n        count += counts[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            if ((first_digit % 2 == 1) && (last_digit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int size = nums.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time\n    int result = 0;\n\n    __m256i vec_10 = _mm256_set1_epi32(10); // Vector of 10\n    __m256i vec_zero = _mm256_set1_epi32(0); // Vector of 0\n    __m256i vec_odd_mask = _mm256_set1_epi32(0x55555555); // Mask for odd digits (binary pattern for odd numbers)\n\n    int i = 0;\n    for (; i <= size - simd_width; i += simd_width) {\n        __m256i vec_nums = _mm256_loadu_si256((__m256i*)&nums[i]); // Load 8 integers\n\n        // Check if nums > 10\n        __m256i cmp_gt_10 = _mm256_cmpgt_epi32(vec_nums, vec_10);\n\n        // Take absolute value of nums\n        __m256i abs_nums = _mm256_abs_epi32(vec_nums);\n\n        // Extract last digit: abs_nums % 10\n        __m256i last_digit = _mm256_rem_epi32(abs_nums, _mm256_set1_epi32(10));\n\n        // Check if last digit is odd\n        __m256i last_digit_odd = _mm256_and_si256(last_digit, vec_odd_mask);\n\n        // Extract first digit\n        __m256i first_digit = abs_nums;\n        while (true) {\n            __m256i cmp_lt_10 = _mm256_cmpgt_epi32(first_digit, vec_10);\n            if (_mm256_testz_si256(cmp_lt_10, cmp_lt_10)) break;\n            first_digit = _mm256_div_epi32(first_digit, _mm256_set1_epi32(10));\n        }\n\n        // Check if first digit is odd\n        __m256i first_digit_odd = _mm256_and_si256(first_digit, vec_odd_mask);\n\n        // Combine conditions: nums > 10 && first_digit_odd && last_digit_odd\n        __m256i condition = _mm256_and_si256(cmp_gt_10, _mm256_and_si256(first_digit_odd, last_digit_odd));\n\n        // Count the number of elements satisfying the condition\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(condition));\n        result += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            if ((last_digit % 2 == 1) && (first_digit % 2 == 1)) {\n                result++;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Step 1: Generate the array `a` using SIMD\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    __m256i idx = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i step = _mm256_set1_epi64x(4);\n    __m256i one = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i i_vec = _mm256_add_epi64(idx, _mm256_set1_epi64x(i));\n        __m256i i_squared = _mm256_mullo_epi64(i_vec, i_vec);\n        __m256i result = _mm256_sub_epi64(_mm256_add_epi64(i_squared, one), i_vec);\n        _mm256_store_si256((__m256i *)(a + i), result);\n        idx = _mm256_add_epi64(idx, step);\n    }\n\n    // Step 2: Count remainders modulo 3\n    uint64_t count[3] = {0, 0, 0};\n    for (uint64_t i = 0; i < n; i++) {\n        count[a[i] % 3]++;\n    }\n\n    free(a);\n\n    // Step 3: Calculate the number of valid triples\n    uint64_t total = 0;\n    // Case 1: All three numbers have the same remainder\n    for (int r = 0; r < 3; r++) {\n        if (count[r] >= 3) {\n            total += (count[r] * (count[r] - 1) * (count[r] - 2)) / 6;\n        }\n    }\n    // Case 2: Three numbers have different remainders (0, 1, 2)\n    total += count[0] * count[1] * count[2];\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Step 1: Generate the array `a` using SIMD\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    __m256i indices = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i increment = _mm256_set1_epi64x(4);\n    __m256i ones = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i idx = _mm256_add_epi64(indices, _mm256_set1_epi64x(i));\n        __m256i squared = _mm256_mullo_epi64(idx, idx);\n        __m256i result = _mm256_sub_epi64(squared, idx);\n        result = _mm256_add_epi64(result, ones);\n        _mm256_store_si256((__m256i *)(a + i), result);\n        indices = _mm256_add_epi64(indices, increment);\n    }\n\n    // Step 2: Count triples where a[i] + a[j] + a[k] is a multiple of 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; i++) {\n        for (uint64_t j = i + 1; j < n - 1; j++) {\n            for (uint64_t k = j + 1; k < n; k++) {\n                uint64_t sum = a[i] + a[j] + a[k];\n                if (sum % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Step 1: Generate the array `a` using SIMD\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    __m256i idx = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i step = _mm256_set1_epi64x(4);\n    __m256i one = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i i_vec = _mm256_add_epi64(idx, _mm256_set1_epi64x(i));\n        __m256i i_squared = _mm256_mullo_epi64(i_vec, i_vec);\n        __m256i result = _mm256_sub_epi64(_mm256_add_epi64(i_squared, one), i_vec);\n        _mm256_store_si256((__m256i *)(a + i), result);\n        idx = _mm256_add_epi64(idx, step);\n    }\n\n    // Step 2: Count occurrences of a[i] % 3\n    uint64_t count_mod[3] = {0, 0, 0};\n    __m256i mod3 = _mm256_set1_epi64x(3);\n    __m256i zero = _mm256_setzero_si256();\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i vec = _mm256_load_si256((__m256i *)(a + i));\n        __m256i mod_result = _mm256_rem_epi64(vec, mod3);\n\n        for (int j = 0; j < 4; ++j) {\n            uint64_t mod_val = ((uint64_t *)&mod_result)[j];\n            count_mod[mod_val]++;\n        }\n    }\n\n    free(a);\n\n    // Step 3: Calculate the number of valid triples\n    uint64_t total_triples = 0;\n\n    // Case 1: All three numbers are in the same mod group\n    for (int i = 0; i < 3; ++i) {\n        if (count_mod[i] >= 3) {\n            uint64_t c = count_mod[i];\n            total_triples += (c * (c - 1) * (c - 2)) / 6;\n        }\n    }\n\n    // Case 2: Numbers are from different mod groups\n    total_triples += count_mod[0] * count_mod[1] * count_mod[2];\n\n    return total_triples;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Allocate memory for the array\n    uint64_t* a = (uint64_t*)aligned_alloc(32, n * sizeof(uint64_t));\n\n    // Compute the array values using SIMD\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i indices = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i squares = _mm256_mullo_epi64(indices, indices);\n        __m256i linear = _mm256_sub_epi64(squares, indices);\n        __m256i result = _mm256_add_epi64(linear, _mm256_set1_epi64x(1));\n        _mm256_store_si256((__m256i*)(a + i), result);\n    }\n\n    // Count triples where the sum is divisible by 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; i++) {\n        for (uint64_t j = i + 1; j < n; j++) {\n            for (uint64_t k = j + 1; k < n; k++) {\n                if ((a[i] + a[j] + a[k]) % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    // Free allocated memory\n    free(a);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Step 1: Generate the array `a` using SIMD\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    __m256i idx = _mm256_set_epi64x(4, 3, 2, 1);\n    __m256i one = _mm256_set1_epi64x(1);\n    __m256i inc = _mm256_set1_epi64x(4);\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i i_vec = _mm256_add_epi64(idx, _mm256_set1_epi64x(i));\n        __m256i i_squared = _mm256_mullo_epi64(i_vec, i_vec);\n        __m256i result = _mm256_sub_epi64(_mm256_add_epi64(i_squared, one), i_vec);\n        _mm256_store_si256((__m256i *)(a + i), result);\n        idx = _mm256_add_epi64(idx, inc);\n    }\n\n    // Step 2: Count remainders of elements in `a` modulo 3\n    uint64_t count[3] = {0, 0, 0};\n    __m256i mod3 = _mm256_set1_epi64x(3);\n    __m256i zero = _mm256_setzero_si256();\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m256i vec = _mm256_load_si256((__m256i *)(a + i));\n        __m256i rem = _mm256_rem_epi64(vec, mod3);\n\n        for (int j = 0; j < 4; ++j) {\n            uint64_t r = ((uint64_t *)&rem)[j];\n            count[r]++;\n        }\n    }\n\n    free(a);\n\n    // Step 3: Calculate the number of valid triples\n    uint64_t c0 = count[0], c1 = count[1], c2 = count[2];\n    uint64_t result = 0;\n\n    // Case 1: All three elements are divisible by 3\n    if (c0 >= 3) result += (c0 * (c0 - 1) * (c0 - 2)) / 6;\n\n    // Case 2: One element from each group (0, 1, 2)\n    result += c0 * c1 * c2;\n\n    // Case 3: Three elements from group 1\n    if (c1 >= 3) result += (c1 * (c1 - 1) * (c1 - 2)) / 6;\n\n    // Case 4: Three elements from group 2\n    if (c2 >= 3) result += (c2 * (c2 - 1) * (c2 - 2)) / 6;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) {\n        return y;\n    }\n\n    // Check for primality using SIMD\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_divisors = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);\n    __m256i vec_zeros = _mm256_setzero_si256();\n\n    // Perform division and check for remainder\n    __m256i vec_remainders = _mm256_rem_epi32(vec_n, vec_divisors);\n    __m256i vec_cmp = _mm256_cmpeq_epi32(vec_remainders, vec_zeros);\n\n    // Check if any divisor divides n\n    int mask = _mm256_movemask_epi8(vec_cmp);\n    if (mask != 0) {\n        return y; // Not a prime number\n    }\n\n    return x; // Prime number\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) return y;\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int limit = (int)sqrt((double)n);\n    __m256i vec = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Broadcast n across all lanes\n    __m256i n_vec = _mm256_set1_epi32(n);\n\n    // Perform modulo operation to check divisibility\n    __m256i mod_vec = _mm256_rem_epi32(n_vec, vec);\n\n    // Compare results to zero\n    __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n\n    // Extract mask\n    int mask = _mm256_movemask_epi8(cmp_vec);\n\n    // If any lane is divisible, n is not prime\n    if (mask != 0) {\n        return y;\n    }\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) return y;\n\n    // Create a vector of integers from 2 to sqrt(n) (or up to n-1 if sqrt(n) < 2)\n    int limit = (int)_mm_sqrt_ps(_mm_set_ss((float)n))[0];\n    limit = (limit < 2) ? n - 1 : limit;\n\n    // Check divisors in parallel using AVX2\n    __m256i divisor_vec = _mm256_set_epi32(8, 7, 6, 5, 4, 3, 2, 1);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (int i = 1; i <= limit; i += 8) {\n        __m256i current_divisors = _mm256_add_epi32(divisor_vec, _mm256_set1_epi32(i - 1));\n        __m256i remainders = _mm256_rem_epi32(n_vec, current_divisors);\n        __m256i cmp_result = _mm256_cmpeq_epi32(remainders, zero_vec);\n\n        if (_mm256_movemask_epi8(cmp_result) != 0) {\n            return y; // Not a prime number\n        }\n    }\n\n    return x; // Prime number\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) return y;\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int limit = (int)sqrt((double)n);\n    __m256i vec = _mm256_set_epi32(limit, limit - 1, limit - 2, limit - 3, limit - 4, limit - 5, limit - 6, limit - 7);\n\n    // Broadcast n across all lanes\n    __m256i n_vec = _mm256_set1_epi32(n);\n\n    // Perform modulo operation for each lane\n    __m256i mod_result = _mm256_rem_epi32(n_vec, vec);\n\n    // Compare results to zero (prime check)\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, zero_vec);\n\n    // Check if any lane is zero (indicating n is divisible by a number other than 1 and itself)\n    int mask = _mm256_movemask_epi8(cmp_result);\n\n    // If mask is non-zero, n is not prime\n    return (mask == 0) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) {\n        return y;\n    }\n\n    // Check divisors from 2 to sqrt(n) using SIMD\n    int limit = (int)sqrt((double)n);\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int i = 2; i <= limit; i += 8) {\n        __m256i vec_i = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec_rem = _mm256_rem_epi32(vec_n, vec_i); // Compute n % i for 8 values\n        __m256i cmp = _mm256_cmpeq_epi32(vec_rem, vec_zero); // Check if remainder is zero\n\n        if (_mm256_movemask_epi8(cmp)) {\n            return y; // n is not prime\n        }\n    }\n\n    return x; // n is prime\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    if (lst.empty()) return 0;\n\n    const size_t size = lst.size();\n    const size_t simd_width = 8; // AVX processes 8 floats at a time\n    size_t i = 0;\n    uint64_t result = 0;\n\n    __m256i sum = _mm256_setzero_si256(); // Initialize sum to zero\n\n    for (; i + simd_width <= size; i += simd_width) {\n        __m256 input = _mm256_loadu_ps(&lst[i]); // Load 8 floats\n        __m256 zero = _mm256_setzero_ps();\n        __m256 mask_positive = _mm256_cmp_ps(input, zero, _CMP_GT_OQ); // Mask for positive numbers\n\n        __m256i input_int = _mm256_cvttps_epi32(input); // Convert to integers\n        __m256 converted_back = _mm256_cvtepi32_ps(input_int); // Convert back to float\n        __m256 mask_integer = _mm256_cmp_ps(input, converted_back, _CMP_EQ_OQ); // Mask for integers\n\n        __m256 combined_mask = _mm256_and_ps(mask_positive, mask_integer); // Combine masks\n        __m256 filtered_input = _mm256_and_ps(input, combined_mask); // Apply mask\n\n        __m256i input_int_filtered = _mm256_cvttps_epi32(filtered_input); // Convert filtered input to integers\n        __m256i odd_mask = _mm256_and_si256(input_int_filtered, _mm256_set1_epi32(1)); // Mask for odd numbers\n        __m256i is_odd = _mm256_cmpeq_epi32(odd_mask, _mm256_set1_epi32(1)); // Check if odd\n\n        __m256i squared = _mm256_mullo_epi32(input_int_filtered, input_int_filtered); // Square the integers\n        __m256i squared_odd = _mm256_and_si256(squared, is_odd); // Apply odd mask\n\n        sum = _mm256_add_epi64(sum, _mm256_cvtepi32_epi64(squared_odd)); // Accumulate\n    }\n\n    // Horizontal sum of the accumulated values in the SIMD register\n    alignas(32) uint64_t temp[4];\n    _mm256_store_si256((__m256i *)temp, sum);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && std::floor(val) == val && ((int)val % 2 != 0)) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    if (lst.empty()) return 0;\n\n    size_t size = lst.size();\n    size_t simd_width = 8; // AVX2 processes 8 floats at a time\n    size_t aligned_size = size - (size % simd_width);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sum = zero;\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        __m256i int_values = _mm256_cvttps_epi32(input);\n\n        __m256 float_values = _mm256_cvtepi32_ps(int_values);\n        __m256 mask_negative = _mm256_cmp_ps(input, float_values, _CMP_EQ_OQ);\n        __m256 mask_positive = _mm256_cmp_ps(input, zero, _CMP_GT_OQ);\n        __m256 mask_valid = _mm256_and_ps(mask_negative, mask_positive);\n\n        __m256i valid_ints = _mm256_and_si256(int_values, _mm256_castps_si256(mask_valid));\n        __m256i odd_mask = _mm256_and_si256(valid_ints, _mm256_set1_epi32(1));\n        __m256i is_odd = _mm256_cmpeq_epi32(odd_mask, _mm256_set1_epi32(1));\n\n        __m256i squared = _mm256_mullo_epi32(valid_ints, valid_ints);\n        __m256i odd_squares = _mm256_and_si256(squared, is_odd);\n\n        sum = _mm256_add_epi64(sum, _mm256_cvtepi32_epi64(odd_squares));\n    }\n\n    uint64_t result = 0;\n    alignas(32) uint64_t buffer[4];\n    _mm256_store_si256((__m256i *)buffer, sum);\n    for (size_t i = 0; i < 4; ++i) {\n        result += buffer[i];\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && val == (int)val && ((int)val % 2 != 0)) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    uint64_t result = 0;\n    __m256i sum = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        __m256i int_values = _mm256_cvttps_epi32(input);\n\n        __m256 float_values = _mm256_cvtepi32_ps(int_values);\n        __m256 mask_negative = _mm256_cmp_ps(input, _mm256_setzero_ps(), _CMP_GE_OQ);\n        __m256 mask_integer = _mm256_cmp_ps(input, float_values, _CMP_EQ_OQ);\n\n        __m256 mask_valid = _mm256_and_ps(mask_negative, mask_integer);\n        __m256i valid_ints = _mm256_and_si256(int_values, _mm256_castps_si256(mask_valid));\n\n        __m256i odd_mask = _mm256_and_si256(valid_ints, _mm256_set1_epi32(1));\n        __m256i is_odd = _mm256_cmpeq_epi32(odd_mask, _mm256_set1_epi32(1));\n\n        __m256i squared = _mm256_mullo_epi32(valid_ints, valid_ints);\n        __m256i odd_squares = _mm256_and_si256(squared, is_odd);\n\n        sum = _mm256_add_epi64(sum, _mm256_cvtepi32_epi64(_mm256_extracti128_si256(odd_squares, 0)));\n        sum = _mm256_add_epi64(sum, _mm256_cvtepi32_epi64(_mm256_extracti128_si256(odd_squares, 1)));\n    }\n\n    alignas(32) uint64_t temp[4];\n    _mm256_store_si256((__m256i *)temp, sum);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val && ((int)val & 1)) {\n            result += (uint64_t)((int)val * (int)val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    size_t i = 0;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i two = _mm256_set1_epi32(2);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        __m256i integers = _mm256_cvttps_epi32(input);\n\n        __m256 float_integers = _mm256_cvtepi32_ps(integers);\n        __m256 mask = _mm256_cmp_ps(input, float_integers, _CMP_EQ_OQ);\n\n        __m256i valid_integers = _mm256_and_si256(_mm256_castps_si256(mask), integers);\n        __m256i positive_mask = _mm256_cmpgt_epi32(valid_integers, zero);\n        __m256i positive_integers = _mm256_and_si256(valid_integers, positive_mask);\n\n        __m256i odd_mask = _mm256_and_si256(positive_integers, _mm256_set1_epi32(1));\n        __m256i odd_integers = _mm256_and_si256(positive_integers, odd_mask);\n\n        __m256i squares = _mm256_mullo_epi32(odd_integers, odd_integers);\n        sum = _mm256_add_epi32(sum, squares);\n    }\n\n    alignas(32) int32_t buffer[8];\n    _mm256_store_si256((__m256i *)buffer, sum);\n    for (int j = 0; j < 8; ++j) {\n        result += buffer[j];\n    }\n\n    for (; i < n; ++i) {\n        float num = lst[i];\n        if (num >= 0 && num == (int)num && ((int)num % 2 != 0)) {\n            int int_num = (int)num;\n            result += int_num * int_num;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    size_t i = 0;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for (; i + 8 <= n; i += 8) {\n        __m256 values = _mm256_loadu_ps(&lst[i]);\n        __m256 mask_positive = _mm256_cmp_ps(values, zero, _CMP_GE_OQ);\n        __m256i int_values = _mm256_cvttps_epi32(values);\n        __m256 float_int_values = _mm256_cvtepi32_ps(int_values);\n        __m256 mask_integer = _mm256_cmp_ps(values, float_int_values, _CMP_EQ_OQ);\n        __m256 mask_odd = _mm256_cmp_ps(_mm256_fmod_ps(values, two), one, _CMP_EQ_OQ);\n        __m256 mask = _mm256_and_ps(mask_positive, _mm256_and_ps(mask_integer, mask_odd));\n        __m256 squared = _mm256_mul_ps(values, values);\n        __m256 filtered = _mm256_and_ps(squared, mask);\n        __m256i filtered_int = _mm256_cvttps_epi32(filtered);\n        sum = _mm256_add_epi64(sum, _mm256_cvtepi32_epi64(filtered_int));\n    }\n\n    alignas(32) uint64_t temp[4];\n    _mm256_store_si256((__m256i *)temp, sum);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        float value = lst[i];\n        if (value >= 0 && value == (int)value && ((int)value % 2 == 1)) {\n            result += (uint64_t)(value * value);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t i = 0;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff_vec);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t i = 0;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + simd_width <= n; i += simd_width) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff_vec);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n\n    // Process 8 elements at a time\n    for (; i + 8 <= size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        __m256i cmp = _mm256_cmpeq_epi32(game_vec, guess_vec);\n        __m256i result_vec = _mm256_blendv_epi8(abs_diff, zero, cmp);\n\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        result[i] = (game[i] == guess[i]) ? 0 : std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time\n    size_t simd_end = size - (size % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i mask_vec = _mm256_cmpeq_epi32(game_vec, guess_vec);\n\n        __m256i result_vec = _mm256_blendv_epi8(abs_diff_vec, zero_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec);\n    }\n\n    for (size_t i = simd_end; i < size; ++i) {\n        result[i] = (game[i] == guess[i]) ? 0 : std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    size_t simd_end = size - (size % simd_width); // Determine where SIMD processing ends\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        // Load 8 integers from game and guess vectors into SIMD registers\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        // Compute the absolute difference: abs(game_vec - guess_vec)\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        // Store the result back into the result vector\n        _mm256_storeu_si256((__m256i*)&result[i], abs_diff_vec);\n    }\n\n    // Handle remaining elements that couldn't be processed by SIMD\n    for (size_t i = simd_end; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time (128-bit registers can hold two 64-bit integers)\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 elements from src and dst\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i*)&dst[i]);\n\n        // Load 2 mask values (convert bool to 64-bit integers)\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1LL : 0LL, mask[i] ? -1LL : 0LL);\n\n        // Perform conditional move: if mask is true, take src; otherwise, keep dst\n        __m128i result_vec = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n\n        // Store the result back to dst\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 elements from src and mask\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        // Blend src and dst based on mask\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)&dst[i]);\n        __m128i result = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n\n        // Store the result back to dst\n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 elements from src and mask\n        __m128i src_vec = _mm_loadu_si128((__m128i *)(src + i));\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        // Perform conditional move\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)(dst + i));\n        __m128i result_vec = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n\n        // Store the result back to dst\n        _mm_storeu_si128((__m128i *)(dst + i), result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time (128 bits / 64 bits per element)\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 elements from src and dst\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i*)&dst[i]);\n\n        // Load 2 mask values and convert them to 64-bit integers\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        // Perform conditional move: blend src and dst based on mask\n        __m128i result_vec = _mm_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result back to dst\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time (128 bits / 64 bits per element)\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 elements from src\n        __m128i src_vec = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Load 2 elements from dst\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)(dst + i));\n\n        // Load 2 mask values and expand them to 64-bit integers\n        __m128i mask_vec = _mm_set_epi64x(mask[i + 1] ? -1LL : 0LL, mask[i] ? -1LL : 0LL);\n\n        // Blend src and dst based on the mask\n        __m128i result_vec = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n\n        // Store the result back to dst\n        _mm_storeu_si128((__m128i *)(dst + i), result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale); // Broadcast scale to all lanes\n    __m128i v_seven = _mm_set1_epi32(7);    // Broadcast 7 to all lanes\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from src\n        __m128i v_src = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Multiply src by scale\n        __m128i v_scaled = _mm_mullo_epi32(v_src, v_scale);\n\n        // Calculate tmp1: (element * scale) >> 3\n        __m128i v_tmp1 = _mm_srai_epi32(v_scaled, 3);\n\n        // Calculate tmp2: (element * scale) << 3\n        __m128i v_tmp2 = _mm_slli_epi32(v_scaled, 3);\n\n        // Calculate tmp1 * tmp2\n        __m128i v_tmp1_tmp2 = _mm_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Calculate final result: (element * scale + tmp1 * tmp2) / 7\n        __m128i v_result = _mm_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm_div_epi32(v_result, v_seven);\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int element = src[i];\n        int scaled = element * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_seven = _mm_set1_epi32(7);\n    size_t i;\n\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)(src + i));\n\n        __m128i v_scaled = _mm_mullo_epi32(v_src, v_scale);\n        __m128i v_tmp1 = _mm_srai_epi32(v_scaled, 3);\n        __m128i v_tmp2 = _mm_slli_epi32(v_scaled, 3);\n\n        __m128i v_result = _mm_add_epi32(v_scaled, _mm_mullo_epi32(v_tmp1, v_tmp2));\n        v_result = _mm_div_epi32(v_result, v_seven);\n\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale); // Broadcast scale to all elements\n    __m128i v_seven = _mm_set1_epi32(7);    // Broadcast 7 to all elements\n\n    size_t i;\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load 4 integers from src\n        __m128i v_src = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Compute element * scale\n        __m128i v_scaled = _mm_mullo_epi32(v_src, v_scale);\n\n        // Compute tmp1 = (element * scale) >> 3\n        __m128i v_tmp1 = _mm_srai_epi32(v_scaled, 3);\n\n        // Compute tmp2 = (element * scale) << 3\n        __m128i v_tmp2 = _mm_slli_epi32(v_scaled, 3);\n\n        // Compute tmp1 * tmp2\n        __m128i v_tmp1_tmp2 = _mm_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Compute final result: (element * scale + tmp1 * tmp2) / 7\n        __m128i v_result = _mm_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm_div_epi32(v_result, v_seven); // Divide by 7\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_seven = _mm_set1_epi32(7);\n    size_t i;\n\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n\n        __m128i v_scaled = _mm_mullo_epi32(v_src, v_scale);\n        __m128i v_tmp1 = _mm_srai_epi32(v_scaled, 3);\n        __m128i v_tmp2 = _mm_slli_epi32(v_scaled, 3);\n\n        __m128i v_tmp1_tmp2 = _mm_mullo_epi32(v_tmp1, v_tmp2);\n        __m128i v_result = _mm_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm_div_epi32(v_result, v_seven);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_seven = _mm_set1_epi32(7);\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_scaled = _mm_mullo_epi32(v_src, v_scale);\n\n        __m128i tmp1 = _mm_srai_epi32(v_scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(v_scaled, 3);\n\n        __m128i tmp1_tmp2 = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i result = _mm_add_epi32(v_scaled, tmp1_tmp2);\n        result = _mm_div_epi32(result, v_seven);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; i++) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n\n    // Process 2 rows at a time using SIMD\n    for (; i + 1 < rows; i += 2) {\n        // Calculate the indices for the strided elements\n        size_t index1 = i * cols + stride;\n        size_t index2 = (i + 1) * cols + stride;\n\n        // Load the strided elements from src\n        __m128d vec = _mm_set_pd(src[index2], src[index1]);\n\n        // Store the results in dst\n        _mm_storeu_pd(&dst[i], vec);\n    }\n\n    // Handle the remaining row if rows is odd\n    if (i < rows) {\n        size_t index = i * cols + stride;\n        dst[i] = src[index];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d temp; // Temporary variable for SIMD operations\n    size_t stride_offset = stride * sizeof(double); // Calculate stride offset in bytes\n\n    for (size_t r = 0; r < rows; r += 2) {\n        if (r + 1 < rows) {\n            // Load two elements from src using stride\n            temp = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n            // Store the two elements into dst\n            _mm_storeu_pd(&dst[r], temp);\n        } else {\n            // Handle the remaining single row if rows is odd\n            dst[r] = src[r * cols + stride];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r = 0;\n\n    // Process rows in chunks of 2 using SSE2\n    for (; r + 2 <= rows; r += 2) {\n        __m128d src_values = _mm_set_pd(\n            src[(r + 1) * cols + stride],\n            src[r * cols + stride]\n        );\n        _mm_storeu_pd(&dst[r], src_values);\n    }\n\n    // Handle remaining rows (if any)\n    for (; r < rows; ++r) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d temp; // Temporary variable for SIMD operations\n    size_t stride_offset = stride * sizeof(double); // Calculate the stride offset in bytes\n\n    for (size_t r = 0; r < rows; r += 2) {\n        if (r + 1 < rows) {\n            // Load two elements from src using the stride\n            temp = _mm_set_pd(src[(r + 1) * cols + stride], src[r * cols + stride]);\n            // Store the two elements into dst\n            _mm_storeu_pd(&dst[r], temp);\n        } else {\n            // Handle the last row if the number of rows is odd\n            dst[r] = src[r * cols + stride];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t stride_offset = stride * sizeof(double); // Calculate stride offset in bytes\n    for (size_t r = 0; r < rows; r += 2) {\n        if (r + 1 < rows) {\n            // Load two elements from src using stride\n            __m128d src_vals = _mm_set_pd(\n                *(src + (r + 1) * cols + stride),\n                *(src + r * cols + stride)\n            );\n            // Store the two elements into dst\n            _mm_storeu_pd(dst + r, src_vals);\n        } else {\n            // Handle the last row if rows is odd\n            dst[r] = *(src + r * cols + stride);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: Copy values from src to dst using indices as lookups\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]); // Load 4 indices\n        __m128 gathered = _mm_set_ps(\n            src[_mm_extract_epi32(idx, 3)],\n            src[_mm_extract_epi32(idx, 2)],\n            src[_mm_extract_epi32(idx, 1)],\n            src[_mm_extract_epi32(idx, 0)]\n        ); // Gather values from src using indices\n        _mm_storeu_ps(&dst[i], gathered); // Store gathered values into dst\n    }\n\n    // Handle remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: Copy values from src to dst using indices as write locations\n    for (i = 0; i + 4 <= length; i += 4) {\n        __m128 values = _mm_loadu_ps(&src[i]); // Load 4 values from src\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]); // Load 4 indices\n        dst[_mm_extract_epi32(idx, 0)] = _mm_cvtss_f32(values);\n        dst[_mm_extract_epi32(idx, 1)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(1, 1, 1, 1)));\n        dst[_mm_extract_epi32(idx, 2)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(2, 2, 2, 2)));\n        dst[_mm_extract_epi32(idx, 3)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(3, 3, 3, 3)));\n    }\n\n    // Handle remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: Copy values from src to dst using indices as lookups\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i *)(indices + i)); // Load 4 indices\n        __m128 gathered = _mm_set_ps(\n            src[_mm_extract_epi32(idx, 3)],\n            src[_mm_extract_epi32(idx, 2)],\n            src[_mm_extract_epi32(idx, 1)],\n            src[_mm_extract_epi32(idx, 0)]\n        );\n        _mm_storeu_ps(dst + i, gathered); // Store gathered values into dst\n    }\n\n    // Handle remaining elements (non-SIMD part)\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: Copy values from src to dst using indices as write locations\n    i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 values = _mm_loadu_ps(src + i); // Load 4 values from src\n        __m128i idx = _mm_loadu_si128((__m128i *)(indices + i)); // Load 4 indices\n        dst[_mm_extract_epi32(idx, 0)] = _mm_cvtss_f32(values);\n        dst[_mm_extract_epi32(idx, 1)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(1, 1, 1, 1)));\n        dst[_mm_extract_epi32(idx, 2)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(2, 2, 2, 2)));\n        dst[_mm_extract_epi32(idx, 3)] = _mm_cvtss_f32(_mm_shuffle_ps(values, values, _MM_SHUFFLE(3, 3, 3, 3)));\n    }\n\n    // Handle remaining elements (non-SIMD part)\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n\n        // Gather values from src using indices\n        __m128 gathered = _mm_set_ps(\n            src[_mm_extract_epi32(idx, 3)],\n            src[_mm_extract_epi32(idx, 2)],\n            src[_mm_extract_epi32(idx, 1)],\n            src[_mm_extract_epi32(idx, 0)]\n        );\n\n        // Store gathered values into dst\n        _mm_storeu_ps(&dst[i], gathered);\n\n        // Scatter values from src to dst using indices\n        dst[_mm_extract_epi32(idx, 0)] = src[i];\n        dst[_mm_extract_epi32(idx, 1)] = src[i + 1];\n        dst[_mm_extract_epi32(idx, 2)] = src[i + 2];\n        dst[_mm_extract_epi32(idx, 3)] = src[i + 3];\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]]; // Gather\n        dst[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process in chunks of 4 using SIMD\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 indices\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n\n        // Gather values from src using indices\n        __m128 gathered = _mm_set_ps(\n            src[_mm_extract_epi32(idx, 3)],\n            src[_mm_extract_epi32(idx, 2)],\n            src[_mm_extract_epi32(idx, 1)],\n            src[_mm_extract_epi32(idx, 0)]\n        );\n\n        // Store gathered values into dst\n        _mm_storeu_ps(&dst[i], gathered);\n\n        // Scatter values from src to dst using indices\n        dst[_mm_extract_epi32(idx, 0)] = src[i];\n        dst[_mm_extract_epi32(idx, 1)] = src[i + 1];\n        dst[_mm_extract_epi32(idx, 2)] = src[i + 2];\n        dst[_mm_extract_epi32(idx, 3)] = src[i + 3];\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]]; // Gather\n        dst[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= length; i += 4) {\n        // Load indices into an SSE register\n        __m128i index_vec = _mm_loadu_si128((__m128i*)&indices[i]);\n\n        // Gather operation: Load values from src using indices\n        __m128 gathered_values = _mm_set_ps(\n            src[_mm_extract_epi32(index_vec, 3)],\n            src[_mm_extract_epi32(index_vec, 2)],\n            src[_mm_extract_epi32(index_vec, 1)],\n            src[_mm_extract_epi32(index_vec, 0)]\n        );\n\n        // Store gathered values into dst\n        _mm_storeu_ps(&dst[i], gathered_values);\n\n        // Scatter operation: Write values from src to dst using indices\n        dst[_mm_extract_epi32(index_vec, 0)] = src[i];\n        dst[_mm_extract_epi32(index_vec, 1)] = src[i + 1];\n        dst[_mm_extract_epi32(index_vec, 2)] = src[i + 2];\n        dst[_mm_extract_epi32(index_vec, 3)] = src[i + 3];\n    }\n\n    // Handle remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        int index = indices[i];\n        dst[i] = src[index];  // Gather operation\n        dst[index] = src[i];  // Scatter operation\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure start and end are within bounds and start < end\n    if (start >= end) return;\n\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (left < right) {\n        // Load 8 elements from the left and right sides using SSE2\n        __m128i left_vec = _mm_loadu_si128((__m128i *)(arr + left));\n        __m128i right_vec = _mm_loadu_si128((__m128i *)(arr + right - 7));\n\n        // Reverse the vectors\n        left_vec = _mm_shuffle_epi8(left_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        right_vec = _mm_shuffle_epi8(right_vec, _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Store reversed vectors back to their respective positions\n        _mm_storeu_si128((__m128i *)(arr + left), right_vec);\n        _mm_storeu_si128((__m128i *)(arr + right - 7), left_vec);\n\n        // Move the pointers inward\n        left += 8;\n        right -= 8;\n    }\n\n    // Handle remaining elements if the range is not a multiple of 8\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start >= end) return;\n\n    uint64_t left = start;\n    uint64_t right = end;\n\n    // Process elements in chunks of 8 (128 bits / 16 bits per element)\n    while (left + 7 <= right - 7) {\n        // Load 8 elements from the left and right sides\n        __m128i left_vec = _mm_loadu_si128((__m128i *)(arr + left));\n        __m128i right_vec = _mm_loadu_si128((__m128i *)(arr + right - 7));\n\n        // Reverse the vectors\n        left_vec = _mm_shuffle_epi8(left_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        right_vec = _mm_shuffle_epi8(right_vec, _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Store reversed vectors in swapped positions\n        _mm_storeu_si128((__m128i *)(arr + left), right_vec);\n        _mm_storeu_si128((__m128i *)(arr + right - 7), left_vec);\n\n        // Move indices inward\n        left += 8;\n        right -= 8;\n    }\n\n    // Handle remaining elements sequentially\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure start and end indices are valid and within bounds\n    if (start >= end) return;\n\n    uint64_t left = start;\n    uint64_t right = end;\n\n    while (left < right) {\n        // Load 8 elements from the left and right sides using SSE intrinsics\n        __m128i left_vec = _mm_loadu_si128((__m128i *)(arr + left));\n        __m128i right_vec = _mm_loadu_si128((__m128i *)(arr + right - 7));\n\n        // Reverse the vectors\n        left_vec = _mm_shuffle_epi8(left_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        right_vec = _mm_shuffle_epi8(right_vec, _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Store reversed vectors back to their respective positions\n        _mm_storeu_si128((__m128i *)(arr + left), right_vec);\n        _mm_storeu_si128((__m128i *)(arr + right - 7), left_vec);\n\n        // Move the pointers inward\n        left += 8;\n        right -= 8;\n    }\n\n    // Handle remaining elements if the range is not a multiple of 8\n    while (left < right) {\n        int16_t temp = arr[left];\n        arr[left] = arr[right];\n        arr[right] = temp;\n        left++;\n        right--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t len = end - start + 1;\n\n    // Process 8 elements at a time using SSE2\n    while (len >= 8) {\n        // Load 8 elements from the start and end ranges\n        __m128i start_vec = _mm_loadu_si128((__m128i *)(arr + start));\n        __m128i end_vec = _mm_loadu_si128((__m128i *)(arr + end - 7));\n\n        // Reverse the elements in the vectors\n        start_vec = _mm_shuffle_epi8(start_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n        end_vec = _mm_shuffle_epi8(end_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Store the reversed vectors back to their respective positions\n        _mm_storeu_si128((__m128i *)(arr + start), end_vec);\n        _mm_storeu_si128((__m128i *)(arr + end - 7), start_vec);\n\n        // Move the pointers inward\n        start += 8;\n        end -= 8;\n        len -= 16;\n    }\n\n    // Handle remaining elements sequentially\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    while (start < end) {\n        // Load 8 elements from the start and end indices\n        __m128i start_vec = _mm_loadu_si128((__m128i *)&arr[start]);\n        __m128i end_vec = _mm_loadu_si128((__m128i *)&arr[end - 7]);\n\n        // Reverse the elements in the start vector\n        start_vec = _mm_shuffle_epi8(start_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Reverse the elements in the end vector\n        end_vec = _mm_shuffle_epi8(end_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Store the reversed end vector at the start position\n        _mm_storeu_si128((__m128i *)&arr[start], end_vec);\n\n        // Store the reversed start vector at the end position\n        _mm_storeu_si128((__m128i *)&arr[end - 7], start_vec);\n\n        // Move the start and end indices closer\n        start += 8;\n        end -= 8;\n    }\n\n    // Handle any remaining elements with a scalar loop\n    while (start < end) {\n        int16_t temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n        start++;\n        end--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset to the start of the slice in the flattened tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 16 <= slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + slice_offset + i)); // Load 16 bytes from the tensor\n        _mm_storeu_si128((__m128i *)(slice + i), data);                         // Store 16 bytes into the slice\n    }\n\n    // Handle remaining elements that don't fit into 16-byte chunks\n    for (; i < slice_size; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset to the start of the slice in the 1D array\n    size_t total_elements = dim1 * dim2;          // Total elements in the 2D slice\n    size_t simd_width = 16;                       // SSE processes 16 bytes (128 bits) at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + slice_offset + i)); // Load 16 bytes from tensor\n        _mm_storeu_si128((__m128i *)(slice + i), data);                         // Store 16 bytes to slice\n    }\n\n    // Handle remaining elements that do not fit into a full SIMD register\n    for (; i < total_elements; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Calculate the offset for the slice in the flattened tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 16 <= slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + slice_offset + i)); // Load 16 bytes from tensor\n        _mm_storeu_si128((__m128i *)(slice + i), data);                         // Store 16 bytes into slice\n    }\n\n    // Handle remaining elements (if slice_size is not a multiple of 16)\n    for (; i < slice_size; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset for the slice in the flattened tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 16 <= slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + slice_offset + i)); // Load 16 bytes from tensor\n        _mm_storeu_si128((__m128i *)(slice + i), data);                         // Store 16 bytes into slice\n    }\n\n    // Handle remaining elements that are less than 16\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2; // Offset to the start of the slice in the flattened tensor\n    size_t slice_size = dim1 * dim2;              // Total number of elements in the slice\n\n    size_t i = 0;\n    for (; i + 16 <= slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + slice_offset + i)); // Load 16 bytes from the tensor\n        _mm_storeu_si128((__m128i *)(slice + i), data);                         // Store 16 bytes into the slice\n    }\n\n    // Handle remaining elements that are not a multiple of 16\n    for (; i < slice_size; i++) {\n        slice[i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 8) {\n                    size_t remaining_cols = block_col_end - bj;\n                    if (remaining_cols >= 8) {\n                        __m128i row_data = _mm_loadu_si128((__m128i *)&src[bi * cols + bj]);\n                        _mm_storeu_si128((__m128i *)&dst[bj * rows + bi], row_data);\n                    } else {\n                        for (size_t k = 0; k < remaining_cols; k++) {\n                            dst[(bj + k) * rows + bi] = src[bi * cols + bj + k];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, bi, bj;\n\n    for (bi = 0; bi < rows; bi += block_size) {\n        for (bj = 0; bj < cols; bj += block_size) {\n            size_t block_row_end = (bi + block_size > rows) ? rows : bi + block_size;\n            size_t block_col_end = (bj + block_size > cols) ? cols : bj + block_size;\n\n            for (i = bi; i < block_row_end; i++) {\n                for (j = bj; j < block_col_end; j += 8) {\n                    __m128i src_data = _mm_loadu_si128((__m128i *)&src[i * cols + j]);\n                    _mm_storeu_si128((__m128i *)&dst[j * rows + i], src_data);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 8) {\n                    __m128i src_data = _mm_loadu_si128((__m128i *)&src[bi * cols + bj]);\n                    _mm_storeu_si128((__m128i *)&dst[bj * rows + bi], src_data);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 8) {\n                    __m128i src_data = _mm_loadu_si128((__m128i *)&src[bi * cols + bj]);\n                    _mm_storeu_si128((__m128i *)&dst[bj * rows + bi], src_data);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_row_end = (i + block_size > rows) ? rows : i + block_size;\n            size_t block_col_end = (j + block_size > cols) ? cols : j + block_size;\n\n            for (size_t bi = i; bi < block_row_end; bi++) {\n                for (size_t bj = j; bj < block_col_end; bj += 8) {\n                    __m128i row_data = _mm_loadu_si128((__m128i *)&src[bi * cols + bj]);\n                    _mm_storeu_si128((__m128i *)&dst[bj * rows + bi], row_data);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps(); // Initialize SIMD register to store the sum\n    size_t i = 0;\n\n    // Process 4 diagonal elements at a time\n    for (; i + 4 <= dim; i += 4) {\n        // Calculate the indices of the diagonal elements\n        size_t idx0 = i * dim * dim + i * dim + i;\n        size_t idx1 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx2 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx3 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        // Load diagonal elements into SIMD register\n        __m128 diag = _mm_set_ps(array[idx3], array[idx2], array[idx1], array[idx0]);\n\n        // Accumulate the sum\n        sum = _mm_add_ps(sum, diag);\n    }\n\n    // Horizontal sum of the SIMD register\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    float total_sum = result[0] + result[1] + result[2] + result[3];\n\n    // Process remaining diagonal elements (if dim is not a multiple of 4)\n    for (; i < dim; i++) {\n        size_t idx = i * dim * dim + i * dim + i;\n        total_sum += array[idx];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps(); // Initialize SIMD register to hold the sum\n    size_t i = 0;\n\n    // Process in chunks of 4 using SIMD\n    for (; i + 3 < dim; i += 4) {\n        // Calculate the indices of the diagonal elements\n        size_t idx0 = i * dim * dim + i * dim + i;\n        size_t idx1 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx2 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx3 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        // Load the diagonal elements into a SIMD register\n        __m128 diag = _mm_set_ps(array[idx3], array[idx2], array[idx1], array[idx0]);\n\n        // Accumulate the sum\n        sum = _mm_add_ps(sum, diag);\n    }\n\n    // Horizontal sum of the SIMD register\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    float total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle the remaining elements\n    for (; i < dim; i++) {\n        size_t idx = i * dim * dim + i * dim + i;\n        total_sum += array[idx];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps(); // Initialize SIMD register to hold the sum\n    size_t i = 0;\n\n    // Process 4 diagonal elements at a time\n    for (; i + 3 < dim; i += 4) {\n        // Calculate the indices of the diagonal elements\n        size_t idx0 = i * dim * dim + i * dim + i;\n        size_t idx1 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx2 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx3 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        // Load the diagonal elements into a SIMD register\n        __m128 diag_elements = _mm_set_ps(array[idx3], array[idx2], array[idx1], array[idx0]);\n\n        // Accumulate the sum\n        sum = _mm_add_ps(sum, diag_elements);\n    }\n\n    // Horizontal add to sum up the values in the SIMD register\n    sum = _mm_hadd_ps(sum, sum);\n    sum = _mm_hadd_ps(sum, sum);\n\n    // Extract the final sum from the SIMD register\n    float diagonal_sum = _mm_cvtss_f32(sum);\n\n    // Handle remaining elements if dim is not a multiple of 4\n    for (; i < dim; ++i) {\n        size_t idx = i * dim * dim + i * dim + i;\n        diagonal_sum += array[idx];\n    }\n\n    return diagonal_sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    size_t diagonal_count = dim; // Number of diagonal elements\n    __m128 sum_vec = _mm_setzero_ps(); // Initialize sum vector to zero\n\n    size_t i = 0;\n    for (; i + 4 <= diagonal_count; i += 4) {\n        // Calculate indices for diagonal elements\n        size_t idx0 = i * dim * dim + i * dim + i;\n        size_t idx1 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx2 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx3 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        // Load diagonal elements into a vector\n        __m128 diag_vec = _mm_set_ps(array[idx3], array[idx2], array[idx1], array[idx0]);\n\n        // Accumulate the sum\n        sum_vec = _mm_add_ps(sum_vec, diag_vec);\n    }\n\n    // Horizontal sum of the vector\n    float sum_array[4];\n    _mm_storeu_ps(sum_array, sum_vec);\n    float total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Handle remaining diagonal elements (if dim is not a multiple of 4)\n    for (; i < diagonal_count; i++) {\n        size_t idx = i * dim * dim + i * dim + i;\n        total_sum += array[idx];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps(); // Initialize the SIMD register to zero\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= dim; i += 4) {\n        // Calculate the indices of the diagonal elements\n        size_t idx0 = i * dim * dim + i * dim + i;\n        size_t idx1 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx2 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx3 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        // Load the diagonal elements into an SSE register\n        __m128 diag = _mm_set_ps(array[idx3], array[idx2], array[idx1], array[idx0]);\n\n        // Accumulate the sum\n        sum = _mm_add_ps(sum, diag);\n    }\n\n    // Horizontal sum of the SIMD register\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    float result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle the remaining elements (if dim is not a multiple of 4)\n    for (; i < dim; i++) {\n        size_t idx = i * dim * dim + i * dim + i;\n        result += array[idx];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    // Process elements in chunks of 2 doubles using SSE2\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]); // Load 2 doubles from src\n        __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec); // Compare src > threshold\n        __m128d scaled_vec = _mm_mul_pd(src_vec, scale_vec); // Scale src by scale factor\n        __m128d result_vec = _mm_or_pd(_mm_and_pd(mask, scaled_vec), _mm_andnot_pd(mask, src_vec)); // Conditional scaling\n        _mm_storeu_pd(&dst[i], result_vec); // Store result in dst\n    }\n\n    // Handle remaining elements (if total_elements is not divisible by 2)\n    for (; i < total_elements; i++) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    // Process elements in chunks of 2 doubles using SSE2\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]); // Load 2 doubles from src\n        __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec); // Compare src > threshold\n        __m128d scaled_vec = _mm_mul_pd(src_vec, scale_vec); // Scale src by scale factor\n        __m128d result_vec = _mm_or_pd(_mm_and_pd(mask, scaled_vec), _mm_andnot_pd(mask, src_vec)); // Conditional scaling\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result in dst\n    }\n\n    // Process remaining elements (if any)\n    for (; i < total_elements; i++) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n        __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n        __m128d scaled_vec = _mm_mul_pd(src_vec, scale_vec);\n        __m128d result_vec = _mm_or_pd(_mm_and_pd(mask, scaled_vec), _mm_andnot_pd(mask, src_vec));\n        _mm_storeu_pd(&dst[i], result_vec);\n    }\n\n    for (; i < total_elements; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    // Load threshold and scale into SSE registers\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    // Process elements in chunks of 2 (SSE2 operates on 128-bit registers, which can hold two doubles)\n    for (; i + 2 <= total_elements; i += 2) {\n        // Load 2 elements from the source array\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n\n        // Compare each element with the threshold\n        __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n\n        // Scale elements that are greater than the threshold\n        __m128d scaled_vec = _mm_mul_pd(src_vec, scale_vec);\n\n        // Blend the scaled and original values based on the mask\n        __m128d result_vec = _mm_or_pd(_mm_and_pd(mask, scaled_vec), _mm_andnot_pd(mask, src_vec));\n\n        // Store the result in the destination array\n        _mm_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is not divisible by 2)\n    for (; i < total_elements; i++) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    // Load threshold and scale into SIMD registers\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n\n    // Process elements in chunks of 2 (since __m128d handles 2 doubles)\n    for (; i + 2 <= total_elements; i += 2) {\n        // Load 2 elements from the source array\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n\n        // Compare src_vec with threshold_vec\n        __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n\n        // Scale the elements that are greater than the threshold\n        __m128d scaled_vec = _mm_mul_pd(src_vec, scale_vec);\n\n        // Blend the scaled and original values based on the mask\n        __m128d result_vec = _mm_or_pd(_mm_and_pd(mask, scaled_vec), _mm_andnot_pd(mask, src_vec));\n\n        // Store the result in the destination array\n        _mm_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle the remaining elements (if total_elements is not a multiple of 2)\n    for (; i < total_elements; i++) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t vectorized_cols = cols & ~1; // Ensure columns are a multiple of 2 for SIMD processing\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * cols;\n        double *dst_row = dst + i * cols;\n\n        // Process columns in chunks of 2 using SIMD\n        for (size_t j = 0; j < vectorized_cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, src_vec);\n        }\n\n        // Handle remaining columns (if cols is not a multiple of 2)\n        for (size_t j = vectorized_cols; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t row_size = cols * sizeof(double); // Size of a row in bytes\n    size_t simd_width = 2; // SSE2 processes 2 doubles (128 bits) at a time\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        // Process the row in chunks of 2 doubles using SSE2\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m128d data = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, data);\n        }\n\n        // Handle remaining elements (if cols is not a multiple of 2)\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t row_size = cols * sizeof(double); // Size of a row in bytes\n    size_t simd_width = 2; // SSE2 processes 2 doubles at a time\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i]; // Get the source row index\n        const double *src_row = src + src_row_index * cols; // Pointer to the source row\n        double *dst_row = dst + i * cols; // Pointer to the destination row\n\n        size_t j = 0;\n        // Process the row in chunks of 2 doubles using SSE2\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m128d src_data = _mm_loadu_pd(src_row + j); // Load 2 doubles from the source row\n            _mm_storeu_pd(dst_row + j, src_data); // Store 2 doubles to the destination row\n        }\n\n        // Handle the remaining elements (if cols is not a multiple of 2)\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_blocks = cols / 2; // Each __m128d processes 2 doubles\n    size_t remaining_cols = cols % 2;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i];\n        const double *src_row = src + src_row_index * cols;\n        double *dst_row = dst + i * cols;\n\n        // Process 2 doubles at a time using SIMD\n        for (size_t j = 0; j < col_blocks; ++j) {\n            __m128d data = _mm_loadu_pd(src_row + j * 2);\n            _mm_storeu_pd(dst_row + j * 2, data);\n        }\n\n        // Handle remaining columns (if cols is not a multiple of 2)\n        for (size_t j = col_blocks * 2; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t col_step = 2; // Process 2 doubles at a time using SSE2\n    size_t col_limit = (cols / col_step) * col_step;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i];\n        const double *src_row = src + src_row_idx * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t j = 0;\n        for (; j < col_limit; j += col_step) {\n            __m128d src_vec = _mm_loadu_pd(&src_row[j]);\n            _mm_storeu_pd(&dst_row[j], src_vec);\n        }\n\n        // Handle remaining elements (if cols is not a multiple of 2)\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stddef.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar); // Load scalar into a SIMD register\n    __m128i divisor_vec = _mm_set1_epi64x(2);    // Load divisor (2) into a SIMD register\n\n    size_t i;\n    for (i = 0; i + 2 <= length; i += 2) { // Process two 64-bit integers at a time\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)&src1[i]); // Load 2 elements from src1\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)&src2[i]); // Load 2 elements from src2\n\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);    // Element-wise addition of src1 and src2\n        __m128i scalar_added_vec = _mm_add_epi64(sum_vec, scalar_vec); // Add scalar to each element\n        __m128i result_vec = _mm_srai_epi64(scalar_added_vec, 1); // Divide by 2 (round-toward-zero)\n\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec); // Store the result in dst\n    }\n\n    // Handle remaining elements (if length is not a multiple of 2)\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i] + scalar) / 2);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m128i divisor = _mm_set1_epi64x(2);        // Broadcast divisor (2) to all elements\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) { // Process two 64-bit integers at a time\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]); // Load 2 elements from src1\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]); // Load 2 elements from src2\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);            // Element-wise addition of src1 and src2\n        __m128i sum_with_scalar = _mm_add_epi64(sum, scalar_vec); // Add scalar to each element\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum_with_scalar, _mm_and_si128(sum_with_scalar, _mm_set1_epi64x(1))), 1); // Divide by 2 (round-toward-zero)\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m128i divisor = _mm_set1_epi64x(2);        // Broadcast divisor (2) to all elements\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]); // Load 2 elements from src1\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]); // Load 2 elements from src2\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);            // Element-wise addition of src1 and src2\n        __m128i sum_with_scalar = _mm_add_epi64(sum, scalar_vec); // Add scalar to the sum\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum_with_scalar, _mm_and_si128(sum_with_scalar, _mm_set1_epi64x(1))), 1); // Divide by 2 (round-toward-zero)\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar); // Broadcast scalar to all elements\n    __m128i divisor = _mm_set1_epi64x(2);        // Set divisor (2) for division\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i *)&src1[i]); // Load 2 elements from src1\n        __m128i vec2 = _mm_loadu_si128((__m128i *)&src2[i]); // Load 2 elements from src2\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);             // Add src1 and src2\n        __m128i sum_with_scalar = _mm_add_epi64(sum, scalar_vec); // Add scalar to the sum\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum_with_scalar, divisor), 1); // Divide by 2 (round-toward-zero)\n\n        _mm_storeu_si128((__m128i *)&dst[i], result);        // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar); // Broadcast scalar to all elements of a 128-bit SIMD register\n    __m128i divisor = _mm_set1_epi64x(2);        // Set divisor (2) for division\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]); // Load 2 elements from src1\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]); // Load 2 elements from src2\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);            // Element-wise addition of src1 and src2\n        __m128i sum_with_scalar = _mm_add_epi64(sum, scalar_vec); // Add scalar to the result\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum_with_scalar, divisor), 1); // Divide by 2 (round-toward-zero)\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);        // Store the result into dst\n    }\n\n    // Handle the remaining elements (if length is not a multiple of 2)\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX); // Initialize with maximum integer value\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i *)(src + i)); // Load 4 integers\n        __m128i mask = _mm_set_epi32((i + 3) % 2 == 0, (i + 2) % 2 == 0, (i + 1) % 2 == 0, i % 2 == 0);\n        __m128i even_vec = _mm_and_si128(vec, mask); // Mask out odd-indexed elements\n        min_vec = _mm_min_epi32(min_vec, even_vec);  // Compare and store minimum values\n    }\n\n    // Extract minimum value from min_vec\n    int min_values[4];\n    _mm_storeu_si128((__m128i *)min_values, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (min_values[j] < min_value) {\n            min_value = min_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value == INT_MAX ? -1 : min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX); // Initialize with max int value\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 integers\n        __m128i mask = _mm_set_epi32(i + 3, i + 2, i + 1, i); // Create index mask\n        __m128i even_mask = _mm_and_si128(mask, _mm_set1_epi32(1)); // Mask for even indices\n        __m128i filtered = _mm_and_si128(data, even_mask); // Filter even indices\n        min_vec = _mm_min_epi32(min_vec,filtered); // Update min\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX); // Initialize with maximum possible value\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i *)(src + i)); // Load 4 integers\n        __m128i mask = _mm_set_epi32((i + 3) % 2 == 0, (i + 2) % 2 == 0, (i + 1) % 2 == 0, i % 2 == 0);\n        __m128i masked_vec = _mm_and_si128(vec, mask); // Mask odd indices\n        min_vec = _mm_min_epi32(min_vec, masked_vec); // Compute minimum\n    }\n\n    // Reduce the vector to find the minimum value\n    int min_values[4];\n    _mm_storeu_si128((__m128i *)min_values, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (min_values[j] < min_value) {\n            min_value = min_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX); // Initialize with maximum integer value\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i *)(src + i)); // Load 4 integers\n        __m128i mask = _mm_set_epi32((i + 3) % 2 == 0, (i + 2) % 2 == 0, (i + 1) % 2 == 0, i % 2 == 0); // Create mask for even indices\n        __m128i filtered = _mm_and_si128(vec, mask); // Apply mask to keep even-indexed elements\n        min_vec = _mm_min_epi32(min_vec, filtered); // Update minimum\n    }\n\n    // Extract minimum value from SIMD register\n    int min_values[4];\n    _mm_storeu_si128((__m128i *)min_values, min_vec);\n    int min_value = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (min_values[j] < min_value) {\n            min_value = min_values[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; i += 2) {\n        if (src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX); // Initialize min_val to the maximum possible integer value\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i *)(src + i)); // Load 4 integers from the array\n        __m128i mask = _mm_set_epi32((i + 3) % 2 == 0 ? -1 : 0, \n                                     (i + 2) % 2 == 0 ? -1 : 0, \n                                     (i + 1) % 2 == 0 ? -1 : 0, \n                                     i % 2 == 0 ? -1 : 0); // Create a mask for even indices\n        __m128i masked_vec = _mm_and_si128(vec, mask); // Apply the mask\n        min_val = _mm_min_epi32(min_val, masked_vec); // Update min_val with the minimum values\n    }\n\n    // Handle remaining elements\n    int min_scalar = INT_MAX;\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_scalar) {\n            min_scalar = src[i];\n        }\n    }\n\n    // Extract the minimum value from the SIMD register\n    int temp[4];\n    _mm_storeu_si128((__m128i *)temp, min_val);\n    for (int j = 0; j < 4; j++) {\n        if (temp[j] < min_scalar) {\n            min_scalar = temp[j];\n        }\n    }\n\n    return min_scalar;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m128i v_shift = _mm_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from the source array\n        __m128i v_src = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m128i v_arithmetic = _mm_srai_epi32(v_src, shift);\n\n        // Perform logical right shift\n        __m128i v_logical = _mm_srli_epi32(v_src, shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m128i v_result = _mm_add_epi32(v_arithmetic, v_logical);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        int arithmetic = src[i] >> shift; // Arithmetic right shift\n        unsigned int logical = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arithmetic + (int)logical; // Add results and store\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m128i shift_vec = _mm_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from the source array\n        __m128i src_vec = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m128i arith_shifted = _mm_srai_epi32(src_vec, shift);\n\n        // Perform logical right shift\n        __m128i logical_shifted = _mm_srl_epi32(_mm_castsi128_si128(src_vec), shift_vec);\n\n        // Add the results of arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shifted, logical_shifted);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shifted = src[i] >> shift; // Arithmetic right shift\n        int logical_shifted = (int)((unsigned int)src[i] >> shift); // Logical right shift\n        dst[i] = arith_shifted + logical_shifted; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m128i shift_vec = _mm_set1_epi32(shift); // Broadcast shift value to all lanes\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from the source array\n        __m128i src_vec = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m128i arith_shifted = _mm_srai_epi32(src_vec, shift);\n\n        // Perform logical right shift\n        __m128i logical_shifted = _mm_srl_epi32(_mm_castsi128_si128(src_vec), shift_vec);\n\n        // Add the results of arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shifted, logical_shifted);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shifted = src[i] >> shift; // Arithmetic right shift\n        unsigned int logical_shifted = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arith_shifted + (int)logical_shifted; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m128i shift_vec = _mm_set1_epi32(shift); // Broadcast shift value to all elements\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from the source array\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Perform arithmetic right shift\n        __m128i arith_shift = _mm_srai_epi32(src_vec, shift);\n\n        // Perform logical right shift\n        __m128i logical_shift = _mm_srl_epi32(_mm_castsi128_si128(src_vec), shift_vec);\n\n        // Add the results of arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shift, logical_shift);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        int arith_shift = src[i] >> shift; // Arithmetic right shift\n        int logical_shift = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arith_shift + logical_shift; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m128i shift_vec = _mm_set1_epi32(shift); // Set the shift value for all lanes\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 integers from the source array\n        __m128i src_vec = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Perform arithmetic right shift\n        __m128i arith_shifted = _mm_srai_epi32(src_vec, shift);\n\n        // Perform logical right shift\n        __m128i logical_shifted = _mm_srl_epi32(_mm_castsi128_si128(src_vec), shift_vec);\n\n        // Add the results of arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shifted, logical_shifted);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int arith_shifted = src[i] >> shift; // Arithmetic right shift\n        int logical_shifted = (unsigned int)src[i] >> shift; // Logical right shift\n        dst[i] = arith_shifted + logical_shifted; // Add results\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize vector to zero\n    size_t i = 0;\n\n    // Process 4 integers at a time using SIMD\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]); // Load 4 integers\n        sum_vec = _mm_add_epi32(sum_vec, vec); // Add to the sum vector\n    }\n\n    // Horizontal sum of the vector\n    __m128i temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n    temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n\n    // Extract the final sum from the vector\n    int partial_sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize a vector to hold the sum\n\n    size_t i = 0;\n    // Process 4 integers at a time\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&matrix[i]); // Load 4 integers\n        sum_vec = _mm_add_epi32(sum_vec, data); // Add to the sum vector\n    }\n\n    // Horizontal sum of the 4 integers in the sum vector\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    int partial_sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Handle any remaining elements\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize vector to hold partial sums\n    size_t i = 0;\n\n    // Process elements in chunks of 4 (128 bits / 32 bits per int)\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i *)(matrix + i)); // Load 4 integers\n        sum_vec = _mm_add_epi32(sum_vec, data); // Add to the sum vector\n    }\n\n    // Horizontal sum of the vector\n    __m128i temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n    temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n\n    // Extract the final sum from the vector\n    int partial_sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize vector sum to zero\n    size_t i = 0;\n\n    // Process 4 integers at a time\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&matrix[i]); // Load 4 integers\n        sum_vec = _mm_add_epi32(sum_vec, data); // Add to the sum vector\n    }\n\n    // Horizontal sum of the 4 integers in sum_vec\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    int partial_sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize vector sum to zero\n    size_t i = 0;\n\n    // Process 4 integers at a time using SSE2\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i *)(matrix + i)); // Load 4 integers\n        sum_vec = _mm_add_epi32(sum_vec, vec); // Add to sum vector\n    }\n\n    // Horizontal sum of the vector\n    __m128i temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n    temp = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, temp);\n\n    // Extract the final sum from the vector\n    int32_t partial_sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Sum remaining elements (if total_elements is not a multiple of 4)\n    for (; i < total_elements; i++) {\n        partial_sum += matrix[i];\n    }\n\n    return (int64_t)partial_sum;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_values = _mm_set1_epi8(INT8_MIN); // Initialize max values to minimum 8-bit integer\n    __m128i max_indices = _mm_setzero_si128();   // Initialize max indices to zero\n    __m128i current_indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); // Indices for the first 16 elements\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i values = _mm_loadu_si128((__m128i *)(src + i)); // Load 16 elements\n        __m128i cmp_mask = _mm_cmpgt_epi8(values, max_values);  // Compare values with current max\n\n        max_values = _mm_max_epi8(values, max_values);          // Update max values\n        max_indices = _mm_blendv_epi8(max_indices, current_indices, cmp_mask); // Update indices where new max is found\n\n        current_indices = _mm_add_epi8(current_indices, _mm_set1_epi8(16)); // Increment indices by 16\n    }\n\n    // Extract max values and indices from SIMD registers\n    int8_t temp_values[16];\n    int8_t temp_indices[16];\n    _mm_storeu_si128((__m128i *)temp_values, max_values);\n    _mm_storeu_si128((__m128i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 16; ++j) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_values = _mm_set1_epi8(INT8_MIN); // Initialize max values to minimum 8-bit integer\n    __m128i max_indices = _mm_setzero_si128();   // Initialize max indices to zero\n    __m128i current_indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); // Indices for current chunk\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i values = _mm_loadu_si128((__m128i *)(src + i)); // Load 16 bytes from src\n        __m128i cmp_mask = _mm_cmpgt_epi8(values, max_values);  // Compare values with current max_values\n\n        max_values = _mm_max_epi8(values, max_values);          // Update max_values\n        max_indices = _mm_blendv_epi8(max_indices, current_indices, cmp_mask); // Update max_indices where cmp_mask is true\n\n        current_indices = _mm_add_epi8(current_indices, _mm_set1_epi8(16)); // Increment indices for next chunk\n    }\n\n    // Extract the max value and index from the SIMD registers\n    int8_t temp_values[16];\n    int8_t temp_indices[16];\n    _mm_storeu_si128((__m128i *)temp_values, max_values);\n    _mm_storeu_si128((__m128i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 16; ++j) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_values = _mm_set1_epi8(INT8_MIN); // Initialize max values to minimum 8-bit integer\n    __m128i max_indices = _mm_setzero_si128();   // Initialize indices to zero\n    __m128i current_indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); // Indices for the current chunk\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i values = _mm_loadu_si128((__m128i *)(src + i)); // Load 16 bytes from the source array\n\n        // Compare values with current max values\n        __m128i mask = _mm_cmpgt_epi8(values, max_values);\n        max_values = _mm_max_epi8(max_values, values);\n\n        // Update indices where mask is true\n        max_indices = _mm_blendv_epi8(max_indices, current_indices, mask);\n\n        // Increment current indices for the next chunk\n        current_indices = _mm_add_epi8(current_indices, _mm_set1_epi8(16));\n    }\n\n    // Find the maximum value and its index in the remaining elements\n    for (; i < length; i++) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    // Extract the maximum value and its index from the SIMD registers\n    int8_t temp_values[16];\n    int8_t temp_indices[16];\n    _mm_storeu_si128((__m128i *)temp_values, max_values);\n    _mm_storeu_si128((__m128i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 16; j++) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j] + i - 16;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_values = _mm_set1_epi8(INT8_MIN); // Initialize with the smallest possible value\n    __m128i max_indices = _mm_setzero_si128();   // Initialize indices to zero\n    __m128i current_indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i values = _mm_loadu_si128((__m128i *)(src + i));\n\n        // Compare and update max values\n        __m128i mask = _mm_cmpgt_epi8(values, max_values);\n        max_values = _mm_blendv_epi8(max_values, values, mask);\n        max_indices = _mm_blendv_epi8(max_indices, current_indices, mask);\n\n        // Increment current indices by 16\n        current_indices = _mm_add_epi8(current_indices, _mm_set1_epi8(16));\n    }\n\n    // Find max value and index in the remaining elements\n    for (; i < length; i++) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    // Extract max value and index from SIMD registers\n    int8_t temp_values[16];\n    int8_t temp_indices[16];\n    _mm_storeu_si128((__m128i *)temp_values, max_values);\n    _mm_storeu_si128((__m128i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 16; j++) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j] + i - 16;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_values = _mm_set1_epi8(INT8_MIN); // Initialize max values to minimum 8-bit integer\n    __m128i max_indices = _mm_set1_epi8(0);      // Initialize indices to 0\n    __m128i current_indices = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); // Indices for current vector\n    size_t max_index = 0;\n    int8_t max_value = INT8_MIN;\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i values = _mm_loadu_si128((__m128i *)(src + i)); // Load 16 values\n        __m128i cmp_mask = _mm_cmpgt_epi8(values, max_values);  // Compare values with current max\n        max_values = _mm_max_epi8(values, max_values);          // Update max values\n        max_indices = _mm_blendv_epi8(max_indices, current_indices, cmp_mask); // Update indices where new max found\n        current_indices = _mm_add_epi8(current_indices, _mm_set1_epi8(16));    // Increment indices by 16\n    }\n\n    // Find the maximum value and index in the remaining elements\n    for (; i < length; i++) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    // Extract the maximum value and index from the SIMD registers\n    int8_t temp_values[16];\n    int8_t temp_indices[16];\n    _mm_storeu_si128((__m128i *)temp_values, max_values);\n    _mm_storeu_si128((__m128i *)temp_indices, max_indices);\n\n    for (int j = 0; j < 16; j++) {\n        if (temp_values[j] > max_value) {\n            max_value = temp_values[j];\n            max_index = temp_indices[j] + i - 16;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vector = _mm_set1_epi16((uint16_t)mask);\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m128i indices = _mm_set_epi16(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m128i src1_values = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_values = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i index_and_mask = _mm_and_si128(indices, mask_vector);\n        __m128i blend_mask = _mm_cmpeq_epi16(index_and_mask, _mm_setzero_si128());\n\n        __m128i result = _mm_or_si128(_mm_andnot_si128(blend_mask, src1_values),\n                                      _mm_and_si128(blend_mask, src2_values));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time (128 bits / 16 bits per element)\n    __m128i mask_vector = _mm_set1_epi16((int16_t)mask); // Broadcast mask to all elements\n    for (; i + 8 <= length; i += 8) {\n        __m128i indices = _mm_set_epi16(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i and_result = _mm_and_si128(indices, mask_vector);\n        __m128i blend_mask = _mm_cmpeq_epi16(and_result, _mm_setzero_si128());\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(blend_mask, src2_vector),\n            _mm_andnot_si128(blend_mask, src1_vector)\n        );\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vector = _mm_set1_epi16((uint16_t)mask);\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i indices = _mm_set_epi16(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)(src2 + i));\n\n        __m128i index_mask = _mm_and_si128(indices, mask_vector);\n        __m128i blend_mask = _mm_cmpeq_epi16(index_mask, _mm_setzero_si128());\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(blend_mask, src2_vector),\n            _mm_andnot_si128(blend_mask, src1_vector)\n        );\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    __m128i mask_vector = _mm_set1_epi16((uint16_t)mask); // Broadcast mask to all lanes\n    for (; i + 8 <= length; i += 8) {\n        __m128i indices = _mm_set_epi16(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i); // Load indices\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)(src1 + i)); // Load 8 elements from src1\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)(src2 + i)); // Load 8 elements from src2\n\n        // Compute the mask condition: (index & mask) != 0\n        __m128i and_result = _mm_and_si128(indices, mask_vector);\n        __m128i blend_mask = _mm_cmpeq_epi16(and_result, _mm_setzero_si128()); // Compare with zero\n\n        // Blend the values based on the mask\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(blend_mask, src2_vector), // Take src2 where mask is zero\n            _mm_andnot_si128(blend_mask, src1_vector) // Take src1 where mask is non-zero\n        );\n\n        _mm_storeu_si128((__m128i*)(dst + i), result); // Store the result\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set1_epi16((uint16_t)mask);\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i indices = _mm_set_epi16(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n\n        __m128i index_mask = _mm_and_si128(indices, mask_vec);\n        __m128i blend_mask = _mm_cmpeq_epi16(index_mask, _mm_setzero_si128());\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(blend_mask, src2_vec),\n            _mm_andnot_si128(blend_mask, src1_vec)\n        );\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask1 = _mm_set1_epi8(0x55); // 01010101\n    __m128i mask2 = _mm_set1_epi8(0x33); // 00110011\n    __m128i mask3 = _mm_set1_epi8(0x0F); // 00001111\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)(src + i));\n\n        // Step 1: Count bits in pairs\n        __m128i t1 = _mm_sub_epi8(v, _mm_and_si128(_mm_srli_epi16(v, 1), mask1));\n\n        // Step 2: Count bits in nibbles\n        __m128i t2 = _mm_add_epi8(_mm_and_si128(t1, mask2), _mm_and_si128(_mm_srli_epi16(t1, 2), mask2));\n\n        // Step 3: Count bits in bytes\n        __m128i t3 = _mm_add_epi8(_mm_and_si128(t2, mask3), _mm_srli_epi16(t2, 4));\n\n        // Horizontal sum of bytes\n        __m128i result = _mm_and_si128(t3, _mm_set1_epi8(0xFF));\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x + (x >> 8);\n        x = x + (x >> 16);\n        dst[i] = x & 0xFF;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE2\n    for (; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Calculate population count for each 32-bit integer\n        __m128i count1 = _mm_sub_epi32(data, _mm_and_si128(_mm_srli_epi32(data, 1), _mm_set1_epi32(0x55555555)));\n        __m128i count2 = _mm_add_epi32(_mm_and_si128(count1, _mm_set1_epi32(0x33333333)), _mm_and_si128(_mm_srli_epi32(count1, 2), _mm_set1_epi32(0x33333333)));\n        __m128i count3 = _mm_and_si128(_mm_add_epi32(count2, _mm_srli_epi32(count2, 4)), _mm_set1_epi32(0x0F0F0F0F));\n        __m128i count4 = _mm_mullo_epi32(count3, _mm_set1_epi32(0x01010101));\n        __m128i popcount = _mm_srli_epi32(count4, 24);\n\n        _mm_storeu_si128((__m128i*)&dst[i], popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        uint32_t x = src[i];\n        x = x - ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x + (x >> 4)) & 0x0F0F0F0F;\n        x = x * 0x01010101;\n        dst[i] = x >> 24;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE2\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)(src + i)); // Load 4 uint32_t values\n        __m128i count = _mm_setzero_si128();             // Initialize count to zero\n\n        // Perform population count using bitwise operations\n        for (int j = 0; j < 32; j++) {\n            __m128i mask = _mm_set1_epi32(1 << j);       // Create mask for current bit\n            __m128i bit = _mm_and_si128(v, mask);        // Extract the bit\n            __m128i bit_count = _mm_srli_epi32(bit, j);  // Shift the bit to the least significant position\n            count = _mm_add_epi32(count, bit_count);     // Accumulate the count\n        }\n\n        _mm_storeu_si128((__m128i*)(dst + i), count);    // Store the result\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        uint32_t value = src[i];\n        uint32_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 integers at a time using SSE2\n    for (; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Calculate population count for each 32-bit integer\n        __m128i count = _mm_setzero_si128();\n        for (int j = 0; j < 32; ++j) {\n            __m128i bit = _mm_and_si128(data, _mm_set1_epi32(1));\n            count = _mm_add_epi32(count, bit);\n            data = _mm_srli_epi32(data, 1);\n        }\n\n        _mm_storeu_si128((__m128i*)&dst[i], count);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; ++i) {\n        uint32_t value = src[i];\n        uint32_t count = 0;\n        while (value) {\n            count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE2\n    for (; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 uint32_t values\n        __m128i count = _mm_setzero_si128();              // Initialize count to zero\n\n        // Perform population count using bitwise operations\n        for (int j = 0; j < 32; ++j) {\n            __m128i mask = _mm_set1_epi32(1 << j);        // Create mask for the j-th bit\n            __m128i bit = _mm_and_si128(data, mask);      // Extract the j-th bit\n            __m128i bit_shifted = _mm_srli_epi32(bit, j); // Shift the bit to the least significant position\n            count = _mm_add_epi32(count, bit_shifted);    // Accumulate the population count\n        }\n\n        _mm_storeu_si128((__m128i*)&dst[i], count);       // Store the result in the destination array\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; ++i) {\n        uint32_t value = src[i];\n        uint32_t pop_count = 0;\n        while (value) {\n            pop_count += value & 1;\n            value >>= 1;\n        }\n        dst[i] = pop_count;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 elements from src1 and src2\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)(src2 + i));\n\n        // Perform saturated addition\n        __m128i v_result = _mm_adds_epu16(v_src1, v_src2);\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)(dst + i), v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= length; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]); // Load 8 uint16_t values from src1\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]); // Load 8 uint16_t values from src2\n        __m128i result = _mm_adds_epu16(v1, v2);          // Perform saturating addition\n        _mm_storeu_si128((__m128i*)&dst[i], result);      // Store the result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SIMD\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 16-bit integers from each source array\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        // Perform saturating addition\n        __m128i result = _mm_adds_epu16(vec1, vec2);\n\n        // Store the result back to the destination array\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Process remaining elements that don't fit into SIMD registers\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= length; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i)); // Load 8 uint16_t elements from src1\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i)); // Load 8 uint16_t elements from src2\n\n        __m128i result = _mm_adds_epu16(v1, v2); // Perform saturating addition\n\n        _mm_storeu_si128((__m128i*)(dst + i), result); // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = src1[i] + src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 elements from each source array\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        // Perform saturated addition\n        __m128i result = _mm_adds_epu16(vec1, vec2);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        uint32_t sum = (uint32_t)src1[i] + (uint32_t)src2[i];\n        dst[i] = (sum > UINT16_MAX) ? UINT16_MAX : (uint16_t)sum;\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from A and B\n                __m128d a_vals = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vals = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if A values are within range [-100, 100]\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a_vals, lower_bound),\n                    _mm_cmple_pd(a_vals, upper_bound)\n                );\n\n                // Check if B values are within range [-100, 100]\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b_vals, lower_bound),\n                    _mm_cmple_pd(b_vals, upper_bound)\n                );\n\n                // Combine range checks for A and B\n                __m128d valid_mask = _mm_and_pd(a_in_range, b_in_range);\n\n                // Perform multiplication only for valid elements\n                __m128d product = _mm_mul_pd(a_vals, b_vals);\n                product = _mm_and_pd(product, valid_mask);\n\n                // Accumulate the sum\n                sum = _mm_add_pd(sum, product);\n            }\n\n            // Horizontal add to reduce sum to a single value\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vals = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vals = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d a_mask_lower = _mm_cmpge_pd(a_vals, lower_bound);\n                __m128d a_mask_upper = _mm_cmple_pd(a_vals, upper_bound);\n                __m128d a_mask = _mm_and_pd(a_mask_lower, a_mask_upper);\n\n                __m128d b_mask_lower = _mm_cmpge_pd(b_vals, lower_bound);\n                __m128d b_mask_upper = _mm_cmple_pd(b_vals, upper_bound);\n                __m128d b_mask = _mm_and_pd(b_mask_lower, b_mask_upper);\n\n                __m128d valid_mask = _mm_and_pd(a_mask, b_mask);\n\n                __m128d mul = _mm_mul_pd(a_vals, b_vals);\n                mul = _mm_and_pd(mul, valid_mask);\n\n                sum = _mm_add_pd(sum, mul);\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 2) {\n                // Load elements from A and B\n                __m128d a_vals = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vals = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if A values are in range [-100, 100]\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a_vals, lower_bound),\n                    _mm_cmple_pd(a_vals, upper_bound)\n                );\n\n                // Check if B values are in range [-100, 100]\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b_vals, lower_bound),\n                    _mm_cmple_pd(b_vals, upper_bound)\n                );\n\n                // Combine range checks for A and B\n                __m128d valid_mask = _mm_and_pd(a_in_range, b_in_range);\n\n                // Multiply A and B\n                __m128d product = _mm_mul_pd(a_vals, b_vals);\n\n                // Mask the product (set invalid elements to 0)\n                product = _mm_and_pd(product, valid_mask);\n\n                // Accumulate the valid products\n                sum = _mm_add_pd(sum, product);\n            }\n\n            // Horizontal add to sum up the two lanes in the SIMD register\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from A and B\n                __m128d a_vals = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vals = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if A values are in range [-100, 100]\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a_vals, lower_bound),\n                    _mm_cmple_pd(a_vals, upper_bound)\n                );\n\n                // Check if B values are in range [-100, 100]\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b_vals, lower_bound),\n                    _mm_cmple_pd(b_vals, upper_bound)\n                );\n\n                // Mask values outside the range\n                a_vals = _mm_and_pd(a_vals, a_in_range);\n                b_vals = _mm_and_pd(b_vals, b_in_range);\n\n                // Multiply and accumulate\n                __m128d product = _mm_mul_pd(a_vals, b_vals);\n                sum = _mm_add_pd(sum, product);\n            }\n\n            // Horizontal add to sum up the two lanes\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                // Load elements from A and B\n                __m128d a_vals = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vals = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check range for A\n                __m128d a_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(a_vals, lower_bound),\n                    _mm_cmple_pd(a_vals, upper_bound)\n                );\n\n                // Check range for B\n                __m128d b_in_range = _mm_and_pd(\n                    _mm_cmpge_pd(b_vals, lower_bound),\n                    _mm_cmple_pd(b_vals, upper_bound)\n                );\n\n                // Mask values outside the range\n                a_vals = _mm_and_pd(a_vals, a_in_range);\n                b_vals = _mm_and_pd(b_vals, b_in_range);\n\n                // Multiply and accumulate\n                __m128d product = _mm_mul_pd(a_vals, b_vals);\n                sum = _mm_add_pd(sum, product);\n            }\n\n            // Horizontal add to get the final sum\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 2; // SSE2 can process two 64-bit integers at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecC = _mm_add_epi64(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecC);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of simd_width)\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2 (since 64-bit integers fit 2 per __m128i register)\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 2 64-bit integers from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 2 64-bit integers from B\n        __m128i vecC = _mm_add_epi64(vecA, vecB);        // Perform element-wise addition\n        _mm_storeu_si128((__m128i*)&C[i], vecC);         // Store the result in C\n    }\n\n    // Handle remaining elements (if total_elements is not divisible by 2)\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]); // Load 2 int64_t elements from A\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]); // Load 2 int64_t elements from B\n        __m128i c = _mm_add_epi64(a, b);             // Perform element-wise addition\n        _mm_storeu_si128((__m128i*)&C[i], c);        // Store the result in C\n    }\n\n    // Handle the remaining elements (if total_elements is odd)\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vecC = _mm_add_epi64(vecA, vecB);\n        _mm_storeu_si128((__m128i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements (if total_elements is not a multiple of 2)\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]); // Load 2 int64_t elements from A\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]); // Load 2 int64_t elements from B\n        __m128i result = _mm_add_epi64(a, b);         // Perform element-wise addition\n        _mm_storeu_si128((__m128i*)&C[i], result);    // Store the result in C\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128 vec1 = _mm_loadu_ps(&src1[i]); // Load 4 floats from src1\n        __m128 vec2 = _mm_loadu_ps(&src2[i]); // Load 4 floats from src2\n        __m128 sum = _mm_add_ps(vec1, vec2);  // Element-wise addition\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm_storeu_ps(&dst[i], rounded);      // Store the result in dst\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 4; // Process 4 floats at a time\n    size_t remainder = length % 4;\n\n    for (i = 0; i < simd_length * 4; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&src1[i]); // Load 4 floats from src1\n        __m128 vec2 = _mm_loadu_ps(&src2[i]); // Load 4 floats from src2\n        __m128 sum = _mm_add_ps(vec1, vec2);  // Element-wise addition\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm_storeu_ps(&dst[i], rounded);     // Store the result in dst\n    }\n\n    // Handle the remainder elements\n    for (i = simd_length * 4; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i); // Load 4 floats from src1\n        __m128 vec2 = _mm_loadu_ps(src2 + i); // Load 4 floats from src2\n        __m128 sum = _mm_add_ps(vec1, vec2);  // Element-wise addition\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm_storeu_ps(dst + i, rounded);      // Store the result in dst\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 floats from src1 and src2\n        __m128 vec1 = _mm_loadu_ps(&src1[i]);\n        __m128 vec2 = _mm_loadu_ps(&src2[i]);\n\n        // Perform element-wise addition\n        __m128 result = _mm_add_ps(vec1, vec2);\n\n        // Round the result to the nearest integer\n        __m128 rounded = _mm_round_ps(result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Store the result in the destination array\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    // Handle the remaining elements (if length is not a multiple of 4)\n    for (; i < length; i++) {\n        float sum = src1[i] + src2[i];\n        dst[i] = roundf(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 4; // Process 4 elements at a time using SIMD\n\n    for (i = 0; i < simd_length * 4; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i); // Load 4 floats from src1\n        __m128 vec2 = _mm_loadu_ps(src2 + i); // Load 4 floats from src2\n        __m128 sum = _mm_add_ps(vec1, vec2);  // Element-wise addition\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        _mm_storeu_ps(dst + i, rounded);      // Store the result in dst\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_dst;\n\n        // Calculate absolute difference\n        __m128 diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute value\n\n        // Compare absolute difference with 50\n        __m128 mask_diff = _mm_cmple_ps(abs_diff, fifty);\n\n        // Check index multiples of 2 (mask for even indices)\n        __m128i index_mask = _mm_set_epi32((i + 3) % 2 == 0, (i + 2) % 2 == 0, (i + 1) % 2 == 0, i % 2 == 0);\n        __m128 mask_index = _mm_castsi128_ps(index_mask);\n\n        // Combine conditions\n        __m128 mask = _mm_and_ps(mask_diff, mask_index);\n\n        // Element-wise multiplication\n        __m128 mul_result = _mm_mul_ps(v_src1, v_src2);\n\n        // Round up toward positive infinity\n        __m128 rounded_result = _mm_ceil_ps(mul_result);\n\n        // Apply mask: if conditions are met, use rounded_result; otherwise, use -1\n        v_dst = _mm_or_ps(_mm_and_ps(mask, rounded_result), _mm_andnot_ps(mask, neg_one));\n\n        // Store the result\n        _mm_storeu_ps(&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 src1_vec = _mm_loadu_ps(&src1[i]);\n        __m128 src2_vec = _mm_loadu_ps(&src2[i]);\n\n        __m128 diff = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute value\n\n        __m128 mask_abs = _mm_cmple_ps(abs_diff, fifty); // abs_diff <= 50\n\n        // Create a mask for indices that are multiples of 2\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(1)), _mm_setzero_si128());\n\n        __m128 mask_even = _mm_castsi128_ps(even_mask);\n        __m128 mask = _mm_and_ps(mask_abs, mask_even); // Combine masks\n\n        __m128 mul_result = _mm_mul_ps(src1_vec, src2_vec);\n        __m128 rounded_result = _mm_ceil_ps(mul_result);\n\n        __m128 final_result = _mm_or_ps(_mm_and_ps(mask, rounded_result), _mm_andnot_ps(mask, neg_one));\n\n        _mm_storeu_ps(&dst[i], final_result);\n    }\n\n    for (; i < length; i++) {\n        if (fabs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= length; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), v_diff); // Absolute value of difference\n        __m128 v_threshold = _mm_set1_ps(50.0f);\n        __m128 v_condition1 = _mm_cmple_ps(v_abs_diff, v_threshold); // Check if abs(diff) <= 50\n\n        // Create mask for indices that are multiples of 2\n        __m128i v_indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i v_mod2_mask = _mm_and_si128(v_indices, _mm_set1_epi32(1));\n        __m128 v_condition2 = _mm_castsi128_ps(_mm_cmpeq_epi32(v_mod2_mask, _mm_setzero_si128())); // Check if index % 2 == 0\n\n        // Combine conditions\n        __m128 v_conditions = _mm_and_ps(v_condition1, v_condition2);\n\n        // Perform multiplication\n        __m128 v_mul = _mm_mul_ps(v_src1, v_src2);\n\n        // Round up toward positive infinity\n        __m128 v_rounded = _mm_ceil_ps(v_mul);\n\n        // Apply conditions: if conditions are false, set result to -1\n        __m128 v_result = _mm_or_ps(_mm_and_ps(v_conditions, v_rounded), _mm_andnot_ps(v_conditions, _mm_set1_ps(-1.0f)));\n\n        // Store result in destination array\n        _mm_storeu_ps(&dst[i], v_result);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50 && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute value\n\n        __m128 mask_within_50 = _mm_cmple_ps(abs_diff, fifty);\n\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i mask_even_indices = _mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(1)), _mm_setzero_si128());\n\n        __m128 mask_conditions = _mm_and_ps(mask_within_50, _mm_castsi128_ps(mask_even_indices));\n\n        __m128 result = _mm_mul_ps(v_src1, v_src2);\n        result = _mm_ceil_ps(result);\n\n        __m128 final_result = _mm_or_ps(_mm_and_ps(mask_conditions, result), _mm_andnot_ps(mask_conditions, neg_one));\n\n        _mm_storeu_ps(&dst[i], final_result);\n    }\n\n    for (; i < length; i++) {\n        if (fabs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(src1 + i);\n        __m128 v_src2 = _mm_loadu_ps(src2 + i);\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, fifty);\n\n        __m128 v_mul = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_ceil_mul = _mm_ceil_ps(v_mul);\n\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1));\n        __m128 v_even_condition = _mm_castsi128_ps(_mm_cmpeq_epi32(even_mask, _mm_setzero_si128()));\n\n        __m128 v_final_condition = _mm_and_ps(v_condition, v_even_condition);\n        __m128 v_result = _mm_or_ps(_mm_and_ps(v_final_condition, v_ceil_mul), _mm_andnot_ps(v_final_condition, neg_one));\n\n        _mm_storeu_ps(dst + i, v_result);\n    }\n\n    for (; i < length; i++) {\n        float diff = fabsf(src1[i] - src2[i]);\n        if (diff <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Load scalar into an SSE2 register\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i + 1 < total_elements; i += 2) {\n        // Load 2 double values from the source matrix\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]);\n\n        // Perform element-wise multiplication\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec);\n\n        // Store the result in the destination matrix\n        _mm_storeu_pd(&dst[i], result_vec);\n    }\n\n    // Handle remaining elements (if total_elements is odd)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Load scalar into an SSE register\n\n    size_t i = 0;\n    // Process 2 elements at a time using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply with scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result in dst\n    }\n\n    // Handle the remaining element if the total number of elements is odd\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Load scalar into a SIMD register\n\n    size_t i = 0;\n    // Process elements in chunks of 2 using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply with scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result into dst\n    }\n\n    // Handle remaining elements (if total_elements is odd)\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Load scalar into a SIMD register\n\n    size_t i = 0;\n    // Process elements in chunks of 2 using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply with scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result in dst\n    }\n\n    // Handle the remaining element (if any)\n    for (; i < total_elements; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Load scalar into a SIMD register\n\n    size_t i = 0;\n    // Process elements in chunks of 2 using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply with scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result in dst\n    }\n\n    // Handle remaining elements if total_elements is not divisible by 2\n    for (; i < total_elements; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute the L^2 norm of the row\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared = _mm_mul_ps(row_elements, row_elements);\n            sum = _mm_add_ps(sum, squared);\n        }\n\n        // Horizontal sum of the 4 values in the sum register\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n        float norm = _mm_cvtss_f32(sum);\n\n        // Handle remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            norm += mat[i * n + j] * mat[i * n + j];\n        }\n\n        norm = sqrtf(norm);\n\n        // Normalize the row\n        __m128 norm_vec = _mm_set1_ps(norm);\n        j = 0;\n\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_div_ps(row_elements, norm_vec);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        // Handle remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute the L^2 norm of the row\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_vec = _mm_mul_ps(row_vec, row_vec);\n            sum_vec = _mm_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of sum_vec\n        float sum_array[4];\n        _mm_storeu_ps(sum_array, sum_vec);\n        float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        __m128 norm_vec = _mm_set1_ps(norm);\n        j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized_vec = _mm_div_ps(row_vec, norm_vec);\n            _mm_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Calculate the L^2 norm of the row\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_vec = _mm_mul_ps(row_vec, row_vec);\n            sum_vec = _mm_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of sum_vec\n        float sum_array[4];\n        _mm_storeu_ps(sum_array, sum_vec);\n        float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        __m128 norm_vec = _mm_set1_ps(norm);\n        j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized_vec = _mm_div_ps(row_vec, norm_vec);\n            _mm_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        // Compute L^2 norm of the row\n        __m128 sum_vec = _mm_setzero_ps();\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_vec = _mm_mul_ps(row_vec, row_vec);\n            sum_vec = _mm_add_ps(sum_vec, squared_vec);\n        }\n\n        // Horizontal sum of sum_vec\n        float temp[4];\n        _mm_storeu_ps(temp, sum_vec);\n        float l2_norm_squared = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            l2_norm_squared += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float l2_norm = sqrtf(l2_norm_squared);\n\n        // Normalize the row\n        __m128 norm_vec = _mm_set1_ps(l2_norm);\n        j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 row_vec = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized_vec = _mm_div_ps(row_vec, norm_vec);\n            _mm_storeu_ps(&mat[i * n + j], normalized_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n\n        // Compute the L^2 norm of the row\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            sum = _mm_add_ps(sum, _mm_mul_ps(vec, vec));\n        }\n\n        // Horizontal sum of the SIMD register\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float l2_norm_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            l2_norm_sq += row[j] * row[j];\n        }\n\n        float l2_norm = sqrtf(l2_norm_sq);\n\n        // Normalize the row\n        __m128 norm_vec = _mm_set1_ps(l2_norm);\n        j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(row + j);\n            vec = _mm_div_ps(vec, norm_vec);\n            _mm_storeu_ps(row + j, vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum vector to zero\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from mat1 and mat2\n                __m128d mat1_vec = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d mat2_vec = _mm_loadu_pd(&mat2[k * p + j]);\n\n                // Perform element-wise multiplication\n                __m128d prod = _mm_mul_pd(mat1_vec, mat2_vec);\n\n                // Accumulate the product into sum\n                sum = _mm_add_pd(sum, prod);\n            }\n\n            // Horizontal add to sum the two elements in the vector\n            double partial_sum[2];\n            _mm_storeu_pd(partial_sum, sum);\n            double total_sum = partial_sum[0] + partial_sum[1];\n\n            // Round the result to the nearest integer and store in dst\n            dst[i * p + j] = round(total_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n            size_t k = 0;\n\n            // Perform dot product using SIMD for blocks of 2 doubles\n            for (; k + 1 < n; k += 2) {\n                __m128d mat1_vals = _mm_loadu_pd(&mat1[i * n + k]); // Load 2 elements from mat1\n                __m128d mat2_vals = _mm_set_pd(mat2[(k + 1) * p + j], mat2[k * p + j]); // Load 2 elements from mat2\n                __m128d prod = _mm_mul_pd(mat1_vals, mat2_vals); // Multiply\n                sum = _mm_add_pd(sum, prod); // Accumulate\n            }\n\n            // Horizontal add to sum up the two elements in the SIMD register\n            sum = _mm_hadd_pd(sum, sum);\n            double partial_sum = _mm_cvtsd_f64(sum);\n\n            // Handle the remaining element if n is odd\n            if (k < n) {\n                partial_sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to the nearest integer and store in dst\n            dst[i * p + j] = (double)((int)(partial_sum + 0.5));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from mat1 and mat2\n                __m128d mat1_vals = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d mat2_vals = _mm_loadu_pd(&mat2[k * p + j]);\n\n                // Multiply and accumulate\n                __m128d mul = _mm_mul_pd(mat1_vals, mat2_vals);\n                sum = _mm_add_pd(sum, mul);\n            }\n\n            // Horizontal add to sum up the two values in the SIMD register\n            double partial_sum[2];\n            _mm_storeu_pd(partial_sum, sum);\n            double total_sum = partial_sum[0] + partial_sum[1];\n\n            // Round the result to the nearest integer and store it in dst\n            dst[i * p + j] = round(total_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from mat1 and mat2\n                __m128d mat1_vals = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d mat2_vals = _mm_loadu_pd(&mat2[k * p + j]);\n\n                // Multiply and accumulate\n                __m128d product = _mm_mul_pd(mat1_vals, mat2_vals);\n                sum = _mm_add_pd(sum, product);\n            }\n\n            // Horizontal add to sum up the two elements in the vector\n            double partial_sum[2];\n            _mm_storeu_pd(partial_sum, sum);\n            double total_sum = partial_sum[0] + partial_sum[1];\n\n            // Round the result to the nearest integer\n            dst[i * p + j] = round(total_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum vector to zero\n            for (size_t k = 0; k < n; k += 2) {\n                // Load two elements from mat1 and mat2\n                __m128d mat1_vec = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d mat2_vec = _mm_loadu_pd(&mat2[k * p + j]);\n\n                // Multiply and accumulate\n                __m128d prod = _mm_mul_pd(mat1_vec, mat2_vec);\n                sum = _mm_add_pd(sum, prod);\n            }\n\n            // Horizontal add to sum up the two elements in the vector\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double total = temp[0] + temp[1];\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = round(total);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            __m128 t0 = _mm_unpacklo_ps(row0, row1);\n            __m128 t1 = _mm_unpackhi_ps(row0, row1);\n            __m128 t2 = _mm_unpacklo_ps(row2, row3);\n            __m128 t3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 col0 = _mm_movelh_ps(t0, t2);\n            __m128 col1 = _mm_movehl_ps(t2, t0);\n            __m128 col2 = _mm_movelh_ps(t1, t3);\n            __m128 col3 = _mm_movehl_ps(t3, t1);\n\n            col0 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(col0, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            col1 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(col1, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            col2 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(col2, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            col3 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(col3, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n\n            _mm_storeu_ps(&dst[j * rows + i], col0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], col1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], col2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], col3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            // Load 4x4 block from src\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            // Transpose the 4x4 block\n            __m128 t0 = _mm_unpacklo_ps(row0, row1);\n            __m128 t1 = _mm_unpackhi_ps(row0, row1);\n            __m128 t2 = _mm_unpacklo_ps(row2, row3);\n            __m128 t3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 col0 = _mm_movelh_ps(t0, t2);\n            __m128 col1 = _mm_movehl_ps(t2, t0);\n            __m128 col2 = _mm_movelh_ps(t1, t3);\n            __m128 col3 = _mm_movehl_ps(t3, t1);\n\n            // Round each element to the nearest quarter\n            col0 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(col0, quarter), _MM_FROUND_TO_NEAREST_INT), quarter), half);\n            col1 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(col1, quarter), _MM_FROUND_TO_NEAREST_INT), quarter), half);\n            col2 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(col2, quarter), _MM_FROUND_TO_NEAREST_INT), quarter), half);\n            col3 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(col3, quarter), _MM_FROUND_TO_NEAREST_INT), quarter), half);\n\n            // Store the transposed and rounded block to dst\n            _mm_storeu_ps(&dst[j * rows + i], col0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], col1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], col2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], col3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            _MM_TRANSPOSE4_PS(row0, row1, row2, row3);\n\n            row0 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row0, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            row1 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row1, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            row2 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row2, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n            row3 = _mm_mul_ps(_mm_round_ps(_mm_div_ps(row3, quarter), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), quarter);\n\n            _mm_storeu_ps(&dst[j * rows + i], row0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], row1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], row2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], row3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n            __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n            __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n            _MM_TRANSPOSE4_PS(row0, row1, row2, row3);\n\n            row0 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(row0, quarter), _MM_FROUND_NEAREST_INT), quarter), half);\n            row1 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(row1, quarter), _MM_FROUND_NEAREST_INT), quarter), half);\n            row2 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(row2, quarter), _MM_FROUND_NEAREST_INT), quarter), half);\n            row3 = _mm_add_ps(_mm_mul_ps(_mm_round_ps(_mm_div_ps(row3, quarter), _MM_FROUND_NEAREST_INT), quarter), half);\n\n            _mm_storeu_ps(&dst[j * rows + i], row0);\n            _mm_storeu_ps(&dst[(j + 1) * rows + i], row1);\n            _mm_storeu_ps(&dst[(j + 2) * rows + i], row2);\n            _mm_storeu_ps(&dst[(j + 3) * rows + i], row3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m128 quarter = _mm_set1_ps(0.25f);\n    const __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 values = _mm_set_ps(\n                src[(i + 3) * cols + j],\n                src[(i + 2) * cols + j],\n                src[(i + 1) * cols + j],\n                src[i * cols + j]\n            );\n\n            // Round to nearest quarter\n            __m128 divided = _mm_div_ps(values, quarter);\n            __m128 rounded = _mm_add_ps(divided, half);\n            __m128 floored = _mm_cvtepi32_ps(_mm_cvttps_epi32(rounded));\n            __m128 result = _mm_mul_ps(floored, quarter);\n\n            // Store transposed and rounded values\n            dst[j * rows + i] = _mm_cvtss_f32(result);\n            dst[j * rows + i + 1] = _mm_cvtss_f32(_mm_shuffle_ps(result, result, _MM_SHUFFLE(1, 1, 1, 1)));\n            dst[j * rows + i + 2] = _mm_cvtss_f32(_mm_shuffle_ps(result, result, _MM_SHUFFLE(2, 2, 2, 2)));\n            dst[j * rows + i + 3] = _mm_cvtss_f32(_mm_shuffle_ps(result, result, _MM_SHUFFLE(3, 3, 3, 3)));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i * simd_width); // Load 4 floats from tensor1\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i * simd_width); // Load 4 floats from tensor2\n        __m128 result = _mm_sub_ps(vec1, vec2);              // Perform element-wise subtraction\n        __m128 rounded = _mm_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm_storeu_ps(dst + i * simd_width, rounded);        // Store the result in dst\n    }\n\n    // Handle remaining elements\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? floor(result) : ceil(result); // Round towards zero manually\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i * simd_width);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i * simd_width);\n        __m128 sub_result = _mm_sub_ps(vec1, vec2);\n        __m128 rounded_result = _mm_cvtepi32_ps(_mm_cvttps_epi32(sub_result));\n        _mm_storeu_ps(dst + i * simd_width, rounded_result);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (float)((int)sub_result); // Round towards zero\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i * simd_width);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i * simd_width);\n        __m128 sub_result = _mm_sub_ps(vec1, vec2);\n        __m128 rounded_result = _mm_round_ps(sub_result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i * simd_width, rounded_result);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        float sub_result = tensor1[i] - tensor2[i];\n        dst[i] = (float)((int)sub_result); // Round towards zero\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using SSE\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128 vec1 = _mm_loadu_ps(&tensor1[i]); // Load 4 floats from tensor1\n        __m128 vec2 = _mm_loadu_ps(&tensor2[i]); // Load 4 floats from tensor2\n        __m128 result = _mm_sub_ps(vec1, vec2);  // Perform element-wise subtraction\n        __m128 rounded = _mm_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC); // Round towards zero\n        _mm_storeu_ps(&dst[i], rounded);         // Store the result in dst\n    }\n\n    // Handle any remaining elements that don't fit into a multiple of 4\n    for (; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? floor(result) : ceil(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_iterations = total_elements / simd_width;\n    size_t remainder = total_elements % simd_width;\n\n    for (size_t i = 0; i < simd_iterations; ++i) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i * simd_width);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i * simd_width);\n        __m128 result = _mm_sub_ps(vec1, vec2);\n        __m128 rounded_result = _mm_cvtepi32_ps(_mm_cvttps_epi32(result));\n        _mm_storeu_ps(dst + i * simd_width, rounded_result);\n    }\n\n    for (size_t i = simd_iterations * simd_width; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (float)((int)result); // Round towards zero\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 2; // SSE2 processes 2 doubles at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 2 doubles using SIMD\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]); // Load 2 doubles from mat1\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]); // Load 2 doubles from mat2\n        __m128d result = _mm_mul_pd(vec1, vec2); // Element-wise multiplication\n        _mm_storeu_pd(&dst[i], result);         // Store the result in dst\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 2; // SSE2 processes 2 double-precision floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]); // Load 2 doubles from mat1\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]); // Load 2 doubles from mat2\n        __m128d result = _mm_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm_storeu_pd(&dst[i], result);         // Store the result in dst\n    }\n\n    // Handle remaining elements (if total_elements is not divisible by simd_width)\n    for (; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]); // Load 2 double values from mat1\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]); // Load 2 double values from mat2\n        __m128d result = _mm_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle the remaining elements (if any)\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 2; // SSE2 processes two double-precision floats at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]); // Load 2 doubles from mat1\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]); // Load 2 doubles from mat2\n        __m128d result = _mm_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle remaining elements that do not fit into SIMD width\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t simd_width = 2; // SSE2 processes two double-precision floats at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]); // Load 2 double values from mat1\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]); // Load 2 double values from mat2\n        __m128d result = _mm_mul_pd(vec1, vec2); // Perform element-wise multiplication\n        _mm_storeu_pd(&dst[i], result); // Store the result in dst\n    }\n\n    // Handle remaining elements that don't fit in SIMD width\n    for (; i < total_elements; i++) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vals = _mm_loadu_ps(&mat[i * n + j]); // Load 4 elements from the matrix row\n            __m128 vec_vals = _mm_loadu_ps(&vec[j]);         // Load 4 elements from the vector\n            __m128 prod = _mm_mul_ps(mat_vals, vec_vals);    // Multiply element-wise\n            sum = _mm_add_ps(sum, prod);                    // Accumulate the sum\n        }\n\n        // Horizontal add to sum the 4 floats in the SIMD register\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n\n        // Store the accumulated sum in the destination vector\n        _mm_store_ss(&dst[i], sum);\n\n        // Handle remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vals = _mm_loadu_ps(&mat[i * n + j]); // Load 4 elements from the matrix row\n            __m128 vec_vals = _mm_loadu_ps(&vec[j]);         // Load 4 elements from the vector\n            __m128 prod = _mm_mul_ps(mat_vals, vec_vals);    // Multiply element-wise\n            sum = _mm_add_ps(sum, prod);                    // Accumulate the sum\n        }\n\n        // Horizontal addition to sum up the 4 values in the sum register\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n\n        float partial_sum = _mm_cvtss_f32(sum); // Extract the final sum from the register\n\n        // Process remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            partial_sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = partial_sum; // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vals = _mm_loadu_ps(&mat[i * n + j]); // Load 4 elements from the matrix row\n            __m128 vec_vals = _mm_loadu_ps(&vec[j]);         // Load 4 elements from the vector\n            __m128 prod = _mm_mul_ps(mat_vals, vec_vals);    // Multiply the matrix row and vector\n            sum = _mm_add_ps(sum, prod);                    // Accumulate the product\n        }\n\n        // Horizontal sum of the 4 elements in the sum register\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n        dst[i] = _mm_cvtss_f32(sum); // Store the result in the destination vector\n\n        // Process remaining elements if n is not a multiple of 4\n        for (; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 4 elements at a time using SIMD\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vals = _mm_loadu_ps(&mat[i * n + j]); // Load 4 elements from the matrix row\n            __m128 vec_vals = _mm_loadu_ps(&vec[j]);         // Load 4 elements from the vector\n            __m128 prod = _mm_mul_ps(mat_vals, vec_vals);    // Multiply the matrix row and vector elements\n            sum = _mm_add_ps(sum, prod);                    // Accumulate the results\n        }\n\n        // Horizontal sum of the 4 elements in the sum vector\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n        dst[i] = _mm_cvtss_f32(sum); // Store the result in the destination vector\n\n        // Handle remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process 4 elements at a time using SIMD\n        for (; j + 4 <= n; j += 4) {\n            __m128 mat_vals = _mm_loadu_ps(&mat[i * n + j]); // Load 4 elements from the matrix row\n            __m128 vec_vals = _mm_loadu_ps(&vec[j]);         // Load 4 elements from the vector\n            __m128 prod = _mm_mul_ps(mat_vals, vec_vals);    // Multiply element-wise\n            sum = _mm_add_ps(sum, prod);                    // Accumulate the results\n        }\n\n        // Horizontal sum of the 4 elements in the sum register\n        float partial_sum[4];\n        _mm_storeu_ps(partial_sum, sum);\n        dst[i] = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n\n        // Process remaining elements (if n is not a multiple of 4)\n        for (; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd(); // Initialize vector sum to zero\n    size_t i = 0;\n\n    // Process elements in chunks of 2 doubles using SSE2\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]); // Load 2 doubles into SSE register\n        sum_vec = _mm_add_pd(sum_vec, vec); // Add to the running sum\n    }\n\n    // Horizontal add to sum up the vector\n    double temp[2];\n    _mm_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1];\n\n    // Handle remaining elements (if total_elements is odd)\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    // Calculate and return the average\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd(); // Initialize a vector to hold the sum\n    size_t i = 0;\n\n    // Process 2 elements at a time using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]); // Load 2 double values\n        sum_vec = _mm_add_pd(sum_vec, vec); // Add to the sum vector\n    }\n\n    // Horizontal addition of the sum vector\n    double temp[2];\n    _mm_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1];\n\n    // Handle any remaining elements\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    // Calculate and return the average\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd(); // Initialize a vector to hold the sum\n    size_t i = 0;\n\n    // Process 2 elements at a time using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]); // Load 2 double values\n        sum_vec = _mm_add_pd(sum_vec, vec); // Add to the sum vector\n    }\n\n    // Horizontal addition of the sum vector\n    double temp[2];\n    _mm_storeu_pd(temp, sum_vec);\n    double total_sum = temp[0] + temp[1];\n\n    // Handle the remaining element if the total number of elements is odd\n    if (i < total_elements) {\n        total_sum += mat[i];\n    }\n\n    // Compute and return the average\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd(); // Initialize vector for summation\n    size_t i = 0;\n\n    // Process elements in chunks of 2 doubles using SSE2\n    for (; i + 2 <= total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]); // Load 2 doubles into vector\n        sum_vec = _mm_add_pd(sum_vec, vec); // Add to the running sum\n    }\n\n    // Horizontal sum of the vector\n    double sum_array[2];\n    _mm_storeu_pd(sum_array, sum_vec);\n    double total_sum = sum_array[0] + sum_array[1];\n\n    // Handle remaining elements (if total_elements is not a multiple of 2)\n    for (; i < total_elements; i++) {\n        total_sum += mat[i];\n    }\n\n    // Compute and return the average\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd(); // Initialize vector to hold the sum\n    size_t i = 0;\n\n    // Process elements in chunks of 2 doubles using SIMD\n    for (; i + 1 < total_elements; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]); // Load 2 doubles into SIMD register\n        sum_vec = _mm_add_pd(sum_vec, vec); // Add to the running sum\n    }\n\n    // Horizontal add to sum up the elements in the vector\n    double temp[2];\n    _mm_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1];\n\n    // Handle remaining elements (if total_elements is odd)\n    for (; i < total_elements; i++) {\n        sum += mat[i];\n    }\n\n    // Compute and return the average\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(current, next);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmplt_epi32(current, next);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(current, next);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(next, current);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            \n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(current, next);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(next, current);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            }\n        }\n        \n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(current, next);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(next, current);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            } else {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n\n        size_t j = 0;\n        __m128i prev_vals = _mm_loadu_si128((__m128i*)(row + j));\n        j += 4;\n\n        while (j + 4 <= cols) {\n            __m128i curr_vals = _mm_loadu_si128((__m128i*)(row + j));\n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(prev_vals, curr_vals);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(curr_vals, prev_vals);\n                if (_mm_movemask_epi8(cmp)) {\n                    return false;\n                }\n            }\n            prev_vals = curr_vals;\n            j += 4;\n        }\n\n        for (; j < cols; ++j) {\n            if (ascending) {\n                if (row[j - 1] > row[j]) {\n                    return false;\n                }\n            } else {\n                if (row[j - 1] < row[j]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base); // Set the base value in all lanes\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i)); // Load 8 elements from src\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);     // Divide src by base (integer division)\n        __m128i mul_vec = _mm_mullo_epi16(div_vec, base_vec);   // Multiply result by base to get nearest multiple\n        _mm_storeu_si128((__m128i*)(dst + i), mul_vec);         // Store result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base);\n    __m128i zero_vec = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Divide src by base: src / base\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);\n\n        // Multiply back to get nearest multiple: (src / base) * base\n        __m128i nearest_multiple = _mm_mullo_epi16(div_vec, base_vec);\n\n        // Ensure the result is no greater than src\n        __m128i mask = _mm_cmpgt_epi16(nearest_multiple, src_vec);\n        nearest_multiple = _mm_blendv_epi8(nearest_multiple, src_vec, mask);\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], nearest_multiple);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        int16_t quotient = src[i] / base;\n        int16_t nearest = quotient * base;\n        dst[i] = (nearest <= src[i]) ? nearest : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base); // Broadcast base to all elements\n    __m128i zero_vec = _mm_setzero_si128(); // Zero vector for comparison\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 elements from src\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Divide each element in src_vec by base (integer division)\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);\n\n        // Multiply the result back by base to get the nearest multiple <= src\n        __m128i nearest_multiple = _mm_mullo_epi16(div_vec, base_vec);\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], nearest_multiple);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base); // Set base value in all lanes\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i)); // Load 8 elements from src\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);     // Divide src by base\n        __m128i mul_vec = _mm_mullo_epi16(div_vec, base_vec);   // Multiply quotient by base\n        _mm_storeu_si128((__m128i*)(dst + i), mul_vec);         // Store result in dst\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base); // Set all elements to 'base'\n    size_t i = 0;\n\n    // Process 8 elements at a time (128 bits / 16 bits per element)\n    for (; i + 8 <= length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i)); // Load 8 elements from src\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);     // Divide src by base\n        __m128i mul_vec = _mm_mullo_epi16(div_vec, base_vec);   // Multiply quotient by base\n        _mm_storeu_si128((__m128i*)(dst + i), mul_vec);         // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Use SIMD for column-wise comparison\n        for (; j + 2 <= cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(row + j);\n            __m128d vector_vec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_vec, vector_vec);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 0b11) {\n                break;\n            }\n        }\n\n        // Check remaining elements (if cols is not divisible by 2)\n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                break;\n            }\n        }\n\n        // If all elements match, return true\n        if (j == cols) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Use SIMD to compare elements in chunks of 2 doubles (128-bit registers)\n        for (; j + 2 <= cols; j += 2) {\n            __m128d row_chunk = _mm_loadu_pd(row + j);\n            __m128d vector_chunk = _mm_loadu_pd(vector + j);\n            __m128d cmp_result = _mm_cmpeq_pd(row_chunk, vector_chunk);\n\n            // Check if all elements in cmp_result are equal\n            if (_mm_movemask_pd(cmp_result) != 0x3) {\n                break;\n            }\n        }\n\n        // Check remaining elements (if cols is not a multiple of 2)\n        for (; j < cols; j++) {\n            if (row[j] != vector[j]) {\n                break;\n            }\n        }\n\n        // If all elements matched, return true\n        if (j == cols) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(row + j);       // Load 2 doubles from the matrix row\n            __m128d vector_vec = _mm_loadu_pd(vector + j); // Load 2 doubles from the vector\n            __m128d cmp = _mm_cmpeq_pd(row_vec, vector_vec); // Compare elements\n            int mask = _mm_movemask_pd(cmp);               // Extract comparison results as a mask\n\n            if (mask != 0b11) { // If any element doesn't match\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t simd_width = 2; // SSE2 processes 2 doubles at a time\n    size_t aligned_cols = cols - (cols % simd_width);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n\n        for (size_t j = 0; j < aligned_cols; j += simd_width) {\n            __m128d row_chunk = _mm_loadu_pd(row + j);\n            __m128d vector_chunk = _mm_loadu_pd(vector + j);\n            __m128d cmp_result = _mm_cmpeq_pd(row_chunk, vector_chunk);\n\n            int mask = _mm_movemask_pd(cmp_result);\n            if (mask != 0b11) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match && aligned_cols < cols) {\n            for (size_t j = aligned_cols; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Use SSE2 intrinsics to compare elements in chunks of 2 doubles\n        __m128d result = _mm_set1_pd(1.0); // Initialize result to all ones (true)\n        for (; j + 2 <= cols; j += 2) {\n            __m128d row_chunk = _mm_loadu_pd(row + j);\n            __m128d vector_chunk = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(row_chunk, vector_chunk);\n            result = _mm_and_pd(result, cmp); // Combine results\n        }\n\n        // Check if all elements in the SIMD register are true\n        if (_mm_movemask_pd(result) != 3) {\n            continue; // Not a match, move to the next row\n        }\n\n        // Handle remaining elements (if cols is not a multiple of 2)\n        for (; j < cols; ++j) {\n            if (row[j] != vector[j]) {\n                goto next_row; // Not a match, move to the next row\n            }\n        }\n\n        // If we reach here, the row matches the vector\n        return true;\n\n    next_row:\n        continue;\n    }\n\n    // No matching row found\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= total_elements; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(vecA, vecB);\n        __m128i mask = _mm_and_si128(cmp, _mm_set1_epi8(1));\n        sum = _mm_add_epi8(sum, mask);\n    }\n\n    // Sum up the elements in the SIMD register\n    sum = _mm_sad_epu8(sum, zero);\n    equal_count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m128i sum_vector = _mm_setzero_si128();\n\n    // Process 16 elements at a time\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i cmp_result = _mm_cmpeq_epi8(vecA, vecB);\n        sum_vector = _mm_add_epi8(sum_vector, cmp_result);\n    }\n\n    // Sum up the results in the SIMD register\n    uint8_t temp[16];\n    _mm_storeu_si128((__m128i*)temp, sum_vector);\n    for (int j = 0; j < 16; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= total_elements; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(vecA, vecB);\n        __m128i mask = _mm_and_si128(cmp, _mm_set1_epi8(1));\n        sum = _mm_add_epi8(sum, mask);\n    }\n\n    // Horizontal sum of the 16-byte SIMD register\n    __m128i sum_high = _mm_unpackhi_epi8(sum, zero);\n    __m128i sum_low = _mm_unpacklo_epi8(sum, zero);\n    __m128i sum_16 = _mm_add_epi16(sum_low, sum_high);\n\n    __m128i sum_32_high = _mm_unpackhi_epi16(sum_16, zero);\n    __m128i sum_32_low = _mm_unpacklo_epi16(sum_16, zero);\n    __m128i sum_32 = _mm_add_epi32(sum_32_low, sum_32_high);\n\n    equal_count += _mm_extract_epi16(sum_32, 0);\n    equal_count += _mm_extract_epi16(sum_32, 1);\n    equal_count += _mm_extract_epi16(sum_32, 2);\n    equal_count += _mm_extract_epi16(sum_32, 3);\n\n    // Process remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    size_t i = 0;\n    __m128i count_vector = _mm_setzero_si128();\n\n    // Process 16 elements at a time using SIMD\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i cmp_result = _mm_cmpeq_epi8(vecA, vecB);\n        count_vector = _mm_add_epi8(count_vector, cmp_result);\n    }\n\n    // Sum up the counts in the count_vector\n    uint8_t temp[16];\n    _mm_storeu_si128((__m128i*)temp, count_vector);\n    for (int j = 0; j < 16; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t equal_count = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= total_elements; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i cmp = _mm_cmpeq_epi8(vecA, vecB);\n        __m128i mask = _mm_and_si128(cmp, _mm_set1_epi8(1));\n        sum = _mm_add_epi8(sum, mask);\n    }\n\n    // Sum up the 16 bytes in the SIMD register\n    uint8_t temp[16];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    for (int j = 0; j < 16; ++j) {\n        equal_count += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            equal_count++;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i scalar = _mm_set1_epi64x(a); // Broadcast scalar `a` to all elements of a 128-bit register\n    size_t i = 0;\n\n    // Process 2 elements at a time\n    for (; i + 2 <= length; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]); // Load 2 elements of x\n        __m128i vec_y = _mm_loadu_si128((__m128i*)&y[i]); // Load 2 elements of y\n\n        __m128i mul_result = _mm_mullo_epi64(vec_x, scalar); // Multiply x[i] by a\n        __m128i abs_y = _mm_abs_epi64(vec_y);               // Take absolute value of y[i]\n        __m128i result = _mm_sub_epi64(mul_result, abs_y);  // Subtract abs(y) from (a * x)\n\n        _mm_storeu_si128((__m128i*)&y[i], result);          // Store the result back to y\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        y[i] = (a * x[i]) - (y[i] < 0 ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i scalar = _mm_set1_epi64x(a); // Broadcast scalar 'a' to all elements\n    size_t i;\n\n    for (i = 0; i + 2 <= length; i += 2) {\n        // Load 2 elements from x\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n\n        // Perform scalar-vector multiplication\n        __m128i mul_result = _mm_mullo_epi64(vec_x, scalar);\n\n        // Load 2 elements from y\n        __m128i vec_y = _mm_loadu_si128((__m128i*)&y[i]);\n\n        // Compute absolute value of y\n        __m128i abs_y = _mm_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m128i result = _mm_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = y[i] < 0 ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i scalar = _mm_set1_epi64x(a); // Load scalar a into all lanes of a 128-bit register\n    size_t i;\n\n    for (i = 0; i + 2 <= length; i += 2) {\n        // Load 2 elements from x\n        __m128i vec_x = _mm_loadu_si128((__m128i *)&x[i]);\n        \n        // Perform scalar-vector multiplication\n        __m128i mul_result = _mm_mullo_epi64(vec_x, scalar);\n        \n        // Load 2 elements from y\n        __m128i vec_y = _mm_loadu_si128((__m128i *)&y[i]);\n        \n        // Compute absolute value of y\n        __m128i abs_y = _mm_abs_epi64(vec_y);\n        \n        // Perform element-wise subtraction\n        __m128i result = _mm_sub_epi64(mul_result, abs_y);\n        \n        // Store the result back to y\n        _mm_storeu_si128((__m128i *)&y[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        y[i] = (a * x[i]) - (y[i] < 0 ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i scalar = _mm_set1_epi64x(a); // Load scalar 'a' into all elements of a 128-bit register\n    size_t i;\n\n    for (i = 0; i + 2 <= length; i += 2) {\n        // Load 2 elements from x\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n\n        // Perform scalar-vector multiplication\n        __m128i mul_result = _mm_mullo_epi64(vec_x, scalar);\n\n        // Load 2 elements from y\n        __m128i vec_y = _mm_loadu_si128((__m128i*)&y[i]);\n\n        // Compute absolute value of y\n        __m128i abs_y = _mm_abs_epi64(vec_y);\n\n        // Perform element-wise subtraction\n        __m128i result = _mm_sub_epi64(mul_result, abs_y);\n\n        // Store the result back to y\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int64_t mul_result = a * x[i];\n        int64_t abs_y = y[i] < 0 ? -y[i] : y[i];\n        y[i] = mul_result - abs_y;\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i scalar = _mm_set1_epi64x(a); // Broadcast scalar 'a' to all elements\n    size_t i;\n\n    for (i = 0; i + 2 <= length; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]); // Load 2 elements of x\n        __m128i vec_y = _mm_loadu_si128((__m128i*)&y[i]); // Load 2 elements of y\n\n        __m128i mul_result = _mm_mullo_epi64(vec_x, scalar); // Multiply x by scalar a\n        __m128i abs_y = _mm_abs_epi64(vec_y);               // Compute absolute value of y\n        __m128i result = _mm_sub_epi64(mul_result, abs_y);  // Subtract abs(y) from mul_result\n\n        _mm_storeu_si128((__m128i*)&y[i], result);          // Store the result back to y\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        y[i] = (a * x[i]) - ((y[i] < 0) ? -y[i] : y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    __m128i min_vals = _mm_set1_epi16(INT16_MAX);\n    __m128i max_vals = _mm_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_vals = _mm_min_epi16(min_vals, current_vals);\n        max_vals = _mm_max_epi16(max_vals, current_vals);\n    }\n\n    int16_t min_array[8], max_array[8];\n    _mm_storeu_si128((__m128i*)min_array, min_vals);\n    _mm_storeu_si128((__m128i*)max_array, max_vals);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (int j = 0; j < 8; ++j) {\n        if (min_array[j] < min_num) min_num = min_array[j];\n        if (max_array[j] > max_num) max_num = max_array[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vals = _mm_set1_epi16(INT16_MAX);\n    __m128i max_vals = _mm_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_vals = _mm_min_epi16(min_vals, current_vals);\n        max_vals = _mm_max_epi16(max_vals, current_vals);\n    }\n\n    int16_t min_array[8], max_array[8];\n    _mm_storeu_si128((__m128i*)min_array, min_vals);\n    _mm_storeu_si128((__m128i*)max_array, max_vals);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (int j = 0; j < 8; ++j) {\n        if (min_array[j] < min_num) min_num = min_array[j];\n        if (max_array[j] > max_num) max_num = max_array[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    MinMaxPair result = {min_num, max_num};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_vals = _mm_set1_epi16(INT16_MAX); // Initialize min vector with max possible value\n    __m128i max_vals = _mm_set1_epi16(INT16_MIN); // Initialize max vector with min possible value\n\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]); // Load 8 elements\n        min_vals = _mm_min_epi16(min_vals, current_vals);          // Compute min\n        max_vals = _mm_max_epi16(max_vals, current_vals);          // Compute max\n    }\n\n    // Reduce min and max vectors to scalar values\n    int16_t min_array[8], max_array[8];\n    _mm_storeu_si128((__m128i*)min_array, min_vals);\n    _mm_storeu_si128((__m128i*)max_array, max_vals);\n\n    int16_t min_result = INT16_MAX;\n    int16_t max_result = INT16_MIN;\n    for (int j = 0; j < 8; ++j) {\n        if (min_array[j] < min_result) min_result = min_array[j];\n        if (max_array[j] > max_result) max_result = max_array[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_result;\n    result.max_num = max_result;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    if (length == 0) {\n        result.min_num = 0;\n        result.max_num = 0;\n        return result;\n    }\n\n    __m128i min_vec = _mm_set1_epi16(INT16_MAX);\n    __m128i max_vec = _mm_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i current_vec = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, current_vec);\n        max_vec = _mm_max_epi16(max_vec, current_vec);\n    }\n\n    // Extract min and max values from the SIMD registers\n    int16_t min_values[8], max_values[8];\n    _mm_storeu_si128((__m128i*)min_values, min_vec);\n    _mm_storeu_si128((__m128i*)max_values, max_vec);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (int j = 0; j < 8; ++j) {\n        if (min_values[j] < min_num) min_num = min_values[j];\n        if (max_values[j] > max_num) max_num = max_values[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    result.min_num = min_num;\n    result.max_num = max_num;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    __m128i min_vec = _mm_set1_epi16(INT16_MAX);\n    __m128i max_vec = _mm_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i current_vec = _mm_loadu_si128((__m128i*)(vec + i));\n        min_vec = _mm_min_epi16(min_vec, current_vec);\n        max_vec = _mm_max_epi16(max_vec, current_vec);\n    }\n\n    int16_t min_values[8], max_values[8];\n    _mm_storeu_si128((__m128i*)min_values, min_vec);\n    _mm_storeu_si128((__m128i*)max_values, max_vec);\n\n    result.min_num = INT16_MAX;\n    result.max_num = INT16_MIN;\n    for (int j = 0; j < 8; ++j) {\n        if (min_values[j] < result.min_num) result.min_num = min_values[j];\n        if (max_values[j] > result.max_num) result.max_num = max_values[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols < 2) {\n        return false; // Empty matrix or not enough columns to compare\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFFFFFF) {\n                return false; // Not strictly increasing\n            }\n        }\n\n        // Handle remaining elements\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false; // Not strictly increasing\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols < 2) {\n        return false; // Empty matrix or not enough columns to compare\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFFFFFF) {\n                return false; // Not strictly increasing\n            }\n        }\n\n        // Handle remaining elements\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false; // Not strictly increasing\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 4 <= cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols < 2) {\n        return false;\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j + 4 <= cols - 1; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n\n            __m128i cmp = _mm_cmpgt_epi32(next, current);\n            int mask = _mm_movemask_epi8(cmp);\n\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n\n    // Pointer to the first block\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            // Skip the first block comparison\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            // Compare the current block with the first block\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* row_first_block = first_block + i * cols;\n                const int* row_current_block = current_block + i * cols;\n\n                for (size_t j = 0; j < block_size; j += 4) {\n                    __m128i first_vals = _mm_loadu_si128((__m128i*)(row_first_block + j));\n                    __m128i current_vals = _mm_loadu_si128((__m128i*)(row_current_block + j));\n\n                    __m128i cmp_result = _mm_cmpeq_epi32(first_vals, current_vals);\n                    int mask = _mm_movemask_epi8(cmp_result);\n\n                    if (mask != 0xFFFF) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Pointer to the first block\n    const int* first_block = matrix;\n\n    // Iterate over all blocks\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            // Skip the first block (already considered as reference)\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            // Pointer to the current block\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            // Compare the current block with the first block\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* first_row = first_block + i * cols;\n                const int* current_row = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 4 <= block_size; j += 4) {\n                    __m128i first_vals = _mm_loadu_si128((__m128i*)(first_row + j));\n                    __m128i current_vals = _mm_loadu_si128((__m128i*)(current_row + j));\n                    __m128i cmp = _mm_cmpeq_epi32(first_vals, current_vals);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                // Handle remaining elements\n                for (; j < block_size; ++j) {\n                    if (first_row[j] != current_row[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n\n    // Pointer to the first block\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            // Skip the first block comparison\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* row_first_block = first_block + i * cols;\n                const int* row_current_block = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 4 <= block_size; j += 4) {\n                    __m128i first_vals = _mm_loadu_si128((__m128i*)(row_first_block + j));\n                    __m128i current_vals = _mm_loadu_si128((__m128i*)(row_current_block + j));\n                    __m128i cmp = _mm_cmpeq_epi32(first_vals, current_vals);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                // Handle remaining elements\n                for (; j < block_size; ++j) {\n                    if (row_first_block[j] != row_current_block[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n\n    // Pointer to the first block\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            // Skip the first block (already considered as reference)\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            // Pointer to the current block\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            // Compare the current block with the first block\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* first_row = first_block + i * cols;\n                const int* current_row = current_block + i * cols;\n\n                size_t j = 0;\n                for (; j + 4 <= block_size; j += 4) {\n                    __m128i first_vec = _mm_loadu_si128((__m128i*)(first_row + j));\n                    __m128i current_vec = _mm_loadu_si128((__m128i*)(current_row + j));\n                    __m128i cmp = _mm_cmpeq_epi32(first_vec, current_vec);\n                    if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                        return false;\n                    }\n                }\n\n                // Handle remaining elements\n                for (; j < block_size; ++j) {\n                    if (first_row[j] != current_row[j]) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    // Check if block_size is valid\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t blocks_per_row = cols / block_size;\n    size_t blocks_per_col = rows / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Pointer to the first block\n    const int* first_block = matrix;\n\n    for (size_t block_row = 0; block_row < blocks_per_col; ++block_row) {\n        for (size_t block_col = 0; block_col < blocks_per_row; ++block_col) {\n            // Skip the first block comparison\n            if (block_row == 0 && block_col == 0) {\n                continue;\n            }\n\n            const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n            // Compare the current block with the first block using SIMD\n            for (size_t i = 0; i < block_size; ++i) {\n                const int* first_row = first_block + i * block_size;\n                const int* current_row = current_block + i * cols;\n\n                for (size_t j = 0; j < block_size; j += 4) {\n                    __m128i first_vec = _mm_loadu_si128((__m128i*)(first_row + j));\n                    __m128i current_vec = _mm_loadu_si128((__m128i*)(current_row + j));\n                    __m128i cmp_result = _mm_cmpeq_epi32(first_vec, current_vec);\n\n                    if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n    const double* first_block = vec;\n\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        const double* current_block = vec + block_idx * block_size;\n\n        for (size_t i = 0; i < block_size; i += 2) {\n            __m128d first_vals = _mm_loadu_pd(first_block + i);\n            __m128d current_vals = _mm_loadu_pd(current_block + i);\n\n            __m128d diff = _mm_sub_pd(first_vals, current_vals);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    size_t simd_width = 2; // SSE2 processes 2 doubles at a time\n    size_t simd_iters = block_size / simd_width;\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        for (size_t i = 0; i < simd_iters; ++i) {\n            size_t base_idx = i * simd_width;\n            size_t ref_idx = base_idx;\n            size_t cmp_idx = block_idx * block_size + base_idx;\n\n            __m128d ref_vec = _mm_loadu_pd(&vec[ref_idx]);\n            __m128d cmp_vec = _mm_loadu_pd(&vec[cmp_idx]);\n\n            __m128d diff = _mm_sub_pd(ref_vec, cmp_vec);\n            __m128d abs_diff = _mm_max_pd(diff, _mm_sub_pd(_mm_setzero_pd(), diff));\n\n            __m128d cmp_result = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp_result)) {\n                return false;\n            }\n        }\n\n        // Handle remaining elements in the block if block_size is not a multiple of simd_width\n        for (size_t i = simd_iters * simd_width; i < block_size; ++i) {\n            size_t ref_idx = i;\n            size_t cmp_idx = block_idx * block_size + i;\n            double diff = vec[ref_idx] - vec[cmp_idx];\n            if (diff > tolerance || diff < -tolerance) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d ref_block = _mm_loadu_pd(&vec[j]);\n            __m128d curr_block = _mm_loadu_pd(&vec[i * block_size + j]);\n\n            __m128d diff = _mm_sub_pd(ref_block, curr_block);\n            __m128d abs_diff = _mm_max_pd(diff, _mm_sub_pd(_mm_setzero_pd(), diff));\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n\n            if (_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d first_block = _mm_loadu_pd(&vec[j]);\n            __m128d current_block = _mm_loadu_pd(&vec[i * block_size + j]);\n\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_max_pd(diff, _mm_sub_pd(_mm_setzero_pd(), diff));\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n\n            if (_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (!vec || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 1) {\n        return false;\n    }\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        for (size_t i = 0; i < block_size; i += 2) {\n            size_t idx1 = i;\n            size_t idx2 = block_idx * block_size + i;\n\n            __m128d block1 = _mm_loadu_pd(&vec[idx1]);\n            __m128d block2 = _mm_loadu_pd(&vec[idx2]);\n\n            __m128d diff = _mm_sub_pd(block1, block2);\n            __m128d abs_diff = _mm_max_pd(diff, _mm_sub_pd(_mm_setzero_pd(), diff));\n\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol_vec);\n            if (_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n        size_t j = 0;\n\n        // Process columns in chunks of 2 using SSE\n        for (; j + 1 < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]); // Load 2 doubles from the row\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec)); // Accumulate squared values\n        }\n\n        // Horizontal add to sum up the two elements in the SSE register\n        double partial_sum[2];\n        _mm_storeu_pd(partial_sum, sum);\n        double row_norm_squared = partial_sum[0] + partial_sum[1];\n\n        // Process remaining columns (if cols is not divisible by 2)\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            row_norm_squared += val * val;\n        }\n\n        // Compute the Euclidean norm and compare with the threshold\n        double row_norm = sqrt(row_norm_squared);\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n\n        // Process 2 elements at a time\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]); // Load 2 elements\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));       // Accumulate squared values\n        }\n\n        // Horizontal add to sum up the two lanes in the SSE register\n        sum = _mm_add_pd(sum, _mm_shuffle_pd(sum, sum, 0x1));\n        double row_sum = _mm_cvtsd_f64(sum);\n\n        // Handle the remaining element if cols is odd\n        if (j < cols) {\n            double remaining = matrix[i * cols + j];\n            row_sum += remaining * remaining;\n        }\n\n        // Compute the square root of the sum (Euclidean norm)\n        double norm = sqrt(row_sum);\n\n        // Compare with the threshold\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum_vec = _mm_setzero_pd(); // Initialize sum vector to zero\n\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            // Load two double-precision values from the row\n            __m128d row_vals = _mm_loadu_pd(&matrix[i * cols + j]);\n            // Square the values\n            __m128d squared_vals = _mm_mul_pd(row_vals, row_vals);\n            // Accumulate the squared values\n            sum_vec = _mm_add_pd(sum_vec, squared_vals);\n        }\n\n        // Horizontal addition to sum the two values in the vector\n        double partial_sum[2];\n        _mm_storeu_pd(partial_sum, sum_vec);\n        double row_sum = partial_sum[0] + partial_sum[1];\n\n        // Handle remaining element if cols is odd\n        if (j < cols) {\n            double val = matrix[i * cols + j];\n            row_sum += val * val;\n        }\n\n        // Compute the square root of the sum (Euclidean norm)\n        double norm = sqrt(row_sum);\n\n        // Compare the norm with the threshold\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n\n        size_t j = 0;\n        for (; j + 1 < cols; j += 2) {\n            __m128d row_vals = _mm_loadu_pd(&matrix[i * cols + j]); // Load two elements of the row\n            __m128d squared_vals = _mm_mul_pd(row_vals, row_vals); // Square the elements\n            sum = _mm_add_pd(sum, squared_vals); // Accumulate the squared values\n        }\n\n        // Horizontal add to get the sum of the two lanes in the SSE register\n        double partial_sum[2];\n        _mm_storeu_pd(partial_sum, sum);\n        double norm = partial_sum[0] + partial_sum[1];\n\n        // Handle remaining element if cols is odd\n        if (j < cols) {\n            double val = matrix[i * cols + j];\n            norm += val * val;\n        }\n\n        norm = sqrt(norm); // Compute the Euclidean norm\n        result[i] = norm > thresholds[i]; // Compare with the threshold\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n\n        size_t j = 0;\n        for (; j + 2 <= cols; j += 2) {\n            __m128d row_vals = _mm_loadu_pd(&matrix[i * cols + j]); // Load two double values from the row\n            sum = _mm_add_pd(sum, _mm_mul_pd(row_vals, row_vals));  // Accumulate squared values\n        }\n\n        // Horizontal sum of the accumulated values in the SIMD register\n        double partial_sum[2];\n        _mm_storeu_pd(partial_sum, sum);\n        double norm_squared = partial_sum[0] + partial_sum[1];\n\n        // Handle remaining elements if cols is not a multiple of 2\n        for (; j < cols; ++j) {\n            norm_squared += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        // Compare the Euclidean norm with the threshold\n        double norm = sqrt(norm_squared);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // Each __m128i can hold 4 32-bit integers\n\n    // Process in chunks of 4 using SIMD\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 integers from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i)); // Load 4 integers from B\n        __m128i vecC = _mm_or_si128(vecA, vecB);           // Perform bitwise OR\n        _mm_storeu_si128((__m128i*)(C + i), vecC);         // Store result in C\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // Each __m128i can hold 4 32-bit integers\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 integers from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i)); // Load 4 integers from B\n        __m128i vecC = _mm_or_si128(vecA, vecB);           // Perform bitwise OR\n        _mm_storeu_si128((__m128i*)(C + i), vecC);         // Store the result in C\n    }\n\n    // Process remaining elements that don't fit in SIMD width\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_width = 4; // Each __m128i can hold 4 32-bit integers\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 integers from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i)); // Load 4 integers from B\n        __m128i vecC = _mm_or_si128(vecA, vecB);           // Perform bitwise OR\n        _mm_storeu_si128((__m128i*)(C + i), vecC);         // Store the result in C\n    }\n\n    // Process remaining elements that don't fit in SIMD width\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= length; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 4 elements from B\n        __m128i vecC = _mm_or_si128(vecA, vecB);         // Perform bitwise OR\n        _mm_storeu_si128((__m128i*)&C[i], vecC);         // Store the result in C\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE2\n    for (; i + 4 <= length; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 4 elements from B\n        __m128i vecC = _mm_or_si128(vecA, vecB);         // Perform bitwise OR\n        _mm_storeu_si128((__m128i*)&C[i], vecC);         // Store the result in C\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i vecA_lo = _mm_unpacklo_epi8(vecA, _mm_setzero_si128());\n        __m128i vecA_hi = _mm_unpackhi_epi8(vecA, _mm_setzero_si128());\n        __m128i vecB_lo = _mm_unpacklo_epi8(vecB, _mm_setzero_si128());\n        __m128i vecB_hi = _mm_unpackhi_epi8(vecB, _mm_setzero_si128());\n\n        __m128i sqA_lo = _mm_mullo_epi16(vecA_lo, vecA_lo);\n        __m128i sqA_hi = _mm_mullo_epi16(vecA_hi, vecA_hi);\n        __m128i sqB_lo = _mm_mullo_epi16(vecB_lo, vecB_lo);\n        __m128i sqB_hi = _mm_mullo_epi16(vecB_hi, vecB_hi);\n\n        __m128i prod_lo = _mm_mullo_epi16(vecA_lo, vecB_lo);\n        __m128i prod_hi = _mm_mullo_epi16(vecA_hi, vecB_hi);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(sqA_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(sqA_lo, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(sqA_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(sqA_hi, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(sqB_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(sqB_lo, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(sqB_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(sqB_hi, _mm_setzero_si128())));\n\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_add_epi32(_mm_unpacklo_epi16(prod_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(prod_lo, _mm_setzero_si128())));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_add_epi32(_mm_unpacklo_epi16(prod_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(prod_hi, _mm_setzero_si128())));\n    }\n\n    int32_t squares[4], products[4];\n    _mm_storeu_si128((__m128i *)squares, sum_of_squares);\n    _mm_storeu_si128((__m128i *)products, sum_of_products);\n\n    int64_t total_squares = (int64_t)squares[0] + squares[1] + squares[2] + squares[3];\n    int64_t total_products = (int64_t)products[0] + products[1] + products[2] + products[3];\n\n    for (; i < length; i++) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += (int64_t)a * a + (int64_t)b * b;\n        total_products += (int64_t)a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i vecA_lo = _mm_unpacklo_epi8(vecA, _mm_setzero_si128());\n        __m128i vecA_hi = _mm_unpackhi_epi8(vecA, _mm_setzero_si128());\n        __m128i vecB_lo = _mm_unpacklo_epi8(vecB, _mm_setzero_si128());\n        __m128i vecB_hi = _mm_unpackhi_epi8(vecB, _mm_setzero_si128());\n\n        __m128i squares_lo = _mm_madd_epi16(vecA_lo, vecA_lo);\n        __m128i squares_hi = _mm_madd_epi16(vecA_hi, vecA_hi);\n        __m128i products_lo = _mm_madd_epi16(vecA_lo, vecB_lo);\n        __m128i products_hi = _mm_madd_epi16(vecA_hi, vecB_hi);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squares_lo);\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squares_hi);\n        sum_of_products = _mm_add_epi32(sum_of_products, products_lo);\n        sum_of_products = _mm_add_epi32(sum_of_products, products_hi);\n    }\n\n    int32_t squares_buffer[4], products_buffer[4];\n    _mm_storeu_si128((__m128i *)squares_buffer, sum_of_squares);\n    _mm_storeu_si128((__m128i *)products_buffer, sum_of_products);\n\n    int64_t total_squares = (int64_t)squares_buffer[0] + squares_buffer[1] + squares_buffer[2] + squares_buffer[3];\n    int64_t total_products = (int64_t)products_buffer[0] + products_buffer[1] + products_buffer[2] + products_buffer[3];\n\n    for (; i < length; i++) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += (int64_t)a * a;\n        total_products += (int64_t)a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i vecA_lo = _mm_unpacklo_epi8(vecA, _mm_setzero_si128());\n        __m128i vecA_hi = _mm_unpackhi_epi8(vecA, _mm_setzero_si128());\n        __m128i vecB_lo = _mm_unpacklo_epi8(vecB, _mm_setzero_si128());\n        __m128i vecB_hi = _mm_unpackhi_epi8(vecB, _mm_setzero_si128());\n\n        __m128i squareA_lo = _mm_madd_epi16(vecA_lo, vecA_lo);\n        __m128i squareA_hi = _mm_madd_epi16(vecA_hi, vecA_hi);\n        __m128i squareB_lo = _mm_madd_epi16(vecB_lo, vecB_lo);\n        __m128i squareB_hi = _mm_madd_epi16(vecB_hi, vecB_hi);\n\n        __m128i product_lo = _mm_madd_epi16(vecA_lo, vecB_lo);\n        __m128i product_hi = _mm_madd_epi16(vecA_hi, vecB_hi);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squareA_lo);\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squareA_hi);\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squareB_lo);\n        sum_of_squares = _mm_add_epi32(sum_of_squares, squareB_hi);\n\n        sum_of_products = _mm_add_epi32(sum_of_products, product_lo);\n        sum_of_products = _mm_add_epi32(sum_of_products, product_hi);\n    }\n\n    int32_t squares[4], products[4];\n    _mm_storeu_si128((__m128i*)squares, sum_of_squares);\n    _mm_storeu_si128((__m128i*)products, sum_of_products);\n\n    int64_t total_squares = (int64_t)squares[0] + squares[1] + squares[2] + squares[3];\n    int64_t total_products = (int64_t)products[0] + products[1] + products[2] + products[3];\n\n    for (; i < length; i++) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += (int64_t)a * a + (int64_t)b * b;\n        total_products += (int64_t)a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i vecA_lo = _mm_unpacklo_epi8(vecA, _mm_setzero_si128());\n        __m128i vecA_hi = _mm_unpackhi_epi8(vecA, _mm_setzero_si128());\n        __m128i vecB_lo = _mm_unpacklo_epi8(vecB, _mm_setzero_si128());\n        __m128i vecB_hi = _mm_unpackhi_epi8(vecB, _mm_setzero_si128());\n\n        __m128i squares_lo = _mm_mullo_epi16(vecA_lo, vecA_lo);\n        __m128i squares_hi = _mm_mullo_epi16(vecA_hi, vecA_hi);\n        __m128i products_lo = _mm_mullo_epi16(vecA_lo, vecB_lo);\n        __m128i products_hi = _mm_mullo_epi16(vecA_hi, vecB_hi);\n\n        sum_squares = _mm_add_epi32(sum_squares, _mm_add_epi32(_mm_unpacklo_epi16(squares_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(squares_lo, _mm_setzero_si128())));\n        sum_squares = _mm_add_epi32(sum_squares, _mm_add_epi32(_mm_unpacklo_epi16(squares_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(squares_hi, _mm_setzero_si128())));\n\n        sum_products = _mm_add_epi32(sum_products, _mm_add_epi32(_mm_unpacklo_epi16(products_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(products_lo, _mm_setzero_si128())));\n        sum_products = _mm_add_epi32(sum_products, _mm_add_epi32(_mm_unpacklo_epi16(products_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(products_hi, _mm_setzero_si128())));\n    }\n\n    int32_t squares[4], products[4];\n    _mm_storeu_si128((__m128i *)squares, sum_squares);\n    _mm_storeu_si128((__m128i *)products, sum_products);\n\n    int64_t total_squares = squares[0] + squares[1] + squares[2] + squares[3];\n    int64_t total_products = products[0] + products[1] + products[2] + products[3];\n\n    for (; i < length; i++) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += a * a;\n        total_products += a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n    size_t i;\n\n    for (i = 0; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i vecA_lo = _mm_unpacklo_epi8(vecA, _mm_setzero_si128());\n        __m128i vecA_hi = _mm_unpackhi_epi8(vecA, _mm_setzero_si128());\n        __m128i vecB_lo = _mm_unpacklo_epi8(vecB, _mm_setzero_si128());\n        __m128i vecB_hi = _mm_unpackhi_epi8(vecB, _mm_setzero_si128());\n\n        __m128i squareA_lo = _mm_mullo_epi16(vecA_lo, vecA_lo);\n        __m128i squareA_hi = _mm_mullo_epi16(vecA_hi, vecA_hi);\n        __m128i squareB_lo = _mm_mullo_epi16(vecB_lo, vecB_lo);\n        __m128i squareB_hi = _mm_mullo_epi16(vecB_hi, vecB_hi);\n\n        __m128i product_lo = _mm_mullo_epi16(vecA_lo, vecB_lo);\n        __m128i product_hi = _mm_mullo_epi16(vecA_hi, vecB_hi);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(squareA_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(squareA_lo, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(squareA_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(squareA_hi, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(squareB_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(squareB_lo, _mm_setzero_si128())));\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_add_epi32(_mm_unpacklo_epi16(squareB_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(squareB_hi, _mm_setzero_si128())));\n\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_add_epi32(_mm_unpacklo_epi16(product_lo, _mm_setzero_si128()), _mm_unpackhi_epi16(product_lo, _mm_setzero_si128())));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_add_epi32(_mm_unpacklo_epi16(product_hi, _mm_setzero_si128()), _mm_unpackhi_epi16(product_hi, _mm_setzero_si128())));\n    }\n\n    int32_t squares[4], products[4];\n    _mm_storeu_si128((__m128i*)squares, sum_of_squares);\n    _mm_storeu_si128((__m128i*)products, sum_of_products);\n\n    int64_t total_squares = (int64_t)squares[0] + squares[1] + squares[2] + squares[3];\n    int64_t total_products = (int64_t)products[0] + products[1] + products[2] + products[3];\n\n    for (; i < length; i++) {\n        int16_t a = A[i];\n        int16_t b = B[i];\n        total_squares += (int64_t)a * a + (int64_t)b * b;\n        total_products += (int64_t)a * b;\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 2 <= length; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);  // Load 2 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);  // Load 2 elements from B\n        __m128i vecNegB = _mm_andnot_si128(vecB, _mm_set1_epi64x(-1));  // Bitwise NOT of B\n        __m128i vecResult = _mm_and_si128(vecA, vecNegB);  // A & ~B\n        _mm_storeu_si128((__m128i*)&C[i], vecResult);  // Store the result in C\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stddef.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 2 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 2 elements from B\n        __m128i vecNegB = _mm_andnot_si128(vecB, _mm_set1_epi64x(-1)); // Compute bitwise NOT of B\n        __m128i vecResult = _mm_and_si128(vecA, vecNegB); // Perform bitwise AND\n        _mm_storeu_si128((__m128i*)&C[i], vecResult); // Store the result in C\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 2 <= length; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 2 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 2 elements from B\n        __m128i vecNegB = _mm_andnot_si128(vecB, _mm_set1_epi64x(-1)); // Bitwise negation of B\n        __m128i vecResult = _mm_and_si128(vecA, vecNegB); // A & ~B\n        _mm_storeu_si128((__m128i*)&C[i], vecResult); // Store the result in C\n    }\n\n    // Handle remaining elements (if length is not a multiple of 2)\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 2; // Each __m128i processes two 64-bit integers\n\n    for (; i < simd_length * 2; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vecB_not = _mm_andnot_si128(vecB, _mm_set1_epi64x(-1)); // Bitwise NOT of B\n        __m128i vecC = _mm_and_si128(vecA, vecB_not); // A & ~B\n        _mm_storeu_si128((__m128i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements if the length is not a multiple of 2\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2 (128-bit registers)\n    for (; i + 2 <= length; i += 2) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);  // Load 2 elements from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);  // Load 2 elements from B\n        __m128i vecNegB = _mm_andnot_si128(vecB, _mm_set1_epi64x(-1));  // Compute bitwise NOT of B\n        __m128i vecC = _mm_and_si128(vecA, vecNegB);  // Perform bitwise AND between A and NOT B\n        _mm_storeu_si128((__m128i*)&C[i], vecC);  // Store the result in C\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using SIMD\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i zero = _mm_setzero_si128();\n\n        // Compare A > 0 and B > 0\n        __m128i maskPosA = _mm_cmpgt_epi8(vecA, zero);\n        __m128i maskPosB = _mm_cmpgt_epi8(vecB, zero);\n        __m128i maskBothPos = _mm_and_si128(maskPosA, maskPosB);\n\n        // Compare A < 0 and B < 0\n        __m128i maskNegA = _mm_cmplt_epi8(vecA, zero);\n        __m128i maskNegB = _mm_cmplt_epi8(vecB, zero);\n        __m128i maskBothNeg = _mm_and_si128(maskNegA, maskNegB);\n\n        // Set values based on masks\n        __m128i resultPos = _mm_and_si128(maskBothPos, _mm_set1_epi8(1));\n        __m128i resultNeg = _mm_and_si128(maskBothNeg, _mm_set1_epi8(-1));\n        __m128i result = _mm_or_si128(resultPos, resultNeg);\n\n        // Store the result in C\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i posA = _mm_cmpgt_epi8(vecA, zero);\n        __m128i posB = _mm_cmpgt_epi8(vecB, zero);\n        __m128i negA = _mm_cmplt_epi8(vecA, zero);\n        __m128i negB = _mm_cmplt_epi8(vecB, zero);\n\n        __m128i bothPos = _mm_and_si128(posA, posB);\n        __m128i bothNeg = _mm_and_si128(negA, negB);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(bothPos, one), _mm_and_si128(bothNeg, neg_one));\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(bothPos, bothNeg), zero));\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using SIMD\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        // Compare A > 0 and B > 0\n        __m128i maskA_pos = _mm_cmpgt_epi8(vecA, _mm_setzero_si128());\n        __m128i maskB_pos = _mm_cmpgt_epi8(vecB, _mm_setzero_si128());\n        __m128i both_pos = _mm_and_si128(maskA_pos, maskB_pos);\n\n        // Compare A < 0 and B < 0\n        __m128i maskA_neg = _mm_cmplt_epi8(vecA, _mm_setzero_si128());\n        __m128i maskB_neg = _mm_cmplt_epi8(vecB, _mm_setzero_si128());\n        __m128i both_neg = _mm_and_si128(maskA_neg, maskB_neg);\n\n        // Set C[i] to 1 where both_pos is true\n        __m128i result_pos = _mm_and_si128(both_pos, _mm_set1_epi8(1));\n\n        // Set C[i] to -1 where both_neg is true\n        __m128i result_neg = _mm_and_si128(both_neg, _mm_set1_epi8(-1));\n\n        // Combine results\n        __m128i result = _mm_or_si128(result_pos, result_neg);\n\n        // Store the result in C\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i is_positive_A = _mm_cmpgt_epi8(vecA, zero);\n        __m128i is_positive_B = _mm_cmpgt_epi8(vecB, zero);\n        __m128i is_negative_A = _mm_cmplt_epi8(vecA, zero);\n        __m128i is_negative_B = _mm_cmplt_epi8(vecB, zero);\n\n        __m128i both_positive = _mm_and_si128(is_positive_A, is_positive_B);\n        __m128i both_negative = _mm_and_si128(is_negative_A, is_negative_B);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_positive, one),\n            _mm_and_si128(both_negative, neg_one)\n        );\n\n        __m128i neither = _mm_andnot_si128(\n            _mm_or_si128(both_positive, both_negative),\n            zero\n        );\n\n        result = _mm_or_si128(result, neither);\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for (; i + 16 <= length; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i is_positive_A = _mm_cmpgt_epi8(vecA, zero);\n        __m128i is_positive_B = _mm_cmpgt_epi8(vecB, zero);\n        __m128i positive_mask = _mm_and_si128(is_positive_A, is_positive_B);\n\n        __m128i is_negative_A = _mm_cmplt_epi8(vecA, zero);\n        __m128i is_negative_B = _mm_cmplt_epi8(vecB, zero);\n        __m128i negative_mask = _mm_and_si128(is_negative_A, is_negative_B);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(positive_mask, one), _mm_and_si128(negative_mask, neg_one));\n\n        __m128i other_mask = _mm_andnot_si128(_mm_or_si128(positive_mask, negative_mask), zero);\n        result = _mm_or_si128(result, other_mask);\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecXOR = _mm_xor_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecXOR);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint with AND\n    for (; i + 8 <= length; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecAND = _mm_and_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecAND);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SIMD\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecXOR = _mm_xor_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecXOR);\n    }\n\n    // Handle remaining elements before the midpoint\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the element at the midpoint\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint using SIMD\n    for (; i + 8 <= length; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecAND = _mm_and_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecAND);\n    }\n\n    // Handle remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SIMD\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecXOR = _mm_xor_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecXOR);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    C[midpoint] = A[midpoint];\n    i = midpoint + 1;\n\n    // Process elements after the midpoint using SIMD\n    for (; i + 8 <= length; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecAND = _mm_and_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecAND);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SIMD\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecXOR = _mm_xor_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecXOR);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the element at the midpoint\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint using SIMD\n    for (; i + 8 <= length; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecAND = _mm_and_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecAND);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SIMD\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecXOR = _mm_xor_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecXOR);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the exact midpoint element\n    if (length % 2 != 0) {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    } else {\n        C[midpoint] = A[midpoint];\n        i = midpoint + 1;\n    }\n\n    // Process elements after the midpoint using SIMD\n    for (; i + 8 <= length; i += 8) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i vecAND = _mm_and_si128(vecA, vecB);\n        _mm_storeu_si128((__m128i*)(C + i), vecAND);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t i = 0;\n\n    __m128 a_vec, b_vec, c_vec, d_vec, result_vec;\n    __m128 cond1_mask, cond2_mask, cond1_and_cond2, cond1_and_not_cond2, not_cond1_and_cond2, not_cond1_and_not_cond2;\n\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 4 elements from A, B, C, D\n        a_vec = _mm_loadu_ps(&A[i]);\n        b_vec = _mm_loadu_ps(&B[i]);\n        c_vec = _mm_loadu_ps(&C[i]);\n        d_vec = _mm_loadu_ps(&D[i]);\n\n        // Load 4 boolean conditions and convert to masks\n        cond1_mask = _mm_castsi128_ps(_mm_set_epi32(cond1[i + 3], cond1[i + 2], cond1[i + 1], cond1[i]));\n        cond2_mask = _mm_castsi128_ps(_mm_set_epi32(cond2[i + 3], cond2[i + 2], cond2[i + 1], cond2[i]));\n\n        // Compute condition masks\n        cond1_and_cond2 = _mm_and_ps(cond1_mask, cond2_mask);\n        cond1_and_not_cond2 = _mm_and_ps(cond1_mask, _mm_xor_ps(cond2_mask, _mm_castsi128_ps(_mm_set1_epi32(-1))));\n        not_cond1_and_cond2 = _mm_and_ps(_mm_xor_ps(cond1_mask, _mm_castsi128_ps(_mm_set1_epi32(-1))), cond2_mask);\n        not_cond1_and_not_cond2 = _mm_and_ps(_mm_xor_ps(cond1_mask, _mm_castsi128_ps(_mm_set1_epi32(-1))), _mm_xor_ps(cond2_mask, _mm_castsi128_ps(_mm_set1_epi32(-1))));\n\n        // Perform the selection\n        result_vec = _mm_or_ps(\n            _mm_or_ps(_mm_and_ps(cond1_and_cond2, a_vec), _mm_and_ps(cond1_and_not_cond2, b_vec)),\n            _mm_or_ps(_mm_and_ps(not_cond1_and_cond2, c_vec), _mm_and_ps(not_cond1_and_not_cond2, d_vec))\n        );\n\n        // Store the result\n        _mm_storeu_ps(&out[i], result_vec);\n    }\n\n    // Handle the remaining elements\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 4 floats from A, B, C, D\n        __m128 vecA = _mm_loadu_ps(&A[i]);\n        __m128 vecB = _mm_loadu_ps(&B[i]);\n        __m128 vecC = _mm_loadu_ps(&C[i]);\n        __m128 vecD = _mm_loadu_ps(&D[i]);\n\n        // Load 4 boolean values from cond1 and cond2\n        __m128i vecCond1 = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i vecCond2 = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        // Convert boolean values to masks (0xFFFFFFFF for true, 0x00000000 for false)\n        __m128 maskCond1 = _mm_castsi128_ps(_mm_cmpeq_epi32(vecCond1, _mm_set1_epi32(1)));\n        __m128 maskCond2 = _mm_castsi128_ps(_mm_cmpeq_epi32(vecCond2, _mm_set1_epi32(1)));\n\n        // Compute masks for each condition\n        __m128 maskA = _mm_and_ps(maskCond1, maskCond2); // cond1 && cond2\n        __m128 maskB = _mm_and_ps(maskCond1, _mm_andnot_ps(maskCond2, _mm_set1_ps(-1))); // cond1 && !cond2\n        __m128 maskC = _mm_and_ps(_mm_andnot_ps(maskCond1, _mm_set1_ps(-1)), maskCond2); // !cond1 && cond2\n        __m128 maskD = _mm_andnot_ps(_mm_or_ps(maskCond1, maskCond2), _mm_set1_ps(-1)); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m128 resultA = _mm_and_ps(maskA, vecA);\n        __m128 resultB = _mm_and_ps(maskB, vecB);\n        __m128 resultC = _mm_and_ps(maskC, vecC);\n        __m128 resultD = _mm_and_ps(maskD, vecD);\n\n        // Combine results\n        __m128 result = _mm_or_ps(_mm_or_ps(resultA, resultB), _mm_or_ps(resultC, resultD));\n\n        // Store the result\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements (non-SIMD)\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        // Load 4 elements from A, B, C, D\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        // Load 4 boolean conditions from cond1 and cond2\n        __m128i cond1_vec = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_vec = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond1_vec, _mm_set1_epi32(1)));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond2_vec, _mm_set1_epi32(1)));\n\n        // Compute intermediate masks\n        __m128 cond1_and_cond2 = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 cond1_and_not_cond2 = _mm_and_ps(cond1_mask, _mm_andnot_ps(cond2_mask, _mm_set1_ps(-1)));\n        __m128 not_cond1_and_cond2 = _mm_and_ps(_mm_andnot_ps(cond1_mask, _mm_set1_ps(-1)), cond2_mask);\n        __m128 not_cond1_and_not_cond2 = _mm_and_ps(_mm_andnot_ps(cond1_mask, _mm_set1_ps(-1)), _mm_andnot_ps(cond2_mask, _mm_set1_ps(-1)));\n\n        // Select values based on conditions\n        __m128 result = _mm_or_ps(\n            _mm_or_ps(_mm_and_ps(cond1_and_cond2, a), _mm_and_ps(cond1_and_not_cond2, b)),\n            _mm_or_ps(_mm_and_ps(not_cond1_and_cond2, c), _mm_and_ps(not_cond1_and_not_cond2, d))\n        );\n\n        // Store the result\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t aligned_size = total_size / simd_width * simd_width;\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i]));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        __m128 cond1_and_cond2 = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 cond1_and_not_cond2 = _mm_and_ps(cond1_mask, _mm_andnot_ps(cond2_mask, _mm_set1_ps(1.0f)));\n        __m128 not_cond1_and_cond2 = _mm_and_ps(_mm_andnot_ps(cond1_mask, _mm_set1_ps(1.0f)), cond2_mask);\n        __m128 not_cond1_and_not_cond2 = _mm_andnot_ps(_mm_or_ps(cond1_mask, cond2_mask), _mm_set1_ps(1.0f));\n\n        __m128 result_a = _mm_and_ps(cond1_and_cond2, a);\n        __m128 result_b = _mm_and_ps(cond1_and_not_cond2, b);\n        __m128 result_c = _mm_and_ps(not_cond1_and_cond2, c);\n        __m128 result_d = _mm_and_ps(not_cond1_and_not_cond2, d);\n\n        __m128 result = _mm_or_ps(_mm_or_ps(result_a, result_b), _mm_or_ps(result_c, result_d));\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        bool c1 = cond1[i];\n        bool c2 = cond2[i];\n        if (c1 && c2) {\n            out[i] = A[i];\n        } else if (c1 && !c2) {\n            out[i] = B[i];\n        } else if (!c1 && c2) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    size_t i = 0;\n    for (; i + simd_width <= total_elements; i += simd_width) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_vec = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_vec = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond1_vec, _mm_set1_epi32(1)));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond2_vec, _mm_set1_epi32(1)));\n\n        __m128 both_true = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 cond1_true_cond2_false = _mm_and_ps(cond1_mask, _mm_andnot_ps(cond2_mask, _mm_set1_ps(1.0f)));\n        __m128 cond1_false_cond2_true = _mm_and_ps(_mm_andnot_ps(cond1_mask, _mm_set1_ps(1.0f)), cond2_mask);\n        __m128 both_false = _mm_andnot_ps(_mm_or_ps(cond1_mask, cond2_mask), _mm_set1_ps(1.0f));\n\n        __m128 result = _mm_or_ps(\n            _mm_or_ps(_mm_and_ps(both_true, a), _mm_and_ps(cond1_true_cond2_false, b)),\n            _mm_or_ps(_mm_and_ps(cond1_false_cond2_true, c), _mm_and_ps(both_false, d))\n        );\n\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555); // Mask for bits 0x01\n    __m128i mask2 = _mm_set1_epi32(0x33333333); // Mask for bits 0x03\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F); // Mask for bits 0x0F\n    __m128i mask4 = _mm_set1_epi32(0x01010101); // Mask for summing bytes\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Step 1: Count bits in each 2-bit group\n        __m128i count = _mm_sub_epi32(data, _mm_and_si128(_mm_srli_epi32(data, 1), mask1));\n\n        // Step 2: Count bits in each 4-bit group\n        count = _mm_add_epi32(_mm_and_si128(count, mask2), _mm_and_si128(_mm_srli_epi32(count, 2), mask2));\n\n        // Step 3: Count bits in each byte\n        count = _mm_and_si128(_mm_add_epi32(count, _mm_srli_epi32(count, 4)), mask3);\n\n        // Step 4: Sum the bits in each byte\n        count = _mm_mullo_epi32(count, mask4);\n        count = _mm_srli_epi32(count, 24);\n\n        // Store the result\n        __m128i result = _mm_packus_epi32(count, count);\n        result = _mm_packus_epi16(result, result);\n        *(int*)&out[i] = _mm_cvtsi128_si32(result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        val = val - ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val + (val >> 4)) & 0x0F0F0F0F;\n        val = (val * 0x01010101) >> 24;\n        out[i] = (uint8_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555); // Mask for bits 0, 2, 4, ...\n    __m128i mask2 = _mm_set1_epi32(0x33333333); // Mask for pairs of bits\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F); // Mask for nibbles\n    __m128i mask4 = _mm_set1_epi32(0x01010101); // Mask for byte sums\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Count bits in each 32-bit integer\n        __m128i count = _mm_sub_epi32(vec, _mm_and_si128(_mm_srli_epi32(vec, 1), mask1));\n        count = _mm_add_epi32(_mm_and_si128(count, mask2), _mm_and_si128(_mm_srli_epi32(count, 2), mask2));\n        count = _mm_add_epi32(_mm_and_si128(count, mask3), _mm_srli_epi32(count, 4));\n        count = _mm_and_si128(count, mask4);\n\n        // Extract the lower 8 bits of each 32-bit integer and store in out\n        __m128i packed = _mm_packus_epi16(count, count); // Pack 16-bit to 8-bit\n        packed = _mm_packus_epi16(packed, packed);       // Pack again to fit into 8-bit\n        _mm_storeu_si32((uint32_t*)&out[i], packed);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        uint32_t val = A[i];\n        uint8_t bit_count = 0;\n        while (val) {\n            bit_count += val & 1;\n            val >>= 1;\n        }\n        out[i] = bit_count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555); // Mask for alternating bits\n    __m128i mask2 = _mm_set1_epi32(0x33333333); // Mask for pairs of bits\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F); // Mask for nibbles\n    __m128i mask4 = _mm_set1_epi32(0x01010101); // Mask for summing bytes\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Step 1: Count bits in pairs using masks\n        __m128i count = _mm_sub_epi32(vec, _mm_and_si128(_mm_srli_epi32(vec, 1), mask1));\n        count = _mm_add_epi32(_mm_and_si128(count, mask2), _mm_and_si128(_mm_srli_epi32(count, 2), mask2));\n        count = _mm_add_epi32(_mm_and_si128(count, mask3), _mm_srli_epi32(count, 4));\n\n        // Step 2: Sum up the counts in each byte\n        count = _mm_mullo_epi32(count, mask4);\n\n        // Store the result\n        __m128i result = _mm_srli_epi32(count, 24);\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        uint32_t value = A[i];\n        uint8_t bit_count = 0;\n        while (value) {\n            bit_count += value & 1;\n            value >>= 1;\n        }\n        out[i] = bit_count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i lookup = _mm_setr_epi8(0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4);\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 uint32_t elements\n        __m128i low_nibble = _mm_and_si128(vec, _mm_set1_epi8(0x0F)); // Extract low nibble\n        __m128i high_nibble = _mm_and_si128(_mm_srli_epi32(vec, 4), _mm_set1_epi8(0x0F)); // Extract high nibble\n\n        __m128i low_count = _mm_shuffle_epi8(lookup, low_nibble); // Lookup low nibble\n        __m128i high_count = _mm_shuffle_epi8(lookup, high_nibble); // Lookup high nibble\n\n        __m128i sum = _mm_add_epi8(low_count, high_count); // Sum low and high nibble counts\n\n        __m128i sum1 = _mm_add_epi8(sum, _mm_srli_epi32(sum,8))\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555); // Mask for bits 0, 2, 4, ...\n    __m128i mask2 = _mm_set1_epi32(0x33333333); // Mask for bits 0-1, 4-5, ...\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F); // Mask for bits 0-3, 8-11, ...\n    __m128i mask4 = _mm_set1_epi32(0x01010101); // Mask for summing bytes\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 uint32_t elements\n\n        // Count bits in parallel using a series of bitwise operations\n        __m128i count = _mm_sub_epi32(vec, _mm_and_si128(_mm_srli_epi32(vec, 1), mask1));\n        count = _mm_add_epi32(_mm_and_si128(count, mask2), _mm_and_si128(_mm_srli_epi32(count, 2), mask2));\n        count = _mm_and_si128(_mm_add_epi32(count, _mm_srli_epi32(count, 4)), mask3);\n        count = _mm_mullo_epi32(count, mask4);\n        count = _mm_srli_epi32(count, 24); // Extract the sum of bits in each byte\n\n        // Store the result as 8-bit integers\n        __m128i result = _mm_packus_epi16(_mm_packus_epi32(count, _mm_setzero_si128()), _mm_setzero_si128());\n        _mm_storel_epi64((__m128i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        uint8_t bit_count = 0;\n        while (val) {\n            bit_count += val & 1;\n            val >>= 1;\n        }\n        out[i] = bit_count;\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128i three = _mm_set1_epi32(3); // Set all elements to 3\n    __m128i zero = _mm_setzero_si128(); // Set all elements to 0\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i absA = _mm_abs_epi32(vecA); // Compute absolute values of A\n\n        __m128i mod = _mm_rem_epi32(vecA, three); // Compute A % 3\n        __m128i condition = _mm_cmpeq_epi32(mod, zero); // Check if A % 3 == 0\n\n        __m128i result = _mm_blendv_epi8(vecA, absA, condition); // Apply condition\n        _mm_storeu_si128((__m128i*)&B[i], result); // Store result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128i three = _mm_set1_epi32(3); // Vector with all elements set to 3\n    __m128i zero = _mm_setzero_si128(); // Vector with all elements set to 0\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i absA = _mm_abs_epi32(vecA); // Compute absolute values of A\n\n        __m128i mod = _mm_rem_epi32(vecA, three); // Compute A % 3\n        __m128i condition = _mm_cmpeq_epi32(mod, zero); // Check if A % 3 == 0\n\n        __m128i result = _mm_blendv_epi8(vecA, absA, condition); // Select absA if condition is true, otherwise vecA\n        _mm_storeu_si128((__m128i*)&B[i], result); // Store the result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] % 3 == 0) {\n            B[i] = (A[i] < 0) ? -A[i] : A[i];\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128i three = _mm_set1_epi32(3); // Vector with all elements set to 3\n    __m128i zero = _mm_setzero_si128(); // Vector with all elements set to 0\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i absA = _mm_abs_epi32(vecA); // Compute absolute value of A\n\n        __m128i mod = _mm_rem_epi32(vecA, three); // Compute A % 3\n        __m128i condition = _mm_cmpeq_epi32(mod, zero); // Check if A % 3 == 0\n\n        __m128i result = _mm_blendv_epi8(vecA, absA, condition); // Select absA if condition is true, else vecA\n        _mm_storeu_si128((__m128i*)&B[i], result); // Store result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128i three = _mm_set1_epi32(3); // Vector with all elements set to 3\n    __m128i zero = _mm_setzero_si128(); // Vector with all elements set to 0\n\n    for (; i + 4 <= total_elements; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i absA = _mm_abs_epi32(vecA); // Compute absolute values of A\n\n        __m128i rem = _mm_rem_epi32(vecA, three); // Compute A % 3\n        __m128i condition = _mm_cmpeq_epi32(rem, zero); // Check if A % 3 == 0\n\n        __m128i result = _mm_blendv_epi8(vecA, absA, condition); // Select absA if condition is true, else vecA\n        _mm_storeu_si128((__m128i*)&B[i], result); // Store the result in B\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t i = 0;\n\n    __m128i three = _mm_set1_epi32(3); // Vector with all elements set to 3\n    __m128i zero = _mm_setzero_si128(); // Vector with all elements set to 0\n\n    for (; i + 4 <= total_elements; i += 4) {\n        // Load 4 integers from A\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Compute A % 3\n        __m128i vecMod = _mm_sub_epi32(vecA, _mm_mullo_epi32(_mm_div_epi32(vecA, three), three));\n\n        // Compare if A % 3 == 0\n        __m128i mask = _mm_cmpeq_epi32(vecMod, zero);\n\n        // Compute absolute value of A\n        __m128i absA = _mm_abs_epi32(vecA);\n\n        // Select absA if condition is true, otherwise select vecA\n        __m128i result = _mm_blendv_epi8(vecA, absA, mask);\n\n        // Store the result in B\n        _mm_storeu_si128((__m128i*)&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low_4 = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    __m128i all_ones = _mm_set1_epi32(0xF);  // Value to check if low 4 bits are all 1\n    __m128i rotate_mask = _mm_set1_epi32(rotate_bits); // Rotate bits value\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 elements from src\n\n        __m128i low_bits = _mm_and_si128(src_vec, mask_low_4); // Extract low 4 bits\n        __m128i condition = _mm_cmpeq_epi32(low_bits, all_ones); // Check if low 4 bits are all 1\n\n        __m128i rotated_left = _mm_or_si128(\n            _mm_sllv_epi32(src_vec, rotate_mask), // src << rotate_bits\n            _mm_srlv_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_mask)) // src >> (32 - rotate_bits)\n        );\n\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF)); // Bitwise negation of src\n\n        __m128i result = _mm_blendv_epi8(negated, rotated_left, condition); // Select based on condition\n        _mm_storeu_si128((__m128i*)&dst[i], result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_bits = src[i] & 0xF;\n        if (low_bits == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low4 = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    __m128i all_ones = _mm_set1_epi32(0xF); // Value to compare low 4 bits\n    __m128i rotate_mask = _mm_set1_epi32(rotate_bits); // Rotate bits mask\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 elements from src\n\n        // Extract low 4 bits and compare with 0xF\n        __m128i low4_bits = _mm_and_si128(src_vec, mask_low4);\n        __m128i condition = _mm_cmpeq_epi32(low4_bits, all_ones);\n\n        // Perform left rotation\n        __m128i rotated_left = _mm_or_si128(\n            _mm_sllv_epi32(src_vec, rotate_mask),\n            _mm_srlv_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_mask))\n        );\n\n        // Perform bitwise negation\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated_left, condition);\n\n        // Store result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low4 = src[i] & 0xF;\n        if (low4 == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low_4 = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    __m128i all_ones = _mm_set1_epi32(0xF);  // Value to compare low 4 bits\n    __m128i rotate_mask = _mm_set1_epi32(rotate_bits); // Rotate bits mask\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i)); // Load 4 elements from src\n\n        // Extract low 4 bits and compare with all ones\n        __m128i low_4_bits = _mm_and_si128(src_vec, mask_low_4);\n        __m128i condition = _mm_cmpeq_epi32(low_4_bits, all_ones);\n\n        // Perform rotation\n        __m128i rotated_left = _mm_sll_epi32(src_vec, rotate_mask); // Shift left\n        __m128i rotated_right = _mm_srl_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_mask)); // Shift right\n        __m128i rotated = _mm_or_si128(rotated_left, rotated_right); // Combine left and right shifts\n\n        // Perform bitwise negation\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition);\n\n        // Store result in dst\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    // Process remaining elements\n    for (; i < length; i++) {\n        uint32_t low_4 = src[i] & 0xF;\n        if (low_4 == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    __m128i all_ones = _mm_set1_epi32(0xF); // Value to compare for low 4 bits being all 1\n    __m128i rotate_mask = _mm_set1_epi32(rotate_bits); // Rotate bits value\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i)); // Load 4 elements from src\n        __m128i low_bits = _mm_and_si128(src_vec, mask); // Extract low 4 bits\n        __m128i condition = _mm_cmpeq_epi32(low_bits, all_ones); // Check if low 4 bits are all 1\n\n        // Perform rotation\n        __m128i rotated_left = _mm_sll_epi32(src_vec, rotate_mask); // Shift left\n        __m128i rotated_right = _mm_srl_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate_mask)); // Shift right\n        __m128i rotated = _mm_or_si128(rotated_left, rotated_right); // Combine left and right shifts\n\n        // Perform negation\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF)); // Bitwise negation\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition); // Blend based on condition\n\n        _mm_storeu_si128((__m128i*)(dst + i), result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_bits = src[i] & 0xF;\n        if (low_bits == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask_low_4 = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    __m128i all_ones = _mm_set1_epi32(0xF);  // Value to compare for all 1s in low 4 bits\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 elements from src\n        __m128i low_bits = _mm_and_si128(src_vec, mask_low_4); // Extract low 4 bits\n        __m128i condition = _mm_cmpeq_epi32(low_bits, all_ones); // Check if low 4 bits are all 1\n\n        // Perform rotation\n        __m128i rotated_left = _mm_slli_epi32(src_vec, rotate_bits); // Shift left\n        __m128i rotated_right = _mm_srli_epi32(src_vec, 32 - rotate_bits); // Shift right\n        __m128i rotated = _mm_or_si128(rotated_left, rotated_right); // Combine left and right shifts\n\n        // Perform negation\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(0xFFFFFFFF)); // Bitwise negation\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition); // Blend based on condition\n\n        _mm_storeu_si128((__m128i*)&dst[i], result); // Store result in dst\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        uint32_t low_bits = src[i] & 0xF;\n        if (low_bits == 0xF) {\n            dst[i] = (src[i] << rotate_bits) | (src[i] >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps_vec = _mm_set1_ps(fabsf(epsilon)); // Broadcast absolute epsilon to all elements\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);      // Load 4 elements from A\n        __m128 b_vec = _mm_loadu_ps(&B[i]);      // Load 4 elements from B\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec); // A[i] - B[i]\n        __m128 abs_diff_vec = _mm_and_ps(diff_vec, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF))); // abs(A[i] - B[i])\n\n        __m128 gt_eps_mask = _mm_cmpgt_ps(diff_vec, eps_vec); // A[i] - B[i] > epsilon\n        __m128 lt_neg_eps_mask = _mm_cmplt_ps(diff_vec, _mm_sub_ps(_mm_setzero_ps(), eps_vec)); // A[i] - B[i] < -epsilon\n        __m128 within_eps_mask = _mm_cmple_ps(abs_diff_vec, eps_vec); // abs(A[i] - B[i]) <= epsilon\n\n        __m128i result_vec = _mm_setzero_si128(); // Initialize result vector to 0\n        result_vec = _mm_or_si128(result_vec, _mm_castps_si128(_mm_and_ps(gt_eps_mask, _mm_set1_ps(1.0f)))); // Set 1 where gt_eps_mask is true\n        result_vec = _mm_or_si128(result_vec, _mm_castps_si128(_mm_and_ps(lt_neg_eps_mask, _mm_set1_ps(-1.0f)))); // Set -1 where lt_neg_eps_mask is true\n\n        // Store results\n        int32_t temp[4];\n        _mm_storeu_si128((__m128i*)temp, result_vec);\n        for (int j = 0; j < 4; ++j) {\n            out[i + j] = (int8_t)temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 epsilon_vec = _mm_set1_ps(fabsf(epsilon));\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128 neg_one_vec = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_and_ps(diff_vec, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));\n\n        __m128 cmp_gt_vec = _mm_cmpgt_ps(diff_vec, epsilon_vec);\n        __m128 cmp_lt_vec = _mm_cmplt_ps(diff_vec, _mm_sub_ps(zero_vec, epsilon_vec));\n        __m128 cmp_eq_vec = _mm_cmple_ps(abs_diff_vec, epsilon_vec);\n\n        __m128 result_vec = _mm_or_ps(\n            _mm_and_ps(cmp_gt_vec, one_vec),\n            _mm_or_ps(\n                _mm_and_ps(cmp_lt_vec, neg_one_vec),\n                _mm_and_ps(cmp_eq_vec, zero_vec)\n            )\n        );\n\n        __m128i result_int_vec = _mm_cvtps_epi32(result_vec);\n        _mm_storeu_si128((__m128i*)&out[i], result_int_vec);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <math.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 epsilon_vec = _mm_set1_ps(fabsf(epsilon));\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128 neg_one_vec = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff_vec); // Absolute value of diff_vec\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff_vec, epsilon_vec); // A[i] - B[i] > epsilon\n        __m128 cmp_lt = _mm_cmplt_ps(diff_vec, _mm_sub_ps(zero_vec, epsilon_vec)); // A[i] - B[i] < -epsilon\n        __m128 cmp_eq = _mm_cmple_ps(abs_diff_vec, epsilon_vec); // abs(A[i] - B[i]) <= epsilon\n\n        __m128 result_vec = _mm_or_ps(\n            _mm_and_ps(cmp_gt, one_vec),\n            _mm_or_ps(\n                _mm_and_ps(cmp_lt, neg_one_vec),\n                _mm_and_ps(cmp_eq, zero_vec)\n            )\n        );\n\n        // Convert result_vec to integers and store in out\n        __m128i result_int_vec = _mm_cvtps_epi32(result_vec);\n        _mm_storeu_si128((__m128i*)&out[i], result_int_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 epsilon_vec = _mm_set1_ps(fabsf(epsilon));\n    __m128 neg_epsilon_vec = _mm_set1_ps(-fabsf(epsilon));\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff_vec, epsilon_vec);\n        __m128 cmp_lt = _mm_cmplt_ps(diff_vec, neg_epsilon_vec);\n        __m128 cmp_eq = _mm_andnot_ps(_mm_or_ps(cmp_gt, cmp_lt), _mm_set1_ps(1.0f));\n\n        __m128i result_gt = _mm_castps_si128(cmp_gt);\n        __m128i result_lt = _mm_castps_si128(cmp_lt);\n        __m128i result_eq = _mm_castps_si128(cmp_eq);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(result_gt, _mm_set1_epi32(1)),\n                                      _mm_and_si128(result_lt, _mm_set1_epi32(-1)));\n        result = _mm_or_si128(result, _mm_and_si128(result_eq, _mm_set1_epi32(0)));\n\n        int32_t temp[4];\n        _mm_storeu_si128((__m128i*)temp, result);\n\n        for (int j = 0; j < 4; ++j) {\n            out[i + j] = (int8_t)temp[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 epsilon_vec = _mm_set1_ps(fabsf(epsilon));\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128 neg_one_vec = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_and_ps(diff_vec, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF))); // abs(diff)\n\n        __m128 gt_epsilon = _mm_cmpgt_ps(diff_vec, epsilon_vec);\n        __m128 lt_neg_epsilon = _mm_cmplt_ps(diff_vec, _mm_sub_ps(zero_vec, epsilon_vec));\n        __m128 within_epsilon = _mm_cmple_ps(abs_diff_vec, epsilon_vec);\n\n        __m128 result_vec = _mm_or_ps(\n            _mm_and_ps(gt_epsilon, one_vec),\n            _mm_or_ps(\n                _mm_and_ps(lt_neg_epsilon, neg_one_vec),\n                _mm_and_ps(within_epsilon, zero_vec)\n            )\n        );\n\n        __m128i result_int_vec = _mm_cvtps_epi32(result_vec);\n        _mm_storeu_si128((__m128i*)&out[i], result_int_vec);\n    }\n\n    for (; i < length; i++) {\n        float diff = A[i] - B[i];\n        float abs_diff = fabsf(diff);\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_int32 = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_int32 = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 float values from A\n        __m128 values = _mm_loadu_ps(&A[i]);\n\n        // Clamp values to INT32_MIN and INT32_MAX\n        __m128 clamped = _mm_min_ps(_mm_max_ps(values, min_int32), max_int32);\n\n        // Convert clamped float values to int32_t\n        __m128i int_values = _mm_cvtps_epi32(clamped);\n\n        // Store the results in B\n        _mm_storeu_si128((__m128i*)&B[i], int_values);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_int32 = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_int32 = _mm_set1_ps((float)INT32_MIN);\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 values = _mm_loadu_ps(&A[i]);\n        \n        // Clamp values to the range [INT32_MIN, INT32_MAX]\n        values = _mm_min_ps(values, max_int32);\n        values = _mm_max_ps(values, min_int32);\n        \n        // Convert to integers\n        __m128i int_values = _mm_cvtps_epi32(values);\n        \n        // Store the results\n        _mm_storeu_si128((__m128i*)&B[i], int_values);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_int32 = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_int32 = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 src = _mm_loadu_ps(&A[i]);\n\n        // Clamp values to INT32_MIN and INT32_MAX\n        __m128 clamped = _mm_min_ps(_mm_max_ps(src, min_int32), max_int32);\n\n        // Convert to integers\n        __m128i converted = _mm_cvtps_epi32(clamped);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_int32 = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_int32 = _mm_set1_ps((float)INT32_MIN);\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 values = _mm_loadu_ps(&A[i]);\n\n        // Clamp values to INT32_MIN and INT32_MAX\n        values = _mm_min_ps(values, max_int32);\n        values = _mm_max_ps(values, min_int32);\n\n        // Convert to integers\n        __m128i int_values = _mm_cvtps_epi32(values);\n\n        // Store the results\n        _mm_storeu_si128((__m128i*)&B[i], int_values);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_int32 = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_int32 = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 src = _mm_loadu_ps(&A[i]);\n\n        // Clamp values to INT32_MIN and INT32_MAX\n        __m128 clamped = _mm_min_ps(_mm_max_ps(src, min_int32), max_int32);\n\n        // Convert to integers\n        __m128i converted = _mm_cvtps_epi32(clamped);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)&B[i], converted);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale); // Broadcast scale factor to all elements\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 int32_t values\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);          // Convert int32_t to float\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm_storeu_ps(B + i, scaled_vec);                    // Store the result in B\n    }\n\n    // Process remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale); // Broadcast scale factor to all elements\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 int32_t values\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);        // Convert int32_t to float\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm_storeu_ps(&B[i], scaled_vec);                   // Store the result in B\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale); // Broadcast scale factor to all elements of the vector\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 int32_t values from A\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);        // Convert int32_t values to float\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm_storeu_ps(&B[i], scaled_vec);                   // Store the scaled float values in B\n    }\n\n    // Process remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale); // Broadcast scale factor to all elements\n    size_t i = 0;\n\n    // Process in chunks of 4 using SIMD\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 int32_t values\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);          // Convert int32_t to float\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm_storeu_ps(B + i, scaled_vec);                    // Store the result in B\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vec = _mm_set1_ps((float)scale); // Broadcast the scale factor to all elements of an SSE register\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)(A + i)); // Load 4 int32_t values from A\n        __m128 float_vec = _mm_cvtepi32_ps(int_vec);          // Convert int32_t values to float\n        __m128 scaled_vec = _mm_mul_ps(float_vec, scale_vec); // Scale the float values\n        _mm_storeu_ps(B + i, scaled_vec);                    // Store the scaled float values into B\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 floats from A and B\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m128 div = _mm_div_ps(a, b);\n\n        // Round the result to the nearest integer\n        __m128 rounded = _mm_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the rounded floats to 32-bit integers\n        __m128i result = _mm_cvtps_epi32(rounded);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle the remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div > 0 ? 0.5f : -0.5f)); // Manual rounding to nearest integer\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 vecA, vecB, vecDiv, vecRounded;\n    __m128i vecResult;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= size; i += 4) {\n        vecA = _mm_loadu_ps(&A[i]);        // Load 4 floats from A\n        vecB = _mm_loadu_ps(&B[i]);        // Load 4 floats from B\n        vecDiv = _mm_div_ps(vecA, vecB);   // Perform element-wise division\n        vecRounded = _mm_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        vecResult = _mm_cvtps_epi32(vecRounded); // Convert rounded floats to 32-bit integers\n        _mm_storeu_si128((__m128i*)&C[i], vecResult); // Store the result in C\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 vecA, vecB, vecDiv, vecRounded;\n    __m128i vecResult;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        vecA = _mm_loadu_ps(&A[i]);       // Load 4 floats from A\n        vecB = _mm_loadu_ps(&B[i]);       // Load 4 floats from B\n        vecDiv = _mm_div_ps(vecA, vecB);  // Perform element-wise division\n        vecRounded = _mm_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC); // Round to nearest integer\n        vecResult = _mm_cvtps_epi32(vecRounded); // Convert rounded floats to 32-bit integers\n        _mm_storeu_si128((__m128i*)&C[i], vecResult); // Store the result in C\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 4 floats from A and B\n        __m128 vecA = _mm_loadu_ps(&A[i]);\n        __m128 vecB = _mm_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m128 vecDiv = _mm_div_ps(vecA, vecB);\n\n        // Round the result to the nearest integer\n        __m128 vecRounded = _mm_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the rounded floats to 32-bit integers\n        __m128i vecInt = _mm_cvtps_epi32(vecRounded);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i*)&C[i], vecInt);\n    }\n\n    // Process remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div > 0 ? 0.5f : -0.5f)); // Manual rounding\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 4 floats from A and B\n        __m128 vecA = _mm_loadu_ps(&A[i]);\n        __m128 vecB = _mm_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m128 vecDiv = _mm_div_ps(vecA, vecB);\n\n        // Round the result to the nearest integer\n        __m128 vecRounded = _mm_round_ps(vecDiv, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the rounded floats to 32-bit integers\n        __m128i vecInt = _mm_cvtps_epi32(vecRounded);\n\n        // Store the result in the destination array\n        _mm_storeu_si128((__m128i*)&C[i], vecInt);\n    }\n\n    // Process remaining elements (if any) sequentially\n    for (; i < size; i++) {\n        C[i] = (int32_t)(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 max_float = _mm_set1_ps(FLT_MAX);\n    __m128 min_float = _mm_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 2 <= size; i += 2) {\n        __m128d values = _mm_loadu_pd(&A[i]);\n        __m128d clamped = _mm_min_pd(_mm_max_pd(values, min_double), max_double);\n        __m128 floats = _mm_cvtpd_ps(clamped);\n        _mm_storel_pi((__m64*)&B[i], floats);\n    }\n\n    for (; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_double = _mm_set1_pd((double)FLT_MAX);  // Set FLT_MAX as double\n    __m128d min_double = _mm_set1_pd((double)-FLT_MAX); // Set -FLT_MAX as double\n    __m128 max_float = _mm_set1_ps(FLT_MAX);            // Set FLT_MAX as float\n    __m128 min_float = _mm_set1_ps(-FLT_MAX);           // Set -FLT_MAX as float\n\n    size_t i = 0;\n    for (; i + 2 <= size; i += 2) {\n        __m128d values = _mm_loadu_pd(&A[i]);           // Load 2 double values\n        __m128d clamped = _mm_min_pd(_mm_max_pd(values, min_double), max_double); // Clamp values\n        __m128 floats = _mm_cvtpd_ps(clamped);          // Convert to float\n        _mm_storel_pi((__m64*)&B[i], floats);           // Store 2 float values\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 max_float = _mm_set1_ps(FLT_MAX);\n    __m128 min_float = _mm_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 2 <= size; i += 2) {\n        __m128d values = _mm_loadu_pd(&A[i]);\n        __m128d clamped = _mm_min_pd(_mm_max_pd(values, min_double), max_double);\n        __m128 floats = _mm_cvtpd_ps(clamped);\n        _mm_storel_pi((__m64*)&B[i], floats);\n    }\n\n    for (; i < size; i++) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 max_float = _mm_set1_ps(FLT_MAX);\n    __m128 min_float = _mm_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n\n    // Process 2 doubles at a time\n    for (; i + 2 <= size; i += 2) {\n        __m128d src = _mm_loadu_pd(&A[i]);\n        __m128d clamped = _mm_min_pd(_mm_max_pd(src, min_double), max_double);\n        __m128 converted = _mm_cvtpd_ps(clamped);\n        _mm_storel_pi((__m64*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 max_float = _mm_set1_ps(FLT_MAX);\n    __m128 min_float = _mm_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n\n    // Process 2 doubles at a time\n    for (; i + 2 <= size; i += 2) {\n        __m128d src = _mm_loadu_pd(&A[i]);\n\n        // Clamp values to the range [-FLT_MAX, FLT_MAX]\n        __m128d clamped = _mm_min_pd(_mm_max_pd(src, min_double), max_double);\n\n        // Convert to float\n        __m128 float_vals = _mm_cvtpd_ps(clamped);\n\n        // Store the result\n        _mm_storel_pi((__m64*)&B[i], float_vals);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 int16_t elements at a time (128 bits / 16 bits = 8 elements)\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 int16_t values from A into an SSE register\n        __m128i src = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Unpack the lower 4 int16_t values to 4 int32_t values\n        __m128i lower = _mm_unpacklo_epi16(src, _mm_setzero_si128());\n\n        // Unpack the upper 4 int16_t values to 4 int32_t values\n        __m128i upper = _mm_unpackhi_epi16(src, _mm_setzero_si128());\n\n        // Store the converted int32_t values into B\n        _mm_storeu_si128((__m128i*)&B[i], lower);\n        _mm_storeu_si128((__m128i*)&B[i + 4], upper);\n    }\n\n    // Handle the remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 int16_t elements at a time (since 128-bit registers can hold 8 int16_t values)\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 int16_t values from A into an SSE register\n        __m128i src = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Unpack the lower 4 int16_t values to 4 int32_t values\n        __m128i lower = _mm_unpacklo_epi16(src, _mm_setzero_si128());\n\n        // Unpack the upper 4 int16_t values to 4 int32_t values\n        __m128i upper = _mm_unpackhi_epi16(src, _mm_setzero_si128());\n\n        // Store the 4 lower int32_t values into B\n        _mm_storeu_si128((__m128i*)&B[i], lower);\n\n        // Store the 4 upper int32_t values into B\n        _mm_storeu_si128((__m128i*)&B[i + 4], upper);\n    }\n\n    // Handle the remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 int16_t elements at a time (since 128-bit registers can hold 8 int16_t values)\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 int16_t values from A into an SSE register\n        __m128i src = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Unpack the lower 4 int16_t values to 4 int32_t values\n        __m128i dst_low = _mm_unpacklo_epi16(src, _mm_setzero_si128());\n\n        // Unpack the upper 4 int16_t values to 4 int32_t values\n        __m128i dst_high = _mm_unpackhi_epi16(src, _mm_setzero_si128());\n\n        // Store the 4 int32_t values from dst_low into B\n        _mm_storeu_si128((__m128i*)&B[i], dst_low);\n\n        // Store the 4 int32_t values from dst_high into B\n        _mm_storeu_si128((__m128i*)&B[i + 4], dst_high);\n    }\n\n    // Handle the remaining elements (if size is not a multiple of 8)\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 int16_t values from A into an SSE register\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Convert the lower 4 int16_t values to int32_t\n        __m128i vecB_low = _mm_cvtepi16_epi32(vecA);\n\n        // Convert the upper 4 int16_t values to int32_t\n        __m128i vecB_high = _mm_cvtepi16_epi32(_mm_srli_si128(vecA, 8));\n\n        // Store the converted int32_t values into B\n        _mm_storeu_si128((__m128i*)&B[i], vecB_low);\n        _mm_storeu_si128((__m128i*)&B[i + 4], vecB_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using SSE2\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 int16_t elements from A into a 128-bit register\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Unpack the lower 4 int16_t elements to 32-bit integers\n        __m128i vecB_low = _mm_unpacklo_epi16(vecA, _mm_setzero_si128());\n\n        // Unpack the upper 4 int16_t elements to 32-bit integers\n        __m128i vecB_high = _mm_unpackhi_epi16(vecA, _mm_setzero_si128());\n\n        // Store the results into B\n        _mm_storeu_si128((__m128i*)(B + i), vecB_low);\n        _mm_storeu_si128((__m128i*)(B + i + 4), vecB_high);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128i control_vec = _mm_loadu_si128((__m128i*)&control[i]);\n\n        // Compare control values to 0\n        __m128 mask = _mm_castsi128_ps(_mm_cmpgt_epi32(control_vec, _mm_setzero_si128()));\n\n        // Normalize values\n        __m128 normalized_vec = _mm_sub_ps(a_vec, min_val_vec);\n        normalized_vec = _mm_div_ps(normalized_vec, range_vec);\n        normalized_vec = _mm_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm_min_ps(normalized_vec, one_vec);\n\n        // Select normalized or original values based on control mask\n        __m128 result_vec = _mm_or_ps(_mm_and_ps(mask, normalized_vec), _mm_andnot_ps(mask, a_vec));\n\n        // Store results in B\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i;\n    for (i = 0; i + 4 <= size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 control_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n\n        // Normalize values\n        __m128 normalized_vec = _mm_sub_ps(a_vec, min_val_vec);\n        normalized_vec = _mm_div_ps(normalized_vec, range_vec);\n        normalized_vec = _mm_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm_min_ps(normalized_vec, one_vec);\n\n        // Mask based on control values\n        __m128 mask_vec = _mm_cmpgt_ps(control_vec, zero_vec);\n        __m128 result_vec = _mm_or_ps(_mm_and_ps(mask_vec, normalized_vec), _mm_andnot_ps(mask_vec, a_vec));\n\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128i control_vec = _mm_loadu_si128((__m128i*)&control[i]);\n\n        // Normalize values\n        __m128 normalized_vec = _mm_sub_ps(a_vec, min_val_vec);\n        normalized_vec = _mm_div_ps(normalized_vec, range_vec);\n        normalized_vec = _mm_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm_min_ps(normalized_vec, one_vec);\n\n        // Compare control > 0\n        __m128i mask_vec = _mm_cmpgt_epi32(control_vec, _mm_setzero_si128());\n        __m128 mask_ps_vec = _mm_castsi128_ps(mask_vec);\n\n        // Select normalized or original values based on control\n        __m128 result_vec = _mm_or_ps(_mm_and_ps(mask_ps_vec, normalized_vec), _mm_andnot_ps(mask_ps_vec, a_vec));\n\n        // Store result\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Process remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128i control_vec = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128 mask_vec = _mm_castsi128_ps(_mm_cmpgt_epi32(control_vec, _mm_setzero_si128()));\n\n        __m128 normalized_vec = _mm_div_ps(_mm_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized_vec = _mm_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm_min_ps(normalized_vec, one_vec);\n\n        __m128 result_vec = _mm_or_ps(_mm_and_ps(mask_vec, normalized_vec), _mm_andnot_ps(mask_vec, a_vec));\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128i control_vec = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128 mask_vec = _mm_castsi128_ps(_mm_cmpgt_epi32(control_vec, _mm_setzero_si128()));\n\n        __m128 normalized_vec = _mm_div_ps(_mm_sub_ps(a_vec, min_val_vec), range_vec);\n        normalized_vec = _mm_max_ps(normalized_vec, zero_vec);\n        normalized_vec = _mm_min_ps(normalized_vec, one_vec);\n\n        __m128 result_vec = _mm_or_ps(\n            _mm_and_ps(mask_vec, normalized_vec),\n            _mm_andnot_ps(mask_vec, a_vec)\n        );\n\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            if (val < 0.0f) val = 0.0f;\n            if (val > 1.0f) val = 1.0f;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 elements at a time\n\n    // Process in chunks of 4 using SSE\n    for (; i + simd_width <= size; i += simd_width) {\n        // Load 4 uint32_t values from A\n        __m128i vec_int = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 bits to extract the high 16 bits\n        __m128i vec_high16 = _mm_srli_epi32(vec_int, 16);\n\n        // Convert the high 16 bits to float\n        __m128 vec_float = _mm_cvtepi32_ps(vec_high16);\n\n        // Compute the square root of the float values\n        __m128 vec_sqrt = _mm_sqrt_ps(vec_float);\n\n        // Store the result in B\n        _mm_storeu_ps(B + i, vec_sqrt);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16; // Extract high 16 bits\n        float value = (float)high16; // Convert to float\n        B[i] = sqrtf(value);         // Compute square root and store\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A\n        __m128i values = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Shift right by 16 bits to extract the high 16 bits\n        __m128i high_bits = _mm_srli_epi32(values, 16);\n\n        // Convert the high 16 bits to float\n        __m128 float_values = _mm_cvtepi32_ps(high_bits);\n\n        // Compute the square root of the float values\n        __m128 sqrt_values = _mm_sqrt_ps(float_values);\n\n        // Store the results in B\n        _mm_storeu_ps(&B[i], sqrt_values);\n    }\n\n    // Process remaining elements (if size is not a multiple of 4)\n    for (; i < size; ++i) {\n        uint32_t high_bits = A[i] >> 16;\n        float float_value = (float)high_bits;\n        B[i] = sqrtf(float_value);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t\n        __m128i highBits = _mm_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m128 vecFloat = _mm_cvtepi32_ps(highBits);\n\n        // Compute the square root of the float values\n        __m128 vecSqrt = _mm_sqrt_ps(vecFloat);\n\n        // Store the result in B\n        _mm_storeu_ps(&B[i], vecSqrt);\n    }\n\n    // Process remaining elements one by one\n    for (; i < size; i++) {\n        uint32_t highBits = A[i] >> 16;\n        float value = (float)highBits;\n        B[i] = sqrtf(value);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A\n        __m128i input = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Shift right by 16 bits to extract the high 16 bits\n        __m128i high16 = _mm_srli_epi32(input, 16);\n\n        // Convert the high 16 bits to float\n        __m128 float_values = _mm_cvtepi32_ps(high16);\n\n        // Compute the square root of the float values\n        __m128 sqrt_values = _mm_sqrt_ps(float_values);\n\n        // Store the results in B\n        _mm_storeu_ps(&B[i], sqrt_values);\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        uint32_t high16 = A[i] >> 16;\n        float float_value = (float)high16;\n        B[i] = sqrtf(float_value);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 bits to extract the high 16 bits\n        __m128i high16 = _mm_srli_epi32(vecA, 16);\n\n        // Convert the high 16 bits to float\n        __m128 vecFloat = _mm_cvtepi32_ps(high16);\n\n        // Compute the square root of the float values\n        __m128 vecSqrt = _mm_sqrt_ps(vecFloat);\n\n        // Store the result in B\n        _mm_storeu_ps(B + i, vecSqrt);\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; ++i) {\n        uint32_t high16 = A[i] >> 16;\n        float value = (float)high16;\n        B[i] = sqrtf(value);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A into an SSE register\n        __m128i int_values = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Cast the integer values to floating-point values without conversion\n        __m128 float_values = _mm_castsi128_ps(int_values);\n\n        // Store the resulting float values into B\n        _mm_storeu_ps(B + i, float_values);\n    }\n\n    // Handle remaining elements (if any) sequentially\n    for (; i < size; i++) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A into an SSE register\n        __m128i int_vec = _mm_loadu_si128((__m128i*)(A + i));\n        \n        // Cast the integer vector to a floating-point vector without conversion\n        __m128 float_vec = _mm_castsi128_ps(int_vec);\n        \n        // Store the resulting float vector into B\n        _mm_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        B[i] = *((float*)&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SSE\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A\n        __m128i int_values = _mm_loadu_si128((__m128i*)&A[i]);\n        \n        // Cast the integer values to float values (reinterpret the bits)\n        __m128 float_values = _mm_castsi128_ps(int_values);\n        \n        // Store the float values into B\n        _mm_storeu_ps(&B[i], float_values);\n    }\n\n    // Handle the remaining elements (if any)\n    for (; i < size; ++i) {\n        // Reinterpret the bits manually for the remaining elements\n        B[i] = *((float*)&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 uint32_t values from A into an SSE register\n        __m128i int_values = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Cast the integer values to float values by reinterpreting the bits\n        __m128 float_values = _mm_castsi128_ps(int_values);\n\n        // Store the resulting float values into B\n        _mm_storeu_ps(B + i, float_values);\n    }\n\n    // Process remaining elements (if size is not a multiple of 4)\n    for (; i < size; i++) {\n        // Reinterpret the bits directly for the remaining elements\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t simd_width = 4; // SSE processes 4 elements at a time\n\n    // Process in chunks of 4 using SIMD\n    for (; i + simd_width <= size; i += simd_width) {\n        __m128i int_vec = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 uint32_t values\n        __m128 float_vec = _mm_castsi128_ps(int_vec);       // Reinterpret as float\n        _mm_storeu_ps(&B[i], float_vec);                   // Store the resulting floats\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i]; // Type-punning for remaining elements\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    size_t i = 1; // Start from the second element\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 elements from A[i] and A[i-1]\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        // Compute the difference (A[i] - A[i-1])\n        __m128i diff_int = _mm_sub_epi32(current, previous);\n\n        // Convert the difference to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Load 4 boolean values from cond[i]\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Create a mask for the condition (true -> 0xFFFFFFFF, false -> 0x00000000)\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1)));\n\n        // Apply the mask: diff[i] = diff_float if cond[i] is true, else 0.0f\n        __m128 result = _mm_and_ps(mask, diff_float);\n\n        // Store the result in the diff array\n        _mm_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1; // Start from the second element\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 consecutive elements from A\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        // Compute differences (current - previous)\n        __m128i diff_int = _mm_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Load 4 consecutive condition values\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Create a mask for the condition (convert bool to 0xFFFFFFFF or 0x00000000)\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1)));\n\n        // Apply mask: set diff[i] to 0.0f where cond[i] is false\n        diff_float = _mm_and_ps(mask, diff_float);\n\n        // Store the result in the diff array\n        _mm_storeu_ps(&diff[i], diff_float);\n    }\n\n    // Handle remaining elements (tail loop)\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1; // Start from the second element\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 consecutive integers from A\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        // Compute differences (current - previous)\n        __m128i diff_int = _mm_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Load 4 consecutive boolean conditions\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Create a mask for conditions (true -> 0xFFFFFFFF, false -> 0x00000000)\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1)));\n\n        // Apply mask: set diff[i] to 0.0f if cond[i] is false\n        __m128 result = _mm_and_ps(mask, diff_float);\n\n        // Store the result in the destination array\n        _mm_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 consecutive integers from A\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        // Compute differences (current - previous)\n        __m128i diff_int = _mm_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Load 4 consecutive boolean conditions\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Create a mask for conditions (convert bool to 0xFFFFFFFF or 0x00000000)\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1)));\n\n        // Apply mask: set diff_float to 0.0f where cond is false\n        __m128 result = _mm_and_ps(mask, diff_float);\n\n        // Store the result in the destination array\n        _mm_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f; // First element is always 0.0f\n\n    size_t i = 1;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 consecutive integers from A\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i previous = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        // Compute differences (current - previous)\n        __m128i diff_int = _mm_sub_epi32(current, previous);\n\n        // Convert differences to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Load 4 consecutive boolean conditions\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Create a mask for conditions (convert boolean to 0xFFFFFFFF or 0x00000000)\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(1)));\n\n        // Apply mask: set diff[i] to 0.0f if cond[i] is false\n        __m128 result = _mm_and_ps(mask, diff_float);\n\n        // Store the result in the diff array\n        _mm_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 32-bit integers from src\n        __m128i src_vals = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Unpack the lower 32 bits to 64 bits\n        __m128i lower = _mm_unpacklo_epi32(src_vals, _mm_set1_epi32(-1));\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], lower);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i] | 0xFFFFFFFF00000000ULL);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 32-bit integers from src\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i));\n\n        // Extract the lower 32 bits and widen to 64 bits\n        __m128i low = _mm_unpacklo_epi32(src_vec, _mm_set1_epi32(-1));\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)(dst + i), low);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 32-bit integers from src\n        __m128i src_vals = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Unpack lower 32 bits to 64 bits\n        __m128i lower = _mm_unpacklo_epi32(src_vals, _mm_set1_epi32(-1));\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], lower);\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 32-bit integers from src\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Extract the lower 32 bits and convert to 64-bit integers\n        __m128i low_32 = _mm_unpacklo_epi32(src_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], low_32);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 2 <= length; i += 2) {\n        // Load 2 32-bit integers from src\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i));\n\n        // Convert 32-bit integers to 64-bit integers\n        __m128i low_64 = _mm_unpacklo_epi32(src_vec, _mm_set1_epi32(-1)); // Set high 32 bits to 1\n        __m128i high_64 = _mm_unpackhi_epi32(src_vec, _mm_set1_epi32(-1)); // Set high 32 bits to 1\n\n        // Store the results in dst\n        _mm_storeu_si128((__m128i*)(dst + i), low_64);\n        _mm_storeu_si128((__m128i*)(dst + i + 2), high_64);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL; // Set high 32 bits to 1\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd(); // Initialize sum vector to zero\n    size_t i = 0;\n\n    // Process two indices at a time using SIMD\n    for (; i + 1 < length; i += 2) {\n        // Load two indices\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Extract the indices as 16-bit integers\n        int idx0 = _mm_extract_epi16(indices, 0);\n        int idx1 = _mm_extract_epi16(indices, 1);\n\n        // Load the corresponding values from vec\n        __m128d values = _mm_set_pd(vec[idx1], vec[idx0]);\n\n        // Accumulate the values into the sum vector\n        sum = _mm_add_pd(sum, values);\n    }\n\n    // Horizontal sum of the sum vector\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    double total = result[0] + result[1];\n\n    // Handle remaining elements (if length is odd)\n    for (; i < length; ++i) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd(); // Initialize sum vector to zero\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        // Load two indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Extract the indices as 16-bit integers\n        int idx0 = _mm_extract_epi16(indices, 0);\n        int idx1 = _mm_extract_epi16(indices, 1);\n\n        // Load the corresponding elements from the vec array\n        __m128d values = _mm_set_pd(vec[idx1], vec[idx0]);\n\n        // Accumulate the values into the sum vector\n        sum = _mm_add_pd(sum, values);\n    }\n\n    // Horizontal sum of the sum vector\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    double total_sum = result[0] + result[1];\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd(); // Initialize the sum vector to zero\n\n    size_t i = 0;\n    for (; i + 2 <= length; i += 2) {\n        // Load two indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Extract the indices as 16-bit integers\n        int idx0 = _mm_extract_epi16(indices, 0);\n        int idx1 = _mm_extract_epi16(indices, 1);\n\n        // Load the corresponding values from the vec array\n        __m128d values = _mm_set_pd(vec[idx1], vec[idx0]);\n\n        // Accumulate the values into the sum vector\n        sum = _mm_add_pd(sum, values);\n    }\n\n    // Horizontal sum of the sum vector\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    double total = result[0] + result[1];\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd(); // Initialize SIMD register for summation\n    size_t i = 0;\n\n    // Process indices in chunks of 2 (since __m128d can hold 2 doubles)\n    for (; i + 2 <= length; i += 2) {\n        // Load two indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)&index[i]);\n\n        // Extract the indices as 16-bit integers\n        int idx0 = _mm_extract_epi16(indices, 0);\n        int idx1 = _mm_extract_epi16(indices, 1);\n\n        // Load the corresponding values from the vec array\n        __m128d values = _mm_set_pd(vec[idx1], vec[idx0]);\n\n        // Accumulate the values into the sum register\n        sum = _mm_add_pd(sum, values);\n    }\n\n    // Horizontal sum of the SIMD register\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    double total = result[0] + result[1];\n\n    // Handle remaining elements (if length is not divisible by 2)\n    for (; i < length; i++) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd(); // Initialize the sum to zero\n    size_t i = 0;\n\n    // Process in chunks of 2 indices using SIMD\n    for (; i + 1 < length; i += 2) {\n        // Load two indices from the index array\n        __m128i indices = _mm_loadu_si128((__m128i*)(index + i));\n\n        // Extract the individual indices\n        int idx0 = _mm_extract_epi16(indices, 0);\n        int idx1 = _mm_extract_epi16(indices, 1);\n\n        // Load the corresponding values from the vec array\n        __m128d values = _mm_set_pd(vec[idx1], vec[idx0]);\n\n        // Accumulate the values into the sum\n        sum = _mm_add_pd(sum, values);\n    }\n\n    // Horizontal add to sum up the two elements in the SIMD register\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    double total_sum = result[0] + result[1];\n\n    // Handle the remaining elements (if length is odd)\n    for (; i < length; i++) {\n        int idx = index[i];\n        total_sum += vec[idx];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx1 = (i + ki) * input_size + (j + kj);\n                    size_t input_idx2 = (i + ki) * input_size + (j + kj + 1);\n                    size_t kernel_idx1 = ki * kernel_size + kj;\n                    size_t kernel_idx2 = ki * kernel_size + kj + 1;\n\n                    __m128d input_vals = _mm_set_pd(input[input_idx2], input[input_idx1]);\n                    __m128d kernel_vals = _mm_set_pd(kernel[kernel_idx2], kernel[kernel_idx1]);\n                    __m128d mul = _mm_mul_pd(input_vals, kernel_vals);\n\n                    sum = _mm_add_pd(sum, mul);\n                }\n            }\n\n            double partial_sum[2];\n            _mm_storeu_pd(partial_sum, sum);\n            double total_sum = partial_sum[0] + partial_sum[1];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = total_sum > 0 ? total_sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx1 = (i + ki) * input_size + (j + kj);\n                    size_t input_idx2 = input_idx1 + 1;\n\n                    size_t kernel_idx1 = ki * kernel_size + kj;\n                    size_t kernel_idx2 = kernel_idx1 + 1;\n\n                    __m128d input_vals = _mm_set_pd(input[input_idx2], input[input_idx1]);\n                    __m128d kernel_vals = _mm_set_pd(kernel[kernel_idx2], kernel[kernel_idx1]);\n\n                    __m128d product = _mm_mul_pd(input_vals, kernel_vals);\n                    sum = _mm_add_pd(sum, product);\n                }\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double conv_result = temp[0] + temp[1];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = conv_result > 0.0 ? conv_result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx1 = (i + ki) * input_size + (j + kj);\n                    size_t input_idx2 = input_idx1 + 1;\n\n                    size_t kernel_idx1 = ki * kernel_size + kj;\n                    size_t kernel_idx2 = kernel_idx1 + 1;\n\n                    __m128d input_vec = _mm_set_pd(input[input_idx2], input[input_idx1]);\n                    __m128d kernel_vec = _mm_set_pd(kernel[kernel_idx2], kernel[kernel_idx1]);\n\n                    __m128d prod = _mm_mul_pd(input_vec, kernel_vec);\n                    sum = _mm_add_pd(sum, prod);\n                }\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double conv_result = temp[0] + temp[1];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = conv_result > 0.0 ? conv_result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m128d input_vec = _mm_loadu_pd(&input[input_idx]);\n                    __m128d kernel_vec = _mm_loadu_pd(&kernel[kernel_idx]);\n                    __m128d mul = _mm_mul_pd(input_vec, kernel_vec);\n\n                    sum = _mm_add_pd(sum, mul);\n                }\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = result > 0.0 ? result : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n\n    for (size_t i = 0; i < output_size; i++) {\n        for (size_t j = 0; j < output_size; j++) {\n            __m128d sum = _mm_setzero_pd(); // Initialize sum to zero\n\n            for (size_t ki = 0; ki < kernel_size; ki++) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    size_t input_idx1 = (i + ki) * input_size + (j + kj);\n                    size_t input_idx2 = input_idx1 + 1;\n\n                    size_t kernel_idx1 = ki * kernel_size + kj;\n                    size_t kernel_idx2 = kernel_idx1 + 1;\n\n                    __m128d input_vals = _mm_set_pd(input[input_idx2], input[input_idx1]);\n                    __m128d kernel_vals = _mm_set_pd(kernel[kernel_idx2], kernel[kernel_idx1]);\n\n                    __m128d product = _mm_mul_pd(input_vals, kernel_vals);\n                    sum = _mm_add_pd(sum, product);\n                }\n            }\n\n            // Horizontal add to sum the two values in the SSE register\n            double partial_sum[2];\n            _mm_storeu_pd(partial_sum, sum);\n            double final_sum = partial_sum[0] + partial_sum[1];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = final_sum > 0.0 ? final_sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return false;\n    }\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m128d current = _mm_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 2) {\n            __m128d next_values;\n\n            if (j + 1 < n) {\n                next_values = _mm_set_pd(numbers[j + 1], numbers[j]);\n            } else {\n                next_values = _mm_set_pd(0.0, numbers[j]); // Handle odd-sized vector\n            }\n\n            __m128d diff = _mm_sub_pd(current, next_values);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <vector>\n#include <cmath>\n#include <emmintrin.h>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        __m128d current = _mm_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j + 1 < numbers.size(); j += 2) {\n            __m128d next_vals = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next_vals);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        // Handle the remaining element if the size is odd\n        if ((numbers.size() - i - 1) % 2 != 0) {\n            double diff = std::abs(numbers[i] - numbers.back());\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m128d num_i = _mm_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 2) {\n            __m128d num_j = _mm_loadu_pd(&numbers[j]);\n\n            __m128d diff = _mm_sub_pd(num_i, num_j);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n            int mask = _mm_movemask_pd(cmp);\n\n            if (mask != 0) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m128d current = _mm_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j < size; j += 2) {\n            __m128d next_values;\n            if (j + 1 < size) {\n                next_values = _mm_loadu_pd(&numbers[j]);\n            } else {\n                double temp[2] = {numbers[j], std::numeric_limits<double>::max()};\n                next_values = _mm_loadu_pd(temp);\n            }\n\n            __m128d diff = _mm_sub_pd(current, next_values);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    if (numbers.size() < 2) {\n        return false;\n    }\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        __m128d current = _mm_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j + 1 < numbers.size(); j += 2) {\n            __m128d next_vals = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(current, next_vals);\n            __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        // Handle remaining element if the size is odd\n        if (numbers.size() % 2 == 1 && i < numbers.size() - 1) {\n            double diff = std::abs(numbers[i] - numbers.back());\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int size = operations.size();\n    __m128i balance = _mm_setzero_si128(); // Initialize balance to zero\n    __m128i zero = _mm_setzero_si128();    // Zero vector for comparison\n\n    int i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 integers into an SSE register\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        // Add the operations to the balance\n        balance = _mm_add_epi32(balance, ops);\n        // Compare the balance with zero\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        // Check if any element in cmp is non-zero (balance < 0)\n        if (_mm_movemask_epi8(cmp)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    int remaining_balance = 0;\n    for (; i < size; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int n = operations.size();\n    __m128i balance = _mm_setzero_si128(); // Initialize balance to zero\n    __m128i zero = _mm_setzero_si128();    // Zero vector for comparison\n\n    for (int i = 0; i < n; i += 4) {\n        // Load the next 4 integers into an SSE register\n        __m128i ops = _mm_set_epi32(\n            i + 3 < n ? operations[i + 3] : 0,\n            i + 2 < n ? operations[i + 2] : 0,\n            i + 1 < n ? operations[i + 1] : 0,\n            operations[i]\n        );\n\n        // Update the balance\n        balance = _mm_add_epi32(balance, ops);\n\n        // Check if any element in the balance is below zero\n        __m128i mask = _mm_cmplt_epi32(balance, zero);\n        if (_mm_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int n = operations.size();\n    __m128i balance = _mm_setzero_si128(); // Initialize balance to zero\n    __m128i zero = _mm_setzero_si128();    // Zero vector for comparison\n\n    // Process operations in chunks of 4 integers\n    for (int i = 0; i < n; i += 4) {\n        // Load next 4 integers into an SSE register\n        __m128i ops = _mm_set_epi32(\n            i + 3 < n ? operations[i + 3] : 0,\n            i + 2 < n ? operations[i + 2] : 0,\n            i + 1 < n ? operations[i + 1] : 0,\n            i < n ? operations[i] : 0\n        );\n\n        // Update balance\n        balance = _mm_add_epi32(balance, ops);\n\n        // Check if any element in the balance is below zero\n        __m128i mask = _mm_cmplt_epi32(balance, zero);\n        if (_mm_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int n = operations.size();\n    __m128i balance = _mm_setzero_si128(); // Initialize balance to zero\n    __m128i zero = _mm_setzero_si128();    // Zero vector for comparison\n\n    for (int i = 0; i < n; i += 4) {\n        // Load the next 4 operations into an SSE register\n        __m128i ops = _mm_set_epi32(\n            i + 3 < n ? operations[i + 3] : 0,\n            i + 2 < n ? operations[i + 2] : 0,\n            i + 1 < n ? operations[i + 1] : 0,\n            operations[i]\n        );\n\n        // Update the balance\n        balance = _mm_add_epi32(balance, ops);\n\n        // Check if any element in the balance vector is less than zero\n        __m128i mask = _mm_cmplt_epi32(balance, zero);\n        if (_mm_movemask_epi8(mask)) {\n            return true; // If any element is below zero, return true\n        }\n    }\n\n    return false; // If no element is below zero, return false\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    __m128i balance = _mm_setzero_si128(); // Initialize balance to zero\n    __m128i zero = _mm_setzero_si128();    // Zero vector for comparison\n\n    int i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 integers from the operations vector\n        __m128i ops = _mm_loadu_si128((__m128i*)&operations[i]);\n        // Update balance\n        balance = _mm_add_epi32(balance, ops);\n        // Check if any value in the balance vector is less than zero\n        __m128i mask = _mm_cmplt_epi32(balance, zero);\n        if (_mm_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    int scalar_balance = 0;\n    for (; i < size; ++i) {\n        scalar_balance += operations[i];\n        if (scalar_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate mean\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / n;\n\n    // Prepare SIMD variables\n    __m128d mean_vec = _mm_set1_pd(mean); // Broadcast mean to all lanes\n    __m128d abs_sum_vec = _mm_setzero_pd(); // Accumulator for absolute differences\n\n    size_t i = 0;\n    // Process in chunks of 2 doubles using SIMD\n    for (; i + 1 < n; i += 2) {\n        __m128d data_vec = _mm_loadu_pd(&numbers[i]); // Load 2 doubles\n        __m128d diff_vec = _mm_sub_pd(data_vec, mean_vec); // Subtract mean\n        __m128d abs_diff_vec = _mm_abs_pd(diff_vec); // Take absolute value\n        abs_sum_vec = _mm_add_pd(abs_sum_vec, abs_diff_vec); // Accumulate\n    }\n\n    // Horizontal sum of abs_sum_vec\n    double abs_sum[2];\n    _mm_storeu_pd(abs_sum, abs_sum_vec);\n    double total_abs_sum = abs_sum[0] + abs_sum[1];\n\n    // Process remaining elements (if n is odd)\n    for (; i < n; ++i) {\n        total_abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Calculate Mean Absolute Deviation\n    return total_abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <numeric>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the mean\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / n;\n\n    // Prepare SIMD variables\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d abs_sum_vec = _mm_setzero_pd();\n\n    size_t i = 0;\n    // Process in chunks of 2 doubles using SSE2\n    for (; i + 1 < n; i += 2) {\n        __m128d data_vec = _mm_loadu_pd(&numbers[i]); // Load 2 doubles\n        __m128d diff_vec = _mm_sub_pd(data_vec, mean_vec); // Calculate difference\n        __m128d abs_diff_vec = _mm_abs_pd(diff_vec); // Absolute difference\n        abs_sum_vec = _mm_add_pd(abs_sum_vec, abs_diff_vec); // Accumulate absolute differences\n    }\n\n    // Horizontal add to sum up the values in abs_sum_vec\n    double abs_sum[2];\n    _mm_storeu_pd(abs_sum, abs_sum_vec);\n    double total_abs_sum = abs_sum[0] + abs_sum[1];\n\n    // Process remaining elements (if n is odd)\n    for (; i < n; ++i) {\n        total_abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Calculate Mean Absolute Deviation\n    return total_abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Compute the mean using SIMD\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i = 0;\n    for (; i + 2 <= n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double sum[2];\n    _mm_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Compute Mean Absolute Deviation using SIMD\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_vec = _mm_setzero_pd();\n    i = 0;\n    for (; i + 2 <= n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(vec, mean_vec);\n        __m128d abs_diff = _mm_max_pd(diff, _mm_sub_pd(_mm_setzero_pd(), diff));\n        mad_vec = _mm_add_pd(mad_vec, abs_diff);\n    }\n\n    double mad[2];\n    _mm_storeu_pd(mad, mad_vec);\n    double total_mad = mad[0] + mad[1];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        total_mad += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Compute the mean\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, vec);\n    }\n\n    double sum_array[2];\n    _mm_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1];\n\n    // Handle remaining element if n is odd\n    if (i < n) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Step 2: Compute the Mean Absolute Deviation\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_vec = _mm_setzero_pd();\n\n    for (i = 0; i + 1 < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(vec, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff); // Absolute value\n        mad_vec = _mm_add_pd(mad_vec, abs_diff);\n    }\n\n    _mm_storeu_pd(sum_array, mad_vec);\n    double mad_sum = sum_array[0] + sum_array[1];\n\n    // Handle remaining element if n is odd\n    if (i < n) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Step 1: Calculate the mean\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 1 < n; i += 2) {\n        __m128d num_vec = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, num_vec);\n    }\n\n    double sum_array[2];\n    _mm_storeu_pd(sum_array, sum_vec);\n    double sum = sum_array[0] + sum_array[1];\n\n    // Handle remaining element if n is odd\n    if (i < n) {\n        sum += numbers[i];\n    }\n\n    double mean = sum / n;\n\n    // Step 2: Calculate the Mean Absolute Deviation (MAD)\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_sum_vec = _mm_setzero_pd();\n\n    for (i = 0; i + 1 < n; i += 2) {\n        __m128d num_vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_vec = _mm_sub_pd(num_vec, mean_vec);\n        __m128d abs_diff_vec = _mm_andnot_pd(_mm_set1_pd(-0.0), diff_vec); // Absolute value\n        mad_sum_vec = _mm_add_pd(mad_sum_vec, abs_diff_vec);\n    }\n\n    _mm_storeu_pd(sum_array, mad_sum_vec);\n    double mad_sum = sum_array[0] + sum_array[1];\n\n    // Handle remaining element if n is odd\n    if (i < n) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= n; i += 4, j += 8) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n\n        // Interleave input_vec and delimiter_vec\n        __m128i low = _mm_unpacklo_epi32(input_vec, delimiter_vec);\n        __m128i high = _mm_unpackhi_epi32(input_vec, delimiter_vec);\n\n        // Store interleaved results\n        _mm_storeu_si128((__m128i*)&result[j], low);\n        _mm_storeu_si128((__m128i*)&result[j + 4], high);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        if (j + 1 < result_size) {\n            result[j + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {};\n    }\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m128i delimeter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= n; i += 4, j += 8) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n\n        // Store the original numbers in the even indices of the result\n        _mm_storeu_si128((__m128i*)&result[j], input_vec);\n\n        // Store the delimiter in the odd indices of the result\n        _mm_storeu_si128((__m128i*)&result[j + 4], delimeter_vec);\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        if (j + 1 < result_size) {\n            result[j + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= n; i += 4, j += 8) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        _mm_storeu_si128((__m128i*)&result[j], input_vec);\n        _mm_storeu_si128((__m128i*)&result[j + 4], delimiter_vec);\n    }\n\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        if (j + 1 < result_size) {\n            result[j + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= n; i += 4, j += 8) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        _mm_storeu_si128((__m128i*)&result[j], input_vec);\n        _mm_storeu_si128((__m128i*)&result[j + 4], delimiter_vec);\n    }\n\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        result[j + 1] = delimeter;\n    }\n\n    result.pop_back(); // Remove the last delimiter\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n\n    __m128i delimeter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= n; i += 4, j += 8) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n\n        // Store original numbers\n        _mm_storeu_si128((__m128i*)&result[j], input_vec);\n\n        // Store delimeter values interleaved\n        _mm_storeu_si128((__m128i*)&result[j + 4], delimeter_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i, j += 2) {\n        result[j] = numbers[i];\n        if (i != n - 1) {\n            result[j + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_size = size / 8 * 8; // Process in chunks of 8 elements (128 bits / 16 bits per element)\n\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set1_epi16(1); // Initialize product vector to one\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 8 elements into SIMD register\n        sum_vec = _mm_add_epi16(sum_vec, data); // Accumulate sum\n        prod_vec = _mm_mullo_epi16(prod_vec, data); // Accumulate product\n    }\n\n    // Horizontal sum for the sum vector\n    int16_t sum_array[8];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for the product vector\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t product = 1;\n    for (int i = 0; i < 8; ++i) {\n        product *= prod_array[i];\n    }\n\n    // Process remaining elements (if size is not a multiple of 8)\n    for (size_t i = simd_size; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_size = size / 8 * 8; // Process in chunks of 8 elements (128 bits / 16 bits per element)\n\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set1_epi16(1); // Initialize product vector to one\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 8 elements into SIMD register\n        sum_vec = _mm_add_epi16(sum_vec, data); // Accumulate sum\n        prod_vec = _mm_mullo_epi16(prod_vec, data); // Accumulate product\n    }\n\n    // Horizontal sum for sum_vec\n    int16_t sum_array[8];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum += sum_array[i];\n    }\n\n    // Horizontal product for prod_vec\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t product = 1;\n    for (int i = 0; i < 8; ++i) {\n        product *= prod_array[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_size = size / 8 * 8; // Process in chunks of 8 (128 bits / 16 bits per element)\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set1_epi16(1); // Initialize product vector to one\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 8 elements\n        sum_vec = _mm_add_epi16(sum_vec, data); // Add elements to sum vector\n        prod_vec = _mm_mullo_epi16(prod_vec, data); // Multiply elements to product vector\n    }\n\n    // Horizontal sum for sum_vec\n    int16_t sum_array[8];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Horizontal product for prod_vec\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t total_product = 1;\n    for (int i = 0; i < 8; ++i) {\n        total_product *= prod_array[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_size = size / 8 * 8; // Process in chunks of 8 (128 bits / 16 bits per element)\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        sum_vec = _mm_add_epi16(sum_vec, data);\n        prod_vec = _mm_mullo_epi16(prod_vec, data);\n    }\n\n    // Horizontal sum for sum_vec\n    int16_t sum_array[8];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Horizontal product for prod_vec\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t total_product = 1;\n    for (int i = 0; i < 8; ++i) {\n        total_product *= prod_array[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    size_t size = numbers.size();\n    size_t simd_size = size / 8 * 8; // Process in chunks of 8 elements (128-bit registers)\n\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to 0\n    __m128i prod_vec = _mm_set1_epi16(1); // Initialize product vector to 1\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 8 elements into SIMD register\n        sum_vec = _mm_add_epi16(sum_vec, data);                // Accumulate sum\n        prod_vec = _mm_mullo_epi16(prod_vec, data);            // Accumulate product\n    }\n\n    // Horizontal sum for the sum vector\n    int16_t sum_array[8];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum_array[i];\n    }\n\n    // Horizontal product for the product vector\n    int16_t prod_array[8];\n    _mm_storeu_si128((__m128i*)prod_array, prod_vec);\n    int64_t total_product = 1;\n    for (int i = 0; i < 8; ++i) {\n        total_product *= prod_array[i];\n    }\n\n    // Process remaining elements (if size is not a multiple of 8)\n    for (size_t i = simd_size; i < size; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) {\n        return result;\n    }\n\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m128i max_vec = _mm_set1_epi32(numbers[0]); // Initialize max vector with the first element\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 4 integers\n        max_vec = _mm_max_epi32(max_vec, current_vec); // Compute max\n        _mm_storeu_si128((__m128i*)&result[i], max_vec); // Store the rolling max\n    }\n\n    // Handle the remaining elements\n    int rolling_max = result[aligned_size - 1];\n    for (size_t i = aligned_size; i < n; ++i) {\n        rolling_max = std::max(rolling_max, numbers[i]);\n        result[i] = rolling_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m128i max_vector = _mm_set1_epi32(numbers[0]); // Initialize with the first element\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i current_vector = _mm_loadu_si128((__m128i*)&numbers[i]);\n        max_vector = _mm_max_epi32(max_vector, current_vector);\n        _mm_storeu_si128((__m128i*)&result[i], max_vector);\n    }\n\n    // Handle remaining elements (tail case)\n    for (size_t i = aligned_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m128i max_vector = _mm_set1_epi32(numbers[0]); // Initialize with the first element\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i current_vector = _mm_loadu_si128((__m128i*)&numbers[i]);\n        max_vector = _mm_max_epi32(max_vector, current_vector);\n        _mm_storeu_si128((__m128i*)&result[i], max_vector);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n\n    if (n == 0) return result;\n\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m128i max_vec = _mm_set1_epi32(numbers[0]); // Initialize max vector with the first element\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 4 integers\n        max_vec = _mm_max_epi32(max_vec, current_vec); // Compute the rolling maximum\n        _mm_storeu_si128((__m128i*)&result[i], max_vec); // Store the result\n    }\n\n    // Handle the remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]); // Initialize with the first element\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= n; i += 4) {\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        _mm_storeu_si128((__m128i*)&result[i], max_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must be of equal length\");\n    }\n\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t simd_blocks = len / simd_width;\n    size_t remainder = len % simd_width;\n\n    std::string result(len, '0');\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m128i vec_a = _mm_loadu_si128((__m128i*)(a.data() + i * simd_width));\n        __m128i vec_b = _mm_loadu_si128((__m128i*)(b.data() + i * simd_width));\n        __m128i vec_result = _mm_xor_si128(vec_a, vec_b);\n        _mm_storeu_si128((__m128i*)(result.data() + i * simd_width), vec_result);\n    }\n\n    for (size_t i = simd_blocks * simd_width; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length\");\n    }\n\n    std::string result(len, '0');\n    size_t i = 0;\n\n    // Process 16 characters at a time using SIMD\n    for (; i + 16 <= len; i += 16) {\n        __m128i vec_a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vec_b = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n\n        __m128i xor_result = _mm_xor_si128(vec_a, vec_b);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), xor_result);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length\");\n    }\n\n    std::string result(len, '0');\n    size_t i = 0;\n\n    // Process 16 bytes (128 bits) at a time using SSE2\n    for (; i + 16 <= len; i += 16) {\n        __m128i vec_a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vec_b = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n\n        __m128i xor_result = _mm_xor_si128(vec_a, vec_b);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), xor_result);\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length\");\n    }\n\n    // Pad strings to be a multiple of 16 for SIMD processing\n    size_t padded_len = (len + 15) & ~15;\n    a.resize(padded_len, '0');\n    b.resize(padded_len, '0');\n\n    std::vector<char> result(padded_len);\n\n    for (size_t i = 0; i < padded_len; i += 16) {\n        // Load 16 bytes from each string into SIMD registers\n        __m128i vec_a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vec_b = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n\n        // Convert ASCII '0' and '1' to binary 0 and 1\n        __m128i zero = _mm_set1_epi8('0');\n        vec_a = _mm_sub_epi8(vec_a, zero);\n        vec_b = _mm_sub_epi8(vec_b, zero);\n\n        // Perform XOR operation\n        __m128i vec_result = _mm_xor_si128(vec_a, vec_b);\n\n        // Convert binary 0 and 1 back to ASCII '0' and '1'\n        vec_result = _mm_add_epi8(vec_result, zero);\n\n        // Store the result back to the output array\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), vec_result);\n    }\n\n    // Convert result vector to string and trim to original length\n    return std::string(result.begin(), result.begin() + len);\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Input strings must have the same length\");\n    }\n\n    std::string result(len, '0');\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t i = 0;\n\n    // Process in chunks of 16 bytes using SIMD\n    for (; i + simd_width <= len; i += simd_width) {\n        __m128i vec_a = _mm_loadu_si128(reinterpret_cast<const __m128i*>(a.data() + i));\n        __m128i vec_b = _mm_loadu_si128(reinterpret_cast<const __m128i*>(b.data() + i));\n\n        __m128i xor_result = _mm_xor_si128(vec_a, vec_b);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), xor_result);\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        result[i] = ((a[i] - '0') ^ (b[i] - '0')) + '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"\"; // Return empty string for empty input\n    }\n\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    size_t n = strings.size();\n    size_t i = 0;\n\n    // Process strings in chunks of 4 using SIMD\n    for (; i + 4 <= n; i += 4) {\n        __m128i lengths = _mm_set_epi32(\n            strings[i].size(),\n            strings[i + 1].size(),\n            strings[i + 2].size(),\n            strings[i + 3].size()\n        );\n\n        __m128i max_lengths = _mm_set1_epi32(max_length);\n        __m128i cmp_mask = _mm_cmpgt_epi32(lengths, max_lengths);\n\n        int mask = _mm_movemask_epi8(cmp_mask);\n        if (mask != 0) {\n            for (int j = 0; j < 4; ++j) {\n                if (strings[i + j].size() > max_length) {\n                    max_length = strings[i + j].size();\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining strings\n    for (; i < n; ++i) {\n        if (strings[i].size() > max_length) {\n            max_length = strings[i].size();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n\n    // Fill lengths array with the lengths of the strings\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].size();\n    }\n\n    int max_length = 0;\n    size_t max_index = 0;\n\n    // Process lengths in chunks of 4 using SSE\n    __m128i max_vec = _mm_set1_epi32(0);\n    __m128i max_index_vec = _mm_set1_epi32(0);\n    for (size_t i = 0; i < n; i += 4) {\n        __m128i len_vec = _mm_set_epi32(\n            i + 3 < n ? lengths[i + 3] : 0,\n            i + 2 < n ? lengths[i + 2] : 0,\n            i + 1 < n ? lengths[i + 1] : 0,\n            lengths[i]\n        );\n\n        __m128i index_vec = _mm_set_epi32(\n            i + 3 < n ? i + 3 : 0,\n            i + 2 < n ? i + 2 : 0,\n            i + 1 < n ? i + 1 : 0,\n            i\n        );\n\n        __m128i cmp_mask = _mm_cmpgt_epi32(len_vec, max_vec);\n        max_vec = _mm_max_epi32(max_vec, len_vec);\n        max_index_vec = _mm_or_si128(\n            _mm_and_si128(cmp_mask, index_vec),\n            _mm_andnot_si128(cmp_mask, max_index_vec)\n        );\n    }\n\n    // Extract the maximum length and corresponding index\n    alignas(16) int max_lengths[4];\n    alignas(16) int max_indices[4];\n    _mm_store_si128((__m128i *)max_lengths, max_vec);\n    _mm_store_si128((__m128i *)max_indices, max_index_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        if (max_lengths[i] > max_length) {\n            max_length = max_lengths[i];\n            max_index = max_indices[i];\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"\"; // Return empty string for empty input\n    }\n\n    size_t n = strings.size();\n    std::vector<int> lengths(n);\n\n    // Populate lengths array with string lengths\n    for (size_t i = 0; i < n; ++i) {\n        lengths[i] = strings[i].size();\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    // Process lengths using SIMD\n    __m128i max_vec = _mm_setzero_si128(); // Initialize max vector to 0\n    __m128i index_vec = _mm_setzero_si128(); // Initialize index vector to 0\n    __m128i increment = _mm_set1_epi32(4); // Increment for indices\n    __m128i current_indices = _mm_set_epi32(3, 2, 1, 0); // Initial indices\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i length_vec = _mm_loadu_si128((__m128i*)&lengths[i]); // Load 4 lengths\n        __m128i cmp_mask = _mm_cmpgt_epi32(length_vec, max_vec); // Compare lengths with max_vec\n        max_vec = _mm_max_epi32(max_vec, length_vec); // Update max_vec\n        index_vec = _mm_blendv_epi8(index_vec, current_indices, cmp_mask); // Update index_vec\n        current_indices = _mm_add_epi32(current_indices, increment); // Increment indices\n    }\n\n    // Extract max length and index from SIMD vectors\n    alignas(16) int max_array[4];\n    alignas(16) int index_array[4];\n    _mm_store_si128((__m128i*)max_array, max_vec);\n    _mm_store_si128((__m128i*)index_array, index_vec);\n\n    for (int j = 0; j < 4; ++j) {\n        if (max_array[j] > max_length) {\n            max_length = max_array[j];\n            max_index = index_array[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (lengths[i] > max_length) {\n            max_length = lengths[i];\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"\"; // Return empty string for empty input\n    }\n\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    size_t n = strings.size();\n    size_t i = 0;\n\n    __m128i max_lengths = _mm_setzero_si128(); // Initialize max lengths\n    __m128i indices = _mm_setzero_si128();    // Initialize indices\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i lengths = _mm_set_epi32(\n            strings[i].size(),\n            strings[i + 1].size(),\n            strings[i + 2].size(),\n            strings[i + 3].size()\n        );\n\n        __m128i mask = _mm_cmpgt_epi32(lengths, max_lengths);\n        max_lengths = _mm_max_epi32(max_lengths, lengths);\n\n        __m128i new_indices = _mm_set_epi32(i, i + 1, i + 2, i + 3);\n        indices = _mm_blendv_epi8(indices, new_indices, mask);\n    }\n\n    // Extract max length and index from SIMD registers\n    int max_lengths_array[4];\n    int indices_array[4];\n    _mm_storeu_si128((__m128i*)max_lengths_array, max_lengths);\n    _mm_storeu_si128((__m128i*)indices_array, indices);\n\n    for (int j = 0; j < 4; ++j) {\n        if (max_lengths_array[j] > max_length) {\n            max_length = max_lengths_array[j];\n            max_index = indices_array[j];\n        }\n    }\n\n    // Process remaining strings\n    for (; i < n; ++i) {\n        size_t length = strings[i].size();\n        if (length > max_length) {\n            max_length = length;\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return \"None\";\n    }\n\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    size_t n = strings.size();\n    size_t i = 0;\n\n    __m128i max_lengths = _mm_setzero_si128();\n    __m128i max_indices = _mm_setzero_si128();\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i lengths = _mm_set_epi32(\n            strings[i].length(),\n            strings[i + 1].length(),\n            strings[i + 2].length(),\n            strings[i + 3].length()\n        );\n\n        __m128i cmp_mask = _mm_cmpgt_epi32(lengths, max_lengths);\n        max_lengths = _mm_max_epi32(max_lengths, lengths);\n        max_indices = _mm_blendv_epi8(max_indices, indices, cmp_mask);\n\n        indices = _mm_add_epi32(indices, _mm_set1_epi32(4));\n    }\n\n    alignas(16) int max_lengths_arr[4];\n    alignas(16) int max_indices_arr[4];\n    _mm_store_si128((__m128i*)max_lengths_arr, max_lengths);\n    _mm_store_si128((__m128i*)max_indices_arr, max_indices);\n\n    for (int j = 0; j < 4; ++j) {\n        if (max_lengths_arr[j] > max_length) {\n            max_length = max_lengths_arr[j];\n            max_index = max_indices_arr[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        size_t length = strings[i].length();\n        if (length > max_length) {\n            max_length = length;\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i vb = _mm_set1_epi64x(b);\n\n    while (1) {\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            __m128i result = va;\n            return _mm_cvtsi128_si64(result);\n        }\n\n        __m128i mod = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i vb = _mm_set1_epi64x(b);\n\n    while (1) {\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            return _mm_cvtsi128_si64(va);\n        }\n\n        __m128i mod = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i vb = _mm_set1_epi64x(b);\n\n    while (1) {\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            __m128i result = va;\n            return _mm_cvtsi128_si64(result);\n        }\n\n        __m128i mod = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i vb = _mm_set1_epi64x(b);\n\n    while (1) {\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            return _mm_cvtsi128_si64(va);\n        }\n\n        __m128i mod = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i vb = _mm_set1_epi64x(b);\n\n    while (1) {\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(cmp)) {\n            __m128i result = va;\n            return _mm_cvtsi128_si64(result);\n        }\n\n        __m128i mod = _mm_sub_epi64(va, _mm_mullo_epi64(_mm_div_epi64(va, vb), vb));\n        va = vb;\n        vb = mod;\n    }\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty vector if input size is less than 2\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128 current = _mm_set1_ps(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            __m128 next = _mm_loadu_ps(&numbers[j]);\n            __m128 diff = _mm_sub_ps(current, next);\n            __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute value\n\n            float diffs[4];\n            _mm_storeu_ps(diffs, abs_diff);\n\n            for (int k = 0; k < 4 && j + k < n; ++k) {\n                if (diffs[k] < min_diff) {\n                    min_diff = diffs[k];\n                    closest_a = numbers[i];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n#include <utility>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128 current = _mm_set1_ps(numbers[i]);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            size_t remaining = n - j;\n            __m128 other;\n\n            if (remaining >= 4) {\n                other = _mm_loadu_ps(&numbers[j]);\n            } else {\n                float temp[4] = {0};\n                for (size_t k = 0; k < remaining; ++k) {\n                    temp[k] = numbers[j + k];\n                }\n                other = _mm_loadu_ps(temp);\n            }\n\n            __m128 diff = _mm_sub_ps(current, other);\n            __m128 abs_diff = _mm_and_ps(diff, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));\n\n            float diffs[4];\n            _mm_storeu_ps(diffs, abs_diff);\n\n            for (size_t k = 0; k < 4 && j + k < n; ++k) {\n                if (diffs[k] < min_diff) {\n                    min_diff = diffs[k];\n                    closest_a = numbers[i];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty if there are fewer than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]); // Load 4 floats from the array\n\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 other = _mm_loadu_ps(&numbers[j]); // Load another 4 floats\n            __m128 diff = _mm_sub_ps(current, other); // Compute differences\n            __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute differences\n\n            float diffs[4];\n            _mm_storeu_ps(diffs, abs_diff); // Store results back to array\n\n            for (int k = 0; k < 4; ++k) {\n                if (i + k < n && j + k < n && i + k != j + k) { // Ensure valid indices and avoid self-comparison\n                    if (diffs[k] < min_diff) {\n                        min_diff = diffs[k];\n                        closest_a = numbers[i + k];\n                        closest_b = numbers[j + k];\n                    }\n                }\n            }\n        }\n    }\n\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b); // Ensure smaller number comes first\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty if there are fewer than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    for (int i = 0; i < n - 1; i++) {\n        __m128 current = _mm_set1_ps(numbers[i]); // Broadcast the current number\n        for (int j = i + 1; j < n; j += 4) {\n            // Load the next 4 numbers into an SSE register\n            __m128 next = _mm_loadu_ps(&numbers[j]);\n\n            // Compute absolute differences\n            __m128 diff = _mm_sub_ps(next, current);\n            __m128 abs_diff = _mm_and_ps(diff, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));\n\n            // Find the minimum difference in the SSE register\n            __m128 min_diff_vec = _mm_min_ps(abs_diff, _mm_set1_ps(min_diff));\n\n            // Store the results back to a temporary array\n            float temp[4];\n            _mm_storeu_ps(temp, abs_diff);\n\n            // Update the closest pair if a smaller difference is found\n            for (int k = 0; k < 4 && (j + k) < n; k++) {\n                if (temp[k] < min_diff) {\n                    min_diff = temp[k];\n                    closest_a = numbers[i];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    // Ensure the result is returned in ascending order\n    if (closest_a > closest_b) {\n        std::swap(closest_a, closest_b);\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n#include <utility>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return {}; // Return empty if there are less than two elements\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    std::pair<float, float> closest_pair;\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 current = _mm_loadu_ps(&numbers[i]); // Load 4 floats\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 other = _mm_loadu_ps(&numbers[j]); // Load another 4 floats\n\n            __m128 diff = _mm_sub_ps(current, other); // Compute differences\n            __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // Absolute differences\n\n            float diffs[4];\n            _mm_storeu_ps(diffs, abs_diff); // Store results back to array\n\n            for (int k = 0; k < 4; ++k) {\n                if (i + k < n && j + k < n && diffs[k] < min_diff && diffs[k] != 0.0f) {\n                    min_diff = diffs[k];\n                    closest_pair = {numbers[i + k], numbers[j + k]};\n                }\n            }\n        }\n    }\n\n    if (closest_pair.first > closest_pair.second) {\n        std::swap(closest_pair.first, closest_pair.second);\n    }\n\n    return {closest_pair.first, closest_pair.second};\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find the minimum and maximum values\n    __m128 min_vals = _mm_set1_ps(numbers[0]);\n    __m128 max_vals = _mm_set1_ps(numbers[0]);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_vals = _mm_min_ps(min_vals, vec);\n        max_vals = _mm_max_ps(max_vals, vec);\n    }\n\n    float min_array[4], max_array[4];\n    _mm_storeu_ps(min_array, min_vals);\n    _mm_storeu_ps(max_array, max_vals);\n\n    float min_val = *std::min_element(min_array, min_array + 4);\n    float max_val = *std::max_element(max_array, max_array + 4);\n\n    // Avoid division by zero\n    if (max_val == min_val) return std::vector<float>(n, 0.0f);\n\n    float range = max_val - min_val;\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n\n    std::vector<float> result(n);\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 normalized = _mm_sub_ps(vec, min_vec);\n        normalized = _mm_div_ps(normalized, range_vec);\n        _mm_storeu_ps(&result[i], normalized);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return numbers; // Return as is if there are less than two elements\n    }\n\n    // Find the minimum and maximum values using SIMD\n    __m128 min_val = _mm_set1_ps(FLT_MAX);\n    __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, vec);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    // Reduce min and max values across the SIMD registers\n    float min_vals[4], max_vals[4];\n    _mm_storeu_ps(min_vals, min_val);\n    _mm_storeu_ps(max_vals, max_val);\n\n    float min_value = std::min({min_vals[0], min_vals[1], min_vals[2], min_vals[3]});\n    float max_value = std::max({max_vals[0], max_vals[1], max_vals[2], max_vals[3]});\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        min_value = std::min(min_value, numbers[i]);\n        max_value = std::max(max_value, numbers[i]);\n    }\n\n    // Compute scale and offset\n    float range = max_value - min_value;\n    if (range == 0.0f) {\n        return std::vector<float>(n, 0.0f); // Avoid division by zero\n    }\n    float scale = 1.0f / range;\n    float offset = -min_value * scale;\n\n    // Apply the linear transformation using SIMD\n    std::vector<float> result(n);\n    __m128 scale_vec = _mm_set1_ps(scale);\n    __m128 offset_vec = _mm_set1_ps(offset);\n\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 rescaled = _mm_add_ps(_mm_mul_ps(vec, scale_vec), offset_vec);\n        _mm_storeu_ps(&result[i], rescaled);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = numbers[i] * scale + offset;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Load data into an aligned array\n    std::vector<float> result(n);\n    float min_val = FLT_MAX;\n    float max_val = -FLT_MAX;\n\n    // Find min and max using SIMD\n    size_t i = 0;\n    __m128 min_vec = _mm_set1_ps(FLT_MAX);\n    __m128 max_vec = _mm_set1_ps(-FLT_MAX);\n\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_vec = _mm_min_ps(min_vec, vec);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    // Reduce min and max vectors\n    float min_array[4], max_array[4];\n    _mm_storeu_ps(min_array, min_vec);\n    _mm_storeu_ps(max_array, max_vec);\n    for (int j = 0; j < 4; ++j) {\n        min_val = std::min(min_val, min_array[j]);\n        max_val = std::max(max_val, max_array[j]);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    // Compute range\n    float range = max_val - min_val;\n    if (range == 0.0f) return result; // Avoid division by zero\n\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n\n    // Rescale numbers using SIMD\n    i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 normalized = _mm_div_ps(_mm_sub_ps(vec, min_val_vec), range_vec);\n        _mm_storeu_ps(&result[i], normalized);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    // Find min and max using SIMD\n    __m128 min_vec = _mm_set1_ps(FLT_MAX);\n    __m128 max_vec = _mm_set1_ps(-FLT_MAX);\n\n    size_t simd_end = n - (n % 4);\n    for (size_t i = 0; i < simd_end; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_vec = _mm_min_ps(min_vec, vec);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    float min_vals[4], max_vals[4];\n    _mm_storeu_ps(min_vals, min_vec);\n    _mm_storeu_ps(max_vals, max_vec);\n\n    float min_val = *std::min_element(min_vals, min_vals + 4);\n    float max_val = *std::max_element(max_vals, max_vals + 4);\n\n    for (size_t i = simd_end; i < n; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    float range = max_val - min_val;\n    if (range == 0.0f) return std::vector<float>(n, 0.0f);\n\n    __m128 min_vec_broadcast = _mm_set1_ps(min_val);\n    __m128 range_vec_broadcast = _mm_set1_ps(range);\n\n    std::vector<float> result(n);\n    for (size_t i = 0; i < simd_end; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 normalized = _mm_sub_ps(vec, min_vec_broadcast);\n        normalized = _mm_div_ps(normalized, range_vec_broadcast);\n        _mm_storeu_ps(&result[i], normalized);\n    }\n\n    for (size_t i = simd_end; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t size = numbers.size();\n    if (size < 2) {\n        return numbers; // Return as is if there are less than two elements\n    }\n\n    // Find min and max using SIMD\n    __m128 min_vec = _mm_set1_ps(FLT_MAX);\n    __m128 max_vec = _mm_set1_ps(-FLT_MAX);\n\n    size_t simd_size = size / 4 * 4; // Round down to nearest multiple of 4\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        min_vec = _mm_min_ps(min_vec, num_vec);\n        max_vec = _mm_max_ps(max_vec, num_vec);\n    }\n\n    // Reduce min and max vectors to single values\n    float min_vals[4], max_vals[4];\n    _mm_storeu_ps(min_vals, min_vec);\n    _mm_storeu_ps(max_vals, max_vec);\n\n    float min_val = min_vals[0];\n    float max_val = max_vals[0];\n    for (int i = 1; i < 4; ++i) {\n        min_val = std::min(min_val, min_vals[i]);\n        max_val = std::max(max_val, max_vals[i]);\n    }\n\n    // Handle remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        min_val = std::min(min_val, numbers[i]);\n        max_val = std::max(max_val, numbers[i]);\n    }\n\n    // Compute scale and offset\n    float range = max_val - min_val;\n    if (range == 0.0f) {\n        return std::vector<float>(size, 0.0f); // Avoid division by zero\n    }\n    float scale = 1.0f / range;\n    float offset = -min_val * scale;\n\n    // Apply linear transformation using SIMD\n    std::vector<float> result(size);\n    __m128 scale_vec = _mm_set1_ps(scale);\n    __m128 offset_vec = _mm_set1_ps(offset);\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 res_vec = _mm_add_ps(_mm_mul_ps(num_vec, scale_vec), offset_vec);\n        _mm_storeu_ps(&result[i], res_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        result[i] = numbers[i] * scale + offset;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    uint64_t result = 1;\n\n    __m128i n_vec = _mm_set1_epi64x(n);\n\n    for (uint64_t i = sqrt_n; i > 1; i -= 2) {\n        __m128i divisors = _mm_set_epi64x(i, i - 1);\n        __m128i remainders = _mm_rem_epi64(n_vec, divisors);\n\n        if (_mm_extract_epi64(remainders, 1) == 0) return i - 1;\n        if (_mm_extract_epi64(remainders, 0) == 0) return i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0; // No valid divisor for n <= 1\n\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    __m128i divisor_vec, n_vec, result_vec;\n    __m128i zero_vec = _mm_setzero_si128();\n    n_vec = _mm_set1_epi64x(n);\n\n    for (uint64_t i = sqrt_n; i >= 1; --i) {\n        divisor_vec = _mm_set1_epi64x(i);\n        result_vec = _mm_div_epi64(n_vec, divisor_vec);\n        if (_mm_cmp_epi64(result_vec,zero_vec))return i;\n    }\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    __m128i divisor = _mm_set_epi64x(sqrt_n, sqrt_n - 1);\n    __m128i n_vec = _mm_set1_epi64x(n);\n\n    while (1) {\n        __m128i rem = _mm_rem_epi64(n_vec, divisor);\n        __m128i zero = _mm_setzero_si128();\n        __m128i cmp = _mm_cmpeq_epi64(rem, zero);\n\n        if (_mm_extract_epi64(cmp, 0)) return _mm_extract_epi64(divisor, 0);\n        if (_mm_extract_epi64(cmp, 1)) return _mm_extract_epi64(divisor, 1);\n\n        divisor = _mm_sub_epi64(divisor, _mm_set1_epi64x(2));\n    }\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    __m128i divisor = _mm_set_epi64x(sqrt_n, sqrt_n - 1);\n    __m128i n_vec = _mm_set_epi64x(n, n);\n\n    while (1) {\n        __m128i rem = _mm_rem_epi64(n_vec, divisor);\n        __m128i cmp = _mm_cmp_epi64(rem, _mm_setzero_si128());\n        if (_mm_test_all_zeros(cmp)) return divisor;\n        divisor = _mm_sub_epi64(divisor, _mm_set_epi64x(1, 1));\n    }\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    if (n <= 1) return 0;\n\n    uint64_t result = 1;\n    uint64_t sqrt_n = (uint64_t)sqrt((double)n);\n    __m128i divisor_vec, n_vec, zero_vec, mask_vec;\n    __m128i step_vec = _mm_set1_epi64x(1);\n    __m128i max_vec = _mm_set1_epi64x(n);\n\n    for (uint64_t i = sqrt_n; i >= 1; i -= 4) {\n        divisor_vec = _mm_set_epi64x(i, i - 1, i - 2, i - 3);\n        zero_vec = _mm_setzero_si128();\n        mask_vec = _mm_cmpeq_epi64(divisor_vec, zero_vec);\n        if mask_vec"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize an array of potential factors\n    const int max_factor = 100; // Arbitrary limit for factors\n    int potential_factors[max_factor];\n    for (int i = 2; i < max_factor; ++i) {\n        potential_factors[i - 2] = i;\n    }\n\n    // Load potential factors into an SSE vector\n    __m128i factor_vec = _mm_loadu_si128((__m128i*)potential_factors);\n\n    while (n > 1) {\n        for (int i = 2; i < max_factor; i += 4) {\n            // Load 4 potential factors into SIMD registers\n            __m128i factors = _mm_loadu_si128((__m128i*)(potential_factors + i - 2));\n\n            // Broadcast the current number `n` to all lanes\n            __m128i n_vec = _mm_set1_epi32(n);\n\n            // Compute modulo operation (n % factors)\n            __m128i mod_vec = _mm_rem_epi32(n_vec, factors);\n\n            // Compare modulo result with zero\n            __m128i cmp_vec = _mm_cmpeq_epi32(mod_vec, _mm_setzero_si128());\n\n            // Extract comparison results\n            int mask = _mm_movemask_epi8(cmp_vec);\n\n            // Check which factors divide `n`\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) {\n                    int factor = potential_factors[i - 2 + j];\n                    factors.push_back(factor);\n                    n /= factor;\n                    break;\n                }\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize an array of potential divisors\n    const int max_divisors = 16;\n    int divisors[max_divisors];\n    for (int i = 0; i < max_divisors; ++i) {\n        divisors[i] = i + 2; // Start from 2\n    }\n\n    __m128i divisor_vec = _mm_loadu_si128((__m128i*)divisors);\n\n    while (n > 1) {\n        bool found = false;\n\n        // Check divisibility in parallel\n        for (int i = 0; i < max_divisors; i += 4) {\n            __m128i n_vec = _mm_set1_epi32(n);\n            __m128i div_vec = _mm_loadu_si128((__m128i*)&divisors[i]);\n\n            __m128i rem_vec = _mm_rem_epi32(n_vec, div_vec);\n            __m128i zero_vec = _mm_setzero_si128();\n            __m128i cmp_vec = _mm_cmpeq_epi32(rem_vec, zero_vec);\n\n            int mask = _mm_movemask_epi8(cmp_vec);\n            if (mask != 0) {\n                for (int j = 0; j < 4; ++j) {\n                    if ((mask >> (j * 4)) & 0xF) {\n                        int factor = divisors[i + j];\n                        factors.push_back(factor);\n                        n /= factor;\n                        found = true;\n                        break;\n                    }\n                }\n                if (found) break;\n            }\n        }\n\n        // If no divisor found in the current batch, increment divisors\n        if (!found) {\n            for (int i = 0; i < max_divisors; ++i) {\n                divisors[i] += max_divisors;\n            }\n            divisor_vec = _mm_loadu_si128((__m128i*)divisors);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n\n    if (n <= 1) {\n        return factors;\n    }\n\n    // Initialize an array of potential divisors\n    const int max_divisor = 100; // Arbitrary limit for divisors\n    alignas(16) int divisors[max_divisor];\n    for (int i = 0; i < max_divisor; ++i) {\n        divisors[i] = i + 2; // Start from 2\n    }\n\n    // Load divisors into SIMD registers\n    __m128i divisor_vec = _mm_load_si128((__m128i*)divisors);\n\n    while (n > 1) {\n        for (int i = 0; i < max_divisor; i += 4) {\n            // Load 4 divisors into SIMD register\n            __m128i current_divisors = _mm_load_si128((__m128i*)(divisors + i));\n\n            // Broadcast the current number `n` into SIMD register\n            __m128i n_vec = _mm_set1_epi32(n);\n\n            // Compute remainder for each divisor\n            __m128i remainder = _mm_rem_epi32(n_vec, current_divisors);\n\n            // Compare remainder with zero\n            __m128i is_factor = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n\n            // Extract mask of valid factors\n            int mask = _mm_movemask_epi8(is_factor);\n\n            // Process each divisor in the SIMD register\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) { // Check if divisor is a factor\n                    int factor = divisors[i + j];\n                    factors.push_back(factor);\n                    n /= factor;\n                    break; // Restart factorization for updated `n`\n                }\n            }\n\n            if (n == 1) {\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize SIMD registers for potential divisors\n    __m128i divisor = _mm_set_epi32(11, 7, 5, 2); // Test divisors: 2, 5, 7, 11\n    __m128i zero = _mm_setzero_si128();\n\n    while (n > 1) {\n        __m128i n_vec = _mm_set1_epi32(n); // Broadcast n to all lanes\n        __m128i remainder = _mm_rem_epi32(n_vec, divisor); // Compute n % divisor for all lanes\n\n        // Compare remainder with zero to find valid divisors\n        __m128i mask = _mm_cmpeq_epi32(remainder, zero);\n\n        // Extract valid divisors\n        int valid_mask = _mm_movemask_epi8(mask);\n        for (int i = 0; i < 4; ++i) {\n            if (valid_mask & (1 << (i * 4))) { // Check if divisor[i] is valid\n                int div = _mm_extract_epi32(divisor, i);\n                factors.push_back(div);\n                n /= div; // Divide n by the found divisor\n                break; // Restart loop to handle repeated factors\n            }\n        }\n\n        // If no valid divisor found, n is prime\n        if (valid_mask == 0) {\n            factors.push_back(n);\n            break;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Initialize an array of potential divisors\n    const int max_divisor = 100; // Arbitrary limit for divisors\n    int divisors[max_divisor];\n    for (int i = 0; i < max_divisor; ++i) {\n        divisors[i] = i + 2; // Start from 2\n    }\n\n    // Load divisors into SIMD registers\n    __m128i divisor_vec = _mm_loadu_si128((__m128i*)divisors);\n\n    while (n > 1) {\n        for (int i = 0; i < max_divisor; i += 4) {\n            // Load 4 divisors into SIMD register\n            __m128i current_divisors = _mm_loadu_si128((__m128i*)(divisors + i));\n\n            // Broadcast n into SIMD register\n            __m128i n_vec = _mm_set1_epi32(n);\n\n            // Perform modulo operation (n % divisors)\n            __m128i mod_result = _mm_sub_epi32(n_vec, _mm_mullo_epi32(_mm_div_epi32(n_vec, current_divisors), current_divisors));\n\n            // Compare mod_result with zero (check divisibility)\n            __m128i is_divisible = _mm_cmpeq_epi32(mod_result, _mm_setzero_si128());\n\n            // Extract results from SIMD register\n            int mask = _mm_movemask_epi8(is_divisible);\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (1 << (j * 4))) { // Check divisibility\n                    int divisor = divisors[i + j];\n                    factors.push_back(divisor);\n                    n /= divisor;\n                    break;\n                }\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_size = size - (size % 4); // Process in chunks of 4\n\n    __m128 zero = _mm_setzero_ps(); // Set all elements to 0\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]); // Load 4 floats\n        __m128 mask = _mm_cmpgt_ps(vec, zero); // Compare > 0\n        __m128 filtered = _mm_and_ps(vec, mask); // Keep positive values, set others to 0\n\n        float temp[4];\n        _mm_storeu_ps(temp, filtered); // Store the result back to a temporary array\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]); // Add positive values to the result\n            }\n        }\n    }\n\n    // Handle the remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    result.reserve(size);\n\n    __m128 zero = _mm_setzero_ps(); // Set all elements to 0\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]); // Load 4 floats from the input vector\n        __m128 mask = _mm_cmpgt_ps(vec, zero); // Compare elements > 0\n        int mask_bits = _mm_movemask_ps(mask); // Get the comparison result as a bitmask\n\n        for (int j = 0; j < 4; ++j) {\n            if (mask_bits & (1 << j)) { // Check if the j-th element is positive\n                result.push_back(l[i + j]);\n            }\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_size = size - (size % 4); // Process in chunks of 4\n\n    __m128 zero = _mm_setzero_ps(); // Set zero vector\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]); // Load 4 floats\n        __m128 mask = _mm_cmpgt_ps(vec, zero); // Compare > 0\n        __m128 positive = _mm_and_ps(vec, mask); // Mask out non-positive values\n\n        float temp[4];\n        _mm_storeu_ps(temp, positive); // Store the result\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]); // Add positive values to the result\n            }\n        }\n    }\n\n    // Handle the remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    result.reserve(size);\n\n    __m128 zero = _mm_setzero_ps(); // Set zero vector\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]); // Load 4 floats\n        __m128 mask = _mm_cmpgt_ps(vec, zero); // Compare > 0\n        int mask_bits = _mm_movemask_ps(mask); // Get mask bits\n\n        for (int j = 0; j < 4; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(l[i + j]); // Add positive numbers\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t simd_size = size - (size % 4); // Process in chunks of 4\n\n    __m128 zero = _mm_setzero_ps(); // Set zero vector\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]); // Load 4 floats\n        __m128 mask = _mm_cmpgt_ps(vec, zero); // Compare > 0\n        __m128 positive = _mm_and_ps(vec, mask); // Mask positive values\n\n        float temp[4];\n        _mm_storeu_ps(temp, positive); // Store results\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n\n    while (i <= limit) {\n        __m128i vec_i = _mm_set_epi64x(i + 2, i);\n        __m128i vec_rem = _mm_rem_epi64(vec_n, vec_i);\n\n        if (_mm_extract_epi64(vec_rem, 0) == 0 || _mm_extract_epi64(vec_rem, 1) == 0) {\n            return false;\n        }\n\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n\n    for (; i <= limit; i += 8) {\n        __m128i vec_i = _mm_set_epi64x(i + 4, i);\n        __m128i vec_i2 = _mm_set_epi64x(i + 6, i + 2);\n\n        __m128i rem1 = _mm_rem_epi64(vec_n, vec_i);\n        __m128i rem2 = _mm_rem_epi64(vec_n, vec_i2);\n\n        __m128i zero = _mm_setzero_si128();\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(rem1, zero)) || \n            _mm_movemask_epi8(_mm_cmpeq_epi64(rem2, zero))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n\n    for (; i <= limit; i += 8) {\n        __m128i vec_i = _mm_set_epi64x(i + 6, i + 4);\n        __m128i vec_i2 = _mm_set_epi64x(i + 2, i);\n\n        __m128i rem1 = _mm_rem_epi64(vec_n, vec_i);\n        __m128i rem2 = _mm_rem_epi64(vec_n, vec_i2);\n\n        __m128i zero = _mm_setzero_si128();\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(rem1, zero)) || _mm_movemask_epi8(_mm_cmpeq_epi64(rem2, zero))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n\n    while (i <= limit) {\n        __m128i vec_i = _mm_set_epi64x(i + 2, i);\n        __m128i vec_rem = _mm_rem_epi64(vec_n, vec_i);\n\n        // Check if any remainder is zero\n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi64(vec_rem, _mm_setzero_si128()));\n        if (mask != 0) return false;\n\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = (int64_t)sqrt((double)n);\n    int64_t i = 5;\n\n    __m128i vec_n = _mm_set1_epi64x(n);\n\n    while (i <= limit) {\n        __m128i vec_i = _mm_set_epi64x(i + 2, i);\n        __m128i vec_rem = _mm_rem_epi64(vec_n, vec_i);\n\n        if (_mm_extract_epi64(vec_rem, 0) == 0 || _mm_extract_epi64(vec_rem, 1) == 0) {\n            return false;\n        }\n\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n\n    // Check if the string contains any letters\n    bool has_letters = false;\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            break;\n        }\n    }\n\n    if (!has_letters) {\n        // Reverse the string if no letters are present\n        for (size_t i = 0; i < n; ++i) {\n            result[i] = s[n - i - 1];\n        }\n        return result;\n    }\n\n    // Process the string using SIMD\n    size_t i = 0;\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i case_flip = _mm_set1_epi8(0x20);\n\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n\n        // Check if characters are lowercase letters\n        __m128i is_lower = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, lower_a),\n            _mm_cmplt_epi8(chunk, lower_z)\n        );\n\n        // Check if characters are uppercase letters\n        __m128i is_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, upper_a),\n            _mm_cmplt_epi8(chunk, upper_z)\n        );\n\n        // Flip case for letters\n        __m128i flip_mask = _mm_or_si128(is_lower, is_upper);\n        __m128i flipped = _mm_xor_si128(chunk, _mm_and_si128(flip_mask, case_flip));\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)&result[i], flipped);\n    }\n\n    // Process remaining characters\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - 32; // Convert to uppercase\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c + 32; // Convert to lowercase\n        } else {\n            result[i] = c; // Keep as is\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i case_mask = _mm_set1_epi8(0x20); // Mask to toggle case\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n\n        // Check if characters are letters\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, lower_a - _mm_set1_epi8(1)), _mm_cmplt_epi8(chunk, lower_z + _mm_set1_epi8(1)));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upper_a - _mm_set1_epi8(1)), _mm_cmplt_epi8(chunk, upper_z + _mm_set1_epi8(1)));\n        __m128i is_letter = _mm_or_si128(is_lower, is_upper);\n\n        // Toggle case for letters\n        __m128i toggled = _mm_xor_si128(chunk, _mm_and_si128(is_letter, case_mask));\n\n        // Store result\n        _mm_storeu_si128((__m128i*)&result[i], toggled);\n\n        // Check if there are any letters\n        has_letters |= _mm_movemask_epi8(is_letter) != 0;\n    }\n\n    // Process remaining characters\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            result[i] = c ^ 0x20; // Toggle case\n        } else {\n            result[i] = c;\n        }\n    }\n\n    // If no letters, reverse the string\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string solve_simd(const std::string &s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]);\n\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chars, lower_a), _mm_cmplt_epi8(chars, lower_z));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chars, upper_a), _mm_cmplt_epi8(chars, upper_z));\n        __m128i is_letter = _mm_or_si128(is_lower, is_upper);\n\n        has_letters |= (_mm_movemask_epi8(is_letter) != 0);\n\n        __m128i toggled = _mm_xor_si128(chars, _mm_and_si128(is_letter, case_mask));\n        _mm_storeu_si128((__m128i*)&result[i], toggled);\n    }\n\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letters = true;\n            result[i] = c ^ 0x20;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result = s;\n\n    bool has_letters = false;\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i lower_z = _mm_set1_epi8('z');\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, lower_a), _mm_cmplt_epi8(chunk, lower_z));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upper_a), _mm_cmplt_epi8(chunk, upper_z));\n        __m128i is_letter = _mm_or_si128(is_lower, is_upper);\n\n        if (_mm_movemask_epi8(is_letter)) {\n            has_letters = true;\n        }\n\n        __m128i toggled = _mm_xor_si128(chunk, _mm_and_si128(is_letter, case_mask));\n        _mm_storeu_si128((__m128i*)&result[i], toggled);\n    }\n\n    for (; i < n; ++i) {\n        if (std::isalpha(s[i])) {\n            has_letters = true;\n            result[i] ^= 0x20;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(const std::string & s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n    bool has_letters = false;\n\n    __m128i mask_lower = _mm_set1_epi8(0x20); // Mask for toggling case\n    __m128i mask_alpha = _mm_set1_epi8(0xDF); // Mask for checking alphabetic characters\n    __m128i mask_upper_bound = _mm_set1_epi8('Z'); // Upper bound for alphabetic characters\n    __m128i mask_lower_bound = _mm_set1_epi8('A'); // Lower bound for alphabetic characters\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(s.data() + i));\n        __m128i is_alpha_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, mask_lower_bound), _mm_cmplt_epi8(chunk, mask_upper_bound));\n        __m128i is_alpha_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, mask_upper_bound), _mm_cmplt_epi8(chunk, mask_alpha));\n        __m128i is_alpha = _mm_or_si128(is_alpha_upper, is_alpha_lower);\n\n        has_letters |= _mm_movemask_epi8(is_alpha) != 0;\n\n        __m128i toggled_case = _mm_xor_si128(chunk, _mm_and_si128(mask_lower, is_alpha));\n        _mm_storeu_si128((__m128i*)(result.data() + i), toggled_case);\n    }\n\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            has_letters = true;\n            result[i] = c ^ 0x20;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    if (!has_letters) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Extract values at indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the extracted values\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace values at indices divisible by 3 with sorted values\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    // Extract indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (int i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the extracted values\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the original indices divisible by 3 with sorted values\n    __m128i sorted_values;\n    int sorted_index = 0;\n    for (int i = 0; i < n; i += 4) {\n        __m128i vec = _mm_set_epi32(\n            (i + 3 < n && (i + 3) % 3 == 0) ? divisible_by_three[sorted_index++] : l[i + 3],\n            (i + 2 < n && (i + 2) % 3 == 0) ? divisible_by_three[sorted_index++] : l[i + 2],\n            (i + 1 < n && (i + 1) % 3 == 0) ? divisible_by_three[sorted_index++] : l[i + 1],\n            (i < n && i % 3 == 0) ? divisible_by_three[sorted_index++] : l[i]\n        );\n\n        sorted_values = vec;\n\n        if (i < n) result[i] = _mm_extract_epi32(sorted_values, 0);\n        if (i + 1 < n) result[i + 1] = _mm_extract_epi32(sorted_values, 1);\n        if (i + 2 < n) result[i + 2] = _mm_extract_epi32(sorted_values, 2);\n        if (i + 3 < n) result[i + 3] = _mm_extract_epi32(sorted_values, 3);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <vector>\n#include <algorithm>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Collect elements at indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i += 3) {\n        divisible_by_three.push_back(l[i]);\n    }\n\n    // Sort the collected elements\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the sorted elements back into the result vector\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i += 3) {\n        result[i] = divisible_by_three[sorted_index++];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(l);\n\n    // Extract indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            divisible_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the extracted values\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the values in the original vector at indices divisible by 3\n    size_t sorted_index = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (i % 3 == 0) {\n            result[i] = divisible_by_three[sorted_index++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result = l;\n\n    // Extract elements at indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (size_t i = 0; i < n; i += 3) {\n        divisible_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    size_t m = divisible_by_three.size();\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = i + 1; j < m; j++) {\n            if (divisible_by_three[i] > divisible_by_three[j]) {\n                std::swap(divisible_by_three[i], divisible_by_three[j]);\n            }\n        }\n    }\n\n    // Store sorted elements back into the result vector\n    size_t idx = 0;\n    for (size_t i = 0; i < n; i += 3) {\n        result[i] = divisible_by_three[idx++];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    size_t simd_size = 4; // SSE processes 4 floats at a time\n    size_t aligned_size = n - (n % simd_size);\n\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < aligned_size; i += simd_size) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    float max_array[simd_size];\n    _mm_storeu_ps(max_array, max_vec);\n\n    float max_value = std::numeric_limits<float>::lowest();\n    for (size_t i = 0; i < simd_size; ++i) {\n        if (max_array[i] > max_value) {\n            max_value = max_array[i];\n        }\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (l[i] > max_value) {\n            max_value = l[i];\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) {\n        return std::numeric_limits<float>::lowest();\n    }\n\n    size_t n = l.size();\n    size_t simd_size = n / 4 * 4; // Round down to the nearest multiple of 4\n\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 current_vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, current_vec);\n    }\n\n    float max_array[4];\n    _mm_storeu_ps(max_array, max_vec);\n\n    float max_val = std::max({max_array[0], max_array[1], max_array[2], max_array[3]});\n\n    for (size_t i = simd_size; i < n; ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    // Extract maximum value from the SIMD vector\n    float max_array[4];\n    _mm_storeu_ps(max_array, max_vec);\n    float max_value = std::max({max_array[0], max_array[1], max_array[2], max_array[3]});\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        max_value = std::max(max_value, l[i]);\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return -std::numeric_limits<float>::infinity();\n\n    // Pad the vector to a multiple of 4 for SIMD processing\n    size_t padded_size = (n + 3) & ~3; // Round up to the nearest multiple of 4\n    l.resize(padded_size, -std::numeric_limits<float>::infinity());\n\n    __m128 max_vec = _mm_set1_ps(-std::numeric_limits<float>::infinity());\n\n    for (size_t i = 0; i < padded_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    // Extract the maximum value from the SIMD register\n    float max_values[4];\n    _mm_storeu_ps(max_values, max_vec);\n\n    return std::max({max_values[0], max_values[1], max_values[2], max_values[3]});\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return std::numeric_limits<float>::lowest();\n\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    // Extract maximum from the SIMD register\n    float max_array[4];\n    _mm_storeu_ps(max_array, max_vec);\n    float max_value = std::max({max_array[0], max_array[1], max_array[2], max_array[3]});\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        max_value = std::max(max_value, l[i]);\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    int count = 0;\n    __m128i seven_mask = _mm_set1_epi8('7'); // Mask for digit '7'\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            char buffer[16];\n            snprintf(buffer, sizeof(buffer), \"%d\", i);\n\n            __m128i digits = _mm_loadu_si128((__m128i*)buffer); // Load digits into SIMD register\n            __m128i cmp_result = _mm_cmpeq_epi8(digits, seven_mask); // Compare digits with '7'\n            int mask = _mm_movemask_epi8(cmp_result); // Get comparison mask\n\n            while (mask) {\n                count += mask & 1; // Count the number of '7's\n                mask >>= 1;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n) {\n    __m128i sevens = _mm_set1_epi8('7');\n    int count = 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i div11 = _mm_setzero_si128();\n        __m128i div13 = _mm_setzero_si128();\n        __m128i valid = _mm_setzero_si128();\n        __m128i result = _mm_setzero_si128();\n\n        for (int j = 0; j < 16 && i + j < n; ++j) {\n            if ((i + j) % 11 == 0 || (i + j) % 13 == 0) {\n                div11 = _mm_insert_epi8(div11, i + j, j);\n            }\n        }\n\n        valid = _mm_or_si128(div11, div13);\n        result = _mm_cmpeq_epi8(valid, sevens);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    int count = 0;\n    __m128i seven = _mm_set1_epi8('7'); // Set all bytes to ASCII '7'\n    __m128i zero = _mm_setzero_si128(); // Zero vector for comparison\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            char buffer[16];\n            snprintf(buffer, sizeof(buffer), \"%d\", i); // Convert number to string\n            __m128i num_vec = _mm_loadu_si128((__m128i*)buffer); // Load into SIMD register\n\n            // Compare each byte with '7'\n            __m128i cmp = _mm_cmpeq_epi8(num_vec, seven);\n\n            // Count the number of matches\n            int mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    __m128i count7 = _mm_setzero_si128();\n    __m128i eleven = _mm_set1_epi32(11);\n    __m128i thirteen = _mm_set1_epi32(13);\n    __m128i seven = _mm_set1_epi32(7);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i mod11 = _mm_rem_epi32(numbers, eleven);\n        __m128i mod13 = _mm_rem_epi32(numbers, thirteen);\n\n        __m128i divisible11 = _mm_cmpeq_epi32(mod11, zero);\n        __m128i divisible13 = _mm_cmpeq_epi32(mod13, zero);\n        __m128i divisible = _mm_or_si128(divisible11, divisible13);\n\n        __m128i mask = _mm_and_si128(divisible, numbers);\n        __m128i digit7 = _mm_cmpeq_epi32(mask, seven);\n\n        count7 = _mm_add_epi32(count7, digit7);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, count7);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    if (n <= 0) return 0;\n\n    int count = 0;\n    __m128i seven_mask = _mm_set1_epi8('7'); // Mask for digit '7'\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i divisible_mask = _mm_setzero_si128();\n\n        for (int j = i; j < i + 16; j++) {\n            if (j % 11 == 0 || j % 13 == 0) divisible_mask |= (1 << j);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n\n    // Extract even indices into a separate vector\n    std::vector<float> evens;\n    for (size_t i = 0; i < n; i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices\n    std::sort(evens.begin(), evens.end());\n\n    // Use SIMD to copy sorted evens back to the result vector\n    size_t evens_index = 0;\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 vec = _mm_setzero_ps();\n\n        if (i < n) vec = _mm_set_ps(\n            (i + 3 < n) ? l[i + 3] : 0.0f,\n            (i + 2 < n) ? evens[evens_index + 1] : 0.0f,\n            (i + 1 < n) ? l[i + 1] : 0.0f,\n            (i < n) ? evens[evens_index] : 0.0f\n        );\n\n        _mm_storeu_ps(&result[i], vec);\n        evens_index += 2;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices using std::sort\n    std::sort(even_indices.begin(), even_indices.end());\n\n    // Create the result vector\n    std::vector<float> result(n);\n\n    // Use SIMD to copy odd indices and sorted even indices back to the result\n    size_t even_idx = 0;\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 vec = _mm_set_ps(\n            (i + 3 < n) ? l[i + 3] : 0.0f,\n            (i + 2 < n) ? even_indices[even_idx++] : 0.0f,\n            (i + 1 < n) ? l[i + 1] : 0.0f,\n            (i < n) ? even_indices[even_idx++] : 0.0f\n        );\n\n        if (i < n) result[i] = _mm_cvtss_f32(vec);\n        if (i + 1 < n) result[i + 1] = l[i + 1];\n        if (i + 2 < n) result[i + 2] = even_indices[even_idx - 1];\n        if (i + 3 < n) result[i + 3] = l[i + 3];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices using SIMD\n    size_t even_size = even_indices.size();\n    for (size_t i = 0; i < even_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&even_indices[i]);\n        for (int j = 0; j < 3; ++j) {\n            __m128 shuffled = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(1, 0, 3, 2));\n            vec = _mm_min_ps(vec, shuffled);\n            shuffled = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(2, 3, 0, 1));\n            vec = _mm_min_ps(vec, shuffled);\n        }\n        _mm_storeu_ps(&even_indices[i], vec);\n    }\n\n    // Merge sorted even indices back into the original vector\n    size_t even_idx = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_indices[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices using SIMD\n    size_t even_size = even_indices.size();\n    for (size_t i = 0; i < even_size; i += 4) {\n        __m128 vec = _mm_set_ps(\n            i + 3 < even_size ? even_indices[i + 3] : FLT_MAX,\n            i + 2 < even_size ? even_indices[i + 2] : FLT_MAX,\n            i + 1 < even_size ? even_indices[i + 1] : FLT_MAX,\n            even_indices[i]\n        );\n\n        // Perform a bubble sort-like operation on the SIMD vector\n        for (int j = 0; j < 3; ++j) {\n            __m128 shuffled = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(3, 2, 1, 0));\n            vec = _mm_min_ps(vec, shuffled);\n            shuffled = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(0, 3, 2, 1));\n            vec = _mm_min_ps(vec, shuffled);\n        }\n\n        // Store sorted values back to the even_indices vector\n        float sorted[4];\n        _mm_store_ps(sorted, vec);\n        for (size_t j = 0; j < 4 && i + j < even_size; ++j) {\n            even_indices[i + j] = sorted[j];\n        }\n    }\n\n    // Merge sorted even indices back into the original vector\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_indices[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n < 2) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_values;\n    for (size_t i = 0; i < n; i += 2) {\n        even_values.push_back(l[i]);\n    }\n\n    // Sort the even values using SIMD\n    size_t even_size = even_values.size();\n    for (size_t i = 0; i < even_size; i += 4) {\n        __m128 vec = _mm_set_ps(\n            i + 3 < even_size ? even_values[i + 3] : FLT_MAX,\n            i + 2 < even_size ? even_values[i + 2] : FLT_MAX,\n            i + 1 < even_size ? even_values[i + 1] : FLT_MAX,\n            even_values[i]\n        );\n\n        // Perform a simple bubble sort on the SIMD vector\n        for (int j = 0; j < 3; ++j) {\n            __m128 shuffled = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(3, 2, 1, 0));\n            __m128 min_vals = _mm_min_ps(vec, shuffled);\n            __m128 max_vals = _mm_max_ps(vec, shuffled);\n            vec = _mm_shuffle_ps(min_vals, max_vals, _MM_SHUFFLE(3, 2, 1, 0));\n        }\n\n        // Store sorted values back into the even_values array\n        float sorted[4];\n        _mm_store_ps(sorted, vec);\n        for (size_t j = 0; j < 4 && i + j < even_size; ++j) {\n            even_values[i + j] = sorted[j];\n        }\n    }\n\n    // Merge sorted even values back into the original vector\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_values[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vi = _mm_set1_epi32(l[i]);\n            __m128i vj = _mm_set1_epi32(l[j]);\n\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n\n                __m128i sum = _mm_add_epi32(vi, _mm_add_epi32(vj, vk));\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n                if (_mm_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vi = _mm_set1_epi32(l[i]);\n            __m128i vj = _mm_set1_epi32(l[j]);\n\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n\n                __m128i sum = _mm_add_epi32(vi, _mm_add_epi32(vj, vk));\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n                if (_mm_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i a = _mm_set1_epi32(l[i] + l[j]); // Broadcast the sum of l[i] and l[j]\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i b = _mm_loadu_si128((__m128i*)&l[k]); // Load 4 integers from l[k]\n                __m128i sum = _mm_add_epi32(a, b); // Add the broadcasted sum to the loaded integers\n                __m128i zero = _mm_setzero_si128(); // Set zero vector\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero); // Compare sum with zero\n                int mask = _mm_movemask_epi8(cmp); // Create a mask from the comparison\n                if (mask != 0) return true; // If any comparison is true, return true\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i vi = _mm_set1_epi32(l[i]);\n            __m128i vj = _mm_set1_epi32(l[j]);\n\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i vk = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i sum = _mm_add_epi32(vi, _mm_add_epi32(vj, vk));\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n                if (_mm_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i a = _mm_set1_epi32(l[i]);\n            __m128i b = _mm_set1_epi32(l[j]);\n\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i c = _mm_loadu_si128((__m128i*)&l[k]);\n\n                __m128i sum_ab = _mm_add_epi32(a, b);\n                __m128i sum_abc = _mm_add_epi32(sum_ab, c);\n\n                __m128i cmp = _mm_cmpeq_epi32(sum_abc, zero);\n                if (_mm_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    // Process 4 integers at a time using SSE intrinsics\n    size_t i = 0;\n    __m128i one = _mm_set1_epi32(1); // Vector with all elements set to 1\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the input vector\n        __m128i incremented = _mm_add_epi32(vec, one);  // Increment each element by 1\n        _mm_storeu_si128((__m128i*)&result[i], incremented); // Store the result\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    // Process in chunks of 4 integers using SSE\n    size_t i = 0;\n    __m128i one = _mm_set1_epi32(1); // Vector with all elements set to 1\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the input vector\n        __m128i incremented = _mm_add_epi32(vec, one);  // Increment each element by 1\n        _mm_storeu_si128((__m128i*)&result[i], incremented); // Store the result\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    __m128i one = _mm_set1_epi32(1); // Vector with all elements set to 1\n    size_t i = 0;\n\n    // Process 4 integers at a time\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the input vector\n        __m128i incremented = _mm_add_epi32(vec, one);  // Increment each element by 1\n        _mm_storeu_si128((__m128i*)&result[i], incremented); // Store the result back to the output vector\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    __m128i one = _mm_set1_epi32(1); // Vector with all elements set to 1\n    size_t i = 0;\n\n    // Process 4 integers at a time\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the input vector\n        __m128i incremented = _mm_add_epi32(vec, one);  // Add 1 to each element\n        _mm_storeu_si128((__m128i*)&result[i], incremented); // Store the result\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    __m128i one = _mm_set1_epi32(1); // Vector with all elements set to 1\n\n    size_t i = 0;\n    // Process 4 integers at a time\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the input vector\n        __m128i incremented = _mm_add_epi32(vec, one);  // Add 1 to each element\n        _mm_storeu_si128((__m128i*)&result[i], incremented); // Store the result\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int size = l.size();\n    if (size < 2) return false;\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vec_i = _mm_set_epi32(\n            i + 3 < size ? l[i + 3] : 0,\n            i + 2 < size ? l[i + 2] : 0,\n            i + 1 < size ? l[i + 1] : 0,\n            l[i]\n        );\n\n        for (int j = i + 1; j < size; j += 4) {\n            __m128i vec_j = _mm_set_epi32(\n                j + 3 < size ? l[j + 3] : 0,\n                j + 2 < size ? l[j + 2] : 0,\n                j + 1 < size ? l[j + 1] : 0,\n                l[j]\n            );\n\n            __m128i sum = _mm_add_epi32(vec_i, vec_j);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n            if (_mm_movemask_epi8(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i vec_i = _mm_set_epi32(\n            i + 3 < n ? l[i + 3] : 0,\n            i + 2 < n ? l[i + 2] : 0,\n            i + 1 < n ? l[i + 1] : 0,\n            l[i]\n        );\n\n        for (int j = i + 1; j < n; j += 4) {\n            __m128i vec_j = _mm_set_epi32(\n                j + 3 < n ? l[j + 3] : 0,\n                j + 2 < n ? l[j + 2] : 0,\n                j + 1 < n ? l[j + 1] : 0,\n                l[j]\n            );\n\n            __m128i sum = _mm_add_epi32(vec_i, vec_j);\n            __m128i zero = _mm_setzero_si128();\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n            if (_mm_movemask_epi8(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i zero = _mm_setzero_si128(); // Vector of zeros\n\n    for (int i = 0; i < n; i++) {\n        __m128i current = _mm_set1_epi32(l[i]); // Broadcast current element to all lanes\n\n        for (int j = i + 1; j < n; j += 4) {\n            // Load next 4 elements into a vector\n            __m128i next = _mm_loadu_si128((__m128i*)&l[j]);\n\n            // Add current element to the next 4 elements\n            __m128i sum = _mm_add_epi32(current, next);\n\n            // Compare if any sum equals zero\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n            // Check if any comparison result is true\n            if (_mm_movemask_epi8(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int size = l.size();\n    if (size < 2) {\n        return false;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i++) {\n        __m128i current = _mm_set1_epi32(l[i]);\n\n        for (int j = i + 1; j < size; j += 4) {\n            int remaining = size - j;\n            __m128i values;\n            if (remaining >= 4) {\n                values = _mm_loadu_si128((__m128i*)&l[j]);\n            } else {\n                int temp[4] = {0, 0, 0, 0};\n                for (int k = 0; k < remaining; k++) {\n                    temp[k] = l[j + k];\n                }\n                values = _mm_loadu_si128((__m128i*)temp);\n            }\n\n            __m128i sum = _mm_add_epi32(current, values);\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n\n            if (_mm_movemask_epi8(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i current = _mm_set1_epi32(l[i]); // Broadcast l[i] to all elements\n        for (int j = i + 1; j < n; j += 4) {\n            __m128i next;\n            if (j + 4 <= n) {\n                next = _mm_loadu_si128((__m128i*)&l[j]); // Load next 4 elements\n            } else {\n                int temp[4] = {0, 0, 0, 0};\n                for (int k = 0; k < n - j; ++k) temp[k] = l[j + k];\n                next = _mm_loadu_si128((__m128i*)temp);\n            }\n\n            __m128i sum = _mm_add_epi32(current, next); // Add current and next\n            __m128i zero = _mm_setzero_si128(); // Set zero vector\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero); // Compare sum with zero\n            if (_mm_movemask_epi8(cmp)) return true; // Check if any comparison is true\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    std::vector<int8_t> digits;\n    __m128i base_vec = _mm_set1_epi32(base); // Broadcast base to all lanes\n    __m128i zero_vec = _mm_setzero_si128();  // Zero vector for comparison\n\n    while (x > 0) {\n        __m128i x_vec = _mm_set1_epi64x(x); // Broadcast x to all lanes\n        __m128i remainder_vec = _mm_rem_epi64(x_vec, base_vec); // Compute remainder\n        __m128i digit_vec = _mm_max_epi8(remainder_vec, zero_vec); // Ensure valid digits\n\n        // Extract the remainder from the SIMD vector\n        int remainder = _mm_extract_epi16(digit_vec, 0);\n        digits.push_back(static_cast<int8_t>(remainder));\n\n        // Update x for the next iteration\n        x /= base;\n    }\n\n    // Convert digits to string representation\n    std::string result;\n    for (auto it = digits.rbegin(); it != digits.rend(); ++it) {\n        result += ('0' + *it);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    std::vector<char> result;\n    __m128i base_vec = _mm_set1_epi32(base); // Broadcast base to all lanes\n    __m128i zero_vec = _mm_setzero_si128();  // Zero vector for comparison\n\n    while (x > 0) {\n        // Extract remainder using SIMD\n        __m128i x_vec = _mm_set1_epi32(static_cast<int>(x));\n        __m128i remainder_vec = _mm_rem_epi32(x_vec, base_vec);\n\n        // Extract remainder from SIMD lanes\n        int remainder = _mm_extract_epi32(remainder_vec, 0);\n        result.push_back('0' + remainder);\n\n        // Divide x by base\n        x /= base;\n    }\n\n    // Reverse the result vector to get the correct order\n    std::reverse(result.begin(), result.end());\n\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    // Handle edge case for zero\n    if (x == 0) {\n        return \"0\";\n    }\n\n    std::vector<char> result;\n    __m128i base_vec = _mm_set1_epi64x(base); // Set base in all lanes\n    __m128i zero_vec = _mm_setzero_si128();  // Zero vector for comparisons\n\n    while (x > 0) {\n        // Compute x % base using SIMD\n        __m128i x_vec = _mm_set1_epi64x(x);\n        __m128i mod_vec = _mm_rem_epi64(x_vec, base_vec); // SIMD modulo operation\n        int64_t mod = _mm_cvtsi128_si64(mod_vec);         // Extract result from SIMD register\n\n        // Convert remainder to character and store\n        result.push_back('0' + mod);\n\n        // Compute x / base using SIMD\n        __m128i div_vec = _mm_div_epi64(x_vec, base_vec); // SIMD division operation\n        x = _mm_cvtsi128_si64(div_vec);                  // Extract result from SIMD register\n    }\n\n    // Reverse the result to get the correct order\n    std::reverse(result.begin(), result.end());\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    char buffer[64];\n    int index = 63;\n    buffer[index--] = '\\0';\n\n    __m128i base_vec = _mm_set1_epi64x(base);\n    __m128i zero_vec = _mm_setzero_si128();\n\n    while (x > 0) {\n        __m128i x_vec = _mm_set1_epi64x(x);\n        __m128i div_result = _mm_div_epi64(x_vec, base_vec);\n        __m128i mod_result = _mm_sub_epi64(x_vec, _mm_mullo_epi64(div_result, base_vec));\n\n        int64_t mod = _mm_cvtsi128_si64(mod_result);\n        buffer[index--] = '0' + mod;\n\n        x = _mm_cvtsi128_si64(div_result);\n    }\n\n    return std::string(&buffer[index + 1]);\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) {\n        return \"\"; // Invalid base\n    }\n\n    std::vector<int8_t> digits;\n    __m128i base_vec = _mm_set1_epi64x(base); // Broadcast base to all lanes\n    __m128i zero_vec = _mm_setzero_si128();   // Zero vector for comparison\n\n    while (x > 0) {\n        __m128i x_vec = _mm_set1_epi64x(x);   // Broadcast x to all lanes\n        __m128i rem_vec = _mm_rem_epi64(x_vec, base_vec); // Compute remainder\n        __m128i digit_vec = _mm_add_epi64(rem_vec, zero_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) {\n        return true;\n    }\n\n    size_t half_len = len / 2;\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* str = text.c_str();\n    const char* rev_str = str + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m128i forward = _mm_loadu_si128((__m128i*)(str + i * simd_width));\n        __m128i backward = _mm_loadu_si128((__m128i*)(rev_str - i * simd_width));\n\n        backward = _mm_shuffle_epi8(backward, _mm_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n\n        __m128i cmp = _mm_cmpeq_epi8(forward, backward);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    size_t remaining = half_len % simd_width;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (str[simd_blocks * simd_width + i] != text[len - 1 - (simd_blocks * simd_width + i)]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) {\n        return true;\n    }\n\n    size_t half_len = len / 2;\n    size_t simd_width = 16; // SSE works with 16 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* str_start = text.data();\n    const char* str_end = text.data() + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m128i start_chunk = _mm_loadu_si128((__m128i*)(str_start + i * simd_width));\n        __m128i end_chunk = _mm_loadu_si128((__m128i*)(str_end - i * simd_width));\n\n        // Reverse the end_chunk\n        __m128i reversed_end_chunk = _mm_shuffle_epi8(end_chunk, _mm_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Compare start_chunk and reversed_end_chunk\n        __m128i cmp_result = _mm_cmpeq_epi8(start_chunk, reversed_end_chunk);\n        int mask = _mm_movemask_epi8(cmp_result);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remaining bytes\n    size_t remaining_start = simd_blocks * simd_width;\n    size_t remaining_end = len - remaining_start - 1;\n\n    while (remaining_start < half_len) {\n        if (text[remaining_start] != text[remaining_end]) {\n            return false;\n        }\n        ++remaining_start;\n        --remaining_end;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) return true;\n\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t half_len = len / 2;\n    size_t aligned_len = half_len - (half_len % simd_width);\n\n    const char* str_start = text.c_str();\n    const char* str_end = text.c_str() + len - simd_width;\n\n    for (size_t i = 0; i < aligned_len; i += simd_width) {\n        __m128i start_chunk = _mm_loadu_si128((__m128i*)(str_start + i));\n        __m128i end_chunk = _mm_loadu_si128((__m128i*)(str_end - i));\n\n        // Reverse the end chunk\n        end_chunk = _mm_shuffle_epi8(end_chunk, _mm_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Compare start_chunk and reversed end_chunk\n        __m128i cmp = _mm_cmpeq_epi8(start_chunk, end_chunk);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) return false;\n    }\n\n    // Handle remaining characters\n    for (size_t i = aligned_len; i < half_len; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) {\n        return true;\n    }\n\n    size_t half_len = len / 2;\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t simd_blocks = half_len / simd_width;\n\n    const char* start_ptr = text.data();\n    const char* end_ptr = text.data() + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m128i start_vec = _mm_loadu_si128((__m128i*)(start_ptr + i * simd_width));\n        __m128i end_vec = _mm_loadu_si128((__m128i*)(end_ptr - i * simd_width));\n\n        // Reverse the end_vec\n        end_vec = _mm_shuffle_epi8(end_vec, _mm_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Compare start_vec and reversed end_vec\n        __m128i cmp = _mm_cmpeq_epi8(start_vec, end_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remaining bytes\n    for (size_t i = simd_blocks * simd_width; i < half_len; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.size();\n    if (len == 0 || len == 1) {\n        return true;\n    }\n\n    size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t half_len = len / 2;\n\n    size_t simd_blocks = half_len / simd_width;\n    size_t remainder = half_len % simd_width;\n\n    const char* start_ptr = text.data();\n    const char* end_ptr = text.data() + len - simd_width;\n\n    for (size_t i = 0; i < simd_blocks; ++i) {\n        __m128i start_vec = _mm_loadu_si128((__m128i*)(start_ptr + i * simd_width));\n        __m128i end_vec = _mm_loadu_si128((__m128i*)(end_ptr - i * simd_width));\n\n        // Reverse the end vector\n        end_vec = _mm_shuffle_epi8(end_vec, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Compare the vectors\n        __m128i cmp = _mm_cmpeq_epi8(start_vec, end_vec);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remainder\n    for (size_t i = 0; i < remainder; ++i) {\n        if (text[i + simd_blocks * simd_width] != text[len - 1 - i - simd_blocks * simd_width]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t n = s.size();\n    std::string out(n, ' ');\n\n    __m128i shift = _mm_set1_epi8(5);          // Shift value (5)\n    __m128i a_val = _mm_set1_epi8('a');       // ASCII value of 'a'\n    __m128i z_val = _mm_set1_epi8('z');       // ASCII value of 'z'\n    __m128i range = _mm_set1_epi8(26);        // Alphabet range (26)\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]); // Load 16 characters\n        __m128i shifted = _mm_sub_epi8(chars, shift);     // Subtract shift value\n        __m128i mask = _mm_cmplt_epi8(shifted, a_val);    // Check if below 'a'\n        __m128i adjusted = _mm_add_epi8(shifted, range);  // Add 26 to wrap around\n        __m128i result = _mm_blendv_epi8(shifted, adjusted, mask); // Select wrapped or original\n        _mm_storeu_si128((__m128i*)&out[i], result);      // Store result\n    }\n\n    for (; i < n; i++) {\n        char c = s[i];\n        char decoded = c - 5;\n        if (decoded < 'a') {\n            decoded += 26;\n        }\n        out[i] = decoded;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    // Process 16 characters at a time (SSE registers are 128 bits, 16 bytes)\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        // Load 16 characters into an SSE register\n        __m128i chars = _mm_loadu_si128((__m128i*)(s.data() + i));\n\n        // Subtract 5 from each character\n        __m128i shifted = _mm_sub_epi8(chars, _mm_set1_epi8(5));\n\n        // Wrap around the alphabet for characters less than 'a'\n        __m128i mask = _mm_cmplt_epi8(shifted, _mm_set1_epi8('a'));\n        __m128i wrapped = _mm_add_epi8(shifted, _mm_and_si128(mask, _mm_set1_epi8(26)));\n\n        // Store the result back into the output string\n        _mm_storeu_si128((__m128i*)(out.data() + i), wrapped);\n    }\n\n    // Process remaining characters sequentially\n    for (; i < len; ++i) {\n        char shifted = s[i] - 5;\n        if (shifted < 'a') {\n            shifted += 26;\n        }\n        out[i] = shifted;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m128i shift = _mm_set1_epi8(5); // Shift value\n    const __m128i a_val = _mm_set1_epi8('a'); // ASCII value of 'a'\n    const __m128i z_val = _mm_set1_epi8('z'); // ASCII value of 'z'\n    const __m128i alphabet_size = _mm_set1_epi8(26); // Alphabet size\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]); // Load 16 characters\n        __m128i shifted = _mm_sub_epi8(chars, shift); // Subtract shift value\n        __m128i mask = _mm_cmpgt_epi8(shifted, a_val); // Check if shifted value is less than 'a'\n        __m128i wrapped = _mm_add_epi8(shifted, alphabet_size); // Wrap around alphabet\n        __m128i result = _mm_blendv_epi8(wrapped, shifted, mask); // Blend wrapped and shifted values\n        _mm_storeu_si128((__m128i*)&out[i], result); // Store result\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        char decoded = (c - 5 < 'a') ? (c - 5 + 26) : (c - 5);\n        out[i] = decoded;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    const __m128i shift = _mm_set1_epi8(5); // Shift value (5)\n    const __m128i a_val = _mm_set1_epi8('a'); // ASCII value of 'a'\n    const __m128i z_val = _mm_set1_epi8('z'); // ASCII value of 'z'\n    const __m128i alphabet_size = _mm_set1_epi8(26); // Alphabet size (26)\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)&s[i]); // Load 16 characters\n        __m128i shifted = _mm_sub_epi8(chars, shift); // Subtract the shift value\n        __m128i mask = _mm_cmplt_epi8(shifted, a_val); // Check if shifted value is less than 'a'\n        __m128i wrapped = _mm_add_epi8(shifted, alphabet_size); // Wrap around by adding 26\n        __m128i result = _mm_blendv_epi8(shifted, wrapped, mask); // Select wrapped or shifted value\n        _mm_storeu_si128((__m128i*)&out[i], result); // Store the result\n    }\n\n    // Handle remaining characters\n    for (; i < len; i++) {\n        char shifted = s[i] - 5;\n        if (shifted < 'a') {\n            shifted += 26;\n        }\n        out[i] = shifted;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.length();\n    std::string out(len, ' ');\n\n    // Process 16 characters at a time using SSE2\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        // Load 16 characters into an SSE register\n        __m128i chars = _mm_loadu_si128((__m128i*)(s.data() + i));\n\n        // Subtract 5 from each character\n        __m128i shifted = _mm_sub_epi8(chars, _mm_set1_epi8(5));\n\n        // Wrap around the alphabet ('a' to 'z')\n        __m128i a = _mm_set1_epi8('a');\n        __m128i z = _mm_set1_epi8('z');\n        __m128i mask = _mm_cmpgt_epi8(shifted, z);\n        shifted = _mm_add_epi8(shifted, _mm_and_si128(mask, _mm_set1_epi8(-26)));\n\n        mask = _mm_cmplt_epi8(shifted, a);\n        shifted = _mm_add_epi8(shifted, _mm_and_si128(mask, _mm_set1_epi8(26)));\n\n        // Store the result back into the output string\n        _mm_storeu_si128((__m128i*)(out.data() + i), shifted);\n    }\n\n    // Process remaining characters sequentially\n    for (; i < len; ++i) {\n        char c = s[i];\n        char decoded = c - 5;\n        if (decoded < 'a') {\n            decoded += 26;\n        }\n        out[i] = decoded;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    if (size == 0) return true;\n\n    __m128i threshold = _mm_set1_epi32(t); // Set threshold value in all SIMD lanes\n    size_t i = 0;\n\n    // Process 4 integers at a time\n    for (; i + 4 <= size; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers into SIMD register\n        __m128i cmp = _mm_cmpgt_epi32(values, threshold); // Compare values > threshold\n        if (_mm_movemask_epi8(cmp)) { // Check if any comparison result is true\n            return false;\n        }\n    }\n\n    // Process remaining integers\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    __m128i threshold = _mm_set1_epi32(t); // Set threshold value in all lanes of the SIMD register\n\n    int i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers from the vector\n        __m128i cmp = _mm_cmpgt_epi32(values, threshold);  // Compare values > threshold\n        if (_mm_movemask_epi8(cmp) != 0) {                 // Check if any comparison result is true\n            return false;\n        }\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    __m128i threshold = _mm_set1_epi32(t); // Set threshold in all 4 lanes\n\n    int i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold); // Compare each element with threshold\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) { // Check if all comparisons are true\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    __m128i threshold = _mm_set1_epi32(t); // Set threshold in all 4 lanes\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold); // Compare vec > threshold\n        if (_mm_movemask_epi8(cmp)) { // If any element is greater, return false\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    size_t size = l.size();\n    __m128i threshold = _mm_set1_epi32(t); // Set threshold value in all lanes\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i*)&l[i]); // Load 4 integers\n        __m128i cmp = _mm_cmplt_epi32(values, threshold);  // Compare values < threshold\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {            // Check if all comparisons are true\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    __m128i open_count = _mm_setzero_si128();\n    __m128i close_count = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n\n        __m128i is_open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i is_close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n\n        open_count = _mm_add_epi8(open_count, is_open);\n        close_count = _mm_add_epi8(close_count, is_close);\n\n        __m128i balance = _mm_sub_epi8(open_count, close_count);\n        if (_mm_movemask_epi8(_mm_cmplt_epi8(balance, zero))) {\n            return false;\n        }\n    }\n\n    int total_open = 0, total_close = 0;\n    alignas(16) char open_buf[16], close_buf[16];\n    _mm_store_si128((__m128i*)open_buf, open_count);\n    _mm_store_si128((__m128i*)close_buf, close_count);\n\n    for (int j = 0; j < 16; ++j) {\n        total_open += open_buf[j];\n        total_close += close_buf[j];\n        if (total_open < total_close) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') total_open++;\n        else if (brackets[i] == '>') total_close++;\n        if (total_open < total_close) return false;\n    }\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.size();\n    if (len == 0) return true;\n\n    __m128i opening = _mm_set1_epi8('<');\n    __m128i closing = _mm_set1_epi8('>');\n    int balance = 0;\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        __m128i is_opening = _mm_cmpeq_epi8(chunk, opening);\n        __m128i is_closing = _mm_cmpeq_epi8(chunk, closing);\n\n        int opening_count = _mm_popcnt_u32(_mm_movemask_epi8(is_opening));\n        int closing_count = _mm_popcnt_u32(_mm_movemask_epi8(is_closing));\n\n        balance += opening_count - closing_count;\n        if (balance < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') balance++;\n        else if (brackets[i] == '>') balance--;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    __m128i opening_count = _mm_setzero_si128();\n    __m128i closing_count = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        __m128i is_open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i is_close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n\n        opening_count = _mm_add_epi8(opening_count, is_open);\n        closing_count = _mm_add_epi8(closing_count, is_close);\n\n        __m128i balance = _mm_sub_epi8(opening_count, closing_count);\n        if (_mm_movemask_epi8(_mm_cmplt_epi8(balance, zero))) {\n            return false;\n        }\n    }\n\n    int open_sum = 0, close_sum = 0;\n    alignas(16) char open_buffer[16], close_buffer[16];\n    _mm_store_si128((__m128i*)open_buffer, opening_count);\n    _mm_store_si128((__m128i*)close_buffer, closing_count);\n\n    for (int j = 0; j < 16; ++j) {\n        open_sum += open_buffer[j];\n        close_sum += close_buffer[j];\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') open_sum++;\n        else if (brackets[i] == '>') close_sum++;\n\n        if (close_sum > open_sum) return false;\n    }\n\n    return open_sum == close_sum;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t len = brackets.size();\n    if (len == 0) return true;\n\n    __m128i open_count = _mm_setzero_si128();\n    __m128i close_count = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        __m128i is_open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i is_close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n\n        open_count = _mm_add_epi8(open_count, _mm_and_si128(is_open, ones));\n        close_count = _mm_add_epi8(close_count, _mm_and_si128(is_close, ones));\n\n        __m128i balance = _mm_sub_epi8(open_count, close_count);\n        if (_mm_movemask_epi8(_mm_cmplt_epi8(balance, _mm_setzero_si128())) != 0) {\n            return false;\n        }\n    }\n\n    int total_open = 0, total_close = 0;\n    alignas(16) char open_buffer[16], close_buffer[16];\n    _mm_store_si128((__m128i*)open_buffer, open_count);\n    _mm_store_si128((__m128i*)close_buffer, close_count);\n\n    for (int j = 0; j < 16; ++j) {\n        total_open += open_buffer[j];\n        total_close += close_buffer[j];\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') total_open++;\n        else if (brackets[i] == '>') total_close++;\n\n        if (total_close > total_open) return false;\n    }\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    __m128i opening = _mm_set1_epi8('<');\n    __m128i closing = _mm_set1_epi8('>');\n    int balance = 0;\n\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(brackets.data() + i));\n        __m128i is_opening = _mm_cmpeq_epi8(chunk, opening);\n        __m128i is_closing = _mm_cmpeq_epi8(chunk, closing);\n\n        int opening_count = _mm_popcnt_u32(_mm_movemask_epi8(is_opening));\n        int closing_count = _mm_popcnt_u32(_mm_movemask_epi8(is_closing));\n\n        balance += opening_count - closing_count;\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') balance++;\n        else if (brackets[i] == '>') balance--;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    size_t n = brackets.size();\n    if (n == 0) return true;\n\n    // Ensure the string length is a multiple of 16 for SIMD processing\n    size_t padded_length = (n + 15) & ~15;\n    std::string padded_brackets = brackets;\n    padded_brackets.resize(padded_length, ')'); // Pad with closing brackets\n\n    __m128i opening_mask = _mm_set1_epi8('(');\n    __m128i closing_mask = _mm_set1_epi8(')');\n    int balance = 0;\n\n    for (size_t i = 0; i < padded_length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(padded_brackets.data() + i));\n        __m128i is_opening = _mm_cmpeq_epi8(chunk, opening_mask);\n        __m128i is_closing = _mm_cmpeq_epi8(chunk, closing_mask);\n\n        // Count opening brackets\n        int opening_count = _mm_movemask_epi8(is_opening);\n        opening_count = __builtin_popcount(opening_count);\n\n        // Count closing brackets\n        int closing_count = _mm_movemask_epi8(is_closing);\n        closing_count = __builtin_popcount(closing_count);\n\n        balance += opening_count - closing_count;\n\n        // If balance goes negative, brackets are incorrect\n        if (balance < 0) return false;\n    }\n\n    // Brackets are correct if balance is zero\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n    int min_balance = 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i counts = zero;\n        int limit = std::min(16, n - i);\n\n        for (int j = 0; j < limit; ++j) {\n            if (brackets[i + j] == '(') {\n                balance++;\n            } else if (brackets[i + j] == ')') {\n                balance--;\n            }\n            min_balance = std::min(min_balance, balance);\n        }\n\n        if (min_balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n    int min_balance = 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i counts = zero;\n        int limit = std::min(16, n - i);\n        char buffer[16] = {0};\n\n        for (int j = 0; j < limit; ++j) {\n            buffer[j] = brackets[i + j];\n        }\n\n        __m128i chars = _mm_loadu_si128((__m128i*)buffer);\n        __m128i open_mask = _mm_cmpeq_epi8(chars, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(chars, _mm_set1_epi8(')'));\n\n        __m128i open_counts = _mm_sub_epi8(open_mask, close_mask);\n        counts = _mm_add_epi8(counts, open_counts);\n\n        alignas(16) char result[16];\n        _mm_store_si128((__m128i*)result, counts);\n\n        for (int j = 0; j < limit; ++j) {\n            balance += result[j];\n            if (balance < min_balance) {\n                min_balance = balance;\n            }\n        }\n    }\n\n    return balance == 0 && min_balance >= 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    // Ensure the string length is a multiple of 16 for SIMD processing\n    int padded_length = (n + 15) & ~15;\n    std::string padded_brackets = brackets;\n    padded_brackets.resize(padded_length, ')'); // Pad with closing brackets\n\n    __m128i opening = _mm_set1_epi8('('); // Vector for '('\n    __m128i closing = _mm_set1_epi8(')'); // Vector for ')'\n\n    int balance = 0;\n\n    for (int i = 0; i < padded_length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&padded_brackets[i]);\n\n        // Compare each character in the chunk to '(' and ')'\n        __m128i is_opening = _mm_cmpeq_epi8(chunk, opening);\n        __m128i is_closing = _mm_cmpeq_epi8(chunk, closing);\n\n        // Count '(' and ')' in the chunk\n        int opening_count = _mm_popcnt_u32(_mm_movemask_epi8(is_opening));\n        int closing_count = _mm_popcnt_u32(_mm_movemask_epi8(is_closing));\n\n        // Update balance\n        balance += opening_count - closing_count;\n\n        // If balance goes negative, brackets are incorrect\n        if (balance < 0) return false;\n    }\n\n    // Check final balance\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n == 0) return true;\n\n    // Ensure the string length is a multiple of 16 for SIMD processing\n    int padded_size = (n + 15) & ~15;\n    std::string padded_brackets = brackets;\n    padded_brackets.resize(padded_size, ')'); // Pad with closing brackets\n\n    __m128i opening_mask = _mm_set1_epi8('(');\n    __m128i closing_mask = _mm_set1_epi8(')');\n    int balance = 0;\n\n    for (int i = 0; i < padded_size; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(padded_brackets.data() + i));\n        __m128i is_opening = _mm_cmpeq_epi8(chunk, opening_mask);\n        __m128i is_closing = _mm_cmpeq_epi8(chunk, closing_mask);\n\n        // Convert masks to counts\n        int opening_count = _mm_popcnt_u32(_mm_movemask_epi8(is_opening));\n        int closing_count = _mm_popcnt_u32(_mm_movemask_epi8(is_closing));\n\n        balance += opening_count - closing_count;\n        if (balance < 0) return false; // Early exit if balance goes negative\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) {\n        return {}; // Return empty vector for constant or empty polynomial\n    }\n\n    std::vector<float> result(n - 1);\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f); // Multiplier for coefficients\n\n    size_t i = 0;\n    for (; i + 4 <= n - 1; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]); // Load next 4 coefficients\n        __m128 indices = _mm_set_ps(i + 4.0f, i + 3.0f, i + 2.0f, i + 1.0f); // Indices for multiplication\n        __m128 result_vec = _mm_mul_ps(coeffs, indices); // Multiply coefficients by indices\n        _mm_storeu_ps(&result[i], result_vec); // Store result\n    }\n\n    // Handle remaining coefficients\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) {\n        return {}; // Derivative of a constant is zero\n    }\n\n    std::vector<float> result(n - 1);\n\n    // Process in chunks of 4 using SSE\n    size_t i = 0;\n    for (; i + 4 <= n - 1; i += 4) {\n        __m128 indices = _mm_set_ps(i + 3, i + 2, i + 1, i); // Set indices {i, i+1, i+2, i+3}\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);            // Load coefficients {xs[i+1], xs[i+2], xs[i+3], xs[i+4]}\n        __m128 product = _mm_mul_ps(indices, coeffs);        // Multiply indices with coefficients\n        _mm_storeu_ps(&result[i], product);                 // Store the result\n    }\n\n    // Process remaining elements\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // No derivative for constant or empty polynomial\n\n    std::vector<float> result(n - 1);\n\n    // Prepare the multiplier vector [1, 2, 3, 4]\n    __m128 multiplier = _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= n - 1; i += 4) {\n        // Load 4 coefficients from xs\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n\n        // Multiply coefficients by their respective powers\n        __m128 product = _mm_mul_ps(coeffs, multiplier);\n\n        // Store the result\n        _mm_storeu_ps(&result[i], product);\n\n        // Increment multiplier for the next batch\n        multiplier = _mm_add_ps(multiplier, _mm_set1_ps(4.0f));\n    }\n\n    // Handle remaining coefficients\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // No derivative for constant or empty polynomial\n\n    std::vector<float> result(n - 1);\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f); // Initial multipliers for SIMD\n\n    for (size_t i = 0; i < n - 1; i += 4) {\n        // Load coefficients into SIMD register\n        __m128 coeffs = _mm_loadu_ps(&xs[i]);\n\n        // Multiply coefficients by their respective powers\n        __m128 product = _mm_mul_ps(coeffs, multiplier);\n\n        // Store the result back into the output vector\n        _mm_storeu_ps(&result[i], product);\n\n        // Update multipliers for the next set of coefficients\n        multiplier = _mm_add_ps(multiplier, _mm_set1_ps(4.0f));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {}; // No derivative for constant or empty polynomial\n\n    std::vector<float> result(n - 1);\n\n    // Prepare the multiplier vector [1, 2, 3, 4]... up to n-1\n    std::vector<float> multipliers(n - 1);\n    for (size_t i = 0; i < n - 1; ++i) {\n        multipliers[i] = static_cast<float>(i + 1);\n    }\n\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_chunks = (n - 1) / simd_width;\n\n    for (size_t i = 0; i < simd_chunks * simd_width; i += simd_width) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]); // Load coefficients xs[1..4], xs[5..8], etc.\n        __m128 mults = _mm_loadu_ps(&multipliers[i]); // Load multipliers [1,2,3,4], [5,6,7,8], etc.\n        __m128 prod = _mm_mul_ps(coeffs, mults); // Multiply element-wise\n        _mm_storeu_ps(&result[i], prod); // Store the result\n    }\n\n    // Handle remaining elements if n-1 is not divisible by simd_width\n    for (size_t i = simd_chunks * simd_width; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * multipliers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int length = s.length();\n    int count = 0;\n\n    // Process 16 characters at a time using SSE2\n    __m128i vowel_mask = _mm_set_epi8('u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A', 'u', 'o', 'i', 'e', 'a', 'U');\n    __m128i y_mask = _mm_set_epi8('y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y');\n    for (int i = 0; i < length; i+=16)\n    {\n    }\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int length = s.size();\n    int count = 0;\n\n    // Process in chunks of 16 characters using SSE\n    __m128i vowel_mask = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0);\n    __m128i y_mask = _mm_setr_epi8('y', 'Y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    int i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i cmp_vowels = _mm_cmpeq_epi8(chunk, vowel_mask);\n        __m128i cmp_y = _mm_cmpeq_epi8(chunk, y_mask);\n\n        int vowel_matches = _mm_movemask_epi8(cmp_vowels);\n        int y_matches = _mm_movemask_epi8(cmp_y);\n        count+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+="}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int len = s.length();\n    int count = 0;\n\n    // Process in chunks of 16 bytes using SSE\n    __m128i vowel_mask = _mm_set_epi8('u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A', 'u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A');\n    __m128i y_mask = _mm_set_epi8('Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y', 'y', 'Y"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    __m128i vowel_set1 = _mm_loadu_si128((__m128i*)vowels); // Load first 16 vowels\n    int len = s.length();\n    int count = 0;\n\n    int i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]); // Load 16 characters from the string\n        for (int j = 0; j < 10; j++) {\n            __m128i match = _mm_cmpeq_epi8(chunk, vowel_set1[j]);\n            count += __builtin_popcount(_mm_movemask_epi8(match));\n        }\n    }\n    // Handle remaining characters\n    for (; i < len; i++) {\n        if (vowels.find(s[i]) != std::string::npos) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    const char vowels[] = \"aeiouAEIOU\";\n    const char y_vowel[] = \"yY\";\n    int length = s.size();\n    int count = 0;\n\n    __m128i vowel_mask = _mm_set_epi8('u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A', 0, 0, 0, 0, 0, 0);\n    __m128i y_mask = _mm_set_epi8('y', 'Y', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < length;+="}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int n = s.size();\n    const char* str = s.c_str();\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i A = _mm_set1_epi8('A');        // ASCII value of 'A'\n    __m128i Z = _mm_set1_epi8('Z');        // ASCII value of 'Z'\n\n    int i = 0;\n    for (; i + 16 <= n; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i)); // Load 16 characters\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chars, A), _mm_cmplt_epi8(chars, Z)); // Check if characters are uppercase\n        __m128i masked_chars = _mm_and_si128(chars, is_upper); // Mask non-uppercase characters\n        sum_vec = _mm_add_epi8(sum_vec, masked_chars); // Add ASCII values of uppercase characters\n    }\n\n    // Sum the values in the sum_vec\n    alignas(16) char temp[16];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    int sum = 0;\n    for (int j = 0; j < 16; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle remaining characters\n    for (; i < n; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i A = _mm_set1_epi8('A');\n    __m128i Z = _mm_set1_epi8('Z');\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chars, A), _mm_cmplt_epi8(chars, Z));\n        __m128i masked = _mm_and_si128(chars, is_upper);\n        sum_vec = _mm_add_epi8(sum_vec, masked);\n    }\n\n    char temp[16];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    int sum = 0;\n    for (int j = 0; j < 16; j++) sum += temp[j];\n    for (; i < len; i++) if (std::isupper(str[i])) sum+=str[i];\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const size_t len = s.size();\n    const size_t simd_width = 16; // SSE processes 16 bytes at a time\n    size_t i = 0;\n    int sum = 0;\n\n    __m128i ascii_A = _mm_set1_epi8('A');\n    __m128i ascii_Z = _mm_set1_epi8('Z');\n    __m128i total_sum = _mm_setzero_si128();\n\n    // Process 16 bytes at a time\n    for (; i + simd_width <= len; i += simd_width) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i mask_upper = _mm_and_si128(\n            _mm_cmpge_epi8(chunk, ascii_A), // chunk >= 'A'\n            _mm_cmple_epi8(chunk, ascii_Z)  // chunk <= 'Z'\n        );\n        __m128i filtered = _mm_and_si128(chunk, mask_upper);\n        total_sum = _mm_add_epi8(total_sum, filtered);\n    }\n\n    // Sum the 16 packed bytes in total_sum\n    alignas(16) char buffer[simd_width];\n    _mm_store_si128((__m128i*)buffer, total_sum);\n    for (int j = 0; j < simd_width; ++j) {\n        sum += buffer[j];\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i)); // Load 16 characters\n        __m128i upper_bound = _mm_set1_epi8('Z'); // ASCII 'Z'\n        __m128i lower_bound = _mm_set1_epi8('A'); // ASCII 'A'\n\n        __m128i is_upper = _mm_and_si128(\n            _mm_cmpgt_epi8(chars, _mm_sub_epi8(lower_bound, _mm_set1_epi8(1))),\n            _mm_cmplt_epi8(chars, _mm_add_epi8(upper_bound, _mm_set1_epi8(1)))\n        );\n\n        __m128i masked_chars = _mm_and_si128(chars, is_upper); // Mask non-uppercase characters\n        sum = _mm_add_epi8(sum, masked_chars); // Add ASCII values of uppercase characters\n    }\n\n    // Horizontal sum of the SIMD register\n    alignas(16) char buffer[16];\n    _mm_store_si128((__m128i*)buffer, sum);\n    int total = 0;\n    for (int j = 0; j < 16; ++j) {\n        total += buffer[j];\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            total += str[i];\n        }\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    const char* str = s.c_str();\n    size_t len = s.size();\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i)); // Load 16 characters\n        __m128i upper_mask = _mm_and_si128(_mm_cmpgt_epi8(chars, _mm_set1_epi8('A' - 1)), \n                                           _mm_cmplt_epi8(chars, _mm_set1_epi8('Z' + 1))); // Mask for uppercase letters\n        __m128i filtered = _mm_and_si128(chars, upper_mask); // Filter uppercase letters\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(filtered, _mm_setzero_si128())); // Add ASCII values\n    }\n\n    // Sum remaining characters\n    int scalar_sum = 0;\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            scalar_sum += str[i];\n        }\n    }\n\n    // Extract SIMD sum\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, sum);\n    return result[0] + result[2] + scalar_sum; // Combine SIMD and scalar sums\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int n = arr.size();\n    int min_even = INT_MAX;\n    int min_index = -1;\n\n    __m128i min_even_vec = _mm_set1_epi32(INT_MAX);\n    __m128i min_index_vec = _mm_set1_epi32(-1);\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0); // Initialize indices for the first 4 elements\n    __m128i increment = _mm_set1_epi32(4); // Increment for indices\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i values;\n        if (i + 4 <= n) {\n            values = _mm_loadu_si128((__m128i*)&arr[i]);\n        } else {\n            int temp[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(values, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i even_values = _mm_or_si128(_mm_and_si128(even_mask, values), _mm_andnot_si128(even_mask, _mm_set1_epi32(INT_MAX)));\n\n        __m128i cmp_mask = _mm_cmplt_epi32(even_values, min_even_vec);\n        min_even_vec = _mm_or_si128(_mm_and_si128(cmp_mask, even_values), _mm_andnot_si128(cmp_mask, min_even_vec));\n        min_index_vec = _mm_or_si128(_mm_and_si128(cmp_mask, indices), _mm_andnot_si128(cmp_mask, min_index_vec));\n\n        indices = _mm_add_epi32(indices, increment);\n    }\n\n    int min_even_arr[4], min_index_arr[4];\n    _mm_storeu_si128((__m128i*)min_even_arr, min_even_vec);\n    _mm_storeu_si128((__m128i*)min_index_arr, min_index_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        if (min_even_arr[i] < min_even) {\n            min_even = min_even_arr[i];\n            min_index = min_index_arr[i];\n        }\n    }\n\n    if (min_even == INT_MAX) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int n = arr.size();\n    int min_even_value = INT_MAX;\n    int min_even_index = -1;\n\n    __m128i min_value_vec = _mm_set1_epi32(INT_MAX);\n    __m128i min_index_vec = _mm_set1_epi32(-1);\n    __m128i indices_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i increment_vec = _mm_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i values_vec;\n        if (i + 4 <= n) {\n            values_vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        } else {\n            int temp[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values_vec = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(values_vec, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i filtered_values = _mm_or_si128(_mm_and_si128(even_mask, values_vec), _mm_andnot_si128(even_mask, _mm_set1_epi32(INT_MAX)));\n\n        __m128i cmp_mask = _mm_cmplt_epi32(filtered_values, min_value_vec);\n        min_value_vec = _mm_or_si128(_mm_and_si128(cmp_mask, filtered_values), _mm_andnot_si128(cmp_mask, min_value_vec));\n        min_index_vec = _mm_or_si128(_mm_and_si128(cmp_mask, indices_vec), _mm_andnot_si128(cmp_mask, min_index_vec));\n\n        indices_vec = _mm_add_epi32(indices_vec, increment_vec);\n    }\n\n    int min_values[4], min_indices[4];\n    _mm_storeu_si128((__m128i*)min_values, min_value_vec);\n    _mm_storeu_si128((__m128i*)min_indices, min_index_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        if (min_values[i] < min_even_value) {\n            min_even_value = min_values[i];\n            min_even_index = min_indices[i];\n        }\n    }\n\n    if (min_even_value == INT_MAX) {\n        return {};\n    }\n\n    return {min_even_value, min_even_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    int n = arr.size();\n    int min_even = INT_MAX;\n    int min_index = -1;\n\n    __m128i min_values = _mm_set1_epi32(INT_MAX);\n    __m128i min_indices = _mm_set1_epi32(-1);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i increment = _mm_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i values;\n        if (i + 4 <= n) {\n            values = _mm_loadu_si128((__m128i*)&arr[i]);\n        } else {\n            int temp[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(values, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i filtered_values = _mm_or_si128(_mm_and_si128(even_mask, values), _mm_andnot_si128(even_mask, _mm_set1_epi32(INT_MAX)));\n\n        __m128i cmp_mask = _mm_cmplt_epi32(filtered_values, min_values);\n        min_values = _mm_or_si128(_mm_and_si128(cmp_mask, filtered_values), _mm_andnot_si128(cmp_mask, min_values));\n        min_indices = _mm_or_si128(_mm_and_si128(cmp_mask, indices), _mm_andnot_si128(cmp_mask, min_indices));\n\n        indices = _mm_add_epi32(indices, increment);\n    }\n\n    int min_values_array[4];\n    int min_indices_array[4];\n    _mm_storeu_si128((__m128i*)min_values_array, min_values);\n    _mm_storeu_si128((__m128i*)min_indices_array, min_indices);\n\n    for (int i = 0; i < 4; ++i) {\n        if (min_values_array[i] < min_even) {\n            min_even = min_values_array[i];\n            min_index = min_indices_array[i];\n        }\n    }\n\n    if (min_even == INT_MAX) {\n        return {};\n    }\n\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <emmintrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int INF = std::numeric_limits<int>::max();\n\n    __m128i min_val = _mm_set1_epi32(INF);\n    __m128i min_idx = _mm_set1_epi32(INF);\n    __m128i idx_increment = _mm_set1_epi32(4);\n\n    __m128i current_idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i values;\n        if (i + 4 <= n) {\n            values = _mm_loadu_si128((__m128i*)&arr[i]);\n        } else {\n            int temp[4] = {INF, INF, INF, INF};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(values, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i even_values = _mm_or_si128(_mm_and_si128(even_mask, values), _mm_andnot_si128(even_mask, _mm_set1_epi32(INF)));\n\n        __m128i cmp_mask = _mm_cmplt_epi32(even_values, min_val);\n        min_val = _mm_min_epi32(min_val, even_values);\n        min_idx = _mm_or_si128(_mm_and_si128(cmp_mask, current_idx), _mm_andnot_si128(cmp_mask, min_idx));\n\n        current_idx = _mm_add_epi32(current_idx, idx_increment);\n    }\n\n    alignas(16) int min_val_arr[4];\n    alignas(16) int min_idx_arr[4];\n    _mm_store_si128((__m128i*)min_val_arr, min_val);\n    _mm_store_si128((__m128i*)min_idx_arr, min_idx);\n\n    int final_min_val = INF;\n    int final_min_idx = INF;\n    for (int i = 0; i < 4; ++i) {\n        if (min_val_arr[i] < final_min_val) {\n            final_min_val = min_val_arr[i];\n            final_min_idx = min_idx_arr[i];\n        }\n    }\n\n    if (final_min_val == INF) {\n        return {};\n    }\n\n    return {final_min_val, final_min_idx};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int n = arr.size();\n    const int simd_width = 4; // SSE processes 4 integers at a time\n    const int max_int = std::numeric_limits<int>::max();\n\n    __m128i min_values = _mm_set1_epi32(max_int); // Initialize minimum values to max_int\n    __m128i min_indices = _mm_set1_epi32(-1);    // Initialize indices to -1\n    __m128i current_indices = _mm_set_epi32(3, 2, 1, 0); // Indices for the current SIMD lane\n    __m128i increment = _mm_set1_epi32(simd_width);      // Increment for indices\n\n    for (int i = 0; i < n; i += simd_width) {\n        __m128i values;\n        if (i + simd_width <= n) {\n            values = _mm_loadu_si128((__m128i*)&arr[i]); // Load 4 integers\n        } else {\n            int temp[simd_width] = {max_int, max_int, max_int, max_int};\n            for (int j = 0; j < n - i; ++j) {\n                temp[j] = arr[i + j];\n            }\n            values = _mm_loadu_si128((__m128i*)temp);\n        }\n\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(values, _mm_set1_epi32(1)), _mm_setzero_si128()); // Check for even numbers\n        __m128i filtered_values = _mm_or_si128(_mm_and_si128(even_mask, values), _mm_andnot_si128(even_mask, _mm_set1_epi32(max_int))); // Replace odd numbers with max_int\n\n        __m128i cmp_mask = _mm_cmplt_epi32(filtered_values, min_values); // Compare current values with min_values\n        min_values = _mm_or_si128(_mm_and_si128(cmp_mask, filtered_values), _mm_andnot_si128(cmp_mask, min_values)); // Update min_values\n        min_indices = _mm_or_si128(_mm_and_si128(cmp_mask, current_indices), _mm_andnot_si128(cmp_mask, min_indices)); // Update min_indices\n\n        current_indices = _mm_add_epi32(current_indices, increment); // Increment indices\n    }\n\n    // Extract the minimum value and its index from the SIMD registers\n    int min_val_arr[simd_width], min_idx_arr[simd_width];\n    _mm_storeu_si128((__m128i*)min_val_arr, min_values);\n    _mm_storeu_si128((__m128i*)min_idx_arr, min_indices);\n\n    int final_min_value = max_int;\n    int final_min_index = -1;\n    for (int i = 0; i < simd_width; ++i) {\n        if (min_val_arr[i] < final_min_value) {\n            final_min_value = min_val_arr[i];\n            final_min_index = min_idx_arr[i];\n        }\n    }\n\n    if (final_min_value == max_int) {\n        return {};\n    }\n\n    return {final_min_value, final_min_index};\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <unordered_map>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Prepare data for SIMD processing\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    int max_result = -1;\n\n    // Step 3: Process in chunks of 4 using SSE\n    __m128i max_val = _mm_set1_epi32(-1); // Initialize max_val to -1\n    for (int i = 0; i < n; i += 4) {\n        // Load keys and values into SIMD registers\n        __m128i key_vec = _mm_set_epi32(\n            i + 3 < n ? keys[i + 3] : 0,\n            i + 2 < n ? keys[i + 2] : 0,\n            i + 1 < n ? keys[i + 1] : 0,\n            keys[i]\n        );\n        __m128i value_vec = _mm_set_epi32(\n            i + 3 < n ? values[i + 3] : 0,\n            i + 2 < n ? values[i + 2] : 0,\n            i + 1 < n ? values[i + 1] : 0,\n            values[i]\n        );\n\n        // Compare values >= keys\n        __m128i cmp_mask = _mm_cmpge_epi32(value_vec, key_vec);\n\n        // Mask keys with comparison result\n        __m128i valid_keys = _mm_and_si128(cmp_mask, key_vec);\n\n        // Find max valid key\n        max_val = _mm_max_epi32(max_val, valid_keys);\n    }\n\n    // Step 4: Extract max value from SIMD register\n    int results[4];\n    _mm_storeu_si128((__m128i*)results, max_val);\n    for (int i = 0; i < 4; ++i) {\n        if (results[i] > max_result) {\n            max_result = results[i];\n        }\n    }\n\n    return max_result > 0 ? max_result : -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <unordered_map>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    int result = -1;\n\n    // Step 3: Use SSE to find the greatest integer satisfying the condition\n    __m128i max_result = _mm_set1_epi32(-1); // Initialize max_result to -1\n    __m128i zero = _mm_set1_epi32(0);\n\n    for (int i = 0; i < n; i += 4) {\n        // Load keys and values into SIMD registers\n        __m128i key_vec = _mm_set_epi32(\n            i + 3 < n ? keys[i + 3] : 0,\n            i + 2 < n ? keys[i + 2] : 0,\n            i + 1 < n ? keys[i + 1] : 0,\n            keys[i]\n        );\n        __m128i value_vec = _mm_set_epi32(\n            i + 3 < n ? values[i + 3] : 0,\n            i + 2 < n ? values[i + 2] : 0,\n            i + 1 < n ? values[i + 1] : 0,\n            values[i]\n        );\n\n        // Compare values >= keys\n        __m128i cmp_mask = _mm_cmpge_epi32(value_vec, key_vec);\n\n        // Mask out invalid keys (keys <= 0)\n        __m128i valid_mask = _mm_cmpgt_epi32(key_vec, zero);\n        cmp_mask = _mm_and_si128(cmp_mask, valid_mask);\n\n        // Update max_result\n        max_result = _mm_max_epi32(\n            max_result,\n            _mm_and_si128(cmp_mask, key_vec)\n        );\n    }\n\n    // Extract the maximum value from max_result\n    int max_values[4];\n    _mm_storeu_si128((__m128i*)max_values, max_result);\n    for (int i = 0; i < 4; ++i) {\n        if (max_values[i] > result) {\n            result = max_values[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <unordered_map>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    int max_result = -1;\n\n    // Step 3: Use SIMD to process keys and values in parallel\n    __m128i max_result_vec = _mm_set1_epi32(-1); // Initialize max result vector with -1\n    for (int i = 0; i < n; i += 4) {\n        // Load keys and values into SIMD registers\n        __m128i keys_vec = _mm_set_epi32(\n            i + 3 < n ? keys[i + 3] : 0,\n            i + 2 < n ? keys[i + 2] : 0,\n            i + 1 < n ? keys[i + 1] : 0,\n            i < n ? keys[i] : 0\n        );\n\n        __m128i values_vec = _mm_set_epi32(\n            i + 3 < n ? values[i + 3] : 0,\n            i + 2 < n ? values[i + 2] : 0,\n            i + 1 < n ? values[i + 1] : 0,\n            i < n ? values[i] : 0\n        );\n\n        // Compare keys and values (frequency >= key)\n        __m128i cmp_mask = _mm_cmpgt_epi32(values_vec, _mm_sub_epi32(keys_vec, _mm_set1_epi32(1)));\n\n        // Mask keys with the comparison result\n        __m128i valid_keys = _mm_and_si128(keys_vec, cmp_mask);\n\n        // Find the maximum valid key\n        max_result_vec = _mm_max_epi32(max_result_vec, valid_keys);\n    }\n\n    // Step 4: Extract the maximum result from the SIMD register\n    int max_results[4];\n    _mm_storeu_si128((__m128i*)max_results, max_result_vec);\n    for (int i = 0; i < 4; ++i) {\n        if (max_results[i] > max_result) {\n            max_result = max_results[i];\n        }\n    }\n\n    return max_result > 0 ? max_result : -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <unordered_map>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    std::unordered_map<int, int> freq_map;\n\n    // Calculate frequency of each number\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    int result = -1;\n\n    // Iterate through the frequency map using SIMD\n    for (const auto& [key, value] : freq_map) {\n        __m128i key_vec = _mm_set1_epi32(key);\n        __m128i freq_vec = _mm_set1_epi32(value);\n\n        // Check if the frequency is greater than or equal to the key\n        __m128i cmp = _mm_cmpgt_epi32(freq_vec,key_vec);\n        if (_mm_movemask_epi32(cmp)!=0)continue;\n    }\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <vector>\n#include <unordered_map>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    if (lst.empty()) return -1;\n\n    // Step 1: Count frequencies using a hash map\n    std::unordered_map<int, int> freq_map;\n    for (int num : lst) {\n        freq_map[num]++;\n    }\n\n    // Step 2: Extract keys and values into separate arrays for SIMD processing\n    std::vector<int> keys, values;\n    for (const auto& pair : freq_map) {\n        keys.push_back(pair.first);\n        values.push_back(pair.second);\n    }\n\n    int n = keys.size();\n    int max_value = -1;\n\n    // Step 3: Process keys and values in chunks using SSE intrinsics\n    for (int i = 0; i < n; i += 4) {\n        __m128i key_vec = _mm_set_epi32(\n            i + 3 < n ? keys[i + 3] : 0,\n            i + 2 < n ? keys[i + 2] : 0,\n            i + 1 < n ? keys[i + 1] : 0,\n            keys[i]\n        );\n\n        __m128i value_vec = _mm_set_epi32(\n            i + 3 < n ? values[i + 3] : 0,\n            i + 2 < n ? values[i + 2] : 0,\n            i + 1 < n ? values[i + 1] : 0,\n            values[i]\n        );\n\n        __m128i cmp_mask = _mm_cmpge_epi32(value_vec, key_vec); // Compare values >= keys\n        __m128i filtered_keys = _mm_and_si128(cmp_mask, key_vec); // Mask keys where condition holds\n\n        // Extract results and find the maximum\n        int temp[4];\n        _mm_storeu_si128((__m128i*)temp, filtered_keys);\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > max_value) {\n                max_value = temp[j];\n            }\n        }\n    }\n\n    return max_value > 0 ? max_value : -1;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    size_t half = n / 2;\n    for (size_t i = 0; i < half; i += 4) {\n        size_t remaining = half - i;\n        size_t chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - chunk_size]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of the elements\n    __m128i sum_vec = _mm_setzero_si128();\n    for (size_t i = 0; i < n; i += 4) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the SIMD vector\n    __m128i sum_high = _mm_unpackhi_epi64(sum_vec, sum_vec);\n    sum_vec = _mm_add_epi32(sum_vec, sum_high);\n    __m128i sum_shuffle = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuffle);\n\n    int sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Check if the sum is less than or equal to the maximum weight\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    int half = n / 2;\n    for (int i = 0; i < half; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3)); // Reverse right\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements in the vector\n    __m128i sum_vec = _mm_setzero_si128();\n    for (int i = 0; i < n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the SIMD register\n    __m128i sum_high = _mm_unpackhi_epi64(sum_vec, sum_vec);\n    sum_vec = _mm_add_epi32(sum_vec, sum_high);\n    __m128i sum_shuffle = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(2, 3, 0, 1));\n    sum_vec = _mm_add_epi32(sum_vec, sum_shuffle);\n    int sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Check if the sum is less than or equal to the maximum weight\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    size_t half = n / 2;\n    for (size_t i = 0; i < half; i++) {\n        if (q[i] != q[n - i - 1]) {\n            return false;\n        }\n    }\n\n    // Sum the elements using SIMD\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i = 0;\n\n    // Process 4 elements at a time\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of sum_vec\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < n; i++) {\n        total_sum += q[i];\n    }\n\n    // Check if the sum is less than or equal to the maximum weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    int half = n / 2;\n    for (int i = 0; i < half; i += 4) {\n        int remaining = half - i;\n        int chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - chunk_size]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3)); // Reverse the right chunk\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) return false;\n    }\n\n    // Compute the sum of the elements\n    __m128i sum_vec = _mm_setzero_si128();\n    for (int i = 0; i < n; i += 4) {\n        int remaining = n - i;\n        int chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i chunk = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, chunk);\n    }\n\n    // Horizontal sum of the SIMD register\n    __m128i temp = _mm_hadd_epi32(sum_vec, sum_vec);\n    temp = _mm_hadd_epi32(temp, temp);\n    int sum = _mm_cvtsi128_si32(temp);\n\n    // Check if the sum is less than or equal to the weight\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic (balanced)\n    int half = n / 2;\n    for (int i = 0; i < half; i += 4) {\n        int remaining = half - i;\n        int chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - chunk_size]);\n\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements\n    __m128i sum_vec = _mm_setzero_si128();\n    for (int i = 0; i < n; i += 4) {\n        int remaining = n - i;\n        int chunk_size = remaining >= 4 ? 4 : remaining;\n\n        __m128i chunk = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, chunk);\n    }\n\n    // Horizontal sum of the SIMD vector\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_vec = _mm_hadd_epi32(sum_vec, sum_vec);\n    int sum = _mm_cvtsi128_si32(sum_vec);\n\n    // Check if the sum is less than or equal to the maximum weight\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Process the vector in chunks of 4 integers using SIMD\n    int i = 0;\n    for (; i + 3 < n / 2; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]); // Load 4 integers from the left side\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]); // Load 4 integers from the right side (reversed)\n\n        // Reverse the right side vector\n        __m128i shuffle_mask = _mm_set_epi32(0, 1, 2, 3);\n        __m128i reversed_right = _mm_shuffle_epi32(right, shuffle_mask);\n\n        // Compare left and reversed right\n        __m128i cmp = _mm_cmpeq_epi32(left, reversed_right);\n\n        // Count mismatches\n        int mask = _mm_movemask_epi8(cmp);\n        changes += 4 - (__builtin_popcount(mask) / 4);\n    }\n\n    // Handle remaining elements sequentially\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Iterate over the first half of the array\n    for (int i = 0; i < n / 2; i += 4) {\n        // Load 4 elements from the left side\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Load 4 elements from the right side in reverse order\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Compare the left and right elements\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n\n        // Count the number of mismatches\n        int mask = _mm_movemask_epi8(cmp);\n        changes += 4 - __builtin_popcount(mask);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Iterate over the first half of the array\n    for (int i = 0; i < n / 2; i += 4) {\n        // Load 4 elements from the left side\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Load 4 elements from the right side (reversed order)\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]);\n\n        // Reverse the order of the right side elements\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n\n        // Compare the left and right elements\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n\n        // Count mismatches\n        int mask = _mm_movemask_epi8(cmp);\n        changes += 4 - (__builtin_popcount(mask) / 4);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Iterate from both ends towards the center\n    for (int i = 0; i < n / 2; i += 4) {\n        // Load 4 elements from the left side\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Load 4 elements from the right side (reversed order)\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15));\n\n        // Compare the left and right elements\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n\n        // Count the number of mismatches\n        int mask = _mm_movemask_ps(_mm_castsi128_ps(cmp));\n        changes += 4 - __builtin_popcount(mask);\n    }\n\n    // Handle the remaining elements if the size is not a multiple of 4\n    for (int i = n / 2 - (n / 2) % 4; i < n / 2; i++) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    // Process the array from both ends towards the center\n    for (int i = 0; i < n / 2; i += 4) {\n        // Load 4 elements from the left side\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Load 4 elements from the right side in reverse order\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(0, 1, 2, 3));\n        right = _mm_shuffle_epi8(right, _mm_set_epi8(0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15));\n\n        // Compare the left and right elements\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n\n        // Count the number of mismatches\n        int mask = _mm_movemask_epi8(cmp);\n        changes += 4 - (__builtin_popcount(mask) / 4);\n    }\n\n    // Handle the remaining elements if the array size is odd\n    if (n % 2 != 0) {\n        for (int i = n / 2; i < n - n / 2; ++i) {\n            if (arr[i] != arr[n - i - 1]) {\n                ++changes;\n            }\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto compute_total_chars = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        size_t i = 0;\n\n        // Process strings in groups of 4 using SIMD\n        __m128i sum = _mm_setzero_si128();\n        for (; i + 4 <= size; i += 4) {\n            __m128i lengths = _mm_set_epi32(\n                lst[i].size(),\n                lst[i + 1].size(),\n                lst[i + 2].size(),\n                lst[i + 3].size()\n            );\n            sum = _mm_add_epi32(sum, lengths);\n        }\n\n        // Horizontal sum of the SIMD register\n        int temp[4];\n        _mm_storeu_si128((__m128i*)temp, sum);\n        total_chars += temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining strings\n        for (; i < size; ++i) {\n            total_chars += lst[i].size();\n        }\n\n        return total_chars;\n    };\n\n    int total1 = compute_total_chars(lst1);\n    int total2 = compute_total_chars(lst2);\n\n    if (total1 < total2) {\n        return lst1;\n    } else if (total2 < total1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (const auto& str : lst) {\n            int len = str.size();\n            __m128i len_vec = _mm_set1_epi32(len);\n            sum = _mm_add_epi32(sum, len_vec);\n        }\n\n        // Extract the sum from the SIMD register\n        alignas(16) int result[4];\n        _mm_store_si128((__m128i*)result, sum);\n        total_chars = result[0] + result[1] + result[2] + result[3];\n\n        return total_chars;\n    };\n\n    int total1 = calculate_total_chars_simd(lst1);\n    int total2 = calculate_total_chars_simd(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        size_t size = lst.size();\n        __m128i sum = _mm_setzero_si128();\n\n        for (size_t i = 0; i < size; i++) {\n            int len = lst[i].size();\n            __m128i len_vec = _mm_set1_epi32(len);\n            sum = _mm_add_epi32(sum, len_vec);\n        }\n\n        alignas(16) int result[4];\n        _mm_store_si128((__m128i*)result, sum);\n        total_chars = result[0] + result[1] + result[2] + result[3];\n\n        return total_chars;\n    };\n\n    int total1 = calculate_total_chars_simd(lst1);\n    int total2 = calculate_total_chars_simd(lst2);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (const auto& str : lst) {\n            int len = str.size();\n            __m128i len_vec = _mm_set1_epi32(len);\n            sum = _mm_add_epi32(sum, len_vec);\n        }\n\n        // Extract the sum from the SIMD register\n        int buffer[4];\n        _mm_storeu_si128((__m128i*)buffer, sum);\n        total_chars = buffer[0] + buffer[1] + buffer[2] + buffer[3];\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    auto calculate_total_chars_simd = [](const std::vector<std::string>& lst) -> int {\n        int total_chars = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (const auto& str : lst) {\n            int len = str.size();\n            __m128i len_vec = _mm_set1_epi32(len);\n            sum = _mm_add_epi32(sum, len_vec);\n        }\n\n        // Extract the sum from the SIMD register\n        int buffer[4];\n        _mm_storeu_si128((__m128i*)buffer, sum);\n        total_chars = buffer[0] + buffer[1] + buffer[2] + buffer[3];\n\n        return total_chars;\n    };\n\n    int total_chars_lst1 = calculate_total_chars_simd(lst1);\n    int total_chars_lst2 = calculate_total_chars_simd(lst2);\n\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x < 1 || n < 1) return false;\n    if (x == 1) return true;\n\n    __m128i xn = _mm_set1_epi64x(n);\n    __m128i xval = _mm_set1_epi64x(x);\n    __m128i current = _mm_set1_epi64x(1);\n\n    while (true) {\n        current = _mm_mul_epu32(current, xn);\n        __m128i cmp = _mm_cmpeq_epi64(current, xval);\n        if (_mm_movemask_epi8(cmp)) return true;\n\n        __m128i overflow_check = _mm_cmpgt_epi64(current, xval);\n        if (_mm_movemask_epi8(overflow_check)) return false;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n^0 = 1 for any n > 0\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m128i xn = _mm_set1_epi64x(n); // Load n into SIMD register\n    __m128i x_val = _mm_set1_epi64x(x); // Load x into SIMD register\n    __m128i current_power = _mm_set1_epi64x(1); // Start with n^0 = 1\n\n    while (true) {\n        current_power = _mm_mul_epu32(current_power, xn); // Compute current_power *= n\n\n        // Compare current_power with x\n        __m128i cmp = _mm_cmpeq_epi64(current_power, x_val);\n        if (_mm_movemask_epi8(cmp)) return true; // If current_power == x, return true\n\n        // Check if current_power exceeds x\n        cmp = _mm_cmpgt_epi64(current_power, x_val);\n        if (_mm_movemask_epi8(cmp)) return false; // If current_power > x, return false\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n^0 = 1 for any n > 0\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m128i xn = _mm_set1_epi64x(n); // Load n into all lanes\n    __m128i xval = _mm_set1_epi64x(x); // Load x into all lanes\n    __m128i power = _mm_set1_epi64x(1); // Start with n^0 = 1\n\n    while (true) {\n        power = _mm_mul_epu32(power, xn); // Compute power *= n\n        __m128i cmp = _mm_cmpeq_epi64(power, xval); // Compare power with x\n        if (_mm_movemask_epi8(cmp)) return true; // If any lane matches, return true\n\n        __m128i overflow_check = _mm_cmpgt_epi64(power, xval); // Check if power > x\n        if (_mm_movemask_epi8(overflow_check)) break; // If power exceeds x, exit loop\n    }\n\n    return false; // If no match found, return false\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n**0 = 1\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m128i xn = _mm_set1_epi64x(n); // Load n into SIMD register\n    __m128i xval = _mm_set1_epi64x(x); // Load x into SIMD register\n    __m128i result = _mm_set1_epi64x(1); // Start with 1 (n**0)\n\n    while (true) {\n        result = _mm_mul_epu32(result, xn); // Multiply result by n\n        __m128i cmp = _mm_cmpeq_epi64(result, xval); // Compare result with x\n        if (_mm_movemask_epi8(cmp)) return true; // If equal, x is a power of n\n        __m128i overflow_check = _mm_cmpgt_epi64(result, xval); // Check overflow\n        if (_mm_movemask_epi8(overflow_check)) return false; // If result > x, stop\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true; // Special case: n^0 = 1 for any n >= 1\n    if (n <= 1 || x < n) return false; // Invalid cases\n\n    __m128i xn = _mm_set1_epi64x(n); // Load n into SIMD register\n    __m128i x_val = _mm_set1_epi64x(x); // Load x into SIMD register\n    __m128i current = _mm_set1_epi64x(1); // Start with n^0 = 1\n\n    while (true) {\n        current = _mm_mul_epu32(current, xn); // Multiply current by n (current = current * n)\n\n        // Compare current with x\n        __m128i cmp = _mm_cmpeq_epi64(current, x_val);\n        if (_mm_movemask_epi8(cmp)) {\n            return true; // Found n^int = x\n        }\n\n        // Check if current exceeds x\n        cmp = _mm_cmpgt_epi64(current, x_val);\n        if (_mm_movemask_epi8(cmp)) {\n            return false; // current > x, so x is not a simple power of n\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(primes) / sizeof(primes[0]);\n\n    __m128i prime_vec = _mm_set_epi8('D', 'B', '7', '5', '3', '2', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n\n    size_t i = 0;\n    size_t len = num.size();\n    while (i + 16 <= len) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&num[i]);\n        for (int j = 0; j < prime_count; ++j) {\n            __m128i prime_char = _mm_set1_epi8(primes[j]);\n            __m128i cmp_result = _mm_cmpeq_epi8(input_vec, prime_char);\n            count += _mm_popcnt_u32(_mm_movemask_epi8(cmp_result));\n        }\n        i += 16;\n    }\n\n    for (; i < len; ++i) {\n        for (int j = 0; j < prime_count; ++j) {\n            if (num[i] == primes[j]) {\n                count++;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = { '2', '3', '5', '7', 'B', 'D' };\n    __m128i prime_chars = _mm_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i input = _mm_loadu_si128((__m128i*)&num[i]);\n        __m128i result = _mm_setzero_si128();\n\n        for (int j = 0; j < 6; ++j) {\n            __m128i prime = _mm_set1_epi8(primes[j]);\n            __m128i cmp = _mm_cmpeq_epi8(input, prime);\n            result = _mm_or_si128(result, cmp);\n        }\n\n        int mask = _mm_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < len; ++i) {\n        for (char prime : primes) {\n            if (num[i] == prime) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(primes) / sizeof(primes[0]);\n\n    __m128i prime_vec = _mm_setr_epi8(\n        primes[0], primes[1], primes[2], primes[3],\n        primes[4], primes[5], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    while (i + 16 <= len) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&num[i]);\n        for (int j = 0; j < prime_count; ++j) {\n            __m128i prime_char = _mm_set1_epi8(primes[j]);\n            __m128i cmp = _mm_cmpeq_epi8(chunk, prime_char);\n            count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n        }\n        i += 16;\n    }\n\n    for (; i < len; ++i) {\n        for (int j = 0; j < prime_count; ++j) {\n            if (num[i] == primes[j]) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const char primes[] = {'2', '3', '5', '7', 'B', 'D'};\n    const int prime_count = sizeof(primes) / sizeof(primes[0]);\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    __m128i prime_vec = _mm_set_epi8('D', 'B', '7', '5', '3', '2', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i input_vec = _mm_loadu_si128((__m128i*)&num[i]);\n        for (int j = 0; j < prime_count; ++j) {\n            __m128i prime_char = _mm_set1_epi8(primes[j]);\n            __m128i cmp_result = _mm_cmpeq_epi8(input_vec, prime_char);\n            count += _mm_popcnt_u32(_mm_movemask_epi8(cmp_result));\n        }\n    }\n\n    for (; i < len; ++i) {\n        for (int j = 0; j < prime_count; ++j) {\n            if (num[i] == primes[j]) {\n                ++count;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    if (num.empty()) return 0;\n\n    const char prime_hex_digits[] = {'2', '3', '5', '7', 'B', 'D'};\n    __m128i prime_digits = _mm_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    while (i + 16 <= len) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&num[i]);\n        for (int j = 0; j < 6; ++j) {\n            __m128i cmp = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(prime_hex_digits[j]));\n            count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n        }\n        i += 16;\n    }\n\n    for (; i < len; ++i) {\n        for (int j = 0; j < 6; ++j) {\n            if (num[i] == prime_hex_digits[j]) {\n                count++;\n                break;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 16) {\n        size_t limit = (i + 16 <= n - 3) ? 16 : n - 3 - i;\n\n        __m128i v1 = _mm_loadu_si128((__m128i*)(s.data() + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(s.data() + i + 1));\n        __m128i v3 = _mm_loadu_si128((__m128i*)(s.data() + i + 2));\n\n        __m128i cmp1 = _mm_cmpeq_epi8(v1, v2);\n        __m128i cmp2 = _mm_cmpeq_epi8(v2, v3);\n        __m128i cmp3 = _mm_cmpeq_epi8(v1, v3);\n\n        __m128i combined = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n        int mask = _mm_movemask_epi8(combined);\n\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 16) {\n        size_t remaining = std::min(n - i, size_t(16));\n\n        __m128i chunk1 = _mm_loadu_si128((__m128i*)(s.data() + i));\n        __m128i chunk2 = _mm_srli_si128(chunk1, 1);\n        __m128i chunk3 = _mm_srli_si128(chunk1, 2);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(chunk1, chunk2);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk1, chunk3);\n        __m128i cmp3 = _mm_cmpeq_epi8(chunk2, chunk3);\n\n        __m128i result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n\n        int mask = _mm_movemask_epi8(result);\n        if (mask != 0) return false;\n\n        if (remaining < 16) break;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 16) {\n        size_t limit = (i + 16 <= n - 3) ? 16 : n - 3 - i;\n\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i + 1));\n        __m128i v3 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i + 2));\n\n        __m128i cmp1 = _mm_cmpeq_epi8(v1, v2);\n        __m128i cmp2 = _mm_cmpeq_epi8(v2, v3);\n        __m128i cmp3 = _mm_cmpeq_epi8(v1, v3);\n\n        __m128i result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n\n        int mask = _mm_movemask_epi8(result);\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 16) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 16 ? 16 : remaining;\n\n        __m128i chars1 = _mm_loadu_si128((__m128i*)(s.data() + i));\n        __m128i chars2 = _mm_loadu_si128((__m128i*)(s.data() + i + 1));\n        __m128i chars3 = _mm_loadu_si128((__m128i*)(s.data() + i + 2));\n\n        __m128i cmp1 = _mm_cmpeq_epi8(chars1, chars2);\n        __m128i cmp2 = _mm_cmpeq_epi8(chars2, chars3);\n        __m128i cmp3 = _mm_cmpeq_epi8(chars1, chars3);\n\n        __m128i combined_cmp = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n\n        int mask = _mm_movemask_epi8(combined_cmp);\n        if (mask != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string &s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    for (size_t i = 0; i <= n - 3; i += 16) {\n        size_t remaining = n - i;\n        size_t chunk_size = remaining >= 16 ? 16 : remaining;\n\n        __m128i current = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&s[i + 1]);\n        __m128i next_next = _mm_loadu_si128((__m128i*)&s[i + 2]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(current, next);\n        __m128i cmp2 = _mm_cmpeq_epi8(next, next_next);\n        __m128i cmp3 = _mm_cmpeq_epi8(current, next_next);\n\n        __m128i or_cmp = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n        int mask = _mm_movemask_epi8(or_cmp);\n\n        if (mask != 0) {\n            for (size_t j = 0; j < chunk_size - 2; ++j) {\n                if (s[i + j] == s[i + j + 1] || s[i + j + 1] == s[i + j + 2] || s[i + j] == s[i + j + 2]) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m128i vec_length = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 4) {\n        __m128i vec_i = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i vec_div = _mm_div_epi32(vec_length, vec_i);\n        __m128i vec_mul = _mm_mullo_epi32(vec_div, vec_i);\n        __m128i vec_cmp = _mm_cmpeq_epi32(vec_mul, vec_length);\n\n        if (_mm_movemask_epi8(vec_cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.size();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m128i vec_length = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 4) {\n        __m128i vec_i = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i vec_rem = _mm_rem_epi32(vec_length, vec_i);\n\n        __m128i vec_zero = _mm_setzero_si128();\n        __m128i cmp_result = _mm_cmpeq_epi32(vec_rem, vec_zero);\n\n        if (_mm_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string &str) {\n    int length = str.length();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m128i vec_length = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 4) {\n        __m128i vec_i = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i vec_rem = _mm_rem_epi32(vec_length, vec_i);\n\n        __m128i vec_zero = _mm_setzero_si128();\n        __m128i cmp_result = _mm_cmpeq_epi32(vec_rem, vec_zero);\n\n        if (_mm_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.size();\n    if (length < 2) return false;\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m128i len_vec = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 4) {\n        __m128i divisor_vec = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i remainder_vec = _mm_rem_epi32(len_vec, divisor_vec);\n\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i cmp_vec = _mm_cmpeq_epi32(remainder_vec, zero_vec);\n\n        if (_mm_movemask_epi8(cmp_vec)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string &str) {\n    int length = str.size();\n    if (length < 2) {\n        return false;\n    }\n\n    int sqrt_length = static_cast<int>(std::sqrt(length));\n    __m128i len_vec = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= sqrt_length; i += 4) {\n        __m128i divisor_vec = _mm_set_epi32(i, i + 1, i + 2, i + 3);\n        __m128i remainder_vec = _mm_rem_epi32(len_vec, divisor_vec);\n\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i cmp_result = _mm_cmpeq_epi32(remainder_vec, zero_vec);\n\n        if (_mm_movemask_epi8(cmp_result)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    char numStr[20];\n    snprintf(numStr, sizeof(numStr), \"%llu\", N);\n\n    // Load digits into an array of integers\n    int digits[16] = {0};\n    int len = 0;\n    for (; numStr[len] != '\\0'; ++len) {\n        digits[len] = numStr[len] - '0';\n    }\n\n    // Use SIMD to sum the digits\n    __m128i sumVec = _mm_setzero_si128();\n    for (int i = 0; i < len; i += 4) {\n        __m128i digitVec = _mm_set_epi32(\n            i + 3 < len ? digits[i + 3] : 0,\n            i + 2 < len ? digits[i + 2] : 0,\n            i + 1 < len ? digits[i + 1] : 0,\n            digits[i]\n        );\n        sumVec = _mm_add_epi32(sumVec, digitVec);\n    }\n\n    // Horizontally sum the vector\n    sumVec = _mm_hadd_epi32(sumVec, sumVec);\n    sumVec = _mm_hadd_epi32(sumVec, sumVec);\n    int totalSum = _mm_cvtsi128_si32(sumVec);\n\n    // Convert the sum to binary\n    std::string binaryResult;\n    while (totalSum > 0) {\n        binaryResult = (char)('0' + (totalSum % 2)) + binaryResult;\n        totalSum /= 2;\n    }\n\n    return binaryResult.empty() ? \"0\" : binaryResult;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Load digits into an array\n    alignas(16) int digits[4] = {0, 0, 0, 0};\n    for (size_t i = 0; i < len && i < 4; ++i) {\n        digits[i] = num_str[i] - '0';\n    }\n\n    // Use SSE intrinsics to sum the digits\n    __m128i digit_vec = _mm_load_si128((__m128i*)digits);\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i sum_vec = _mm_add_epi32(digit_vec, zero_vec);\n\n    // Extract the sum from the vector\n    alignas(16) int result[4];\n    _mm_store_si128((__m128i*)result, sum_vec);\n\n    int total_sum = 0;\n    for (size_t i = 0; i < len && i < 4; ++i) {\n        total_sum += result[i];\n    }\n\n    // Convert the sum to binary string\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = (total_sum % 2 == 0 ? \"0\" : \"1\") + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number into a string to extract digits\n    char buffer[20];\n    snprintf(buffer, sizeof(buffer), \"%llu\", N);\n\n    // Load the digits into an array\n    size_t len = strlen(buffer);\n    __m128i digit_vector = _mm_setzero_si128();\n    for (size_t i = 0; i < len; i++) {\n        // Convert char digit to integer and store in vector\n        __m128i temp = _mm_set1_epi16(buffer[i] - '0');\n        digit_vector = _mm_add_epi16(digit_vector, temp);\n    }\n\n    // Sum up the digits in the vector\n    __m128i sum_vector = _mm_hadd_epi16(digit_vector, digit_vector);\n    sum_vector = _mm_hadd_epi16(sum_vector, sum_vector);\n\n    // Extract the final sum from the vector\n    int sum = _mm_extract_epi16(sum_vector, 0);\n\n    // Convert the sum to binary string\n    std::string binary_result;\n    while (sum > 0) {\n        binary_result = (char)((sum % 2) + '0') + binary_result;\n        sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate space for digits as integers\n    alignas(16) int digits[4] = {0, 0, 0, 0};\n    alignas(16) int results[4] = {0, 0, 0, 0};\n\n    // Process digits in chunks of 4 using SIMD\n    size_t i = 0;\n    int total_sum = 0;\n    while (i < len) {\n        size_t chunk_size = std::min(size_t(4), len - i);\n\n        // Load digits into the array\n        for (size_t j = 0; j < chunk_size; ++j) {\n            digits[j] = num_str[i + j] - '0';\n        }\n\n        // Load digits into SIMD register\n        __m128i vec_digits = _mm_load_si128((__m128i*)digits);\n\n        // Perform horizontal addition\n        __m128i vec_sum = _mm_hadd_epi32(vec_digits, vec_digits);\n        vec_sum = _mm_hadd_epi32(vec_sum, vec_sum);\n\n        // Store the result\n        _mm_store_si128((__m128i*)results, vec_sum);\n\n        // Accumulate the sum\n        total_sum += results[0];\n\n        // Move to the next chunk\n        i += chunk_size;\n    }\n\n    // Convert the total sum to binary string\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = char((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    // Convert the number to a string to extract digits\n    std::string num_str = std::to_string(N);\n    size_t len = num_str.size();\n\n    // Allocate an array to store the digits\n    alignas(16) int digits[4] = {0, 0, 0, 0};\n    alignas(16) int sums[4] = {0, 0, 0, 0};\n\n    // Process digits in chunks of 4\n    size_t i = 0;\n    __m128i sum_vec = _mm_setzero_si128();\n    while (i < len) {\n        // Load up to 4 digits into the array\n        for (size_t j = 0; j < 4 && i + j < len; ++j) {\n            digits[j] = num_str[i + j] - '0';\n        }\n\n        // Load digits into an SSE register\n        __m128i digit_vec = _mm_load_si128(reinterpret_cast<const __m128i*>(digits));\n\n        // Accumulate the sum\n        sum_vec = _mm_add_epi32(sum_vec, digit_vec);\n\n        // Clear the digits array for the next iteration\n        for (size_t j = 0; j < 4; ++j) {\n            digits[j] = 0;\n        }\n\n        i += 4;\n    }\n\n    // Extract the sum from the SSE register\n    _mm_store_si128(reinterpret_cast<__m128i*>(sums), sum_vec);\n    int total_sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    // Convert the sum to binary string\n    std::string binary_result;\n    while (total_sum > 0) {\n        binary_result = char((total_sum % 2) + '0') + binary_result;\n        total_sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    // Ensure the vector size is a multiple of 4 by padding with zeros\n    int padded_size = (n + 3) & ~3; // Round up to the nearest multiple of 4\n    std::vector<int> padded_lst(padded_size, 0);\n    for (int i = 0; i < n; ++i) {\n        padded_lst[i] = lst[i];\n    }\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n\n    for (int i = 1; i < padded_size; i += 8) {\n        // Load 8 integers (two 128-bit registers)\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&padded_lst[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&padded_lst[i + 4]);\n\n        // Mask to extract even numbers (x % 2 == 0)\n        __m128i even_mask1 = _mm_cmpeq_epi32(_mm_and_si128(vec1, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i even_mask2 = _mm_cmpeq_epi32(_mm_and_si128(vec2, _mm_set1_epi32(1)), _mm_setzero_si128());\n\n        // Apply the masks\n        __m128i even_vec1 = _mm_and_si128(vec1, even_mask1);\n        __m128i even_vec2 = _mm_and_si128(vec2, even_mask2);\n\n        // Add the results\n        sum = _mm_add_epi32(sum, even_vec1);\n        sum = _mm_add_epi32(sum, even_vec2);\n    }\n\n    // Horizontal sum of the 128-bit register\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    // Ensure the vector size is a multiple of 4 for SIMD processing\n    int padded_size = (n + 3) & ~3;\n    std::vector<int> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n    __m128i mask = _mm_set_epi32(0, 0, -1, 0); // Mask for odd indices (0-based)\n\n    for (int i = 0; i < padded_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst[i]); // Load 4 integers\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(1)), _mm_setzero_si128()); // Check even numbers\n        __m128i odd_index_mask = _mm_and_si128(mask, even_mask); // Combine masks for even numbers at odd indices\n        __m128i filtered = _mm_and_si128(vec, odd_index_mask); // Apply mask\n        sum = _mm_add_epi32(sum, filtered); // Accumulate the sum\n    }\n\n    // Extract the sum from the SIMD register\n    alignas(16) int result[4];\n    _mm_store_si128((__m128i*)result, sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 1; i < n; i += 8) {\n        __m128i indices = _mm_set_epi32(i + 6, i + 4, i + 2, i);\n        __m128i mask = _mm_set_epi32((i + 6) % 2, (i + 4) % 2, (i + 2) % 2, i % 2);\n        __m128i filtered = _mm_add_epi32(mask,lst);\n    }\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 1; i < n; i += 8) {\n        __m128i indices = _mm_set_epi32(i + 6, i + 4, i + 2, i);\n        __m128i mask = _mm_set_epi32((i + 6) % 2 == 1, (i + 4) % 2 == 1, (i + 2) % 2 == 1, i % 2 == 1);\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i filtered = _mm_and_si128(values, mask);\n        sum = _mm_add_epi32(sum, filtered);\n    }\n    return _mm_sum_epi32(sum);\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int n = lst.size();\n    if (n < 2) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 1; i < n; i += 8) {\n        // Load 8 integers from the vector\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        // Create a mask for odd indices (1, 3, 5, 7)\n        __m128i mask_indices = _mm_set_epi32(0, -1, 0, -1);\n\n        // Apply mask to keep only odd indices\n        __m128i masked_data = _mm_and_si128(data, mask_indices);\n\n        // Create a mask for even values\n        __m128i mask_even = _mm_set1_epi32(1);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(masked_data, mask_even), _mm_setzero_si128());\n\n        // Apply mask to keep only even values at odd indices\n        __m128i even_values = _mm_and_si128(masked_data, even_mask);\n\n        // Accumulate the sum\n        sum = _mm_add_epi32(sum, even_values);\n    }\n\n    // Extract the sum from the SIMD register\n    alignas(16) int result[4];\n    _mm_store_si128((__m128i*)result, sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n\n    // Process 16 characters at a time (128-bit registers)\n    size_t i = 0;\n    for (; i + 16 <= n; i += 16) {\n        // Load 16 characters into an SSE register\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Subtract 'a' to normalize to 0-based alphabet index\n        __m128i normalized = _mm_sub_epi8(input, _mm_set1_epi8('a'));\n\n        // Add the rotation value (2 * 2 = 4)\n        __m128i rotated = _mm_add_epi8(normalized, _mm_set1_epi8(4));\n\n        // Wrap around alphabet by applying modulo 26 (using masking)\n        __m128i wrapped = _mm_and_si128(rotated, _mm_set1_epi8(25));\n\n        // Add 'a' back to convert to ASCII\n        __m128i encrypted = _mm_add_epi8(wrapped, _mm_set1_epi8('a'));\n\n        // Store the result back to the output string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), encrypted);\n    }\n\n    // Process remaining characters sequentially\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = 'a' + ((c - 'a' + 4) % 26);\n        } else {\n            result[i] = c; // Non-alphabet characters remain unchanged\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    // Process in chunks of 16 bytes (128 bits) using SSE2\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        // Load 16 characters into an SSE register\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Add 4 to each character (rotate alphabet by 2 * 2 places)\n        __m128i rotated = _mm_add_epi8(input, _mm_set1_epi8(4));\n\n        // Store the result back into the result string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), rotated);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const int shift = 4; // Alphabet rotation by 2 * 2 places\n    const __m128i shift_vector = _mm_set1_epi8(shift);\n    const __m128i lower_bound = _mm_set1_epi8('a');\n    const __m128i upper_bound = _mm_set1_epi8('z');\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i *)(s.data() + i));\n        __m128i rotated = _mm_add_epi8(chars, shift_vector);\n\n        // Wrap around for alphabet rotation\n        __m128i mask = _mm_cmpgt_epi8(rotated, upper_bound);\n        rotated = _mm_sub_epi8(rotated, _mm_and_si128(mask, _mm_set1_epi8(26)));\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), rotated);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        char rotated = c + shift;\n        if (rotated > 'z') {\n            rotated -= 26;\n        }\n        result[i] = rotated;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t n = s.size();\n    std::string result(n, '\\0');\n\n    const __m128i shift = _mm_set1_epi8(4); // Rotate alphabet by 2 * 2 = 4 places\n    const __m128i lower_bound = _mm_set1_epi8('a');\n    const __m128i upper_bound = _mm_set1_epi8('z');\n\n    size_t i = 0;\n\n    // Process 16 characters at a time using SIMD\n    for (; i + 16 <= n; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(s.data() + i));\n        \n        // Check if characters are within 'a' to 'z'\n        __m128i is_alpha = _mm_and_si128(\n            _mm_cmpgt_epi8(chunk, lower_bound),\n            _mm_cmplt_epi8(chunk, upper_bound)\n        );\n\n        // Rotate alphabet by 4 places\n        __m128i rotated = _mm_add_epi8(chunk, shift);\n\n        // Wrap around if necessary (only for alphabetic characters)\n        __m128i wrapped = _mm_sub_epi8(rotated, _mm_set1_epi8(26));\n        __m128i needs_wrap = _mm_cmpgt_epi8(rotated, upper_bound);\n        rotated = _mm_blendv_epi8(rotated, wrapped, needs_wrap);\n\n        // Keep original characters if not alphabetic\n        __m128i final_chunk = _mm_blendv_epi8(chunk, rotated, is_alpha);\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), final_chunk);\n    }\n\n    // Process remaining characters sequentially\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            char rotated = c + 4;\n            if (rotated > 'z') {\n                rotated -= 26;\n            }\n            result[i] = rotated;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string &s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    // Process in chunks of 16 characters (128 bits)\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        // Load 16 characters into an SSE register\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Add 4 to each character (rotate alphabet by 2 * 2 places)\n        __m128i rotated_chunk = _mm_add_epi8(chunk, _mm_set1_epi8(4));\n\n        // Store the result back into the result string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), rotated_chunk);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = s[i] + 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m128i vowels = _mm_set_epi8('u', 'U', 'o', 'O', 'i', 'I', 'e', 'E', 'a', 'A', 0, 0, 0, 0, 0, 0);\n    const __m128i vowel_replacements = _mm_set_epi8('w', 'W', 'q', 'Q', 'k', 'K', 'g', 'G', 'c', 'C', 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(message.data() + i));\n\n        // Swap case\n        __m128i is_upper = _mm_and_si128(chunk, _mm_set1_epi8(0x20));\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(0x20));\n\n        // Replace vowels\n        __m128i vowel_mask = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i replaced_vowels = _mm_shuffle_epi8(vowel_replacements, vowel_mask);\n\n        // Combine swapped case and replaced vowels\n        __m128i final_chunk = _mm_blendv_epi8(swapped_case, replaced_vowels, vowel_mask);\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), final_chunk);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        char swapped = (c >= 'A' && c <= 'Z') ? c + 32 : (c >= 'a' && c <= 'z') ? c - 32 : c;\n        if (swapped == 'a' || swapped == 'A') swapped = 'c';\n        else if (swapped == 'e' || swapped == 'E') swapped = 'g';\n        else if (swapped == 'i' || swapped == 'I') swapped = 'k';\n        else if (swapped == 'o' || swapped == 'O') swapped = 'q';\n        else if (swapped == 'u' || swapped == 'U') swapped = 'w';\n        result[i] = swapped;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m128i vowels = _mm_set_epi8('u', 'U', 'o', 'O', 'i', 'I', 'e', 'E', 'a', 'A', 0, 0, 0, 0, 0, 0);\n    const __m128i vowel_shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i < len; i += 16) {\n        size_t chunk_size = (i + 16 <= len) ? 16 : len - i;\n\n        __m128i chunk = _mm_loadu_si128((__m128i *)(message.data() + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)), _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1)));\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)), _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1)));\n\n        __m128i swap_case_upper = _mm_add_epi8(chunk, _mm_set1_epi8('a' - 'A'));\n        __m128i swap_case_lower = _mm_sub_epi8(chunk, _mm_set1_epi8('a' - 'A'));\n\n        __m128i swapped = _mm_or_si128(_mm_and_si128(is_upper, swap_case_upper), _mm_and_si128(is_lower, swap_case_lower));\n\n        __m128i is_vowel = _mm_or_si128(\n            _mm_cmpeq_epi8(swapped, _mm_set1_epi8('a')),\n            _mm_or_si128(\n                _mm_cmpeq_epi8(swapped, _mm_set1_epi8('e')),\n                _mm_or_si128(\n                    _mm_cmpeq_epi8(swapped, _mm_set1_epi8('i')),\n                    _mm_or_si128(\n                        _mm_cmpeq_epi8(swapped, _mm_set1_epi8('o')),\n                        _mm_cmpeq_epi8(swapped, _mm_set1_epi8('u'))))));\n\n        __m128i is_vowel_upper = _mm_or_si128(\n            _mm_cmpeq_epi8(swapped, _mm_set1_epi8('A')),\n            _mm_or_si128(\n                _mm_cmpeq_epi8(swapped, _mm_set1_epi8('E')),\n                _mm_or_si128(\n                    _mm_cmpeq_epi8(swapped, _mm_set1_epi8('I')),\n                    _mm_or_si128(\n                        _mm_cmpeq_epi8(swapped, _mm_set1_epi8('O')),\n                        _mm_cmpeq_epi8(swapped, _mm_set1_epi8('U'))))));\n\n        __m128i shifted_vowel = _mm_add_epi8(swapped, vowel_shift);\n        __m128i encoded = _mm_or_si128(_mm_and_si128(is_vowel, shifted_vowel), _mm_andnot_si128(is_vowel, swapped));\n        encoded = _mm_or_si128(_mm_and_si128(is_vowel_upper, shifted_vowel), _mm_andnot_si128(is_vowel_upper, encoded));\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), encoded);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::vector<char> result(len);\n\n    const __m128i vowels = _mm_setr_epi8('A', 'E', 'I', 'O', 'U', 'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0);\n    const __m128i vowel_shift = _mm_set1_epi8(2); // Shift vowels by 2 places ahead\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A' - 1)), _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z' + 1)));\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('a' - 1)), _mm_cmplt_epi8(chunk, _mm_set1_epi8('z' + 1)));\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_and_si128(is_letter, _mm_set1_epi8(0x20))); // Swap case\n\n        __m128i is_vowel = _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('A')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('E')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('I')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('O')),\n                                        _mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('U'))))));\n        is_vowel = _mm_or_si128(is_vowel, _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('a')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('e')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('i')),\n                                        _mm_or_si128(_mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('o')),\n                                        _mm_cmpeq_epi8(swapped_case, _mm_set1_epi8('u')))))));\n\n        __m128i shifted_vowels = _mm_add_epi8(swapped_case, _mm_and_si128(is_vowel, vowel_shift));\n        __m128i final_chunk = _mm_or_si128(_mm_andnot_si128(is_vowel, swapped_case), _mm_and_si128(is_vowel, shifted_vowels));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), final_chunk);\n    }\n\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::string result(len, '\\0');\n\n    const __m128i lower_a = _mm_set1_epi8('a');\n    const __m128i upper_a = _mm_set1_epi8('A');\n    const __m128i lower_z = _mm_set1_epi8('z');\n    const __m128i upper_z = _mm_set1_epi8('Z');\n    const __m128i case_mask = _mm_set1_epi8(0x20); // Mask to toggle case\n    const __m128i vowel_mask = _mm_set1_epi8(2);  // Offset for vowels\n\n    const __m128i vowels_lower = _mm_set_epi8('u', 'o', 'i', 'e', 'a', 'u', 'o', 'i', 'e', 'a', 'u', 'o', 'i', 'e', 'a', 'u');\n    const __m128i vowels_upper = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U');\n\n    size_t i = 0;\n    for (; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(message.data() + i));\n\n        // Swap case\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, lower_a), _mm_cmplt_epi8(chunk, lower_z));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upper_a), _mm_cmplt_epi8(chunk, upper_z));\n        __m128i toggle_case = _mm_or_si128(is_lower, is_upper);\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_and_si128(toggle_case, case_mask));\n\n        // Replace vowels\n        __m128i is_vowel_lower = _mm_cmpeq_epi8(swapped_case, vowels_lower);\n        __m128i is_vowel_upper = _mm_cmpeq_epi8(swapped_case, vowels_upper);\n        __m128i is_vowel = _mm_or_si128(is_vowel_lower, is_vowel_upper);\n        __m128i vowel_offset = _mm_and_si128(is_vowel, vowel_mask);\n        __m128i encoded_chunk = _mm_add_epi8(swapped_case, vowel_offset);\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), encoded_chunk);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = message[i];\n        char swapped = (c >= 'a' && c <= 'z') ? c - 32 : (c >= 'A' && c <= 'Z') ? c + 32 : c;\n        if (swapped == 'a' || swapped == 'e' || swapped == 'i' || swapped == 'o' || swapped == 'u' ||\n            swapped == 'A' || swapped == 'E' || swapped == 'I' || swapped == 'O' || swapped == 'U') {\n            swapped += 2;\n        }\n        result[i] = swapped;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    std::vector<char> result(len);\n\n    const __m128i vowels = _mm_setr_epi8('A', 'E', 'I', 'O', 'U', 'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0);\n    const __m128i vowel_shift = _mm_set1_epi8(2); // Shift vowels by 2 places ahead\n    const __m128i case_mask = _mm_set1_epi8(0x20); // Mask to toggle case\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&message[i]);\n        __m128i is_vowel = _mm_cmpeq_epi8(chunk, vowels);\n\n        // Toggle case\n        __m128i toggled_case = _mm_xor_si128(chunk, case_mask);\n\n        // Replace vowels\n        __m128i shifted_vowels = _mm_add_epi8(chunk, vowel_shift);\n        __m128i encoded_chunk = _mm_blendv_epi8(toggled_case, shifted_vowels, is_vowel);\n\n        _mm_storeu_si128((__m128i*)&result[i], encoded_chunk);\n    }\n\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        for (size_t i = 0; i < len; i += 16) {\n            __m128i chunk;\n            if (i + 16 <= len) {\n                chunk = _mm_loadu_si128((__m128i*)&key[i]);\n            } else {\n                char temp[16] = {0};\n                for (size_t j = 0; j < len - i; ++j) {\n                    temp[j] = key[i + j];\n                }\n                chunk = _mm_loadu_si128((__m128i*)temp);\n            }\n\n            __m128i lower_bound = _mm_set1_epi8('a');\n            __m128i upper_bound = _mm_set1_epi8('z');\n            __m128i upper_lower_bound = _mm_set1_epi8('A');\n            __m128i upper_upper_bound = _mm_set1_epi8('Z');\n\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_bound, _mm_set1_epi8(1)))\n            );\n\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_upper_bound, _mm_set1_epi8(1)))\n            );\n\n            int lower_mask = _mm_movemask_epi8(is_lower);\n            int upper_mask = _mm_movemask_epi8(is_upper);\n\n            if (lower_mask != (1 << (len - i)) - 1) {\n                all_lower = false;\n            }\n            if (upper_mask != (1 << (len - i)) - 1) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        for (size_t i = 0; i < len; i += 16) {\n            size_t chunk_size = (i + 16 <= len) ? 16 : len - i;\n            __m128i chunk = _mm_setzero_si128();\n\n            if (chunk_size == 16) {\n                chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            } else {\n                char temp[16] = {0};\n                std::memcpy(temp, key.data() + i, chunk_size);\n                chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n            }\n\n            __m128i lower_bound = _mm_set1_epi8('a');\n            __m128i upper_bound = _mm_set1_epi8('z');\n            __m128i upper_lower_bound = _mm_set1_epi8('A');\n            __m128i upper_upper_bound = _mm_set1_epi8('Z');\n\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_bound, _mm_set1_epi8(1)))\n            );\n\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_upper_bound, _mm_set1_epi8(1)))\n            );\n\n            int lower_mask = _mm_movemask_epi8(is_lower);\n            int upper_mask = _mm_movemask_epi8(is_upper);\n\n            if (lower_mask != (1 << chunk_size) - 1) {\n                all_lower = false;\n            }\n            if (upper_mask != (1 << chunk_size) - 1) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        for (size_t i = 0; i < len; i += 16) {\n            __m128i chunk;\n            if (i + 16 <= len) {\n                chunk = _mm_loadu_si128((__m128i*)(key.data() + i));\n            } else {\n                char temp[16] = {0};\n                size_t remaining = len - i;\n                for (size_t j = 0; j < remaining; ++j) {\n                    temp[j] = key[i + j];\n                }\n                chunk = _mm_loadu_si128((__m128i*)temp);\n            }\n\n            __m128i lower_bound = _mm_set1_epi8('a');\n            __m128i upper_bound = _mm_set1_epi8('z');\n            __m128i upper_lower_bound = _mm_set1_epi8('A');\n            __m128i upper_upper_bound = _mm_set1_epi8('Z');\n\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_bound, _mm_set1_epi8(1)))\n            );\n\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chunk, _mm_sub_epi8(upper_lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_upper_bound, _mm_set1_epi8(1)))\n            );\n\n            int lower_mask = _mm_movemask_epi8(is_lower);\n            int upper_mask = _mm_movemask_epi8(is_upper);\n\n            if (lower_mask != 0xFFFF) all_lower = false;\n            if (upper_mask != 0xFFFF) all_upper = false;\n\n            if (!all_lower && !all_upper) return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        for (size_t i = 0; i < key.size(); i += 16) {\n            __m128i chars = _mm_setzero_si128();\n            if (i + 16 <= key.size()) {\n                chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            } else {\n                char buffer[16] = {0};\n                std::memcpy(buffer, key.data() + i, key.size() - i);\n                chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer));\n            }\n\n            __m128i lower_bound = _mm_set1_epi8('a');\n            __m128i upper_bound = _mm_set1_epi8('z');\n            __m128i upper_lower_bound = _mm_set1_epi8('A');\n            __m128i upper_upper_bound = _mm_set1_epi8('Z');\n\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_sub_epi8(lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chars, _mm_add_epi8(upper_bound, _mm_set1_epi8(1)))\n            );\n\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_sub_epi8(upper_lower_bound, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chars, _mm_add_epi8(upper_upper_bound, _mm_set1_epi8(1)))\n            );\n\n            int lower_mask = _mm_movemask_epi8(is_lower);\n            int upper_mask = _mm_movemask_epi8(is_upper);\n\n            if (lower_mask != 0xFFFF) {\n                all_lower = false;\n            }\n            if (upper_mask != 0xFFFF) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        size_t len = key.size();\n        size_t i = 0;\n\n        __m128i lower_mask = _mm_set1_epi8(0x20); // Mask for converting to lowercase\n        __m128i a = _mm_set1_epi8('a');\n        __m128i z = _mm_set1_epi8('z');\n        __m128i A = _mm_set1_epi8('A');\n        __m128i Z = _mm_set1_epi8('Z');\n\n        while (i + 16 <= len) {\n            __m128i chars = _mm_loadu_si128((__m128i*)&key[i]);\n\n            __m128i lower_chars = _mm_or_si128(chars, lower_mask);\n            __m128i is_lower = _mm_and_si128(\n                _mm_cmpgt_epi8(lower_chars, _mm_sub_epi8(a, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(lower_chars, _mm_add_epi8(z, _mm_set1_epi8(1)))\n            );\n\n            __m128i is_upper = _mm_and_si128(\n                _mm_cmpgt_epi8(chars, _mm_sub_epi8(A, _mm_set1_epi8(1))),\n                _mm_cmplt_epi8(chars, _mm_add_epi8(Z, _mm_set1_epi8(1)))\n            );\n\n            if (_mm_movemask_epi8(is_lower) != 0xFFFF) {\n                all_lower = false;\n            }\n            if (_mm_movemask_epi8(is_upper) != 0xFFFF) {\n                all_upper = false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n\n            i += 16;\n        }\n\n        for (; i < len; ++i) {\n            if (std::islower(key[i])) {\n                all_upper = false;\n            } else if (std::isupper(key[i])) {\n                all_lower = false;\n            } else {\n                return false;\n            }\n\n            if (!all_lower && !all_upper) {\n                return false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Initialize a mask for marking non-prime numbers\n    std::vector<int> is_prime(n, 1);\n    is_prime[0] = 0; // 0 is not prime\n    is_prime[1] = 0; // 1 is not prime\n\n    // Use SIMD to mark non-prime numbers\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            __m128i vec_i = _mm_set1_epi32(i);\n            for (int j = i * i; j < n; j += 4 * i) {\n                __m128i vec_j = _mm_set_epi32(j + 3 * i, j + 2 * i, j + i, j);\n                __m128i vec_mod = _mm_set1_epi32(0);\n                __m128i mask = _mm_cmpeq_epi32(_mm_rem_epi32(vec_j, vec_i), vec_mod);\n                _mm_storeu_si128((__m128i*)&is_prime[j], mask);\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 1) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Create a mask for numbers from 2 to n-1\n    int limit = n - 1;\n    int size = (limit + 15) / 16 * 16; // Round up to nearest multiple of 16\n    std::vector<int> is_prime(size, 1);\n\n    // Use SIMD to mark non-prime numbers\n    for (int i = 2; i * i <= limit; ++i) {\n        if (is_prime[i]) {\n            __m128i vec_i = _mm_set1_epi32(i);\n            for (int j = i * i; j <= limit; j += 4 * i) {\n                __m128i vec_j = _mm_set1_epi32(j);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n / 2); // Reserve space for efficiency\n\n    __m128i two = _mm_set1_epi32(2);\n    for (int i = 2; i < n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i is_prime = _mm_set1_epi32(1);\n\n        for (int j = 2; j * j < n; ++j) {\n            __m128i divisor = _mm_set1_epi32(j);\n            __m128i remainders = _mm_rem_epi32(numbers, divisor);\n            __m128i zero = _mm_set1_epi32(0);\n            __m128i cmp = _mm_cmpeq_epi32(remainders, zero);\n            is_prime = _mm_andnot_si128(cmp, is_prime);\n        }\n\n        int buffer[4];\n        _mm_storeu_si128((__m128i*)buffer, is_prime);\n\n        for (int k = 0; k < 4; ++k) {\n            if (buffer[k] && (i + k) < n) {\n                primes.push_back(i + k);\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Initialize a mask for prime numbers\n    std::vector<int> is_prime(n, 1);\n    is_prime[0] = is_prime[1] = 0; // 0 and 1 are not prime\n\n    // Use SIMD to mark non-prime numbers\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            __m128i vec_i = _mm_set1_epi32(i);\n            for (int j = i * i; j < n; j += 4 * i) {\n                __m128i vec_j = _mm_set_epi32(j + 3 * i, j + 2 * i, j + i, j);\n                __m128i mod = _mm_mullo_epi32(vec_i, vec_j);\n                _mm_storeu_si128((__m128i*)&is_prime[j], mod);\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    // Initialize a mask for numbers from 2 to n-1\n    std::vector<int> is_prime(n, 1);\n    is_prime[0] = is_prime[1] = 0; // 0 and 1 are not prime\n\n    int sqrt_n = static_cast<int>(sqrt(n)) + 1;\n\n    for (int i = 2; i <= sqrt_n; ++i) {\n        if (is_prime[i]) {\n            // Use SIMD to mark multiples of i as non-prime\n            __m128i vec_i = _mm_set1_epi32(i);\n            for (int j = i * i; j < n; j += 4 * i) {\n                __m128i vec_j = _mm_set_epi32(j + 3 * i, j + 2 * i, j + i, j);\n                __m128i vec_mask = _mm_cmpeq_epi32(_mm_rem_epi32(vec_j, vec_i), _mm_setzero_si128());\n                _mm_storeu_si128(reinterpret_cast<__m128i*>(&is_prime[j]), vec_mask);\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *data = s.c_str();\n    size_t len = s.size();\n    __m128i uppercase_vowels = _mm_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n\n    for (size_t i = 0; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(data + i));\n        __m128i indices = _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, 0, 0, 0, 0, 0, 0, 0, 0);\n        __m128i even_indices = _mm_shuffle_epi8(chunk, indices);\n        __m128i mask = _mm_cmpeq_epi8(even_indices, uppercase_vowels);\n        count += _mm_popcnt_epi8(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char uppercase_vowels[] = \"AEIOU\";\n    __m128i vowel_mask = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U');\n    int count = 0;\n    size_t len = s.size();\n\n    for (size_t i = 0; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(s.data() + i));\n        __m128i result = _mm_cmpeq_epi8(chunk, vowel_mask);\n        count += _mm_popcnt_epi8(result);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *data = s.c_str();\n    size_t len = s.size();\n    __m128i uppercase_vowels = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U');\n    int count = 0;\n\n    for (size_t i = 0; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(data + i));\n        __m128i even_mask = _mm_set_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m128i even_indices = _mm_and_si128(chunk, even_mask);\n\n        __m128i cmp = _mm_cmpeq_epi8(even_indices, uppercase_vowels);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n    }\n\n    for (size_t i = len - len % 16; i < len; i += 2) {\n        if (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char uppercase_vowels[] = \"AEIOU\";\n    __m128i vowels = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U');\n    int count = 0;\n\n    size_t len = s.size();\n    size_t i = 0;\n\n    for (; i + 16 <= len; i += 16) {\n        __m128i chars = _mm_loadu_si128((__m128i *)(s.data() + i));\n        __m128i indices = _mm_set_epi8(15, 13, 11, 9, 7, 5, 3, 1, 15, 13, 11, 9, 7, 5, 3, 1);\n        __m128i matches = _mm_cmpeq_epi8(chars, vowels);\n        count += _mm_popcnt_epi8(matches);\n    }\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    const char *str = s.c_str();\n    size_t len = s.size();\n    __m128i uppercase_vowels = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U');\n    int count = 0;\n\n    for (size_t i = 0; i + 16 <= len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(str + i));\n        __m128i mask_even_indices = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0, 0xFF, 0);\n        __m128i masked_chunk = _mm_and_si128(chunk, mask_even_indices);\n        __m128i cmp = _mm_cmpeq_epi8(masked_chunk, uppercase_vowels);\n        int result = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(result);\n    }\n\n    for (size_t i = len - len % 16; i < len; i += 2) {\n        if (str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i current_level = _mm_set1_epi32(n); // Initialize with n\n    __m128i increment = _mm_set1_epi32(2);     // Increment by 2 for each level\n\n    for (int i = 0; i < n; i += 4) {\n        // Compute up to 4 levels at a time\n        __m128i levels = _mm_add_epi32(current_level, _mm_set_epi32(6, 4, 2, 0));\n        _mm_storeu_si128((__m128i*)&result[i], levels);\n        current_level = _mm_add_epi32(current_level, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i current_level = _mm_set1_epi32(n); // Initialize all elements to n\n    __m128i increment = _mm_set1_epi32((n % 2 == 0) ? 2 : 2); // Increment is always 2 (odd/even logic handled by starting value)\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 0; i < n; i+=4) {\n        _mm_storeu_si128((__m128i*)&result[i],current_level)\n        current_level+=increment\n    }\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i current_level = _mm_set1_epi32(n); // Initialize all 4 lanes with n\n    __m128i increment = _mm_set1_epi32((n % 2 == 0) ? 2 : 2); // Increment is always 2\n    __m128i two = _mm_set1_epi32(2);\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        _mm_storeu_si128((__m128i*)&result[i], current_level); // Store current levels\n        current_level = _mm_add_epi32(current_level, increment); // Increment levels\n    }\n\n    // Handle remaining levels if n is not a multiple of 4\n    for (; i < n; ++i) {\n        result[i] = n + i * 2;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    // Determine the number of levels\n    std::vector<int> result(n);\n\n    // Initialize the first level\n    result[0] = n;\n\n    // Prepare SIMD variables\n    __m128i current = _mm_set1_epi32(n); // Initialize with n\n    __m128i increment = _mm_set1_epi32((n % 2 == 0) ? 2 : 2); // Increment by 2 (odd/even adjustment)\n\n    for (int i = 1; i < n; i+=4) {\n        // Generate next levels\n        __m128i next = _mm_add_epi32(current,increment);\n    }\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    // Determine the number of levels\n    std::vector<int> result(n);\n\n    // Initialize the first level\n    int current = n;\n\n    // Load the initial value into an SSE register\n    __m128i current_vec = _mm_set1_epi32(current);\n\n    // Create a vector for the increment (odd/even progression)\n    __m128i increment_vec = _mm_set1_epi32(2);\n\n    for (int i = 0; i < n; i+=4) {\n        // Store the current value into the result vector\n        _mm_storeu_si128((__m128i*)&result[i], current_vec);\n\n        // Increment the current value\n        current_vec = _mm_add_epi32(current_vec, increment_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Compute the sum of integers from n to m using SIMD\n    int64_t range = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i step = _mm_set1_epi64x(1);\n    __m128i current = _mm_set_epi64x(n + 1, n);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i values = _mm_add_epi64(current, step);\n        sum = _mm_add_epi64(sum, values);\n        current = _mm_add_epi64(current, _mm_set1_epi64x(2));\n    }\n\n    // Extract the sum from the SIMD register\n    int64_t sum_array[2];\n    _mm_storeu_si128((__m128i*)sum_array, sum);\n    int64_t total_sum = sum_array[0] + sum_array[1];\n\n    // Compute the average\n    int64_t average = total_sum / range;\n\n    // Round the average to the nearest integer (smaller one)\n    int64_t rounded_avg = std::floor(average);\n\n    // Convert the rounded average to binary\n    std::string binary_result = \"\";\n    while (rounded_avg > 0) {\n        binary_result = std::to_string(rounded_avg % 2) + binary_result;\n        rounded_avg /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Calculate the number of elements\n    int64_t count = m - n + 1;\n\n    // Use SIMD to compute the sum of integers from n to m\n    __m128i sum = _mm_setzero_si128();\n    __m128i vec_n = _mm_set1_epi64x(n);\n    __m128i vec_step = _mm_set1_epi64x(1);\n\n    for (int64_t i = 0; i < count; i += 2) {\n        __m128i vec_i = _mm_set_epi64x(n + i + 1, n + i);\n        sum = _mm_add_epi64(sum, vec_i);\n    }\n\n    // Extract the sum from the SIMD register\n    int64_t sum_array[2];\n    _mm_store_si128((__m128i*)sum_array, sum);\n    int64_t total_sum = sum_array[0] + sum_array[1];\n\n    // Compute the average\n    int64_t avg = total_sum / count;\n\n    // Round to the nearest integer (smaller one)\n    avg = std::floor(avg);\n\n    // Convert to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = std::to_string(avg % 2) + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Calculate the number of elements in the range\n    int64_t count = m - n + 1;\n\n    // Split the range into chunks of 4 for SIMD processing\n    int64_t chunks = count / 4;\n    int64_t remainder = count % 4;\n\n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n\n    // Process chunks of 4 using SIMD\n    for (int64_t i = 0; i < chunks; ++i) {\n        __m128i values = _mm_set_epi32(n + i * 4 + 3, n + i * 4 + 2, n + i * 4 + 1, n + i * 4);\n        sum_vec = _mm_add_epi32(sum_vec, values);\n    }\n\n    // Extract the sum from the SIMD vector\n    int64_t sum = 0;\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    // Process the remainder elements\n    for (int64_t i = 0; i < remainder; ++i) {\n        sum += n + chunks * 4 + i;\n    }\n\n    // Compute the average and round to the nearest integer\n    int64_t avg = std::round(static_cast<double>(sum) / count);\n\n    // Convert the average to binary\n    std::string binary = \"\";\n    while (avg > 0) {\n        binary = (avg % 2 == 0 ? \"0\" : \"1\") + binary;\n        avg /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Calculate the number of elements in the range [n, m]\n    int64_t count = m - n + 1;\n\n    // Split the range into chunks of 4 for SIMD processing\n    __m128i sum = _mm_setzero_si128();\n    int64_t i = n;\n\n    while (i + 4 <= m + 1) {\n        __m128i values = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        sum = _mm_add_epi32(sum, values);\n        i += 4;\n    }\n\n    // Handle remaining elements\n    int64_t remaining_sum = 0;\n    while (i <= m) {\n        remaining_sum += i;\n        i++;\n    }\n\n    // Extract the sum from the SIMD register\n    int64_t simd_sum[4];\n    _mm_storeu_si128((__m128i*)simd_sum, sum);\n    int64_t total_sum = simd_sum[0] + simd_sum[1] + simd_sum[2] + simd_sum[3] + remaining_sum;\n\n    // Compute the average and round to the nearest integer (smaller one)\n    int64_t avg = total_sum / count;\n\n    // Convert the average to binary\n    std::string binary_result = \"\";\n    while (avg > 0) {\n        binary_result = (avg % 2 == 0 ? \"0\" : \"1\") + binary_result;\n        avg /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    // Compute the sum of integers from n to m using SIMD\n    int64_t range = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i current = _mm_set_epi64x(n, n + 1);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i temp = _mm_add_epi64(current, _mm_set_epi64x(1, 1));\n        sum = _mm_add_epi64(sum, current);\n        current = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_val = _mm_set1_epi32(10000); // Modulo value for factorial computation\n\n    for (int i = 0; i < n; i += 4) {\n        // Load indices i, i+1, i+2, i+3\n        __m128i indices = _mm_set_epi32(i + 4, i + 3, i + 2, i + 1);\n\n        // Compute even mask\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1)); // Check if index is even\n        even_mask = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());   // Mask for even indices\n\n        // Compute factorials for even indices\n        __m128i factorials = _mm_set1_epi32(1);\n        for (int j = 1; j <= 4; ++j) {\n            __m128i temp = _mm_set_epi32(i + j, i + j, i + j, i + j);\n            factorials = _mm_mullo_epi32(factorials,temp)\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_val = _mm_set1_epi32(10000); // Modulo value for factorial calculations\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i); // Load indices i, i+1, i+2, i+3\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1)); // Check if indices are even\n        even_mask = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128()); // Mask for even indices\n\n        __m128i factorials = _mm_set1_epi32(1); // Initialize factorials to 1\n        __m128i sums = _mm_setzero_si128(); // Initialize sums to 0\n\n        for (int j = 1; j <= 4; ++j) {\n            __m128i current = _mm_set1_epi32(j);\n            factorials = _mm_mullo_epi32(factorials, current); // Calculate factorials\n            sums = _mm_add_epi32(sums, current); // Calculate sums\n        }\n\n        factorials = _mm_rem_epi32(factorials, mod_val); // Apply modulo 10,000 to factorials\n\n        __m128i result_values = _mm_blendv_epi8(sums, factorials, even_mask); // Select factorials for even indices, sums for odd\n        _mm_storeu_si128((__m128i*)&result[i - 1], result_values); // Store results in the output vector\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_val = _mm_set1_epi32(10000); // Modulo value for factorial computation\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 4, i + 3, i + 2, i + 1); // Load indices\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1)); // Check if indices are even\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128()); // Compare with zero for evenness\n\n        // Compute factorial for even indices\n        __m128i factorial = _mm_set1_epi32(1);\n        for (int j = 1; j <= 4; ++j) {\n            __m128i current = _mm_set_epi32(i + 4, i + 3, i + 2, i + 1);\n            factorial = _mm_mullo_epi32(factorial, current);\n            factorial = _mm_rem_epi32(factorial, mod_val);\n        }\n\n        // Compute sum for odd indices\n        __m128i sum = _mm_setzero_si128();\n        for (int j = 1; j <= 4; ++j) {\n            sum = _mm_add_epi32(sum, current);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i modulo = _mm_set1_epi32(10000); // For modulo 10,000\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 4, i + 3, i + 2, i + 1);\n\n        // Check if indices are even\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1));\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n\n        // Compute factorial for even indices\n        __m128i factorial = _mm_set1_epi32(1);\n        for (int j = 1; j <= 4; ++j) {\n            __m128i current = _mm_set_epi32(\n                (i + 4 >= j) ? j : 1,\n                (i + 3 >= j) ? j : 1,\n                (i + 2 >= j) ? j : 1,\n                (i + 1 >= j) ? j : 1\n            );\n            factorial = _mm_mullo_epi32(factorial, current);\n            factorial = _mm_and_si128(factorial,modulo)\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i mod_val = _mm_set1_epi32(10000); // Modulo value for factorial computation\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 4, i + 3, i + 2, i + 1); // Load indices\n        __m128i even_mask = _mm_and_si128(indices, _mm_set1_epi32(1)); // Check if indices are even\n        even_mask = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128()); // Create mask for even indices\n\n        // Compute factorial for even indices\n        __m128i factorial = _mm_set1_epi32(1);\n        for (int j = 1; j <= 4; ++j) {\n            __m128i multiplier = _mm_set_epi32(\n                (i + 4 >= j) ? j : 1,\n                (i + 3 >= j) ? j : 1,\n                (i + 2 >= j) ? j : 1,\n                (i + 1 >= j) ? j : 1\n            );\n            factorial = _mm_mullo_epi32(factorial, multiplier);\n            factorial = _mm_rem_epi32(factorial, mod_val); // Apply modulo\n        }\n\n        // Compute sum for odd indices\n        __m128i sum = _mm_set_epi32(\n            (i + 4) * (i + 4 + 1) / 2,\n            (i + 3) * (i + 3 + 1) / 2,\n            (i + 2) * (i + 2 + 1) / 2,\n            (i + 1) * (i + 1 + 1) / 2\n        );\n\n        // Blend results based on even_mask\n        __m128i blended = _mm_blendv_epi8(sum, factorial, even_mask);\n\n        // Store results\n        _mm_storeu_si128((__m128i*)&result[i], blended);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i even_count = _mm_setzero_si128(); // Vector to store even count\n    __m128i odd_count = _mm_setzero_si128();  // Vector to store odd count\n\n    for (int i = 1; i <= n; i += 4) {\n        // Load 4 consecutive integers into an SSE vector\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n\n        // Check if each number is a palindrome\n        __m128i reversed = _mm_set_epi32(\n            (i + 3 == reverse_number(i + 3)) ? 1 : 0,\n            (i + 2 == reverse_number(i + 2)) ? 1 : 0,\n            (i + 1 == reverse_number(i + 1)) ? 1 : 0,\n            (i == reverse_number(i)) ? 1 : 0\n        );\n\n        // Mask to check even numbers\n        __m128i even_mask = _mm_and_si128(numbers, _mm_set1_epi32(1));\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n\n        // Count even and odd palindromes\n        even_count = _mm_add_epi32(even_count, _mm_and_si128(reversed, is_even));\n        odd_count = _mm_add_epi32(odd_count, _mm_and_si128(reversed, _mm_xor_si128(is_even, _mm_set1_epi32(1))));\n    }\n\n    // Sum up the counts in the vector\n    int even_result[4], odd_result[4];\n    _mm_storeu_si128((__m128i*)even_result, even_count);\n    _mm_storeu_si128((__m128i*)odd_result, odd_count);\n\n    int total_even = even_result[0] + even_result[1] + even_result[2] + even_result[3];\n    int total_odd = odd_result[0] + odd_result[1] + odd_result[2] + odd_result[3];\n\n    return {total_even, total_odd};\n}\n\nint reverse_number(int num) {\n    int reversed = 0;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return reversed;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\ninline bool is_palindrome(int num) {\n    std::string str = std::to_string(num);\n    int len = str.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks of 4 using SSE intrinsics\n    for (int i = 1; i <= n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i); // Load 4 numbers\n        __m128i mask = _mm_set1_epi32(0); // Initialize mask for palindromes\n\n        // Check each number for palindrome property\n        for (int j = 0; j < 4; ++j) {\n            int num = _mm_extract_epi32(numbers, j);\n            if (num <= n && is_palindrome(num)) {\n                mask = _mm_insert_epi32(mask, 1, j); // Mark as palindrome\n            }\n        }\n\n        // Separate even and odd palindromes\n        __m128i evens = _mm_and_si128(mask, _mm_set1_epi32(0xFFFFFFFE)); // Mask even numbers\n        __m128i odds = _mm_and_si128(mask, _mm_set1_epi32(0x00000001));  // Mask odd numbers\n\n        even_count += _mm_popcnt_u32(_mm_movemask_epi8(evens)); // Count even palindromes\n        odd_count += _mm_popcnt_u32(_mm_movemask_epi8(odds));   // Count odd palindromes\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n#include <xmmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome(int num) {\n    int reversed = 0, original = num;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return original == reversed;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks of 4 using SIMD\n    for (int i = 1; i <= n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i mask = _mm_set1_epi32(0);\n\n        // Check if each number is a palindrome\n        for (int j = 0; j < 4; ++j) {\n            int num = _mm_extract_epi32(numbers, j);\n            if (num > n) break; // Skip numbers outside the range\n            if (is_palindrome(num)) {\n                if (num % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <string>\n\n// Helper function to check if a number is a palindrome\nbool is_palindrome(int num) {\n    std::string s = std::to_string(num);\n    int len = s.size();\n    for (int i = 0; i < len / 2; ++i) {\n        if (s[i] != s[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks of 4 using SSE2\n    __m128i even_vec = _mm_setzero_si128(); // Vector to count even numbers\n    __m128i odd_vec = _mm_setzero_si128();  // Vector to count odd numbers\n\n    for (int i = 1; i <= n; i += 4) {\n        // Load 4 consecutive numbers\n        __m128i nums = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n\n        // Check if each number is a palindrome\n        int results[4];\n        for (int j = 0; j < 4; ++j) {\n            int num = i + j;\n            results[j] = (num <= n && is_palindrome(num)) ? 1 : 0;\n        }\n\n        // Create a mask for palindromes\n        __m128i mask = _mm_set_epi32(results[3], results[2], results[1], results[0]);\n\n        // Extract even/odd numbers\n        __m128i evens = _mm_and_si128(mask, _mm_set_epi32((i + 3) % 2 == 0, (i + 2) % 2 == 0, (i + 1) % 2 == 0, i % 2 == 0));\n        __m128i odds = _mm_andnot_si128(evens, mask);\n\n        // Accumulate counts\n        even_vec = _mm_add_epi32(even_vec, evens);\n        odd_vec = _mm_add_epi32(odd_vec, odds);\n    }\n\n    // Sum up the results in the vectors\n    int even_arr[4], odd_arr[4];\n    _mm_storeu_si128((__m128i*)even_arr, even_vec);\n    _mm_storeu_si128((__m128i*)odd_arr, odd_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        even_count += even_arr[i];\n        odd_count += odd_arr[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome(int num) {\n    int reversed = 0, original = num;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return original == reversed;\n}\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n\n    // Process numbers in chunks of 4 using SIMD\n    for (int i = 1; i <= n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i mask = _mm_set1_epi32(n);\n        __m128i cmp = _mm_cmpgt_epi32(numbers, mask); // Mask out-of-range numbers\n        numbers = _mm_andnot_si128(cmp, numbers);\n\n        int buffer[4];\n        _mm_storeu_si128((__m128i*)buffer, numbers);\n\n        for (int j = 0; j < 4; ++j) {\n            if (buffer[j] > 0 && is_palindrome(buffer[j])) {\n                if (buffer[j] % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int result = 0;\n    int remainder = size % 4;\n    int limit = size - remainder;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < limit; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n\n        // Extract digits and compute sum of digits for each number\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sum_digits = zero;\n\n        for (int j = 0; j < 10; ++j) {\n            __m128i mod10 = _mm_rem_epi32(abs_vec, _mm_set1_epi32(10));\n            sum_digits = _mm_add_epi32(sum_digits, mod10);\n            abs_vec = _mm_div_epi32(abs_vec, _mm_set1_epi32(10));\n        }\n\n        // Check if sum of digits > 0\n        __m128i cmp = _mm_cmpgt_epi32(sum_digits, zero);\n        count = _mm_add_epi32(count, _mm_and_si128(cmp, _mm_set1_epi32(1)));\n    }\n\n    // Sum up the counts from the SIMD registers\n    int counts[4];\n    _mm_storeu_si128((__m128i*)counts, count);\n    for (int i = 0; i < 4; ++i) {\n        result += counts[i];\n    }\n\n    // Handle the remainder elements\n    for (int i = limit; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n\n        if (sum_digits > 0) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int result = 0;\n    int remainder = size % 4;\n    int limit = size - remainder;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < limit; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n\n        __m128i sum_digits = zero;\n\n        for (int j = 0; j < 10; ++j) {\n            __m128i mod = _mm_rem_epi32(abs_vec, _mm_set1_epi32(10));\n            sum_digits = _mm_add_epi32(sum_digits, mod);\n            abs_vec = _mm_div_epi32(abs_vec, _mm_set1_epi32(10));\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sum_digits, zero);\n        count = _mm_add_epi32(count, _mm_and_si128(cmp, _mm_set1_epi32(1)));\n    }\n\n    alignas(16) int buffer[4];\n    _mm_store_si128((__m128i*)buffer, count);\n    result = buffer[0] + buffer[1] + buffer[2] + buffer[3];\n\n    for (int i = limit; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n\n        if (sum_digits > 0) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int result = 0;\n    int remainder = size % 4;\n    int limit = size - remainder;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < limit; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n\n        __m128i sum_digits = zero;\n        for (int j = 0; j < 10; ++j) {\n            __m128i mod = _mm_rem_epi32(abs_vec, _mm_set1_epi32(10));\n            sum_digits = _mm_add_epi32(sum_digits, mod);\n            abs_vec = _mm_div_epi32(abs_vec, _mm_set1_epi32(10));\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sum_digits, zero);\n        count = _mm_add_epi32(count, _mm_and_si128(cmp, _mm_set1_epi32(1)));\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, count);\n    for (int i = 0; i < 4; ++i) {\n        result += temp[i];\n    }\n\n    for (int i = limit; i < size; ++i) {\n        int num = n[i];\n        int abs_num = std::abs(num);\n        int sum_digits = 0;\n\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n\n        if (sum_digits > 0) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int count = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = _mm_setzero_si128();\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sum_digits = _mm_setzero_si128();\n\n        while (!_mm_test_all_zeros(abs_vec, abs_vec)) {\n            __m128i digit = _mm_rem_epi32(abs_vec, _mm_set1_epi32(10));\n            sum_digits = _mm_add_epi32(sum_digits, digit);\n            abs_vec = _mm_div_epi32(abs_vec, _mm_set1_epi32(10));\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sum_digits, zero);\n        result = _mm_add_epi32(result, _mm_and_si128(cmp, _mm_set1_epi32(1)));\n    }\n\n    alignas(16) int result_array[4];\n    _mm_store_si128((__m128i*)result_array, result);\n    for (int j = 0; j < 4; ++j) {\n        count += result_array[j];\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        num = std::abs(num);\n        while (num > 0) {\n            sum += num % 10;\n            num /= 10;\n        }\n        if (sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int size = n.size();\n    if (size == 0) return 0;\n\n    int count = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = _mm_setzero_si128();\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n\n        __m128i sum_digits = _mm_setzero_si128();\n\n        while (!_mm_test_all_zeros(abs_nums, abs_nums)) {\n            __m128i digits = _mm_rem_epi32(abs_nums, _mm_set1_epi32(10));\n            sum_digits = _mm_add_epi32(sum_digits, digits);\n            abs_nums = _mm_div_epi32(abs_nums, _mm_set1_epi32(10));\n        }\n\n        __m128i mask = _mm_cmpgt_epi32(sum_digits, zero);\n        result = _mm_add_epi32(result, _mm_and_si128(mask, _mm_set1_epi32(1)));\n    }\n\n    alignas(16) int result_array[4];\n    _mm_store_si128((__m128i*)result_array, result);\n\n    for (int j = 0; j < 4; ++j) {\n        count += result_array[j];\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum_digits = 0;\n        int abs_num = std::abs(num);\n\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n\n        if (sum_digits > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            sorted = false;\n            break;\n        }\n    }\n    if (sorted) return true;\n\n    // Perform the check for all possible right shifts\n    for (int shift = 1; shift < n; ++shift) {\n        // Create the shifted array\n        std::vector<int> shifted(n);\n        for (int i = 0; i < n; ++i) {\n            shifted[i] = arr[(i + shift) % n];\n        }\n\n        // Use SSE/SSE2 intrinsics to check if the shifted array is sorted\n        bool is_sorted = true;\n        for (int i = 0; i < n - 4; i += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)&shifted[i]);\n            __m128i next = _mm_loadu_si128((__m128i*)&shifted[i + 1]);\n            __m128i cmp = _mm_cmpgt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        // Check remaining elements\n        for (int i = n - (n % 4); i < n - 1; ++i) {\n            if (shifted[i] > shifted[i + 1]) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool is_sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n    if (is_sorted) return true;\n\n    // Perform SIMD-based check for all possible rotations\n    for (int shift = 0; shift < n; ++shift) {\n        // Create a rotated array\n        std::vector<int> rotated(n);\n        for (int i = 0; i < n; ++i) {\n            rotated[i] = arr[(i + shift) % n];\n        }\n\n        // Use SIMD to check if the rotated array is sorted\n        __m128i prev = _mm_set1_epi32(rotated[0]);\n        bool sorted = true;\n        for (int i = 1; i < n; i += 4) {\n            __m128i curr = _mm_loadu_si128((__m128i*)&rotated[i]);\n            __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n            prev = curr;\n        }\n\n        // Handle remaining elements if n is not a multiple of 4\n        if (sorted && n % 4 != 0) {\n            for (int i = n - n % 4; i < n; ++i) {\n                if (rotated[i] < rotated[i - 1]) {\n                    sorted = false;\n                    break;\n                }\n            }\n        }\n\n        if (sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool sorted = true;\n    for (int i = 1; i < n; ++i) {\n        if (arr[i - 1] > arr[i]) {\n            sorted = false;\n            break;\n        }\n    }\n    if (sorted) return true;\n\n    // Prepare the array for SIMD processing\n    std::vector<int> extended_arr(arr.begin(), arr.end());\n    extended_arr.insert(extended_arr.end(), arr.begin(), arr.end());\n\n    // Use SIMD to check for sorted subarray\n    __m128i prev, curr;\n    for (int shift = 0; shift < n; ++shift) {\n        bool is_sorted = true;\n\n        for (int i = 0; i < n; i += 4) {\n            if (i + 4 > n) {\n                // Handle remaining elements without SIMD\n                for (int j = i; j < n; ++j) {\n                    if (extended_arr[shift + j] > extended_arr[shift + j + 1]) {\n                        is_sorted = false;\n                        break;\n                    }\n                }\n                break;\n            }\n\n            prev = _mm_loadu_si128((__m128i*)&extended_arr[shift + i]);\n            curr = _mm_loadu_si128((__m128i*)&extended_arr[shift + i + 1]);\n\n            __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool sorted = true;\n    for (int i = 1; i < n; i++) {\n        if (arr[i] < arr[i - 1]) {\n            sorted = false;\n            break;\n        }\n    }\n    if (sorted) return true;\n\n    // Use SIMD to check for the sorted condition after each right shift\n    for (int shift = 1; shift < n; shift++) {\n        // Perform the right shift\n        std::vector<int> shifted(n);\n        for (int i = 0; i < n; i++) {\n            shifted[i] = arr[(i - shift + n) % n];\n        }\n\n        // Check if the shifted array is sorted using SSE intrinsics\n        __m128i prev = _mm_set1_epi32(shifted[0]);\n        bool is_sorted = true;\n\n        for (int i = 1; i < n; i += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)&shifted[i]);\n            __m128i cmp = _mm_cmpgt_epi32(prev, current);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                is_sorted = false;\n                break;\n            }\n            prev = current;\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Check if the array is already sorted\n    bool is_sorted = true;\n    for (int i = 1; i < n; i++) {\n        if (arr[i] < arr[i - 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n    if (is_sorted) return true;\n\n    // Create a double-sized array to simulate the circular shifts\n    std::vector<int> extended_arr(2 * n);\n    for (int i = 0; i < n; i++) {\n        extended_arr[i] = arr[i];\n        extended_arr[i + n] = arr[i];\n    }\n\n    // Use SIMD to check for sorted subarrays of size n\n    for (int shift = 0; shift < n; shift++) {\n        bool sorted = true;\n\n        // Use SIMD to compare elements in blocks of 4\n        for (int i = 0; i < n; i += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)&extended_arr[shift + i]);\n            __m128i next = _mm_loadu_si128((__m128i*)&extended_arr[shift + i + 1]);\n\n            __m128i cmp = _mm_cmpgt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                sorted = false;\n                break;\n            }\n        }\n\n        // Check remaining elements if n is not a multiple of 4\n        for (int i = (n / 4) * 4; i < n; i++) {\n            if (extended_arr[shift + i] > extended_arr[shift + i + 1]) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Ensure both vectors are divisible by 4 for SIMD processing\n    size_t padded_size1 = (size1 + 3) & ~3;\n    size_t padded_size2 = (size2 + 3) & ~3;\n\n    std::vector<int> padded_lst1 = lst1;\n    std::vector<int> padded_lst2 = lst2;\n\n    padded_lst1.resize(padded_size1, 0);\n    padded_lst2.resize(padded_size2, 0);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1); // Mask for checking even numbers\n\n    // Check lst1 for odd numbers\n    bool has_odd_in_lst1 = false;\n    for (size_t i = 0; i < padded_size1; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst1[i]);\n        __m128i odd_mask = _mm_and_si128(vec, mask_even);\n        if (!_mm_test_all_zeros(odd_mask, odd_mask)) {\n            has_odd_in_lst1 = true;\n            break;\n        }\n    }\n\n    if (!has_odd_in_lst1) {\n        return \"YES\"; // lst1 already contains only even numbers\n    }\n\n    // Check lst2 for even numbers\n    bool has_even_in_lst2 = false;\n    for (size_t i = 0; i < padded_size2; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst2[i]);\n        __m128i odd_mask = _mm_and_si128(vec, mask_even);\n        if (_mm_test_all_zeros(odd_mask, odd_mask)) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    if (has_even_in_lst2) {\n        return \"YES\"; // lst2 contains even numbers that can be exchanged\n    }\n\n    return \"NO\"; // No way to make lst1 contain only even numbers\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Convert vectors to aligned arrays for SIMD processing\n    std::vector<int> aligned_lst1 = lst1;\n    std::vector<int> aligned_lst2 = lst2;\n\n    // Check lst1 for any odd numbers\n    bool has_odd_in_lst1 = false;\n    for (size_t i = 0; i < size1; i += 4) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&aligned_lst1[i]);\n        __m128i mod2 = _mm_and_si128(vec1, _mm_set1_epi32(1)); // Check if odd\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(mod2, _mm_setzero_si128())) != 0xFFFF) {\n            has_odd_in_lst1 = true;\n            break;\n        }\n    }\n\n    if (!has_odd_in_lst1) {\n        return \"YES\"; // lst1 already contains only even numbers\n    }\n\n    // Check lst2 for at least one even number\n    bool has_even_in_lst2 = false;\n    for (size_t i = 0; i < size2; i += 4) {\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&aligned_lst2[i]);\n        __m128i mod2 = _mm_and_si128(vec2, _mm_set1_epi32(1)); // Check if even\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(mod2, _mm_setzero_si128())) != 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    return has_even_in_lst2 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Ensure both vectors have a size divisible by 4 for SIMD processing\n    size_t padded_size1 = (size1 + 3) & ~3;\n    size_t padded_size2 = (size2 + 3) & ~3;\n\n    std::vector<int> padded_lst1(padded_size1, 0);\n    std::vector<int> padded_lst2(padded_size2, 0);\n\n    std::copy(lst1.begin(), lst1.end(), padded_lst1.begin());\n    std::copy(lst2.begin(), lst2.end(), padded_lst2.begin());\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i two = _mm_set1_epi32(2);\n\n    // Check lst1 for odd numbers\n    bool has_odd_in_lst1 = false;\n    for (size_t i = 0; i < padded_size1; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst1[i]);\n        __m128i remainder = _mm_and_si128(vec, _mm_set1_epi32(1));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            has_odd_in_lst1 = true;\n            break;\n        }\n    }\n\n    if (!has_odd_in_lst1) {\n        return \"YES\";\n    }\n\n    // Check lst2 for even numbers\n    bool has_even_in_lst2 = false;\n    for (size_t i = 0; i < padded_size2; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst2[i]);\n        __m128i remainder = _mm_and_si128(vec, _mm_set1_epi32(1));\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    if (has_even_in_lst2) {\n        return \"YES\";\n    }\n\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Convert input vectors to aligned arrays for SIMD processing\n    std::vector<int> aligned_lst1(size1 + (4 - size1 % 4) % 4, 0);\n    std::vector<int> aligned_lst2(size2 + (4 - size2 % 4) % 4, 0);\n\n    std::copy(lst1.begin(), lst1.end(), aligned_lst1.begin());\n    std::copy(lst2.begin(), lst2.end(), aligned_lst2.begin());\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1); // Mask for checking even numbers\n\n    // Check lst1 for odd numbers\n    bool has_odd_in_lst1 = false;\n    for (size_t i = 0; i < aligned_lst1.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&aligned_lst1[i]);\n        __m128i remainder = _mm_and_si128(vec, mask_even);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, mask_even);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_odd_in_lst1 = true;\n            break;\n        }\n    }\n\n    if (!has_odd_in_lst1) {\n        return \"YES\";\n    }\n\n    // Check lst2 for even numbers\n    bool has_even_in_lst2 = false;\n    for (size_t i = 0; i < aligned_lst2.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&aligned_lst2[i]);\n        __m128i remainder = _mm_and_si128(vec, mask_even);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    return has_even_in_lst2 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <vector>\n#include <string>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Ensure both vectors are padded to multiples of 4 for SIMD processing\n    size_t padded_size1 = (size1 + 3) & ~3;\n    size_t padded_size2 = (size2 + 3) & ~3;\n\n    std::vector<int> padded_lst1 = lst1;\n    std::vector<int> padded_lst2 = lst2;\n\n    padded_lst1.resize(padded_size1, 0);\n    padded_lst2.resize(padded_size2, 0);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1); // Mask to check even numbers\n\n    // Check lst1 for odd numbers\n    bool has_odd_in_lst1 = false;\n    for (size_t i = 0; i < padded_size1; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst1[i]);\n        __m128i remainder = _mm_and_si128(vec, mask_even);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, mask_even);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_odd_in_lst1 = true;\n            break;\n        }\n    }\n\n    if (!has_odd_in_lst1) {\n        return \"YES\";\n    }\n\n    // Check lst2 for even numbers\n    bool has_even_in_lst2 = false;\n    for (size_t i = 0; i < padded_size2; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&padded_lst2[i]);\n        __m128i remainder = _mm_and_si128(vec, mask_even);\n        __m128i cmp = _mm_cmpeq_epi32(remainder, zero);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            has_even_in_lst2 = true;\n            break;\n        }\n    }\n\n    return has_even_in_lst2 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        const char* input = str.c_str();\n        size_t len = str.size();\n        __m128i odd_mask = _mm_set1_epi8(1); // Mask for odd digits\n        __m128i zero = _mm_set1_epi8('0');  // ASCII value of '0'\n        __m128i two = _mm_set1_epi8(2);     // ASCII value of '2'\n        __m128i four = _mm_set1_epi8(4);    // ASCII value of '4'\n        __m128i six = _mm_set1_epi8(6);     // ASCII value of '6'\n        __m128i eight = _mm_set1_epi8(8);   // ASCII value of '8'\n\n        int odd_count = 0;\n        size_t i = 0;\n\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((__m128i*)(input + i));\n            __m128i digits = _mm_sub_epi8(chars, zero);\n\n            __m128i is_two = _mm_cmpeq_epi8(digits, two);\n            __m128i is_four = _mm_cmpeq_epi8(digits, four);\n            __m128i is_six = _mm_cmpeq_epi8(digits, six);\n            __m128i is_eight = _mm_cmpeq_epi8(digits, eight);\n\n            __m128i even_mask = _mm_or_si128(_mm_or_si128(is_two, is_four), _mm_or_si128(is_six, is_eight));\n            __m128i odd_mask = _mm_andnot_si128(even_mask, _mm_cmpgt_epi8(digits, zero));\n\n            odd_count += _mm_popcnt_u32(_mm_movemask_epi8(odd_mask));\n        }\n\n        for (; i < len; ++i) {\n            char c = input[i];\n            if (c == '1' || c == '3' || c == '5' || c == '7' || c == '9') {\n                odd_count++;\n            }\n        }\n\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << odd_count << \"n the str\" << odd_count << \"ng \" << odd_count << \" of the \" << odd_count << \"nput.\";\n        result.push_back(oss.str());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 16 characters using SIMD\n        for (size_t i = 0; i < len; i += 16) {\n            // Load up to 16 characters into an SSE register\n            __m128i chunk = _mm_setzero_si128();\n            if (i + 16 <= len) {\n                chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n            } else {\n                char temp[16] = {0};\n                std::copy(str.begin() + i, str.end(), temp);\n                chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n            }\n\n            // Subtract '0' to convert ASCII digits to integers\n            __m128i digits = _mm_sub_epi8(chunk, _mm_set1_epi8('0'));\n\n            // Mask to identify odd digits\n            __m128i odd_mask = _mm_and_si128(digits, _mm_set1_epi8(1));\n\n            // Count odd digits\n            __m128i odd_count_vec = _mm_cmpeq_epi8(odd_mask, _mm_set1_epi8(1));\n            int mask = _mm_movemask_epi8(odd_count_vec);\n            odd_count += __builtin_popcount(mask);\n        }\n\n        // Construct the output string\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) +\n                             \"n the str\" + std::to_string(odd_count) +\n                             \"ng \" + std::to_string(odd_count) + \" of the \" +\n                             std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n\n    for (const auto& str : lst) {\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 16 characters using SIMD\n        for (size_t i = 0; i < len; i += 16) {\n            __m128i chunk = _mm_setzero_si128();\n            size_t chunk_size = std::min(static_cast<size_t>(16), len - i);\n\n            // Load the chunk into an SSE register\n            for (size_t j = 0; j < chunk_size; ++j) {\n                reinterpret_cast<char*>(&chunk)[j] = str[i + j];\n            }\n\n            // Compare each character with odd digits ('1', '3', '5', '7', '9')\n            __m128i ones = _mm_set1_epi8('1');\n            __m128i threes = _mm_set1_epi8('3');\n            __m128i fives = _mm_set1_epi8('5');\n            __m128i sevens = _mm_set1_epi8('7');\n            __m128i nines = _mm_set1_epi8('9');\n\n            __m128i cmp1 = _mm_cmpeq_epi8(chunk, ones);\n            __m128i cmp3 = _mm_cmpeq_epi8(chunk, threes);\n            __m128i cmp5 = _mm_cmpeq_epi8(chunk, fives);\n            __m128i cmp7 = _mm_cmpeq_epi8(chunk, sevens);\n            __m128i cmp9 = _mm_cmpeq_epi8(chunk, nines);\n\n            // Combine the comparison results\n            __m128i odd_mask = _mm_or_si128(_mm_or_si128(cmp1, cmp3), _mm_or_si128(cmp5, _mm_or_si128(cmp7, cmp9)));\n\n            // Count the number of odd digits in the chunk\n            odd_count += _mm_popcnt_u32(_mm_movemask_epi8(odd_mask));\n        }\n\n        // Construct the output string\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) +\n                             \"n the str\" + std::to_string(odd_count) + \"ng \" +\n                             std::to_string(odd_count) + \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        size_t len = str.size();\n        size_t odd_count = 0;\n\n        // Process the string in chunks of 16 characters using SIMD\n        for (size_t i = 0; i < len; i += 16) {\n            // Load 16 characters into an SSE register\n            __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n\n            // Subtract '0' from each character to convert to integers\n            __m128i digits = _mm_sub_epi8(chars, _mm_set1_epi8('0'));\n\n            // Mask to identify odd digits (digits & 1 == 1)\n            __m128i odd_mask = _mm_and_si128(digits, _mm_set1_epi8(1));\n\n            // Compare odd_mask with 1 to get a mask of odd digits\n            __m128i odd_digits = _mm_cmpeq_epi8(odd_mask, _mm_set1_epi8(1));\n\n            // Count the number of odd digits in the 16-character chunk\n            int mask = _mm_movemask_epi8(odd_digits);\n            odd_count += __builtin_popcount(mask);\n        }\n\n        // Construct the output string\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" +\n                             std::to_string(odd_count) + \"ng \" + std::to_string(odd_count) + \" of the \" +\n                             std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        const char* input = str.c_str();\n        size_t len = str.size();\n        __m128i odd_mask = _mm_set1_epi8(1); // Mask for odd digits\n        __m128i zero = _mm_set1_epi8('0');  // ASCII value of '0'\n        __m128i nine = _mm_set1_epi8('9'); // ASCII value of '9'\n        int odd_count = 0;\n\n        size_t i = 0;\n        for (; i + 16 <= len; i += 16) {\n            __m128i chars = _mm_loadu_si128((__m128i*)(input + i));\n            __m128i is_digit = _mm_and_si128(_mm_cmpgt_epi8(chars, zero), _mm_cmplt_epi8(chars, nine));\n            __m128i digit_values = _mm_sub_epi8(chars, zero);\n            __m128i is_odd = _mm_and_si128(digit_values, odd_mask);\n            __m128i odd_flags = _mm_and_si128(is_odd, is_digit);\n            odd_count += _mm_popcnt_u32(_mm_movemask_epi8(odd_flags));\n        }\n\n        for (; i < len; ++i) {\n            char c = input[i];\n            if (c >= '0' && c <= '9' && ((c - '0') & 1)) {\n                odd_count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \" + std::to_string(odd_count) +\n                             \"n the str\" + std::to_string(odd_count) +\n                             \"ng \" + std::to_string(odd_count) +\n                             \" of the \" + std::to_string(odd_count) + \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <emmintrin.h>\n#include <stdint.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    const int64_t INF = std::numeric_limits<int64_t>::max();\n    __m128i min_sum = _mm_set1_epi64x(INF);\n    __m128i current_sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 2 <= n; i += 2) {\n        __m128i vec = _mm_set_epi64x(nums[i + 1], nums[i]);\n        current_sum = _mm_add_epi64(current_sum, vec);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n\n        // Reset current_sum to zero if it becomes positive\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpgt_epi64(current_sum, zero);\n        current_sum = _mm_andnot_si128(mask, current_sum);\n    }\n\n    // Handle the remaining elements\n    int64_t scalar_min = INF;\n    for (; i < n; ++i) {\n        scalar_min = std::min(scalar_min, nums[i]);\n    }\n\n    // Extract the minimum value from the SIMD register\n    int64_t min_values[2];\n    _mm_storeu_si128((__m128i*)min_values, min_sum);\n    scalar_min = std::min({scalar_min, min_values[0], min_values[1]});\n\n    return scalar_min;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    size_t n = nums.size();\n    size_t simd_width = 2; // SSE2 processes two 64-bit integers at a time\n    size_t aligned_size = (n / simd_width) * simd_width;\n\n    __m128i min_sum = _mm_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m128i current_sum = _mm_setzero_si128();\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_sum = _mm_add_epi64(current_sum, vec);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n\n        // Reset current_sum to zero if it becomes positive\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpgt_epi64(current_sum, zero);\n        current_sum = _mm_andnot_si128(mask, current_sum);\n    }\n\n    // Extract the minimum sum from the SIMD register\n    int64_t min_sum_array[simd_width];\n    _mm_storeu_si128((__m128i*)min_sum_array, min_sum);\n    int64_t result = std::min(min_sum_array[0], min_sum_array[1]);\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        result = std::min(result, nums[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    const size_t n = nums.size();\n    const size_t simd_width = 2; // SSE2 processes 2 int64_t values at a time\n    const size_t aligned_size = n - (n % simd_width);\n\n    __m128i current_min = _mm_set1_epi64x(0);\n    __m128i global_min = _mm_set1_epi64x(std::numeric_limits<int64_t>::max());\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_min = _mm_add_epi64(current_min, vec);\n        global_min = _mm_min_epi64(global_min, current_min);\n\n        // Reset current_min to zero if it becomes positive\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpgt_epi64(current_min, zero);\n        current_min = _mm_andnot_si128(mask, current_min);\n    }\n\n    // Extract the global minimum from the SIMD register\n    int64_t global_min_array[simd_width];\n    _mm_storeu_si128((__m128i*)global_min_array, global_min);\n\n    int64_t result = std::min(global_min_array[0], global_min_array[1]);\n\n    // Process remaining elements\n    int64_t current_min_scalar = 0;\n    for (size_t i = aligned_size; i < n; ++i) {\n        current_min_scalar += nums[i];\n        result = std::min(result, current_min_scalar);\n        if (current_min_scalar > 0) current_min_scalar = 0;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <emmintrin.h>\n#include <stdint.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    const size_t n = nums.size();\n    const size_t simd_width = 2; // SSE2 processes two 64-bit integers at a time\n    const size_t simd_iterations = n / simd_width;\n\n    __m128i current_min = _mm_set1_epi64x(std::numeric_limits<int64_t>::max());\n    __m128i current_sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i < simd_iterations * simd_width; i += simd_width) {\n        __m128i vec = _mm_set_epi64x(nums[i + 1], nums[i]);\n        current_sum = _mm_add_epi64(current_sum, vec);\n        current_min = _mm_min_epi64(current_min, current_sum);\n        current_sum = _mm_min_epi64(current_sum, _mm_setzero_si128());\n    }\n\n    int64_t min_result[simd_width];\n    _mm_storeu_si128((__m128i*)min_result, current_min);\n\n    int64_t global_min = std::min(min_result[0], min_result[1]);\n\n    int64_t scalar_sum = 0;\n    for (; i < n; ++i) {\n        scalar_sum += nums[i];\n        global_min = std::min(global_min, scalar_sum);\n        scalar_sum = std::min(scalar_sum, int64_t(0));\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    const size_t n = nums.size();\n    const size_t simd_width = 2; // SSE2 processes 2 int64_t values at a time\n    size_t simd_end = n - (n % simd_width);\n\n    __m128i current_min = _mm_set1_epi64x(0);\n    __m128i global_min = _mm_set1_epi64x(std::numeric_limits<int64_t>::max());\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_min = _mm_add_epi64(current_min, vec);\n        global_min = _mm_min_epi64(global_min, current_min);\n\n        // Reset current_min to 0 if it becomes positive (Kadane's algorithm logic)\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpgt_epi64(current_min, zero);\n        current_min = _mm_andnot_si128(mask, current_min);\n    }\n\n    // Reduce the global_min vector to a single value\n    int64_t result[simd_width];\n    _mm_storeu_si128((__m128i*)result, global_min);\n    int64_t min_sum = std::min(result[0], result[1]);\n\n    // Process remaining elements\n    int64_t current_min_scalar = 0;\n    for (size_t i = simd_end; i < n; ++i) {\n        current_min_scalar += nums[i];\n        min_sum = std::min(min_sum, current_min_scalar);\n        if (current_min_scalar > 0) current_min_scalar = 0;\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Process the row using SIMD\n        __m128i bucket_sum = _mm_setzero_si128();\n        int simd_width = 4; // SSE processes 4 integers at a time\n        int i = 0;\n\n        for (; i + simd_width <= row_size; i += simd_width) {\n            __m128i data = _mm_loadu_si128((__m128i *)&row[i]);\n            bucket_sum = _mm_add_epi32(bucket_sum, data);\n        }\n\n        // Sum the SIMD register values\n        alignas(16) int buffer[simd_width];\n        _mm_store_si128((__m128i *)buffer, bucket_sum);\n        for (int j = 0; j < simd_width; ++j) {\n            sum += buffer[j];\n        }\n\n        // Process the remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of buckets needed for this row\n        total_buckets += (sum + capacity - 1) / capacity; // Ceiling division\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto& row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Process 128-bit chunks (4 integers at a time)\n        __m128i total_sum = _mm_setzero_si128();\n        int i = 0;\n        for (; i + 4 <= row_size; i += 4) {\n            __m128i data = _mm_loadu_si128((__m128i*)&row[i]);\n            total_sum = _mm_add_epi32(total_sum, data);\n        }\n\n        // Horizontal sum of the SIMD register\n        int buffer[4];\n        _mm_storeu_si128((__m128i*)buffer, total_sum);\n        sum += buffer[0] + buffer[1] + buffer[2] + buffer[3];\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of bucket operations for this row\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto& row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Process 128-bit chunks (4 integers at a time)\n        __m128i sum_vec = _mm_setzero_si128();\n        int i = 0;\n        for (; i + 4 <= row_size; i += 4) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[i]));\n            sum_vec = _mm_add_epi32(sum_vec, data);\n        }\n\n        // Sum up the elements in the SIMD register\n        alignas(16) int temp[4];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum_vec);\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of bucket operations for this row\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Process the row using SIMD intrinsics\n        __m128i bucket_sum = _mm_setzero_si128(); // Initialize sum to zero\n        int i = 0;\n\n        // Process 128 bits (16 integers) at a time\n        for (; i + 16 <= row_size; i += 16) {\n            __m128i data1 = _mm_loadu_si128((__m128i *)&row[i]);\n            __m128i data2 = _mm_loadu_si128((__m128i *)&row[i + 4]);\n            __m128i data3 = _mm_loadu_si128((__m128i *)&row[i + 8]);\n            __m128i data4 = _mm_loadu_si128((__m128i *)&row[i + 12]);\n\n            bucket_sum = _mm_add_epi32(bucket_sum, data1);\n            bucket_sum = _mm_add_epi32(bucket_sum, data2);\n            bucket_sum = _mm_add_epi32(bucket_sum, data3);\n            bucket_sum = _mm_add_epi32(bucket_sum, data4);\n        }\n\n        // Extract the sum from the SIMD register\n        int temp[4];\n        _mm_storeu_si128((__m128i *)temp, bucket_sum);\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of buckets needed for this row\n        total_buckets += (sum + capacity - 1) / capacity; // Ceiling division\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_operations = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int sum = 0;\n\n        // Process the row using SIMD\n        __m128i bucket_sum = _mm_setzero_si128();\n        int i = 0;\n\n        for (; i + 4 <= row_size; i += 4) {\n            __m128i data = _mm_loadu_si128((__m128i *)&row[i]);\n            bucket_sum = _mm_add_epi32(bucket_sum, data);\n        }\n\n        // Extract the sum from the SIMD register\n        alignas(16) int temp[4];\n        _mm_store_si128((__m128i *)temp, bucket_sum);\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            sum += row[i];\n        }\n\n        // Calculate the number of operations for this row\n        total_operations += (sum + capacity - 1) / capacity;\n    }\n\n    return total_operations;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <vector>\n#include <cstdint>\n#include <emmintrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    // Ensure the vector size is a multiple of 8 for SIMD processing\n    size_t padded_size = (n + 7) & ~7;\n    std::vector<int16_t> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask_even_positions = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1); // Mask for even positions\n    __m128i mask_odd_values = _mm_set1_epi16(1); // Mask for odd values\n\n    for (size_t i = 0; i < padded_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&padded_lst[i]);\n\n        // Mask for even positions\n        __m128i even_pos = _mm_and_si128(data, mask_even_positions);\n\n        // Mask for odd values\n        __m128i odd_values = _mm_and_si128(data, mask_odd_values);\n\n        // Combine masks to find odd values in even positions\n        __m128i filtered = _mm_and_si128(even_pos, odd_values);\n\n        // Accumulate the filtered values\n        sum_vec = _mm_add_epi16(sum_vec, filtered);\n    }\n\n    // Horizontal sum of the accumulated vector\n    int16_t result[8];\n    _mm_storeu_si128((__m128i*)result, sum_vec);\n\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <vector>\n#include <cstdint>\n#include <emmintrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    // Ensure the vector size is a multiple of 8 for SIMD processing\n    size_t padded_size = (n + 7) & ~7;\n    std::vector<int16_t> padded_lst = lst;\n    padded_lst.resize(padded_size, 0);\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_even_positions = _mm_set_epi16(0, 0, 0, 0, 0xFFFF, 0, 0xFFFF, 0); // Mask for even positions\n    __m128i mask_odd_values = _mm_set1_epi16(1); // Mask for odd values\n\n    for (size_t i = 0; i < padded_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&padded_lst[i]);\n\n        // Mask for even positions\n        __m128i even_positions = _mm_and_si128(data, mask_even_positions);\n\n        // Check for odd values\n        __m128i odd_values = _mm_and_si128(data, mask_odd_values);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_values, mask_odd_values);\n\n        // Combine masks for odd values in even positions\n        __m128i valid_values = _mm_and_si128(even_positions, is_odd);\n\n        // Accumulate the sum\n        sum = _mm_add_epi16(sum, valid_values);\n    }\n\n    // Horizontal sum of the accumulated values\n    int16_t result[8];\n    _mm_storeu_si128((__m128i*)result, sum);\n\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <vector>\n#include <cstdint>\n#include <emmintrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_even_positions = _mm_set_epi16(0, 0, 0, 0, -1, 0, -1, 0); // Mask for even positions\n    __m128i mask_odd_values = _mm_set1_epi16(1); // Mask for odd values\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&lst[i]); // Load 8 elements\n        __m128i even_positions = _mm_and_si128(data, mask_even_positions); // Mask even positions\n        __m128i odd_values = _mm_and_si128(data, mask_odd_values); // Mask odd values\n        __m128i odd_in_even_positions = _mm_and_si128(even_positions, odd_values); // Odd values in even positions\n        sum = _mm_add_epi16(sum, odd_in_even_positions); // Accumulate\n    }\n\n    // Handle remaining elements\n    int16_t temp[8];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    int64_t result = 0;\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n    for (; i < n; i += 2) {\n        if (lst[i] & 1) { // Check if odd\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n#include <stdint.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    // Ensure the vector size is a multiple of 8 for SIMD processing\n    size_t padded_size = (n + 7) & ~7;\n    std::vector<int16_t> padded_lst = lst;\n    padded_lst.resize(padded_size, 0);\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_even_positions = _mm_set_epi16(0, 0, 0, 0, 0xFFFF, 0, 0xFFFF, 0); // Mask for even positions\n    __m128i mask_odd_values = _mm_set1_epi16(1); // Mask for odd values\n\n    for (size_t i = 0; i < padded_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&padded_lst[i]);\n\n        // Mask for even positions\n        __m128i even_positions = _mm_and_si128(data, mask_even_positions);\n\n        // Check for odd values\n        __m128i odd_values = _mm_and_si128(data, mask_odd_values);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_values, mask_odd_values);\n\n        // Combine masks for even positions and odd values\n        __m128i valid_values = _mm_and_si128(even_positions, is_odd);\n\n        // Accumulate the valid values\n        sum = _mm_add_epi16(sum, valid_values);\n    }\n\n    // Horizontal sum of the 128-bit register\n    int16_t result[8];\n    _mm_storeu_si128((__m128i*)result, sum);\n\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n#include <stdint.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    // Ensure the vector size is a multiple of 8 for SIMD processing\n    size_t padded_size = (n + 7) & ~7;\n    std::vector<int16_t> padded_lst = lst;\n    padded_lst.resize(padded_size, 0);\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask_even_positions = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1); // Mask for even positions\n    __m128i mask_odd_values = _mm_set1_epi16(1); // Mask for odd values\n\n    for (size_t i = 0; i < padded_size; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&padded_lst[i]);\n\n        // Mask for even positions\n        __m128i even_positions = _mm_and_si128(data, mask_even_positions);\n\n        // Mask for odd values\n        __m128i odd_values = _mm_and_si128(data, mask_odd_values);\n\n        // Combine masks: odd values in even positions\n        __m128i filtered = _mm_and_si128(even_positions, odd_values);\n\n        // Accumulate the sum\n        sum = _mm_add_epi16(sum, filtered);\n    }\n\n    // Horizontal sum of the 128-bit register\n    int16_t result[8];\n    _mm_storeu_si128((__m128i*)result, sum);\n\n    int64_t total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += result[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int n = arr.size();\n    if (k > n) k = n;\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to 0\n    __m128i mask_two_digits = _mm_set1_epi32(99); // Mask for two-digit numbers\n    __m128i zero = _mm_setzero_si128(); // Zero vector\n\n    int i = 0;\n    for (; i + 4 <= k; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]); // Load 4 integers\n        __m128i abs_data = _mm_abs_epi32(data); // Take absolute value\n        __m128i cmp = _mm_cmpgt_epi32(abs_data, mask_two_digits); // Compare with 99\n        __m128i filtered = _mm_andnot_si128(cmp, data); // Zero out elements > 99\n        sum = _mm_add_epi32(sum, filtered); // Add filtered elements to sum\n    }\n\n    // Horizontal sum of the SIMD register\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (; i < k; i++) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            total_sum += arr[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int result = 0;\n    int n = std::min((int)arr.size(), k);\n    int i = 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(100); // Mask for two-digit check\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec); // Get absolute values\n        __m128i cmp = _mm_cmpgt_epi32(mask, abs_vec); // Compare with 100\n        __m128i filtered = _mm_and_si128(vec, cmp); // Mask out values >= 100\n        sum = _mm_add_epi32(sum, filtered); // Accumulate the sum\n    }\n\n    // Horizontal sum of the SIMD register\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (std::abs(arr[i]) < 100) {\n            result += arr[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int n = std::min((int)arr.size(), k);\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = _mm_setzero_si128();\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_values = _mm_abs_epi32(values);\n\n        // Mask for numbers with at most two digits\n        __m128i mask = _mm_and_si128(\n            _mm_cmpgt_epi32(abs_values, _mm_set1_epi32(9)),\n            _mm_cmplt_epi32(abs_values, _mm_set1_epi32(100))\n        );\n\n        // Apply mask\n        __m128i filtered_values = _mm_and_si128(values, mask);\n\n        // Accumulate the filtered values\n        result = _mm_add_epi32(result, filtered_values);\n    }\n\n    // Horizontal sum of the result vector\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, result);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int value = arr[i];\n        if (value >= -99 && value <= 99 && (value >= 10 || value <= -10)) {\n            sum += value;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int n = std::min((int)arr.size(), k);\n    int sum = 0;\n\n    // Process elements in chunks of 4 using SSE intrinsics\n    __m128i vec_sum = _mm_setzero_si128(); // Initialize sum vector to zero\n    for (int i = 0; i < n; i += 4) {\n        // Load up to 4 integers into an SSE register\n        __m128i vec = _mm_set_epi32(\n            (i + 3 < n) ? arr[i + 3] : 0,\n            (i + 2 < n) ? arr[i + 2] : 0,\n            (i + 1 < n) ? arr[i + 1] : 0,\n            arr[i]\n        );\n\n        // Mask to check if elements have at most two digits\n        __m128i mask = _mm_and_si128(\n            _mm_cmpgt_epi32(_mm_set1_epi32(100), vec), // vec < 100\n            _mm_cmpgt_epi32(vec, _mm_set1_epi32(9))   // vec >= 10\n        );\n\n        // Zero out elements that don't satisfy the condition\n        __m128i filtered_vec = _mm_and_si128(vec, mask);\n\n        // Accumulate the filtered elements into the sum vector\n        vec_sum = _mm_add_epi32(vec_sum, filtered_vec);\n    }\n\n    // Extract the sum from the SSE register\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vec_sum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int n = std::min(k, (int)arr.size());\n    int sum = 0;\n\n    // Prepare an array for SIMD processing\n    int* data = arr.data();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(100); // Mask for checking two-digit numbers\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        // Load 4 integers into SIMD register\n        __m128i vec = _mm_loadu_si128((__m128i*)&data[i]);\n\n        // Absolute value (to handle negative numbers)\n        __m128i abs_vec = _mm_abs_epi32(vec);\n\n        // Compare values to mask (less than 100)\n        __m128i cmp = _mm_cmplt_epi32(abs_vec, mask);\n\n        // Mask the values that satisfy the condition\n        __m128i filtered = _mm_and_si128(vec, cmp);\n\n        // Horizontal sum of filtered values\n        __m128i sum_vec = _mm_hadd_epi32(filtered, zero);\n        sum_vec = _mm_hadd_epi32(sum_vec, zero);\n\n        // Extract the sum from the SIMD register\n        sum += _mm_cvtsi128_si32(sum_vec);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (abs(data[i]) < 100) {\n            sum += data[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Process in chunks of 4 integers using SSE\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&lst[i + 1]);\n\n        // Check if current <= next\n        __m128i cmp = _mm_cmpgt_epi32(current, next);\n        if (_mm_movemask_epi8(cmp)) return false;\n\n        // Check for duplicates (more than 1 duplicate)\n        __m128i dup_check = _mm_cmpeq_epi32(current, next);\n        if (_mm_movemask_epi8(dup_check) > 0xFFFF) return false;\n    }\n\n    // Handle remaining elements\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) return false;\n        if (i + 2 < n && lst[i] == lst[i + 1] && lst[i] == lst[i + 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    for (size_t i = 0; i + 4 <= n; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&lst[i + 1]);\n\n        __m128i cmp = _mm_cmpgt_epi32(current, next);\n        if (_mm_movemask_epi8(cmp)) return false;\n\n        __m128i duplicates = _mm_cmpeq_epi32(current, next);\n        if (_mm_movemask_epi8(duplicates)) return false;\n    }\n\n    for (size_t i = n - (n % 4); i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1] || (lst[i] == lst[i + 1] && i + 2 < n && lst[i] == lst[i + 2])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    size_t simd_size = n / 4 * 4; // Round down to nearest multiple of 4\n    __m128i prev = _mm_set1_epi32(lst[0]);\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i cmp_gt = _mm_cmpgt_epi32(prev, current);\n        __m128i cmp_eq = _mm_cmpeq_epi32(prev, current);\n\n        if (_mm_movemask_epi8(cmp_gt)) return false;\n\n        __m128i duplicates = _mm_and_si128(cmp_eq, _mm_srli_si128(cmp_eq, 4));\n        if (_mm_movemask_epi8(duplicates)) return false;\n\n        prev = current;\n    }\n\n    for (size_t i = simd_size; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) return false;\n        if (lst[i] == lst[i + 1] && (i + 2 < n && lst[i] == lst[i + 2])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Process in chunks of 4 integers using SSE\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&lst[i + 1]);\n\n        // Compare adjacent elements\n        __m128i cmp = _mm_cmpgt_epi32(current, next);\n        if (_mm_movemask_epi8(cmp)) return false;\n\n        // Check for duplicates\n        __m128i eq = _mm_cmpeq_epi32(current, next);\n        __m128i dup_check = _mm_and_si128(eq, _mm_cmpeq_epi32(next, _mm_loadu_si128((__m128i*)&lst[i + 2])));\n        if (_mm_movemask_epi8(dup_check)) return false;\n    }\n\n    // Handle remaining elements\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) return false;\n        if (i < n - 2 && lst[i] == lst[i + 1] && lst[i + 1] == lst[i + 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Process in chunks of 4 integers using SIMD\n    size_t simd_size = n / 4 * 4; // Round down to nearest multiple of 4\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i next = _mm_loadu_si128((__m128i*)&lst[i + 1]);\n\n        // Check if current <= next\n        __m128i cmp = _mm_cmpgt_epi32(current, next);\n        if (_mm_movemask_epi8(cmp)) return false;\n\n        // Check for duplicates (more than 1 duplicate)\n        __m128i duplicates = _mm_cmpeq_epi32(current, next);\n        if (_mm_movemask_epi8(duplicates) > 0) {\n            // Count duplicates\n            int duplicate_count = _mm_movemask_epi8(duplicates);\n            if (duplicate_count > 1) return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = simd_size; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) return false;\n        if (lst[i] == lst[i + 1] && (i + 2 < n && lst[i] == lst[i + 2])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_magnitudes = _mm_setzero_si128();\n    int product_of_signs = 1;\n\n    size_t size = arr.size();\n    size_t i = 0;\n\n    // Process 4 integers at a time using SIMD\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Compute absolute values\n        __m128i abs_vec = _mm_abs_epi32(vec);\n\n        // Accumulate magnitudes\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, abs_vec);\n\n        // Compute signs (-1, 0, 1)\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero_vec);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero_vec);\n\n        __m128i signs = _mm_sub_epi32(pos_mask, neg_mask);\n\n        // Extract signs and compute product\n        int signs_array[4];\n        _mm_storeu_si128((__m128i*)signs_array, signs);\n        for (int j = 0; j < 4; ++j) {\n            product_of_signs *= signs_array[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int value = arr[i];\n        int abs_value = std::abs(value);\n        int sign = (value > 0) - (value < 0);\n\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, _mm_set1_epi32(abs_value));\n        product_of_signs *= sign;\n    }\n\n    // Sum up magnitudes from SIMD register\n    int magnitudes_array[4];\n    _mm_storeu_si128((__m128i*)magnitudes_array, sum_magnitudes);\n    int total_magnitude = magnitudes_array[0] + magnitudes_array[1] + magnitudes_array[2] + magnitudes_array[3];\n\n    return total_magnitude * product_of_signs;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> &arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_magnitudes = _mm_setzero_si128();\n    int product_of_signs = 1;\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, abs_vec);\n\n        __m128i signs = _mm_srai_epi32(vec, 31);\n        __m128i neg_signs = _mm_andnot_si128(signs, _mm_set1_epi32(-1));\n        __m128i combined_signs = _mm_or_si128(signs, neg_signs);\n\n        int signs_array[4];\n        _mm_storeu_si128((__m128i*)signs_array, combined_signs);\n\n        for (int j = 0; j < 4; ++j) {\n            product_of_signs *= signs_array[j];\n        }\n    }\n\n    int remaining_sum = 0;\n    for (; i < size; ++i) {\n        int val = arr[i];\n        remaining_sum += std::abs(val);\n        product_of_signs *= (val > 0) - (val < 0);\n    }\n\n    int magnitudes_array[4];\n    _mm_storeu_si128((__m128i*)magnitudes_array, sum_magnitudes);\n\n    int total_sum = magnitudes_array[0] + magnitudes_array[1] + magnitudes_array[2] + magnitudes_array[3] + remaining_sum;\n\n    return total_sum * product_of_signs;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_magnitudes = _mm_setzero_si128();\n    __m128i product_signs = _mm_set1_epi32(1);\n\n    size_t size = arr.size();\n    size_t i = 0;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        // Compute magnitudes\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, abs_vec);\n\n        // Compute signs\n        __m128i signs = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        product_signs = _mm_mullo_epi32(product_signs, signs);\n    }\n\n    // Horizontal sum for magnitudes\n    int magnitudes[4];\n    _mm_storeu_si128((__m128i*)magnitudes, sum_magnitudes);\n    int total_magnitude = magnitudes[0] + magnitudes[1] + magnitudes[2] + magnitudes[3];\n\n    // Horizontal product for signs\n    int signs[4];\n    _mm_storeu_si128((__m128i*)signs, product_signs);\n    int total_sign = signs[0] * signs[1] * signs[2] * signs[3];\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        total_magnitude += std::abs(arr[i]);\n        total_sign *= (arr[i] > 0) ? 1 : (arr[i] < 0) ? -1 : 0;\n    }\n\n    return total_sign * total_magnitude;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum_magnitudes = _mm_setzero_si128();\n    __m128i product_signs = _mm_set1_epi32(1);\n\n    size_t size = arr.size();\n    size_t i = 0;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, abs_vec);\n\n        __m128i signs = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n        signs = _mm_or_si128(signs, _mm_cmpeq_epi32(vec, _mm_setzero_si128()));\n\n        __m128i sign_values = _mm_sub_epi32(_mm_set1_epi32(1), signs);\n        product_signs = _mm_mullo_epi32(product_signs, sign_values);\n    }\n\n    alignas(16) int sum_array[4];\n    alignas(16) int product_array[4];\n\n    _mm_store_si128((__m128i*)sum_array, sum_magnitudes);\n    _mm_store_si128((__m128i*)product_array, product_signs);\n\n    int sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    int product = product_array[0] * product_array[1] * product_array[2] * product_array[3];\n\n    for (; i < size; ++i) {\n        sum += std::abs(arr[i]);\n        int sign = (arr[i] > 0) ? 1 : (arr[i] < 0) ? -1 : 0;\n        product *= sign;\n    }\n\n    return sum * product;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    const int n = arr.size();\n    __m128i sum_magnitudes = _mm_setzero_si128();\n    __m128i product_signs = _mm_set1_epi32(1);\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        sum_magnitudes = _mm_add_epi32(sum_magnitudes, abs_vec);\n\n        __m128i signs = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n        signs = _mm_or_si128(signs, _mm_cmpeq_epi32(vec, _mm_setzero_si128()));\n        signs = _mm_sub_epi32(_mm_set1_epi32(1), signs);\n        product_signs = _mm_mullo_epi32(product_signs, signs);\n    }\n\n    int sum = 0;\n    int prod = 1;\n    alignas(16) int sum_buffer[4];\n    alignas(16) int prod_buffer[4];\n    _mm_store_si128((__m128i*)sum_buffer, sum_magnitudes);\n    _mm_store_si128((__m128i*)prod_buffer, product_signs);\n\n    for (int j = 0; j < 4; ++j) {\n        sum += sum_buffer[j];\n        prod *= prod_buffer[j];\n    }\n\n    for (; i < n; ++i) {\n        sum += std::abs(arr[i]);\n        prod *= (arr[i] > 0 ? 1 : (arr[i] < 0 ? -1 : 0));\n    }\n\n    return sum * prod;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, std::numeric_limits<int>::max())));\n    std::vector<std::vector<std::vector<std::vector<int>>>> path(N, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(k + 1)));\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n            path[i][j][1] = {grid[i][j]};\n        }\n    }\n\n    for (int step = 2; step <= k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n                std::vector<int> best_path;\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) + abs(dj) != 1) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m128i current_val = _mm_set1_epi32(dp[ni][nj][step - 1]);\n                            __m128i grid_val = _mm_set1_epi32(grid[i][j]);\n                            __m128i sum_val = _mm_add_epi32(current_val, grid_val);\n\n                            int sum = _mm_cvtsi128_si32(sum_val);\n                            if (sum < _mm_cvtsi128_si32(min_val)) {\n                                min_val = sum_val;\n                                best_path = path[ni][nj][step - 1];\n                                best_path.push_back(grid[i][j]);\n                            }\n                        }\n                    }\n                }\n\n                dp[i][j][step] = _mm_cvtsi128_si32(min_val);\n                path[i][j][step] = best_path;\n            }\n        }\n    }\n\n    std::vector<int> result;\n    __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m128i current_val = _mm_set1_epi32(dp[i][j][k]);\n            if (_mm_cvtsi128_si32(current_val) < _mm_cvtsi128_si32(min_val)) {\n                min_val = current_val;\n                result = path[i][j][k];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k + 1, std::numeric_limits<int>::max())));\n    std::vector<std::vector<std::vector<std::vector<int>>>> path(N, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(k + 1)));\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][1] = grid[i][j];\n            path[i][j][1] = {grid[i][j]};\n        }\n    }\n\n    for (int step = 2; step <= k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n                std::vector<int> best_path;\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (di * dj != 0 || (di == 0 && dj == 0)) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m128i current_val = _mm_set1_epi32(dp[ni][nj][step - 1]);\n                            __m128i grid_val = _mm_set1_epi32(grid[i][j]);\n                            __m128i sum_val = _mm_add_epi32(current_val, grid_val);\n\n                            if (_mm_cvtsi128_si32(sum_val) < _mm_cvtsi128_si32(min_val)) {\n                                min_val = sum_val;\n                                best_path = path[ni][nj][step - 1];\n                                best_path.push_back(grid[i][j]);\n                            }\n                        }\n                    }\n                }\n\n                dp[i][j][step] = _mm_cvtsi128_si32(min_val);\n                path[i][j][step] = best_path;\n            }\n        }\n    }\n\n    std::vector<int> result;\n    __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m128i current_val = _mm_set1_epi32(dp[i][j][k]);\n            if (_mm_cvtsi128_si32(current_val) < _mm_cvtsi128_si32(min_val)) {\n                min_val = current_val;\n                result = path[i][j][k];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    const int INF = std::numeric_limits<int>::max();\n    std::vector<std::vector<std::vector<int>>> dp(k, std::vector<std::vector<int>>(N, std::vector<int>(N, INF)));\n    std::vector<std::vector<std::vector<std::vector<int>>>> path(k, std::vector<std::vector<std::vector<int>>>(N, std::vector<std::vector<int>>(N)));\n\n    // Initialize the first step\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[0][i][j] = grid[i][j];\n            path[0][i][j] = {grid[i][j]};\n        }\n    }\n\n    // Directions for neighbors\n    int dx[4] = {0, 0, 1, -1};\n    int dy[4] = {1, -1, 0, 0};\n\n    // DP computation\n    for (int step = 1; step < k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m128i min_val = _mm_set1_epi32(INF);\n                std::vector<int> best_path;\n\n                for (int d = 0; d < 4; ++d) {\n                    int ni = i + dx[d];\n                    int nj = j + dy[d];\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        __m128i current_val = _mm_set1_epi32(dp[step - 1][ni][nj]);\n                        min_val = _mm_min_epi32(min_val, current_val);\n\n                        if (dp[step - 1][ni][nj] < _mm_extract_epi32(min_val, 0)) {\n                            best_path = path[step - 1][ni][nj];\n                        }\n                    }\n                }\n\n                dp[step][i][j] = _mm_extract_epi32(min_val, 0);\n                best_path.push_back(grid[i][j]);\n                path[step][i][j] = best_path;\n            }\n        }\n    }\n\n    // Find the minimum path of length k\n    __m128i global_min = _mm_set1_epi32(INF);\n    std::vector<int> result;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m128i current_val = _mm_set1_epi32(dp[k - 1][i][j]);\n            global_min = _mm_min_epi32(global_min, current_val);\n\n            if (dp[k - 1][i][j] < _mm_extract_epi32(global_min, 0)) {\n                result = path[k - 1][i][j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, INT_MAX);\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k, INT_MAX)));\n\n    // Initialize dp for paths of length 1\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][0] = grid[i][j];\n        }\n    }\n\n    // Compute dp for paths of length 2 to k using SIMD\n    for (int step = 1; step < k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m128i minValues = _mm_set1_epi32(INT_MAX);\n\n                // Check neighbors (up, down, left, right)\n                if (i > 0) {\n                    __m128i neighbor = _mm_set1_epi32(dp[i - 1][j][step - 1]);\n                    __m128i current = _mm_set1_epi32(grid[i][j]);\n                    __m128i combined = _mm_add_epi32(neighbor, current);\n                    minValues = _mm_min_epi32(minValues, combined);\n                }\n                if (i < N - 1) {\n                    __m128i neighbor = _mm_set1_epi32(dp[i + 1][j][step - 1]);\n                    __m128i current = _mm_set1_epi32(grid[i][j]);\n                    __m128i combined = _mm_add_epi32(neighbor, current);\n                    minValues = _mm_min_epi32(minValues, combined);\n                }\n                if (j > 0) {\n                    __m128i neighbor = _mm_set1_epi32(dp[i][j - 1][step - 1]);\n                    __m128i current = _mm_set1_epi32(grid[i][j]);\n                    __m128i combined = _mm_add_epi32(neighbor, current);\n                    minValues = _mm_min_epi32(minValues, combined);\n                }\n                if (j < N - 1) {\n                    __m128i neighbor = _mm_set1_epi32(dp[i][j + 1][step - 1]);\n                    __m128i current = _mm_set1_epi32(grid[i][j]);\n                    __m128i combined = _mm_add_epi32(neighbor, current);\n                    minValues = _mm_min_epi32(minValues, combined);\n                }\n\n                // Store the minimum value for the current cell and step\n                dp[i][j][step] = _mm_extract_epi32(minValues, 0);\n            }\n        }\n    }\n\n    // Find the lexicographically smallest path of length k\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> path(k);\n            path[0] = grid[i][j];\n            for (int step = 1; step < k; ++step) {\n                path[step] = dp[i][j][step];\n            }\n            if (path < result) {\n                result = path;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, std::numeric_limits<int>::max());\n    std::vector<std::vector<std::vector<int>>> dp(N, std::vector<std::vector<int>>(N, std::vector<int>(k, std::numeric_limits<int>::max())));\n\n    __m128i minPathVec = _mm_set1_epi32(std::numeric_limits<int>::max());\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j][0] = grid[i][j];\n        }\n    }\n\n    for (int step = 1; step < k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m128i currentCellVec = _mm_set1_epi32(dp[i][j][step - 1]);\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (di == 0 && dj == 0) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m128i neighborCellVec = _mm_set1_epi32(dp[ni][nj][step - 1]);\n                            __m128i gridValueVec = _mm_set1_epi32(grid[ni][nj]);\n                            __m128i sumVec = _mm_add_epi32(neighborCellVec, gridValueVec);\n                            currentCellVec = _mm_min_epi32(currentCellVec, sumVec);\n                        }\n                    }\n                }\n                dp[i][j][step] = _mm_extract_epi32(currentCellVec, 0);\n            }\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m128i pathVec = _mm_set1_epi32(dp[i][j][k - 1]);\n            minPathVec = _mm_min_epi32(minPathVec, pathVec);\n        }\n    }\n\n    int minPathValue = _mm_extract_epi32(minPathVec, 0);\n    for (int step = 0; step < k; ++step) {\n        result[step] = minPathValue;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    __m128i product = _mm_set1_epi64x(1); // Initialize product to 1\n    __m128i zero = _mm_setzero_si128();   // Zero vector\n    int has_odd = 0;                      // Flag to check if there are odd digits\n\n    while (n > 0) {\n        // Extract the last 4 digits of n\n        uint64_t digits = n % 10000;\n        n /= 10000;\n\n        // Split digits into an array\n        uint16_t digit_array[4] = {\n            (uint16_t)(digits % 10),\n            (uint16_t)((digits / 10) % 10),\n            (uint16_t)((digits / 100) % 10),\n            (uint16_t)((digits / 1000) % 10)\n        };\n\n        // Load the digits into an SSE register\n        __m128i digit_vec = _mm_set_epi16(0, 0, 0, 0, digit_array[3], digit_array[2], digit_array[1], digit_array[0]);\n\n        // Check if the digits are odd\n        __m128i odd_mask = _mm_and_si128(digit_vec, _mm_set1_epi16(1)); // Mask for odd digits\n        __m128i is_odd = _mm_cmpeq_epi16(odd_mask, _mm_set1_epi16(1)); // Compare with 1\n\n        // Multiply only the odd digits\n        __m128i odd_digits = _mm_and_si128(digit_vec, is_odd);\n        __m128i non_zero_mask = _mm_cmpgt_epi16(odd_digits, zero); // Check for non-zero digits\n        has_odd |= _mm_movemask_epi8(non_zero_mask);               // Update has_odd flag\n\n        // Convert to 32-bit integers for multiplication\n        __m128i odd_digits_lo = _mm_unpacklo_epi16(odd_digits, zero);\n        __m128i odd_digits_hi = _mm_unpackhi_epi16(odd_digits, zero);\n\n        // Multiply the odd digits\n        product = _mm_mullo_epi32(product, odd_digits_lo);\n        product = _mm_mullo_epi32(product, odd_digits_hi);\n    }\n\n    // If no odd digits were found, return 0\n    if (!has_odd) return 0;\n\n    // Extract the final product from the SSE register\n    uint64_t result[2];\n    _mm_storeu_si128((__m128i*)result, product);\n\n    return result[0] * result[1];\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint8_t digits[16] = {0};\n    int count = 0;\n\n    // Extract digits from the number\n    while (n > 0) {\n        digits[count++] = n % 10;\n        n /= 10;\n    }\n\n    __m128i result = _mm_set1_epi16(1); // Initialize result to 1\n    __m128i zero = _mm_setzero_si128(); // Zero vector\n    __m128i even_mask = _mm_set1_epi16(1); // Mask for odd/even check\n\n    // Load digits into SIMD registers\n    __m128i digit_vec = _mm_loadu_si128((__m128i *)digits);\n\n    // Check if digits are odd\n    __m128i odd_mask = _mm_and_si128(digit_vec, even_mask);\n\n    // Compare odd_mask with zero to find odd digits\n    __m128i is_odd = _mm_cmpgt_epi16(odd_mask, zero);\n\n    // Mask out even digits by setting them to 1\n    __m128i odd_digits = _mm_or_si128(_mm_and_si128(is_odd, digit_vec), _mm_andnot_si128(is_odd, even_mask));\n\n    // Multiply all the odd digits\n    for (int i = 0; i < count; i += 8) {\n        __m128i partial = _mm_loadu_si128((__m128i *)(digits + i));\n        result = _mm_mullo_epi16(result, partial);\n    }\n\n    // Extract the final result\n    uint16_t final_result[8];\n    _mm_storeu_si128((__m128i *)final_result, result);\n\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    for (int i = 0; i < count; i++) {\n        if (digits[i] % 2 != 0) {\n            product *= digits[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    int count = 0;\n    while (n > 0) {\n        digits[count++] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an SSE register\n    __m128i vec_digits = _mm_set_epi64x(*(int64_t*)&digits[4], *(int64_t*)&digits[0]);\n\n    // Mask for odd digits\n    __m128i vec_mask = _mm_set1_epi16(1);\n\n    // Check oddness\n    __m128i vec_mod2 = _mm_and_si128(vec_digits, vec_mask);\n\n    // Compare with 1 (odd check)\n    __m128i vec_is_odd = _mm_cmpeq_epi16(vec_mod2, vec_mask);\n\n    // Multiply odd digits\n    for (int i = 0; i < count; i++) {\n        if (_mm_extract_epi16(vec_is_odd, i) != 0) {\n            product *= digits[i];\n            has_odd = 1;\n        }\n    }\n\n    return has_odd ? product : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    for (int i = 0; n > 0 && i < 8; ++i) {\n        digits[i] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an SSE register\n    __m128i vec = _mm_set_epi64x(digits[4] | (digits[5] << 8) | (digits[6] << 16) | (digits[7] << 24) );\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    uint64_t digits[8] = {0};\n    uint64_t product = 1;\n    int has_odd = 0;\n\n    // Extract digits into an array\n    for (int i = 0; n > 0 && i < 8; ++i) {\n        digits[i] = n % 10;\n        n /= 10;\n    }\n\n    // Load digits into an SSE register\n    __m128i digit_vec = _mm_set_epi64x(digits[4] | (digits[5] << 8) | digits[6<<8) digits[7<<8) digits[8<<8) digits[9<<8) digits[10<<8) digits[11<<8) digits[12<<8) digits[13<<8) digits[14<<8) digits[15<<8) digits[16<<8) digits[17<<8) digits[18<<8) digits[19<<8) digits[20<<8) digits[21<<8) digits[22<<8) digits[23<<8) digits[24<<8) digits[25<<8) digits[26<<8) digits[27<<8) digits[28<<8) digits[29<<8) digits[30<<8) digits[31<<8) digits[32<<8) digits[33<<8) digits[34<<8) digits[35<<8) digits[36<<8) digits[37<<8) digits[38<<8) digits[39<<8) digits[40<<8) digits[41<<8) digits[42<<8) digits[43<<8) digits[44<<8) digits[45<<8) digits[46<<8) digits[47<<8) digits[48<<8) digits[49<<8) digits[50<<8) digits[51<<8) digits[52<<8) digits[53<<8) digits[54<<8) digits[55<<8) digits[56<<8) digits[57<<8) digits[58<<8) digits[59<<8) digits[60<<8) digits[61<<8) digits[62<<8) digits[63<<8) digits[64<<8) digits[65<<8) digits[66<<8) digits[67<<8) digits[68<<8) digits[69<<8) digits[70<<8) digits[71<<8) digits[72<<8) digits[73<<8) digits[74<<8) digits[75<<8) digits[76<<8) digits[77<<8) digits[78<<8) digits[79<<8) digits[80<<8) digits[81<<8) digits[82<<8) digits[83<<8) digits[84<<8) digits[85<<8) digits[86<<8) digits[87<<8) digits[88<<8) digits[89<<8) digits[90<<8) digits[91<<8) digits[92<<8) digits[93<<8) digits[94<<8) digits[95<<8) digits[96<<8) digits[97<<8) digits[98<<8) digits[99<<8) digits[100<<8) digits[101<<8) digits[102<<8) digits[103<<8) digits[104<<8) digits[105<<8) digits[106<<8) digits[107<<8) digits[108<<8) digits[109<<8) digits[110<<8) digits[111<<8) digits[112<<8) digits[113<<8) digits[114<<8) digits[115<<8) digits[116<<8) digits[117<<8) digits[118<<8) digits[119<<8) digits[120<<8) digits[121<<8) digits[122<<8) digits[123<<8) digits[124<<8) digits[125<<8) digits[126<<8) digits[127<<8) digits[128<<8) digits[129<<8) digits[130<<8) digits[131<<8) digits[132<<8) digits[133<<8) digits[134<<8) digits[135<<8) digits[136<<8) digits[137<<8) digits[138<<8) digits[139<<8) digits[140<<8) digits[141<<8) digits[142<<8) digits[143<<8) digits[144<<8) digits[145<<8) digits[146<<8) digits[147<<8) digits[148<<8) digits[149<<8) digits[150<<8) digits[151<<8) digits[152<<8) digits[153<<8) digits[154<<8) digits[155<<8) digits[156<<8) digits[157<<8) digits[158<<8) digits[159<<8) digits[160<<8) digits[161<<8) digits[162<<8) digits[163<<8) digits[164<<8) digits[165<<8) digits[166<<8) digits[167<<8) digits[168<<8) digits[169<<8) digits[170<<8) digits[171<<8) digits[172<<8) digits[173<<8) digits[174<<8) digits[175<<8) digits[176<<8) digits[177<<8) digits[178<<8) digits[179<<8) digits[180<<8) digits[181<<8) digits[182<<8) digits[183<<8) digits[184<<8) digits[185<<8) digits[186<<8) digits[187<<8) digits[188<<8) digits[189<<8) digits[190<<8) digits[191<<8) digits[192<<8) digits[193<<8) digits[194<<8) digits[195<<8) digits[196<<8) digits[197<<8) digits[198<<8) digits[199<<8) digits[200<<8) digits[201<<8) digits[202<<8) digits[203<<8) digits[204<<8) digits[205<<8) digits[206<<8) digits[207<<8) digits[208<<8) digits[209<<8) digits[210<<8) digits[211<<8) digits[212<<8) digits[213<<8) digits[214<<8) digits[215<<8) digits[216<<8) digits[217<<8) digits[218<<8) digits[219<<8) digits[220<<8) digits[221<<8) digits[222<<8) digits[223<<8) digits[224<<8) digits[225<<8) digits[226<<8) digits[227<<8) digits[228<<8) digits[229<<8) digits[230<<8) digits[231<<8) digits[232<<8) digits[233<<8) digits[234<<8) digits[235<<8) digits[236<<8) digits[237<<8) digits[238<<8) digits[239<<8) digits[240<<8) digits[241<<8) digits[242<<8) digits[243<<8) digits[244<<8) digits[245<<8) digits[246<<8) digits[247<<8) digits[248<<8) digits[249<<8) digits[250<<8) digits[251<<8) digits[252<<8) digits[253<<8) digits[254<<8) digits[255<<8) digits[256<<8) digits[257<<8) digits[258<<8) digits[259<<8) digits[260<<8) digits[261<<8) digits[262<<8) digits[263<<8) digits[264<<8) digits[265<<8) digits[266<<8) digits[267<<8) digits[268<<8) digits[269<<8) digits[270<<8) digits[271<<8) digits[272<<8) digits[273<<8) digits[274<<8) digits[275<<8) digits[276<<8) digits[277<<8) digits[278<<8) digits[279<<8) digits[280<<8) digits[281<<8) digits[282<<8) digits[283<<8) digits[284<<8) digits[285<<8) digits[286<<8) digits[287<<8) digits[288<<8) digits[289<<8) digits[290<<8) digits[291<<8) digits[292<<8) digits[293<<8) digits[294<<8) digits[295<<8) digits[296<<8) digits[297<<8) digits[298<<8) digits[299<<8) digits[300<<8) digits[301<<8) digits[302<<8) digits[303<<8) digits[304<<8) digits[305<<8) digits[306<<8) digits[307<<8) digits[308<<8) digits[309<<8) digits[310<<8) digits[311<<8) digits[312<<8) digits[313<<8) digits[314<<8) digits[315<<8) digits[316<<8) digits[317<<8) digits[318<<8) digits[319<<8) digits[320<<8) digits[321<<8) digits[322<<8) digits[323<<8) digits[324<<8) digits[325<<8) digits[326<<8) digits[327<<8) digits[328<<8) digits[329<<8) digits[330<<8) digits[331<<8) digits[332<<8) digits[333<<8) digits[334<<8) digits[335<<8) digits[336<<8) digits[337<<8) digits[338<<8) digits[339<<8) digits[340<<8) digits[341<<8) digits[342<<8) digits[343<<8) digits[344<<8) digits[345<<8) digits[346<<8) digits[347<<8) digits[348<<8) digits[349<<8) digits[350<<8) digits[351<<8) digits[352<<8) digits[353<<8) digits[354<<8) digits[355<<8) digits[356<<8) digits[357<<8) digits[358<<8) digits[359<<8) digits[360<<8) digits[361<<8) digits[362<<8) digits[363<<8) digits[364<<8) digits[365<<8) digits[366<<8) digits[367<<8) digits[368<<8) digits[369<<8) digits[370<<8) digits[371<<8) digits[372<<8) digits[373<<8) digits[374<<8) digits[375<<8) digits[376<<8) digits[377<<8) digits[378<<8) digits[379<<8) digits[380<<8) digits[381<<8) digits[382<<8) digits[383<<8) digits[384<<8) digits[385<<8) digits[386<<8) digits[387<<8) digits[388<<8) digits[389<<8) digits[390<<8) digits[391<<8) digits[392<<8) digits[393<<8) digits[394<<8) digits[395<<8) digits[396<<8) digits[397<<8) digits[398<<8) digits[399<<8) digits[400<<8) digits[401<<8) digits[402<<8) digits[403<<8) digits[404<<8) digits[405<<8) digits[406<<8) digits[407<<8) digits[408<<8) digits[409<<8) digits[410<<8) digits[411<<8) digits[412<<8) digits[413<<8) digits[414<<8) digits[415<<8) digits[416<<8) digits[417<<8) digits[418<<8) digits[419<<8) digits[420<<8) digits[421<<8) digits[422<<8) digits[423<<8) digits[424<<8) digits[425<<8) digits[426<<8) digits[427<<8) digits[428<<8) digits[429<<8) digits[430<<8) digits[431<<8) digits[432<<8) digits[433<<8) digits[434<<8) digits[435<<8) digits[436<<8) digits[437<<8) digits[438<<8) digits[439<<8) digits[440<<8) digits[441<<8) digits[442<<8) digits[443<<8) digits[444<<8) digits[445<<8) digits[446<<8) digits[447<<8) digits[448<<8) digits[449<<8) digits[450<<8) digits[451<<8) digits[452<<8) digits[453<<8) digits[454<<8) digits[455<<8) digits[456<<8) digits[457<<8) digits[458<<8) digits[459<<8) digits[460<<8) digits[461<<8) digits[462<<8) digits[463<<8) digits[464<<8) digits[465<<8) digits[466<<8) digits[467<<8) digits[468<<8) digits[469<<8) digits[470<<8) digits[471<<8) digits[472<<8) digits[473<<8) digits[474<<8) digits[475<<8) digits[476<<8) digits[477<<8) digits[478<<8) digits[479<<8) digits[480<<8) digits[481<<8) digits[482<<8) digits[483<<8) digits[484<<8) digits[485<<8) digits[486<<8) digits[487<<8) digits[488<<8) digits[489<<8) digits[490<<8) digits[491<<8) digits[492<<8) digits[493<<8) digits[494<<8) digits[495<<8) digits[496<<8) digits[497<<8) digits[498<<8) digits[499<<8) digits[500<<8) digits[501<<8) digits[502<<8) digits[503<<8) digits[504<<8) digits[505<<8) digits[506<<8) digits[507<<8) digits[508<<8) digits[509<<8) digits[510<<8) digits[511<<8) digits[512<<8) digits[513<<8) digits[514<<8) digits[515<<8) digits[516<<8) digits[517<<8) digits[518<<8) digits[519<<8) digits[520<<8) digits[521<<8) digits[522<<8) digits[523<<8) digits[524<<8) digits[525<<8) digits[526<<8) digits[527<<8) digits[528<<8) digits[529<<8) digits[530<<8) digits[531<<8) digits[532<<8) digits[533<<8) digits[534<<8) digits[535<<8) digits[536<<8) digits[537<<8) digits[538<<8) digits[539<<8) digits[540<<8) digits[541<<8) digits[542<<8) digits[543<<8) digits[544<<8) digits[545<<8) digits[546<<8) digits[547<<8) digits[548<<8) digits[549<<8) digits[550<<8) digits[551<<8) digits[552<<8) digits[553<<8) digits[554<<8) digits[555<<8) digits[556<<8) digits[557<<8) digits[558<<8) digits[559<<8) digits[560<<8) digits[561<<8) digits[562<<8) digits[563<<8) digits[564<<8) digits[565<<8) digits[566<<8) digits[567<<8) digits[568<<8) digits[569<<8) digits[570<<8) digits[571<<8) digits[572<<8) digits[573<<8) digits[574<<8) digits[575<<8) digits[576<<8) digits[577<<8) digits[578<<8) digits[579<<8) digits[580<<8) digits[581<<8) digits[582<<8) digits[583<<8) digits[584<<8) digits[585<<8) digits[586<<8) digits[587<<8) digits[588<<8) digits[589<<8) digits[590<<8) digits[591<<8) digits[592<<8) digits[593<<8) digits[594<<8) digits[595<<8) digits[596<<8) digits[597<<8) digits[598<<8) digits[599<<8) digits[600<<8) digits[601<<8) digits[602<<8) digits[603<<8) digits[604<<8) digits[605<<8) digits[606<<8) digits[607<<8) digits[608<<8) digits[609<<8) digits[610<<8) digits[611<<8) digits[612<<8) digits[613<<8) digits[614<<8) digits[615<<8) digits[616<<8) digits[617<<8) digits[618<<8) digits[619<<8) digits[620<<8) digits[621<<8) digits[622<<8) digits[623<<8) digits[624<<8) digits[625<<8) digits[626<<8) digits[627<<8) digits[628<<8) digits[629<<8) digits[630<<8) digits[631<<8) digits[632<<8) digits[633<<8) digits[634<<8) digits[635<<8) digits[636<<8) digits[637<<8) digits[638<<8) digits[639<<8) digits[640<<8) digits[641<<8) digits[642<<8) digits[643<<8) digits[644<<8) digits[645<<8) digits[646<<8) digits[647<<8) digits[648<<8) digits[649<<8) digits[650<<8) digits[651<<8) digits[652<<8) digits[653<<8) digits[654<<8) digits[655<<8) digits[656<<8) digits[657<<8) digits[658<<8) digits[659<<8) digits[660<<8) digits[661<<8) digits[662<<8) digits[663<<8) digits[664<<8) digits[665<<8) digits[666<<8) digits[667<<8) digits[668<<8) digits[669<<8) digits[670<<8) digits[671<<8) digits[672<<8) digits[673<<8) digits[674<<8) digits[675<<8) digits[676<<8) digits[677<<8) digits[678<<8) digits[679<<8) digits[680<<8) digits[681<<8) digits[682<<8) digits[683<<8) digits[684<<8) digits[685<<8) digits[686<<8) digits[687<<8) digits[688<<8) digits[689<<8) digits[690<<8) digits[691<<8) digits[692<<8) digits[693<<8) digits[694<<8) digits[695<<8) digits[696<<8) digits[697<<8) digits[698<<8) digits[699<<8) digits[700<<8) digits[701<<8) digits[702<<8) digits[703<<8) digits[704<<8) digits[705<<8) digits[706<<8) digits[707<<8) digits[708<<8) digits[709<<8) digits[710<<8) digits[711<<8) digits[712<<8) digits[713<<8) digits[714<<8) digits[715<<8) digits[716<<8) digits[717<<8) digits[718<<8) digits[719<<8) digits[720<<8) digits[721<<8) digits[722<<8) digits[723<<8) digits[724<<8) digits[725<<8) digits[726<<8) digits[727<<8) digits[728<<8) digits[729<<8) digits[730<<8) digits[731<<8) digits[732<<8) digits[733<<8) digits[734<<8) digits[735<<8) digits[736<<8) digits[737<<8) digits[738<<8) digits[739<<8) digits[740<<8) digits[741<<8) digits[742<<8) digits[743<<8) digits[744<<8) digits[745<<8) digits[746<<8) digits[747<<8) digits[748<<8) digits[749<<8) digits[750<<8) digits[751<<8) digits[752<<8) digits[753<<8) digits[754<<8) digits[755<<8) digits[756<<8) digits[757<<8) digits[758<<8) digits[759<<8) digits[760<<8) digits[761<<8) digits[762<<8) digits[763<<8) digits[764<<8) digits[765<<8) digits[766<<8) digits[767<<8) digits[768<<8) digits[769<<8) digits[770<<8) digits[771<<8) digits[772<<8) digits[773<<8) digits[774<<8) digits[775<<8) digits[776<<8) digits[777<<8) digits[778<<8) digits[779<<8) digits[780<<8) digits[781<<8) digits[782<<8) digits[783<<8) digits[784<<8) digits[785<<8) digits[786<<8) digits[787<<8) digits[788<<8) digits[789<<8) digits[790<<8) digits[791<<8) digits[792<<8) digits[793<<8) digits[794<<8) digits[795<<8) digits[796<<8) digits[797<<8) digits[798<<8) digits[799<<8) digits[800<<8) digits[801<<8) digits[802<<8) digits[803<<8) digits[804<<8) digits[805<<8) digits[806<<8) digits[807<<8) digits[808<<8) digits[809<<8) digits[810<<8) digits[811<<8) digits[812<<8) digits[813<<8) digits[814<<8) digits[815<<8) digits[816<<8) digits[817<<8) digits[818<<8) digits[819<<8) digits[820<<8) digits[821<<8) digits[822<<8) digits[823<<8) digits[824<<8) digits[825<<8) digits[826<<8) digits[827<<8) digits[828<<8) digits[829<<8) digits[830<<8) digits[831<<8) digits[832<<8) digits[833<<8) digits[834<<8) digits[835<<8) digits[836<<8) digits[837<<8) digits[838<<8) digits[839<<8) digits[840<<8) digits[841<<8) digits[842<<8) digits[843<<8) digits[844<<8) digits[845<<8) digits[846<<8) digits[847<<8) digits[848<<8) digits[849<<8) digits[850<<8) digits[851<<8) digits[852<<8) digits[853<<8) digits[854<<8) digits[855<<8) digits[856<<8) digits[857<<8) digits[858<<8) digits[859<<8) digits[860<<8) digits[861<<8) digits[862<<8) digits[863<<8) digits[864<<8) digits[865<<8) digits[866<<8) digits[867<<8) digits[868<<8) digits[869<<8) digits[870<<8) digits[871<<8) digits[872<<8) digits[873<<8) digits[874<<8) digits[875<<8) digits[876<<8) digits[877<<8) digits[878<<8) digits[879<<8) digits[880<<8) digits[881<<8) digits[882<<8) digits[883<<8) digits[884<<8) digits[885<<8) digits[886<<8) digits[887<<8) digits[888<<8) digits[889<<8) digits[890<<8) digits[891<<8) digits[892<<8) digits[893<<8) digits[894<<8) digits[895<<8) digits[896<<8) digits[897<<8) digits[898<<8) digits[899<<8) digits[900<<8) digits[901<<8) digits[902<<8) digits[903<<8) digits[904<<8) digits[905<<8) digits[906<<8) digits[907<<8) digits[908<<8) digits[909<<8) digits[910<<8) digits[911<<8) digits[912<<8) digits[913<<8) digits[914<<8) digits[915<<8) digits[916<<8) digits[917<<8) digits[918<<8) digits[919<<8) digits[920<<8) digits[921<<8) digits[922<<8) digits[923<<8) digits[924<<8) digits[925<<8) digits[926<<8) digits[927<<8) digits[928<<8) digits[929<<8) digits[930<<8) digits[931<<8) digits[932<<8) digits[933<<8) digits[934<<8) digits[935<<8) digits[936<<8) digits[937<<8) digits[938<<8) digits[939<<8) digits[940<<8) digits[941<<8) digits[942<<8) digits[943<<8) digits[944<<8) digits[945<<8) digits[946<<8) digits[947<<8) digits[948<<8) digits[949<<8) digits[950<<8) digits[951<<8) digits[952<<8) digits[953<<8) digits[954<<8) digits[955<<8) digits[956<<8) digits[957<<8) digits[958<<8) digits[959<<8) digits[960<<8) digits[961<<8) digits[962<<8) digits[963<<8) digits[964<<8) digits[965<<8) digits[966<<8) digits[967<<8) digits[968<<8) digits[969<<8) digits[970<<8) digits[971<<8) digits[972<<8) digits[973<<8) digits[974<<8) digits[975<<8) digits[976<<8) digits[977<<8) digits[978<<8) digits[979<<8) digits[980<<8) digits[981<<8) digits[982<<8) digits[983<<8) digits[984<<8) digits[985<<8) digits[986<<8) digits[987<<8) digits[988<<8) digits[989<<8) digits[990<<8) digits[991<<8) digits[992<<8) digits[993<<8) digits[994<<8) digits[995<<8) digits[996<<8) digits[997<<8) digits[998<<8) digits[999<<8) digits[1000<<8) digits[1001<<8) digits[1002<<8) digits[1003<<8) digits[1004<<8) digits[1005<<8) digits[1006<<8) digits[1007<<8) digits[1008<<8) digits[1009<<8) digits[1010<<8) digits[1011<<8) digits[1012<<8) digits[1013<<8) digits[1014<<8) digits[1015<<8) digits[1016<<8) digits[1017<<8) digits[1018<<8) digits[1019<<8) digits[1020<<8) digits[1021<<8) digits[1022<<8) digits[1023<<8) digits[1024<<8) digits[1025<<8) digits[1026<<8) digits[1027<<8) digits[1028<<8) digits[1029<<8) digits[1030<<8) digits[1031<<8) digits[1032<<8) digits[1033<<8) digits[1034<<8) digits[1035<<8) digits[1036<<8) digits[1037<<8) digits[1038<<8) digits[1039<<8) digits[1040<<8) digits[1041<<8) digits[1042<<8) digits[1043<<8) digits[1044<<8) digits[1045<<8) digits[1046<<8) digits[1047<<8) digits[1048<<8) digits[1049<<8) digits[1050<<8) digits[1051<<8) digits[1052<<8) digits[1053<<8) digits[1054<<8) digits[1055<<8) digits[1056<<8) digits[1057<<8) digits[1058<<8) digits[1059<<8) digits[1060<<8) digits[1061<<8) digits[1062<<8) digits[1063<<8) digits[1064<<8) digits[1065<<8) digits[1066<<8) digits[1067<<8) digits[1068<<8) digits[1069<<8) digits[1070<<8) digits[1071<<8) digits[1072<<8) digits[1073<<8) digits[1074<<8) digits[1075<<8) digits[1076<<8) digits[1077<<8) digits[1078<<8) digits[1079<<8) digits[1080<<8) digits[1081<<8) digits[1082<<8) digits[1083<<8) digits[1084<<8) digits[1085<<8) digits[1086<<8) digits[1087<<8) digits[1088<<8) digits[1089<<8) digits[1090<<8) digits[1091<<8) digits[1092<<8) digits[1093<<8) digits[1094<<8) digits[1095<<8) digits[1096<<8) digits[1097<<8) digits[1098<<8) digits[1099<<8) digits[1100<<8) digits[1101<<8) digits[1102<<8) digits[1103<<8) digits[1104<<8) digits[1105<<8) digits[1106<<8) digits[1107<<8) digits[1108<<8) digits[1109<<8) digits[1110<<8) digits[1111<<8) digits[1112<<8) digits[1113<<8) digits[1114<<8) digits[1115<<8) digits[1116<<8) digits[1117<<8) digits[1118<<8) digits[1119<<8) digits[1120<<8) digits[1121<<8) digits[1122<<8) digits[1123<<8) digits[1124<<8) digits[1125<<8) digits[1126<<8) digits[1127<<8) digits[1128<<8) digits[1129<<8) digits[1130<<8) digits[1131<<8) digits[1132<<8) digits[1133<<8) digits[1134<<8) digits[1135<<8) digits[1136<<8) digits[1137<<8) digits[1138<<8) digits[1139<<8) digits[1140<<8) digits[1141<<8) digits[1142<<8) digits[1143<<8) digits[1144<<8) digits[1145<<8) digits[1146<<8) digits[1147<<8) digits[1148<<8) digits[1149<<8) digits[1150<<8) digits[1151<<8) digits[1152<<8) digits[1153<<8) digits[1154<<8) digits[1155<<8) digits[1156<<8) digits[1157<<8) digits[1158<<8) digits[1159<<8) digits[1160<<8) digits[1161<<8) digits[1162<<8) digits[1163<<8) digits[1164<<8) digits[1165<<8) digits[1166<<8) digits[1167<<8) digits[1168<<8) digits[1169<<8) digits[1170<<8) digits[1171<<8) digits[1172<<8) digits[1173<<8) digits[1174<<8) digits[1175<<8) digits[1176<<8) digits[1177<<8) digits[1178<<8) digits[1179<<8) digits[1180<<8) digits[1181<<8) digits[1182<<8) digits[1183<<8) digits[1184<<8) digits[1185<<8) digits[1186<<8) digits[1187<<8) digits[1188<<8) digits[1189<<8) digits[1190<<8) digits[1191<<8) digits[1192<<8) digits[1193<<8) digits[1194<<8) digits[1195<<8) digits[1196<<8) digits[1197<<8) digits[1198<<8) digits[1199<<8) digits[1200<<8) digits[1201<<8) digits[1202<<8) digits[1203<<8) digits[1204<<8) digits[1205<<8) digits[1206<<8) digits[1207<<8) digits[1208<<8) digits[1209<<8) digits[1210<<8) digits[1211<<8) digits[1212<<8) digits[1213<<8) digits[1214<<8) digits[1215<<8) digits[1216<<8) digits[1217<<8) digits[1218<<8) digits[1219<<8) digits[1220<<8) digits[1221<<8) digits[1222<<8) digits[1223<<8) digits[1224<<8) digits[1225<<8) digits[1226<<8) digits[1227<<8) digits[1228<<8) digits[1229<<8) digits[1230<<8) digits[1231<<8) digits[1232<<8) digits[1233<<8) digits[1234<<8) digits[1235<<8) digits[1236<<8) digits[1237<<8) digits[1238<<8) digits[1239<<8) digits[1240<<8) digits[1241<<8) digits[1242<<8) digits[1243<<8) digits[1244<<8) digits[1245<<8) digits[1246<<8) digits[1247<<8) digits[1248<<8) digits[1249<<8) digits[1250<<8) digits[1251<<8) digits[1252<<8) digits[1253<<8) digits[1254<<8) digits[1255<<8) digits[1256<<8) digits[1257<<8) digits[1258<<8) digits[1259<<8) digits[1260<<8) digits[1261<<8) digits[1262<<8) digits[1263<<8) digits[1264<<8) digits[1265<<8) digits[1266<<8) digits[1267<<8) digits[1268<<8) digits[1269<<8) digits[1270<<8) digits[1271<<8) digits[1272<<8) digits[1273<<8) digits[1274<<8) digits[1275<<8) digits[1276<<8) digits[1277<<8) digits[1278<<8) digits[1279<<8) digits[1280<<8) digits[1281<<8) digits[1282<<8) digits[1283<<8) digits[1284<<8) digits[1285<<8) digits[1286<<8) digits[1287<<8) digits[1288<<8) digits[1289<<8) digits[1290<<8) digits[1291<<8) digits[1292<<8) digits[1293<<8) digits[1294<<8) digits[1295<<8) digits[1296<<8) digits[1297<<8) digits[1298<<8) digits[1299<<8) digits[1300<<8) digits[1301<<8) digits[1302<<8) digits[1303<<8) digits[1304<<8) digits[1305<<8) digits[1306<<8) digits[1307<<8) digits[1308<<8) digits[1309<<8) digits[1310<<8) digits[1311<<8) digits[1312<<8) digits[1313<<8) digits[1314<<8) digits[1315<<8) digits[1316<<8) digits[1317<<8) digits[1318<<8) digits[1319<<8) digits[1320<<8) digits[1321<<8) digits[1322<<8) digits[1323<<8) digits[1324<<8) digits[1325<<8) digits[1326<<8) digits[1327<<8) digits[1328<<8) digits[1329<<8) digits[1330<<8) digits[1331<<8) digits[1332<<8) digits[1333<<8) digits[1334<<8) digits[1335<<8) digits[1336<<8) digits[1337<<8) digits[1338<<8) digits[1339<<8) digits[1340<<8) digits[1341<<8) digits[1342<<8) digits[1343<<8) digits[1344<<8) digits[1345<<8) digits[1346<<8) digits[1347<<8) digits[1348<<8) digits[1349<<8) digits[1350<<8) digits[1351<<8) digits[1352<<8) digits[1353<<8) digits[1354<<8) digits[1355<<8) digits[1356<<8) digits[1357<<8) digits[1358<<8) digits[1359<<8) digits[1360<<8) digits[1361<<8) digits[1362<<8) digits[1363<<8) digits[1364<<8) digits[1365<<8) digits[1366<<8) digits[1367<<8) digits[1368<<8) digits[1369<<8) digits[1370<<8) digits[1371<<8) digits[1372<<8) digits[1373<<8) digits[1374<<8) digits[1375<<8) digits[1376<<8) digits[1377<<8) digits[1378<<8) digits[1379<<8) digits[1380<<8) digits[1381<<8) digits[1382<<8) digits[1383<<8) digits[1384<<8) digits[1385<<8) digits[1386<<8) digits[1387<<8) digits[1388<<8) digits[1389<<8) digits[1390<<8) digits[1391<<8) digits[1392<<8) digits[1393<<8) digits[1394<<8) digits[1395<<8) digits[1396<<8) digits[1397<<8) digits[1398<<8) digits[1399<<8) digits[1400<<8) digits[1401<<8) digits[1402<<8) digits[1403<<8) digits[1404<<8) digits[1405<<8) digits[1406<<8) digits[1407<<8) digits[1408<<8) digits[1409<<8) digits[1410<<8) digits[1411<<8) digits[1412<<8) digits[1413<<8) digits[1414<<8) digits[1415<<8) digits[1416<<8) digits[1417<<8) digits[1418<<8) digits[1419<<8) digits[1420<<8) digits[1421<<8) digits[1422<<8) digits[1423<<8) digits[1424<<8) digits[1425<<8) digits[1426<<8) digits[1427<<8) digits[1428<<8) digits[1429<<8) digits[1430<<8) digits[1431<<8) digits[1432<<8) digits[1433<<8) digits[1434<<8) digits[1435<<8) digits[1436<<8) digits[1437<<8) digits[1438<<8) digits[1439<<8) digits[1440<<8) digits[1441<<8) digits[1442<<8) digits[1443<<8) digits[1444<<8) digits[1445<<8) digits[1446<<8) digits[1447<<8) digits[1448<<8) digits[1449<<8) digits[1450<<8) digits[1451<<8) digits[1452<<8) digits[1453<<8) digits[1454<<8) digits[1455<<8) digits[1456<<8) digits[1457<<8) digits[1458<<8) digits[1459<<8) digits[1460<<8) digits[1461<<8) digits[1462<<8) digits[1463<<8) digits[1464<<8) digits[1465<<8) digits[1466<<8) digits[1467<<8) digits[1468<<8) digits[1469<<8) digits[1470<<8) digits[1471<<8) digits[1472<<8) digits[1473<<8) digits[1474<<8) digits[1475<<8) digits[1476<<8) digits[1477<<8) digits[1478<<8) digits[1479<<8) digits[1480<<8) digits[1481<<8) digits[1482<<8) digits[1483<<8) digits[1484<<8) digits[1485<<8) digits[1486<<8) digits[1487<<8) digits[1488<<8) digits[1489<<8) digits[1490<<8) digits[1491<<8) digits[1492<<8) digits[1493<<8) digits[1494<<8) digits[1495<<8) digits[1496<<8) digits[1497<<8) digits[1498<<8) digits[1499<<8) digits[1500<<8) digits[1501<<8) digits[1502<<8) digits[1503<<8) digits[1504<<8) digits[1505<<8) digits[1506<<8) digits[1507<<8) digits[1508<<8) digits[1509<<8) digits[1510<<8) digits[1511<<8) digits[1512<<8) digits[1513<<8) digits[1514<<8) digits[1515<<8) digits[1516<<8) digits[1517<<8) digits[1518<<8) digits[1519<<8) digits[1520<<8) digits[1521<<8) digits[1522<<8) digits[1523<<8) digits[1524<<8) digits[1525<<8) digits[1526<<8) digits[1527<<8) digits[1528<<8) digits[1529<<8) digits[1530<<8) digits[1531<<8) digits[1532<<8) digits[1533<<8) digits[1534<<8) digits[1535<<8) digits[1536<<8) digits[1537<<8) digits[1538<<8) digits[1539<<8) digits[1540<<8) digits[1541<<8) digits[1542<<8) digits[1543<<8) digits[1544<<8) digits[1545<<8) digits[1546<<8) digits[1547<<8) digits[1548<<8) digits[1549<<8) digits[1550<<8) digits[1551<<8) digits[1552<<8) digits[1553<<8) digits[1554<<8) digits[1555<<8) digits[1556<<8) digits[1557<<8) digits[1558<<8) digits[1559<<8) digits[1560<<8) digits[1561<<8) digits[1562<<8) digits[1563<<8) digits[1564<<8) digits[1565<<8) digits[1566<<8) digits[1567<<8) digits[1568<<8) digits[1569<<8) digits[1570<<8) digits[1571<<8) digits[1572<<8) digits[1573<<8) digits[1574<<8) digits[1575<<8) digits[1576<<8) digits[1577<<8) digits[1578<<8) digits[1579<<8) digits[1580<<8) digits[1581<<8) digits[1582<<8) digits[1583<<8) digits[1584<<8) digits[1585<<8) digits[1586<<8) digits[1587<<8) digits[1588<<8) digits[1589<<8) digits[1590<<8) digits[1591<<8) digits[1592<<8) digits[1593<<8) digits[1594<<8) digits[1595<<8) digits[1596<<8) digits[1597<<8) digits[1598<<8) digits[1599<<8) digits[1600<<8) digits[1601<<8) digits[1602<<8) digits[1603<<8) digits[1604<<8) digits[1605<<8) digits[1606<<8) digits[1607<<8) digits[1608<<8) digits[1609<<8) digits[1610<<8) digits[1611<<8) digits[1612<<8) digits[1613<<8) digits[1614<<8) digits[1615<<8) digits[1616<<8) digits[1617<<8) digits[1618<<8) digits[1619<<8) digits[1620<<8) digits[1621<<8) digits[1622<<8) digits[1623<<8) digits[1624<<8) digits[1625<<8) digits[1626<<8) digits[1627<<8) digits[1628<<8) digits[1629<<8) digits[1630<<8) digits[1631<<8) digits[1632<<8) digits[1633<<8) digits[1634<<8) digits[1635<<8) digits[1636<<8) digits[1637<<8) digits[1638<<8) digits[1639<<8) digits[1640<<8) digits[1641<<8) digits[1642<<8) digits[1643<<8) digits[1644<<8) digits[1645<<8) digits[1646<<8) digits[1647<<8) digits[1648<<8) digits[1649<<8) digits[1650<<8) digits[1651<<8) digits[1652<<8) digits[1653<<8) digits[1654<<8) digits[1655<<8) digits[1656<<8) digits[1657<<8) digits[1658<<8) digits[1659<<8) digits[1660<<8) digits[1661<<8) digits[1662<<8) digits[1663<<8) digits[1664<<8) digits[1665<<8) digits[1666<<8) digits[1667<<8) digits[1668<<8) digits[1669<<8) digits[1670<<8) digits[1671<<8) digits[1672<<8) digits[1673<<8) digits[1674<<8) digits[1675<<8) digits[1676<<8) digits[1677<<8) digits[1678<<8) digits[1679<<8) digits[1680<<8) digits[1681<<8) digits[1682<<8) digits[1683<<8) digits[1684<<8) digits[1685<<8) digits[1686<<8) digits[1687<<8) digits[1688<<8) digits[1689<<8) digits[1690<<8) digits[1691<<8) digits[1692<<8) digits[1693<<8) digits[1694<<8) digits[1695<<8) digits[1696<<8) digits[1697<<8) digits[1698<<8) digits[1699<<8) digits[1700<<8) digits[1701<<8) digits[1702<<8) digits[1703<<8) digits[1704<<8) digits[1705<<8) digits[1706<<8) digits[1707<<8) digits[1708<<8) digits[1709<<8) digits[1710<<8) digits[1711<<8) digits[1712<<8) digits[1713<<8) digits[1714<<8) digits[1715<<8) digits[1716<<8) digits[1717<<8) digits[1718<<8) digits[1719<<8) digits[1720<<8) digits[1721<<8) digits[1722<<8) digits[1723<<8) digits[1724<<8) digits[1725<<8) digits[1726<<8) digits[1727<<8) digits[1728<<8) digits[1729<<8) digits[1730<<8) digits[1731<<8) digits[1732<<8) digits[1733<<8) digits[1734<<8) digits[1735<<8) digits[1736<<8) digits[1737<<8) digits[1738<<8) digits[1739<<8) digits[1740<<8) digits[1741<<8) digits[1742<<8) digits[1743<<8) digits[1744<<8) digits[1745<<8) digits[1746<<8) digits[1747<<8) digits[1748<<8) digits[1749<<8) digits[1750<<8) digits[1751<<8) digits[1752<<8) digits[1753<<8) digits[1754<<8) digits[1755<<8) digits[1756<<8) digits[1757<<8) digits[1758<<8) digits[1759<<8) digits[1760<<8) digits[1761<<8) digits[1762<<8) digits[1763<<8) digits[1764<<8) digits[1765<<8) digits[1766<<8) digits[1767<<8) digits[1768<<8) digits[1769<<8) digits[1770<<8) digits[1771<<8) digits[1772<<8) digits[1773<<8) digits[1774<<8) digits[1775<<8) digits[1776<<8) digits[1777<<8) digits[1778<<8) digits[1779<<8) digits[1780<<8) digits[1781<<8) digits[1782<<8) digits[1783<<8) digits[1784<<8) digits[1785<<8) digits[1786<<8) digits[1787<<8) digits[1788<<8) digits[1789<<8) digits[1790<<8) digits[1791<<8) digits[1792<<8) digits[1793<<8) digits[1794<<8) digits[1795<<8) digits[1796<<8) digits[1797<<8) digits[1798<<8) digits[1799<<8) digits[1800<<8) digits[1801<<8) digits[1802<<8) digits[1803<<8) digits[1804<<8) digits[1805<<8) digits[1806<<8) digits[1807<<8) digits[1808<<8) digits[1809<<8) digits[1810<<8) digits[1811<<8) digits[1812<<8) digits[1813<<8) digits[1814<<8) digits[1815<<8) digits[1816<<8) digits[1817<<8) digits[1818<<8) digits[1819<<8) digits[1820<<8) digits[1821<<8) digits[1822<<8) digits[1823<<8) digits[1824<<8) digits[1825<<8) digits[1826<<8) digits[1827<<8) digits[1828<<8) digits[1829<<8) digits[1830<<8) digits[1831<<8) digits[1832<<8) digits[1833<<8) digits[1834<<8) digits[1835<<8) digits[1836<<8) digits[1837<<8) digits[1838<<8) digits[1839<<8) digits[1840<<8) digits[1841<<8) digits[1842<<8) digits[1843<<8) digits[1844<<8) digits[1845<<8) digits[1846<<8) digits[1847<<8) digits[1848<<8) digits[1849<<8) digits[1850<<8) digits[1851<<8) digits[1852<<8) digits[1853<<8) digits[1854<<8) digits[1855<<8) digits[1856<<8) digits[1857<<8) digits[1858<<8) digits[1859<<8) digits[1860<<8) digits[1861<<8) digits[1862<<8) digits[1863<<8) digits[1864<<8) digits[1865<<8) digits[1866<<8) digits[1867<<8) digits[1868<<8) digits[1869<<8) digits[1870<<8) digits[1871<<8) digits[1872<<8) digits[1873<<8) digits[1874<<8) digits[1875<<8) digits[1876<<8) digits[1877<<8) digits[1878<<8) digits[1879<<8) digits[1880<<8) digits[1881<<8) digits[1882<<8) digits[1883<<8) digits[1884<<8) digits[1885<<8) digits[1886<<8) digits[1887<<8) digits[1888<<8) digits[1889<<8) digits[1890<<8) digits[1891<<8) digits[1892<<8) digits[1893<<8) digits[1894<<8) digits[1895<<8) digits[1896<<8) digits[1897<<8) digits[1898<<8) digits[1899<<8) digits[1900<<8) digits[1901<<8) digits[1902<<8) digits[1903<<8) digits[1904<<8) digits[1905<<8) digits[1906<<8) digits[1907<<8) digits[1908<<8) digits[1909<<8) digits[1910<<8) digits[1911<<8) digits[1912<<8) digits[1913<<8) digits[1914<<8) digits[1915<<8) digits[1916<<8) digits[1917<<8) digits[1918<<8) digits[1919<<8) digits[1920<<8) digits[1921<<8) digits[1922<<8) digits[1923<<8) digits[1924<<8) digits[1925<<8) digits[1926<<8) digits[1927<<8) digits[1928<<8) digits[1929<<8) digits[1930<<8) digits[1931<<8) digits[1932<<8) digits[1933<<8) digits[1934<<8) digits[1935<<8) digits[1936<<8) digits[1937<<8) digits[1938<<8) digits[1939<<8) digits[1940<<8) digits[1941<<8) digits[1942<<8) digits[1943<<8) digits[1944<<8) digits[1945<<8) digits[1946<<8) digits[1947<<8) digits[1948<<8) digits[1949<<8) digits[1950<<8) digits[1951<<8) digits[1952<<8) digits[1953<<8) digits[1954<<8) digits[1955<<8) digits[1956<<8) digits[1957<<8) digits[1958<<8) digits[1959<<8) digits[1960<<8) digits[1961<<8) digits[1962<<8) digits[1963<<8) digits[1964<<8) digits[1965<<8) digits[1966<<8) digits[1967<<8) digits[1968<<8) digits[1969<<8) digits[1970<<8) digits[1971<<8) digits[1972<<8) digits[1973<<8) digits[1974<<8) digits[1975<<8) digits[1976<<8) digits[1977<<8) digits[1978<<8) digits[1979<<8) digits[1980<<8) digits[1981<<8) digits[1982<<8) digits[1983<<8) digits[1984<<8) digits[1985<<8) digits[1986<<8) digits[1987<<8) digits[1988<<8) digits[1989<<8) digits[1990<<8) digits[1991<<8) digits[1992<<8) digits[1993<<8) digits[1994<<8) digits[1995<<8) digits[1996<<8) digits[1997<<8) digits[1998<<8) digits[1999<<8) digits[2000<<8) digits[2001<<8) digits[2002<<8) digits[2003<<8) digits[2004<<8) digits[2005<<8) digits[2006<<8) digits[2007<<8) digits[2008<<8) digits[2009<<8) digits[2010<<8) digits[2011<<8) digits[2012<<8) digits[2013<<8) digits[2014<<8) digits[2015<<8) digits[2016<<8) digits[2017<<8) digits[2018<<8) digits[2019<<8) digits[2020<<8) digits[2021<<8) digits[2022<<8) digits[2023<<8) digits[2024<<8) digits[2025<<8) digits[2026<<8) digits[2027<<8) digits[2028<<8) digits[2029<<8) digits[2030<<8) digits[2031<<8) digits[2032<<8) digits[2033<<8) digits[2034<<8) digits[2035<<8) digits[2036<<8) digits[2037<<8) digits[2038<<8) digits[2039<<8) digits[2040<<8) digits[2041<<8) digits[2042<<8) digits[2043<<8) digits[2044<<8) digits[2045<<8) digits[2046<<8) digits[2047<<8) digits[2048<<8) digits[2049<<8) digits[2050<<8) digits[2051<<8) digits[2052<<8) digits[2053<<8) digits[2054<<8) digits[2055<<8) digits[2056<<8) digits[2057<<8) digits[2058<<8) digits[2059<<8) digits[2060<<8) digits[2061<<8) digits[2062<<8) digits[2063<<8) digits[2064<<8) digits[2065<<8) digits[2066<<8) digits[2067<<8) digits[2068<<8) digits[2069<<8) digits[2070<<8) digits[2071<<8) digits[2072<<8) digits[2073<<8) digits[2074<<8) digits[2075<<8) digits[2076<<8) digits[2077<<8) digits[2078<<8) digits[2079<<8) digits[2080<<8) digits[2081<<8) digits[2082<<8) digits[2083<<8) digits[2084<<8) digits[2085<<8) digits[2086<<8) digits[2087<<8) digits[2088<<8) digits[2089<<8) digits[2090<<8) digits[2091<<8) digits[2092<<8) digits[2093<<8) digits[2094<<8) digits[2095<<8) digits[2096<<8) digits[2097<<8) digits[2098<<8) digits[2099<<8) digits[2100<<8) digits[2101<<8) digits[2102<<8) digits[2103<<8) digits[2104<<8) digits[2105<<8) digits[2106<<8) digits[2107<<8) digits[2108<<8) digits[2109<<8) digits[2110<<8) digits[2111<<8) digits[2112<<8) digits[2113<<8) digits[2114<<8) digits[2115<<8) digits[2116<<8) digits[2117<<8) digits[2118<<8) digits[2119<<8) digits[2120<<8) digits[2121<<8) digits[2122<<8) digits[2123<<8) digits[2124<<8) digits[2125<<8) digits[2126<<8) digits[2127<<8) digits[2128<<8) digits[2129<<8) digits[2130<<8) digits[2131<<8) digits[2132<<8) digits[2133<<8) digits[2134<<8) digits[2135<<8) digits[2136<<8) digits[2137<<8) digits[2138<<8) digits[2139<<8) digits[2140<<8) digits[2141<<8) digits[2142<<8) digits[2143<<8) digits[2144<<8) digits[2145<<8) digits[2146<<8) digits[2147<<8) digits[2148<<8) digits[2149<<8) digits[2150<<8) digits[2151<<8) digits[2152<<8) digits[2153<<8) digits[2154<<8) digits[2155<<8) digits[2156<<8) digits[2157<<8) digits[2158<<8) digits[2159<<8) digits[2160<<8) digits[2161<<8) digits[2162<<8) digits[2163<<8) digits[2164<<8) digits[2165<<8) digits[2166<<8) digits[2167<<8) digits[2168<<8) digits[2169<<8) digits[2170<<8) digits[2171<<8) digits[2172<<8) digits[2173<<8) digits[2174<<8) digits[2175<<8) digits[2176<<8) digits[2177<<8) digits[2178<<8) digits[2179<<8) digits[2180<<8) digits[2181<<8) digits[2182<<8) digits[2183<<8) digits[2184<<8) digits[2185<<8) digits[2186<<8) digits[2187<<8) digits[2188<<8) digits[2189<<8) digits[2190<<8) digits[2191<<8) digits[2192<<8) digits[2193<<8) digits[2194<<8) digits[2195<<8) digits[2196<<8) digits[2197<<8) digits[2198<<8) digits[2199<<8) digits[2200<<8) digits[2201<<8) digits[2202<<8) digits[2203<<8) digits[2204<<8) digits[2205<<8) digits[2206<<8) digits[2207<<8) digits[2208<<8) digits[2209<<8) digits[2210<<8) digits[2211<<8) digits[2212<<8) digits[2213<<8) digits[2214<<8) digits[2215<<8) digits[2216<<8) digits[2217<<8) digits[2218<<8) digits[2219<<8) digits[2220<<8) digits[2221<<8) digits[2222<<8) digits[2223<<8) digits[2224<<8) digits[2225<<8) digits[2226<<8) digits[2227<<8) digits[2228<<8) digits[2229<<8) digits[2230<<8) digits[2231<<8) digits[2232<<8) digits[2233<<8) digits[2234<<8) digits[2235<<8) digits[2236<<8) digits[2237<<8) digits[2238<<8) digits[2239<<8) digits[2240<<8) digits[2241<<8) digits[2242<<8) digits[2243<<8) digits[2244<<8) digits[2245<<8) digits[2246<<8) digits[2247<<8) digits[2248<<8) digits[2249<<8) digits[2250<<8) digits[2251<<8) digits[2252<<8) digits[2253<<8) digits[2254<<8) digits[2255<<8) digits[2256<<8) digits[2257<<8) digits[2258<<8) digits[2259<<8) digits[2260<<8) digits[2261<<8) digits[2262<<8) digits[2263<<8) digits[2264<<8) digits[2265<<8) digits[2266<<8) digits[2267<<8) digits[2268<<8) digits[2269<<8) digits[2270<<8) digits[2271<<8) digits[2272<<8) digits[2273<<8) digits[2274<<8) digits[2275<<8) digits[2276<<8) digits[2277<<8) digits[2278<<8) digits[2279<<8) digits[2280<<8) digits[2281<<8) digits[2282<<8) digits[2283<<8) digits[2284<<8) digits[2285<<8) digits[2286<<8) digits[2287<<8) digits[2288<<8) digits[2289<<8) digits[2290<<8) digits[2291<<8) digits[2292<<8) digits[2293<<8) digits[2294<<8) digits[2295<<8) digits[2296<<8) digits[2297<<8) digits[2298<<8) digits[2299<<8) digits[2300<<8) digits[2301<<8) digits[2302<<8) digits[2303<<8) digits[2304<<8) digits[2305<<8) digits[2306<<8) digits[2307<<8) digits[2308<<8) digits[2309<<8) digits[2310<<8) digits[2311<<8) digits[2312<<8) digits[2313<<8) digits[2314<<8) digits[2315<<8) digits[2316<<8) digits[2317<<8) digits[2318<<8) digits[2319<<8) digits[2320<<8) digits[2321<<8) digits[2322<<8) digits[2323<<8) digits[2324<<8) digits[2325<<8) digits[2326<<8) digits[2327<<8) digits[2328<<8) digits[2329<<8) digits[2330<<8) digits[2331<<8) digits[2332<<8) digits[2333<<8) digits[2334<<8) digits[2335<<8) digits[2336<<8) digits[2337<<8) digits[2338<<8) digits[2339<<8) digits[2340<<8) digits[2341<<8) digits[2342<<8) digits[2343<<8) digits[2344<<8) digits[2345<<8) digits[2346<<8) digits[2347<<8) digits[2348<<8) digits[2349<<8) digits[2350<<8) digits[2351<<8) digits[2352<<8) digits[2353<<8) digits[2354<<8) digits[2355<<8) digits[2356<<8) digits[2357<<8) digits[2358<<8) digits[2359<<8) digits[2360<<8) digits[2361<<8) digits[2362<<8) digits[2363<<8) digits[2364<<8) digits[2365<<8) digits[2366<<8) digits[2367<<8) digits[2368<<8) digits[2369<<8) digits[2370<<8) digits[2371<<8) digits[2372<<8) digits[2373<<8) digits[2374<<8) digits[2375<<8) digits[2376<<8) digits[2377<<8) digits[2378<<8) digits[2379<<8) digits[2380<<8) digits[2381<<8) digits[2382<<8) digits[2383<<8) digits[2384<<8) digits[2385<<8) digits[2386<<8) digits[2387<<8) digits[2388<<8) digits[2389<<8) digits[2390<<8) digits[2391<<8) digits[2392<<8) digits[2393<<8) digits[2394<<8) digits[2395<<8) digits[2396<<8) digits[2397<<8) digits[2398<<8) digits[2399<<8) digits[2400<<8) digits[2401<<8) digits[2402<<8) digits[2403<<8) digits[2404<<8) digits[2405<<8) digits[2406<<8) digits[2407<<8) digits[2408<<8) digits[2409<<8) digits[2410<<8) digits[2411<<8) digits[2412<<8) digits[2413<<8) digits[2414<<8) digits[2415<<8) digits[2416<<8) digits[2417<<8) digits[2418<<8) digits[2419<<8) digits[2420<<8) digits[2421<<8) digits[2422<<8) digits[2423<<8) digits[2424<<8) digits[2425<<8) digits[2426<<8) digits[2427<<8) digits[2428<<8) digits[2429<<8) digits[2430<<8) digits[2431<<8) digits[2432<<8) digits[2433<<8) digits[2434<<8) digits[2435<<8) digits[2436<<8) digits[2437<<8) digits[2438<<8) digits[2439<<8) digits[2440<<8) digits[2441<<8) digits[2442<<8) digits[2443<<8) digits[2444<<8) digits[2445<<8) digits[2446<<8) digits[2447<<8) digits[2448<<8) digits[2449<<8) digits[2450<<8) digits[2451<<8) digits[2452<<8) digits[2453<<8) digits[2454<<8) digits[2455<<8) digits[2456<<8) digits[2457<<8) digits[2458<<8) digits[2459<<8) digits[2460<<8) digits[2461<<8) digits[2462<<8) digits[2463<<8) digits"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t size = lst.size();\n    size_t simd_size = size / 4 * 4; // Round down to nearest multiple of 4\n    __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]); // Load 4 floats\n        __m128 ceil_vec = _mm_ceil_ps(vec); // Apply ceiling\n        __m128 squared_vec = _mm_mul_ps(ceil_vec, ceil_vec); // Square each element\n        sum = _mm_add_ps(sum, squared_vec); // Accumulate the sum\n    }\n\n    // Horizontal sum of the SIMD register\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    float total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (size_t i = simd_size; i < size; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_width = 4; // SSE processes 4 floats at a time\n    size_t simd_end = n / simd_width * simd_width; // Align to SIMD width\n\n    __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        // Load 4 floats into an SSE register\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n\n        // Apply ceiling to each element\n        __m128 ceil_vec = _mm_ceil_ps(vec);\n\n        // Square each element\n        __m128 squared_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n\n        // Accumulate the squared values\n        sum = _mm_add_ps(sum, squared_vec);\n    }\n\n    // Horizontal sum of the accumulated values\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    float total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements (if any) after SIMD processing\n    for (size_t i = simd_end; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_size = n / 4 * 4; // Ensure we process in multiples of 4\n    __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]); // Load 4 floats into SIMD register\n        __m128 ceil_vec = _mm_ceil_ps(vec); // Apply ceiling to each element\n        __m128 squared_vec = _mm_mul_ps(ceil_vec, ceil_vec); // Square each element\n        sum = _mm_add_ps(sum, squared_vec); // Accumulate squared values\n    }\n\n    // Horizontal sum of the SIMD register\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    uint64_t result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        result += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cmath>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_size = n / 4 * 4; // Ensure multiple of 4 for SIMD processing\n    __m128 sum = _mm_setzero_ps(); // Initialize sum to zero\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        // Load 4 floats into an SSE register\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        // Apply ceiling to each element\n        __m128 ceil_vec = _mm_ceil_ps(vec);\n        // Square each element\n        __m128 squared_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n        // Accumulate the squared values\n        sum = _mm_add_ps(sum, squared_vec);\n    }\n\n    // Horizontal sum of the accumulated values\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    float total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements (if n is not a multiple of 4)\n    for (size_t i = simd_size; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return static_cast<uint64_t>(total_sum);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t simd_size = n / 4 * 4; // Number of elements that can be processed in SIMD\n    __m128 sum = _mm_setzero_ps(); // Accumulator for SIMD sum\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]); // Load 4 floats into SIMD register\n        __m128 ceil_vec = _mm_ceil_ps(vec); // Apply ceiling to each element\n        __m128 squared_vec = _mm_mul_ps(ceil_vec, ceil_vec); // Square each element\n        sum = _mm_add_ps(sum, squared_vec); // Accumulate squared values\n    }\n\n    // Horizontal sum of the SIMD register\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    uint64_t result = static_cast<uint64_t>(temp[0] + temp[1] + temp[2] + temp[3]);\n\n    // Process remaining elements\n    for (size_t i = simd_size; i < n; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        result += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process elements in chunks of 4 using SSE\n    __m128i prev, curr;\n    int result = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        // Load 4 elements into SSE registers\n        curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        prev = _mm_loadu_si128((__m128i*)&arr[i - 1]);\n\n        // Compare adjacent elements\n        __m128i cmp = _mm_cmpgt_epi32(curr, prev);\n        if (cmp) result = i;\n    }\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process in chunks of 4 elements using SSE\n    int i = 0;\n    __m128i prev, curr;\n    for (; i + 4 <= n; i += 4) {\n        prev = _mm_loadu_si128((__m128i*)&arr[i]);       // Load 4 elements starting at arr[i]\n        curr = _mm_loadu_si128((__m128i*)&arr[i + 1]);   // Load next 4 elements starting at arr[i+1]\n        \n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);       // Compare prev > curr\n        int mask = _mm_movemask_epi8(cmp);               // Extract comparison results as a mask\n        \n        if (mask != 0) {                                 // If any comparison is true\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;                    // Return the index of the first mismatch\n                }\n            }\n        }\n    }\n\n    // Process remaining elements sequentially\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1; // No mismatch found\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process elements in chunks of 4 using SSE\n    __m128i prev, curr;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        // Load 4 elements into SSE registers\n        prev = _mm_loadu_si128((__m128i*)&arr[i]);\n        curr = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n\n        // Compare adjacent elements (curr < prev)\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n\n        // Check if any comparison result is true\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            // Find the first failing index\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process elements in chunks of 4 using SSE\n    int i = 0;\n    __m128i prev, curr, mask;\n\n    for (; i + 4 <= n; i += 4) {\n        // Load 4 elements into SSE registers\n        prev = _mm_loadu_si128((__m128i*)&arr[i]);\n        curr = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n\n        // Compare adjacent elements\n        mask = _mm_cmpgt_epi32(prev, curr);\n\n        // Check if any comparison is true\n        int result_mask = _mm_movemask_epi8(mask);\n        if (result_mask != 0) {\n            // Find the first failing index within the chunk\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements sequentially\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> &arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    // Process in chunks of 4 using SIMD\n    __m128i prev, curr, mask;\n    int result = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        // Load 4 elements into SIMD registers\n        prev = _mm_loadu_si128((__m128i*)&arr[i]);\n        curr = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n\n        // Compare adjacent elements (curr < prev)\n        mask = _mm_cmpgt_epi32(prev, curr);\n\n        // Check if any comparison is true\n        int mask_result = _mm_movemask_epi8(mask);\n        if (mask_result != 0) {\n            // Find the first failing index in the chunk\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] > arr[i + j + 1]) {\n                    return i + j + 1;\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - (n % 4); i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg = _mm_set1_epi32(INT_MIN); // Initialize max_neg to smallest possible integer\n    __m128i min_pos = _mm_set1_epi32(INT_MAX); // Initialize min_pos to largest possible integer\n\n    size_t size = lst.size();\n    size_t simd_size = size / 4 * 4; // Round down to nearest multiple of 4\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        // Mask for negative integers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n        __m128i neg_values = _mm_and_si128(vec, neg_mask);\n        max_neg = _mm_max_epi32(max_neg, neg_values);\n\n        // Mask for positive integers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i pos_values = _mm_and_si128(vec, pos_mask);\n        min_pos = _mm_min_epi32(min_pos, pos_values);\n    }\n\n    // Handle remaining elements\n    int max_neg_scalar = INT_MIN;\n    int min_pos_scalar = INT_MAX;\n    for (size_t i = simd_size; i < size; ++i) {\n        if (lst[i] < 0) {\n            max_neg_scalar = std::max(max_neg_scalar, lst[i]);\n        } else if (lst[i] > 0) {\n            min_pos_scalar = std::min(min_pos_scalar, lst[i]);\n        }\n    }\n\n    // Extract max_neg and min_pos from SIMD registers\n    int max_neg_array[4], min_pos_array[4];\n    _mm_storeu_si128((__m128i*)max_neg_array, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_array, min_pos);\n\n    for (int i = 0; i < 4; ++i) {\n        max_neg_scalar = std::max(max_neg_scalar, max_neg_array[i]);\n        min_pos_scalar = std::min(min_pos_scalar, min_pos_array[i]);\n    }\n\n    // If no negative or positive integers found, return 0 for respective values\n    if (max_neg_scalar == INT_MIN) max_neg_scalar = 0;\n    if (min_pos_scalar == INT_MAX) min_pos_scalar = 0;\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <vector>\n#include <limits>\n#include <emmintrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i largest_neg = _mm_set1_epi32(std::numeric_limits<int>::min());\n    __m128i smallest_pos = _mm_set1_epi32(std::numeric_limits<int>::max());\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        // Mask for negative integers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        // Mask for positive integers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n\n        // Update largest negative\n        __m128i neg_values = _mm_and_si128(vec, neg_mask);\n        largest_neg = _mm_max_epi32(largest_neg, neg_values);\n\n        // Update smallest positive\n        __m128i pos_values = _mm_and_si128(vec, pos_mask);\n        smallest_pos = _mm_min_epi32(smallest_pos, pos_values);\n    }\n\n    // Handle remaining elements\n    int largest_neg_scalar = std::numeric_limits<int>::min();\n    int smallest_pos_scalar = std::numeric_limits<int>::max();\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0) {\n            largest_neg_scalar = std::max(largest_neg_scalar, lst[i]);\n        } else if (lst[i] > 0) {\n            smallest_pos_scalar = std::min(smallest_pos_scalar, lst[i]);\n        }\n    }\n\n    // Extract results from SIMD registers\n    alignas(16) int neg_array[4];\n    alignas(16) int pos_array[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(neg_array), largest_neg);\n    _mm_store_si128(reinterpret_cast<__m128i*>(pos_array), smallest_pos);\n\n    for (int j = 0; j < 4; ++j) {\n        largest_neg_scalar = std::max(largest_neg_scalar, neg_array[j]);\n        smallest_pos_scalar = std::min(smallest_pos_scalar, pos_array[j]);\n    }\n\n    // If no valid negative or positive integers were found, set them to 0\n    if (largest_neg_scalar == std::numeric_limits<int>::min()) {\n        largest_neg_scalar = 0;\n    }\n    if (smallest_pos_scalar == std::numeric_limits<int>::max()) {\n        smallest_pos_scalar = 0;\n    }\n\n    return {largest_neg_scalar, smallest_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i largest_negative = _mm_set1_epi32(INT_MIN);\n    __m128i smallest_positive = _mm_set1_epi32(INT_MAX);\n\n    size_t size = lst.size();\n    size_t simd_size = size / 4 * 4;\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        __m128i negatives_mask = _mm_cmplt_epi32(values, _mm_setzero_si128());\n        __m128i positives_mask = _mm_cmpgt_epi32(values, _mm_setzero_si128());\n\n        __m128i negative_values = _mm_and_si128(values, negatives_mask);\n        __m128i positive_values = _mm_and_si128(values, positives_mask);\n\n        largest_negative = _mm_max_epi32(largest_negative, negative_values);\n        smallest_positive = _mm_min_epi32(smallest_positive, positive_values);\n    }\n\n    int largest_neg_array[4];\n    int smallest_pos_array[4];\n    _mm_storeu_si128((__m128i*)largest_neg_array, largest_negative);\n    _mm_storeu_si128((__m128i*)smallest_pos_array, smallest_positive);\n\n    int largest_negative_result = INT_MIN;\n    int smallest_positive_result = INT_MAX;\n\n    for (int i = 0; i < 4; ++i) {\n        largest_negative_result = std::max(largest_negative_result, largest_neg_array[i]);\n        smallest_positive_result = std::min(smallest_positive_result, smallest_pos_array[i]);\n    }\n\n    for (size_t i = simd_size; i < size; ++i) {\n        if (lst[i] < 0) {\n            largest_negative_result = std::max(largest_negative_result, lst[i]);\n        } else if (lst[i] > 0) {\n            smallest_positive_result = std::min(smallest_positive_result, lst[i]);\n        }\n    }\n\n    if (largest_negative_result == INT_MIN) {\n        largest_negative_result = 0;\n    }\n    if (smallest_positive_result == INT_MAX) {\n        smallest_positive_result = 0;\n    }\n\n    return {largest_negative_result, smallest_positive_result};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i largest_neg = _mm_set1_epi32(std::numeric_limits<int>::min());\n    __m128i smallest_pos = _mm_set1_epi32(std::numeric_limits<int>::max());\n    __m128i zero = _mm_setzero_si128();\n\n    size_t size = lst.size();\n    size_t simd_size = size / 4 * 4;\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        // Mask for negative integers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i neg_values = _mm_and_si128(vec, neg_mask);\n        largest_neg = _mm_max_epi32(largest_neg, neg_values);\n\n        // Mask for positive integers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n        __m128i pos_values = _mm_and_si128(vec, pos_mask);\n        smallest_pos = _mm_min_epi32(smallest_pos, pos_values);\n    }\n\n    // Handle remaining elements\n    int largest_neg_scalar = std::numeric_limits<int>::min();\n    int smallest_pos_scalar = std::numeric_limits<int>::max();\n    for (size_t i = simd_size; i < size; ++i) {\n        if (lst[i] < 0) {\n            largest_neg_scalar = std::max(largest_neg_scalar, lst[i]);\n        } else if (lst[i] > 0) {\n            smallest_pos_scalar = std::min(smallest_pos_scalar, lst[i]);\n        }\n    }\n\n    // Extract results from SIMD registers\n    alignas(16) int neg_array[4];\n    alignas(16) int pos_array[4];\n    _mm_store_si128((__m128i*)neg_array, largest_neg);\n    _mm_store_si128((__m128i*)pos_array, smallest_pos);\n\n    for (int i = 0; i < 4; ++i) {\n        largest_neg_scalar = std::max(largest_neg_scalar, neg_array[i]);\n        smallest_pos_scalar = std::min(smallest_pos_scalar, pos_array[i]);\n    }\n\n    // If no negative or positive integers, return 0 for respective values\n    if (largest_neg_scalar == std::numeric_limits<int>::min()) {\n        largest_neg_scalar = 0;\n    }\n    if (smallest_pos_scalar == std::numeric_limits<int>::max()) {\n        smallest_pos_scalar = 0;\n    }\n\n    return {largest_neg_scalar, smallest_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i max_neg = _mm_set1_epi32(INT_MIN); // Initialize to smallest possible integer\n    __m128i min_pos = _mm_set1_epi32(INT_MAX); // Initialize to largest possible integer\n    __m128i zero = _mm_setzero_si128();\n\n    size_t size = lst.size();\n    size_t simd_size = size / 4 * 4; // Round down to nearest multiple of 4\n\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        // Mask for negative integers\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i neg_values = _mm_and_si128(vec, neg_mask);\n        max_neg = _mm_max_epi32(max_neg, neg_values);\n\n        // Mask for positive integers\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n        __m128i pos_values = _mm_and_si128(vec, pos_mask);\n        min_pos = _mm_min_epi32(min_pos, pos_values);\n    }\n\n    // Process remaining elements\n    int max_neg_scalar = INT_MIN;\n    int min_pos_scalar = INT_MAX;\n    for (size_t i = simd_size; i < size; ++i) {\n        if (lst[i] < 0) {\n            max_neg_scalar = std::max(max_neg_scalar, lst[i]);\n        } else if (lst[i] > 0) {\n            min_pos_scalar = std::min(min_pos_scalar, lst[i]);\n        }\n    }\n\n    // Extract results from SIMD registers\n    int max_neg_array[4];\n    int min_pos_array[4];\n    _mm_storeu_si128((__m128i*)max_neg_array, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_array, min_pos);\n\n    for (int i = 0; i < 4; ++i) {\n        max_neg_scalar = std::max(max_neg_scalar, max_neg_array[i]);\n        min_pos_scalar = std::min(min_pos_scalar, min_pos_array[i]);\n    }\n\n    // Handle cases where no negative or positive integers exist\n    if (max_neg_scalar == INT_MIN) {\n        max_neg_scalar = 0;\n    }\n    if (min_pos_scalar == INT_MAX) {\n        min_pos_scalar = 0;\n    }\n\n    return {max_neg_scalar, min_pos_scalar};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    __m128i result = _mm_set1_epi64x(1); // Initialize result to 1\n    for (uint64_t i = n; i > 0; i -= 2) {\n        __m128i factorials = _mm_set_epi64x(i, i - 1); // Load two consecutive numbers\n        __m128i temp = _mm_set1_epi64x(1);\n\n        // Compute factorials for both numbers in parallel\n        for (uint64_t j = i; j > 0; j--) {\n            temp = _mm_mul_epu32(temp, _mm_set1_epi64x(j));\n        }\n        for (uint64_t j = i - 1; j > 0; j--) {\n            temp = _mm_mul_epu32(temp, _mm_set1_epi64x(j));\n        }\n\n        result = _mm_mul_epu32(result, temp); // Multiply the results\n    }\n\n    // Extract the final result\n    uint64_t final_result[2];\n    _mm_storeu_si128((__m128i *)final_result, result);\n    return final_result[0] * final_result[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    __m128i result = _mm_set1_epi64x(1); // Initialize result to 1\n    __m128i one = _mm_set1_epi64x(1);    // Constant 1\n    __m128i current = _mm_set1_epi64x(n); // Start with n\n\n    while (n > 0) {\n        // Calculate n factorial\n        __m128i factorial = _mm_set1_epi64x(1);\n        __m128i counter = current;\n\n        while (_mm_cvtsi128_si64(counter) > 1) {\n            factorial = _mm_mul_epu32(factorial, counter);\n            counter = _mm_sub_epi64(counter, one);\n        }\n\n        // Multiply the result by the current factorial\n        result = _mm_mul_epu32(result, factorial);\n\n        // Decrement n and update current\n        n--;\n        current = _mm_set1_epi64x(n);\n    }\n\n    // Extract the final result\n    return _mm_cvtsi128_si64(result);\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t factorials[4] = {1, 1, 1, 1}; // Array to store intermediate factorials\n    uint64_t temp[4]; // Temporary array for SIMD operations\n\n    __m128i vec_factorials = _mm_set_epi64x(1, 1); // Initialize SIMD vector with 1\n    __m128i vec_indices = _mm_set_epi64x(1, 2); // Initialize indices for factorial computation\n\n    for (uint64_t i = 1; i <= n; i+=4){\n    }\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    __m128i result = _mm_set1_epi64x(1); // Initialize result vector to 1\n    for (uint64_t i = n; i > 0; i -= 2) {\n        __m128i factorials = _mm_set_epi64x(i, (i > 1 ? i - 1 : 1)); // Load i and i-1 into vector\n\n        // Compute factorials for i and i-1\n        __m128i temp = factorials;\n        for (uint64_t j = i - 1; j > 1; j--) {\n            temp = _mm_mul_epu32(temp, _mm_set1_epi64x(j));\n        }\n\n        // Multiply the result vector by the computed factorials\n        result = _mm_mul_epu32(result, temp);\n    }\n\n    // Extract the results from the vector and multiply them together\n    uint64_t res[2];\n    _mm_storeu_si128((__m128i*)res, result);\n    return res[0] * res[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    uint64_t factorials[4] = {1, 1, 1, 1}; // Array to store intermediate factorials\n    uint64_t temp[4];                      // Temporary array for SIMD operations\n\n    for (uint64_t i = n; i > 0; i -= 4) {\n        // Compute factorials for four numbers in parallel\n        uint64_t a = i;\n        uint64_t b = (i > 1) ? i - 1 : 1;\n        uint64_t c = (i > 2) ? i - 2 : 1;\n        uint64_t d = (i > 3) ? i - 3 : 1;\n\n        factorials[0] *= a;\n        factorials[1] *= b;\n        factorials[2] *= c;\n        factorials[3] *= d;\n\n        temp[0] = factorials[0];\n        temp[1] = factorials[1];\n        temp[2] = factorials[2];\n        temp[3] = factorials[3];\n\n        // Load values into SIMD registers\n        __m128i vec = _mm_set_epi64x(temp[1], temp[0]);\n        __m128i vec2 = _mm_set_epi64x(temp[3], temp[2]);\n\n        // Multiply the values in parallel\n        __m128i prod = _mm_mul_epu32(vec, vec2);\n\n        // Store the result back into the array\n        _mm_store_si128((__m128i*)temp, prod);\n\n        // Accumulate the result\n        result *= temp[0];\n        result *= temp[1];\n        result *= temp[2];\n        result *= temp[3];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i = 0;\n\n    for (; i + 4 <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        __m128i mod3_mask = _mm_cmpeq_epi32(_mm_rem_epi32(indices, _mm_set1_epi32(3)), _mm_setzero_si128());\n        __m128i mod4_mask = _mm_and_si128(\n            _mm_cmpeq_epi32(_mm_rem_epi32(indices, _mm_set1_epi32(4)), _mm_setzero_si128()),\n            _mm_cmpeq_epi32(mod3_mask, _mm_setzero_si128())\n        );\n\n        __m128i squared = _mm_mullo_epi32(values, values);\n        __m128i cubed = _mm_mullo_epi32(squared, values);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mod3_mask, squared),\n            _mm_and_si128(mod4_mask, cubed)\n        );\n\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(mod3_mask, mod4_mask), values));\n        sum_vec = _mm_add_epi32(sum_vec, result);\n    }\n\n    int sum = 0;\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, sum_vec);\n    for (int j = 0; j < 4; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        sum += value;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to 0\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i); // Load indices\n        __m128i values = _mm_set_epi32(\n            i + 3 < n ? lst[i + 3] : 0,\n            i + 2 < n ? lst[i + 2] : 0,\n            i + 1 < n ? lst[i + 1] : 0,\n            i < n ? lst[i] : 0); // Load values\n\n        __m128i mod3 = _mm_rem_epi32(indices, three); // indices % 3\n        __m128i mod4 = _mm_rem_epi32(indices, four);  // indices % 4\n\n        __m128i is_multiple_of_3 = _mm_cmpeq_epi32(mod3, _mm_setzero_si128());\n        __m128i is_multiple_of_4 = _mm_cmpeq_epi32(mod4, _mm_setzero_si128());\n        __m128i is_multiple_of_4_not_3 = _mm_and_si128(is_multiple_of_4, _mm_xor_si128(is_multiple_of_3, _mm_set1_epi32(-1)));\n\n        __m128i squared = _mm_mullo_epi32(values, values); // Square values\n        __m128i cubed = _mm_mullo_epi32(squared, values);  // Cube values\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(is_multiple_of_3, squared),\n            _mm_and_si128(is_multiple_of_4_not_3, cubed)); // Apply conditions\n\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(is_multiple_of_3, is_multiple_of_4_not_3), values)); // Keep unchanged values\n\n        sum = _mm_add_epi32(sum, result); // Accumulate sum\n    }\n\n    // Horizontal sum of the final vector\n    int result[4];\n    _mm_storeu_si128((__m128i *)result, sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n    __m128i zero = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        __m128i mod3 = _mm_rem_epi32(indices, three);\n        __m128i mod4 = _mm_rem_epi32(indices, four);\n\n        __m128i is_multiple_of_3 = _mm_cmpeq_epi32(mod3, zero);\n        __m128i is_multiple_of_4 = _mm_andnot_si128(is_multiple_of_3, _mm_cmpeq_epi32(mod4, zero));\n\n        __m128i squared = _mm_mullo_epi32(values, values);\n        __m128i cubed = _mm_mullo_epi32(squared, values);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(is_multiple_of_3, squared),\n            _mm_and_si128(is_multiple_of_4, cubed)\n        );\n\n        __m128i unchanged = _mm_andnot_si128(\n            _mm_or_si128(is_multiple_of_3, is_multiple_of_4),\n            values\n        );\n\n        result = _mm_or_si128(result, unchanged);\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    int total_sum = 0;\n    int buffer[4];\n    _mm_storeu_si128((__m128i*)buffer, sum);\n    for (int j = 0; j < 4; ++j) {\n        total_sum += buffer[j];\n    }\n\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        total_sum += value;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n    int i = 0;\n\n    // Process in chunks of 4 integers using SIMD\n    for (; i + 4 <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i); // Indices\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]); // Load 4 integers\n\n        // Check if index is a multiple of 3\n        __m128i mod3 = _mm_set1_epi32(3);\n        __m128i mod3_result = _mm_rem_epi32(indices, mod3); // indices % 3\n        __m128i is_multiple_of_3 = _mm_cmpeq_epi32(mod3_result, _mm_setzero_si128());\n\n        // Check if index is a multiple of 4\n        __m128i mod4 = _mm_set1_epi32(4);\n        __m128i mod4_result = _mm_rem_epi32(indices, mod4); // indices % 4\n        __m128i is_multiple_of_4 = _mm_cmpeq_epi32(mod4_result, _mm_setzero_si128());\n\n        // Check if index is a multiple of 4 but not 3\n        __m128i is_multiple_of_4_not_3 = _mm_andnot_si128(is_multiple_of_3, is_multiple_of_4);\n\n        // Square values where index is a multiple of 3\n        __m128i squared_values = _mm_mullo_epi32(values, values);\n        __m128i result_for_3 = _mm_and_si128(is_multiple_of_3, squared_values);\n\n        // Cube values where index is a multiple of 4 but not 3\n        __m128i cubed_values = _mm_mullo_epi32(values, _mm_mullo_epi32(values, values));\n        __m128i result_for_4 = _mm_and_si128(is_multiple_of_4_not_3, cubed_values);\n\n        // Keep original values for other indices\n        __m128i result_for_other = _mm_andnot_si128(_mm_or_si128(is_multiple_of_3, is_multiple_of_4_not_3), values);\n\n        // Combine results\n        __m128i final_result = _mm_or_si128(_mm_or_si128(result_for_3, result_for_4), result_for_other);\n\n        // Accumulate sum\n        sum = _mm_add_epi32(sum, final_result);\n    }\n\n    // Horizontal sum of the SIMD register\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            total_sum += value * value;\n        } else if (i % 4 == 0) {\n            total_sum += value * value * value;\n        } else {\n            total_sum += value;\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m128i sum = _mm_setzero_si128(); // Initialize sum to zero\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using SIMD\n    for (; i + 4 <= n; i += 4) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n\n        // Compute masks for multiples of 3 and 4\n        __m128i mod3_mask = _mm_cmpeq_epi32(_mm_rem_epi32(indices, _mm_set1_epi32(3)), _mm_setzero_si128());\n        __m128i mod4_mask = _mm_andnot_si128(mod3_mask, _mm_cmpeq_epi32(_mm_rem_epi32(indices, _mm_set1_epi32(4)), _mm_setzero_si128()));\n\n        // Square values for indices that are multiples of 3\n        __m128i squared_values = _mm_mullo_epi32(values, values);\n        __m128i cube_values = _mm_mullo_epi32(squared_values, values);\n\n        // Apply masks\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mod3_mask, squared_values),\n            _mm_and_si128(mod4_mask, cube_values)\n        );\n\n        // Add unchanged values for indices not multiple of 3 or 4\n        __m128i unchanged_mask = _mm_andnot_si128(_mm_or_si128(mod3_mask, mod4_mask), _mm_set1_epi32(-1));\n        result = _mm_or_si128(result, _mm_and_si128(unchanged_mask, values));\n\n        // Accumulate the result into sum\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    // Horizontal sum of the accumulated sum\n    int final_sum[4];\n    _mm_storeu_si128((__m128i*)final_sum, sum);\n    int total_sum = final_sum[0] + final_sum[1] + final_sum[2] + final_sum[3];\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            value *= value;\n        } else if (i % 4 == 0) {\n            value = value * value * value;\n        }\n        total_sum += value;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int size = nums.size();\n    const int simdWidth = 4; // SSE processes 4 integers at a time\n    int result = 0;\n    __m128i count = _mm_setzero_si128(); // Initialize SIMD counter to zero\n\n    for (int i = 0; i < size; i += simdWidth) {\n        // Load 4 integers into an SSE register\n        __m128i vec = _mm_set_epi32(\n            i + 3 < size ? nums[i + 3] : 0,\n            i + 2 < size ? nums[i + 2] : 0,\n            i + 1 < size ? nums[i + 1] : 0,\n            nums[i]\n        );\n\n        // Check if numbers are greater than 10\n        __m128i ten = _mm_set1_epi32(10);\n        __m128i greaterThanTen = _mm_cmpgt_epi32(vec, ten);\n\n        // Compute absolute values of the numbers\n        __m128i negVec = _mm_sub_epi32(_mm_setzero_si128(), vec);\n        __m128i absVec = _mm_max_epi32(vec, negVec);\n\n        // Extract the last digit (absVec % 10)\n        __m128i lastDigit = _mm_sub_epi32(absVec, _mm_mullo_epi32(_mm_div_epi32(absVec, ten), ten));\n\n        // Extract the first digit\n        __m128i firstDigit = absVec;\n        __m128i zero = _mm_setzero_si128();\n        while (_mm_movemask_epi8(_mm_cmpgt_epi32(firstDigit, ten))) {\n            firstDigit = _mm_div_epi32(firstDigit, ten);\n        }\n\n        // Check if first and last digits are odd\n        __m128i one = _mm_set1_epi32(1);\n        __m128i oddMask = _mm_set1_epi32(1);\n        __m128i firstOdd = _mm_and_si128(firstDigit, oddMask);\n        __m128i lastOdd = _mm_and_si128(lastDigit, oddMask);\n        __m128i bothOdd = _mm_and_si128(firstOdd, lastOdd);\n\n        // Combine all conditions\n        __m128i valid = _mm_and_si128(greaterThanTen, bothOdd);\n\n        // Count valid elements\n        count = _mm_add_epi32(count, _mm_and_si128(valid, one));\n    }\n\n    // Sum up the counts in the SIMD register\n    int counts[4];\n    _mm_storeu_si128((__m128i *)counts, count);\n    result = counts[0] + counts[1] + counts[2] + counts[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int size = nums.size();\n    int count = 0;\n\n    // Process in chunks of 4 integers using SSE\n    __m128i vec_10 = _mm_set1_epi32(10); // Vector with all elements set to 10\n    __m128i odd_mask = _mm_set1_epi32(0x55555555); // Mask for odd digits (binary pattern)\n\n    int i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]); // Load 4 integers\n        __m128i abs_vec = _mm_abs_epi32(vec); // Take absolute value\n\n        // Check if elements are greater than 10\n        __m128i cmp_gt_10 = _mm_cmpgt_epi32(abs_vec, vec_10);\n\n        // Extract first and last digits\n        __m128i first_digit = _mm_div_epi32(abs_vec, _mm_set1_epi32(1000000000)); // Divide by 10^9 (assuming max 10 digits)\n        __m128i last_digit = _mm_rem_epi32(abs_vec, _mm_set1_epi32(10)); // Modulo 10\n\n        // Check if first and last digits are odd\n        __m128i first_odd = _mm_and_si128(first_digit, odd_mask);\n        __m128i last_odd = _mm_and_si128(last_digit, odd_mask);\n\n        __m128i odd_check = _mm_and_si128(first_odd, last_odd);\n\n        // Combine conditions\n        __m128i result = _mm_and_si128(cmp_gt_10, odd_check);\n\n        // Count the number of valid elements\n        int mask = _mm_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10 || num < -10) {\n            int abs_num = std::abs(num);\n            int first_digit = abs_num;\n            while (first_digit >= 10) first_digit /= 10;\n            int last_digit = abs_num % 10;\n            if ((first_digit % 2 == 1) && (last_digit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    const int size = nums.size();\n    int count = 0;\n\n    // Process in chunks of 4 integers using SSE\n    __m128i vec_gt10, vec_nums, vec_mask, vec_count;\n    vec_count = _mm_setzero_si128(); // Initialize count to zero\n\n    for (int i = 0; i < size; i += 4) {\n        // Load 4 integers into SSE register\n        int temp[4] = {0, 0, 0, 0};\n        for (int j = 0; j < 4 && (i + j) < size; ++j) {\n            temp[j] = nums[i + j];\n        }\n        vec_nums = _mm_loadu_si128((__m128i*)temp);\n\n        // Check if numbers are greater than 10\n        vec_gt10 = _mm_cmpgt_epi32(vec_nums, _mm_set1_epi32(10));\n\n        // Mask for numbers > 10\n        vec_mask = _mm_and_si128(vec_gt10, vec_nums);\n\n        // Check first and last digits are odd\n        for (int j = 0; j < 4; ++j) {\n            int num = temp[j];\n            if (num > 10 && num%10%2==odd)\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t size = nums.size();\n    size_t simd_size = size / 4 * 4; // Ensure size is divisible by 4 for SIMD processing\n\n    __m128i threshold = _mm_set1_epi32(10); // Set threshold value\n    __m128i odd_digits = _mm_set_epi32(1, 3, 5, 7); // Odd digits mask\n\n    for (size_t i = 0; i < simd_size; i+=4) {\n        __m128i vector = _mm_loadu_si128((__m128i *)&nums[i]); // Load 4 ints into vector\n        __m128i greater_than = _mm_cmpgt_epi32(vector, threshold); // Compare greater than threshold\n        __m128i odd_digits = _mm_set_epi32(1,3,5,7); // \n    }\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> &nums) {\n    const int size = nums.size();\n    const int simdWidth = 4; // SSE processes 4 integers at a time\n    int result = 0;\n\n    __m128i oddMask = _mm_set1_epi32(0xAAAAAAAA); // Mask for odd digits (binary pattern for odd numbers)\n    __m128i ten = _mm_set1_epi32(10);            // Constant 10 for comparison\n\n    int i = 0;\n    for (; i <= size - simdWidth; i += simdWidth) {\n        __m128i data = _mm_loadu_si128((__m128i*)&nums[i]); // Load 4 integers\n\n        __m128i absData = _mm_abs_epi32(data);              // Take absolute values\n        __m128i cmpGt10 = _mm_cmpgt_epi32(absData, ten);    // Compare > 10\n\n        // Extract first and last digits\n        __m128i firstDigit = absData;\n        while (true) {\n            __m128i div10 = _mm_div_epi32(firstDigit,ten)\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Allocate memory for the array\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n\n    // Compute the array values using SIMD\n    __m128i idx = _mm_set_epi32(4, 3, 2, 1);\n    __m128i one = _mm_set1_epi32(1);\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m128i i_vec = _mm_add_epi32(idx, _mm_set1_epi32(i));\n        __m128i i_squared = _mm_mullo_epi32(i_vec, i_vec);\n        __m128i result = _mm_sub_epi32(_mm_add_epi32(i_squared, one), i_vec);\n        _mm_store_si128((__m128i *)(a + i), result);\n    }\n\n    // Count triples (a[i], a[j], a[k]) where i < j < k and sum is divisible by 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; ++k) {\n                if ((a[i] + a[j] + a[k]) % 3 == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    // Free allocated memory\n    _mm_free(a);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Step 1: Generate the array `a` using SIMD\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    __m128i idx = _mm_set_epi32(4, 3, 2, 1); // Initialize indices\n    __m128i step = _mm_set1_epi32(4);        // Step for incrementing indices\n    __m128i one = _mm_set1_epi32(1);         // Constant 1\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m128i i_squared = _mm_mullo_epi32(idx, idx); // i * i\n        __m128i i_minus_i = _mm_sub_epi32(i_squared, idx); // i * i - i\n        __m128i result = _mm_add_epi32(i_minus_i, one); // i * i - i + 1\n        _mm_storeu_si128((__m128i *)&a[i], result);\n        idx = _mm_add_epi32(idx, step); // Increment indices\n    }\n\n    // Step 2: Count remainders modulo 3\n    uint64_t count_mod[3] = {0, 0, 0};\n    __m128i mod3 = _mm_set1_epi32(3);\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m128i values = _mm_loadu_si128((__m128i *)&a[i]);\n        __m128i remainders = _mm_rem_epi32(values, mod3);\n\n        for (int j = 0; j < 4 && i + j < n; ++j) {\n            int rem = _mm_extract_epi32(remainders, j);\n            count_mod[rem]++;\n        }\n    }\n\n    _mm_free(a);\n\n    // Step 3: Calculate the number of valid triples\n    uint64_t count = 0;\n    count += (count_mod[0] * (count_mod[0] - 1) * (count_mod[0] - 2)) / 6; // All 0s\n    count += (count_mod[1] * (count_mod[1] - 1) * (count_mod[1] - 2)) / 6; // All 1s\n    count += (count_mod[2] * (count_mod[2] - 1) * (count_mod[2] - 2)) / 6; // All 2s\n    count += count_mod[0] * count_mod[1] * count_mod[2]; // Mixed triples\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Allocate memory for the array\n    uint64_t* a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 16);\n\n    // Compute values of a[i] using SIMD\n    __m128i idx = _mm_set_epi32(4, 3, 2, 1); // Initialize indices\n    __m128i step = _mm_set1_epi32(4);        // Step for incrementing indices\n    __m128i one = _mm_set1_epi32(1);         // Constant 1\n    __m128i result;\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        if (i + 4 > n) { // Handle remaining elements\n            for (uint64_t j = i; j < n; ++j) {\n                a[j] = j * j - j + 1;\n            }\n            break;\n        }\n\n        __m128i idx_sq = _mm_mullo_epi32(idx, idx); // idx * idx\n        result = _mm_sub_epi32(idx_sq, idx);        // idx * idx - idx\n        result = _mm_add_epi32(result, one);        // idx * idx - idx + 1\n\n        _mm_store_si128((__m128i*)(a + i), result); // Store results in array\n\n        idx = _mm_add_epi32(idx, step);             // Increment indices\n    }\n\n    // Count triples where a[i] + a[j] + a[k] is divisible by 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; ++k) {\n                if ((a[i] + a[j] + a[k]) % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    // Free allocated memory\n    _mm_free(a);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) {\n        return 0;\n    }\n\n    // Allocate memory for the array\n    uint64_t* a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 16);\n\n    // Compute the values of a[i] using SIMD\n    __m128i idx = _mm_set_epi32(4, 3, 2, 1);\n    __m128i one = _mm_set1_epi32(1);\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m128i i_vec = _mm_add_epi32(idx, _mm_set1_epi32(i));\n        __m128i i_squared = _mm_mullo_epi32(i_vec, i_vec);\n        __m128i result = _mm_sub_epi32(_mm_add_epi32(i_squared, one), i_vec);\n        _mm_storeu_si128((__m128i*)&a[i], result);\n    }\n\n    // Count triples where a[i] + a[j] + a[k] is a multiple of 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; ++k) {\n                if ((a[i] + a[j] + a[k]) % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    // Free allocated memory\n    _mm_free(a);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    // Allocate memory for the array\n    uint64_t* a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 16);\n\n    // Compute the values of a[i] using SIMD\n    __m128i idx = _mm_set_epi32(4, 3, 2, 1); // Indices for SIMD computation\n    __m128i step = _mm_set1_epi32(4);        // Step for incrementing indices\n    __m128i one = _mm_set1_epi32(1);         // Constant 1\n    __m128i result;\n\n    for (uint64_t i = 0; i < n; i += 4) {\n        // Handle boundary cases where n is not a multiple of 4\n        if (i + 4 > n) {\n            for (uint64_t j = i; j < n; ++j) {\n                a[j] = j * j - j + 1;\n            }\n            break;\n        }\n\n        // Compute a[i:i+3] = i * i - i + 1\n        result = _mm_mullo_epi32(idx, idx);  // idx * idx\n        result = _mm_sub_epi32(result, idx); // idx * idx - idx\n        result = _mm_add_epi32(result, one); // idx * idx - idx + 1\n\n        // Store the result in the array\n        _mm_store_si128((__m128i*)&a[i], result);\n\n        // Increment indices\n        idx = _mm_add_epi32(idx, step);\n    }\n\n    // Count triples where a[i] + a[j] + a[k] is a multiple of 3\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; ++k) {\n                if ((a[i] + a[j] + a[k]) % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    // Free allocated memory\n    _mm_free(a);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    // Check if n is less than 2 (not prime)\n    if (n < 2) {\n        return y;\n    }\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int limit = (int)_mm_sqrt_ps(_mm_set_ss((float)n))[0];\n    __m128i vec = _mm_set_epi32(limit - 3, limit - 2, limit - 1, limit);\n\n    // Broadcast n to all lanes\n    __m128i n_vec = _mm_set1_epi32(n);\n\n    // Perform modulo operation for each element in vec\n    __m128i mod_results = _mm_rem_epi32(n_vec, vec);\n\n    // Compare results with zero (if any result is zero, n is not prime)\n    __m128i cmp = _mm_cmpeq_epi32(mod_results, _mm_setzero_si128());\n\n    // Check if any lane is true (non-prime)\n    int mask = _mm_movemask_epi8(cmp);\n    if (mask != 0) {\n        return y;\n    }\n\n    // If no lane is true, n is prime\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) return y;\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int limit = (int)_mm_sqrt_ps(_mm_set_ps1((float)n))[0];\n    __m128i vec = _mm_set_epi32(limit, limit - 1, limit - 2, limit - 3);\n\n    // Check divisibility for each number in the vector\n    for (int i = 2; i <= limit; i++)\n        if (n % i == 0)\n            return y;\n\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    // Check for edge cases\n    if (n <= 1) {\n        return y; // Not a prime number\n    }\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int limit = (int)_mm_sqrt_ps(_mm_set_ps1((float)n))[0];\n    __m128i vec = _mm_set_epi32(limit, limit - 1, limit - 2, limit - 3);\n\n    // Broadcast n to all lanes\n    __m128i n_vec = _mm_set1_epi32(n);\n\n    // Perform modulo operation in parallel\n    __m128i mod_results = _mm_rem_epi32(n_vec, vec);\n\n    // Check if any result is zero (indicating n is divisible by vec[i])\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i cmp = _mm_cmpeq_epi32(mod_results, zero_vec);\n\n    // Extract results\n    int mask = _mm_movemask_epi8(cmp);\n\n    // If any lane is zero, n is not prime\n    if (mask != 0) {\n        return y;\n    }\n\n    return x; // n is prime\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) return y;\n\n    // Create a vector of integers from 2 to sqrt(n) (approximation)\n    int limit = (int)_mm_sqrt_ss(_mm_set_ss((float)n));\n    __m128i vec = _mm_set_epi32(limit - 3, limit - 2, limit - 1, limit);\n\n    // Broadcast n across all lanes\n    __m128i n_vec = _mm_set1_epi32(n);\n\n    // Perform modulo operation for each lane\n    __m128i mod_result = _mm_rem_epi32(n_vec, vec);\n\n    // Compare results to zero (prime check)\n    __m128i cmp = _mm_cmpeq_epi32(mod_result, _mm_setzero_si128());\n\n    // Check if any lane is zero (non-prime)\n    int mask = _mm_movemask_epi8(cmp);\n\n    // If mask is non-zero, n is not prime\n    return mask ? y : x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n < 2) {\n        return y;\n    }\n\n    // Create a vector of integers from 2 to sqrt(n)\n    int sqrt_n = (int)_mm_sqrt_ss(_mm_set_ss((float)n));\n    int range = sqrt_n - 1;\n    __m128i vec = _mm_set_epi32(2, 3, 4, 5);\n    __m128i vec = _mm_set_epi32(2, 3, 4, 5);\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    __m128 sum = _mm_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i int_vec = _mm_cvttps_epi32(vec);\n\n        __m128i odd_mask = _mm_and_si128(int_vec, _mm_set1_epi32(1));\n        __m128i is_odd = _mm_cmpeq_epi32(odd_mask, _mm_set1_epi32(1));\n        __m128i is_positive = _mm_cmpgt_epi32(int_vec, _mm_setzero_si128());\n\n        __m128i valid_mask = _mm_and_si128(is_odd, is_positive);\n        __m128 valid_vec = _mm_and_ps(vec, _mm_castsi128_ps(valid_mask));\n\n        __m128 squared = _mm_mul_ps(valid_vec, valid_vec);\n        sum = _mm_add_ps(sum, squared);\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    result += (uint64_t)(temp[0] + temp[1] + temp[2] + temp[3]);\n\n    for (; i < n; ++i) {\n        int val = (int)lst[i];\n        if (val > 0 && val % 2 != 0) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i zero_int = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi32(1);\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 input = _mm_loadu_ps(&lst[i]);\n        __m128 positive_mask = _mm_cmpge_ps(input, zero);\n        __m128 truncated = _mm_floor_ps(input);\n        __m128i integers = _mm_cvtps_epi32(truncated);\n        __m128i odd_mask = _mm_and_si128(integers, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi32(odd_mask, mask_odd);\n\n        __m128i valid_mask = _mm_and_si128(is_odd, _mm_castps_si128(positive_mask));\n        __m128i squared = _mm_mullo_epi32(integers, integers);\n        __m128i valid_squares = _mm_and_si128(squared, valid_mask);\n\n        __m128i sum = _mm_hadd_epi32(valid_squares, zero_int);\n        sum = _mm_hadd_epi32(sum, zero_int);\n\n        result += _mm_cvtsi128_si32(sum);\n    }\n\n    for (; i < n; ++i) {\n        float val = lst[i];\n        if (val >= 0 && std::floor(val) == val && ((int)val % 2 != 0)) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    __m128 sum = _mm_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128i vec_int = _mm_cvtps_epi32(vec); // Convert to integers\n        __m128 vec_float = _mm_cvtepi32_ps(vec_int); // Convert back to float\n        __m128 mask_negative = _mm_cmplt_ps(vec, _mm_setzero_ps()); // Negative mask\n        __m128 mask_integer = _mm_cmpeq_ps(vec, vec_float); // Integer mask\n        __m128 mask_odd = _mm_and_ps(_mm_cmpeq_ps(_mm_sub_ps(vec_float, _mm_floor_ps(vec_float)), _mm_set1_ps(1.0f)), mask_integer); // Odd mask\n        __m128 mask = _mm_andnot_ps(mask_negative, mask_odd); // Combine masks\n\n        __m128 squared = _mm_mul_ps(vec, vec); // Square the values\n        squared = _mm_and_ps(squared, mask); // Apply mask\n        sum = _mm_add_ps(sum, squared); // Accumulate\n    }\n\n    // Horizontal sum of the accumulated values\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        float val = lst[i];\n        if (val >= 0 && val == (int)val && ((int)val % 2 != 0)) {\n            result += (uint64_t)(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <vector>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i zero_int = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 values = _mm_loadu_ps(&lst[i]);\n        __m128 positive_mask = _mm_cmpge_ps(values, zero);\n        __m128 truncated_values = _mm_floor_ps(values);\n        __m128 integer_mask = _mm_cmpeq_ps(truncated_values, values);\n        __m128 valid_mask = _mm_and_ps(positive_mask, integer_mask);\n\n        __m128i int_values = _mm_cvtps_epi32(truncated_values);\n        __m128i odd_mask = _mm_and_si128(int_values, _mm_set1_epi32(1));\n        odd_mask = _mm_cmpeq_epi32(odd_mask, _mm_set1_epi32(1));\n\n        __m128i valid_odd_mask = _mm_and_si128(_mm_castps_si128(valid_mask), odd_mask);\n        __m128i squared_values = _mm_mullo_epi32(int_values, int_values);\n        __m128i masked_squares = _mm_and_si128(squared_values, valid_odd_mask);\n\n        __m128i sum = _mm_hadd_epi32(masked_squares, zero_int);\n        sum = _mm_hadd_epi32(sum, zero_int);\n\n        result += _mm_cvtsi128_si32(sum);\n    }\n\n    for (; i < n; ++i) {\n        float value = lst[i];\n        if (value >= 0 && value == (int)value && ((int)value & 1)) {\n            result += (uint64_t)((int)value * (int)value);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> &lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i sum = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128 values = _mm_loadu_ps(&lst[i]);\n        __m128 positive_mask = _mm_cmpge_ps(values, zero);\n        __m128i int_values = _mm_cvttps_epi32(values);\n        __m128 float_int_values = _mm_cvtepi32_ps(int_values);\n        __m128 is_integer_mask = _mm_cmpeq_ps(values, float_int_values);\n        __m128 combined_mask = _mm_and_ps(positive_mask, is_integer_mask);\n\n        __m128i odd_mask = _mm_and_si128(int_values, _mm_set1_epi32(1));\n        __m128i is_odd = _mm_cmpeq_epi32(odd_mask, _mm_set1_epi32(1));\n\n        __m128i squared = _mm_mullo_epi32(int_values, int_values);\n        __m128i filtered = _mm_and_si128(squared, is_odd);\n\n        sum = _mm_add_epi32(sum, filtered);\n    }\n\n    alignas(16) int32_t temp[4];\n    _mm_store_si128((__m128i *)temp, sum);\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j];\n    }\n\n    for (; i < n; ++i) {\n        float value = lst[i];\n        if (value >= 0 && value == (int)value && ((int)value % 2 != 0)) {\n            result += (uint64_t)((int)value * (int)value);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n\n        __m128i diff_vec = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff_vec = _mm_abs_epi32(diff_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff_vec);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t simd_end = size - (size % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n\n        __m128i diff_vec = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff_vec = _mm_abs_epi32(diff_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff_vec);\n    }\n\n    for (size_t i = simd_end; i < size; i++) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t simd_size = size / 4 * 4; // Process in chunks of 4 (SSE processes 4 integers at a time)\n    for (size_t i = 0; i < simd_size; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);   // Load 4 integers from game\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]); // Load 4 integers from guess\n\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);        // Compute difference\n        __m128i abs_diff = _mm_abs_epi32(diff);                  // Compute absolute difference\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);         // Store result\n    }\n\n    // Handle remaining elements (if size is not a multiple of 4)\n    for (size_t i = simd_size; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    size_t simd_width = 4; // SSE processes 4 integers at a time\n    size_t simd_end = size - (size % simd_width);\n\n    for (size_t i = 0; i < simd_end; i += simd_width) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n\n        __m128i diff_vec = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff_vec = _mm_abs_epi32(diff_vec);\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff_vec);\n    }\n\n    for (size_t i = simd_end; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <vector>\n#include <emmintrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    size_t i = 0;\n    for (; i + 4 <= n; i += 4) {\n        __m128i game_vec = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i guess_vec = _mm_loadu_si128((__m128i*)&guess[i]);\n\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)(dst + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const*)(mask + i));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)(dst + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const*)(mask + i));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)(dst + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const*)(mask + i));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)(dst + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const*)(mask + i));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)(dst + i));\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const*)(mask + i));\n\n        __m256i result_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vseven = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vprod = _mm256_mullo_epi32(vsrc, vscale);\n\n        __m256i vtmp1 = _mm256_srai_epi32(vprod, 3);\n        __m256i vtmp2 = _mm256_slli_epi32(vprod, 3);\n\n        __m256i vtmp3 = _mm256_mullo_epi32(vtmp1, vtmp2);\n        __m256i vresult = _mm256_add_epi32(vprod, vtmp3);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vseven = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vprod = _mm256_mullo_epi32(vsrc, vscale);\n\n        __m256i vtmp1 = _mm256_srli_epi32(vprod, 3);\n        __m256i vtmp2 = _mm256_slli_epi32(vprod, 3);\n\n        __m256i vtmp3 = _mm256_mullo_epi32(vtmp1, vtmp2);\n        __m256i vresult = _mm256_add_epi32(vprod, vtmp3);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vseven = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vprod = _mm256_mullo_epi32(vsrc, vscale);\n\n        __m256i vtmp1 = _mm256_srli_epi32(vprod, 3);\n        __m256i vtmp2 = _mm256_slli_epi32(vprod, 3);\n\n        __m256i vtmp3 = _mm256_mullo_epi32(vtmp1, vtmp2);\n        __m256i vresult = _mm256_add_epi32(vprod, vtmp3);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vseven = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vprod = _mm256_mullo_epi32(vsrc, vscale);\n\n        __m256i vtmp1 = _mm256_srai_epi32(vprod, 3);\n        __m256i vtmp2 = _mm256_slli_epi32(vprod, 3);\n\n        __m256i vtmp3 = _mm256_mullo_epi32(vtmp1, vtmp2);\n        __m256i vresult = _mm256_add_epi32(vprod, vtmp3);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vseven = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vscale_mul = _mm256_mullo_epi32(vsrc, vscale);\n\n        __m256i vtmp1 = _mm256_srli_epi32(vscale_mul, 3);\n        __m256i vtmp2 = _mm256_slli_epi32(vscale_mul, 3);\n\n        __m256i vtmp1_mul_tmp2 = _mm256_mullo_epi32(vtmp1, vtmp2);\n        __m256i vresult = _mm256_add_epi32(vscale_mul, vtmp1_mul_tmp2);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i <= rows - 4; i += 4) {\n        __m256d vsrc = _mm256_set_pd(\n            src[(i + 3) * cols + stride],\n            src[(i + 2) * cols + stride],\n            src[(i + 1) * cols + stride],\n            src[i * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vsrc);\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d ymm0;\n\n    for (; i <= rows - 4; i += 4) {\n        ymm0 = _mm256_set_pd(\n            src[(i + 3) * cols + stride],\n            src[(i + 2) * cols + stride],\n            src[(i + 1) * cols + stride],\n            src[i * cols + stride]\n        );\n\n        _mm256_storeu_pd(&dst[i], ymm0);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i <= rows - 4; i += 4) {\n        __m256d vsrc = _mm256_set_pd(\n            src[(i+3) * cols + stride],\n            src[(i+2) * cols + stride],\n            src[(i+1) * cols + stride],\n            src[(i+0) * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vsrc);\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    for (; i <= rows - 4; i += 4) {\n        __m256d vsrc = _mm256_set_pd(\n            src[(i+3) * cols + stride],\n            src[(i+2) * cols + stride],\n            src[(i+1) * cols + stride],\n            src[i * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vsrc);\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d ymm0;\n\n    for (; i <= rows - 4; i += 4) {\n        ymm0 = _mm256_set_pd(\n            src[(i + 3) * cols + stride],\n            src[(i + 2) * cols + stride],\n            src[(i + 1) * cols + stride],\n            src[i * cols + stride]\n        );\n\n        _mm256_storeu_pd(&dst[i], ymm0);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 vals = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, vals);\n\n        __m256i idx_scatter = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 vals_scatter = _mm256_loadu_ps(src + i);\n        for (int j = 0; j < 8; j++) {\n            dst[idx_scatter[j]] = vals_scatter[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 vidx = _mm256_cvtepi32_ps(idx);\n\n        __m256 gathered = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, gathered);\n\n        __m256 scatter0 = _mm256_i32gather_ps(src + i, _mm256_castps_si256(vidx), sizeof(float));\n        __m256 scatter1 = _mm256_i32gather_ps(src + i + 8, _mm256_castps_si256(vidx), sizeof(float));\n\n        __m256 mask = _mm256_cmp_ps(vidx, _mm256_set1_ps(length), _CMP_LT_OQ);\n        _mm256_maskstore_ps(dst, mask, scatter0);\n        _mm256_maskstore_ps(dst + 8, mask, scatter1);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 val = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(dst + i, val);\n\n        __m256i idx_scatter = _mm256_loadu_si256((__m256i const*)(indices + i));\n        __m256 val_scatter = _mm256_loadu_ps(src + i);\n        for (int j = 0; j < 8; j++) {\n            dst[idx_scatter.m256i_i32[j]] = ((float*)&val_scatter)[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 vals = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(&dst[i], vals);\n\n        __m256 idx_f = _mm256_cvtepi32_ps(idx);\n        __m256 src_vals = _mm256_loadu_ps(&src[i]);\n        __m256i idx_mask = _mm256_cvttps_epi32(idx_f);\n        _mm256_maskstore_ps(&dst[i], idx_mask, src_vals);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 vidx = _mm256_cvtepi32_ps(idx);\n\n        __m256 gathered = _mm256_i32gather_ps(src, idx, sizeof(float));\n        _mm256_storeu_ps(&dst[i], gathered);\n\n        __m256i idx_scatter = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 vidx_scatter = _mm256_cvtepi32_ps(idx_scatter);\n        __m256 scattered = _mm256_i32gather_ps(src, idx_scatter, sizeof(float));\n\n        float temp[8];\n        _mm256_storeu_ps(temp, scattered);\n\n        for (int j = 0; j < 8; j++) {\n            dst[indices[i + j]] = temp[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    // Handle the first few elements if the start index is not aligned\n    if (start % 16 != 0) {\n        uint64_t alignment = (16 - (start % 16)) / 2;\n        for (i = 0; i < alignment && i < length / 2; ++i) {\n            int16_t temp = arr[start + i];\n            arr[start + i] = arr[end - i];\n            arr[end - i] = temp;\n        }\n        start += alignment;\n    }\n\n    // Process the main part of the array using AVX2 intrinsics\n    for (; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[start + i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[end - i - 15]);\n        v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        v1 = _mm256_permute4x64_epi64(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm256_storeu_si256((__m256i*)&arr[start + i], v2);\n        _mm256_storeu_si256((__m256i*)&arr[end - i - 15], v1);\n    }\n\n    // Handle the remaining elements\n    for (; i < length / 2; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    // Handle the first few elements if the start index is not aligned\n    if (start % 16 != 0) {\n        uint64_t misalign = 16 - (start % 16);\n        for (; i < misalign / 2 && i < length / 2; ++i) {\n            int16_t temp = arr[start + i];\n            arr[start + i] = arr[end - i];\n            arr[end - i] = temp;\n        }\n        start += misalign;\n    }\n\n    // Use AVX2 intrinsics to reverse the elements in chunks of 16\n    for (; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[start + i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[end - i - 15]);\n        v1 = _mm256_permute4x64_epi64(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm256_storeu_si256((__m256i*)&arr[start + i], v2);\n        _mm256_storeu_si256((__m256i*)&arr[end - i - 15], v1);\n    }\n\n    // Handle the remaining elements\n    for (; i < length / 2; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[start + i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[end - i - 15]);\n        v1 = _mm256_permute4x64_epi64(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm256_storeu_si256((__m256i*)&arr[start + i], v2);\n        _mm256_storeu_si256((__m256i*)&arr[end - i - 15], v1);\n    }\n\n    for (; i < length; i++) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    // Handle the first few elements if the start index is not aligned to 16 bytes\n    if (start % 8 != 0) {\n        uint64_t misalign = 8 - (start % 8);\n        for (i = 0; i < misalign && i < length / 2; i++) {\n            int16_t temp = arr[start + i];\n            arr[start + i] = arr[end - i];\n            arr[end - i] = temp;\n        }\n        start += misalign;\n    }\n\n    // Process 16 bytes (8 int16_t elements) at a time using AVX2 intrinsics\n    for (; i + 7 < length / 2; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[end - i - 7]);\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm_shufflehi_epi16(v2, _MM_SHUFFLE(2, 3, 0, 1));\n        v2 = _mm_shufflelo_epi16(v2, _MM_SHUFFLE(2, 3, 0, 1));\n        _mm_storeu_si128((__m128i*)&arr[start + i], v2);\n        _mm_storeu_si128((__m128i*)&arr[end - i - 7], v1);\n    }\n\n    // Handle the remaining elements\n    for (; i < length / 2; i++) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[start + i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[start + length - i - 16]);\n        v1 = _mm256_permute4x64_epi64(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm256_storeu_si256((__m256i*)&arr[start + i], v2);\n        _mm256_storeu_si256((__m256i*)&arr[start + length - i - 16], v1);\n    }\n\n    for (; i < length - i; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[start + length - i - 1];\n        arr[start + length - i - 1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * slice_size;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        size_t base_index = offset + i * dim2;\n        size_t j = 0;\n\n        for (; j <= dim2 - 32; j += 32) {\n            __m256i v = _mm256_loadu_si256((__m256i*)&tensor[base_index + j]);\n            _mm256_storeu_si256((__m256i*)&slice[i * dim2 + j], v);\n        }\n\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = tensor[base_index + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i, j;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 32) {\n            __m256i v = _mm256_loadu_si256((__m256i const*)(tensor_slice + i * dim2 + j));\n            _mm256_storeu_si256((__m256i*)(slice + i * dim2 + j), v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * slice_size;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 32) {\n            size_t index = offset + i * dim2 + j;\n            __m256i v = _mm256_loadu_si256((__m256i*)&tensor[index]);\n            _mm256_storeu_si256((__m256i*)&slice[i * dim2 + j], v);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * slice_size;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        size_t base_index = offset + i * dim2;\n        size_t j = 0;\n\n        for (; j <= dim2 - 32; j += 32) {\n            __m256i v = _mm256_loadu_si256((__m256i*)&tensor[base_index + j]);\n            _mm256_storeu_si256((__m256i*)&slice[i * dim2 + j], v);\n        }\n\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = tensor[base_index + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i;\n    for (i = 0; i + 31 < slice_size; i += 32) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&tensor_slice[i]);\n        _mm256_storeu_si256((__m256i*)&slice[i], v);\n    }\n\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor_slice[i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, ii, jj;\n    __m256i row0, row1, row2, row3, row4, row5, row6, row7;\n    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    row0 = _mm256_loadu_si256((__m256i const*)&src[(ii + 0) * cols + jj]);\n                    row1 = _mm256_loadu_si256((__m256i const*)&src[(ii + 1) * cols + jj]);\n                    row2 = _mm256_loadu_si256((__m256i const*)&src[(ii + 2) * cols + jj]);\n                    row3 = _mm256_loadu_si256((__m256i const*)&src[(ii + 3) * cols + jj]);\n                    row4 = _mm256_loadu_si256((__m256i const*)&src[(ii + 4) * cols + jj]);\n                    row5 = _mm256_loadu_si256((__m256i const*)&src[(ii + 5) * cols + jj]);\n                    row6 = _mm256_loadu_si256((__m256i const*)&src[(ii + 6) * cols + jj]);\n                    row7 = _mm256_loadu_si256((__m256i const*)&src[(ii + 7) * cols + jj]);\n\n                    tmp0 = _mm256_unpacklo_epi16(row0, row1);\n                    tmp1 = _mm256_unpackhi_epi16(row0, row1);\n                    tmp2 = _mm256_unpacklo_epi16(row2, row3);\n                    tmp3 = _mm256_unpackhi_epi16(row2, row3);\n                    tmp4 = _mm256_unpacklo_epi16(row4, row5);\n                    tmp5 = _mm256_unpackhi_epi16(row4, row5);\n                    tmp6 = _mm256_unpacklo_epi16(row6, row7);\n                    tmp7 = _mm256_unpackhi_epi16(row6, row7);\n\n                    row0 = _mm256_unpacklo_epi32(tmp0, tmp2);\n                    row1 = _mm256_unpackhi_epi32(tmp0, tmp2);\n                    row2 = _mm256_unpacklo_epi32(tmp1, tmp3);\n                    row3 = _mm256_unpackhi_epi32(tmp1, tmp3);\n                    row4 = _mm256_unpacklo_epi32(tmp4, tmp6);\n                    row5 = _mm256_unpackhi_epi32(tmp4, tmp6);\n                    row6 = _mm256_unpacklo_epi32(tmp5, tmp7);\n                    row7 = _mm256_unpackhi_epi32(tmp5, tmp7);\n\n                    tmp0 = _mm256_unpacklo_epi64(row0, row4);\n                    tmp1 = _mm256_unpackhi_epi64(row0, row4);\n                    tmp2 = _mm256_unpacklo_epi64(row1, row5);\n                    tmp3 = _mm256_unpackhi_epi64(row1, row5);\n                    tmp4 = _mm256_unpacklo_epi64(row2, row6);\n                    tmp5 = _mm256_unpackhi_epi64(row2, row6);\n                    tmp6 = _mm256_unpacklo_epi64(row3, row7);\n                    tmp7 = _mm256_unpackhi_epi64(row3, row7);\n\n                    row0 = _mm256_permute2x128_si256(tmp0, tmp4, 0x20);\n                    row1 = _mm256_permute2x128_si256(tmp1, tmp5, 0x20);\n                    row2 = _mm256_permute2x128_si256(tmp2, tmp6, 0x20);\n                    row3 = _mm256_permute2x128_si256(tmp3, tmp7, 0x20);\n                    row4 = _mm256_permute2x128_si256(tmp0, tmp4, 0x31);\n                    row5 = _mm256_permute2x128_si256(tmp1, tmp5, 0x31);\n                    row6 = _mm256_permute2x128_si256(tmp2, tmp6, 0x31);\n                    row7 = _mm256_permute2x128_si256(tmp3, tmp7, 0x31);\n\n                    for (k = 0; k < 8; ++k) {\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 0], _mm256_permute4x64_epi64(row0, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 8], _mm256_permute4x64_epi64(row1, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 16], _mm256_permute4x64_epi64(row2, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 24], _mm256_permute4x64_epi64(row3, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 32], _mm256_permute4x64_epi64(row4, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 40], _mm256_permute4x64_epi64(row5, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 48], _mm256_permute4x64_epi64(row6, 0x4E));\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii + 56], _mm256_permute4x64_epi64(row7, 0x4E));\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, r, c;\n    __m256i row0, row1, row2, row3, row4, row5, row6, row7;\n    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (r = i; r < i + block_size && r < rows; r += 8) {\n                for (c = j; c < j + block_size && c < cols; c += 8) {\n                    row0 = _mm256_loadu_si256((__m256i const*)&src[r * cols + c]);\n                    row1 = _mm256_loadu_si256((__m256i const*)&src[(r + 1) * cols + c]);\n                    row2 = _mm256_loadu_si256((__m256i const*)&src[(r + 2) * cols + c]);\n                    row3 = _mm256_loadu_si256((__m256i const*)&src[(r + 3) * cols + c]);\n                    row4 = _mm256_loadu_si256((__m256i const*)&src[(r + 4) * cols + c]);\n                    row5 = _mm256_loadu_si256((__m256i const*)&src[(r + 5) * cols + c]);\n                    row6 = _mm256_loadu_si256((__m256i const*)&src[(r + 6) * cols + c]);\n                    row7 = _mm256_loadu_si256((__m256i const*)&src[(r + 7) * cols + c]);\n\n                    tmp0 = _mm256_unpacklo_epi16(row0, row1);\n                    tmp1 = _mm256_unpackhi_epi16(row0, row1);\n                    tmp2 = _mm256_unpacklo_epi16(row2, row3);\n                    tmp3 = _mm256_unpackhi_epi16(row2, row3);\n                    tmp4 = _mm256_unpacklo_epi16(row4, row5);\n                    tmp5 = _mm256_unpackhi_epi16(row4, row5);\n                    tmp6 = _mm256_unpacklo_epi16(row6, row7);\n                    tmp7 = _mm256_unpackhi_epi16(row6, row7);\n\n                    row0 = _mm256_unpacklo_epi32(tmp0, tmp2);\n                    row1 = _mm256_unpackhi_epi32(tmp0, tmp2);\n                    row2 = _mm256_unpacklo_epi32(tmp1, tmp3);\n                    row3 = _mm256_unpackhi_epi32(tmp1, tmp3);\n                    row4 = _mm256_unpacklo_epi32(tmp4, tmp6);\n                    row5 = _mm256_unpackhi_epi32(tmp4, tmp6);\n                    row6 = _mm256_unpacklo_epi32(tmp5, tmp7);\n                    row7 = _mm256_unpackhi_epi32(tmp5, tmp7);\n\n                    tmp0 = _mm256_unpacklo_epi64(row0, row4);\n                    tmp1 = _mm256_unpackhi_epi64(row0, row4);\n                    tmp2 = _mm256_unpacklo_epi64(row1, row5);\n                    tmp3 = _mm256_unpackhi_epi64(row1, row5);\n                    tmp4 = _mm256_unpacklo_epi64(row2, row6);\n                    tmp5 = _mm256_unpackhi_epi64(row2, row6);\n                    tmp6 = _mm256_unpacklo_epi64(row3, row7);\n                    tmp7 = _mm256_unpackhi_epi64(row3, row7);\n\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r], tmp0);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 8], tmp1);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 16], tmp2);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 24], tmp3);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 32], tmp4);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 40], tmp5);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 48], tmp6);\n                    _mm256_storeu_si256((__m256i*)&dst[c * rows + r + 56], tmp7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, l;\n    __m256i row[8], tmp[8];\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; ++k) {\n                for (l = 0; l < block_size; l += 16) {\n                    row[0] = _mm256_loadu_si256((__m256i const*)&src[(i + k) * cols + j + l]);\n                    row[1] = _mm256_loadu_si256((__m256i const*)&src[(i + k) * cols + j + l + 8]);\n                    tmp[0] = _mm256_unpacklo_epi16(row[0], row[1]);\n                    tmp[1] = _mm256_unpackhi_epi16(row[0], row[1]);\n                    tmp[4] = _mm256_permute2x128_si256(tmp[0], tmp[1], 0x20);\n                    tmp[5] = _mm256_permute2x128_si256(tmp[0], tmp[1], 0x31);\n                    tmp[0] = _mm256_permute2x128_si256(tmp[0], tmp[1], 0x02);\n                    tmp[1] = _mm256_permute2x128_si256(tmp[0], tmp[1], 0x13);\n                    row[0] = _mm256_unpacklo_epi32(tmp[0], tmp[4]);\n                    row[1] = _mm256_unpackhi_epi32(tmp[0], tmp[4]);\n                    row[2] = _mm256_unpacklo_epi32(tmp[1], tmp[5]);\n                    row[3] = _mm256_unpackhi_epi32(tmp[1], tmp[5]);\n                    row[4] = _mm256_unpacklo_epi64(row[0], row[2]);\n                    row[5] = _mm256_unpackhi_epi64(row[0], row[2]);\n                    row[6] = _mm256_unpacklo_epi64(row[1], row[3]);\n                    row[7] = _mm256_unpackhi_epi64(row[1], row[3]);\n\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k) * rows + i + l], row[0]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k) * rows + i + l + 8], row[4]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 1) * rows + i + l], row[1]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 1) * rows + i + l + 8], row[5]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 2) * rows + i + l], row[2]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 2) * rows + i + l + 8], row[6]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 3) * rows + i + l], row[3]);\n                    _mm256_storeu_si256((__m256i*)&dst[(j + k + 3) * rows + i + l + 8], row[7]);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    __m256i temp[8];\n    size_t i, j, r, c;\n\n    for (r = 0; r < rows; r += block_size) {\n        for (c = 0; c < cols; c += block_size) {\n            for (i = 0; i < block_size; ++i) {\n                for (j = 0; j < block_size; j += 16) {\n                    size_t idx = r + i;\n                    size_t idx2 = c + j;\n                    if (idx < rows && idx2 < cols) {\n                        temp[j / 16] = _mm256_loadu_si256((__m256i const*)(&src[idx * cols + idx2]));\n                    }\n                }\n                for (j = 0; j < block_size; j += 16) {\n                    size_t idx = r + i;\n                    size_t idx2 = c + j;\n                    if (idx < rows && idx2 < cols) {\n                        _mm256_storeu_si256((__m256i*)(&dst[idx2 * rows + idx]), temp[j / 16]);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, ii, jj;\n    __m256i temp[8];\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    for (size_t k = 0; k < 8; ++k) {\n                        temp[k] = _mm256_setzero_si256();\n                    }\n\n                    for (size_t k = 0; k < 8 && (ii + k < rows); ++k) {\n                        __m256i row = _mm256_loadu_si256((__m256i*)&src[(ii + k) * cols + jj]);\n                        for (size_t l = 0; l < 8 && (jj + l < cols); ++l) {\n                            temp[l] = _mm256_insert_epi16(temp[l], _mm256_extract_epi16(row, l), k);\n                        }\n                    }\n\n                    for (size_t k = 0; k < 8 && (jj + k < cols); ++k) {\n                        _mm256_storeu_si256((__m256i*)&dst[(jj + k) * rows + ii], temp[k]);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    for (size_t i = 0; i < dim; i += 8) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp = _mm256_loadu_ps(&array[index]);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    return result[0] + result[1] + result[2] + result[3] + result[4] + result[5] + result[6] + result[7];\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    for (size_t i = 0; i < dim; i += 8) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp = _mm256_loadu_ps(&array[index]);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    return result[0] + result[1] + result[2] + result[3] + result[4] + result[5] + result[6] + result[7];\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    for (size_t i = 0; i < dim; i += 8) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp = _mm256_loadu_ps(&array[index]);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    return result[0] + result[1] + result[2] + result[3] + result[4] + result[5] + result[6] + result[7];\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    for (size_t i = 0; i < dim; i += 8) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp = _mm256_loadu_ps(&array[index]);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    return result[0] + result[1] + result[2] + result[3] + result[4] + result[5] + result[6] + result[7];\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    for (size_t i = 0; i < dim; i += 8) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp = _mm256_loadu_ps(&array[index]);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n\n    return result[0] + result[1] + result[2] + result[3] + result[4] + result[5] + result[6] + result[7];\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vsrc = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d vmask = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OS);\n            __m256d vscaled = _mm256_mul_pd(vsrc, vscale);\n            __m256d vresult = _mm256_blendv_pd(vsrc, vscaled, vmask);\n            _mm256_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vsrc = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d vmask = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OQ);\n            __m256d vscaled = _mm256_mul_pd(vsrc, vscale);\n            __m256d vresult = _mm256_blendv_pd(vsrc, vscaled, vmask);\n            _mm256_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vsrc = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d vmask = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OQ);\n            __m256d vscaled = _mm256_mul_pd(vsrc, vscale);\n            __m256d vresult = _mm256_blendv_pd(vsrc, vscaled, vmask);\n            _mm256_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vsrc = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d vmask = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OQ);\n            __m256d vscaled = _mm256_mul_pd(vsrc, vscale);\n            __m256d vresult = _mm256_blendv_pd(vsrc, vscaled, vmask);\n            _mm256_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n    size_t totalElements = rows * cols;\n\n    for (size_t i = 0; i < totalElements; i += 4) {\n        __m256d vsrc = _mm256_loadu_pd(&src[i]);\n        __m256d vmask = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OS);\n        __m256d vscaled = _mm256_mul_pd(vsrc, vscale);\n        __m256d vresult = _mm256_blendv_pd(vsrc, vscaled, vmask);\n        _mm256_storeu_pd(&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_aligned = cols & ~3; // Ensure cols_aligned is a multiple of 4\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i] * cols;\n        size_t dst_row_index = i * cols;\n        size_t j = 0;\n\n        for (; j < cols_aligned; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row_index + j]);\n            _mm256_storeu_pd(&dst[dst_row_index + j], src_vec);\n        }\n\n        for (; j < cols; ++j) {\n            dst[dst_row_index + j] = src[src_row_index + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_aligned = cols & ~0x3; // Ensure cols is a multiple of 4 for AVX2\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n\n        for (size_t j = 0; j < cols_aligned; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row + j]);\n            _mm256_storeu_pd(&dst[dst_row + j], src_vec);\n        }\n\n        for (size_t j = cols_aligned; j < cols; ++j) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_aligned = cols & ~0x3; // Ensure cols is a multiple of 4 for AVX2\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i] * cols;\n        size_t dst_row_index = i * cols;\n\n        for (size_t j = 0; j < cols_aligned; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row_index + j]);\n            _mm256_storeu_pd(&dst[dst_row_index + j], src_vec);\n        }\n\n        for (size_t j = cols_aligned; j < cols; ++j) {\n            dst[dst_row_index + j] = src[src_row_index + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_aligned = cols & ~0x3; // cols aligned to nearest lower multiple of 4\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_index = indices[i] * cols;\n        size_t dst_row_index = i * cols;\n\n        size_t j = 0;\n        for (; j < cols_aligned; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row_index + j]);\n            _mm256_storeu_pd(&dst[dst_row_index + j], src_vec);\n        }\n\n        for (; j < cols; ++j) {\n            dst[dst_row_index + j] = src[src_row_index + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t cols_aligned = cols & ~0x3; // Make cols a multiple of 4\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        size_t j = 0;\n\n        for (; j < cols_aligned; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[src_row + j]);\n            _mm256_storeu_pd(&dst[dst_row + j], src_vec);\n        }\n\n        for (; j < cols; ++j) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i half_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n        __m256i result = _mm256_srli_epi64(_mm256_add_epi64(sum, half_vec), 1);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i vadd = _mm256_add_epi64(vsrc1, vsrc2);\n        vadd = _mm256_add_epi64(vadd, vscalar);\n        __m256i vresult = _mm256_div_epi64(vadd, vtwo);\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n    __m256i half_vec = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n\n        __m256i sum = _mm256_add_epi64(vec1, vec2);\n        sum = _mm256_add_epi64(sum, scalar_vec);\n\n        __m256i result = _mm256_srli_epi64(_mm256_add_epi64(sum, half_vec), 1);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n\n        __m256i vadd = _mm256_add_epi64(vsrc1, vsrc2);\n        vadd = _mm256_add_epi64(vadd, vscalar);\n\n        __m256i vresult = _mm256_div_epi64(vadd, vtwo);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i vadd = _mm256_add_epi64(vsrc1, vsrc2);\n        vadd = _mm256_add_epi64(vadd, vscalar);\n        __m256i vresult = _mm256_srli_epi64(_mm256_add_epi64(vadd, vtwo), 1);\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i even_mask = _mm256_set1_epi32(0x1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_vec = _mm256_andnot_si256(even_mask, vec);\n        min_vec = _mm256_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[8];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (int i = 0; i < 8; ++i) {\n        if (min_vals[i] < min_value) {\n            min_value = min_vals[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i even_mask = _mm256_set1_epi32(0x1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_vec = _mm256_andnot_si256(even_mask, vec);\n        min_vec = _mm256_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[8];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (int i = 0; i < 8; ++i) {\n        if (min_vals[i] < min_value) {\n            min_value = min_vals[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i even_mask = _mm256_set1_epi32(0x1);\n\n    for (size_t i = 0; i < length - 7; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i even_vec = _mm256_andnot_si256(even_mask, vec);\n        min_vec = _mm256_min_epi32(min_vec, even_vec);\n    }\n\n    int min_values[8];\n    _mm256_storeu_si256((__m256i*)min_values, min_vec);\n\n    int min_even = INT_MAX;\n    for (size_t i = 0; i < 8; ++i) {\n        if (min_values[i] < min_even) {\n            min_even = min_values[i];\n        }\n    }\n\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_even) {\n            min_even = src[i];\n        }\n    }\n\n    return min_even == INT_MAX ? -1 : min_even;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    __m256i even_mask = _mm256_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_chunk = _mm256_and_si256(chunk, even_mask);\n        __m256i cmp = _mm256_cmpgt_epi32(even_chunk, _mm256_setzero_si256());\n        even_chunk = _mm256_and_si256(even_chunk, cmp);\n        min_val = _mm256_min_epi32(min_val, even_chunk);\n    }\n\n    int min_vals[8];\n    _mm256_storeu_si256((__m256i*)min_vals, min_val);\n\n    int min = INT_MAX;\n    for (int i = 0; i < 8; ++i) {\n        if (min_vals[i] < min && min_vals[i] != 0) {\n            min = min_vals[i];\n        }\n    }\n\n    return min == INT_MAX ? -1 : min;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i even_mask = _mm256_set1_epi32(0x1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_vec = _mm256_andnot_si256(even_mask, vec);\n        min_vec = _mm256_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[8];\n    _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (int i = 0; i < 8; ++i) {\n        if (min_vals[i] < min_value && (min_vals[i] & 0x1) == 0) {\n            min_value = min_vals[i];\n        }\n    }\n\n    return min_value == INT_MAX ? -1 : min_value;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vector = _mm256_set1_epi32(shift);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vector = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        __m256i arith_shift_vector = _mm256_srai_epi32(src_vector, shift_vector);\n        __m256i logical_shift_vector = _mm256_srli_epi32(_mm256_xor_si256(src_vector, _mm256_set1_epi32(0x80000000)), shift_vector);\n\n        __m256i result_vector = _mm256_add_epi32(arith_shift_vector, logical_shift_vector);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vector);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i varith_shift = _mm256_srai_epi32(vsrc, vshift);\n        __m256i vlogic_shift = _mm256_srli_epi32(_mm256_xor_si256(vsrc, _mm256_set1_epi32(0x80000000)), vshift);\n        __m256i vresult = _mm256_add_epi32(varith_shift, vlogic_shift);\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        __m256i arith_shift_vec = _mm256_sra_epi32(src_vec, shift_vec);\n        __m256i logical_shift_vec = _mm256_srl_epi32(_mm256_xor_si256(src_vec, _mm256_set1_epi32(0x80000000)), shift_vec);\n\n        __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        int src_val = src[i];\n        int arith_shift_val = src_val >> shift;\n        int logical_shift_val = (src_val ^ 0x80000000) >> shift;\n\n        dst[i] = arith_shift_val + logical_shift_val;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vector = _mm256_set1_epi32(shift);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vector = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        __m256i arith_shift_result = _mm256_srai_epi32(src_vector, shift_vector);\n        __m256i logical_shift_result = _mm256_srli_epi32(_mm256_castsi256_si256(src_vector), shift_vector);\n\n        __m256i result = _mm256_add_epi32(arith_shift_result, logical_shift_result);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        int arith_shift_result = src[i] >> shift;\n        int logical_shift_result = (unsigned int)src[i] >> shift;\n        dst[i] = arith_shift_result + logical_shift_result;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m256i shift_vec = _mm256_set1_epi32(shift);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        __m256i arith_shift_vec = _mm256_srai_epi32(src_vec, shift_vec);\n\n        __m256i logical_shift_vec = _mm256_srli_epi32(_mm256_xor_si256(src_vec, zero_vec), shift_vec);\n\n        __m256i result_vec = _mm256_add_epi32(arith_shift_vec, logical_shift_vec);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size - 7; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    int64_t sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (size_t i = size - (size % 8); i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t sums[8];\n    _mm256_storeu_si256((__m256i*)sums, sum_vec);\n    int64_t sum = sums[0] + sums[1] + sums[2] + sums[3] + sums[4] + sums[5] + sums[6] + sums[7];\n\n    for (; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size - 7; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    int64_t sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (size_t i = size - (size % 8); i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n    const int *end = matrix + size;\n\n    for (const int *ptr = matrix; ptr + 7 < end; ptr += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)ptr);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t sums[8];\n    _mm256_storeu_si256((__m256i*)sums, sum_vec);\n    int64_t sum = sums[0] + sums[1] + sums[2] + sums[3] + sums[4] + sums[5] + sums[6] + sums[7];\n\n    for (const int *ptr = matrix + (size & ~7); ptr < end; ++ptr) {\n        sum += *ptr;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size - 7; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    int64_t sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (size_t i = size - (size % 8); i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    size_t i;\n\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_cmpgt_epi8(chunk, max_val);\n\n        if (_mm256_movemask_epi8(mask) != 0) {\n            max_val = chunk;\n            max_idx = i + _mm_tzcnt_32(_mm256_movemask_epi8(mask));\n        }\n    }\n\n    int8_t max_val_scalar = INT8_MIN;\n    for (; i < length; ++i) {\n        if (src[i] > max_val_scalar) {\n            max_val_scalar = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m256i index = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length; i += 32) {\n        size_t remaining = length - i;\n        if (remaining < 32) {\n            __m256i perm_mask = _mm256_loadu_si256((__m256i const *)(src + i));\n            perm_mask = _mm256_permutevar8x32_epi32(_mm256_set1_epi32(INT8_MIN), _mm256_set1_epi32(remaining));\n            __m256i chunk = _mm256_maskload_epi32((int const *)(src + i), perm_mask);\n\n            __m256i cmp = _mm256_cmpgt_epi8(chunk, max_val);\n            uint32_t mask = _mm256_movemask_epi8(cmp);\n\n            while (mask) {\n                int bit = __builtin_ctz(mask);\n                max_val = chunk;\n                max_idx = i + bit;\n                mask &= ~(1 << bit);\n            }\n\n            break;\n        }\n\n        __m256i chunk = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(chunk, max_val);\n        uint32_t mask = _mm256_movemask_epi8(cmp);\n\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            max_val = chunk;\n            max_idx = i + bit;\n            mask &= ~(1 << bit);\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m256i index = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                      16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length - 31; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cmp = _mm256_cmpgt_epi8(chunk, max_val);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            max_val = chunk;\n            max_idx = i + bit;\n            mask &= ~(1 << bit);\n        }\n    }\n\n    for (size_t i = (length & ~31); i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m256i idx = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                   16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_cmpgt_epi8(chunk, max_val);\n\n        if (_mm256_movemask_epi8(mask) != 0) {\n            max_val = _mm256_blendv_epi8(max_val, chunk, mask);\n            idx = _mm256_add_epi8(idx, _mm256_set1_epi8(32));\n            max_idx = i + _mm256_extract_epi8(idx, _mm256_movemask_epi8(mask));\n        }\n\n        idx = _mm256_add_epi8(idx, _mm256_set1_epi8(32));\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_val = _mm256_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    size_t i;\n\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cmp = _mm256_cmpgt_epi8(values, max_val);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            max_val = values;\n            max_idx = i + bit;\n            mask &= ~(1 << bit);\n        }\n    }\n\n    int8_t max_val_scalar = _mm256_extract_epi8(max_val, 0);\n    for (; i < length; ++i) {\n        if (src[i] > max_val_scalar) {\n            max_val_scalar = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    __m256i index_vec = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i index_add = _mm256_add_epi32(index_vec, _mm256_set1_epi32(i));\n        __m256i blend_mask = _mm256_and_si256(index_add, mask_vec);\n        __m256i result_vec = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i mask_vector = _mm256_set1_epi32(mask);\n    __m256i index_vector = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i src1_vector = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vector = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i index_add = _mm256_add_epi32(index_vector, _mm256_set1_epi32(i));\n        __m256i condition = _mm256_and_si256(index_add, mask_vector);\n        __m256i blend_mask = _mm256_cmpeq_epi32(condition, _mm256_setzero_si256());\n        __m256i result = _mm256_blendv_epi8(src2_vector, src1_vector, blend_mask);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    __m256i index_vec = _mm256_set_epi32(15, 14, 13, 12, 11, 10, 9, 8);\n\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i index_add_vec = _mm256_add_epi32(index_vec, _mm256_set1_epi32(i));\n        __m256i blend_mask = _mm256_and_si256(index_add_vec, mask_vec);\n        __m256i result_vec = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n        index_vec = _mm256_add_epi32(index_vec, _mm256_set1_epi32(16));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i index = _mm256_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n        __m256i mask_vector = _mm256_set1_epi32(mask);\n        __m256i index_masked = _mm256_and_si256(_mm256_add_epi16(index, _mm256_set1_epi16(i)), mask_vector);\n        __m256i src1_vals = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vals = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i result = _mm256_blendv_epi8(src2_vals, src1_vals, _mm256_cmpeq_epi16(index_masked, _mm256_setzero_si256()));\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    __m256i index_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i index_add = _mm256_add_epi32(index_vec, _mm256_set1_epi32(i));\n        __m256i blend_mask = _mm256_and_si256(index_add, mask_vec);\n        __m256i result_vec = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m256i v, v2, v3, v4, tmp1, tmp2, tmp3, tmp4, tmp5;\n    const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mask4 = _mm256_set1_epi32(0x01010101);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        v2 = _mm256_srli_epi32(v, 1);\n        v2 = _mm256_and_si256(v2, mask1);\n        v = _mm256_sub_epi32(v, v2);\n\n        v3 = _mm256_srli_epi32(v, 2);\n        v3 = _mm256_and_si256(v3, mask2);\n        v = _mm256_add_epi32(v, v3);\n\n        v = _mm256_and_si256(v, mask3);\n        tmp1 = _mm256_srli_epi32(v, 4);\n        tmp2 = _mm256_srli_epi32(v, 8);\n        tmp3 = _mm256_srli_epi32(v, 16);\n        tmp4 = _mm256_srli_epi32(v, 24);\n\n        tmp1 = _mm256_add_epi32(tmp1, tmp2);\n        tmp3 = _mm256_add_epi32(tmp3, tmp4);\n        tmp1 = _mm256_add_epi32(tmp1, tmp3);\n\n        tmp5 = _mm256_srli_epi32(tmp1, 8);\n        tmp1 = _mm256_add_epi32(tmp1, tmp5);\n\n        tmp5 = _mm256_srli_epi32(tmp1, 16);\n        tmp1 = _mm256_add_epi32(tmp1, tmp5);\n\n        tmp5 = _mm256_srli_epi32(tmp1, 24);\n        tmp1 = _mm256_add_epi32(tmp1, tmp5);\n\n        tmp1 = _mm256_and_si256(tmp1, mask4);\n        tmp1 = _mm256_sad_epu8(tmp1, _mm256_setzero_si256());\n\n        _mm256_storeu_si256((__m256i*)&dst[i], tmp1);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m256i v, v1, v2, v3, v4, tmp1, tmp2, tmp3;\n    const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mask4 = _mm256_set1_epi32(0x01010101);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        v1 = _mm256_and_si256(v, mask1);\n        v2 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1);\n        v = _mm256_add_epi32(v1, v2);\n\n        v1 = _mm256_and_si256(v, mask2);\n        v2 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask2);\n        v = _mm256_add_epi32(v1, v2);\n\n        v = _mm256_add_epi32(_mm256_and_si256(v, mask3), _mm256_and_si256(_mm256_srli_epi32(v, 4), mask3));\n\n        v = _mm256_add_epi32(_mm256_and_si256(v, mask4), _mm256_and_si256(_mm256_srli_epi32(v, 24), mask4));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        val = (val & 0x55555555) + ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val & 0x0F0F0F0F) + ((val >> 4) & 0x0F0F0F0F);\n        val = (val & 0x01010101) + ((val >> 24) & 0x01010101);\n        dst[i] = val;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m256i v, v2, v3, v4, tmp1, tmp2, tmp3, tmp4;\n    const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mask4 = _mm256_set1_epi32(0x01010101);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        v2 = _mm256_and_si256(v, mask1);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1);\n        v2 = _mm256_add_epi32(v2, v3);\n\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask1);\n        v2 = _mm256_add_epi32(v2, v3);\n\n        v2 = _mm256_and_si256(v2, mask2);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v2, 4), mask2);\n        v2 = _mm256_add_epi32(v2, v3);\n\n        v2 = _mm256_and_si256(v2, mask3);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v2, 8), mask3);\n        v2 = _mm256_add_epi32(v2, v3);\n\n        v2 = _mm256_and_si256(v2, mask4);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v2, 16), mask4);\n        v2 = _mm256_add_epi32(v2, v3);\n\n        v2 = _mm256_and_si256(v2, _mm256_set1_epi32(0x0000003F));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v2);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m256i v, v2, v3, v4, v5, tmp1, tmp2, tmp3;\n    const __m256i mask1 = _mm256_set1_epi32(0x55555555);\n    const __m256i mask2 = _mm256_set1_epi32(0x33333333);\n    const __m256i mask3 = _mm256_set1_epi32(0x0F0F0F0F);\n    const __m256i mask4 = _mm256_set1_epi32(0x00FF00FF);\n    const __m256i mask5 = _mm256_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v = _mm256_loadu_si256((__m256i const*)(src + i));\n        v2 = _mm256_add_epi32(_mm256_and_si256(v, mask1), _mm256_and_si256(_mm256_srli_epi32(v, 1), mask1));\n        v3 = _mm256_add_epi32(_mm256_and_si256(v2, mask2), _mm256_and_si256(_mm256_srli_epi32(v2, 2), mask2));\n        tmp1 = _mm256_and_si256(v3, mask3);\n        tmp2 = _mm256_and_si256(_mm256_srli_epi32(v3, 4), mask3);\n        v4 = _mm256_add_epi32(tmp1, tmp2);\n        tmp1 = _mm256_and_si256(v4, mask4);\n        tmp2 = _mm256_and_si256(_mm256_srli_epi32(v4, 8), mask4);\n        tmp3 = _mm256_add_epi32(tmp1, tmp2);\n        v5 = _mm256_add_epi32(_mm256_and_si256(tmp3, mask5), _mm256_and_si256(_mm256_srli_epi32(tmp3, 16), mask5));\n        _mm256_storeu_si256((__m256i*)(dst + i), v5);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t v = src[i];\n        v = (v & mask1) + ((v >> 1) & mask1);\n        v = (v & mask2) + ((v >> 2) & mask2);\n        v = (v + (v >> 4)) & mask3;\n        v = (v + (v >> 8)) & mask4;\n        v = (v + (v >> 16)) & mask5;\n        dst[i] = v;\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m256i v, v2, v3, v4, tmp1, tmp2, tmp3, tmp4, mask;\n    const __m256i lut = _mm256_setr_epi8(0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4);\n    mask = _mm256_set1_epi32(0x0f0f0f0f);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v = _mm256_loadu_si256((__m256i*)&src[i]);\n        v2 = _mm256_srli_epi32(v, 4);\n        v = _mm256_and_si256(v, mask);\n        v2 = _mm256_and_si256(v2, mask);\n        v = _mm256_add_epi8(v, v2);\n\n        v3 = _mm256_srli_epi16(v, 8);\n        v = _mm256_and_si256(v, mask);\n        v3 = _mm256_and_si256(v3, mask);\n        v = _mm256_add_epi8(v, v3);\n\n        tmp1 = _mm256_srli_epi16(v, 4);\n        tmp2 = _mm256_srli_epi16(v, 8);\n        tmp3 = _mm256_srli_epi16(v, 12);\n        tmp1 = _mm256_and_si256(tmp1, mask);\n        tmp2 = _mm256_and_si256(tmp2, mask);\n        tmp3 = _mm256_and_si256(tmp3, mask);\n        tmp4 = _mm256_and_si256(v, mask);\n\n        v = _mm256_shuffle_epi8(lut, tmp1);\n        v2 = _mm256_shuffle_epi8(lut, tmp2);\n        v3 = _mm256_shuffle_epi8(lut, tmp3);\n        v4 = _mm256_shuffle_epi8(lut, tmp4);\n\n        v = _mm256_add_epi8(v, v2);\n        v = _mm256_add_epi8(v, v3);\n        v = _mm256_add_epi8(v, v4);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i sum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i sum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        dst[i] = (result < src1[i]) ? UINT16_MAX : result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i sum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), sum);\n    }\n\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i sum = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256((__m256i*)(dst + i), sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        dst[i] = (result < src1[i]) ? UINT16_MAX : result;\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d c = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_cmp_pd(a, _mm256_set1_pd(100.0), _CMP_LT_OS);\n                mask_a = _mm256_and_pd(mask_a, _mm256_cmp_pd(a, _mm256_set1_pd(-100.0), _CMP_GT_OS));\n\n                __m256d mask_b = _mm256_cmp_pd(b, _mm256_set1_pd(100.0), _CMP_LT_OS);\n                mask_b = _mm256_and_pd(mask_b, _mm256_cmp_pd(b, _mm256_set1_pd(-100.0), _CMP_GT_OS));\n\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                a = _mm256_and_pd(a, mask);\n                b = _mm256_and_pd(b, mask);\n\n                c = _mm256_add_pd(c, _mm256_mul_pd(a, b));\n            }\n            double result[4];\n            _mm256_storeu_pd(result, c);\n            C[i * p + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d minVal = _mm256_set1_pd(-100.0);\n    __m256d maxVal = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d aVal = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d bVal = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d maskA = _mm256_and_pd(_mm256_cmp_pd(aVal, minVal, _CMP_GE_OQ), _mm256_cmp_pd(aVal, maxVal, _CMP_LE_OQ));\n                __m256d maskB = _mm256_and_pd(_mm256_cmp_pd(bVal, minVal, _CMP_GE_OQ), _mm256_cmp_pd(bVal, maxVal, _CMP_LE_OQ));\n\n                __m256d mask = _mm256_and_pd(maskA, maskB);\n                __m256d prod = _mm256_mul_pd(aVal, bVal);\n                prod = _mm256_blendv_pd(zero, prod, mask);\n\n                sum = _mm256_add_pd(sum, prod);\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d min_range = _mm256_set1_pd(-100.0);\n    __m256d max_range = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_val = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_val = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a_val, min_range, _CMP_GE_OQ), _mm256_cmp_pd(a_val, max_range, _CMP_LE_OQ));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b_val, min_range, _CMP_GE_OQ), _mm256_cmp_pd(b_val, max_range, _CMP_LE_OQ));\n\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                __m256d prod = _mm256_mul_pd(a_val, b_val);\n                prod = _mm256_and_pd(prod, mask);\n\n                sum = _mm256_add_pd(sum, prod);\n            }\n\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d min = _mm256_set1_pd(-100.0);\n    __m256d max = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a, min, _CMP_GE_OQ), _mm256_cmp_pd(a, max, _CMP_LE_OQ));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b, min, _CMP_GE_OQ), _mm256_cmp_pd(b, max, _CMP_LE_OQ));\n\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d min = _mm256_set1_pd(-100.0);\n    __m256d max = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a, min, _CMP_GE_OQ), _mm256_cmp_pd(a, max, _CMP_LE_OQ));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b, min, _CMP_GE_OQ), _mm256_cmp_pd(b, max, _CMP_LE_OQ));\n\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                __m256d prod = _mm256_mul_pd(a, b);\n                prod = _mm256_and_pd(prod, mask);\n\n                sum = _mm256_add_pd(sum, prod);\n            }\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vc = _mm256_add_epi64(va, vb);\n        _mm256_storeu_si256((__m256i*)(C + i), vc);\n    }\n\n    for (; i < total_size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < total_size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i c = _mm256_add_epi64(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), c);\n    }\n\n    for (; i < total_size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vc = _mm256_add_epi64(va, vb);\n        _mm256_storeu_si256((__m256i*)(C + i), vc);\n    }\n\n    for (; i < total_size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_size - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_add_epi64(va, vb);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n\n    for (; i < total_size; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        result = nearbyintf(result);\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = roundf(sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    __m256 diff, abs_diff, mask1, mask2, result, rounded;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n\n    for (i = 0; i < length - 7; i += 8) {\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n\n        diff = _mm256_sub_ps(src1_vec, src2_vec);\n        abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        mask1 = _mm256_cmp_ps(abs_diff, threshold, _CMP_LT_OQ);\n\n        __m256 index_mask = _mm256_set_ps(\n            0.0f, 2.0f, 0.0f, 2.0f, 0.0f, 2.0f, 0.0f, 2.0f);\n        mask2 = _mm256_cmp_ps(index_mask, _mm256_setzero_ps(), _CMP_EQ_OQ);\n\n        mask1 = _mm256_and_ps(mask1, mask2);\n\n        result = _mm256_mul_ps(src1_vec, src2_vec);\n        result = _mm256_blendv_ps(negative_one, result, mask1);\n\n        rounded = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC);\n\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        if (fabs(src1[i] - src2[i]) < 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    __m256 absmask = _mm256_set1_ps(0x7FFFFFFF);\n    __m256 fifty = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v2 = _mm256_loadu_ps(&src2[i]);\n        __m256 absdiff = _mm256_and_ps(_mm256_sub_ps(v1, v2), absmask);\n        __m256 cmp = _mm256_cmp_ps(absdiff, fifty, _CMP_LT_OQ);\n\n        __m256 mul_result = _mm256_mul_ps(v1, v2);\n        __m256 round_result = _mm256_ceil_ps(mul_result);\n\n        __m256 even_mask = _mm256_set_ps(\n            (i + 7) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 6) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 5) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 4) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 3) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 2) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 1) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 0) % 2 == 0 ? 1.0f : 0.0f\n        );\n\n        __m256 mask = _mm256_and_ps(cmp, even_mask);\n        __m256 result = _mm256_blendv_ps(negative_one, round_result, mask);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabsf(src1[i] - src2[i]) < 50.0f)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    __m256 diff, abs_diff, mask1, mask2, result, rounded;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n\n        diff = _mm256_sub_ps(src1_vec, src2_vec);\n        abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n        mask1 = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        __m256 index_vec = _mm256_setr_ps(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256 index_mod = _mm256_and_ps(index_vec, _mm256_set1_ps(2.0f));\n        mask2 = _mm256_cmp_ps(index_mod, _mm256_set1_ps(0.0f), _CMP_EQ_OQ);\n\n        mask1 = _mm256_and_ps(mask1, mask2);\n        result = _mm256_mul_ps(src1_vec, src2_vec);\n        result = _mm256_and_ps(mask1, result);\n\n        rounded = _mm256_ceil_ps(result);\n        result = _mm256_blendv_ps(negative_one, rounded, mask1);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabs(src1[i] - src2[i]) <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    __m256 abs_diff_limit = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n        __m256 abs_diff = _mm256_abs_ps(_mm256_sub_ps(src1_vec, src2_vec));\n        __m256 mask1 = _mm256_cmp_ps(abs_diff, abs_diff_limit, _CMP_LE_OQ);\n\n        __m256 mul_result = _mm256_mul_ps(src1_vec, src2_vec);\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n\n        __m256 mask2 = _mm256_set_ps(\n            (i + 7) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 6) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 5) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 4) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 3) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 2) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 1) % 2 == 0 ? 0xFFFFFFFF : 0,\n            (i + 0) % 2 == 0 ? 0xFFFFFFFF : 0\n        );\n\n        __m256 combined_mask = _mm256_and_ps(mask1, mask2);\n        __m256 result = _mm256_blendv_ps(negative_one, rounded_result, combined_mask);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabs(src1[i] - src2[i]) <= 50.0f && i % 2 == 0) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    __m256 diff, abs_diff, mask1, mask2, result, rounded;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 negative_one = _mm256_set1_ps(-1.0f);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n\n        diff = _mm256_sub_ps(src1_vec, src2_vec);\n        abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        mask1 = _mm256_cmp_ps(abs_diff, threshold, _CMP_LT_OQ);\n\n        __m256 idx_mask = _mm256_set_ps(\n            0.0f, 2.0f, 0.0f, 2.0f, 0.0f, 2.0f, 0.0f, 2.0f\n        );\n        __m256 idx_vec = _mm256_set_ps(\n            (float)(i+7), (float)(i+6), (float)(i+5), (float)(i+4),\n            (float)(i+3), (float)(i+2), (float)(i+1), (float)(i)\n        );\n        mask2 = _mm256_cmp_ps(idx_vec, idx_mask, _CMP_EQ_OQ);\n\n        mask1 = _mm256_and_ps(mask1, mask2);\n\n        result = _mm256_mul_ps(src1_vec, src2_vec);\n        result = _mm256_and_ps(result, mask1);\n\n        rounded = _mm256_round_ps(result, _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC);\n\n        result = _mm256_blendv_ps(negative_one, rounded, mask1);\n\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && fabsf(src1[i] - src2[i]) < 50.0f) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        for (; j <= n - 4; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        for (; j <= n - 4; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + j]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        __m256 row = _mm256_loadu_ps(&mat[i * n]);\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 chunk = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(chunk, chunk, sum);\n        }\n\n        float row_sum[8];\n        _mm256_storeu_ps(row_sum, sum);\n        float norm = sqrtf(row_sum[0] + row_sum[1] + row_sum[2] + row_sum[3] + row_sum[4] + row_sum[5] + row_sum[6] + row_sum[7]);\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 chunk = _mm256_loadu_ps(&mat[i * n + j]);\n            chunk = _mm256_div_ps(chunk, _mm256_set1_ps(norm));\n            _mm256_storeu_ps(&mat[i * n + j], chunk);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n\n        for (; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(row, row, sum);\n        }\n\n        float temp[8] __attribute__((aligned(32)));\n        _mm256_store_ps(temp, sum);\n        float row_sum = 0.0f;\n        for (int k = 0; k < 8; ++k) {\n            row_sum += temp[k];\n        }\n\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float inv_norm = 1.0f / sqrtf(row_sum);\n        __m256 invNormVec = _mm256_set1_ps(inv_norm);\n\n        for (j = 0; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            row = _mm256_mul_ps(row, invNormVec);\n            _mm256_storeu_ps(&mat[i * n + j], row);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        __m256 row = _mm256_setzero_ps();\n\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            row = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(row, row, sum);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        row_sum = sqrtf(row_sum);\n\n        __m256 norm = _mm256_set1_ps(row_sum);\n\n        for (j = 0; j + 7 < n; j += 8) {\n            row = _mm256_loadu_ps(&mat[i * n + j]);\n            row = _mm256_div_ps(row, norm);\n            _mm256_storeu_ps(&mat[i * n + j], row);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] /= row_sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        __m256 row = _mm256_setzero_ps();\n\n        size_t j = 0;\n        for (; j + 7 < n; j += 8) {\n            row = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(row, row, sum);\n        }\n\n        __m128 sum_low = _mm256_extractf128_ps(sum, 0);\n        __m128 sum_high = _mm256_extractf128_ps(sum, 1);\n        sum_low = _mm_add_ps(sum_low, sum_high);\n        sum_low = _mm_hadd_ps(sum_low, sum_low);\n        sum_low = _mm_hadd_ps(sum_low, sum_low);\n\n        float norm = _mm_cvtss_f32(sum_low);\n        norm = sqrtf(norm);\n\n        __m256 norm_vec = _mm256_set1_ps(norm);\n\n        j = 0;\n        for (; j + 7 < n; j += 8) {\n            row = _mm256_loadu_ps(&mat[i * n + j]);\n            row = _mm256_div_ps(row, norm_vec);\n            _mm256_storeu_ps(&mat[i * n + j], row);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        __m256 norm = _mm256_setzero_ps();\n\n        size_t j = 0;\n        for (; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(row, row, sum);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float row_sum = 0.0f;\n        for (int k = 0; k < 8; ++k) {\n            row_sum += temp[k];\n        }\n\n        for (; j < n; ++j) {\n            row_sum += mat[i * n + j] * mat[i * n + j];\n        }\n\n        float row_norm = 1.0f / sqrtf(row_sum);\n        norm = _mm256_set1_ps(row_norm);\n\n        j = 0;\n        for (; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            row = _mm256_mul_ps(row, norm);\n            _mm256_storeu_ps(&mat[i * n + j], row);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= row_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d vec_sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_b = _mm256_loadu_pd(&mat2[k * p + j]);\n                vec_sum = _mm256_fmadd_pd(vec_a, vec_b, vec_sum);\n            }\n            __m256d vec_round = _mm256_round_pd(vec_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], vec_round);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d vec_sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_mat1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_mat2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                vec_sum = _mm256_fmadd_pd(vec_mat1, vec_mat2, vec_sum);\n            }\n            __m256d vec_rounded = _mm256_round_pd(vec_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], vec_rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d vec_sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_mat1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_mat2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                vec_sum = _mm256_fmadd_pd(vec_mat1, vec_mat2, vec_sum);\n            }\n            __m256d vec_round = _mm256_round_pd(vec_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], vec_round);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d vec_sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_b = _mm256_loadu_pd(&mat2[k * p + j]);\n                vec_sum = _mm256_fmadd_pd(vec_a, vec_b, vec_sum);\n            }\n            __m256d vec_round = _mm256_round_pd(vec_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], vec_round);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d vec_sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_mat1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_mat2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                vec_sum = _mm256_fmadd_pd(vec_mat1, vec_mat2, vec_sum);\n            }\n            __m256d vec_rounded = _mm256_round_pd(vec_sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(&dst[i * p + j], vec_rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(&src[i * cols + j]);\n            row = _mm256_mul_ps(row, factor);\n            row = _mm256_round_ps(row, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            row = _mm256_mul_ps(row, _mm256_set1_ps(0.25f));\n            _mm256_storeu_ps(&dst[j * rows + i], row);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 scaled = _mm256_mul_ps(row, factor);\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            __m256 result = _mm256_div_ps(rounded, factor);\n\n            for (size_t k = 0; k < 8; ++k) {\n                dst[(j + k) * rows + i] = ((float*)&result)[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256 scaled = _mm256_mul_ps(row, factor);\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 result = _mm256_div_ps(rounded, factor);\n\n            for (size_t k = 0; k < 8; ++k) {\n                dst[(j + k) * rows + i] = ((float*)&result)[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(&src[i * cols + j]);\n            row = _mm256_mul_ps(row, factor);\n            row = _mm256_round_ps(row, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            row = _mm256_mul_ps(row, _mm256_set1_ps(0.25f));\n\n            for (size_t k = 0; k < 8; ++k) {\n                dst[(j + k) * rows + i] = ((float*)&row)[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m256 factor = _mm256_set1_ps(4.0f);\n    __m256 half = _mm256_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(&src[i * cols + j]);\n            row = _mm256_mul_ps(row, factor);\n            row = _mm256_round_ps(row, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            row = _mm256_div_ps(row, factor);\n            __m256 temp = _mm256_loadu_ps(&dst[j * rows + i]);\n            temp = _mm256_blendv_ps(temp, row, _mm256_cmp_ps(row, half, _CMP_GT_OQ));\n            _mm256_storeu_ps(&dst[j * rows + i], temp);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        result = nearbyintf(result);\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = round(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = result > 0 ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        result = nearbyint(result);\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO);\n        _mm256_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        result = round(result);\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m256d a = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d b = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d c = _mm256_mul_pd(a, b);\n            _mm256_storeu_pd(&dst[i * n + j], c);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m256d v1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d v2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(v1, v2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m256d v1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d v2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(v1, v2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m256d v1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d v2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(v1, v2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m256d v1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d v2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(v1, v2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 vec_reg;\n    __m256 mat_reg;\n    __m256 result_reg;\n\n    for (i = 0; i < m; ++i) {\n        result_reg = _mm256_setzero_ps();\n\n        for (j = 0; j < n; j += 8) {\n            vec_reg = _mm256_loadu_ps(&vec[j]);\n            mat_reg = _mm256_loadu_ps(&mat[i * n + j]);\n            result_reg = _mm256_fmadd_ps(mat_reg, vec_reg, result_reg);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, result_reg);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            __m256 vecVals = _mm256_loadu_ps(&vec[j]);\n            __m256 matVals = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(matVals, vecVals, sum);\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            __m256 vecVals = _mm256_loadu_ps(&vec[j]);\n            __m256 matVals = _mm256_loadu_ps(&mat[i * n + j]);\n            sum = _mm256_fmadd_ps(matVals, vecVals, sum);\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 vec_reg;\n    __m256 mat_reg;\n    __m256 result_reg;\n\n    for (i = 0; i < m; ++i) {\n        result_reg = _mm256_setzero_ps();\n\n        for (j = 0; j < n; j += 8) {\n            vec_reg = _mm256_loadu_ps(&vec[j]);\n            mat_reg = _mm256_loadu_ps(&mat[i * n + j]);\n            result_reg = _mm256_fmadd_ps(mat_reg, vec_reg, result_reg);\n        }\n\n        float temp[8];\n        _mm256_storeu_ps(temp, result_reg);\n        dst[i] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 vec_reg[8];\n    __m256 mat_reg;\n    __m256 res_reg;\n\n    // Load vector into registers\n    for (i = 0; i < n; i += 8) {\n        vec_reg[i / 8] = _mm256_loadu_ps(&vec[i]);\n    }\n\n    for (i = 0; i < m; ++i) {\n        res_reg = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            mat_reg = _mm256_loadu_ps(&mat[i * n + j]);\n            res_reg = _mm256_fmadd_ps(mat_reg, vec_reg[j / 8], res_reg);\n        }\n        _mm256_storeu_ps(&dst[i * 8], res_reg);\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t size = m * n;\n    __m256d sum = _mm256_setzero_pd();\n    __m256d temp;\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        temp = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, temp);\n    }\n\n    double sums[4] __attribute__((aligned(32)));\n    _mm256_store_pd(sums, sum);\n\n    double total_sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    for (; i < size; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / size;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i const*)(matrix + i * cols + j));\n            __m128i vec_shifted = _mm_alignr_epi8(vec, vec, 4);\n            __m128i cmp = direction ? _mm_cmpgt_epi32(vec_shifted, vec) : _mm_cmpgt_epi32(vec, vec_shifted);\n            if (_mm_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n        for (size_t j = cols - cols % 4; j < cols - 1; ++j) {\n            if (direction ? (matrix[i * cols + j] > matrix[i * cols + j + 1]) : (matrix[i * cols + j] < matrix[i * cols + j + 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i vec_shifted = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n\n            if (direction) {\n                __m128i cmp = _mm_cmpgt_epi32(vec, vec_shifted);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(vec_shifted, vec);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (size_t j = cols - (cols % 4); j < cols - 1; ++j) {\n            if (direction) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i vec_shifted = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n\n            if (direction) {\n                __m128i cmp = _mm_cmpgt_epi32(vec, vec_shifted);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(vec_shifted, vec);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (size_t j = cols - (cols % 4); j < cols - 1; ++j) {\n            if (direction) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool isAscending = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j + 1));\n\n            if (isAscending) {\n                __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(vec1, vec2);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        // Check remaining elements if cols is not a multiple of 8\n        for (size_t j = (cols / 8) * 8; j < cols - 1; ++j) {\n            if (isAscending) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j + 1));\n\n            if (direction) {\n                __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(vec1, vec2);\n                if (_mm256_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (size_t j = (cols / 8) * 8; j < cols - 1; ++j) {\n            if (direction) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vrem = _mm256_rem_epu16(vsrc, vbase);\n        __m256i vsub = _mm256_sub_epi16(vsrc, vrem);\n        _mm256_storeu_si256((__m256i*)(dst + i), vsub);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vrem = _mm256_rem_epu16(vsrc, vbase);\n        __m256i vsub = _mm256_sub_epi16(vsrc, vrem);\n        _mm256_storeu_si256((__m256i*)&dst[i], vsub);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    size_t i;\n\n    for (i = 0; i + 15 < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vrem = _mm256_rem_epu16(vsrc, vbase);\n        __m256i vsub = _mm256_sub_epi16(vsrc, vrem);\n        _mm256_storeu_si256((__m256i*)&dst[i], vsub);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vrem = _mm256_rem_epu16(vsrc, vbase);\n        __m256i vsub = _mm256_sub_epi16(vsrc, vrem);\n        _mm256_storeu_si256((__m256i*)&dst[i], vsub);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vrem = _mm256_rem_epu16(vsrc, vbase);\n        __m256i vresult = _mm256_sub_epi16(vsrc, vrem);\n        _mm256_storeu_si256((__m256i*)(dst + i), vresult);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    __m256d row;\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 4) {\n            row = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d cmp = _mm256_cmp_pd(row, vec, _CMP_EQ_OQ);\n            if (!_mm256_testz_pd(cmp, _mm256_set1_pd(-1.0))) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    __m256d row;\n    __m256d cmp;\n    int mask;\n\n    for (size_t i = 0; i < rows; ++i) {\n        row = _mm256_loadu_pd(&matrix[i * cols]);\n        cmp = _mm256_cmp_pd(row, vec, _CMP_EQ_OQ);\n        mask = _mm256_movemask_pd(cmp);\n        if (mask == 0b1111) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    __m256d vec2 = _mm256_loadu_pd(vector + 4);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256d mat = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d mat2 = _mm256_loadu_pd(&matrix[i * cols + j + 4]);\n\n            __m256d cmp = _mm256_cmp_pd(mat, vec, _CMP_EQ_OQ);\n            __m256d cmp2 = _mm256_cmp_pd(mat2, vec2, _CMP_EQ_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp) || !_mm256_testz_pd(cmp2, cmp2)) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    __m256d vec2 = _mm256_loadu_pd(vector + 4);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256d mat = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d mat2 = _mm256_loadu_pd(&matrix[i * cols + j + 4]);\n\n            __m256d cmp = _mm256_cmp_pd(mat, vec, _CMP_EQ_OQ);\n            __m256d cmp2 = _mm256_cmp_pd(mat2, vec2, _CMP_EQ_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0xF || _mm256_movemask_pd(cmp2) != 0xF) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d vec = _mm256_loadu_pd(vector);\n    __m256d row;\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 4) {\n            row = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d cmp = _mm256_cmp_pd(row, vec, _CMP_EQ_OQ);\n            if (!_mm256_testz_pd(cmp, _mm256_set1_pd(-1.0))) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        __m256i eq_count = _mm256_sub_epi8(_mm256_set1_epi8(0), eq);\n        count += _mm256_extract_epi32(_mm256_sad_epu8(eq_count, _mm256_setzero_si256()), 0);\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    __m256i vecA, vecB, cmpResult;\n    __m256i mask = _mm256_set1_epi8(-1);\n\n    for (; i <= totalElements - 32; i += 32) {\n        vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        cmpResult = _mm256_cmpeq_epi8(vecA, vecB);\n        mask = _mm256_and_si256(mask, cmpResult);\n        count += _mm256_movemask_epi8(mask);\n        mask = _mm256_set1_epi8(-1);\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        __m256i mask = _mm256_movemask_epi8(eq);\n\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 0));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 1));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 2));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 3));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 4));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 5));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 6));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 7));\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        __m256i eq_count = _mm256_sub_epi8(_mm256_set1_epi8(0), eq);\n        count += _mm256_extract_epi8(eq_count, 0) + _mm256_extract_epi8(eq_count, 16);\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(a, b);\n        __m256i mask = _mm256_movemask_epi8(eq);\n\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 0));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 1));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 2));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 3));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 4));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 5));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 6));\n        count += _mm_popcnt_u32(_mm256_extract_epi32(mask, 7));\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        __m256i vax = _mm256_mul_epi64(va, vx);\n        __m256i vabs_y = _mm256_abs_epi64(vy);\n        __m256i vresult = _mm256_sub_epi64(vax, vabs_y);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        __m256i vax = _mm256_mul_epi32(va, vx);\n        __m256i vabs_vy = _mm256_abs_epi64(vy);\n        __m256i vresult = _mm256_sub_epi64(vax, vabs_vy);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i const*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i const*)&y[i]);\n        __m256i vax = _mm256_mul_epi32(va, vx);\n        __m256i vabs = _mm256_abs_epi64(vy);\n        __m256i vresult = _mm256_sub_epi64(vax, vabs);\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - abs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i;\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n        __m256i vax = _mm256_mul_epi64(va, vx);\n        __m256i vabs_vy = _mm256_abs_epi64(vy);\n        __m256i vresult = _mm256_sub_epi64(vax, vabs_vy);\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - abs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i vy = _mm256_loadu_si256((__m256i*)&y[i]);\n\n        __m256i vax = _mm256_mullo_epi64(va, vx);\n        __m256i vabs_y = _mm256_abs_epi64(vy);\n        __m256i vresult = _mm256_sub_epi64(vax, vabs_y);\n\n        _mm256_storeu_si256((__m256i*)&y[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length == 0) {\n        return {INT16_MAX, INT16_MIN};\n    }\n\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, vmin);\n    _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) min_num = min_vals[j];\n        if (max_vals[j] > max_num) max_num = max_vals[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length == 0) {\n        return {0, 0};\n    }\n\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    int16_t min_values[16];\n    int16_t max_values[16];\n    _mm256_storeu_si256((__m256i*)min_values, vmin);\n    _mm256_storeu_si256((__m256i*)max_values, vmax);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_values[j] < min_num) {\n            min_num = min_values[j];\n        }\n        if (max_values[j] > max_num) {\n            max_num = max_values[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) {\n            min_num = vec[i];\n        }\n        if (vec[i] > max_num) {\n            max_num = vec[i];\n        }\n    }\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length == 0) {\n        return {0, 0};\n    }\n\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, vmin);\n    _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) {\n            min_num = min_vals[j];\n        }\n        if (max_vals[j] > max_num) {\n            max_num = max_vals[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) {\n            min_num = vec[i];\n        }\n        if (vec[i] > max_num) {\n            max_num = vec[i];\n        }\n    }\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length == 0) {\n        return {0, 0};\n    }\n\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, vmin);\n    _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) {\n            min_num = min_vals[j];\n        }\n        if (max_vals[j] > max_num) {\n            max_num = max_vals[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) {\n            min_num = vec[i];\n        }\n        if (vec[i] > max_num) {\n            max_num = vec[i];\n        }\n    }\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    if (length == 0) {\n        return {0, 0};\n    }\n\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    int16_t min_vals[16];\n    int16_t max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, vmin);\n    _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n    for (size_t j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) {\n            min_num = min_vals[j];\n        }\n        if (max_vals[j] > max_num) {\n            max_num = max_vals[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) {\n            min_num = vec[i];\n        }\n        if (vec[i] > max_num) {\n            max_num = vec[i];\n        }\n    }\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j + 1));\n            __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 8) * 8; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = cols - (cols % 8); j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 8) * 8; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 8) * 8; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 7; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j + 1]);\n            __m256i cmp = _mm256_cmpgt_epi32(vec2, vec1);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = cols - (cols % 8); j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_area = block_size * block_size;\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t num_blocks = num_blocks_row * num_blocks_col;\n\n    __m256i first_block[block_area / 8];\n    for (size_t i = 0; i < block_area; i += 8) {\n        first_block[i / 8] = _mm256_loadu_si256((__m256i const*)(matrix + i));\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t row_start = (b / num_blocks_col) * block_size;\n        size_t col_start = (b % num_blocks_col) * block_size;\n        for (size_t i = 0; i < block_area; i += 8) {\n            size_t index = (row_start * cols + col_start) + i;\n            __m256i current_block = _mm256_loadu_si256((__m256i const*)(matrix + index));\n            __m256i cmp = _mm256_cmpgt_epi32(first_block[i / 8], current_block);\n            cmp = _mm256_or_si256(cmp, _mm256_cmpgt_epi32(current_block, first_block[i / 8]));\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n    size_t matrix_elements = rows * cols;\n\n    __m256i first_block[block_elements / 8];\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i const*)(matrix + i * 8));\n    }\n\n    for (size_t b_row = 0; b_row < num_blocks_row; ++b_row) {\n        for (size_t b_col = 0; b_col < num_blocks_col; ++b_col) {\n            if (b_row == 0 && b_col == 0) continue;\n\n            size_t block_start = (b_row * block_size * cols) + (b_col * block_size);\n            for (size_t i = 0; i < block_elements / 8; ++i) {\n                __m256i current_block = _mm256_loadu_si256((__m256i const*)(matrix + block_start + i * 8));\n                __m256i cmp = _mm256_cmpgt_epi32(first_block[i], current_block);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size <= 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_rows = rows / block_size;\n    size_t block_cols = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    for (size_t i = 1; i < block_rows * block_cols; ++i) {\n        size_t current_block_start = i * block_elements;\n        size_t first_block_start = 0;\n\n        for (size_t j = 0; j < block_elements; j += 8) {\n            __m256i first_block_vals = _mm256_loadu_si256((__m256i*)&matrix[first_block_start + j]);\n            __m256i current_block_vals = _mm256_loadu_si256((__m256i*)&matrix[current_block_start + j]);\n\n            __m256i cmp_result = _mm256_cmpeq_epi32(first_block_vals, current_block_vals);\n            if (_mm256_movemask_ps((__m256)cmp_result) != 0xFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size <= 0 || block_size > rows || block_size > cols || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_elements = block_size * block_size;\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t num_blocks = num_blocks_row * num_blocks_col;\n\n    __m256i first_block[block_elements / 8];\n    for (size_t i = 0; i < block_elements / 8; ++i) {\n        first_block[i] = _mm256_loadu_si256((__m256i*)&matrix[i * 8]);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t row_start = (b / num_blocks_col) * block_size;\n        size_t col_start = (b % num_blocks_col) * block_size;\n        for (size_t i = 0; i < block_elements / 8; ++i) {\n            size_t index = (row_start + (i * 8) / block_size) * cols + col_start + (i * 8) % block_size;\n            __m256i current_block = _mm256_loadu_si256((__m256i*)&matrix[index]);\n            __m256i cmp = _mm256_cmpgt_epi32(first_block[i], current_block);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n    size_t total_blocks = num_blocks_row * num_blocks_col;\n\n    for (size_t block_idx = 1; block_idx < total_blocks; ++block_idx) {\n        size_t row_start = (block_idx / num_blocks_col) * block_size;\n        size_t col_start = (block_idx % num_blocks_col) * block_size;\n\n        for (size_t i = 0; i < block_size; ++i) {\n            for (size_t j = 0; j < block_size; j += 8) {\n                size_t idx1 = (row_start + i) * cols + (col_start + j);\n                size_t idx2 = i * cols + j;\n\n                __m256i vec1 = _mm256_loadu_si256((__m256i const*)(matrix + idx1));\n                __m256i vec2 = _mm256_loadu_si256((__m256i const*)(matrix + idx2));\n\n                __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n                if (_mm256_movemask_ps((__m256)cmp) != 0xFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks < 2) {\n        return false;\n    }\n\n    __m256d tolerance_v = _mm256_set1_pd(tolerance);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < block_size; i += 4) {\n        __m256d first_block_v = _mm256_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m256d current_block_v = _mm256_loadu_pd(&vec[j * block_size + i]);\n            __m256d diff_v = _mm256_sub_pd(first_block_v, current_block_v);\n            __m256d abs_diff_v = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_v);\n\n            __m256d cmp_v = _mm256_cmp_pd(abs_diff_v, tolerance_v, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp_v) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks == 0) {\n        return false;\n    }\n\n    __m256d tolerance_v = _mm256_set1_pd(tolerance);\n    __m256d zero_v = _mm256_setzero_pd();\n\n    for (size_t i = block_size; i < full_blocks * block_size; i += block_size) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&vec[j]);\n            __m256d vec2 = _mm256_loadu_pd(&vec[i + j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tolerance_v, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks == 0) {\n        return false;\n    }\n\n    __m256d tolerance_v = _mm256_set1_pd(tolerance);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < block_size; i += 4) {\n        __m256d first_block_v = _mm256_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m256d current_block_v = _mm256_loadu_pd(&vec[j * block_size + i]);\n            __m256d diff_v = _mm256_sub_pd(first_block_v, current_block_v);\n            __m256d abs_diff_v = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_v);\n            __m256d cmp_v = _mm256_cmp_pd(abs_diff_v, tolerance_v, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp_v, _mm256_set1_pd(-0.0))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks == 0) {\n        return false;\n    }\n\n    __m256d tolerance_v = _mm256_set1_pd(tolerance);\n    __m256d zero_v = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < block_size; i += 4) {\n        __m256d first_block_v = _mm256_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m256d current_block_v = _mm256_loadu_pd(&vec[j * block_size + i]);\n            __m256d diff_v = _mm256_sub_pd(first_block_v, current_block_v);\n            __m256d abs_diff_v = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_v);\n            __m256d cmp_v = _mm256_cmp_pd(abs_diff_v, tolerance_v, _CMP_GT_OQ);\n\n            if (_mm256_movemask_pd(cmp_v) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || block_size == 0 || length < block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks < 2) {\n        return false;\n    }\n\n    __m256d tolerance_v = _mm256_set1_pd(tolerance);\n    __m256d zero_v = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < block_size; i += 4) {\n        __m256d first_block_v = _mm256_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m256d current_block_v = _mm256_loadu_pd(&vec[j * block_size + i]);\n            __m256d diff_v = _mm256_sub_pd(first_block_v, current_block_v);\n            __m256d abs_diff_v = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_v);\n\n            __m256d cmp_v = _mm256_cmp_pd(abs_diff_v, tolerance_v, _CMP_GT_OQ);\n            if (_mm256_movemask_pd(cmp_v) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t cols_aligned = cols & ~0x3; // Make cols a multiple of 4\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n\n        const double* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 4 elements at a time\n        for (; j < cols_aligned; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row + j);\n            vec = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, vec);\n        }\n\n        // Horizontally add the elements of sum\n        __m128d sum_low = _mm256_extractf128_pd(sum, 0);\n        __m128d sum_high = _mm256_extractf128_pd(sum, 1);\n        sum_low = _mm_add_pd(sum_low, sum_high);\n        sum_high = _mm_unpackhi_pd(sum_low, sum_low);\n        sum_low = _mm_add_sd(sum_low, sum_high);\n\n        double row_norm = sqrt(_mm_cvtsd_f64(sum_low));\n\n        // Process remaining elements\n        for (; j < cols; ++j) {\n            row_norm += row[j] * row[j];\n        }\n\n        row_norm = sqrt(row_norm);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d row = _mm256_loadu_pd(&matrix[i * cols + j]);\n            row = _mm256_mul_pd(row, row);\n            sum = _mm256_add_pd(sum, row);\n        }\n\n        double temp[4];\n        _mm256_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1] + temp[2] + temp[3]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            vec = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, vec);\n        }\n\n        double temp[4];\n        _mm256_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1] + temp[2] + temp[3]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t cols_aligned = cols & ~0x3; // Make cols multiple of 4 for AVX2\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n\n        for (size_t j = 0; j < cols_aligned; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            vec = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, vec);\n        }\n\n        double temp[4];\n        _mm256_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1] + temp[2] + temp[3]);\n\n        for (size_t j = cols_aligned; j < cols; ++j) {\n            row_norm += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n\n        row_norm = sqrt(row_norm);\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d row = _mm256_loadu_pd(&matrix[i * cols + j]);\n            row = _mm256_mul_pd(row, row);\n            sum = _mm256_add_pd(sum, row);\n        }\n\n        double temp[4];\n        _mm256_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1] + temp[2] + temp[3]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c = _mm256_or_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a, a);\n        __m256i b_sq = _mm256_mullo_epi16(b, b);\n        __m256i prod = _mm256_mullo_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm256_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm256_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = 0;\n    int64_t sum_prod_total = 0;\n\n    int64_t* sum_sq_array = (int64_t*)&sum_sq;\n    int64_t* sum_prod_array = (int64_t*)&sum_prod;\n\n    for (int j = 0; j < 4; j++) {\n        sum_sq_total += sum_sq_array[j];\n        sum_prod_total += sum_prod_array[j];\n    }\n\n    for (; i < length; ++i) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_sq = _mm256_maddubs_epi16(a, a);\n        __m256i b_sq = _mm256_maddubs_epi16(b, b);\n        __m256i prod = _mm256_maddubs_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(a_sq, _mm256_set1_epi16(1)));\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_madd_epi16(b_sq, _mm256_set1_epi16(1)));\n        sum_prod = _mm256_add_epi32(sum_prod, _mm256_madd_epi16(prod, _mm256_set1_epi16(1)));\n    }\n\n    int64_t sum_sq_total = 0;\n    int64_t sum_prod_total = 0;\n\n    sum_sq_total += _mm256_extract_epi64(sum_sq, 0);\n    sum_sq_total += _mm256_extract_epi64(sum_sq, 1);\n    sum_sq_total += _mm256_extract_epi64(sum_sq, 2);\n    sum_sq_total += _mm256_extract_epi64(sum_sq, 3);\n\n    sum_prod_total += _mm256_extract_epi64(sum_prod, 0);\n    sum_prod_total += _mm256_extract_epi64(sum_prod, 1);\n    sum_prod_total += _mm256_extract_epi64(sum_prod, 2);\n    sum_prod_total += _mm256_extract_epi64(sum_prod, 3);\n\n    for (; i < length; ++i) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_sq = _mm256_madd_epi16(_mm256_madd_epi16(a, a), _mm256_set1_epi32(1));\n        __m256i b_sq = _mm256_madd_epi16(_mm256_madd_epi16(b, b), _mm256_set1_epi32(1));\n        __m256i prod = _mm256_madd_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm256_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm256_add_epi32(sum_prod, prod);\n    }\n\n    int32_t sum_sq_arr[8] = {0};\n    int32_t sum_prod_arr[8] = {0};\n\n    _mm256_storeu_si256((__m256i*)sum_sq_arr, sum_sq);\n    _mm256_storeu_si256((__m256i*)sum_prod_arr, sum_prod);\n\n    int64_t sum_sq_total = 0;\n    int64_t sum_prod_total = 0;\n\n    for (int j = 0; j < 8; j++) {\n        sum_sq_total += sum_sq_arr[j];\n        sum_prod_total += sum_prod_arr[j];\n    }\n\n    for (; i < length; i++) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a, a);\n        __m256i b_sq = _mm256_mullo_epi16(b, b);\n        __m256i prod = _mm256_mullo_epi16(a, b);\n\n        sum_sq = _mm256_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm256_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm256_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = 0;\n    int64_t sum_prod_total = 0;\n\n    int64_t* sum_sq_array = (int64_t*)&sum_sq;\n    int64_t* sum_prod_array = (int64_t*)&sum_prod;\n\n    for (int j = 0; j < 4; j++) {\n        sum_sq_total += sum_sq_array[j];\n        sum_prod_total += sum_prod_array[j];\n    }\n\n    for (; i < length; i++) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_sq = _mm256_setzero_si256();\n    __m256i sum_prod = _mm256_setzero_si256();\n\n    size_t i;\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i a_sq = _mm256_madd_epi16(_mm256_maddubs_epi16(a, a), _mm256_set1_epi32(1));\n        __m256i b_sq = _mm256_madd_epi16(_mm256_maddubs_epi16(b, b), _mm256_set1_epi32(1));\n        __m256i prod = _mm256_madd_epi16(_mm256_maddubs_epi16(a, b), _mm256_set1_epi32(1));\n\n        sum_sq = _mm256_add_epi32(sum_sq, _mm256_add_epi32(a_sq, b_sq));\n        sum_prod = _mm256_add_epi32(sum_prod, prod);\n    }\n\n    __m256i sum_sq_final = _mm256_hadd_epi32(_mm256_hadd_epi32(sum_sq, sum_sq), sum_sq);\n    __m256i sum_prod_final = _mm256_hadd_epi32(_mm256_hadd_epi32(sum_prod, sum_prod), sum_prod);\n\n    int64_t result = _mm256_extract_epi64(sum_sq_final, 0) - _mm256_extract_epi64(sum_prod_final, 0);\n\n    for (; i < length; ++i) {\n        result += (A[i] * A[i] + B[i] * B[i] - A[i] * B[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b = _mm256_andnot_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n        __m256i neg_mask = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i pos_mask_b = _mm256_cmpgt_epi8(_mm256_setzero_si256(), b);\n        __m256i neg_mask_b = _mm256_cmpgt_epi8(b, _mm256_setzero_si256());\n\n        __m256i both_pos = _mm256_and_si256(pos_mask, pos_mask_b);\n        __m256i both_neg = _mm256_and_si256(neg_mask, neg_mask_b);\n\n        __m256i result = _mm256_or_si256(both_pos, both_neg);\n        result = _mm256_add_epi8(result, both_neg);\n        result = _mm256_sub_epi8(result, both_pos);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n        __m256i neg_mask = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n\n        __m256i pos_result = _mm256_and_si256(pos_mask, _mm256_cmpgt_epi8(_mm256_setzero_si256(), b));\n        __m256i neg_result = _mm256_and_si256(neg_mask, _mm256_cmpgt_epi8(b, _mm256_setzero_si256()));\n\n        __m256i result = _mm256_or_si256(pos_result, neg_result);\n        result = _mm256_add_epi8(result, neg_result);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_mask = _mm256_and_si256(a, b);\n        __m256i neg_mask = _mm256_or_si256(a, b);\n\n        pos_mask = _mm256_cmpgt_epi8(pos_mask, _mm256_setzero_si256());\n        neg_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), neg_mask);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(pos_mask, _mm256_set1_epi8(1)),\n            _mm256_andnot_si256(pos_mask, _mm256_and_si256(neg_mask, _mm256_set1_epi8(-1)))\n        );\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_mask = _mm256_cmpgt_epi8(a, _mm256_setzero_si256());\n        __m256i neg_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);\n\n        __m256i pos_result = _mm256_and_si256(pos_mask, _mm256_cmpgt_epi8(b, _mm256_setzero_si256()));\n        __m256i neg_result = _mm256_and_si256(neg_mask, _mm256_cmpgt_epi8(_mm256_setzero_si256(), b));\n\n        __m256i result = _mm256_or_si256(pos_result, neg_result);\n        result = _mm256_add_epi8(result, neg_result);\n        result = _mm256_sub_epi8(result, _mm256_andnot_si256(_mm256_or_si256(pos_result, neg_result), _mm256_set1_epi8(1)));\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_mask = _mm256_and_si256(a, b);\n        __m256i neg_mask = _mm256_or_si256(a, b);\n\n        pos_mask = _mm256_cmpgt_epi8(pos_mask, _mm256_setzero_si256());\n        neg_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), neg_mask);\n\n        __m256i result = _mm256_or_si256(pos_mask, neg_mask);\n        result = _mm256_add_epi8(result, neg_mask);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m256i* A_vec = (__m256i*)A;\n    __m256i* B_vec = (__m256i*)B;\n    __m256i* C_vec = (__m256i*)C;\n\n    size_t vec_length = length / 16;\n    size_t remainder = length % 16;\n\n    for (i = 0; i < vec_length; ++i) {\n        if (i < mid / 16) {\n            C_vec[i] = _mm256_xor_si256(A_vec[i], B_vec[i]);\n        } else if (i > mid / 16) {\n            C_vec[i] = _mm256_and_si256(A_vec[i], B_vec[i]);\n        } else {\n            C_vec[i] = A_vec[i];\n        }\n    }\n\n    size_t offset = vec_length * 16;\n    for (i = 0; i < remainder; ++i) {\n        if (offset + i < mid) {\n            C[offset + i] = A[offset + i] ^ B[offset + i];\n        } else if (offset + i > mid) {\n            C[offset + i] = A[offset + i] & B[offset + i];\n        } else {\n            C[offset + i] = A[offset + i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2 intrinsics\n    for (; i <= mid - 16; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Keep the value in A at the midpoint\n    if (i < length) {\n        C[i] = A[i];\n        i++;\n    }\n\n    // Process elements after the midpoint using AVX2 intrinsics\n    for (; i <= length - 16; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m256i* A_vec = (__m256i*)A;\n    __m256i* B_vec = (__m256i*)B;\n    __m256i* C_vec = (__m256i*)C;\n\n    size_t vec_length = length / 16;\n    size_t remainder = length % 16;\n\n    for (i = 0; i < vec_length; ++i) {\n        if (i < mid / 16) {\n            C_vec[i] = _mm256_xor_si256(A_vec[i], B_vec[i]);\n        } else if (i > mid / 16) {\n            C_vec[i] = _mm256_and_si256(A_vec[i], B_vec[i]);\n        } else {\n            C_vec[i] = A_vec[i];\n        }\n    }\n\n    i *= 16;\n    for (; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > mid) {\n            C[i] = A[i] & B[i];\n        } else {\n            C[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m256i* A_vec = (__m256i*)A;\n    __m256i* B_vec = (__m256i*)B;\n    __m256i* C_vec = (__m256i*)C;\n\n    size_t vec_length = length / 16;\n    size_t remainder = length % 16;\n\n    for (i = 0; i < vec_length; ++i) {\n        if (i < mid / 16) {\n            C_vec[i] = _mm256_xor_si256(A_vec[i], B_vec[i]);\n        } else if (i > mid / 16) {\n            C_vec[i] = _mm256_and_si256(A_vec[i], B_vec[i]);\n        } else {\n            uint16_t* A_mid = (uint16_t*)&A_vec[i];\n            uint16_t* C_mid = (uint16_t*)&C_vec[i];\n            C_mid[mid % 16] = A_mid[mid % 16];\n            if (mid % 16 > 0) {\n                __m256i mask = _mm256_setr_epi16(\n                    -1, -1, -1, -1, -1, -1, -1, -1,\n                    0, 0, 0, 0, 0, 0, 0, 0);\n                __m256i xor_result = _mm256_xor_si256(A_vec[i], B_vec[i]);\n                __m256i and_result = _mm256_and_si256(A_vec[i], B_vec[i]);\n                C_vec[i] = _mm256_blendv_epi8(and_result, xor_result, mask);\n            } else {\n                C_vec[i] = _mm256_and_si256(A_vec[i], B_vec[i]);\n            }\n        }\n    }\n\n    for (i = vec_length * 16; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > mid) {\n            C[i] = A[i] & B[i];\n        } else {\n            C[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m256i* A_vec = (__m256i*)A;\n    __m256i* B_vec = (__m256i*)B;\n    __m256i* C_vec = (__m256i*)C;\n\n    size_t vec_length = length / 16;\n    size_t remainder = length % 16;\n\n    for (i = 0; i < vec_length; ++i) {\n        if (i < mid / 16) {\n            C_vec[i] = _mm256_xor_si256(A_vec[i], B_vec[i]);\n        } else if (i > mid / 16) {\n            C_vec[i] = _mm256_and_si256(A_vec[i], B_vec[i]);\n        } else {\n            uint16_t mid_value = A[mid];\n            C_vec[i] = _mm256_insert_epi16(_mm256_setzero_si256(), mid_value, mid % 16);\n        }\n    }\n\n    for (i = vec_length * 16; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > mid) {\n            C[i] = A[i] & B[i];\n        } else {\n            C[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_size - 8; i += 8) {\n        __m256 cond1_vec = _mm256_loadu_ps((float*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_ps((float*)&cond2[i]);\n\n        __m256 A_vec = _mm256_loadu_ps(&A[i]);\n        __m256 B_vec = _mm256_loadu_ps(&B[i]);\n        __m256 C_vec = _mm256_loadu_ps(&C[i]);\n        __m256 D_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 mask1 = _mm256_castps_si256(cond1_vec);\n        __m256 mask2 = _mm256_castps_si256(cond2_vec);\n\n        __m256 result = _mm256_blendv_ps(D_vec, C_vec, _mm256_castsi256_ps(mask2));\n        result = _mm256_blendv_ps(result, B_vec, _mm256_castsi256_ps(mask1));\n        result = _mm256_blendv_ps(result, A_vec, _mm256_and_ps(_mm256_castsi256_ps(mask1), _mm256_castsi256_ps(mask2)));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_size - 8; i += 8) {\n        __m256 mask1 = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&cond1[i]));\n        __m256 mask2 = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&cond2[i]));\n\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        __m256 result = _mm256_blendv_ps(d, c, mask2);\n        result = _mm256_blendv_ps(result, b, mask1);\n        result = _mm256_blendv_ps(result, a, _mm256_and_ps(mask1, mask2));\n\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_size - 8; i += 8) {\n        __m256 cond1_vec = _mm256_loadu_ps((float*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_ps((float*)&cond2[i]);\n\n        __m256 A_vec = _mm256_loadu_ps(&A[i]);\n        __m256 B_vec = _mm256_loadu_ps(&B[i]);\n        __m256 C_vec = _mm256_loadu_ps(&C[i]);\n        __m256 D_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_mask = _mm256_cmp_ps(cond1_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n        __m256 cond2_mask = _mm256_cmp_ps(cond2_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n\n        __m256 out_vec = _mm256_blendv_ps(D_vec, C_vec, cond2_mask);\n        out_vec = _mm256_blendv_ps(out_vec, B_vec, cond1_mask);\n        out_vec = _mm256_blendv_ps(out_vec, A_vec, _mm256_and_ps(cond1_mask, cond2_mask));\n\n        _mm256_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (; i <= total_size - 8; i += 8) {\n        __m256 cond1_vec = _mm256_loadu_ps((float*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_ps((float*)&cond2[i]);\n\n        __m256 A_vec = _mm256_loadu_ps(&A[i]);\n        __m256 B_vec = _mm256_loadu_ps(&B[i]);\n        __m256 C_vec = _mm256_loadu_ps(&C[i]);\n        __m256 D_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_mask = _mm256_cmp_ps(cond1_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n        __m256 cond2_mask = _mm256_cmp_ps(cond2_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n\n        __m256 out_vec = _mm256_blendv_ps(D_vec, C_vec, cond2_mask);\n        out_vec = _mm256_blendv_ps(out_vec, B_vec, cond1_mask);\n        out_vec = _mm256_blendv_ps(out_vec, A_vec, _mm256_and_ps(cond1_mask, cond2_mask));\n\n        _mm256_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 7 < total_size; i += 8) {\n        __m256 cond1_vec = _mm256_loadu_ps((float*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_ps((float*)&cond2[i]);\n        __m256 A_vec = _mm256_loadu_ps(&A[i]);\n        __m256 B_vec = _mm256_loadu_ps(&B[i]);\n        __m256 C_vec = _mm256_loadu_ps(&C[i]);\n        __m256 D_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_mask = _mm256_cmp_ps(cond1_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n        __m256 cond2_mask = _mm256_cmp_ps(cond2_vec, _mm256_setzero_ps(), _CMP_NEQ_UQ);\n\n        __m256 out_vec = _mm256_blendv_ps(D_vec, C_vec, cond2_mask);\n        out_vec = _mm256_blendv_ps(out_vec, B_vec, cond1_mask);\n        out_vec = _mm256_blendv_ps(out_vec, A_vec, _mm256_and_ps(cond1_mask, cond2_mask));\n\n        _mm256_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m256i mask_0F = _mm256_set1_epi8(0x0F);\n    __m256i mask_33 = _mm256_set1_epi8(0x33);\n    __m256i mask_55 = _mm256_set1_epi8(0x55);\n\n    for (i = 0; i <= total_size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        __m256i v1 = _mm256_srli_epi32(v, 1);\n        v = _mm256_sub_epi32(v, v1);\n        __m256i v2 = _mm256_and_si256(v1, mask_55);\n        v = _mm256_add_epi32(v, v2);\n\n        v1 = _mm256_srli_epi32(v, 2);\n        v = _mm256_add_epi32(v, v1);\n        v = _mm256_and_si256(v, mask_33);\n\n        v = _mm256_add_epi32(v, _mm256_srli_epi32(v, 4));\n        v = _mm256_and_si256(v, mask_0F);\n\n        v = _mm256_add_epi32(v, _mm256_srli_epi32(v, 8));\n        v = _mm256_add_epi32(v, _mm256_srli_epi32(v, 16));\n\n        __m256i result = _mm256_and_si256(v, _mm256_set1_epi32(0x3F));\n\n        _mm256_storeu_si256((__m256i*)(out + i), result);\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        v = v - ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v + (v >> 4)) & 0x0F0F0F0F;\n        v = v + (v >> 8);\n        v = v + (v >> 16);\n        out[i] = v & 0x3F;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m256i mask_one = _mm256_set1_epi32(1);\n    __m256i mask_two = _mm256_set1_epi32(2);\n    __m256i mask_four = _mm256_set1_epi32(4);\n    __m256i mask_eight = _mm256_set1_epi32(8);\n    __m256i mask_sixteen = _mm256_set1_epi32(16);\n\n    for (i = 0; i < total_size - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        __m256i v1 = _mm256_and_si256(v, mask_one);\n        __m256i v2 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask_one);\n        __m256i v3 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask_one);\n        __m256i v4 = _mm256_and_si256(_mm256_srli_epi32(v, 3), mask_one);\n        __m256i v5 = _mm256_and_si256(_mm256_srli_epi32(v, 4), mask_one);\n        __m256i v6 = _mm256_and_si256(_mm256_srli_epi32(v, 5), mask_one);\n        __m256i v7 = _mm256_and_si256(_mm256_srli_epi32(v, 6), mask_one);\n        __m256i v8 = _mm256_and_si256(_mm256_srli_epi32(v, 7), mask_one);\n\n        v1 = _mm256_add_epi32(v1, v2);\n        v3 = _mm256_add_epi32(v3, v4);\n        v5 = _mm256_add_epi32(v5, v6);\n        v7 = _mm256_add_epi32(v7, v8);\n\n        v1 = _mm256_add_epi32(v1, v3);\n        v5 = _mm256_add_epi32(v5, v7);\n\n        v1 = _mm256_add_epi32(v1, v5);\n\n        __m256i count = _mm256_sad_epu8(v1, _mm256_setzero_si256());\n\n        uint8_t counts[8];\n        _mm_storeu_si128((__m128i*)counts, _mm256_extracti128_si256(count, 0));\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = counts[j];\n        }\n    }\n\n    for (; i < total_size; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m256i mask_one = _mm256_set1_epi32(1);\n    __m256i mask_two = _mm256_set1_epi32(2);\n    __m256i mask_four = _mm256_set1_epi32(4);\n    __m256i mask_eight = _mm256_set1_epi32(8);\n    __m256i mask_sixteen = _mm256_set1_epi32(16);\n\n    for (i = 0; i < total_size - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        __m256i v1 = _mm256_and_si256(v, mask_one);\n        __m256i v2 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask_one);\n        __m256i v3 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask_one);\n        __m256i v4 = _mm256_and_si256(_mm256_srli_epi32(v, 3), mask_one);\n\n        v1 = _mm256_add_epi32(v1, v2);\n        v1 = _mm256_add_epi32(v1, v3);\n        v1 = _mm256_add_epi32(v1, v4);\n\n        v2 = _mm256_and_si256(_mm256_srli_epi32(v, 4), mask_four);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v, 5), mask_four);\n        v4 = _mm256_and_si256(_mm256_srli_epi32(v, 6), mask_four);\n        __m256i v5 = _mm256_and_si256(_mm256_srli_epi32(v, 7), mask_four);\n\n        v2 = _mm256_add_epi32(v2, v3);\n        v2 = _mm256_add_epi32(v2, v4);\n        v2 = _mm256_add_epi32(v2, v5);\n\n        v1 = _mm256_add_epi32(v1, v2);\n\n        v2 = _mm256_and_si256(_mm256_srli_epi32(v, 8), mask_eight);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v, 9), mask_eight);\n        v4 = _mm256_and_si256(_mm256_srli_epi32(v, 10), mask_eight);\n        v5 = _mm256_and_si256(_mm256_srli_epi32(v, 11), mask_eight);\n        __m256i v6 = _mm256_and_si256(_mm256_srli_epi32(v, 12), mask_eight);\n        __m256i v7 = _mm256_and_si256(_mm256_srli_epi32(v, 13), mask_eight);\n        __m256i v8 = _mm256_and_si256(_mm256_srli_epi32(v, 14), mask_eight);\n        __m256i v9 = _mm256_and_si256(_mm256_srli_epi32(v, 15), mask_eight);\n\n        v2 = _mm256_add_epi32(v2, v3);\n        v2 = _mm256_add_epi32(v2, v4);\n        v2 = _mm256_add_epi32(v2, v5);\n        v2 = _mm256_add_epi32(v2, v6);\n        v2 = _mm256_add_epi32(v2, v7);\n        v2 = _mm256_add_epi32(v2, v8);\n        v2 = _mm256_add_epi32(v2, v9);\n\n        v1 = _mm256_add_epi32(v1, v2);\n\n        v2 = _mm256_and_si256(_mm256_srli_epi32(v, 16), mask_sixteen);\n        v3 = _mm256_and_si256(_mm256_srli_epi32(v, 17), mask_sixteen);\n        v4 = _mm256_and_si256(_mm256_srli_epi32(v, 18), mask_sixteen);\n        v5 = _mm256_and_si256(_mm256_srli_epi32(v, 19), mask_sixteen);\n        v6 = _mm256_and_si256(_mm256_srli_epi32(v, 20), mask_sixteen);\n        v7 = _mm256_and_si256(_mm256_srli_epi32(v, 21), mask_sixteen);\n        v8 = _mm256_and_si256(_mm256_srli_epi32(v, 22), mask_sixteen);\n        v9 = _mm256_and_si256(_mm256_srli_epi32(v, 23), mask_sixteen);\n        __m256i v10 = _mm256_and_si256(_mm256_srli_epi32(v, 24), mask_sixteen);\n        __m256i v11 = _mm256_and_si256(_mm256_srli_epi32(v, 25), mask_sixteen);\n        __m256i v12 = _mm256_and_si256(_mm256_srli_epi32(v, 26), mask_sixteen);\n        __m256i v13 = _mm256_and_si256(_mm256_srli_epi32(v, 27), mask_sixteen);\n        __m256i v14 = _mm256_and_si256(_mm256_srli_epi32(v, 28), mask_sixteen);\n        __m256i v15 = _mm256_and_si256(_mm256_srli_epi32(v, 29), mask_sixteen);\n        __m256i v16 = _mm256_and_si256(_mm256_srli_epi32(v, 30), mask_sixteen);\n        __m256i v17 = _mm256_and_si256(_mm256_srli_epi32(v, 31), mask_sixteen);\n\n        v2 = _mm256_add_epi32(v2, v3);\n        v2 = _mm256_add_epi32(v2, v4);\n        v2 = _mm256_add_epi32(v2, v5);\n        v2 = _mm256_add_epi32(v2, v6);\n        v2 = _mm256_add_epi32(v2, v7);\n        v2 = _mm256_add_epi32(v2, v8);\n        v2 = _mm256_add_epi32(v2, v9);\n        v2 = _mm256_add_epi32(v2, v10);\n        v2 = _mm256_add_epi32(v2, v11);\n        v2 = _mm256_add_epi32(v2, v12);\n        v2 = _mm256_add_epi32(v2, v13);\n        v2 = _mm256_add_epi32(v2, v14);\n        v2 = _mm256_add_epi32(v2, v15);\n        v2 = _mm256_add_epi32(v2, v16);\n        v2 = _mm256_add_epi32(v2, v17);\n\n        v1 = _mm256_add_epi32(v1, v2);\n\n        __m256i result = _mm256_packus_epi32(v1, v1);\n        result = _mm256_permute4x64_epi64(result, _MM_SHUFFLE(3, 1, 2, 0));\n        result = _mm256_packus_epi16(result, result);\n\n        _mm_storeu_si128((__m128i*)(out + i), _mm256_castsi256_si128(result));\n    }\n\n    for (; i < total_size; ++i) {\n        out[i] = _mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m256i mask_0F = _mm256_set1_epi8(0x0F);\n    __m256i mask_33 = _mm256_set1_epi8(0x33);\n    __m256i mask_55 = _mm256_set1_epi8(0x55);\n    __m256i mask_77 = _mm256_set1_epi8(0x77);\n\n    for (i = 0; i < total_size - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        __m256i count1 = _mm256_and_si256(v, mask_55);\n        __m256i count2 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask_55);\n        __m256i count4 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask_55);\n        __m256i count8 = _mm256_and_si256(_mm256_srli_epi32(v, 3), mask_55);\n        __m256i count16 = _mm256_and_si256(_mm256_srli_epi32(v, 4), mask_55);\n\n        count1 = _mm256_add_epi8(count1, count2);\n        count4 = _mm256_add_epi8(count4, count8);\n        count1 = _mm256_add_epi8(count1, count4);\n        count1 = _mm256_add_epi8(count1, count16);\n\n        count2 = _mm256_and_si256(count1, mask_33);\n        count4 = _mm256_and_si256(_mm256_srli_epi32(count1, 2), mask_33);\n        count8 = _mm256_and_si256(_mm256_srli_epi32(count1, 4), mask_33);\n        count16 = _mm256_and_si256(_mm256_srli_epi32(count1, 6), mask_33);\n\n        count2 = _mm256_add_epi8(count2, count4);\n        count8 = _mm256_add_epi8(count8, count16);\n        count2 = _mm256_add_epi8(count2, count8);\n\n        count4 = _mm256_and_si256(count2, mask_0F);\n        count8 = _mm256_and_si256(_mm256_srli_epi32(count2, 4), mask_0F);\n        count4 = _mm256_add_epi8(count4, count8);\n\n        _mm256_storeu_si256((__m256i*)&out[i], count4);\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        uint8_t count = 0;\n        count += (v >> 0) & 1;\n        count += (v >> 1) & 1;\n        count += (v >> 2) & 1;\n        count += (v >> 3) & 1;\n        count += (v >> 4) & 1;\n        count += (v >> 5) & 1;\n        count += (v >> 6) & 1;\n        count += (v >> 7) & 1;\n        count += (v >> 8) & 1;\n        count += (v >> 9) & 1;\n        count += (v >> 10) & 1;\n        count += (v >> 11) & 1;\n        count += (v >> 12) & 1;\n        count += (v >> 13) & 1;\n        count += (v >> 14) & 1;\n        count += (v >> 15) & 1;\n        count += (v >> 16) & 1;\n        count += (v >> 17) & 1;\n        count += (v >> 18) & 1;\n        count += (v >> 19) & 1;\n        count += (v >> 20) & 1;\n        count += (v >> 21) & 1;\n        count += (v >> 22) & 1;\n        count += (v >> 23) & 1;\n        count += (v >> 24) & 1;\n        count += (v >> 25) & 1;\n        count += (v >> 26) & 1;\n        count += (v >> 27) & 1;\n        count += (v >> 28) & 1;\n        count += (v >> 29) & 1;\n        count += (v >> 30) & 1;\n        count += (v >> 31) & 1;\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m256i mask_one = _mm256_set1_epi32(1);\n    __m256i mask_two = _mm256_set1_epi32(2);\n    __m256i mask_four = _mm256_set1_epi32(4);\n    __m256i mask_eight = _mm256_set1_epi32(8);\n    __m256i mask_sixteen = _mm256_set1_epi32(16);\n\n    for (i = 0; i < total_size - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        __m256i v1 = _mm256_and_si256(v, mask_one);\n        __m256i v2 = _mm256_and_si256(_mm256_srli_epi32(v, 1), mask_one);\n        __m256i v4 = _mm256_and_si256(_mm256_srli_epi32(v, 2), mask_one);\n        __m256i v8 = _mm256_and_si256(_mm256_srli_epi32(v, 3), mask_one);\n        __m256i v16 = _mm256_and_si256(_mm256_srli_epi32(v, 4), mask_one);\n\n        v = _mm256_add_epi32(v1, v2);\n        v = _mm256_add_epi32(v, v4);\n        v = _mm256_add_epi32(v, v8);\n        v = _mm256_add_epi32(v, v16);\n\n        v = _mm256_add_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 4), mask_sixteen));\n        v = _mm256_add_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 5), mask_sixteen));\n        v = _mm256_add_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 6), mask_sixteen));\n        v = _mm256_add_epi32(v, _mm256_and_si256(_mm256_srli_epi32(v, 7), mask_sixteen));\n\n        v = _mm256_sad_epu8(v, _mm256_setzero_si256());\n\n        __m128i v_low = _mm256_extracti128_si256(v, 0);\n        __m128i v_high = _mm256_extracti128_si256(v, 1);\n\n        v_low = _mm_add_epi32(v_low, v_high);\n\n        uint32_t result[4];\n        _mm_storeu_si128((__m128i*)result, v_low);\n\n        for (int j = 0; j < 8; ++j) {\n            out[i + j] = (uint8_t)result[j];\n        }\n    }\n\n    for (; i < total_size; ++i) {\n        out[i] = (uint8_t)_mm_popcnt_u32(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i mask_div3, mask_abs, mask_not_div3;\n    __m256i vecA, vecB, vecAbs;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 7; j += 8) {\n            vecA = _mm256_loadu_si256((__m256i const*)&A[i * cols + j]);\n            mask_div3 = _mm256_set_epi32(\n                (A[i * cols + j + 7] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 6] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 5] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 4] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 3] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 2] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 1] % 3 == 0) ? -1 : 0,\n                (A[i * cols + j + 0] % 3 == 0) ? -1 : 0\n            );\n            mask_not_div3 = _mm256_xor_si256(mask_div3, _mm256_set1_epi32(-1));\n            vecAbs = _mm256_abs_epi32(vecA);\n            vecB = _mm256_or_si256(_mm256_and_si256(mask_div3, vecAbs), _mm256_and_si256(mask_not_div3, vecA));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], vecB);\n        }\n        for (; j < cols; ++j) {\n            B[i * cols + j] = (A[i * cols + j] % 3 == 0) ? abs(A[i * cols + j]) : A[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i mask;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 7; j += 8) {\n            __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i * cols + j));\n            __m256i rem = _mm256_rem_epi32(vec, three);\n            mask = _mm256_cmpeq_epi32(rem, _mm256_setzero_si256());\n            __m256i abs_vec = _mm256_abs_epi32(vec);\n            __m256i result = _mm256_blendv_epi8(vec, abs_vec, mask);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result);\n        }\n\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 7; j += 8) {\n            __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i * cols + j));\n            __m256i vMask = _mm256_set1_epi32(3);\n            __m256i vRem = _mm256_rem_epi32(vA, vMask);\n            __m256i vZero = _mm256_setzero_si256();\n            __m256i vComp = _mm256_cmpeq_epi32(vRem, vZero);\n            __m256i vAbs = _mm256_abs_epi32(vA);\n            __m256i vResult = _mm256_blendv_epi8(vA, vAbs, vComp);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), vResult);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 7; j += 8) {\n            __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i * cols + j));\n            __m256i vMask = _mm256_set1_epi32(3);\n            __m256i vRem = _mm256_rem_epi32(vA, vMask);\n            __m256i vZero = _mm256_setzero_si256();\n            __m256i vComp = _mm256_cmpeq_epi32(vRem, vZero);\n            __m256i vAbs = _mm256_abs_epi32(vA);\n            __m256i vResult = _mm256_blendv_epi8(vA, vAbs, vComp);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), vResult);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 7; j += 8) {\n            __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i * cols + j));\n            __m256i vMask = _mm256_set1_epi32(3);\n            __m256i vRem = _mm256_rem_epi32(vA, vMask);\n            __m256i vZero = _mm256_setzero_si256();\n            __m256i vComp = _mm256_cmpeq_epi32(vRem, vZero);\n            __m256i vAbs = _mm256_abs_epi32(vA);\n            __m256i vResult = _mm256_blendv_epi8(vA, vAbs, vComp);\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), vResult);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i mask = _mm256_set1_epi32(0xF);\n    __m256i rotate = _mm256_set1_epi32(rotate_bits);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i cond = _mm256_and_si256(srcVec, mask);\n        __m256i condEq = _mm256_cmpeq_epi32(cond, mask);\n\n        __m256i rotated = _mm256_or_si256(_mm256_srli_epi32(srcVec, rotate_bits), _mm256_slli_epi32(srcVec, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m256i negated = _mm256_xor_si256(srcVec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condEq);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t element = src[i];\n        if ((element & 0xF) == 0xF) {\n            dst[i] = (element >> rotate_bits) | (element << (32 - rotate_bits));\n        } else {\n            dst[i] = ~element;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm256_set1_epi32(0x0F);\n    __m256i rotate = _mm256_set1_epi32(rotate_bits);\n    size_t i;\n\n    for (i = 0; i + 7 < length; i += 8) {\n        __m256i srcVec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i lowBits = _mm256_and_si256(srcVec, mask);\n        __m256i cmp = _mm256_cmpeq_epi32(lowBits, mask);\n\n        __m256i rotated = _mm256_or_si256(_mm256_sllv_epi32(srcVec, rotate), _mm256_srli_epi32(_mm256_srlv_epi32(srcVec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate)), 32));\n        __m256i negated = _mm256_xor_si256(srcVec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, cmp);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t value = src[i];\n        if ((value & 0x0F) == 0x0F) {\n            dst[i] = (value >> rotate_bits) | (value << (32 - rotate_bits));\n        } else {\n            dst[i] = ~value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i mask = _mm256_set1_epi32(0xF);\n    __m256i rotate = _mm256_set1_epi32(rotate_bits);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i lowBits = _mm256_and_si256(srcVec, mask);\n        __m256i cmp = _mm256_cmpeq_epi32(lowBits, mask);\n\n        __m256i rotated = _mm256_or_si256(_mm256_sllv_epi32(srcVec, rotate), _mm256_srlv_epi32(srcVec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate)));\n        __m256i negated = _mm256_xor_si256(srcVec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, cmp);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i mask = _mm256_set1_epi32(0xF);\n    __m256i rotate = _mm256_set1_epi32(rotate_bits);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i lowBits = _mm256_and_si256(srcVec, mask);\n        __m256i cmpRes = _mm256_cmpeq_epi32(lowBits, mask);\n\n        __m256i rotated = _mm256_or_si256(_mm256_slli_epi32(srcVec, rotate), _mm256_srli_epi32(srcVec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate)));\n        __m256i negated = _mm256_xor_si256(srcVec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, cmpRes);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val << rotate_bits) | (val >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    size_t i;\n    __m256i mask = _mm256_set1_epi32(0xF);\n    __m256i rotate = _mm256_set1_epi32(rotate_bits);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i lowBits = _mm256_and_si256(srcVec, mask);\n        __m256i cmpRes = _mm256_cmpeq_epi32(lowBits, mask);\n\n        __m256i rotated = _mm256_or_si256(_mm256_sllv_epi32(srcVec, rotate), _mm256_srlv_epi32(srcVec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate)));\n        __m256i negated = _mm256_xor_si256(srcVec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, cmpRes);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t element = src[i];\n        if ((element & 0xF) == 0xF) {\n            dst[i] = (element >> rotate_bits) | (element << (32 - rotate_bits));\n        } else {\n            dst[i] = ~element;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256 result = _mm256_blendv_ps(_mm256_blendv_ps(zero, _mm256_set1_ps(-1.0f), cmp_lt), _mm256_set1_ps(1.0f), cmp_gt);\n\n        __m128i result_i = _mm256_cvtps_ph(result, _MM_FROUND_TRUNC);\n        _mm_storeu_si128((__m128i*)&out[i], result_i);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256 result = _mm256_blendv_ps(zero, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n\n        __m128i result_i = _mm256_cvtps_ph(result, _MM_FROUND_TRUNC);\n        _mm_storeu_si128((__m128i*)&out[i], result_i);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256 result = _mm256_blendv_ps(zero, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n\n        __m256i int_result = _mm256_cvtps_epi32(result);\n        _mm256_storeu_si256((__m256i*)&out[i], int_result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256 result = _mm256_blendv_ps(zero, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n\n        __m128i result_i = _mm256_cvtps_ph(result, _MM_FROUND_TRUNC);\n        _mm_storeu_si128((__m128i*)&out[i], result_i);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps = _mm256_set1_ps(epsilon);\n    __m256 neg_eps = _mm256_set1_ps(-epsilon);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff, eps, _CMP_GT_OQ);\n        __m256 cmp_lt = _mm256_cmp_ps(diff, neg_eps, _CMP_LT_OQ);\n\n        __m256 result = _mm256_blendv_ps(zero, _mm256_set1_ps(1.0f), cmp_gt);\n        result = _mm256_blendv_ps(result, _mm256_set1_ps(-1.0f), cmp_lt);\n\n        __m128i result_i = _mm256_cvtps_ph(result, _MM_FROUND_TRUNC);\n        _mm_storeu_si128((__m128i*)&out[i], result_i);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    __m256 max_val = _mm256_set1_ps(INT32_MAX);\n    __m256 min_val = _mm256_set1_ps(INT32_MIN);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 float_val = _mm256_loadu_ps(&A[i]);\n        __m256 max_cmp = _mm256_max_ps(float_val, min_val);\n        __m256 min_cmp = _mm256_min_ps(max_cmp, max_val);\n        __m256i int_val = _mm256_cvtps_epi32(min_cmp);\n        _mm256_storeu_si256((__m256i*)&B[i], int_val);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    __m256 float_vec;\n    __m256i int_vec;\n    __m256 max_val = _mm256_set1_ps(INT32_MAX);\n    __m256 min_val = _mm256_set1_ps(INT32_MIN);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        float_vec = _mm256_loadu_ps(&A[i]);\n        float_vec = _mm256_min_ps(float_vec, max_val);\n        float_vec = _mm256_max_ps(float_vec, min_val);\n        int_vec = _mm256_cvtps_epi32(float_vec);\n        _mm256_storeu_si256((__m256i*)&B[i], int_vec);\n    }\n\n    for (; i < size; ++i) {\n        float value = A[i];\n        if (value > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (value < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    __m256 max_val = _mm256_set1_ps(INT32_MAX);\n    __m256 min_val = _mm256_set1_ps(INT32_MIN);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 float_val = _mm256_loadu_ps(&A[i]);\n        float_val = _mm256_min_ps(float_val, max_val);\n        float_val = _mm256_max_ps(float_val, min_val);\n        __m256i int_val = _mm256_cvtps_epi32(float_val);\n        _mm256_storeu_si256((__m256i*)&B[i], int_val);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    __m256 vmax = _mm256_set1_ps(INT32_MAX);\n    __m256 vmin = _mm256_set1_ps(INT32_MIN);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 vfloat = _mm256_loadu_ps(&A[i]);\n        vfloat = _mm256_min_ps(vfloat, vmax);\n        vfloat = _mm256_max_ps(vfloat, vmin);\n        __m256i vint = _mm256_cvtps_epi32(vfloat);\n        _mm256_storeu_si256((__m256i*)&B[i], vint);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i;\n    __m256 max_val = _mm256_set1_ps(INT32_MAX);\n    __m256 min_val = _mm256_set1_ps(INT32_MIN);\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 v = _mm256_loadu_ps(&A[i]);\n        v = _mm256_min_ps(v, max_val);\n        v = _mm256_max_ps(v, min_val);\n        __m256i int_v = _mm256_cvtps_epi32(v);\n        _mm256_storeu_si256((__m256i*)&B[i], int_v);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vector = _mm256_set1_epi32(scale);\n    __m256 scale_float_vector = _mm256_cvtepi32_ps(_mm256_set1_epi32(scale));\n\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i int_vector = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vector = _mm256_cvtepi32_ps(int_vector);\n        float_vector = _mm256_mul_ps(float_vector, scale_float_vector);\n        _mm256_storeu_ps(B + i, float_vector);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vector = _mm256_set1_epi32(scale);\n    __m256 scale_float = _mm256_cvtepi32_ps(scale_vector);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i int_vector = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vector = _mm256_cvtepi32_ps(int_vector);\n        float_vector = _mm256_mul_ps(float_vector, scale_float);\n        _mm256_storeu_ps(B + i, float_vector);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vector = _mm256_set1_epi32(scale);\n    __m256 scale_float_vector = _mm256_cvtepi32_ps(_mm256_set1_epi32(scale));\n\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i int_vector = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vector = _mm256_cvtepi32_ps(int_vector);\n        float_vector = _mm256_mul_ps(float_vector, scale_float_vector);\n        _mm256_storeu_ps(B + i, float_vector);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256 scale_float = _mm256_cvtepi32_ps(vscale);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vA_float = _mm256_cvtepi32_ps(vA);\n        __m256 vB = _mm256_mul_ps(vA_float, scale_float);\n        _mm256_storeu_ps(B + i, vB);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256 scale_float = _mm256_cvtepi32_ps(vscale);\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vA_float = _mm256_cvtepi32_ps(vA);\n        __m256 vB = _mm256_mul_ps(vA_float, scale_float);\n        _mm256_storeu_ps(B + i, vB);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vc = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vc);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vc = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vc);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vc = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vc);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vc = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vc);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vc = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vc);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d v = _mm256_loadu_pd(&A[i]);\n        v = _mm256_min_pd(v, vmax);\n        v = _mm256_max_pd(v, vmin);\n        __m128 f = _mm256_cvtpd_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d vA = _mm256_loadu_pd(&A[i]);\n        vA = _mm256_min_pd(vA, vmax);\n        vA = _mm256_max_pd(vA, vmin);\n        __m128 fA1 = _mm256_cvtpd_ps(vA);\n        _mm_storeu_ps(&B[i], fA1);\n        __m128 fA2 = _mm256_extractf128_ps(vA, 1);\n        _mm_storeu_ps(&B[i + 4], fA2);\n    }\n\n    for (; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm256_set1_pd(FLT_MAX);\n    __m256d min_val = _mm256_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n        a = _mm256_min_pd(a, max_val);\n        a = _mm256_max_pd(a, min_val);\n        __m128 f = _mm256_cvtpd_ps(a);\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            val = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            val = -FLT_MAX;\n        }\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d v = _mm256_loadu_pd(&A[i]);\n        v = _mm256_min_pd(v, vmax);\n        v = _mm256_max_pd(v, vmin);\n        __m128 fv = _mm256_cvtpd_ps(v);\n        _mm_storeu_ps(&B[i], fv);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m256d v = _mm256_loadu_pd(&A[i]);\n        v = _mm256_min_pd(v, vmax);\n        v = _mm256_max_pd(v, vmin);\n        __m128 f = _mm256_cvtpd_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 15 < size; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 0));\n        __m256i hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 15 < size; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 0));\n        __m256i hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 15 < size; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 0));\n        __m256i hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 15 < size; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 0));\n        __m256i hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 15 < size; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i lo = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 0));\n        __m256i hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v, 1));\n        _mm256_storeu_si256((__m256i*)(B + i), lo);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_v = _mm256_set1_ps(min_val);\n    __m256 max_val_v = _mm256_set1_ps(max_val);\n    __m256 range_v = _mm256_set1_ps(max_val - min_val);\n    __m256 zero_v = _mm256_set1_ps(0.0f);\n    __m256 one_v = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 A_v = _mm256_loadu_ps(&A[i]);\n        __m256i control_v = _mm256_loadu_si256((__m256i const*)&control[i]);\n        __m256 control_mask = _mm256_cmpgt_epi32(control_v, _mm256_setzero_si256());\n\n        __m256 normalized_v = _mm256_div_ps(_mm256_sub_ps(A_v, min_val_v), range_v);\n        normalized_v = _mm256_max_ps(_mm256_min_ps(normalized_v, one_v), zero_v);\n\n        __m256 result_v = _mm256_blendv_ps(A_v, normalized_v, _mm256_castsi256_ps(control_mask));\n\n        _mm256_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_v = _mm256_set1_ps(min_val);\n    __m256 max_val_v = _mm256_set1_ps(max_val);\n    __m256 range_v = _mm256_sub_ps(max_val_v, min_val_v);\n    __m256 zero_v = _mm256_set1_ps(0.0f);\n    __m256 one_v = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 A_v = _mm256_loadu_ps(&A[i]);\n        __m256i control_v = _mm256_loadu_si256((__m256i const*)&control[i]);\n        __m256 control_mask = _mm256_cmpgt_epi32(control_v, _mm256_setzero_si256());\n\n        __m256 normalized_v = _mm256_sub_ps(A_v, min_val_v);\n        normalized_v = _mm256_div_ps(normalized_v, range_v);\n        normalized_v = _mm256_max_ps(normalized_v, zero_v);\n        normalized_v = _mm256_min_ps(normalized_v, one_v);\n\n        __m256 result_v = _mm256_blendv_ps(A_v, normalized_v, _mm256_castsi256_ps(control_mask));\n        _mm256_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_v = _mm256_set1_ps(min_val);\n    __m256 max_val_v = _mm256_set1_ps(max_val);\n    __m256 range_v = _mm256_sub_ps(max_val_v, min_val_v);\n    __m256 zero_v = _mm256_set1_ps(0.0f);\n    __m256 one_v = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 A_v = _mm256_loadu_ps(&A[i]);\n        __m256i control_v = _mm256_loadu_si256((__m256i const*)&control[i]);\n        __m256 control_mask = _mm256_cmpgt_epi32(control_v, _mm256_setzero_si256());\n\n        __m256 normalized_v = _mm256_sub_ps(A_v, min_val_v);\n        normalized_v = _mm256_div_ps(normalized_v, range_v);\n        normalized_v = _mm256_max_ps(normalized_v, zero_v);\n        normalized_v = _mm256_min_ps(normalized_v, one_v);\n\n        __m256 result_v = _mm256_blendv_ps(A_v, normalized_v, _mm256_castsi256_ps(control_mask));\n        _mm256_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_v = _mm256_set1_ps(min_val);\n    __m256 max_val_v = _mm256_set1_ps(max_val);\n    __m256 range_v = _mm256_sub_ps(max_val_v, min_val_v);\n    __m256 zero_v = _mm256_set1_ps(0.0f);\n    __m256 one_v = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 a_v = _mm256_loadu_ps(&A[i]);\n        __m256i control_v = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 control_mask = _mm256_cmp_ps(_mm256_cvtepi32_ps(control_v), zero_v, _CMP_GT_OQ);\n\n        __m256 norm_v = _mm256_sub_ps(a_v, min_val_v);\n        norm_v = _mm256_div_ps(norm_v, range_v);\n        norm_v = _mm256_max_ps(norm_v, zero_v);\n        norm_v = _mm256_min_ps(norm_v, one_v);\n\n        __m256 result_v = _mm256_blendv_ps(a_v, norm_v, control_mask);\n\n        _mm256_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_v = _mm256_set1_ps(min_val);\n    __m256 max_val_v = _mm256_set1_ps(max_val);\n    __m256 range_v = _mm256_sub_ps(max_val_v, min_val_v);\n    __m256 zero_v = _mm256_set1_ps(0.0f);\n    __m256 one_v = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 A_v = _mm256_loadu_ps(&A[i]);\n        __m256i control_v = _mm256_loadu_si256((__m256i*)&control[i]);\n        __m256 mask_v = _mm256_cmp_ps(_mm256_cvtepi32_ps(control_v), zero_v, _CMP_GT_OQ);\n\n        __m256 norm_v = _mm256_sub_ps(A_v, min_val_v);\n        norm_v = _mm256_div_ps(norm_v, range_v);\n        norm_v = _mm256_max_ps(norm_v, zero_v);\n        norm_v = _mm256_min_ps(norm_v, one_v);\n\n        __m256 result_v = _mm256_blendv_ps(A_v, norm_v, mask_v);\n        _mm256_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vHigh = _mm256_srli_epi32(vA, 16);\n        __m256 vFloat = _mm256_cvtepi32_ps(vHigh);\n        __m256 vSqrt = _mm256_sqrt_ps(vFloat);\n        _mm256_storeu_ps(B + i, vSqrt);\n    }\n    for (; i < size; ++i) {\n        uint32_t high = A[i] >> 16;\n        float f = (float)high;\n        B[i] = sqrtf(f);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vHigh = _mm256_srli_epi32(vA, 16);\n        __m256 vFloat = _mm256_cvtepi32_ps(vHigh);\n        __m256 vSqrt = _mm256_sqrt_ps(vFloat);\n        _mm256_storeu_ps(B + i, vSqrt);\n    }\n    for (; i < size; ++i) {\n        uint32_t value = A[i];\n        float high = (float)((value >> 16) & 0xFFFF);\n        B[i] = sqrtf(high);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vA_high = _mm256_srli_epi32(vA, 16);\n        __m256 vA_float = _mm256_cvtepi32_ps(vA_high);\n        __m256 vB = _mm256_sqrt_ps(vA_float);\n        _mm256_storeu_ps(B + i, vB);\n    }\n    for (; i < size; ++i) {\n        uint32_t value = A[i];\n        float high_part = (value >> 16);\n        B[i] = sqrtf(high_part);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i high16 = _mm256_srli_epi32(vA, 16);\n        __m256 fA = _mm256_cvtepi32_ps(high16);\n        __m256 fB = _mm256_sqrt_ps(fA);\n        _mm256_storeu_ps(&B[i], fB);\n    }\n    for (; i < size; ++i) {\n        uint32_t value = A[i] >> 16;\n        float fValue = (float)value;\n        B[i] = sqrtf(fValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i high16 = _mm256_srli_epi32(vA, 16);\n        __m256 fA = _mm256_cvtepi32_ps(high16);\n        __m256 fB = _mm256_sqrt_ps(fA);\n        _mm256_storeu_ps(B + i, fB);\n    }\n    for (; i < size; ++i) {\n        uint32_t value = A[i];\n        float high16 = (value >> 16);\n        B[i] = sqrtf(high16);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vB = _mm256_castsi256_ps(vA);\n        _mm256_storeu_ps(B + i, vB);\n    }\n    for (; i < size; ++i) {\n        __m128i vA = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 vB = _mm_castsi128_ps(vA);\n        _mm_storeu_ps(B + i, vB);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vB = _mm256_castsi256_ps(vA);\n        _mm256_storeu_ps(B + i, vB);\n    }\n    for (; i < size; ++i) {\n        __m128i vA = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 vB = _mm_castsi128_ps(vA);\n        _mm_storeu_ps(B + i, vB);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 fv = _mm256_castsi256_ps(v);\n        _mm256_storeu_ps(B + i, fv);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 fv = _mm_castsi128_ps(v);\n        _mm_storeu_ps(B + i, fv);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i vA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vB = _mm256_castsi256_ps(vA);\n        _mm256_storeu_ps(B + i, vB);\n    }\n    for (; i < size; ++i) {\n        __m128i vA = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 vB = _mm_castsi128_ps(vA);\n        _mm_storeu_ps(B + i, vB);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 7 < size; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 f = _mm256_castsi256_ps(v);\n        _mm256_storeu_ps(B + i, f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(B + i, f);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i prev = _mm256_set1_epi32(0);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i mask = _mm256_loadu_si256((__m256i const*)(cond + i));\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        diff_float = _mm256_and_ps(_mm256_castsi256_ps(mask), diff_float);\n\n        _mm256_storeu_ps(diff + i, diff_float);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    __m256i prev = _mm256_set1_epi32(A[0]);\n    __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 1; i < size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n        __m256 mask = _mm256_loadu_ps((float const*)(cond + i));\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        diff_float = _mm256_blendv_ps(zero, diff_float, mask);\n        _mm256_storeu_ps(diff + i, diff_float);\n        prev = curr;\n    }\n\n    for (size_t i = size; i < size + 7; ++i) {\n        if (i < size) {\n            diff[i] = cond[i] ? (float)(A[i] - A[i - 1]) : 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i prev = _mm256_set1_epi32(0);\n    __m256 zero = _mm256_set1_ps(0.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i mask = _mm256_loadu_si256((__m256i const*)(cond + i));\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        diff_float = _mm256_and_ps(_mm256_castsi256_ps(mask), diff_float);\n\n        _mm256_storeu_ps(diff + i, diff_float);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i prev = _mm256_setzero_si256();\n    __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i prev_shifted = _mm256_alignr_epi8(prev, curr, 4);\n        __m256i diff_int = _mm256_sub_epi32(curr, prev_shifted);\n\n        __m256 mask = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i const*)(cond + i)));\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        diff_float = _mm256_blendv_ps(zero, diff_float, mask);\n\n        _mm256_storeu_ps(diff + i, diff_float);\n\n        prev = curr;\n    }\n\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    diff[0] = 0.0f;\n    __m256i prev_val = _mm256_set1_epi32(A[0]);\n\n    for (size_t i = 1; i < size; i += 8) {\n        __m256i curr_val = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 cond_mask = _mm256_loadu_ps((float const*)(cond + i));\n\n        __m256i diff_val = _mm256_sub_epi32(curr_val, prev_val);\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_val);\n\n        __m256 zero_float = _mm256_setzero_ps();\n        diff_float = _mm256_blendv_ps(zero_float, diff_float, cond_mask);\n\n        _mm256_storeu_ps(diff + i, diff_float);\n\n        prev_val = curr_val;\n    }\n\n    for (size_t i = size; i < size + 7; ++i) {\n        if (i < size) {\n            diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m256i ones = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_cvtepu32_epi64(_mm_loadu_si128((__m128i*)&src[i]));\n        __m256i result = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m256i ones = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_cvtepu32_epi64(_mm_loadu_si128((__m128i*)&src[i]));\n        __m256i dst_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m256i high32bits = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_cvtepu32_epi64(_mm_loadu_si128((__m128i*)&src[i]));\n        __m256i result = _mm256_or_si256(srcVec, high32bits);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n    __m256i ones = _mm256_set1_epi64x(0xFFFFFFFF);\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m256i dst_vec = _mm256_cvtepu32_epi64(src_vec);\n        dst_vec = _mm256_or_si256(dst_vec, mask);\n        dst_vec = _mm256_add_epi64(dst_vec, ones);\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m256i mask = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m256i srcVec = _mm256_cvtepu32_epi64(_mm_loadu_si128((__m128i const*)(src + i)));\n        __m256i result = _mm256_or_si256(srcVec, mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256i idx_256 = _mm256_cvtepu16_epi32(idx);\n\n        __m256d vec0 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 0), sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 1), sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec0);\n        sum = _mm256_add_pd(sum, vec1);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256i idx_256 = _mm256_cvtepu16_epi32(idx);\n\n        __m256d vec0 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 0), sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 1), sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec0);\n        sum = _mm256_add_pd(sum, vec1);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    __m128i indices = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 4) {\n        indices = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256i idx = _mm256_cvtepu16_epi32(indices);\n\n        __m256d vec0 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx, 0), sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx, 1), sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec0);\n        sum = _mm256_add_pd(sum, vec1);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256i idx_256 = _mm256_cvtepu16_epi32(idx);\n\n        __m256d vec1 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 0), sizeof(double));\n        __m256d vec2 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 1), sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec1);\n        sum = _mm256_add_pd(sum, vec2);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m256i idx_256 = _mm256_cvtepu16_epi32(idx);\n\n        __m256d vec0 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 0), sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(vec, _mm256_extractf128_si256(idx_256, 1), sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec0);\n        sum = _mm256_add_pd(sum, vec1);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t i, j, k, l;\n\n    for (i = 0; i < output_size; ++i) {\n        for (j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (k = 0; k < kernel_size; ++k) {\n                for (l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_size + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m256d zero = _mm256_setzero_pd();\n            __m256d relu_mask = _mm256_cmp_pd(sum, zero, _CMP_GT_OQ);\n            sum = _mm256_and_pd(sum, relu_mask);\n\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n\n            output[i * output_size + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m256d zero = _mm256_setzero_pd();\n            sum = _mm256_max_pd(sum, zero);\n\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            output[i * output_stride + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    __m256d zeros = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; l += 4) {\n                    __m256d kernel_val = _mm256_loadu_pd(&kernel[k * kernel_size + l]);\n                    __m256d input_val = _mm256_loadu_pd(&input[(i + k) * input_stride + j + l]);\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(kernel_val, input_val));\n                }\n            }\n\n            __m256d result = _mm256_max_pd(sum, zeros);\n            double temp[4];\n            _mm256_storeu_pd(temp, result);\n            output[i * output_stride + j] = temp[0] + temp[1] + temp[2] + temp[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            double temp[4];\n            _mm256_storeu_pd(temp, sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n            result = result > 0 ? result : 0;\n\n            output[i * output_stride + j] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m256d input_val = _mm256_set1_pd(input[input_index]);\n                    __m256d kernel_val = _mm256_set1_pd(kernel[kernel_index]);\n\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m256d zero = _mm256_setzero_pd();\n            __m256d relu_mask = _mm256_cmp_pd(sum, zero, _CMP_GT_OS);\n            sum = _mm256_and_pd(sum, relu_mask);\n\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n\n            output[i * output_stride + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n    __m256d prev_v = _mm256_loadu_pd(&numbers[0]);\n\n    for (size_t i = 4; i < n; i += 4) {\n        __m256d current_v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff_v = _mm256_sub_pd(current_v, prev_v);\n        __m256d abs_diff_v = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff_v);\n\n        __m256d cmp_v = _mm256_cmp_pd(abs_diff_v, threshold_v, _CMP_LT_OQ);\n        if (_mm256_movemask_pd(cmp_v) != 0) {\n            return true;\n        }\n\n        prev_v = current_v;\n    }\n\n    for (size_t i = n - (n % 4); i < n; ++i) {\n        for (size_t j = i + 1; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    int n = numbers.size();\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n    __m256d min_diff_v = _mm256_set1_pd(DBL_MAX);\n\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n\n        for (int j = i + 4; j < n; j += 4) {\n            __m256d vec2 = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n\n            __m256d min_diff = _mm256_min_pd(abs_diff, min_diff_v);\n            __m256d mask = _mm256_cmp_pd(min_diff, threshold_v, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(mask) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    size_t n = numbers.size();\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n    __m256d inf_v = _mm256_set1_pd(std::numeric_limits<double>::infinity());\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d vi = _mm256_loadu_pd(&numbers[i]);\n\n        for (size_t j = i + 4; j < n; j += 4) {\n            __m256d vj = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vi, vj);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d min_diff = _mm256_min_pd(abs_diff, inf_v);\n            __m256d cmp = _mm256_cmp_pd(min_diff, threshold_v, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    size_t n = numbers.size();\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d zero_v = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d vi = _mm256_loadu_pd(&numbers[i]);\n\n        for (size_t j = i + 4; j < n; j += 4) {\n            __m256d vj = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vi, vj);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_v, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n\n        for (size_t j = i + 1; j < i + 4 && j < n; ++j) {\n            double diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    size_t n = numbers.size();\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d zero_v = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d vi = _mm256_loadu_pd(&numbers[i]);\n\n        for (size_t j = i + 4; j < n; j += 4) {\n            __m256d vj = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vi, vj);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_v, _CMP_LT_OQ);\n\n            if (_mm256_movemask_pd(cmp) != 0) {\n                return true;\n            }\n        }\n\n        for (size_t j = i + 1; j < i + 4 && j < n; ++j) {\n            double diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i negative = _mm256_set1_epi32(0x80000000);\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        sum = _mm256_add_epi32(sum, chunk);\n\n        __m256i cmp = _mm256_cmpgt_epi32(zero, sum);\n        if (_mm256_movemask_epi8(_mm256_xor_si256(cmp, negative)) != 0) {\n            return true;\n        }\n    }\n\n    int remaining = size % 8;\n    if (remaining > 0) {\n        __m256i chunk = _mm256_maskload_epi32(&operations[size - remaining], _mm256_set1_epi32(0xFFFFFFFF));\n        sum = _mm256_add_epi32(sum, chunk);\n\n        __m256i cmp = _mm256_cmpgt_epi32(zero, sum);\n        if (_mm256_movemask_epi8(_mm256_xor_si256(cmp, negative)) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n    __m256i neg_mask = _mm256_set1_epi32(0x80000000);\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&operations[i]);\n        sum = _mm256_add_epi32(sum, chunk);\n\n        __m256i neg_check = _mm256_xor_si256(sum, neg_mask);\n        __m256i below_zero = _mm256_cmpgt_epi32(zero, neg_check);\n\n        if (_mm256_movemask_epi8(_mm256_packs_epi32(below_zero, below_zero)) != 0) {\n            return true;\n        }\n    }\n\n    int32_t balance[8];\n    _mm256_storeu_si256((__m256i*)balance, sum);\n    int total_balance = balance[0] + balance[1] + balance[2] + balance[3] + balance[4] + balance[5] + balance[6] + balance[7];\n\n    return total_balance < 0;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    int balance = 0;\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n        __m256i balance_vec = _mm256_set1_epi32(balance);\n        __m256i new_balance_vec = _mm256_add_epi32(balance_vec, vec);\n\n        __m256i min_balance_vec = _mm256_min_epi32(balance_vec, new_balance_vec);\n        int min_balance[8];\n        _mm256_storeu_si256((__m256i*)min_balance, min_balance_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (min_balance[j] < 0) {\n                return true;\n            }\n        }\n\n        int temp_balance[8];\n        _mm256_storeu_si256((__m256i*)temp_balance, new_balance_vec);\n        balance = temp_balance[7];\n    }\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    int i = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i <= size - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        sum = _mm256_add_epi32(sum, ops);\n        __m256i mask = _mm256_cmpgt_epi32(zero, sum);\n        if (!_mm256_testz_si256(mask, mask)) {\n            return true;\n        }\n    }\n\n    int balance = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                  _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                  _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                  _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    int i = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    for (; i <= size - 8; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&operations[i]);\n        sum = _mm256_add_epi32(sum, chunk);\n\n        __m256i mask = _mm256_cmpgt_epi32(zero, sum);\n        int subMask = _mm256_movemask_ps((__m256)mask);\n        if (subMask != 0) {\n            return true;\n        }\n    }\n\n    int balance = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                  _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                  _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                  _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n    for (; i < size; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    double sum = 0.0;\n    for (size_t i = 0; i < n; ++i) {\n        sum += numbers[i];\n    }\n    double mean = sum / n;\n\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d vsum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m256d vnumbers = _mm256_loadu_pd(&numbers[i]);\n        __m256d vdiff = _mm256_sub_pd(vnumbers, vmean);\n        __m256d vabsdiff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), vdiff);\n        vsum = _mm256_add_pd(vsum, vabsdiff);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, vsum);\n    double mad_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    double sum = 0.0;\n    __m256d vsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        vsum = _mm256_add_pd(vsum, v);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    double mean = sum / n;\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d vabsdiff = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d vdiff = _mm256_sub_pd(v, vmean);\n        vabsdiff = _mm256_add_pd(vabsdiff, _mm256_andnot_pd(_mm256_set1_pd(-0.0), vdiff));\n    }\n\n    _mm256_storeu_pd(temp, vabsdiff);\n    double mad = (temp[0] + temp[1] + temp[2] + temp[3]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    double sum = 0.0;\n    __m256d vsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        vsum = _mm256_add_pd(vsum, v);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    double mean = sum / n;\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d vmadsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d vdiff = _mm256_sub_pd(v, vmean);\n        __m256d vabsdiff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), vdiff);\n        vmadsum = _mm256_add_pd(vmadsum, vabsdiff);\n    }\n\n    _mm256_storeu_pd(temp, vmadsum);\n    double madsum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return madsum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    double sum = 0.0;\n    __m256d vsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        vsum = _mm256_add_pd(vsum, v);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    double mean = sum / n;\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d vmadsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d vdiff = _mm256_sub_pd(v, vmean);\n        __m256d vabsdiff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), vdiff);\n        vmadsum = _mm256_add_pd(vmadsum, vabsdiff);\n    }\n\n    _mm256_storeu_pd(temp, vmadsum);\n    double madsum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return madsum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    double sum = 0.0;\n    __m256d vsum = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        vsum = _mm256_add_pd(vsum, v);\n    }\n\n    double temp[4];\n    _mm256_storeu_pd(temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    double mean = sum / n;\n    __m256d vmean = _mm256_set1_pd(mean);\n    __m256d vabsdiff = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d vdiff = _mm256_sub_pd(v, vmean);\n        vabsdiff = _mm256_add_pd(vabsdiff, _mm256_andnot_pd(_mm256_set1_pd(-0.0), vdiff));\n    }\n\n    _mm256_storeu_pd(temp, vabsdiff);\n    double mad = (temp[0] + temp[1] + temp[2] + temp[3]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimiter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    if (numbers.empty()) {\n        return result;\n    }\n\n    size_t i = 0;\n    __m256i delimiter_vec = _mm256_set1_epi32(delimiter);\n\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256i num_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i mask = _mm256_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n        __m256i interspersed = _mm256_blendv_epi8(delimiter_vec, num_vec, mask);\n\n        int32_t temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), interspersed);\n\n        for (int j = 0; j < 8; ++j) {\n            result.push_back(temp[j]);\n        }\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i < numbers.size() - 1) {\n            result.push_back(delimiter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    int i = 0;\n    for (; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i del = _mm_set1_epi32(delimeter);\n\n        __m128i res1 = _mm_unpacklo_epi32(num, del);\n        __m128i res2 = _mm_unpackhi_epi32(num, del);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], res1);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], res2);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    int i = 0;\n    for (; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i del = _mm_set1_epi32(delimeter);\n\n        __m128i res1 = _mm_unpacklo_epi32(num, del);\n        __m128i res2 = _mm_unpackhi_epi32(num, del);\n\n        _mm_storeu_si128((__m128i*)&result[i * 2], res1);\n        _mm_storeu_si128((__m128i*)&result[i * 2 + 4], res2);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    int i = 0;\n    for (; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i del = _mm_set1_epi32(delimeter);\n\n        __m128i res1 = _mm_shuffle_epi32(num, _MM_SHUFFLE(1, 0, 3, 2));\n        __m128i res2 = _mm_blend_epi16(num, del, 0b10101010);\n        __m128i res3 = _mm_blend_epi16(res1, del, 0b01010101);\n\n        _mm_storeu_si128((__m128i*)&result[2*i], res2);\n        _mm_storeu_si128((__m128i*)&result[2*i + 4], res3);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    int i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256i num_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i del_vec = _mm256_set1_epi32(delimeter);\n\n        __m256i mask = _mm256_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n        __m256i inter_vec = _mm256_blendv_epi8(del_vec, num_vec, mask);\n\n        int* inter_arr = (int*)&inter_vec;\n        for (int j = 0; j < 8; ++j) {\n            result.push_back(inter_arr[j]);\n        }\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i < numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i temp_prod;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i vec_lo = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec));\n        __m256i vec_hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec, 1));\n\n        sum = _mm256_add_epi32(sum, vec_lo);\n        sum = _mm256_add_epi32(sum, vec_hi);\n\n        temp_prod = _mm256_mullo_epi32(prod, vec_lo);\n        prod = _mm256_mullo_epi32(temp_prod, vec_hi);\n    }\n\n    int64_t sum_total = 0;\n    int64_t prod_total = 1;\n\n    int32_t temp_sum[8];\n    int32_t temp_prod[8];\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum), sum);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_prod), prod);\n\n    for (int j = 0; j < 8; ++j) {\n        sum_total += temp_sum[j];\n        prod_total *= temp_prod[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        sum_total += numbers[i];\n        prod_total *= numbers[i];\n    }\n\n    return {sum_total, prod_total};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i temp_prod;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 16; i += 16) {\n        __m256i num = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum = _mm256_add_epi16(sum, num);\n\n        temp_prod = _mm256_mullo_epi16(prod, num);\n        prod = _mm256_mulhi_epi16(temp_prod, _mm256_set1_epi16(1));\n    }\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n\n    int16_t temp_sum[16] __attribute__((aligned(32)));\n    int16_t temp_prod[16] __attribute__((aligned(32)));\n\n    _mm256_store_si256((__m256i*)temp_sum, sum);\n    _mm256_store_si256((__m256i*)temp_prod, prod);\n\n    for (int j = 0; j < 16; ++j) {\n        total_sum += temp_sum[j];\n        total_prod *= temp_prod[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i temp_prod;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 16; i += 16) {\n        __m256i num = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum = _mm256_add_epi16(sum, num);\n\n        temp_prod = _mm256_mullo_epi16(prod, num);\n        prod = _mm256_mulhi_epi16(temp_prod, num);\n    }\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n\n    int16_t sum_array[16];\n    int16_t prod_array[16];\n\n    _mm256_storeu_si256((__m256i*)sum_array, sum);\n    _mm256_storeu_si256((__m256i*)prod_array, prod);\n\n    for (int j = 0; j < 16; ++j) {\n        total_sum += sum_array[j];\n        total_prod *= prod_array[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i temp_prod;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i vec_lo = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(vec, 0));\n        __m256i vec_hi = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(vec, 1));\n\n        sum = _mm256_add_epi32(sum, vec_lo);\n        sum = _mm256_add_epi32(sum, vec_hi);\n\n        temp_prod = _mm256_mullo_epi32(prod, vec_lo);\n        prod = _mm256_mullo_epi32(temp_prod, vec_hi);\n    }\n\n    int32_t sum_arr[8] = {0};\n    int32_t prod_arr[8] = {1};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_arr), sum);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(prod_arr), prod);\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_arr[j];\n        total_prod *= prod_arr[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i temp_prod;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i vec_lo = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(vec, 0));\n        __m256i vec_hi = _mm256_cvtepu16_epi32(_mm256_extracti128_si256(vec, 1));\n\n        sum = _mm256_add_epi32(sum, vec_lo);\n        sum = _mm256_add_epi32(sum, vec_hi);\n\n        temp_prod = _mm256_mullo_epi32(prod, vec_lo);\n        prod = _mm256_mullo_epi32(temp_prod, vec_hi);\n    }\n\n    int32_t sum_array[8] = {0};\n    int32_t prod_array[8] = {1};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_array), sum);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(prod_array), prod);\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n        total_prod *= prod_array[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m256i max_vec = _mm256_set1_epi32(INT_MIN);\n    __m256i current_vec;\n\n    for (size_t i = 0; i < numbers.size(); i += 8) {\n        size_t remaining = numbers.size() - i;\n        if (remaining >= 8) {\n            current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, current_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        } else {\n            std::vector<int> temp(numbers.begin() + i, numbers.end());\n            temp.resize(8, INT_MIN);\n            current_vec = _mm256_loadu_si256((__m256i*)&temp[0]);\n            max_vec = _mm256_max_epi32(max_vec, current_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n            break;\n        }\n    }\n\n    int max_val = INT_MIN;\n    for (size_t i = 0; i < result.size(); ++i) {\n        max_val = std::max(max_val, result[i]);\n        result[i] = max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); i += 8) {\n        size_t j = i;\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[j]));\n        max_vec = _mm256_max_epi32(max_vec, vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[j]), max_vec);\n\n        int max_val = _mm256_extract_epi32(max_vec, 7);\n        for (size_t k = 0; k < 8 && j < numbers.size(); ++k, ++j) {\n            if (j < numbers.size()) {\n                max_val = std::max(max_val, numbers[j]);\n                result[j] = max_val;\n            }\n        }\n        max_vec = _mm256_set1_epi32(max_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m256i max_vec = _mm256_set1_epi32(INT_MIN);\n    __m256i curr_vec;\n\n    for (size_t i = 0; i < numbers.size(); i += 8) {\n        size_t remaining = numbers.size() - i;\n        if (remaining >= 8) {\n            curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n            max_vec = _mm256_max_epi32(max_vec, curr_vec);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), max_vec);\n        } else {\n            std::vector<int> temp(numbers.begin() + i, numbers.end());\n            temp.resize(8, INT_MIN);\n            curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp.data()));\n            max_vec = _mm256_max_epi32(max_vec, curr_vec);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp.data()), max_vec);\n            std::copy(temp.begin(), temp.begin() + remaining, result.begin() + i);\n            break;\n        }\n    }\n\n    int rolling_max = INT_MIN;\n    for (size_t i = 0; i < result.size(); ++i) {\n        rolling_max = std::max(rolling_max, result[i]);\n        result[i] = rolling_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m256i max_vec = _mm256_set1_epi32(INT_MIN);\n    __m256i current_vec;\n\n    for (size_t i = 0; i < numbers.size(); i += 8) {\n        size_t remaining = numbers.size() - i;\n        if (remaining >= 8) {\n            current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n            max_vec = _mm256_max_epi32(max_vec, current_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n        } else {\n            std::vector<int> temp(numbers.begin() + i, numbers.end());\n            temp.resize(8, INT_MIN);\n            current_vec = _mm256_loadu_si256((__m256i*)&temp[0]);\n            max_vec = _mm256_max_epi32(max_vec, current_vec);\n            _mm256_storeu_si256((__m256i*)&result[i], max_vec);\n            break;\n        }\n    }\n\n    int max_val = INT_MIN;\n    for (size_t i = 0; i < result.size(); ++i) {\n        max_val = std::max(max_val, result[i]);\n        result[i] = max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); i += 8) {\n        size_t j = i;\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[j]));\n        max_vec = _mm256_max_epi32(max_vec, vec);\n\n        __m256i mask = _mm256_cmpgt_epi32(max_vec, vec);\n        max_vec = _mm256_blendv_epi8(vec, max_vec, mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[j]), max_vec);\n\n        int max_val = _mm256_extract_epi32(max_vec, 7);\n        for (size_t k = 0; k < 8 && j + k < numbers.size(); ++k) {\n            result[j + k] = std::max(numbers[j + k], max_val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i vb = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), vx);\n    }\n\n    for (; i < length; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i av = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i bv = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i rv = _mm256_xor_si256(av, bv);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), rv);\n    }\n\n    for (; i < length; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i av = _mm256_loadu_si256((__m256i*)&a[i]);\n        __m256i bv = _mm256_loadu_si256((__m256i*)&b[i]);\n        __m256i xor_result = _mm256_xor_si256(av, bv);\n        _mm256_storeu_si256((__m256i*)&result[i], xor_result);\n    }\n\n    for (; i < length; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    std::string result(a.size(), ' ');\n\n    size_t i = 0;\n    for (; i <= a.size() - 32; i += 32) {\n        __m256i av = _mm256_loadu_si256((__m256i*)(a.data() + i));\n        __m256i bv = _mm256_loadu_si256((__m256i*)(b.data() + i));\n        __m256i xorv = _mm256_xor_si256(av, bv);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), xorv);\n    }\n\n    for (; i < a.size(); ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    std::string result(a.size(), ' ');\n    int i = 0;\n    for (; i <= a.size() - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&a[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&b[i]);\n        __m256i vx = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256((__m256i*)&result[i], vx);\n    }\n    for (; i < a.size(); ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, 0, 0, 0, 0);\n\n        for (size_t j = 0; j < 4 && i + j < strings.size(); ++j) {\n            size_t len = strings[i + j].length();\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(len), j);\n        }\n\n        __m256i max_len = _mm256_set1_epi32(static_cast<int>(max_length));\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_len);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i max_len_vec = _mm256_set1_epi32(static_cast<int>(max_length));\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t len = strings[i + j].length();\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(len), j);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_len_vec);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        if (mask != 0) {\n            int index = __builtin_ctz(mask);\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_setzero_si256();\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t length = strings[i + j].length();\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(length), j);\n            indices = _mm256_insert_epi32(indices, static_cast<int>(i + j), j);\n        }\n\n        __m256i max_len = _mm256_set1_epi32(static_cast<int>(max_length));\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_len);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, _mm256_setzero_si256()));\n        if (mask) {\n            int index = __builtin_ctz(mask) / 4;\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_setzero_si256();\n\n        for (size_t j = 0; j < 4 && i + j < strings.size(); ++j) {\n            size_t length = strings[i + j].length();\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(length), j);\n            indices = _mm256_insert_epi32(indices, static_cast<int>(i + j), j);\n        }\n\n        __m256i max_len = _mm256_set1_epi32(static_cast<int>(max_length));\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_len);\n\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if (mask != 0) {\n            int index = __builtin_ctz(mask);\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i max_len = _mm256_set1_epi32(max_length);\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t len = strings[i + j].length();\n            lengths = _mm256_insert_epi32(lengths, len, j);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, max_len);\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i vzero = _mm256_setzero_si256();\n\n    __m256i vtmp1, vtmp2;\n    while (1) {\n        vtmp1 = _mm256_andnot_si256(vzero, _mm256_cmpgt_epu64(vb, va));\n        vtmp2 = _mm256_blendv_epi8(vb, va, vtmp1);\n        vb = _mm256_blendv_epi8(va, vb, vtmp1);\n        va = vtmp2;\n\n        vtmp1 = _mm256_sub_epi64(vb, va);\n        vb = _mm256_blendv_epi8(vtmp1, vb, _mm256_cmpeq_epi64(vtmp1, vzero));\n\n        if (_mm256_testz_si256(vb, vzero)) {\n            break;\n        }\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i vzero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i vtmp = _mm256_min_epu32(va, vb);\n        va = vtmp;\n        vb = _mm256_sub_epi64(vb, vtmp);\n\n        __m256i vcmp = _mm256_cmpeq_epi64(vb, vzero);\n        if (_mm256_movemask_epi8(vcmp) == 0xFFFF) {\n            break;\n        }\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i vzero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i vtmp = _mm256_min_epu64(va, vb);\n        va = _mm256_sub_epi64(va, vtmp);\n        vb = _mm256_sub_epi64(vb, vtmp);\n\n        if (_mm256_testz_si256(va, va) || _mm256_testz_si256(vb, vb)) {\n            __m256i vres = _mm256_or_si256(va, vb);\n            return (uint64_t)_mm256_extract_epi64(vres, 0);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i vzero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i vrem = _mm256_rem_epu64(va, vb);\n        __m256i vmask = _mm256_cmpeq_epi64(vrem, vzero);\n\n        if (_mm256_testz_si256(vmask, vmask)) {\n            break;\n        }\n\n        va = vb;\n        vb = vrem;\n    }\n\n    return (uint64_t)_mm256_extract_epi64(vb, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i av = _mm256_set1_epi64x(a);\n    __m256i bv = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i rem = _mm256_setzero_si256();\n\n    while (1) {\n        rem = _mm256_rem_epu64(av, bv);\n        __m256i mask = _mm256_cmpeq_epi64(rem, zero);\n        if (_mm256_testz_si256(mask, mask)) {\n            break;\n        }\n        av = bv;\n        bv = rem;\n    }\n\n    return (uint64_t)_mm256_extract_epi64(bv, 0);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 shifted_vec = _mm256_permute_ps(vec, _MM_SHUFFLE(1, 0, 3, 2));\n        __m256 diff = _mm256_sub_ps(vec, shifted_vec);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        float diff_array[8];\n        _mm256_storeu_ps(diff_array, abs_diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diff_array[j] < min_diff) {\n                min_diff = diff_array[j];\n                closest1 = numbers[i + j];\n                closest2 = numbers[i + (j + 1) % 8];\n            }\n        }\n    }\n\n    for (; i < numbers.size() - 1; ++i) {\n        float diff = std::abs(numbers[i] - numbers[i + 1]);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest1 = numbers[i];\n            closest2 = numbers[i + 1];\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f;\n    float closest2 = 0.0f;\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 shifted_vec = _mm256_permute_ps(vec, _MM_SHUFFLE(0, 0, 0, 0));\n\n        for (int j = 1; j < 8; ++j) {\n            __m256 diff = _mm256_sub_ps(vec, shifted_vec);\n            diff = _mm256_and_ps(diff, _mm256_set1_ps(std::copysign(1.0f, _mm256_permutexvar_ps(_mm256_setr_epi32(j, j, j, j, j, j, j, j), vec))));\n            __m256 abs_diff = _mm256_abs_ps(diff);\n\n            float temp_diff[8];\n            _mm256_storeu_ps(temp_diff, abs_diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (temp_diff[k] < min_diff) {\n                    min_diff = temp_diff[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[i + ((k + j) % 8)];\n                }\n            }\n\n            shifted_vec = _mm256_permute_ps(vec, _MM_SHUFFLE(j, j, j, j));\n        }\n    }\n\n    for (; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        __m256 vec2 = _mm256_loadu_ps(&numbers[i + 4]);\n\n        for (int j = 0; j < 4; ++j) {\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            float diffs[8];\n            _mm256_storeu_ps(diffs, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diffs[k] < min_diff) {\n                    min_diff = diffs[k];\n                    closest1 = numbers[i + j];\n                    closest2 = numbers[i + 4 + k];\n                }\n            }\n\n            vec2 = _mm256_permute_ps(vec2, _MM_SHUFFLE(0, 3, 2, 1));\n        }\n    }\n\n    for (; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 shifted_vec = _mm256_permutevar8x32_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256 diff = _mm256_sub_ps(vec, shifted_vec);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        float diff_array[8];\n        _mm256_storeu_ps(diff_array, abs_diff);\n\n        for (int j = 0; j < 8; ++j) {\n            if (diff_array[j] < min_diff) {\n                min_diff = diff_array[j];\n                closest1 = numbers[i + j];\n                closest2 = numbers[i + (j + 1) % 8];\n            }\n        }\n    }\n\n    for (; i < numbers.size() - 1; ++i) {\n        float diff = std::abs(numbers[i] - numbers[i + 1]);\n        if (diff < min_diff) {\n            min_diff = diff;\n            closest1 = numbers[i];\n            closest2 = numbers[i + 1];\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f;\n    float closest2 = 0.0f;\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 shifted_vec = _mm256_permute_ps(vec, _MM_SHUFFLE(0, 0, 0, 0));\n\n        for (int j = 1; j < 8; ++j) {\n            __m256 diff = _mm256_sub_ps(vec, shifted_vec);\n            diff = _mm256_and_ps(diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF)));\n            __m256 min_val = _mm256_min_ps(diff, _mm256_set1_ps(min_diff));\n\n            float min_vals[8];\n            _mm256_storeu_ps(min_vals, min_val);\n\n            for (int k = 0; k < 8; ++k) {\n                if (min_vals[k] < min_diff) {\n                    min_diff = min_vals[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[i + ((k + j) % 8)];\n                }\n            }\n\n            shifted_vec = _mm256_permute_ps(vec, _MM_SHUFFLE(j, j, j, j));\n        }\n    }\n\n    for (; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::fabs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 normalized = _mm256_sub_ps(vec, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float minVal = *std::min_element(numbers.begin(), numbers.end());\n    float maxVal = *std::max_element(numbers.begin(), numbers.end());\n    float range = maxVal - minVal;\n\n    std::vector<float> result(n);\n    __m256 minVec = _mm256_set1_ps(minVal);\n    __m256 maxVec = _mm256_set1_ps(maxVal);\n    __m256 rangeVec = _mm256_set1_ps(range);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_sub_ps(vec, minVec);\n        normalized = _mm256_div_ps(normalized, rangeVec);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float minVal = *std::min_element(numbers.begin(), numbers.end());\n    float maxVal = *std::max_element(numbers.begin(), numbers.end());\n    float range = maxVal - minVal;\n\n    std::vector<float> result(n);\n    __m256 minVec = _mm256_set1_ps(minVal);\n    __m256 maxVec = _mm256_set1_ps(maxVal);\n    __m256 rangeVec = _mm256_set1_ps(range);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_sub_ps(vec, minVec);\n        normalized = _mm256_div_ps(normalized, rangeVec);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 sub_vec = _mm256_sub_ps(vec, min_vec);\n        __m256 div_vec = _mm256_div_ps(sub_vec, range_vec);\n\n        _mm256_storeu_ps(&result[i], div_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(min_val);\n        __m256 range_vec = _mm256_set1_ps(range);\n\n        __m256 normalized = _mm256_sub_ps(vec, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_setr_epi64x(n-1, n-2, n-3, n-4);\n    __m256i remainders = _mm256_rem_epu64(divisors, n_vec);\n\n    uint64_t results[4];\n    _mm256_storeu_si256((__m256i*)results, remainders);\n\n    for (int i = 0; i < 4; ++i) {\n        if (results[i] == 0) {\n            return n - 4 + i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_setr_epi64x(n/2, n/3, n/4, n/5);\n    __m256i remainders = _mm256_rem_epu64(n_vec, divisors);\n    __m256i zeros = _mm256_setzero_si256();\n    __m256i mask = _mm256_cmpeq_epi64(remainders, zeros);\n    uint64_t mask_bits = _mm256_movemask_epi8(_mm256_packs_epi16(_mm256_packs_epi32(mask, zeros), zeros));\n\n    if (mask_bits & 1) return n / 2;\n    if (mask_bits & 2) return n / 3;\n    if (mask_bits & 4) return n / 4;\n    if (mask_bits & 8) return n / 5;\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_setr_epi64x(n/2, n/3, n/4, n/5);\n    __m256i remainders = _mm256_rem_epu64(n_vec, divisors);\n    __m256i mask = _mm256_cmpeq_epi64(remainders, _mm256_setzero_si256());\n\n    uint64_t results[4];\n    _mm256_storeu_si256((__m256i*)results, divisors);\n\n    for (int i = 0; i < 4; i++) {\n        if (results[i] != 0 && n % results[i] == 0) {\n            return results[i];\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_i = _mm256_set_epi64x(n/2, n/2 - 1, n/2 - 2, n/2 - 3);\n    __m256i v_result = _mm256_setzero_si256();\n\n    for (uint64_t i = n/2; i > 0; i -= 4) {\n        __m256i v_i = _mm256_set_epi64x(i, i-1, i-2, i-3);\n        __m256i v_rem = _mm256_rem_epu64(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi64(v_rem, v_one);\n\n        if (_mm256_movemask_epi8(_mm256_castsi256_si128(v_mask)) != 0) {\n            v_result = v_i;\n            break;\n        }\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n\n    uint64_t max_result = 0;\n    for (int i = 0; i < 4; ++i) {\n        if (result[i] > max_result && n % result[i] == 0) {\n            max_result = result[i];\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_setr_epi64x(n/2, n/3, n/4, n/5);\n    __m256i remainders = _mm256_setr_epi64x(n%2, n%3, n%4, n%5);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_cmpeq_epi64(remainders, zero);\n\n    uint64_t results[4];\n    _mm256_storeu_si256((__m256i*)results, divisors);\n\n    for (int i = 3; i >= 0; --i) {\n        if ((mask[i] & 1) == 1) {\n            return results[i];\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i twos = _mm256_set1_epi32(2);\n    __m256i threes = _mm256_set1_epi32(3);\n    __m256i fives = _mm256_set1_epi32(5);\n    __m256i sevens = _mm256_set1_epi32(7);\n    __m256i elevens = _mm256_set1_epi32(11);\n    __m256i thirteens = _mm256_set1_epi32(13);\n\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i remainder = _mm256_set1_epi32(n);\n    __m256i divisor = _mm256_set1_epi32(2);\n\n    while (_mm256_movemask_epi8(_mm256_cmpeq_epi32(remainder, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n        __m256i quotient = _mm256_div_epi32(remainder, divisor);\n        __m256i product = _mm256_mul_epu32(quotient, divisor);\n        __m256i new_remainder = _mm256_sub_epi32(remainder, product);\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(new_remainder, _mm256_setzero_si256())) == 0xFFFFFFFF) {\n            remainder = quotient;\n            int div = _mm256_extract_epi32(divisor, 0);\n            factors.push_back(div);\n        } else {\n            divisor = _mm256_add_epi32(divisor, _mm256_set1_epi32(1));\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i twos = _mm256_set1_epi32(2);\n    __m256i threes = _mm256_set1_epi32(3);\n    __m256i fives = _mm256_set1_epi32(5);\n    __m256i sevens = _mm256_set1_epi32(7);\n    __m256i elevens = _mm256_set1_epi32(11);\n    __m256i thirteens = _mm256_set1_epi32(13);\n    __m256i seventeens = _mm256_set1_epi32(17);\n    __m256i nineteens = _mm256_set1_epi32(19);\n\n    __m256i num = _mm256_set1_epi32(n);\n    __m256i remainder = _mm256_set1_epi32(n);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, twos)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, twos);\n        __m256i mul = _mm256_mullo_epi32(div, twos);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(2);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, threes)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, threes);\n        __m256i mul = _mm256_mullo_epi32(div, threes);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(3);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, fives)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, fives);\n        __m256i mul = _mm256_mullo_epi32(div, fives);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(5);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, sevens)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, sevens);\n        __m256i mul = _mm256_mullo_epi32(div, sevens);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(7);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, elevens)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, elevens);\n        __m256i mul = _mm256_mullo_epi32(div, elevens);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(11);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, thirteens)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, thirteens);\n        __m256i mul = _mm256_mullo_epi32(div, thirteens);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(13);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, seventeens)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, seventeens);\n        __m256i mul = _mm256_mullo_epi32(div, seventeens);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(17);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(remainder, nineteens)) != 0) {\n        __m256i div = _mm256_div_epi32(remainder, nineteens);\n        __m256i mul = _mm256_mullo_epi32(div, nineteens);\n        __m256i cmp = _mm256_cmpeq_epi32(remainder, mul);\n        if (_mm256_movemask_epi8(cmp) == 0xFFFFFFFF) {\n            factors.push_back(19);\n            remainder = div;\n        } else {\n            break;\n        }\n    }\n\n    int rem = _mm256_extract_epi32(remainder, 0);\n    if (rem > 1) {\n        factors.push_back(rem);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i _2 = _mm256_set1_epi32(2);\n    __m256i _n = _mm256_set1_epi32(n);\n    __m256i _factors = _mm256_setzero_si256();\n    __m256i _i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 2, 2);\n    __m256i _one = _mm256_set1_epi32(1);\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (__m256i _j = _mm256_set1_epi32(3); _mm256_testz_si256(_j, _j) == 0; _j = _mm256_add_epi32(_j, _one)) {\n        __m256i _rem = _mm256_set1_epi32(n % _mm256_extract_epi32(_j, 0));\n        __m256i _zero = _mm256_setzero_si256();\n        while (_mm256_testz_si256(_rem, _rem) == 0) {\n            factors.push_back(_mm256_extract_epi32(_j, 0));\n            n /= _mm256_extract_epi32(_j, 0);\n            _rem = _mm256_set1_epi32(n % _mm256_extract_epi32(_j, 0));\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i _n = _mm256_set1_epi32(n);\n    __m256i _two = _mm256_set1_epi32(2);\n    __m256i _i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 2, 2);\n    __m256i _one = _mm256_set1_epi32(1);\n    __m256i _increment = _mm256_set1_epi32(8);\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (__m256i _j = _mm256_set1_epi32(3); _mm256_testz_si256(_j, _j) == 0; _j = _mm256_add_epi32(_j, _increment)) {\n        __m256i _rem = _mm256_set1_epi32(n % _mm256_extract_epi32(_j, 0));\n        if (_mm256_testz_si256(_rem, _rem) == 1) {\n            while (n % _mm256_extract_epi32(_j, 0) == 0) {\n                factors.push_back(_mm256_extract_epi32(_j, 0));\n                n /= _mm256_extract_epi32(_j, 0);\n            }\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int primes[] = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31};\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_primes = _mm256_loadu_si256((__m256i*)primes);\n    __m256i vec_rem = _mm256_setzero_si256();\n    __m256i vec_quot = _mm256_setzero_si256();\n    __m256i vec_one = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < 8; ++i) {\n        vec_rem = _mm256_rem_epi32(vec_n, _mm256_set1_epi32(primes[i]));\n        vec_quot = _mm256_div_epi32(vec_n, _mm256_set1_epi32(primes[i]));\n\n        while (_mm256_testz_si256(vec_rem, vec_rem)) {\n            factors.push_back(primes[i]);\n            vec_n = vec_quot;\n            vec_rem = _mm256_rem_epi32(vec_n, _mm256_set1_epi32(primes[i]));\n            vec_quot = _mm256_div_epi32(vec_n, _mm256_set1_epi32(primes[i]));\n        }\n    }\n\n    if (n > 31) {\n        for (int i = 32; i <= std::sqrt(n); i += 8) {\n            __m256i vec_i = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n            vec_rem = _mm256_rem_epi32(vec_n, vec_i);\n            vec_quot = _mm256_div_epi32(vec_n, vec_i);\n\n            for (int j = 0; j < 8; ++j) {\n                if (_mm256_extract_epi32(vec_rem, j) == 0) {\n                    int factor = _mm256_extract_epi32(vec_i, j);\n                    while (n % factor == 0) {\n                        factors.push_back(factor);\n                        n /= factor;\n                    }\n                }\n            }\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (((1 << j) & _mm256_movemask_ps(mask)) != 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (((int*)&mask)[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (((1 << j) & _mm256_movemask_ps(mask)) != 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (((1 << j) & _mm256_movemask_ps(mask)) != 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        float temp[8];\n        _mm256_storeu_ps(temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (((1 << j) & _mm256_movemask_ps(mask)) != 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n    __m256i v_step = _mm256_set_epi64x(6, 6, 6, 6);\n\n    while (i * i <= n) {\n        __m256i v_mod = _mm256_rem_epu64(v_n, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mod, _mm256_setzero_si256());\n        uint64_t mask = _mm256_movemask_epi8(_mm256_packs_epi16(_mm256_packs_epi32(v_cmp, _mm256_setzero_si256()), _mm256_setzero_si256()));\n        if (mask) return false;\n\n        v_i = _mm256_add_epi64(v_i, v_step);\n        i += 4 * 6;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    int64_t i = 5;\n\n    __m256i v5 = _mm256_set1_epi64x(5);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vlimit = _mm256_set1_epi64x(limit);\n    __m256i vi = _mm256_set1_epi64x(i);\n    __m256i v1 = _mm256_set1_epi64x(1);\n    __m256i v2 = _mm256_set1_epi64x(2);\n    __m256i v4 = _mm256_set1_epi64x(4);\n\n    __m256i vmod1 = _mm256_set1_epi64x(n % i);\n    __m256i vmod2 = _mm256_set1_epi64x(n % (i + 2));\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi64(vi, vlimit)) != 0) {\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi64(vmod1, v1)) != 0 ||\n            _mm256_movemask_epi8(_mm256_cmpeq_epi64(vmod2, v1)) != 0) {\n            return false;\n        }\n        vi = _mm256_add_epi64(vi, v4);\n        vmod1 = _mm256_set1_epi64x(n % _mm256_extract_epi64(vi, 0));\n        vmod2 = _mm256_set1_epi64x(n % (_mm256_extract_epi64(vi, 0) + 2));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n    __m256i v_step = _mm256_set_epi64x(6, 6, 6, 6);\n\n    while (i * i <= n) {\n        __m256i v_mod = _mm256_rem_epu64(v_n, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mod, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi16(_mm256_packs_epi32(v_cmp, v_cmp), _mm256_setzero_si256()));\n        if (mask != 0) return false;\n        v_i = _mm256_add_epi64(v_i, v_step);\n        i += 4;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v5 = _mm256_set1_epi64x(5);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vi = _mm256_set_epi64x(i+3, i+2, i+1, i);\n\n    for (; i * i <= n; i += 4) {\n        __m256i vmod = _mm256_set_epi64x(i+3, i+2, i+1, i);\n        __m256i vrem = _mm256_rem_epu64(vn, vmod);\n        unsigned int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi64(vrem, _mm256_setzero_si256()));\n        if (mask != 0) return false;\n        vi = _mm256_add_epi64(vi, _mm256_set1_epi64x(4));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stdint.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t i = 5;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_set_epi64x(i+3, i+2, i+1, i);\n    __m256i v_step = _mm256_set1_epi64x(4);\n    __m256i v_rem = _mm256_set1_epi64x(n % i);\n\n    while (i * i <= n) {\n        v_rem = _mm256_rem_epu64(v_n, v_i);\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi64(v_rem, _mm256_setzero_si256())) != 0) {\n            return false;\n        }\n        v_i = _mm256_add_epi64(v_i, v_step);\n        i += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result(s);\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i case_bit = _mm256_set1_epi8('a' - 'A');\n    __m256i lower_case_start = _mm256_set1_epi8('a');\n    __m256i upper_case_end = _mm256_set1_epi8('Z');\n    __m256i lower_case_end = _mm256_set1_epi8('z');\n    __m256i upper_case_start = _mm256_set1_epi8('A');\n\n    bool hasLetter = false;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, lower_case_start), _mm256_cmplt_epi8(chunk, lower_case_end));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_case_start), _mm256_cmplt_epi8(chunk, upper_case_end));\n\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        hasLetter |= _mm256_movemask_epi8(_mm256_cmpeq_epi8(is_letter, _mm256_setzero_si256())) != 0xFFFFFFFF;\n\n        __m256i toggled_case = _mm256_xor_si256(chunk, _mm256_and_si256(is_letter, case_bit));\n\n        _mm256_storeu_si256((__m256i*)(result.data() + i), toggled_case);\n    }\n\n    for (; i < len; ++i) {\n        if ((s[i] >= 'a' && s[i] <= 'z') || (s[i] >= 'A' && s[i] <= 'Z')) {\n            hasLetter = true;\n            result[i] = (s[i] >= 'a' && s[i] <= 'z') ? (s[i] - 'a' + 'A') : (s[i] - 'A' + 'a');\n        } else {\n            result[i] = s[i];\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    std::string result(len, ' ');\n    bool hasLetter = false;\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t j = std::min(len - i, size_t(32));\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i mask = _mm256_set1_epi8('a');\n        __m256i upper = _mm256_set1_epi8('A');\n        __m256i lower = _mm256_set1_epi8('Z');\n        __m256i is_lower = _mm256_or_si256(_mm256_cmpgt_epi8(chunk, mask), _mm256_cmpgt_epi8(lower, chunk));\n        __m256i is_upper = _mm256_or_si256(_mm256_cmpgt_epi8(chunk, upper), _mm256_cmpgt_epi8(mask, chunk));\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        hasLetter |= _mm256_movemask_epi8(_mm256_cmpeq_epi8(is_letter, _mm256_setzero_si256())) != 0xFFFFFFFF;\n        __m256i toggled = _mm256_xor_si256(chunk, _mm256_set1_epi8(32));\n        chunk = _mm256_blendv_epi8(chunk, toggled, is_letter);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), chunk);\n    }\n\n    if (!hasLetter) {\n        for (size_t i = 0; i < len; i += 32) {\n            size_t j = std::min(len - i, size_t(32));\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(result.data() + i));\n            chunk = _mm256_permute4x64_epi64(chunk, _MM_SHUFFLE(0, 1, 2, 3));\n            chunk = _mm256_shuffle_epi8(chunk, _mm256_setr_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n            _mm256_storeu_si256((__m256i*)(result.data() + i), chunk);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result(s);\n    size_t len = s.length();\n    size_t i = 0;\n    bool hasLetter = false;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i isLower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z')), _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), chunk));\n        __m256i isUpper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('@')), _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), chunk));\n        __m256i isLetter = _mm256_or_si256(isLower, isUpper);\n        hasLetter |= _mm256_movemask_epi8(isLetter) != 0;\n        __m256i toggled = _mm256_xor_si256(chunk, _mm256_set1_epi8(32));\n        __m256i resultChunk = _mm256_or_si256(_mm256_andnot_si256(isLetter, chunk), _mm256_and_si256(isLetter, toggled));\n        _mm256_storeu_si256((__m256i*)&result[i], resultChunk);\n    }\n\n    for (; i < len; ++i) {\n        if (islower(s[i])) {\n            result[i] = toupper(s[i]);\n            hasLetter = true;\n        } else if (isupper(s[i])) {\n            result[i] = tolower(s[i]);\n            hasLetter = true;\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result(s);\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t j = std::min(len - i, 32);\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i mask_lower = _mm256_set1_epi8('a' - 1);\n        __m256i mask_upper = _mm256_set1_epi8('Z' - 'A' + 1);\n        __m256i mask_letter = _mm256_or_si256(_mm256_cmpgt_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0xdf)), mask_lower),\n                                               _mm256_cmpgt_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0xdf)), mask_upper));\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(mask_letter, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n            hasLetter = true;\n            __m256i mask_alpha = _mm256_or_si256(_mm256_cmpgt_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0xdf)), _mm256_set1_epi8('Z' - 'A' + 1)),\n                                                 _mm256_cmpgt_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0xdf)), _mm256_set1_epi8('a' - 1)));\n            __m256i toggle_case = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n            chunk = _mm256_blendv_epi8(chunk, toggle_case, mask_alpha);\n        }\n        _mm256_storeu_si256((__m256i*)&result[i], chunk);\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result(s);\n    size_t len = s.length();\n    size_t i = 0;\n    bool hasLetter = false;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i isLower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z')), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('a')));\n        __m256i isUpper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('@')), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('[')));\n        __m256i isLetter = _mm256_or_si256(isLower, isUpper);\n        hasLetter |= _mm256_movemask_epi8(_mm256_cmpgt_epi8(isLetter, _mm256_setzero_si256())) != 0;\n        __m256i toggleCase = _mm256_xor_si256(chunk, _mm256_set1_epi8(32));\n        chunk = _mm256_or_si256(_mm256_and_si256(isLower, toggleCase), _mm256_andnot_si256(isLower, chunk));\n        chunk = _mm256_or_si256(_mm256_and_si256(isUpper, toggleCase), _mm256_andnot_si256(isUpper, chunk));\n        _mm256_storeu_si256((__m256i*)(result.data() + i), chunk);\n    }\n\n    for (; i < len; ++i) {\n        if (std::isalpha(s[i])) {\n            hasLetter = true;\n            result[i] = std::isupper(s[i]) ? std::tolower(s[i]) : std::toupper(s[i]);\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_third;\n\n    for (int i = 2; i < n; i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    int j = 0;\n    for (int i = 2; i < n; i += 3) {\n        l[i] = sorted_third[j++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> sorted_thirds;\n\n    for (int i = 2; i < n; i += 3) {\n        sorted_thirds.push_back(l[i]);\n    }\n\n    std::sort(sorted_thirds.begin(), sorted_thirds.end());\n\n    int sorted_index = 0;\n    for (int i = 2; i < n; i += 3) {\n        l[i] = sorted_thirds[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) {\n        return std::numeric_limits<float>::lowest();\n    }\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n    size_t i;\n    for (i = 0; i <= l.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_values[8];\n    _mm256_storeu_ps(max_values, max_vec);\n\n    float max_element = max_values[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_values[j] > max_element) {\n            max_element = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_element) {\n            max_element = l[i];\n        }\n    }\n\n    return max_element;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) {\n        return std::numeric_limits<float>::quiet_NaN();\n    }\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n    size_t i;\n    for (i = 0; i <= l.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_values[8];\n    _mm256_storeu_ps(max_values, max_vec);\n\n    float max_elem = max_values[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_values[j] > max_elem) {\n            max_elem = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_elem) {\n            max_elem = l[i];\n        }\n    }\n\n    return max_elem;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n    __m256 vec;\n    float max_val = std::numeric_limits<float>::lowest();\n\n    size_t i;\n    for (i = 0; i <= l.size() - 8; i += 8) {\n        vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_arr[8];\n    _mm256_storeu_ps(max_arr, max_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (max_arr[j] > max_val) {\n            max_val = max_arr[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) {\n        return std::numeric_limits<float>::lowest();\n    }\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n    __m256 vec;\n\n    size_t i;\n    for (i = 0; i <= l.size() - 8; i += 8) {\n        vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_val = std::numeric_limits<float>::lowest();\n    for (size_t j = 0; j < 8; ++j) {\n        max_val = std::max(max_val, ((float*)&max_vec)[j]);\n    }\n\n    for (; i < l.size(); ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) {\n        return std::numeric_limits<float>::quiet_NaN();\n    }\n\n    __m256 max_vec = _mm256_set1_ps(std::numeric_limits<float>::lowest());\n    __m256 vec;\n    size_t i;\n\n    for (i = 0; i <= l.size() - 8; i += 8) {\n        vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_values[8];\n    _mm256_storeu_ps(max_values, max_vec);\n\n    float max_elem = max_values[0];\n    for (int j = 1; j < 8; ++j) {\n        if (max_values[j] > max_elem) {\n            max_elem = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_elem) {\n            max_elem = l[i];\n        }\n    }\n\n    return max_elem;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n\nint fizz_buzz_simd(int n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sevens = _mm256_set1_epi8('7');\n    int count = 0;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i div11 = _mm256_set1_epi32(11);\n        __m256i div13 = _mm256_set1_epi32(13);\n        __m256i mask11 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div11), _mm256_setzero_si256());\n        __m256i mask13 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div13), _mm256_setzero_si256());\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        __m256i chars = _mm256_setr_epi8(\n            (i % 10) + '0', (i / 10) % 10 + '0', (i / 100) % 10 + '0', (i / 1000) % 10 + '0',\n            ((i+1) % 10) + '0', ((i+1) / 10) % 10 + '0', ((i+1) / 100) % 10 + '0', ((i+1) / 1000) % 10 + '0',\n            ((i+2) % 10) + '0', ((i+2) / 10) % 10 + '0', ((i+2) / 100) % 10 + '0', ((i+2) / 1000) % 10 + '0',\n            ((i+3) % 10) + '0', ((i+3) / 10) % 10 + '0', ((i+3) / 100) % 10 + '0', ((i+3) / 1000) % 10 + '0',\n            ((i+4) % 10) + '0', ((i+4) / 10) % 10 + '0', ((i+4) / 100) % 10 + '0', ((i+4) / 1000) % 10 + '0',\n            ((i+5) % 10) + '0', ((i+5) / 10) % 10 + '0', ((i+5) / 100) % 10 + '0', ((i+5) / 1000) % 10 + '0',\n            ((i+6) % 10) + '0', ((i+6) / 10) % 10 + '0', ((i+6) / 100) % 10 + '0', ((i+6) / 1000) % 10 + '0',\n            ((i+7) % 10) + '0', ((i+7) / 10) % 10 + '0', ((i+7) / 100) % 10 + '0', ((i+7) / 1000) % 10 + '0'\n        );\n\n        __m256i cmp = _mm256_cmpeq_epi8(chars, sevens);\n        cmp = _mm256_and_si256(cmp, mask);\n        __m256i count7 = _mm256_sad_epu8(cmp, _mm256_setzero_si256());\n        count += _mm256_extract_epi32(count7, 0) + _mm256_extract_epi32(count7, 2) + _mm256_extract_epi32(count7, 4) + _mm256_extract_epi32(count7, 6);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sevens = _mm256_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i numbers = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i div11 = _mm256_set1_epi32(11);\n        __m256i div13 = _mm256_set1_epi32(13);\n        __m256i mask11 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div11), _mm256_setzero_si256());\n        __m256i mask13 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div13), _mm256_setzero_si256());\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        __m256i digits = _mm256_setr_epi8(\n            i%10, (i/10)%10, (i/100)%10, (i/1000)%10,\n            (i+1)%10, ((i+1)/10)%10, ((i+1)/100)%10, ((i+1)/1000)%10,\n            (i+2)%10, ((i+2)/10)%10, ((i+2)/100)%10, ((i+2)/1000)%10,\n            (i+3)%10, ((i+3)/10)%10, ((i+3)/100)%10, ((i+3)/1000)%10,\n            (i+4)%10, ((i+4)/10)%10, ((i+4)/100)%10, ((i+4)/1000)%10,\n            (i+5)%10, ((i+5)/10)%10, ((i+5)/100)%10, ((i+5)/1000)%10,\n            (i+6)%10, ((i+6)/10)%10, ((i+6)/100)%10, ((i+6)/1000)%10,\n            (i+7)%10, ((i+7)/10)%10, ((i+7)/100)%10, ((i+7)/1000)%10\n        );\n\n        __m256i cmp = _mm256_cmpeq_epi8(digits, sevens);\n        __m256i count7 = _mm256_and_si256(cmp, ones);\n        count += _mm256_extract_epi8(_mm256_sad_epu8(count7, _mm256_setzero_si256()), 0);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sevens = _mm256_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i div11 = _mm256_set1_epi32(11);\n        __m256i div13 = _mm256_set1_epi32(13);\n        __m256i mask11 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div11), _mm256_setzero_si256());\n        __m256i mask13 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div13), _mm256_setzero_si256());\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        for (int j = 0; j < 8; j++) {\n            if (((int*)&mask)[j] == 0) {\n                int num = i + j;\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (int k = 0; k < strlen(str); k++) {\n                    if (str[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sevens = _mm256_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i div11 = _mm256_set1_epi32(11);\n        __m256i div13 = _mm256_set1_epi32(13);\n        __m256i mask11 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div11), _mm256_setzero_si256());\n        __m256i mask13 = _mm256_cmpgt_epi32(_mm256_rem_epi32(numbers, div13), _mm256_setzero_si256());\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        for (int j = 0; j < 8; ++j) {\n            if (_mm256_extract_epi32(mask, j)) {\n                int num = i + j;\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        count++;\n                    }\n                    num /= 10;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sevens = _mm256_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i numbers = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i div11 = _mm256_set1_epi32(11);\n        __m256i div13 = _mm256_set1_epi32(13);\n\n        __m256i mod11 = _mm256_rem_epu32(numbers, div11);\n        __m256i mod13 = _mm256_rem_epu32(numbers, div13);\n\n        __m256i mask11 = _mm256_cmpeq_epi32(mod11, _mm256_setzero_si256());\n        __m256i mask13 = _mm256_cmpeq_epi32(mod13, _mm256_setzero_si256());\n\n        __m256i mask = _mm256_or_si256(mask11, mask13);\n\n        int mask_array[8];\n        _mm256_storeu_si256((__m256i*)mask_array, mask);\n\n        for (int j = 0; j < 8; j++) {\n            if (mask_array[j] && i + j < n) {\n                int num = i + j;\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        count++;\n                    }\n                    num /= 10;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < l.size(); ++i) {\n        seen.insert(l[i]);\n    }\n\n    for (size_t i = 0; i < l.size(); i++) {\n        for (size_t j = i + 1; j < l.size(); j++) {\n            int target = -(l[i] + l[j]);\n            if (seen.count(target)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m256i vec1, vec2, vec3, zero = _mm256_setzero_si256();\n    for (int i = 0; i < n - 2; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = i + 1; j < n - 1; j += 8) {\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            for (int k = j + 1; k < n; k += 8) {\n                vec3 = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(vec1, vec2);\n                sum = _mm256_add_epi32(sum, vec3);\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    for (int i = 0; i < n - 2; i++) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; j++) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            for (int k = j + 1; k < n; k += 8) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&l[k]);\n                __m256i sum = _mm256_add_epi32(vi, _mm256_add_epi32(vj, vk));\n                __m256i zero = _mm256_setzero_si256();\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n                if (mask != 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m256i vec1, vec2, vec3, zero = _mm256_setzero_si256();\n    int i, j, k;\n\n    for (i = 0; i < n - 2; i++) {\n        vec1 = _mm256_set1_epi32(l[i]);\n        for (j = i + 1; j < n - 1; j++) {\n            vec2 = _mm256_set1_epi32(l[j]);\n            for (k = j + 1; k < n; k++) {\n                vec3 = _mm256_set1_epi32(l[k]);\n                __m256i sum = _mm256_add_epi32(vec1, _mm256_add_epi32(vec2, vec3));\n                __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n                int mask = _mm256_movemask_ps((__m256)cmp);\n                if (mask != 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < l.size(); ++i) {\n        __m256i vec = _mm256_set1_epi32(-l[i]);\n        for (int j = i + 1; j < l.size(); ++j) {\n            __m256i vec2 = _mm256_set1_epi32(l[j]);\n            __m256i sum = _mm256_add_epi32(vec, vec2);\n            int temp[8];\n            _mm256_storeu_si256((__m256i*)temp, sum);\n            for (int k = 0; k < 8; ++k) {\n                if (seen.find(temp[k]) != seen.end()) {\n                    return true;\n                }\n            }\n            seen.insert(l[j]);\n        }\n        seen.insert(l[i]);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        v = _mm256_add_epi32(v, v_one);\n        _mm256_storeu_si256((__m256i*)&result[i], v);\n    }\n\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        v = _mm256_add_epi32(v, _mm256_set1_epi32(1));\n        _mm256_storeu_si256((__m256i*)&result[i], v);\n    }\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        v = _mm256_add_epi32(v, _mm256_set1_epi32(1));\n        _mm256_storeu_si256((__m256i*)&result[i], v);\n    }\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        v = _mm256_add_epi32(v, _mm256_set1_epi32(1));\n        _mm256_storeu_si256((__m256i*)&result[i], v);\n    }\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        v = _mm256_add_epi32(v, _mm256_set1_epi32(1));\n        _mm256_storeu_si256((__m256i*)&result[i], v);\n    }\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m256i vec1, vec2, zero = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = i + 8; j < n; j += 8) {\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i sum = _mm256_add_epi32(vec1, vec2);\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m256i vec1, vec2, sum;\n    int mask;\n\n    for (int i = 0; i < n; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = i + 8; j < n; j += 8) {\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            sum = _mm256_add_epi32(vec1, vec2);\n            mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(sum, _mm256_setzero_si256()));\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    for (int i = 0; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    std::unordered_set<int> seen;\n    int n = l.size();\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (seen.find(temp[j]) != seen.end() || seen.find(-temp[j]) != seen.end()) {\n                return true;\n            }\n            seen.insert(temp[j]);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (seen.find(l[i]) != seen.end() || seen.find(-l[i]) != seen.end()) {\n            return true;\n        }\n        seen.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m256i vec1, vec2, abs_vec1, abs_vec2, cmp_mask;\n    int mask;\n\n    for (int i = 0; i < n; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&l[i]);\n        for (int j = i + 8; j < n; j += 8) {\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            abs_vec1 = _mm256_abs_epi32(vec1);\n            abs_vec2 = _mm256_abs_epi32(vec2);\n            cmp_mask = _mm256_cmpeq_epi32(abs_vec1, abs_vec2);\n            mask = _mm256_movemask_ps((__m256)cmp_mask);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    int remainder = n % 8;\n    if (remainder > 1) {\n        vec1 = _mm256_maskload_epi32(&l[n - remainder], _mm256_set1_epi32((1 << remainder) - 1));\n        for (int j = 0; j < n - remainder; j += 8) {\n            vec2 = _mm256_loadu_si256((__m256i*)&l[j]);\n            abs_vec1 = _mm256_abs_epi32(vec1);\n            abs_vec2 = _mm256_abs_epi32(vec2);\n            cmp_mask = _mm256_cmpeq_epi32(abs_vec1, abs_vec2);\n            mask = _mm256_movemask_ps((__m256)cmp_mask);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    std::unordered_set<int> seen;\n    int n = l.size();\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (seen.find(temp[j]) != seen.end() || seen.find(-temp[j]) != seen.end()) {\n                return true;\n            }\n            seen.insert(temp[j]);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (seen.find(l[i]) != seen.end() || seen.find(-l[i]) != seen.end()) {\n            return true;\n        }\n        seen.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result = \"\";\n    while (x > 0) {\n        int64_t divisor = 1;\n        while (x / divisor >= (1LL << 32)) {\n            divisor *= base;\n        }\n\n        __m256i x_vec = _mm256_set1_epi64x(x);\n        __m256i div_vec = _mm256_set1_epi64x(divisor);\n        __m256i base_vec = _mm256_set1_epi64x(base);\n\n        __m256i remainder_vec = _mm256_rem_epu64(x_vec, div_vec);\n        __m256i quotient_vec = _mm256_div_epi64(x_vec, div_vec);\n\n        int64_t remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, remainder_vec);\n\n        for (int i = 0; i < 4; ++i) {\n            if (remainder[i] == 0) continue;\n            for (int j = 0; j < 4; ++j) {\n                result = std::to_string(remainder[i] % base) + result;\n                remainder[i] /= base;\n            }\n        }\n\n        x /= divisor;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result;\n    __m256i _base = _mm256_set1_epi8(base);\n    __m256i _x = _mm256_set1_epi64x(x);\n    __m256i _rem = _mm256_setzero_si256();\n    __m256i _div = _mm256_setzero_si256();\n\n    while (_mm256_movemask_epi8(_mm256_cmpeq_epi64(_x, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n        _rem = _mm256_rem_epi8(_x, _base);\n        _div = _mm256_div_epi8(_x, _base);\n        _x = _mm256_srli_epi64(_div, 8);\n\n        int8_t rem_values[32];\n        _mm256_storeu_si256((__m256i*)rem_values, _rem);\n\n        for (int i = 0; i < 32; ++i) {\n            if (rem_values[i] != 0) {\n                result.push_back('0' + rem_values[i]);\n            }\n        }\n    }\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n\n    char buffer[64];\n    int pos = sizeof(buffer) - 1;\n    buffer[pos] = '\\0';\n\n    __m256i divisor = _mm256_set1_epi64x(base);\n    __m256i remainder = _mm256_set1_epi64x(0);\n    __m256i current = _mm256_set1_epi64x(x);\n    __m256i mask = _mm256_set1_epi64x(0xF);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi64(current, _mm256_setzero_si256())) != 0) {\n        remainder = _mm256_and_si256(current, mask);\n        current = _mm256_div_epi64(current, divisor);\n\n        pos--;\n        buffer[pos] = '0' + _mm256_extract_epi64(remainder, 0);\n    }\n\n    return std::string(buffer + pos);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    char buffer[64];\n    int index = 0;\n\n    __m256i _base = _mm256_set1_epi8(base);\n    __m256i _x = _mm256_set1_epi64x(x);\n    __m256i _remainders = _mm256_setzero_si256();\n    __m256i _quotients = _mm256_setzero_si256();\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi64(_x, _mm256_setzero_si256())) != 0) {\n        _remainders = _mm256_set_epi64x(\n            x % base,\n            (x / base) % base,\n            (x / base / base) % base,\n            (x / base / base / base) % base\n        );\n        _quotients = _mm256_set_epi64x(\n            x / base,\n            x / base / base,\n            x / base / base / base,\n            x / base / base / base / base\n        );\n        _x = _quotients;\n\n        __m256i _remainders_plus_48 = _mm256_add_epi8(_remainders, _mm256_set1_epi8('0'));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(buffer + index), _remainders_plus_48);\n        index += 32;\n    }\n\n    buffer[index] = '\\0';\n    std::string result(buffer);\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n\n    char buffer[64];\n    int pos = sizeof(buffer) - 1;\n    buffer[pos] = '\\0';\n\n    __m256i divisor = _mm256_set1_epi64x(base);\n    __m256i remainder = _mm256_setzero_si256();\n    __m256i current = _mm256_set1_epi64x(x);\n    __m256i mask = _mm256_set1_epi64x(0xF);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi64(current, _mm256_setzero_si256()))) {\n        remainder = _mm256_and_si256(current, mask);\n        current = _mm256_div_epi64(current, divisor);\n\n        int64_t* rem = (int64_t*)&remainder;\n        for (int i = 0; i < 4; ++i) {\n            if (rem[i] <= 9) {\n                buffer[--pos] = '0' + rem[i];\n            }\n        }\n    }\n\n    return std::string(buffer + pos);\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    const char* data = text.c_str();\n    size_t i = 0;\n\n    // Process 32 bytes at a time\n    for (; i <= len - 32; i += 32) {\n        __m256i head = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i tail = _mm256_loadu_si256((__m256i*)&data[len - i - 32]);\n        tail = _mm256_permute4x64_epi64(_mm256_permute2x128_si256(tail, tail, _MM_SHUFFLE(0, 1, 0, 1)), _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m256i cmp = _mm256_cmpeq_epi8(head, tail);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) return false;\n    }\n\n    // Process remaining 16 bytes\n    if (i <= len - 16) {\n        __m128i head = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i tail = _mm_loadu_si128((__m128i*)&data[len - i - 16]);\n        tail = _mm_shuffle_epi8(tail, _mm_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        __m128i cmp = _mm_cmpeq_epi8(head, tail);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) return false;\n        i += 16;\n    }\n\n    // Process remaining bytes\n    for (; i < len - i; i++) {\n        if (data[i] != data[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    const char* data = text.c_str();\n    size_t i = 0;\n    size_t j = len - 1;\n\n    __m256i v1, v2, v_cmp;\n    __m256i v_mask = _mm256_set1_epi8(1);\n    __m256i v_true = _mm256_set1_epi8(0xff);\n\n    while (j - i >= 32) {\n        v1 = _mm256_loadu_si256((__m256i*)&data[i]);\n        v2 = _mm256_loadu_si256((__m256i*)&data[j - 31]);\n        v2 = _mm256_permute2x128_si256(v2, v2, _MM_SHUFFLE(0, 1, 0, 1));\n        v2 = _mm256_permutevar8x32_epi32(v2, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        v_cmp = _mm256_cmpeq_epi8(v1, v2);\n        v_cmp = _mm256_and_si256(v_cmp, v_mask);\n        if (_mm256_testz_si256(v_cmp, v_true)) return false;\n\n        i += 32;\n        j -= 32;\n    }\n\n    if (j > i) {\n        v1 = _mm256_maskload_epi32((int*)&data[i], _mm256_set1_epi32((1 << (j - i + 1)) - 1));\n        v2 = _mm256_maskload_epi32((int*)&data[i], _mm256_set1_epi32((1 << (j - i + 1)) - 1));\n        v2 = _mm256_permute2x128_si256(v2, v2, _MM_SHUFFLE(0, 1, 0, 1));\n        v2 = _mm256_permutevar8x32_epi32(v2, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        v_cmp = _mm256_cmpeq_epi8(v1, v2);\n        v_cmp = _mm256_and_si256(v_cmp, v_mask);\n        if (_mm256_testz_si256(v_cmp, v_true)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len == 0) return true;\n\n    const char* data = text.c_str();\n    int i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i head = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i tail = _mm256_loadu_si256((__m256i*)&data[len - i - 32]);\n        tail = _mm256_permute4x64_epi64(_mm256_permute4x64_epi64(tail, _MM_SHUFFLE(0, 1, 2, 3)), _MM_SHUFFLE(0, 1, 2, 3));\n        __m256i cmp = _mm256_cmpeq_epi8(head, tail);\n        if (!_mm256_testz_si256(cmp, _mm256_set1_epi8(-1))) {\n            return false;\n        }\n    }\n\n    for (; i < len - i - 1; i++) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    const char* data = text.c_str();\n    size_t i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i head = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i tail = _mm256_loadu_si256((__m256i*)&data[len - i - 32]);\n        tail = _mm256_permute4x64_epi64(_mm256_permute4x64_epi64(tail, _MM_SHUFFLE(0, 1, 2, 3)), _MM_SHUFFLE(0, 1, 2, 3));\n        __m256i cmp = _mm256_cmpeq_epi8(head, tail);\n        if (!_mm256_testz_si256(cmp, _mm256_set1_epi8(-1))) {\n            return false;\n        }\n    }\n\n    for (; i < len - i - 1; i++) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    const char* data = text.c_str();\n    size_t i = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i head = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i tail = _mm256_loadu_si256((__m256i*)&data[len - i - 32]);\n        tail = _mm256_permute4x64_epi64(_mm256_permute4x64_epi64(tail, _MM_SHUFFLE(0, 1, 2, 3)), _MM_SHUFFLE(0, 1, 2, 3));\n        tail = _mm256_shuffle_epi8(tail, _mm256_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        __m256i cmp = _mm256_cmpeq_epi8(head, tail);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < len - i - 1; i++) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    int i;\n\n    __m256i offset = _mm256_set1_epi8(-5);\n    __m256i mod_val = _mm256_set1_epi8(26);\n    __m256i base_a = _mm256_set1_epi8('a');\n\n    for (i = 0; i <= s.length() - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shifted = _mm256_add_epi8(chunk, offset);\n        __m256i normalized = _mm256_add_epi8(shifted, base_a);\n        __m256i mod_result = _mm256_sub_epi8(normalized, _mm256_and_si256(_mm256_cmpgt_epi8(normalized, _mm256_set1_epi8('z')), mod_val));\n        _mm256_storeu_si256((__m256i*)&out[i], mod_result);\n    }\n\n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a') % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    int i;\n\n    __m256i offset = _mm256_set1_epi8(-5);\n    __m256i mod_val = _mm256_set1_epi8(26);\n    __m256i base_a = _mm256_set1_epi8('a');\n\n    for (i = 0; i <= s.length() - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shifted = _mm256_add_epi8(chunk, offset);\n        __m256i normalized = _mm256_add_epi8(shifted, mod_val);\n        normalized = _mm256_and_si256(normalized, _mm256_set1_epi8(0x1F));\n        normalized = _mm256_add_epi8(normalized, base_a);\n        _mm256_storeu_si256((__m256i*)&out[i], normalized);\n    }\n\n    for (; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    __m256i key = _mm256_set1_epi8(5);\n    __m256i mod_val = _mm256_set1_epi8(26);\n    __m256i char_a = _mm256_set1_epi8('a');\n\n    for (size_t i = 0; i < s.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i shifted = _mm256_sub_epi8(chunk, key);\n        __m256i adjusted = _mm256_add_epi8(shifted, char_a);\n        __m256i mod_result = _mm256_sub_epi8(adjusted, _mm256_and_si256(_mm256_cmpgt_epi8(adjusted, _mm256_set1_epi8('z')), mod_val));\n        _mm256_storeu_si256((__m256i*)(out.data() + i), mod_result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    __m256i offset = _mm256_set1_epi8(-5);\n    __m256i mod_val = _mm256_set1_epi8(26);\n    __m256i base_a = _mm256_set1_epi8('a');\n\n    for (size_t i = 0; i < s.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i shifted = _mm256_add_epi8(chunk, offset);\n        __m256i normalized = _mm256_add_epi8(shifted, base_a);\n        __m256i mod_result = _mm256_sub_epi8(normalized, _mm256_and_si256(_mm256_cmpgt_epi8(normalized, _mm256_set1_epi8('z')), mod_val));\n        _mm256_storeu_si256((__m256i*)(out.data() + i), mod_result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    __m256i offset = _mm256_set1_epi8(-5);\n    __m256i mod_val = _mm256_set1_epi8(26);\n    __m256i base_a = _mm256_set1_epi8('a');\n\n    for (size_t i = 0; i <= s.length() - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_add_epi8(chunk, offset);\n        __m256i normalized = _mm256_add_epi8(shifted, mod_val);\n        normalized = _mm256_and_si256(normalized, _mm256_set1_epi8(0x1F));\n        normalized = _mm256_add_epi8(normalized, base_a);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), normalized);\n    }\n\n    for (size_t i = (s.length() / 32) * 32; i < s.length(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        result = _mm256_cmpgt_epi32(threshold, vec);\n        mask = _mm256_and_si256(mask, result);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, mask);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] == 0) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    bool result = true;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, vec);\n        if (_mm256_movemask_ps((__m256)cmp) != 0xFF) {\n            result = false;\n            break;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i result;\n    bool flag = true;\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        result = _mm256_cmpgt_epi32(threshold, vec);\n        if (_mm256_movemask_ps((__m256)result) != 0xFF) {\n            flag = false;\n            break;\n        }\n    }\n\n    if (flag && (size % 8 != 0)) {\n        int remainder = size % 8;\n        std::vector<int> temp(l.end() - remainder, l.end());\n        temp.resize(8, 0);\n        __m256i vec = _mm256_loadu_si256((__m256i*)temp.data());\n        result = _mm256_cmpgt_epi32(threshold, vec);\n        int mask = (1 << remainder) - 1;\n        if ((_mm256_movemask_ps((__m256)result) & mask) != mask) {\n            flag = false;\n        }\n    }\n\n    return flag;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result;\n    bool below_threshold = true;\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        result = _mm256_cmpgt_epi32(threshold, vec);\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(result, mask)) != -1) {\n            below_threshold = false;\n            break;\n        }\n    }\n\n    if (below_threshold) {\n        int remainder = size % 8;\n        if (remainder > 0) {\n            int remainder_array[8] = {0};\n            for (int i = 0; i < remainder; ++i) {\n                remainder_array[i] = l[size - remainder + i];\n            }\n            __m256i vec = _mm256_loadu_si256((__m256i*)remainder_array);\n            result = _mm256_cmpgt_epi32(threshold, vec);\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(result, mask)) != (0xFFFF >> ((8 - remainder) * 16))) {\n                below_threshold = false;\n            }\n        }\n    }\n\n    return below_threshold;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int size = l.size();\n    int i = 0;\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        result = _mm256_cmpgt_epi32(threshold, vec);\n        mask = _mm256_and_si256(mask, result);\n    }\n\n    int mask_value = _mm256_movemask_epi8(_mm256_packs_epi32(mask, mask));\n    if (mask_value != 0xFFFFFFFF) {\n        return false;\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n\n        int open_count = _mm256_movemask_epi8(open);\n        int close_count = _mm256_movemask_epi8(close);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.length();\n    int i = 0;\n    int count = 0;\n\n    for (; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open = _mm256_set1_epi8('<');\n        __m256i close = _mm256_set1_epi8('>');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, open);\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, close);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        count += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n\n        if (count < 0) return false;\n    }\n\n    for (; i < len; ++i) {\n        if (brackets[i] == '<') {\n            ++count;\n        } else if (brackets[i] == '>') {\n            --count;\n        }\n        if (count < 0) return false;\n    }\n\n    return count == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n    int i = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n\n        int open_count = _mm256_movemask_epi8(open);\n        int close_count = _mm256_movemask_epi8(close);\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i brackets_chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_brackets = _mm256_cmpeq_epi8(brackets_chunk, _mm256_set1_epi8('<'));\n        __m256i close_brackets = _mm256_cmpeq_epi8(brackets_chunk, _mm256_set1_epi8('>'));\n\n        int open_count = _mm256_movemask_epi8(open_brackets);\n        int close_count = _mm256_movemask_epi8(close_brackets);\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n\n        int open_count = _mm256_movemask_epi8(open);\n        int close_count = _mm256_movemask_epi8(close);\n\n        balance += _mm_popcnt_u32(open_count) - _mm_popcnt_u32(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '<') {\n            ++balance;\n        } else if (brackets[i] == '>') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i bracket_chars = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_brackets = _mm256_cmpeq_epi8(bracket_chars, _mm256_set1_epi8('('));\n        __m256i close_brackets = _mm256_cmpeq_epi8(bracket_chars, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_movemask_epi8(open_brackets);\n        int close_count = _mm256_movemask_epi8(close_brackets);\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i bracket_chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open_mask = _mm256_cmpeq_epi8(bracket_chunk, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(bracket_chunk, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_movemask_epi8(_mm256_cmpgt_epi8(open_mask, _mm256_setzero_si256()));\n        int close_count = _mm256_movemask_epi8(_mm256_cmpgt_epi8(close_mask, _mm256_setzero_si256()));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open = _mm256_set1_epi8('(');\n        __m256i close = _mm256_set1_epi8(')');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, open);\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, close);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        open_count = _mm_popcnt_u32(open_count);\n        close_count = _mm_popcnt_u32(close_count);\n\n        balance += open_count - close_count;\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i brackets_vec = _mm256_loadu_si256((__m256i*)(brackets.data() + i));\n        __m256i open_mask = _mm256_set1_epi8('(');\n        __m256i close_mask = _mm256_set1_epi8(')');\n\n        __m256i open_cmp = _mm256_cmpeq_epi8(brackets_vec, open_mask);\n        __m256i close_cmp = _mm256_cmpeq_epi8(brackets_vec, close_mask);\n\n        __m256i open_count = _mm256_sub_epi8(_mm256_setzero_si256(), open_cmp);\n        __m256i close_count = _mm256_sub_epi8(_mm256_setzero_si256(), close_cmp);\n\n        __m256i balance_vec = _mm256_add_epi8(open_count, close_count);\n\n        balance += _mm256_extract_epi8(balance_vec, 0) + _mm256_extract_epi8(balance_vec, 16);\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int i = 0;\n    int balance = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&brackets[i]);\n        __m256i open = _mm256_set1_epi8('(');\n        __m256i close = _mm256_set1_epi8(')');\n\n        __m256i open_mask = _mm256_cmpeq_epi8(chunk, open);\n        __m256i close_mask = _mm256_cmpeq_epi8(chunk, close);\n\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        open_count = _mm_popcnt_u32(open_count);\n        close_count = _mm_popcnt_u32(close_count);\n\n        balance += open_count - close_count;\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (brackets[i] == '(') {\n            ++balance;\n        } else if (brackets[i] == ')') {\n            --balance;\n        }\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size() - 1;\n    std::vector<float> result(n);\n\n    for (int i = 0; i < n; i += 8) {\n        int j = i + 1;\n        __m256 coeff = _mm256_loadu_ps(&xs[j]);\n        __m256 exponents = _mm256_setr_ps(j, j+1, j+2, j+3, j+4, j+5, j+6, j+7);\n        __m256 derivative = _mm256_mul_ps(coeff, exponents);\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    for (int i = 0; i < n - 1; i += 8) {\n        int j = i + 1;\n        __m256 coeff = _mm256_loadu_ps(&xs[j]);\n        __m256 indices = _mm256_setr_ps(j, j + 1, j + 2, j + 3, j + 4, j + 5, j + 6, j + 7);\n        __m256 deriv = _mm256_mul_ps(coeff, indices);\n        _mm256_storeu_ps(&result[i], deriv);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256 coeff = _mm256_loadu_ps(&xs[i]);\n        __m256 exponent = _mm256_setr_ps(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256 derivative = _mm256_mul_ps(coeff, exponent);\n        _mm256_storeu_ps(&result[i - 1], derivative);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    for (int i = 0; i < n - 1; i += 8) {\n        __m256 coeff = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 exponent = _mm256_setr_ps(i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7, i + 8);\n        __m256 derivative = _mm256_mul_ps(coeff, exponent);\n        _mm256_storeu_ps(&result[i], derivative);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    size_t n = xs.size() - 1;\n    std::vector<float> result(n);\n\n    __m256 coeff, mult, deriv;\n    float *coeff_ptr = xs.data() + 1;\n    float *result_ptr = result.data();\n\n    for (size_t i = 0; i < n; i += 8) {\n        size_t j = i + 8;\n        coeff = _mm256_loadu_ps(coeff_ptr + i);\n        mult = _mm256_setr_ps(1, 2, 3, 4, 5, 6, 7, 8);\n        mult = _mm256_add_ps(mult, _mm256_set1_ps(i));\n        deriv = _mm256_mul_ps(coeff, mult);\n        _mm256_storeu_ps(result_ptr + i, deriv);\n    }\n\n    for (size_t i = n; i < n + 8; ++i) {\n        if (i < n) {\n            result[i] = xs[i + 1] * (i + 1);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    int len = s.length();\n    char *str = const_cast<char*>(s.c_str());\n\n    __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    for (int i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        count += _mm256_movemask_epi8(cmp);\n    }\n\n    for (int i = (len / 32) * 32; i < len; ++i) {\n        if (str[i] == 'y' && i == len - 1) {\n            count++;\n        } else if (str[i] == 'a' || str[i] == 'e' || str[i] == 'i' || str[i] == 'o' || str[i] == 'u' ||\n                   str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n    int count = 0;\n\n    __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    for (int i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (int i = (len / 32) * 32; i < len; ++i) {\n        if (strchr(\"aeiouAEIOU\", str[i]) != nullptr) {\n            count++;\n        }\n    }\n\n    if (str[len - 1] == 'y' || str[len - 1] == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    int count = 0;\n    char vowels[] = \"aeiouAEIOU\";\n    __m256i vowel_mask = _mm256_loadu_si256((__m256i*)vowels);\n\n    for (int i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i result = _mm256_cmpeq_epi8(chunk, vowel_mask);\n        count += _mm256_movemask_epi8(result);\n    }\n\n    for (int i = (len / 32) * 32; i < len; ++i) {\n        if (strchr(vowels, s[i])) {\n            count++;\n        }\n    }\n\n    if (s.back() == 'y' || s.back() == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    int len = s.length();\n    char *str = const_cast<char*>(s.c_str());\n\n    __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    for (int i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        count += _mm256_movemask_epi8(cmp);\n    }\n\n    for (int i = (len / 32) * 32; i < len; ++i) {\n        if (strchr(\"aeiouAEIOU\", str[i]) != NULL) {\n            count++;\n        }\n    }\n\n    if (len > 0 && toupper(str[len - 1]) == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    int len = s.length();\n    char *str = const_cast<char*>(s.c_str());\n\n    __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U',\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'\n    );\n\n    for (int i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        count += _mm256_movemask_epi8(cmp);\n    }\n\n    for (int i = (len / 32) * 32; i < len; ++i) {\n        if (str[i] == 'a' || str[i] == 'e' || str[i] == 'i' || str[i] == 'o' || str[i] == 'u' ||\n            str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U') {\n            count++;\n        }\n    }\n\n    if (!s.empty() && (s.back() == 'y' || s.back() == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int n = s.size();\n    int i = 0;\n\n    __m256i vecSum = _mm256_setzero_si256();\n    __m256i upperCaseMask = _mm256_set1_epi8('A');\n    __m256i lowerCaseMask = _mm256_set1_epi8('Z');\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i isUpper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upperCaseMask), _mm256_cmplt_epi8(chunk, lowerCaseMask));\n        __m256i upperCharacters = _mm256_and_si256(chunk, isUpper);\n        vecSum = _mm256_add_epi64(vecSum, _mm256_sad_epu8(upperCharacters, _mm256_setzero_si256()));\n    }\n\n    unsigned int temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vecSum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < n; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int n = s.size();\n    int i = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper_mask = _mm256_set1_epi8('A');\n        __m256i lower_mask = _mm256_set1_epi8('Z');\n        __m256i upper_chars = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_mask), _mm256_cmplt_epi8(chunk, lower_mask));\n        __m256i zero = _mm256_setzero_si256();\n        __m256i ascii_values = _mm256_unpacklo_epi8(upper_chars, zero);\n        ascii_values = _mm256_add_epi16(ascii_values, _mm256_unpackhi_epi8(upper_chars, zero));\n        ascii_values = _mm256_add_epi32(ascii_values, _mm256_srli_si256(ascii_values, 8));\n        ascii_values = _mm256_add_epi32(ascii_values, _mm256_srli_si256(ascii_values, 4));\n        sum += _mm256_extract_epi32(ascii_values, 0) + _mm256_extract_epi32(ascii_values, 4);\n    }\n\n    for (; i < n; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int n = s.size();\n    int i = 0;\n\n    // Process 32 bytes (32 characters) at a time\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i upper_mask = _mm256_set1_epi8('A');\n        __m256i lower_mask = _mm256_set1_epi8('Z');\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, upper_mask), _mm256_cmplt_epi8(chunk, lower_mask));\n\n        // Extract the uppercase characters and sum their ASCII values\n        __m256i upper_chars = _mm256_and_si256(chunk, is_upper);\n        __m256i sums = _mm256_sad_epu8(upper_chars, _mm256_setzero_si256());\n\n        // Horizontally add the sums\n        sum += _mm256_extract_epi32(sums, 0) + _mm256_extract_epi32(sums, 2) +\n               _mm256_extract_epi32(sums, 4) + _mm256_extract_epi32(sums, 6);\n    }\n\n    // Process remaining characters\n    for (; i < n; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int n = s.size();\n    int i = 0;\n\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z' - 'A' + 1));\n        __m256i upper = _mm256_and_si256(chunk, mask);\n        __m256i zero = _mm256_setzero_si256();\n\n        __m256i sum1 = _mm256_sad_epu8(zero, upper);\n        __m256i sum2 = _mm256_sad_epu8(zero, _mm256_shuffle_epi32(upper, _MM_SHUFFLE(1, 0, 3, 2)));\n\n        sum += _mm256_extract_epi16(sum1, 0) + _mm256_extract_epi16(sum1, 4) +\n               _mm256_extract_epi16(sum2, 0) + _mm256_extract_epi16(sum2, 4);\n    }\n\n    for (; i < n; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int n = s.size();\n    char* data = s.data();\n\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_upper_mask = _mm256_set1_epi8('A');\n    __m256i v_lower_mask = _mm256_set1_epi8('Z');\n\n    for (int i = 0; i <= n - 32; i += 32) {\n        __m256i v_data = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i v_upper = _mm256_and_si256(_mm256_cmpgt_epi8(v_data, v_upper_mask), _mm256_cmpgt_epi8(v_lower_mask, v_data));\n        __m256i v_ascii = _mm256_and_si256(v_data, v_upper);\n        v_sum = _mm256_add_epi64(v_sum, _mm256_sad_epu8(v_ascii, _mm256_setzero_si256()));\n    }\n\n    unsigned int temp[4];\n    _mm256_storeu_si256((__m256i*)temp, v_sum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (int i = (n & ~31); i < n; ++i) {\n        if (data[i] >= 'A' && data[i] <= 'Z') {\n            sum += data[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    __m256i even_mask = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i is_even = _mm256_and_si256(chunk, even_mask);\n        __m256i is_zero = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(is_zero));\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            mask ^= (1 << bit);\n            if (arr[i + bit] < min_even) {\n                min_even = arr[i + bit];\n                min_index = i + bit;\n            }\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    __m256i min_even_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i index_vec = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec, _mm256_set1_epi32(1)));\n        __m256i even_vec = _mm256_blendv_epi8(vec, _mm256_set1_epi32(INT_MAX), even_mask);\n\n        __m256i min_vec = _mm256_min_epi32(min_even_vec, even_vec);\n        __m256i min_index_vec = _mm256_blendv_epi8(index_vec, _mm256_set1_epi32(i + 7), _mm256_cmpeq_epi32(min_vec, even_vec));\n\n        int min_vals[8];\n        int min_indices[8];\n        _mm256_storeu_si256((__m256i*)min_vals, min_vec);\n        _mm256_storeu_si256((__m256i*)min_indices, min_index_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (min_vals[j] < min_even) {\n                min_even = min_vals[j];\n                min_index = min_indices[j];\n            }\n        }\n\n        index_vec = _mm256_add_epi32(index_vec, _mm256_set1_epi32(8));\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = INT_MAX;\n    int min_index = -1;\n    __m256i vmin_even = _mm256_set1_epi32(INT_MAX);\n    __m256i vmin_index = _mm256_set1_epi32(-1);\n    __m256i vindex = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vdata = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vis_even = _mm256_and_si256(vdata, _mm256_set1_epi32(1));\n        __m256i vcmp = _mm256_cmpeq_epi32(vis_even, _mm256_setzero_si256());\n        __m256i vmin_curr = _mm256_blendv_epi8(vmin_even, vdata, vcmp);\n        __m256i vindex_curr = _mm256_add_epi32(vindex, _mm256_set1_epi32(i));\n        __m256i vmin_index_curr = _mm256_blendv_epi8(vmin_index, vindex_curr, vcmp);\n\n        vmin_even = _mm256_min_epu32(vmin_even, vmin_curr);\n        vmin_index = _mm256_blendv_epi8(vmin_index, vmin_index_curr, _mm256_cmplt_epi32(vmin_even, vmin_curr));\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vmin_even);\n    for (int i = 0; i < 8; ++i) {\n        if (temp[i] < min_even) {\n            min_even = temp[i];\n        }\n    }\n\n    _mm256_storeu_si256((__m256i*)temp, vmin_index);\n    for (int i = 0; i < 8; ++i) {\n        if (temp[i] != -1 && (temp[i] < min_index || min_index == -1)) {\n            min_index = temp[i];\n        }\n    }\n\n    if (min_even == INT_MAX) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_val = INT_MAX;\n    int min_index = -1;\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i index_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n        __m256i zero_vec = _mm256_cmpeq_epi32(even_vec, _mm256_setzero_si256());\n        __m256i min_cmp = _mm256_min_epu32(min_vec, vec);\n        __m256i mask = _mm256_cmpgt_epi32(min_vec, vec);\n\n        int mask_bits = _mm256_movemask_epi8(_mm256_packs_epi32(mask, mask));\n        if (mask_bits) {\n            int offset = __builtin_ctz(mask_bits) / 4;\n            int value = _mm256_extract_epi32(vec, offset);\n            int index = i + offset;\n            if (value < min_val) {\n                min_val = value;\n                min_index = index;\n            }\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_val, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int n = arr.size();\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    __m256i min_idx = _mm256_set1_epi32(INT_MAX);\n    __m256i idx = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec, _mm256_set1_epi32(1)));\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n\n        __m256i cmp = _mm256_cmpgt_epi32(min_val, even_vec);\n        min_val = _mm256_blendv_epi8(min_val, even_vec, cmp);\n        min_idx = _mm256_blendv_epi8(min_idx, _mm256_add_epi32(idx, _mm256_set1_epi32(i)), cmp);\n\n        idx = _mm256_add_epi32(idx, _mm256_set1_epi32(8));\n    }\n\n    int min_vals[8];\n    int min_idxs[8];\n    _mm256_storeu_si256((__m256i*)min_vals, min_val);\n    _mm256_storeu_si256((__m256i*)min_idxs, min_idx);\n\n    int smallest_val = INT_MAX;\n    int smallest_idx = INT_MAX;\n    for (int i = 0; i < 8; ++i) {\n        if (min_vals[i] < smallest_val) {\n            smallest_val = min_vals[i];\n            smallest_idx = min_idxs[i];\n        }\n    }\n\n    if (smallest_val == INT_MAX) return {};\n    return {smallest_val, smallest_idx};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int max_val = -1;\n\n    // Pad the vector to make its size a multiple of 8\n    while (n % 8 != 0) {\n        lst.push_back(0);\n        n++;\n    }\n\n    // Allocate memory for frequency count\n    std::vector<int> freq(n, 0);\n\n    // Count frequencies using AVX2 intrinsics\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i freq_vec = _mm256_loadu_si256((__m256i*)&freq[i]);\n\n        for (int j = 0; j < n; j += 8) {\n            __m256i comp_vec = _mm256_loadu_si256((__m256i*)&lst[j]);\n            __m256i mask = _mm256_cmpeq_epi32(vec, comp_vec);\n            freq_vec = _mm256_add_epi32(freq_vec, mask);\n        }\n\n        _mm256_storeu_si256((__m256i*)&freq[i], freq_vec);\n    }\n\n    // Find the greatest integer that meets the criteria\n    for (int i = 0; i < n; ++i) {\n        if (lst[i] > 0 && freq[i] >= lst[i] && freq[i] > max_val) {\n            max_val = lst[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int max_val = -1;\n\n    // Pad the list to make its size a multiple of 8\n    while (n % 8 != 0) {\n        lst.push_back(0);\n        n = lst.size();\n    }\n\n    // Allocate memory for frequency count\n    std::vector<int> freq(n, 0);\n\n    // Count frequencies using AVX2 intrinsics\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i ones = _mm256_set1_epi32(1);\n\n        for (int j = 0; j < n; j += 8) {\n            __m256i cmp = _mm256_cmpeq_epi32(vec, _mm256_loadu_si256((__m256i*)&lst[j]));\n            __m256i count = _mm256_sub_epi32(_mm256_setzero_si256(), _mm256_and_si256(cmp, ones));\n            _mm256_storeu_si256((__m256i*)&freq[j], _mm256_add_epi32(_mm256_loadu_si256((__m256i*)&freq[j]), count));\n        }\n    }\n\n    // Find the greatest integer that meets the criteria\n    for (int i = 0; i < n; ++i) {\n        if (lst[i] > 0 && freq[i] >= lst[i] && lst[i] > max_val) {\n            max_val = lst[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int max_val = -1;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        for (int j = 0; j < n; j += 8) {\n            __m256i cmp = _mm256_cmpeq_epi32(vec, _mm256_loadu_si256((__m256i*)&lst[j]));\n            count = _mm256_add_epi32(count, cmp);\n        }\n\n        int counts[8];\n        _mm256_storeu_si256((__m256i*)counts, count);\n\n        for (int k = 0; k < 8; ++k) {\n            if (counts[k] >= lst[i + k] && lst[i + k] > max_val) {\n                max_val = lst[i + k];\n            }\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int max_val = -1;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        for (int j = 0; j < n; j += 8) {\n            __m256i cmp = _mm256_cmpeq_epi32(vec, _mm256_loadu_si256((__m256i*)&lst[j]));\n            count = _mm256_add_epi32(count, cmp);\n        }\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, count);\n\n        for (int k = 0; k < 8; ++k) {\n            if (temp[k] >= lst[i + k] && lst[i + k] > max_val) {\n                max_val = lst[i + k];\n            }\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    int max_val = -1;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i count = _mm256_setzero_si256();\n\n        for (int j = 0; j < n; j += 8) {\n            __m256i cmp = _mm256_cmpeq_epi32(vec, _mm256_loadu_si256((__m256i*)&lst[j]));\n            count = _mm256_add_epi32(count, cmp);\n        }\n\n        int counts[8];\n        _mm256_storeu_si256((__m256i*)counts, count);\n\n        for (int k = 0; k < 8; ++k) {\n            if (counts[k] >= lst[i + k] && lst[i + k] > max_val) {\n                max_val = lst[i + k];\n            }\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using AVX2 intrinsics\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum = _mm256_add_epi32(sum, vec);\n    }\n\n    // Horizontal sum of the vector\n    __m128i sum_lo = _mm256_castsi256_si128(sum);\n    __m128i sum_hi = _mm256_extractf128_si256(sum, 1);\n    sum_lo = _mm_add_epi32(sum_lo, sum_hi);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n\n    int total_sum = _mm_extract_epi32(sum_lo, 0);\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; i += 8) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&q[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&q[n - i - 8]);\n        __m256i cmp = _mm256_cmpgt_epi32(v1, _mm256_setzero_si256());\n        if (!_mm256_testz_si256(cmp, _mm256_xor_si256(v1, v2))) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of the vector elements\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum = _mm256_add_epi32(sum, v);\n    }\n\n    // Horizontal sum of the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    // Calculate the sum of elements using AVX2 intrinsics\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < n; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum = _mm256_add_epi32(sum, chunk);\n    }\n\n    // Horizontally add the elements of the sum vector\n    __m128i sum_lo = _mm256_castsi256_si128(sum);\n    __m128i sum_hi = _mm256_extractf128_si256(sum, 1);\n    sum_lo = _mm_add_epi32(sum_lo, sum_hi);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n\n    int total_sum = _mm_extract_epi32(sum_lo, 0);\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    bool is_palindromic = true;\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindromic = false;\n            break;\n        }\n    }\n\n    if (!is_palindromic) return false;\n\n    // Calculate the sum of the vector elements using AVX2\n    __m256i sum = _mm256_setzero_si256();\n    __m256i* ptr = reinterpret_cast<__m256i*>(q.data());\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        sum = _mm256_add_epi32(sum, _mm256_loadu_si256(&ptr[i / 8]));\n    }\n\n    // Horizontal sum of the remaining elements\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (; i < n; ++i) {\n        total_sum += q[i];\n    }\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    bool is_palindromic = true;\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindromic = false;\n            break;\n        }\n    }\n\n    if (!is_palindromic) return false;\n\n    // Calculate the sum of the vector elements using AVX2\n    __m256i sum = _mm256_setzero_si256();\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum = _mm256_add_epi32(sum, vec);\n    }\n\n    // Horizontally add the elements of the sum vector\n    __m128i sum_lo = _mm256_castsi256_si128(sum);\n    __m128i sum_hi = _mm256_extractf128_si256(sum, 1);\n    sum_lo = _mm_add_epi32(sum_lo, sum_hi);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n    sum_lo = _mm_hadd_epi32(sum_lo, sum_lo);\n    int total_sum = _mm_extract_epi32(sum_lo, 0);\n\n    // Add any remaining elements\n    for (; i < n; ++i) {\n        total_sum += q[i];\n    }\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[n - 8 - i]);\n        v2 = _mm256_permute4x64_epi64(_mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0), 0b11011000);\n\n        __m256i cmp = _mm256_cmpeq_epi32(v1, v2);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        changes += __builtin_popcount(~mask);\n    }\n\n    for (int i = (n / 8) * 8; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n        __m256i vec2_rev = _mm256_permute4x64_epi64(_mm256_permute2x128_si256(vec2, vec2, _MM_SHUFFLE(0, 1, 0, 1)), _MM_SHUFFLE(0, 1, 2, 3));\n        __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2_rev);\n        changes += 8 - _mm256_movemask_ps((__m256)cmp);\n    }\n\n    for (int i = (n / 2) * 8; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    for (; i < j; i += 8, j -= 8) {\n        __m256i vi = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vj = _mm256_loadu_si256((__m256i*)&arr[j]);\n        __m256i diff = _mm256_xor_si256(vi, vj);\n        __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i cmp = _mm256_cmpeq_epi32(diff, mask);\n        int mask_cmp = _mm256_movemask_epi8(cmp);\n        changes += _mm_popcnt_u32(mask_cmp);\n    }\n\n    for (; i < j; ++i, --j) {\n        if (arr[i] != arr[j]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 8) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n        v2 = _mm256_permute4x64_epi64(_mm256_set_epi64x(arr[n - i - 1], arr[n - i - 2], arr[n - i - 3], arr[n - i - 4]), _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        __m256i cmp = _mm256_cmpeq_epi32(v1, v2);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        changes += _mm_popcnt_u32(~mask);\n    }\n\n    for (int i = (n / 2) * 8; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&arr[n - 8 - i]);\n        v2 = _mm256_permute4x64_epi64(_mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3)), _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m256i cmp = _mm256_cmpeq_epi32(v1, v2);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        changes += _mm_popcnt_u32(~mask);\n    }\n\n    for (int i = (n / 8) * 8; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m256i vec1 = _mm256_set1_epi32(totalChars1);\n    __m256i vec2 = _mm256_set1_epi32(totalChars2);\n\n    __m256i result = _mm256_cmpgt_epi32(vec2, vec1);\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(result, result));\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    if (totalChars1 <= totalChars2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m256i vec1 = _mm256_set1_epi32(totalChars1);\n    __m256i vec2 = _mm256_set1_epi32(totalChars2);\n\n    __m256i result = _mm256_cmpgt_epi32(vec1, vec2);\n\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(result, result));\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m256i vec1 = _mm256_set1_epi32(totalChars1);\n    __m256i vec2 = _mm256_set1_epi32(totalChars2);\n\n    __m256i result = _mm256_cmpgt_epi32(vec2, vec1);\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(result, result));\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string>& lst1, const std::vector<std::string>& lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m256i vec1 = _mm256_set1_epi32(totalChars1);\n    __m256i vec2 = _mm256_set1_epi32(totalChars2);\n\n    __m256i result = _mm256_cmpgt_epi32(vec1, vec2);\n\n    int mask = _mm256_movemask_epi8(result);\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1) return true;\n    if (n == 1) return false;\n\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i power_vec = _mm256_set1_epi64x(n);\n    __m256i exponent_vec = _mm256_set_epi64x(3, 2, 1, 0);\n\n    __m256i result_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i exp = _mm256_set1_epi64x(exponent_vec[i]);\n        __m256i temp = _mm256_set1_epi64x(n);\n\n        while (_mm256_movemask_epi8(_mm256_cmpeq_epi64(exp, one_vec)) != 0xFFFFFFFF) {\n            temp = _mm256_mul_epu32(temp, n_vec);\n            exp = _mm256_sub_epi64(exp, one_vec);\n        }\n\n        result_vec = _mm256_or_si256(result_vec, _mm256_cmpeq_epi64(temp, x_vec));\n    }\n\n    return _mm256_movemask_epi8(result_vec) != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i result_vec = one_vec;\n\n    for (int i = 0; i < 4; ++i) {\n        result_vec = _mm256_mul_epu32(result_vec, n_vec);\n        __m256i cmp_vec = _mm256_cmpeq_epi64(result_vec, x_vec);\n        if (_mm256_movemask_epi8(cmp_vec)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i result_vec = one_vec;\n    __m256i power_vec = one_vec;\n\n    for (int i = 0; i < 4; ++i) {\n        power_vec = _mm256_mul_epu32(power_vec, n_vec);\n        result_vec = _mm256_or_si256(result_vec, _mm256_cmpeq_epi64(power_vec, x_vec));\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, result_vec);\n\n    return (result[0] || result[1] || result[2] || result[3]);\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i result_vec = one_vec;\n    __m256i power_vec = one_vec;\n\n    for (int i = 0; i < 4; ++i) {\n        power_vec = _mm256_mul_epu32(power_vec, n_vec);\n        result_vec = _mm256_or_si256(result_vec, _mm256_cmpeq_epi64(power_vec, x_vec));\n    }\n\n    uint64_t mask = _mm256_movemask_pd(_mm256_castsi256_pd(result_vec));\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n == 1) return false;\n    if (x == 1) return true;\n\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i x_vec = _mm256_set1_epi64x(x);\n    __m256i one_vec = _mm256_set1_epi64x(1);\n    __m256i power_vec = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 4; ++i) {\n        power_vec = _mm256_mul_epu32(power_vec, n_vec);\n        __m256i cmp_vec = _mm256_cmpeq_epi64(power_vec, x_vec);\n        if (_mm256_movemask_epi8(cmp_vec)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    const char* str = num.c_str();\n    size_t length = num.length();\n\n    __m256i prime_mask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    __m256i count = _mm256_setzero_si256();\n    size_t i;\n\n    for (i = 0; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(str + i));\n        __m256i is_prime = _mm256_and_si256(_mm256_cmpeq_epi8(chunk, chunk), prime_mask);\n        count = _mm256_add_epi8(count, is_prime);\n    }\n\n    __m128i count_low = _mm256_castsi256_si128(count);\n    __m128i count_high = _mm256_extractf128_si256(count, 1);\n    count_low = _mm_add_epi8(count_low, count_high);\n\n    __m128i sum = _mm_sad_epu8(count_low, _mm_setzero_si128());\n    int result = _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4);\n\n    for (; i < length; ++i) {\n        char c = str[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    int len = num.length();\n    int count = 0;\n\n    __m256i primeMask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    for (int i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(hexChars + i));\n        __m256i masked = _mm256_and_si256(chunk, primeMask);\n        __m256i compared = _mm256_cmpeq_epi8(masked, primeMask);\n        int mask = _mm256_movemask_epi8(compared);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    int len = num.length();\n    int count = 0;\n\n    __m256i primeMask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    for (int i = 0; i < len; i += 32) {\n        int remaining = len - i;\n        int loadSize = remaining < 32 ? remaining : 32;\n\n        __m256i hexChunk = _mm256_loadu_si256((__m256i*)(hexChars + i));\n        __m256i masked = _mm256_and_si256(hexChunk, _mm256_set1_epi8(0xDF));\n        __m256i cmpResult = _mm256_cmpeq_epi8(masked, primeMask);\n\n        unsigned int mask = _mm256_movemask_epi8(cmpResult);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    size_t length = num.length();\n    int count = 0;\n\n    __m256i primeMask = _mm256_setr_epi8(\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0\n    );\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(hexChars + i));\n        __m256i masked = _mm256_and_si256(chunk, primeMask);\n        __m256i compared = _mm256_cmpeq_epi8(masked, primeMask);\n        count += _mm256_movemask_epi8(compared) & 0b00000101000001010000010100000101;\n    }\n\n    if (length % 32 != 0) {\n        size_t remaining = length % 32;\n        char temp[32] = {0};\n        memcpy(temp, hexChars + length - remaining, remaining);\n\n        __m256i chunk = _mm256_loadu_si256((__m256i*)temp);\n        __m256i masked = _mm256_and_si256(chunk, primeMask);\n        __m256i compared = _mm256_cmpeq_epi8(masked, primeMask);\n        count += _mm256_movemask_epi8(compared) & 0b00000101000001010000010100000101;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string &num) {\n    const __m256i prime_hex = _mm256_setr_epi8(\n        '2', '3', '5', '7', 'B', 'D', 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    int count = 0;\n    for (size_t i = 0; i < num.length(); i += 32) {\n        size_t remaining = num.length() - i;\n        size_t chunk_size = remaining < 32 ? remaining : 32;\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        __m256i mask = _mm256_set1_epi8(0xDF); // Mask to convert to uppercase\n        chunk = _mm256_and_si256(chunk, mask);\n\n        __m256i cmp_result = _mm256_cmpeq_epi8(chunk, prime_hex);\n        __m256i mask_result = _mm256_and_si256(cmp_result, _mm256_set1_epi8(1));\n\n        int temp_count = _mm256_movemask_epi8(mask_result);\n        count += _mm_popcnt_u32(temp_count);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    __m256i prev2, prev1, curr;\n    prev2 = _mm256_set1_epi8(s[0]);\n    prev1 = _mm256_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < len; ++i) {\n        curr = _mm256_set1_epi8(s[i]);\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(prev2, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(prev1, curr);\n        __m256i cmp3 = _mm256_cmpeq_epi8(prev2, curr);\n\n        __m256i result = _mm256_or_si256(cmp1, cmp2);\n        result = _mm256_or_si256(result, cmp3);\n\n        unsigned int mask = _mm256_movemask_epi8(result);\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = curr;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    __m256i prev2, prev1, curr;\n    prev2 = _mm256_set1_epi8(s[0]);\n    prev1 = _mm256_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < len; ++i) {\n        curr = _mm256_set1_epi8(s[i]);\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(prev2, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(prev1, curr);\n        __m256i cmp3 = _mm256_cmpeq_epi8(prev2, curr);\n\n        __m256i result = _mm256_or_si256(cmp1, cmp2);\n        result = _mm256_or_si256(result, cmp3);\n\n        unsigned int mask = _mm256_movemask_epi8(result);\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = curr;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    __m256i prev2, prev1, curr;\n    prev2 = _mm256_set1_epi8(s[0]);\n    prev1 = _mm256_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < len; ++i) {\n        curr = _mm256_set1_epi8(s[i]);\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(prev2, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(prev1, curr);\n        __m256i cmp3 = _mm256_cmpeq_epi8(prev2, curr);\n\n        __m256i result = _mm256_or_si256(cmp1, cmp2);\n        result = _mm256_or_si256(result, cmp3);\n\n        unsigned int mask = _mm256_movemask_epi8(result);\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = curr;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    __m256i prev2 = _mm256_set1_epi8(s[0]);\n    __m256i prev1 = _mm256_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < len; i++) {\n        __m256i current = _mm256_set1_epi8(s[i]);\n        __m256i cmp1 = _mm256_cmpeq_epi8(current, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(current, prev2);\n        __m256i cmp3 = _mm256_cmpeq_epi8(prev1, prev2);\n\n        __m256i result = _mm256_or_si256(cmp1, cmp2);\n        result = _mm256_or_si256(result, cmp3);\n\n        unsigned int mask = _mm256_movemask_epi8(result);\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = current;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    __m256i prev2, prev1, curr;\n    prev2 = _mm256_set1_epi8(' ');\n    prev1 = _mm256_set1_epi8(' ');\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t remaining = len - i;\n        size_t load_size = remaining < 32 ? remaining : 32;\n\n        curr = _mm256_loadu_si256((__m256i*)(s.data() + i));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(curr, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(curr, prev2);\n        __m256i cmp_mask = _mm256_or_si256(cmp1, cmp2);\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(cmp_mask, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n            return false;\n        }\n\n        prev2 = _mm256_bslli_si256(prev1, 1);\n        prev1 = _mm256_bslli_si256(curr, 1);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    __m256i vlen = _mm256_set1_epi32(len);\n    __m256i vprime = _mm256_set1_epi32(1);\n\n    for (int i = 2; i * i <= len; i++) {\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i vmod = _mm256_rem_epu32(vlen, vi);\n        __m256i vcmp = _mm256_cmpeq_epi32(vmod, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(_mm256_packs_epi32(vcmp, vcmp)) != 0) {\n            vprime = _mm256_set1_epi32(0);\n            break;\n        }\n    }\n\n    int prime_mask = _mm256_movemask_epi8(_mm256_packs_epi32(vprime, vprime));\n    return (prime_mask != 0) && is_prime(len);\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    __m256i len_vec = _mm256_set1_epi32(len);\n    __m256i prime_vec = _mm256_set1_epi32(1);\n\n    for (int i = 2; i <= std::sqrt(len); ++i) {\n        __m256i i_vec = _mm256_set1_epi32(i);\n        __m256i mod_vec = _mm256_rem_epi32(len_vec, i_vec);\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, zero_vec);\n        int mask = _mm256_movemask_epi8(cmp_vec);\n        if (mask != 0) {\n            prime_vec = _mm256_set1_epi32(0);\n            break;\n        }\n    }\n\n    int result = _mm256_movemask_epi8(prime_vec);\n    return (result != 0);\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    return is_prime(len);\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    return is_prime(len);\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    return is_prime(len);\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    __m256i vec = _mm256_set1_epi64x(sum);\n    __m256i mask = _mm256_set1_epi64x(1);\n    std::string binary;\n\n    for (int i = 63; i >= 0; --i) {\n        __m256i result = _mm256_and_si256(vec, mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i compare = _mm256_cmpeq_epi64(result, zero);\n        int bit = !_mm256_testz_si256(compare, _mm256_set1_epi64x(-1));\n        binary += std::to_string(bit);\n        vec = _mm256_srli_epi64(vec, 1);\n    }\n\n    while (binary.back() == '0') {\n        binary.pop_back();\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    __m256i v = _mm256_set1_epi64x(sum);\n    __m256i mask = _mm256_set1_epi64x(1);\n    std::string binary;\n\n    for (int i = 0; i < 64; ++i) {\n        __m256i result = _mm256_and_si256(v, mask);\n        int bit = _mm256_movemask_pd((__m256d)result) & 1;\n        binary = (bit ? \"1\" : \"0\") + binary;\n        v = _mm256_srli_epi64(v, _mm256_set1_epi64x(1));\n    }\n\n    while (binary.back() == '0') {\n        binary.pop_back();\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    __m256i v = _mm256_set1_epi64x(sum);\n    __m256i mask = _mm256_set1_epi64x(1);\n    std::string binary;\n\n    for (int i = 0; i < 64; i++) {\n        __m256i result = _mm256_and_si256(v, mask);\n        if (_mm256_movemask_epi8(result)) {\n            binary = \"1\" + binary;\n        } else {\n            binary = \"0\" + binary;\n        }\n        v = _mm256_srli_epi64(v, _mm256_set1_epi64x(1));\n    }\n\n    while (binary.front() == '0' && binary.size() > 1) {\n        binary.erase(0, 1);\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    uint64_t sum = 0;\n    while (N > 0) {\n        sum += N % 10;\n        N /= 10;\n    }\n\n    std::string binary_result;\n    while (sum > 0) {\n        binary_result = (sum % 2 == 0 ? \"0\" : \"1\") + binary_result;\n        sum /= 2;\n    }\n\n    return binary_result.empty() ? \"0\" : binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i digits = _mm256_set1_epi8(N % 10);\n    N /= 10;\n\n    while (N > 0) {\n        __m256i temp = _mm256_set1_epi8(N % 10);\n        sum = _mm256_add_epi8(sum, temp);\n        N /= 10;\n    }\n\n    sum = _mm256_add_epi8(sum, digits);\n    sum = _mm256_sad_epu8(sum, _mm256_setzero_si256());\n\n    uint64_t result = _mm256_extract_epi64(sum, 0) + _mm256_extract_epi64(sum, 1) + _mm256_extract_epi64(sum, 2) + _mm256_extract_epi64(sum, 3);\n\n    std::string binary = \"\";\n    while (result > 0) {\n        binary = (result % 2 == 0 ? \"0\" : \"1\") + binary;\n        result /= 2;\n    }\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int n = lst.size();\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i veven = _mm256_set1_epi32(2);\n\n    for (int i = 1; i < n - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i vmod = _mm256_and_si256(v, veven);\n        __m256i vcmp = _mm256_cmpeq_epi32(vmod, _mm256_setzero_si256());\n        __m256i vadd = _mm256_and_si256(v, vcmp);\n        vsum = _mm256_add_epi32(vsum, vadd);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (int i = n - n % 8; i < n; i += 2) {\n        if (i % 2 != 0 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int size = lst.size();\n    int i = 1;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i even_mask = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_vec = _mm256_and_si256(vec, even_mask);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        even_vec = _mm256_and_si256(even_vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, even_vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < size; i += 2) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x00000001);\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i even = _mm256_and_si256(vec, _mm256_set1_epi32(0xFFFFFFFE));\n        __m256i odd_indices = _mm256_and_si256(mask, _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0));\n        __m256i result = _mm256_and_si256(even, odd_indices);\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n    return temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i evenMask = _mm256_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_lddqu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i evenElements = _mm256_and_si256(vec, evenMask);\n        sum = _mm256_add_epi32(sum, evenElements);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n    return temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    int n = lst.size();\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_even = _mm256_set1_epi32(2);\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i vec_and = _mm256_and_si256(vec, vec_even);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_and, _mm256_setzero_si256());\n        vec = _mm256_and_si256(vec, vec_cmp);\n        vec_sum = _mm256_add_epi32(vec_sum, vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vec_sum);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    std::string result(s.size(), '\\0');\n    __m256i key = _mm256_set1_epi8(4);\n\n    for (size_t i = 0; i < s.size(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n        __m256i lower = _mm256_and_si256(mask, _mm256_add_epi8(chunk, key));\n        __m256i upper = _mm256_andnot_si256(mask, _mm256_add_epi8(chunk, key));\n        upper = _mm256_sub_epi8(upper, _mm256_set1_epi8(26));\n        __m256i encrypted = _mm256_or_si256(lower, upper);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), encrypted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const __m256i alpha_start = _mm256_set1_epi8('a');\n    const __m256i alpha_end = _mm256_set1_epi8('z' + 1);\n    const __m256i shift_vector = _mm256_set1_epi8(shift);\n\n    for (size_t i = 0; i < s.size(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, alpha_start), _mm256_cmplt_epi8(chunk, alpha_end));\n        __m256i shifted = _mm256_add_epi8(chunk, shift_vector);\n        __m256i overflow = _mm256_sub_epi8(_mm256_add_epi8(shifted, _mm256_set1_epi8('z' + 1)), alpha_end);\n        __m256i corrected = _mm256_or_si256(_mm256_and_si256(is_lower, overflow), _mm256_andnot_si256(is_lower, shifted));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), corrected);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const __m256i alpha_start = _mm256_set1_epi8('a');\n    const __m256i alpha_end = _mm256_set1_epi8('z' + 1);\n    const __m256i shift_vector = _mm256_set1_epi8(shift);\n\n    for (size_t i = 0; i < s.size(); i += 32) {\n        size_t remaining = s.size() - i;\n        size_t chunk_size = (remaining >= 32) ? 32 : remaining;\n\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, alpha_start), _mm256_cmplt_epi8(chunk, alpha_end));\n        __m256i shifted = _mm256_add_epi8(chunk, shift_vector);\n        __m256i overflow = _mm256_cmpgt_epi8(shifted, alpha_end);\n        __m256i corrected = _mm256_sub_epi8(shifted, _mm256_and_si256(overflow, _mm256_set1_epi8(26)));\n        __m256i result_chunk = _mm256_blendv_epi8(chunk, corrected, is_lower);\n\n        _mm256_storeu_si256((__m256i*)&result[i], result_chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int vec_size = 32; // AVX2 register size in bytes\n\n    for (size_t i = 0; i < s.size(); i += vec_size) {\n        size_t len = std::min(s.size() - i, vec_size);\n\n        // Load data into AVX2 register\n        __m256i data = _mm256_loadu_si256((__m256i*)(s.data() + i));\n\n        // Encrypt by shifting characters\n        data = _mm256_add_epi8(data, _mm256_set1_epi8(shift));\n\n        // Store the result back into the string\n        _mm256_storeu_si256((__m256i*)(result.data() + i), data);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    std::string result(s.size(), '\\0');\n    __m256i key = _mm256_set1_epi8(4);\n\n    for (size_t i = 0; i < s.size(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i mask = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n        __m256i lower = _mm256_and_si256(mask, _mm256_add_epi8(chunk, key));\n        __m256i upper = _mm256_andnot_si256(mask, _mm256_add_epi8(chunk, key));\n        upper = _mm256_sub_epi8(upper, _mm256_set1_epi8(26));\n        __m256i encrypted = _mm256_or_si256(lower, upper);\n        _mm256_storeu_si256((__m256i*)(result.data() + i), encrypted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    const int step = 32;\n    const __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0\n    );\n    const __m256i replacements = _mm256_setr_epi8(\n        'c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0,\n        'c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0\n    );\n    const __m256i case_diff = _mm256_set1_epi8('a' - 'A');\n\n    for (size_t i = 0; i < message.size(); i += step) {\n        size_t chunk_size = std::min(message.size() - i, step);\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i replaced = _mm256_blendv_epi8(chunk, replacements, is_vowel);\n\n        __m256i is_lower = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n        __m256i is_upper = _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), chunk);\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n\n        __m256i swapped_case = _mm256_xor_si256(chunk, case_diff);\n        __m256i result = _mm256_blendv_epi8(replaced, swapped_case, is_letter);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(encoded.data() + i), result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), '\\0');\n    const int vec_size = 32; // 256-bit register holds 32 chars\n\n    for (size_t i = 0; i < message.size(); i += vec_size) {\n        size_t j = i + vec_size < message.size() ? vec_size : message.size() - i;\n\n        // Load 256-bit chunk of data\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Create masks for vowels and case conversion\n        __m256i vowels = _mm256_set1_epi8('a');\n        __m256i VOWELS = _mm256_set1_epi8('A');\n        __m256i offset = _mm256_set1_epi8(2);\n        __m256i case_mask = _mm256_set1_epi8(0x20);\n\n        // Swap case\n        __m256i lower_case = _mm256_and_si256(chunk, case_mask);\n        __m256i upper_case = _mm256_andnot_si256(case_mask, chunk);\n        __m256i swapped_case = _mm256_or_si256(lower_case, _mm256_xor_si256(upper_case, case_mask));\n\n        // Replace vowels\n        __m256i is_vowel = _mm256_or_si256(_mm256_cmpeq_epi8(chunk, vowels), _mm256_cmpeq_epi8(chunk, VOWELS));\n        __m256i replaced_vowels = _mm256_add_epi8(chunk, _mm256_and_si256(is_vowel, offset));\n\n        // Combine results\n        __m256i result = _mm256_or_si256(_mm256_andnot_si256(is_vowel, swapped_case), _mm256_and_si256(is_vowel, replaced_vowels));\n\n        // Store the result back into the encoded string\n        _mm256_storeu_si256((__m256i*)&encoded[i], result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    const int step = 32;\n    const __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i case_bit = _mm256_set1_epi8(0x20);\n\n    for (size_t i = 0; i < message.size(); i += step) {\n        size_t len = std::min(message.size() - i, step);\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i replaced = _mm256_blendv_epi8(chunk, replacements, is_vowel);\n\n        __m256i is_lower = _mm256_cmpeq_epi8(_mm256_and_si256(chunk, case_bit), case_bit);\n        __m256i swapped_case = _mm256_xor_si256(replaced, is_lower);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(encoded.data() + i), swapped_case);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i case_mask = _mm256_set1_epi8(0x20);\n\n    for (size_t i = 0; i < message.size(); i += 32) {\n        size_t block_size = std::min(message.size() - i, size_t(32));\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i replaced = _mm256_blendv_epi8(chunk, replacements, is_vowel);\n\n        __m256i is_lower = _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('a'));\n        __m256i is_upper = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n        __m256i is_letter = _mm256_or_si256(is_lower, is_upper);\n        __m256i swapped_case = _mm256_xor_si256(chunk, case_mask);\n        __m256i result = _mm256_blendv_epi8(replaced, swapped_case, is_letter);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(encoded.data() + i), result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    const int vec_size = 32; // 32 bytes = 256 bits for AVX\n    const __m256i vowels = _mm256_setr_epi8(\n        'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    const __m256i replacements = _mm256_setr_epi8(\n        'c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    const __m256i case_diff = _mm256_set1_epi8('a' - 'A');\n    const __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n    const __m256i upper_mask = _mm256_set1_epi8('Z' - 'A' + 1);\n\n    for (size_t i = 0; i < message.size(); i += vec_size) {\n        size_t remaining = message.size() - i;\n        size_t chunk_size = remaining < vec_size ? remaining : vec_size;\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i replaced = _mm256_blendv_epi8(chunk, replacements, is_vowel);\n\n        __m256i is_lower = _mm256_cmpgt_epi8(chunk, lower_mask);\n        __m256i is_upper = _mm256_cmpgt_epi8(upper_mask, chunk);\n        __m256i is_letter = _mm256_and_si256(is_lower, is_upper);\n\n        __m256i swapped_case = _mm256_xor_si256(chunk, case_diff);\n        __m256i result = _mm256_blendv_epi8(replaced, swapped_case, is_letter);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(encoded.data() + i), result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(key.data() + i));\n            __m256i isUpper = _mm256_and_si256(chunk, _mm256_set1_epi8('A' - 1));\n            __m256i isLower = _mm256_and_si256(chunk, _mm256_set1_epi8('a' - 1));\n\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(isUpper, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n                allLower = false;\n            }\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(isLower, _mm256_setzero_si256())) != 0xFFFFFFFF) {\n                allUpper = false;\n            }\n        }\n\n        if (!allLower && !allUpper) return false;\n    }\n\n    return allLower || allUpper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n#include <vector>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    std::vector<std::string> keys;\n    for (const auto& pair : dict) {\n        keys.push_back(pair.first);\n    }\n\n    size_t numKeys = keys.size();\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (size_t i = 0; i < numKeys; i += 8) {\n        __m256i vUpper = _mm256_set1_epi8('A');\n        __m256i vLower = _mm256_set1_epi8('a');\n        __m256i vZero = _mm256_setzero_si256();\n\n        for (size_t j = 0; j < 8 && (i + j < numKeys); ++j) {\n            const char* key = keys[i + j].c_str();\n            size_t len = keys[i + j].length();\n\n            for (size_t k = 0; k < len; k += 32) {\n                __m256i vKey = _mm256_loadu_si256((__m256i*)(key + k));\n                __m256i vUpperCheck = _mm256_or_si256(_mm256_cmpgt_epi8(vKey, vUpper), _mm256_cmplt_epi8(vKey, _mm256_add_epi8(vUpper, _mm256_set1_epi8(26))));\n                __m256i vLowerCheck = _mm256_or_si256(_mm256_cmpgt_epi8(vKey, vLower), _mm256_cmplt_epi8(vKey, _mm256_add_epi8(vLower, _mm256_set1_epi8(26))));\n\n                if (!_mm256_testz_si256(vUpperCheck, vUpperCheck)) {\n                    allUpper = false;\n                }\n                if (!_mm256_testz_si256(vLowerCheck, vLowerCheck)) {\n                    allLower = false;\n                }\n\n                if (!allUpper && !allLower) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return allUpper || allLower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(key.data() + i));\n            __m256i upper = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z'));\n            __m256i lower = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), chunk);\n            __m256i notUpper = _mm256_xor_si256(upper, _mm256_set1_epi8(0xFF));\n            __m256i notLower = _mm256_xor_si256(lower, _mm256_set1_epi8(0xFF));\n\n            if (_mm256_movemask_epi8(notUpper) != 0xFFFFFFFF) {\n                allUpper = false;\n            }\n            if (_mm256_movemask_epi8(notLower) != 0xFFFFFFFF) {\n                allLower = false;\n            }\n        }\n\n        if (!allUpper && !allLower) return false;\n    }\n\n    return allUpper || allLower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(key.data() + i));\n            __m256i isUpper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('@')), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('[')));\n            __m256i isLower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('`')), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('{')));\n\n            if (_mm256_movemask_epi8(_mm256_xor_si256(isUpper, _mm256_set1_epi8(-1))) != 0xFFFFFFFF) {\n                allUpper = false;\n            }\n            if (_mm256_movemask_epi8(_mm256_xor_si256(isLower, _mm256_set1_epi8(-1))) != 0xFFFFFFFF) {\n                allLower = false;\n            }\n        }\n\n        if (!allUpper && !allLower) return false;\n    }\n\n    return allUpper || allLower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allLower = true;\n    bool allUpper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        size_t i = 0;\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(key.data() + i));\n            __m256i lower = _mm256_cmpeq_epi8(chunk, _mm256_or_si256(_mm256_set1_epi8('a' - 1), _mm256_set1_epi8('z' + 1)));\n            __m256i upper = _mm256_cmpeq_epi8(chunk, _mm256_or_si256(_mm256_set1_epi8('A' - 1), _mm256_set1_epi8('Z' + 1)));\n            if (_mm256_movemask_epi8(_mm256_andnot_si256(lower, upper)) != 0) {\n                allLower = false;\n                allUpper = false;\n                return false;\n            }\n            if (_mm256_movemask_epi8(lower) != 0xFFFFFFFF) allLower = false;\n            if (_mm256_movemask_epi8(upper) != 0xFFFFFFFF) allUpper = false;\n        }\n\n        for (; i < len; ++i) {\n            if (std::islower(key[i])) allUpper = false;\n            if (std::isupper(key[i])) allLower = false;\n            if (!std::islower(key[i]) && !std::isupper(key[i])) return false;\n        }\n    }\n\n    return allLower || allUpper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    std::vector<uint8_t> is_prime(n, 1);\n    is_prime[0] = is_prime[1] = 0;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = 0;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    std::vector<uint32_t> is_prime(n / 32 + 1, 0xFFFFFFFF);\n    is_prime[0] &= ~(1U << 0); // 0 is not prime\n    is_prime[0] &= ~(1U << 1); // 1 is not prime\n\n    for (uint32_t i = 2; i * i < n; ++i) {\n        if (is_prime[i / 32] & (1U << (i % 32))) {\n            __m256i index = _mm256_setr_epi32(i * i, i * i + i, i * i + 2 * i, i * i + 3 * i, i * i + 4 * i, i * i + 5 * i, i * i + 6 * i, i * i + 7 * i);\n            __m256i mask = _mm256_set1_epi32(1U << (i % 32));\n\n            for (uint32_t j = i * i; j < n; j += 8 * i) {\n                __m256i indices = _mm256_add_epi32(index, _mm256_setr_epi32(0, i, 2 * i, 3 * i, 4 * i, 5 * i, 6 * i, 7 * i));\n                __m256i offsets = _mm256_srli_epi32(indices, 5);\n                __m256i bit_offsets = _mm256_and_si256(indices, _mm256_set1_epi32(31));\n\n                uint32_t offset_array[8];\n                uint32_t bit_offset_array[8];\n                _mm256_storeu_si256((__m256i*)offset_array, offsets);\n                _mm256_storeu_si256((__m256i*)bit_offset_array, bit_offsets);\n\n                for (int k = 0; k < 8; ++k) {\n                    if (offset_array[k] < is_prime.size()) {\n                        is_prime[offset_array[k]] &= ~(1U << bit_offset_array[k]);\n                    }\n                }\n                index = _mm256_add_epi32(index, _mm256_set1_epi32(8 * i));\n            }\n        }\n    }\n\n    for (uint32_t i = 2; i < n; ++i) {\n        if (is_prime[i / 32] & (1U << (i % 32))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    const int size = n;\n    std::vector<uint8_t> is_prime(size, 1);\n    is_prime[0] = is_prime[1] = 0;\n\n    int sqrt_n = static_cast<int>(std::sqrt(n));\n    for (int i = 2; i <= sqrt_n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = 0;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    const int size = (n + 255) / 256 * 256;\n    uint32_t *is_prime = (uint32_t*)_mm_malloc(size * sizeof(uint32_t), 32);\n    for (int i = 0; i < size; ++i) is_prime[i] = 0xFFFFFFFF;\n\n    is_prime[0] &= ~1;\n    is_prime[1] &= ~2;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i / 32] & (1 << (i % 32))) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j / 32] &= ~(1 << (j % 32));\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i / 32] & (1 << (i % 32))) {\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(is_prime);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n < 2) return {};\n\n    std::vector<int> primes;\n    bool *is_prime = new bool[n];\n    for (int i = 2; i < n; ++i) is_prime[i] = true;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.c_str();\n\n    __m256i vowels = _mm256_setr_epi8(\n        'A', 0, 'E', 0, 'I', 0, 'O', 0,\n        'U', 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i even_indices = _mm256_setr_epi8(\n            1, 0, 3, 0, 5, 0, 7, 0,\n            9, 0, 11, 0, 13, 0, 15, 0,\n            17, 0, 19, 0, 21, 0, 23, 0,\n            25, 0, 27, 0, 29, 0, 31, 0\n        );\n\n        __m256i mask = _mm256_cmpeq_epi8(chunk, vowels);\n        mask = _mm256_and_si256(mask, even_indices);\n\n        int mask_ints[8];\n        _mm256_storeu_si256((__m256i*)mask_ints, mask);\n\n        for (int j = 0; j < 8; ++j) {\n            count += _mm_popcnt_u32(mask_ints[j]);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.data();\n\n    __m256i vowels = _mm256_setr_epi8(\n        'A', 0, 'E', 0, 'I', 0, 'O', 0,\n        'U', 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i < len - 31; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i even_indices = _mm256_setr_epi8(\n            1, 0, 3, 0, 5, 0, 7, 0,\n            9, 0, 11, 0, 13, 0, 15, 0,\n            17, 0, 19, 0, 21, 0, 23, 0,\n            25, 0, 27, 0, 29, 0, 31, 0\n        );\n\n        __m256i mask = _mm256_cmpeq_epi8(chunk, vowels);\n        mask = _mm256_and_si256(mask, even_indices);\n\n        __m256i temp = _mm256_hadd_epi16(_mm256_hadd_epi16(mask, mask), mask);\n        count += _mm256_extract_epi16(temp, 0) + _mm256_extract_epi16(temp, 8);\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; i += 2) {\n        if (i % 2 == 0 && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.data();\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                      'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t remaining = len - i;\n        size_t chunk_size = remaining < 32 ? remaining : 32;\n\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i mask = _mm256_set1_epi8(0xDF);\n        chunk = _mm256_and_si256(chunk, mask);\n\n        __m256i cmp = _mm256_cmpeq_epi8(chunk, vowels);\n        cmp = _mm256_and_si256(cmp, _mm256_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n                                                     1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0));\n\n        unsigned int mask_even = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask_even);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.data();\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                      'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i even_mask = _mm256_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n                                         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i is_vowel = _mm256_cmpeq_epi8(chunk, vowels);\n        __m256i even_vowels = _mm256_and_si256(is_vowel, even_mask);\n        count += _mm256_movemask_epi8(even_vowels) & 0x55555555;\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; i++) {\n        if (i % 2 == 0 && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.c_str();\n\n    __m256i vowels = _mm256_setr_epi8(\n        'A', 0, 'E', 0, 'I', 0, 'O', 0,\n        'U', 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t remaining = len - i;\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(data + i));\n\n        __m256i even_indices = _mm256_setr_epi8(\n            1, 0, 3, 0, 5, 0, 7, 0,\n            9, 0, 11, 0, 13, 0, 15, 0,\n            17, 0, 19, 0, 21, 0, 23, 0,\n            25, 0, 27, 0, 29, 0, 31, 0\n        );\n\n        __m256i mask = _mm256_cmpeq_epi8(chunk, vowels);\n        mask = _mm256_and_si256(mask, even_indices);\n\n        __m256i result = _mm256_sad_epu8(mask, _mm256_setzero_si256());\n        count += _mm256_extract_epi16(result, 0) + _mm256_extract_epi16(result, 4);\n\n        if (remaining < 32) break;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_add_epi32(current, _mm256_set1_epi32(i / 8 * 8));\n        alignas(32) int temp_array[8];\n        _mm256_store_si256((__m256i*)temp_array, temp);\n\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result.push_back(temp_array[j]);\n        }\n\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 2);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_add_epi32(current, _mm256_set1_epi32(i / 2 * increment[0]));\n        alignas(32) int temp_array[8];\n        _mm256_store_si256((__m256i*)temp_array, temp);\n\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result.push_back(temp_array[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_add_epi32(current, _mm256_set1_epi32(i * increment[0]));\n        alignas(32) int temp_array[8];\n        _mm256_store_si256((__m256i*)temp_array, temp);\n\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result.push_back(temp_array[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_add_epi32(current, _mm256_set1_epi32(i / 8 * 8));\n        alignas(32) int temp_result[8];\n        _mm256_store_si256((__m256i*)temp_result, temp);\n\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result.push_back(temp_result[j]);\n        }\n\n        current = _mm256_add_epi32(current, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m256i current = _mm256_set1_epi32(n);\n    __m256i increment = _mm256_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_add_epi32(current, _mm256_set1_epi32(i * increment[0]));\n        alignas(32) int temp_array[8];\n        _mm256_store_si256((__m256i*)temp_array, temp);\n\n        for (int j = 0; j < 8 && (i + j) < n; ++j) {\n            result.push_back(temp_array[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count_vector = _mm256_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        __m256i values = _mm256_set_epi64x(i + 3 <= m ? i + 3 : 0, i + 2 <= m ? i + 2 : 0, i + 1 <= m ? i + 1 : 0, i);\n        sum = _mm256_add_epi64(sum, values);\n    }\n\n    __m256i sum_hi128 = _mm256_extracti128_si256(sum, 1);\n    __m128i sum_lo128 = _mm256_castsi256_si128(sum);\n    __m128i sum_total = _mm_add_epi64(sum_lo128, sum_hi128);\n\n    int64_t total_sum = _mm_extract_epi64(sum_total, 0) + _mm_extract_epi64(sum_total, 1);\n    int64_t avg = total_sum / count;\n\n    std::bitset<64> binary(avg);\n    return binary.to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t size = m - n + 1;\n    int64_t sum = 0;\n\n    int64_t i;\n    for (i = n; i <= m - 3; i += 4) {\n        __m256i v = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        __m256i add = _mm256_add_epi64(v, _mm256_set1_epi64x(1));\n        __m128i hi = _mm256_extractf128_si256(add, 1);\n        __m128i lo = _mm256_castsi256_si128(add);\n        sum += _mm_extract_epi64(lo, 0);\n        sum += _mm_extract_epi64(lo, 1);\n        sum += _mm_extract_epi64(hi, 0);\n        sum += _mm_extract_epi64(hi, 1);\n    }\n\n    for (; i <= m; ++i) {\n        sum += i;\n    }\n\n    int64_t avg = sum / size;\n    std::bitset<64> binary(avg);\n    return binary.to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        __m256i vec = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        sum = _mm256_add_epi64(sum, vec);\n    }\n\n    __m256i total_sum = _mm256_hadd_epi64(sum, sum);\n    total_sum = _mm256_hadd_epi64(total_sum, total_sum);\n\n    int64_t total = _mm256_extract_epi64(total_sum, 0) + _mm256_extract_epi64(total_sum, 1) +\n                    _mm256_extract_epi64(total_sum, 2) + _mm256_extract_epi64(total_sum, 3);\n\n    int64_t avg = total / count;\n    if (total % count > count / 2) avg++;\n\n    std::bitset<64> binary(avg);\n    return binary.to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count_vector = _mm256_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        __m256i values = _mm256_setr_epi64x(i, i + 1, i + 2, i + 3);\n        sum = _mm256_add_epi64(sum, values);\n    }\n\n    __m256i total_sum = _mm256_hadd_epi64(sum, sum);\n    total_sum = _mm256_hadd_epi64(total_sum, total_sum);\n\n    int64_t total = _mm256_extract_epi64(total_sum, 0) + _mm256_extract_epi64(total_sum, 1) +\n                    _mm256_extract_epi64(total_sum, 2) + _mm256_extract_epi64(total_sum, 3);\n\n    int64_t avg = total / count;\n    int64_t rounded_avg = (total + count / 2) / count;\n\n    std::bitset<64> binary(rounded_avg);\n    std::string binary_string = binary.to_string();\n\n    auto pos = binary_string.find('1');\n    if (pos != std::string::npos) {\n        return binary_string.substr(pos);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_set1_epi64x(count);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    __m256i m_vec = _mm256_set1_epi64x(m);\n\n    for (int64_t i = n; i <= m; i += 4) {\n        __m256i vec = _mm256_set_epi64x(i + 3, i + 2, i + 1, i);\n        sum = _mm256_add_epi64(sum, vec);\n    }\n\n    __m256i total_sum = _mm256_hadd_epi64(sum, sum);\n    total_sum = _mm256_hadd_epi64(total_sum, total_sum);\n\n    int64_t total = _mm256_extract_epi64(total_sum, 0) + _mm256_extract_epi64(total_sum, 1);\n\n    int64_t avg = total / count;\n\n    std::bitset<64> binary(avg);\n    return binary.to_string();\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i modulus = _mm256_set1_epi32(10000);\n    __m256i sums = _mm256_set1_epi32(0);\n    __m256i factors = _mm256_set1_epi32(1);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i index = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i even_mask = _mm256_set1_epi32(0xAAAAAAAA);\n        __m256i odd_mask = _mm256_set1_epi32(0x55555555);\n\n        __m256i even_indices = _mm256_and_si256(index, even_mask);\n        __m256i odd_indices = _mm256_and_si256(index, odd_mask);\n\n        __m256i even_factors = _mm256_mul_epi32(factors, even_indices);\n        factors = _mm256_blendv_epi8(factors, even_factors, even_mask);\n\n        __m256i odd_sums = _mm256_add_epi32(sums, odd_indices);\n        sums = _mm256_blendv_epi8(sums, odd_sums, odd_mask);\n\n        __m256i temp_factors = _mm256_permute4x64_epi64(_mm256_mul_epu32(factors, modulus), 0xD8);\n        factors = _mm256_blendv_epi8(factors, temp_factors, modulus);\n\n        __m256i temp_sums = _mm256_permute4x64_epi64(_mm256_add_epi32(sums, ones), 0xD8);\n        sums = _mm256_blendv_epi8(sums, temp_sums, ones);\n\n        __m256i result_vec = _mm256_blendv_epi8(factors, sums, odd_mask);\n\n        int temp_result[8];\n        _mm256_storeu_si256((__m256i*)temp_result, result_vec);\n\n        for (int j = 0; j < 8 && i + j <= n; ++j) {\n            result[i + j - 1] = temp_result[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i mod = _mm256_set1_epi32(10000);\n    int factorial = 1;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8);\n        __m256i evenMask = _mm256_set1_epi32(0x00000001);\n        __m256i evenIndices = _mm256_and_si256(indices, evenMask);\n        __m256i oddIndices = _mm256_xor_si256(indices, evenMask);\n\n        __m256i evenResults = _mm256_setzero_si256();\n        __m256i oddResults = _mm256_setzero_si256();\n\n        for (int j = 1; j <= i + 8; ++j) {\n            factorial *= j;\n            __m256i factorialVec = _mm256_set1_epi32(factorial);\n            evenResults = _mm256_blendv_epi8(evenResults, _mm256_rem_epi32(factorialVec, mod), evenIndices);\n\n            __m256i sumVec = _mm256_set1_epi32((j * (j + 1)) / 2);\n            oddResults = _mm256_blendv_epi8(oddResults, sumVec, oddIndices);\n        }\n\n        __m256i results = _mm256_or_si256(evenResults, oddResults);\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, results);\n\n        for (int j = 0; j < 8 && i + j < n; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int factorial = 1;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i modulus = _mm256_set1_epi32(10000);\n\n    for (int i = 1; i <= n; ++i) {\n        if (i % 2 == 0) {\n            factorial = (factorial * i) % 10000;\n            result[i - 1] = factorial;\n        } else {\n            __m256i idx = _mm256_set1_epi32(i);\n            __m256i range = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n            __m256i mask = _mm256_cmpgt_epi32(idx, range);\n            sum = _mm256_add_epi32(sum, _mm256_and_si256(mask, ones));\n\n            int temp[8];\n            _mm256_storeu_si256((__m256i*)temp, sum);\n            result[i - 1] = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i sum = _mm256_setzero_si256();\n    __m256i fact = _mm256_set1_epi32(1);\n    __m256i mod = _mm256_set1_epi32(10000);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i even_mask = _mm256_set1_epi32(0xAAAAAAAA);\n        __m256i odd_mask = _mm256_set1_epi32(0x55555555);\n\n        __m256i even = _mm256_and_si256(idx, even_mask);\n        __m256i odd = _mm256_and_si256(idx, odd_mask);\n\n        __m256i fact_update = _mm256_mullo_epi32(fact, idx);\n        fact = _mm256_and_si256(fact_update, even_mask);\n        fact = _mm256_add_epi32(fact, one);\n        fact = _mm256_and_si256(fact, mod);\n\n        __m256i sum_update = _mm256_add_epi32(sum, idx);\n        sum = _mm256_and_si256(sum_update, odd_mask);\n\n        __m256i result_vec = _mm256_or_si256(fact, sum);\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, result_vec);\n\n        for (int j = 0; j < 8 && i + j <= n; ++j) {\n            result[i + j - 1] = temp[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i sum = _mm256_setzero_si256();\n    __m256i fact = _mm256_set1_epi32(1);\n    __m256i mod = _mm256_set1_epi32(10000);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i even_mask = _mm256_set1_epi32(0x55555555);\n        __m256i even = _mm256_and_si256(idx, even_mask);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi32(even, zero);\n\n        __m256i add = _mm256_add_epi32(idx, one);\n        sum = _mm256_add_epi32(sum, add);\n\n        fact = _mm256_mullo_epi32(fact, idx);\n        fact = _mm256_and_si256(fact, mod);\n\n        __m256i res = _mm256_blendv_epi8(sum, fact, cmp);\n\n        int* res_ptr = (int*)&res;\n        for (int j = 0; j < 8 && i + j <= n; ++j) {\n            result[i + j - 1] = res_ptr[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    std::vector<int> result(2, 0);\n    if (n < 1) return result;\n\n    int palindromes[10000] = {0};\n    int count = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        while (num > 0) {\n            int dig = num % 10;\n            rev = rev * 10 + dig;\n            num = num / 10;\n        }\n        if (i == rev) {\n            palindromes[count++] = i;\n        }\n    }\n\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i odd_mask = _mm256_set1_epi32(2);\n    __m256i even_count = _mm256_setzero_si256();\n    __m256i odd_count = _mm256_setzero_si256();\n\n    for (int i = 0; i < count; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&palindromes[i]);\n        __m256i even_check = _mm256_and_si256(chunk, even_mask);\n        __m256i odd_check = _mm256_and_si256(chunk, odd_mask);\n\n        even_count = _mm256_add_epi32(even_count, _mm256_and_si256(even_check, _mm256_cmpeq_epi32(even_check, even_mask)));\n        odd_count = _mm256_add_epi32(odd_count, _mm256_and_si256(odd_check, _mm256_cmpeq_epi32(odd_check, odd_mask)));\n    }\n\n    int even = 0;\n    int odd = 0;\n    for (int i = 0; i < 8; ++i) {\n        even += _mm256_extract_epi32(even_count, i);\n        odd += _mm256_extract_epi32(odd_count, i);\n    }\n\n    result[0] = even;\n    result[1] = odd;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i evens = _mm256_set1_epi32(2);\n    __m256i odds = _mm256_set1_epi32(1);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_nums = _mm256_setr_epi32(\n            ((i%10)*1000 + ((i/10)%10)*100 + ((i/100)%10)*10 + (i/1000)%10),\n            (((i+1)%10)*1000 + (((i+1)/10)%10)*100 + (((i+1)/100)%10)*10 + ((i+1)/1000)%10),\n            (((i+2)%10)*1000 + (((i+2)/10)%10)*100 + (((i+2)/100)%10)*10 + ((i+2)/1000)%10),\n            (((i+3)%10)*1000 + (((i+3)/10)%10)*100 + (((i+3)/100)%10)*10 + ((i+3)/1000)%10),\n            (((i+4)%10)*1000 + (((i+4)/10)%10)*100 + (((i+4)/100)%10)*10 + ((i+4)/1000)%10),\n            (((i+5)%10)*1000 + (((i+5)/10)%10)*100 + (((i+5)/100)%10)*10 + ((i+5)/1000)%10),\n            (((i+6)%10)*1000 + (((i+6)/10)%10)*100 + (((i+6)/100)%10)*10 + ((i+6)/1000)%10),\n            (((i+7)%10)*1000 + (((i+7)/10)%10)*100 + (((i+7)/100)%10)*10 + ((i+7)/1000)%10)\n        );\n\n        __m256i palindromes = _mm256_cmpeq_epi32(nums, rev_nums);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(nums, ones), evens);\n        __m256i odd_mask = _mm256_cmpeq_epi32(_mm256_and_si256(nums, ones), odds);\n\n        even_count += _mm256_movemask_ps((__m256) _mm256_and_si256(palindromes, even_mask));\n        odd_count += _mm256_movemask_ps((__m256) _mm256_and_si256(palindromes, odd_mask));\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i twos = _mm256_set1_epi32(2);\n    __m256i fours = _mm256_set1_epi32(4);\n    __m256i eights = _mm256_set1_epi32(8);\n    __m256i sixteens = _mm256_set1_epi32(16);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_nums = _mm256_setr_epi32(\n            (i % 10) * ones + ((i / 10) % 10) * twos + ((i / 100) % 10) * fours + ((i / 1000) % 10) * eights + ((i / 10000) % 10) * sixteens,\n            ((i+1) % 10) * ones + (((i+1) / 10) % 10) * twos + (((i+1) / 100) % 10) * fours + (((i+1) / 1000) % 10) * eights + (((i+1) / 10000) % 10) * sixteens,\n            ((i+2) % 10) * ones + (((i+2) / 10) % 10) * twos + (((i+2) / 100) % 10) * fours + (((i+2) / 1000) % 10) * eights + (((i+2) / 10000) % 10) * sixteens,\n            ((i+3) % 10) * ones + (((i+3) / 10) % 10) * twos + (((i+3) / 100) % 10) * fours + (((i+3) / 1000) % 10) * eights + (((i+3) / 10000) % 10) * sixteens,\n            ((i+4) % 10) * ones + (((i+4) / 10) % 10) * twos + (((i+4) / 100) % 10) * fours + (((i+4) / 1000) % 10) * eights + (((i+4) / 10000) % 10) * sixteens,\n            ((i+5) % 10) * ones + (((i+5) / 10) % 10) * twos + (((i+5) / 100) % 10) * fours + (((i+5) / 1000) % 10) * eights + (((i+5) / 10000) % 10) * sixteens,\n            ((i+6) % 10) * ones + (((i+6) / 10) % 10) * twos + (((i+6) / 100) % 10) * fours + (((i+6) / 1000) % 10) * eights + (((i+6) / 10000) % 10) * sixteens,\n            ((i+7) % 10) * ones + (((i+7) / 10) % 10) * twos + (((i+7) / 100) % 10) * fours + (((i+7) / 1000) % 10) * eights + (((i+7) / 10000) % 10) * sixteens\n        );\n\n        __m256i cmp = _mm256_cmpeq_epi32(nums, rev_nums);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask & (1 << j)) {\n                if ((i + j) % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i tens = _mm256_set1_epi32(10);\n    __m256i hundreds = _mm256_set1_epi32(100);\n    __m256i thousands = _mm256_set1_epi32(1000);\n    __m256i ten_thousands = _mm256_set1_epi32(10000);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_nums = _mm256_setzero_si256();\n\n        __m256i temp = _mm256_and_si256(nums, _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_mul_epu32(temp, ten_thousands);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(nums, 4), _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_add_epi32(rev_nums, _mm256_mul_epu32(temp, thousands));\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(nums, 8), _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_add_epi32(rev_nums, _mm256_mul_epu32(temp, hundreds));\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(nums, 12), _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_add_epi32(rev_nums, _mm256_mul_epu32(temp, tens));\n\n        temp = _mm256_srli_epi32(nums, 16);\n        rev_nums = _mm256_add_epi32(rev_nums, temp);\n\n        __m256i palindromes = _mm256_cmpeq_epi32(nums, rev_nums);\n        int mask = _mm256_movemask_epi8(palindromes);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask & (0xFF << (j * 4))) {\n                if ((i + j) % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i tens = _mm256_set1_epi32(10);\n    __m256i hundreds = _mm256_set1_epi32(100);\n    __m256i thousands = _mm256_set1_epi32(1000);\n    __m256i ten_thousands = _mm256_set1_epi32(10000);\n\n    for (int i = 1; i <= n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_nums = _mm256_setzero_si256();\n\n        __m256i temp = _mm256_and_si256(nums, _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_or_si256(rev_nums, _mm256_slli_epi32(temp, 12));\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(nums, 4), _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_or_si256(rev_nums, _mm256_slli_epi32(temp, 8));\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(nums, 8), _mm256_set1_epi32(0xF));\n        rev_nums = _mm256_or_si256(rev_nums, _mm256_slli_epi32(temp, 4));\n\n        temp = _mm256_srli_epi32(nums, 12);\n        rev_nums = _mm256_or_si256(rev_nums, temp);\n\n        __m256i palindromes = _mm256_cmpeq_epi32(nums, rev_nums);\n        int mask = _mm256_movemask_epi8(palindromes);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask & (0xFF << (j * 4))) {\n                if ((i + j) % 2 == 0) {\n                    even_count++;\n                } else {\n                    odd_count++;\n                }\n            }\n        }\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m256i sum_digits = _mm256_setzero_si256();\n\n        __m256i tmp = _mm256_and_si256(abs_v, _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 4), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 8), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 12), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 16), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 20), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 24), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 28), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, _mm256_setzero_si256());\n        int mask_ints[8];\n        _mm256_storeu_si256((__m256i*)mask_ints, mask);\n\n        for (int j = 0; j < 8; ++j) {\n            if (mask_ints[j] != 0) {\n                count++;\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum_digits = 0;\n        do {\n            sum_digits += abs(num % 10);\n            num /= 10;\n        } while (num != 0);\n\n        if (sum_digits > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m256i sum_digits = _mm256_setzero_si256();\n\n        __m256i temp = _mm256_and_si256(abs_v, _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 4), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 8), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 12), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 16), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 20), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 24), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        temp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 28), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, temp);\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, _mm256_setzero_si256());\n        count += _mm256_movemask_ps((__m256)mask);\n    }\n\n    for (; i < size; ++i) {\n        int sum = 0;\n        int num = n[i];\n        do {\n            sum += num % 10;\n            num /= 10;\n        } while (num != 0);\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sums = _mm256_setzero_si256();\n\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i tens = _mm256_set1_epi32(10);\n        __m256i hundreds = _mm256_set1_epi32(100);\n        __m256i thousands = _mm256_set1_epi32(1000);\n        __m256i tenthousands = _mm256_set1_epi32(10000);\n        __m256i hundredthousands = _mm256_set1_epi32(100000);\n        __m256i million = _mm256_set1_epi32(1000000);\n        __m256i tenthmillion = _mm256_set1_epi32(10000000);\n        __m256i hundredmillion = _mm256_set1_epi32(100000000);\n\n        __m256i mask = _mm256_set1_epi32(0xF);\n\n        __m256i v_abs = _mm256_abs_epi32(v);\n        __m256i v_sign = _mm256_sign_epi32(ones, v);\n\n        __m256i v_ones = _mm256_and_si256(v_abs, mask);\n        __m256i v_tens = _mm256_and_si256(_mm256_srli_epi32(v_abs, 4), mask);\n        __m256i v_hundreds = _mm256_and_si256(_mm256_srli_epi32(v_abs, 8), mask);\n        __m256i v_thousands = _mm256_and_si256(_mm256_srli_epi32(v_abs, 12), mask);\n        __m256i v_tenthousands = _mm256_and_si256(_mm256_srli_epi32(v_abs, 16), mask);\n        __m256i v_hundredthousands = _mm256_and_si256(_mm256_srli_epi32(v_abs, 20), mask);\n        __m256i v_million = _mm256_and_si256(_mm256_srli_epi32(v_abs, 24), mask);\n        __m256i v_tenthmillion = _mm256_and_si256(_mm256_srli_epi32(v_abs, 28), mask);\n\n        sums = _mm256_add_epi32(sums, v_ones);\n        sums = _mm256_add_epi32(sums, v_tens);\n        sums = _mm256_add_epi32(sums, v_hundreds);\n        sums = _mm256_add_epi32(sums, v_thousands);\n        sums = _mm256_add_epi32(sums, v_tenthousands);\n        sums = _mm256_add_epi32(sums, v_hundredthousands);\n        sums = _mm256_add_epi32(sums, v_million);\n        sums = _mm256_add_epi32(sums, v_tenthmillion);\n\n        sums = _mm256_mullo_epi32(sums, v_sign);\n\n        __m256i cmp = _mm256_cmpgt_epi32(sums, _mm256_setzero_si256());\n        int mask_cmp = _mm256_movemask_epi8(cmp);\n\n        count += _mm_popcnt_u32(mask_cmp & 0b00000000000000000000000010101010);\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int sign = (num < 0) ? -1 : 1;\n        num = abs(num);\n\n        while (num > 0) {\n            sum += num % 10;\n            num /= 10;\n        }\n\n        if (sum * sign > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_v = _mm256_abs_epi32(v);\n        __m256i sum_digits = _mm256_setzero_si256();\n\n        __m256i tmp = _mm256_and_si256(abs_v, _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 4), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 8), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 12), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 16), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 20), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 24), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        tmp = _mm256_and_si256(_mm256_srli_epi32(abs_v, 28), _mm256_set1_epi32(0xF));\n        sum_digits = _mm256_add_epi32(sum_digits, tmp);\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, _mm256_setzero_si256());\n        count += _mm256_movemask_ps((__m256)mask);\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        do {\n            sum += num % 10;\n            num /= 10;\n        } while (num != 0);\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    int size = n.size();\n    int i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sign = _mm256_srli_epi32(v, 31);\n        __m256i abs_v = _mm256_abs_epi32(v);\n\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i tens = _mm256_set1_epi32(10);\n        __m256i hundreds = _mm256_set1_epi32(100);\n        __m256i thousands = _mm256_set1_epi32(1000);\n        __m256i tenthousands = _mm256_set1_epi32(10000);\n        __m256i hundredthousands = _mm256_set1_epi32(100000);\n        __m256i million = _mm256_set1_epi32(1000000);\n        __m256i tenmillion = _mm256_set1_epi32(10000000);\n        __m256i hundredmillion = _mm256_set1_epi32(100000000);\n\n        __m256i sum_digits = _mm256_add_epi32(_mm256_and_si256(abs_v, ones), _mm256_and_si256(_mm256_srli_epi32(abs_v, 1), tens));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 2), hundreds));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 3), thousands));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 4), tenthousands));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 5), hundredthousands));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 6), million));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 7), tenmillion));\n        sum_digits = _mm256_add_epi32(sum_digits, _mm256_and_si256(_mm256_srli_epi32(abs_v, 8), hundredmillion));\n\n        __m256i mask = _mm256_cmpgt_epi32(sum_digits, _mm256_setzero_si256());\n        int mask_int = _mm256_movemask_epi8(mask);\n        count += _mm_popcnt_u32(mask_int & 0x11111111);\n    }\n\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum_digits = 0;\n        int abs_num = abs(num);\n\n        while (abs_num > 0) {\n            sum_digits += abs_num % 10;\n            abs_num /= 10;\n        }\n\n        if (sum_digits > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[j]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            if (_mm256_movemask_epi8(cmp) != -1) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n        std::rotate(arr.rbegin(), arr.rbegin() + 1, arr.rend());\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool matched = true;\n        for (int j = 0; j < N; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[(j + i) % N]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[j]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                matched = false;\n                break;\n            }\n        }\n        if (matched) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; ++j) {\n            if (arr[(j + i) % N] != sorted_arr[j]) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[j]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[j]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            if (_mm256_movemask_epi8(cmp) != -1) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n        std::rotate(arr.rbegin(), arr.rbegin() + 1, arr.rend());\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 8) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[(j + i) % N]);\n            __m256i vec2 = _mm256_loadu_si256((__m256i*)&sorted_arr[j]);\n            __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n            if (_mm256_movemask_epi8(cmp) != -1) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i lst1_even_count = _mm256_setzero_si256();\n    __m256i lst2_even_count = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i lst1_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i lst2_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i lst1_even = _mm256_andnot_si256(even_mask, lst1_vals);\n        __m256i lst2_even = _mm256_andnot_si256(even_mask, lst2_vals);\n\n        lst1_even_count = _mm256_add_epi32(lst1_even_count, _mm256_and_si256(lst1_even, even_mask));\n        lst2_even_count = _mm256_add_epi32(lst2_even_count, _mm256_and_si256(lst2_even, even_mask));\n    }\n\n    int lst1_even = _mm256_extract_epi32(lst1_even_count, 0) + _mm256_extract_epi32(lst1_even_count, 1) +\n                    _mm256_extract_epi32(lst1_even_count, 2) + _mm256_extract_epi32(lst1_even_count, 3) +\n                    _mm256_extract_epi32(lst1_even_count, 4) + _mm256_extract_epi32(lst1_even_count, 5) +\n                    _mm256_extract_epi32(lst1_even_count, 6) + _mm256_extract_epi32(lst1_even_count, 7);\n\n    int lst2_even = _mm256_extract_epi32(lst2_even_count, 0) + _mm256_extract_epi32(lst2_even_count, 1) +\n                    _mm256_extract_epi32(lst2_even_count, 2) + _mm256_extract_epi32(lst2_even_count, 3) +\n                    _mm256_extract_epi32(lst2_even_count, 4) + _mm256_extract_epi32(lst2_even_count, 5) +\n                    _mm256_extract_epi32(lst2_even_count, 6) + _mm256_extract_epi32(lst2_even_count, 7);\n\n    return (lst1_even >= size && lst2_even >= size) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m256i even_mask = _mm256_set1_epi32(1);\n    __m256i result1, result2;\n    int mask1 = 0, mask2 = 0;\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        result1 = _mm256_and_si256(vec1, even_mask);\n        result2 = _mm256_and_si256(vec2, even_mask);\n\n        mask1 |= _mm256_movemask_epi8(_mm256_cmpeq_epi32(result1, even_mask));\n        mask2 |= _mm256_movemask_epi8(_mm256_cmpeq_epi32(result2, even_mask));\n    }\n\n    if ((mask1 & mask2) == mask1) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i even_mask = _mm256_set1_epi32(2);\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        __m256i vec1_even = _mm256_andnot_si256(mask, vec1);\n        __m256i vec2_even = _mm256_andnot_si256(mask, vec2);\n\n        __m256i vec1_odd = _mm256_and_si256(mask, vec1);\n        __m256i vec2_odd = _mm256_and_si256(mask, vec2);\n\n        __m256i vec1_even_count = _mm256_and_si256(vec1_even, even_mask);\n        __m256i vec2_even_count = _mm256_and_si256(vec2_even, even_mask);\n\n        __m256i vec1_odd_count = _mm256_and_si256(vec1_odd, even_mask);\n        __m256i vec2_odd_count = _mm256_and_si256(vec2_odd, even_mask);\n\n        int vec1_even_sum = _mm256_movemask_epi8(_mm256_packs_epi32(vec1_even_count, vec1_even_count));\n        int vec2_even_sum = _mm256_movemask_epi8(_mm256_packs_epi32(vec2_even_count, vec2_even_count));\n\n        int vec1_odd_sum = _mm256_movemask_epi8(_mm256_packs_epi32(vec1_odd_count, vec1_odd_count));\n        int vec2_odd_sum = _mm256_movemask_epi8(_mm256_packs_epi32(vec2_odd_count, vec2_odd_count));\n\n        if (vec1_even_sum + vec2_even_sum < vec1_odd_sum + vec2_odd_sum) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m256i mask_even = _mm256_set1_epi32(1);\n    __m256i mask_odd = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n\n        __m256i even1 = _mm256_andnot_si256(_mm256_set1_epi32(1), vec1);\n        __m256i even2 = _mm256_andnot_si256(_mm256_set1_epi32(1), vec2);\n\n        __m256i odd1 = _mm256_and_si256(vec1, _mm256_set1_epi32(1));\n        __m256i odd2 = _mm256_and_si256(vec2, _mm256_set1_epi32(1));\n\n        if (_mm256_testz_si256(even1, mask_even) != 1 && _mm256_testz_si256(even2, mask_even) != 1) {\n            return \"NO\";\n        }\n\n        if (_mm256_testz_si256(odd1, mask_odd) != 1 && _mm256_testz_si256(odd2, mask_odd) != 1) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m256i mask1 = _mm256_set1_epi32(1);\n    __m256i mask2 = _mm256_set1_epi32(1);\n    int i;\n\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst2[i]);\n        mask1 = _mm256_and_si256(vec1, mask1);\n        mask2 = _mm256_and_si256(vec2, mask2);\n    }\n\n    int mask1_res = _mm256_movemask_epi8(_mm256_cmpeq_epi32(mask1, _mm256_setzero_si256()));\n    int mask2_res = _mm256_movemask_epi8(_mm256_cmpeq_epi32(mask2, _mm256_setzero_si256()));\n\n    for (; i < size; ++i) {\n        if (lst1[i] % 2 != 0 && lst2[i] % 2 != 0) {\n            return \"NO\";\n        }\n    }\n\n    if (mask1_res == 0xffff && mask2_res == 0xffff) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto &str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(str.data() + i));\n            __m256i odd_mask = _mm256_set1_epi8('1' | '3' | '5' | '7' | '9');\n            __m256i cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        result.push_back(\"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\");\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        size_t len = str.length();\n        size_t odd_count = 0;\n\n        size_t i = 0;\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(str.data() + i));\n            __m256i odd_mask = _mm256_set1_epi8('1');\n            __m256i cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n\n            odd_mask = _mm256_set1_epi8('3');\n            cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n\n            odd_mask = _mm256_set1_epi8('5');\n            cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n\n            odd_mask = _mm256_set1_epi8('7');\n            cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n\n            odd_mask = _mm256_set1_epi8('9');\n            cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b01010101010101010101010101010101;\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                ++odd_count;\n            }\n        }\n\n        result.push_back(\"the number of odd elements \" + std::to_string(odd_count) + \"n the str\" + std::to_string(odd_count) + \"ng \" + std::to_string(odd_count) + \" of the \" + std::to_string(odd_count) + \"nput.\");\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto & str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&str[i]);\n            __m256i odd_mask = _mm256_set1_epi8(0x1);\n            __m256i odd_bits = _mm256_and_si256(chunk, odd_mask);\n            __m256i odd_count = _mm256_sub_epi8(_mm256_set1_epi8(1), _mm256_cmpeq_epi8(odd_bits, _mm256_setzero_si256()));\n            count += _mm256_movemask_epi8(_mm256_cmpeq_epi8(odd_count, _mm256_set1_epi8(1)));\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n\n        result.push_back(\"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\");\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto &str : lst) {\n        size_t len = str.length();\n        size_t odd_count = 0;\n\n        size_t i = 0;\n        for (; i <= len - 32; i += 32) {\n            __m256i v = _mm256_loadu_si256((__m256i*)(str.data() + i));\n            __m256i odd_mask = _mm256_set1_epi8('1');\n            __m256i cmp = _mm256_cmpeq_epi8(v, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp);\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') % 2 != 0) {\n                odd_count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(odd_count);\n        output += \"n the str\";\n        output += std::to_string(odd_count);\n        output += \"ng \";\n        output += std::to_string(odd_count);\n        output += \" of the \";\n        output += std::to_string(odd_count);\n        output += \"nput.\";\n\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        size_t len = str.length();\n        size_t i = 0;\n        int odd_count = 0;\n\n        // Process 32 bytes at a time using AVX2\n        for (; i <= len - 32; i += 32) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(str.data() + i));\n            __m256i odd_mask = _mm256_set1_epi8('1');\n            __m256i cmp_mask = _mm256_cmpeq_epi8(chunk, odd_mask);\n            odd_count += _mm256_movemask_epi8(cmp_mask) & 0b10101010101010101010101010101010;\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                ++odd_count;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(odd_count);\n        output += \"n the str\";\n        output += std::to_string(odd_count);\n        output += \"ng \";\n        output += std::to_string(odd_count);\n        output += \" of the \";\n        output += std::to_string(odd_count);\n        output += \"nput.\";\n\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t global_min = LLONG_MAX;\n    int64_t current_sum = 0;\n    int64_t min_ending_here = 0;\n\n    __m256i vglobal_min = _mm256_set1_epi64x(LLONG_MAX);\n    __m256i vcurrent_sum = _mm256_setzero_si256();\n    __m256i vmin_ending_here = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= nums.size() - 4; i += 4) {\n        __m256i vnums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        vcurrent_sum = _mm256_add_epi64(vcurrent_sum, vnums);\n        vmin_ending_here = _mm256_min_epi64(vmin_ending_here, vcurrent_sum);\n        vglobal_min = _mm256_min_epi64(vglobal_min, vmin_ending_here);\n\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi64(vcurrent_sum, _mm256_setzero_si256())) != 0) {\n            vcurrent_sum = _mm256_setzero_si256();\n            vmin_ending_here = _mm256_setzero_si256();\n        }\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vglobal_min);\n    for (int j = 0; j < 4; ++j) {\n        if (temp[j] < global_min) {\n            global_min = temp[j];\n        }\n    }\n\n    for (; i < nums.size(); ++i) {\n        current_sum += nums[i];\n        min_ending_here = std::min(min_ending_here, current_sum);\n        global_min = std::min(global_min, min_ending_here);\n        if (current_sum > 0) {\n            current_sum = 0;\n            min_ending_here = 0;\n        }\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n    __m256i vminSum = _mm256_set1_epi64x(LLONG_MAX);\n    __m256i vcurrentSum = _mm256_set1_epi64x(0);\n    __m256i vzero = _mm256_set1_epi64x(0);\n\n    size_t i = 0;\n    for (; i <= nums.size() - 4; i += 4) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        vcurrentSum = _mm256_add_epi64(vcurrentSum, vnums);\n        vminSum = _mm256_min_epi64(vminSum, vcurrentSum);\n\n        __m256i mask = _mm256_cmpgt_epi64(vcurrentSum, vzero);\n        vcurrentSum = _mm256_blendv_epi8(vzero, vcurrentSum, mask);\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vminSum);\n    for (int j = 0; j < 4; ++j) {\n        if (temp[j] < minSum) {\n            minSum = temp[j];\n        }\n    }\n\n    for (; i < nums.size(); ++i) {\n        currentSum += nums[i];\n        if (currentSum < minSum) {\n            minSum = currentSum;\n        }\n        if (currentSum > 0) {\n            currentSum = 0;\n        }\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t global_min = LLONG_MAX;\n    int64_t current_sum = 0;\n    int64_t min_ending_here = 0;\n\n    __m256i vglobal_min = _mm256_set1_epi64x(LLONG_MAX);\n    __m256i vcurrent_sum = _mm256_set1_epi64x(0);\n    __m256i vmin_ending_here = _mm256_set1_epi64x(0);\n\n    size_t i = 0;\n    for (; i <= nums.size() - 4; i += 4) {\n        __m256i vnums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        vcurrent_sum = _mm256_add_epi64(vcurrent_sum, vnums);\n        vmin_ending_here = _mm256_min_epi64(vmin_ending_here, vcurrent_sum);\n        vglobal_min = _mm256_min_epi64(vglobal_min, vmin_ending_here);\n\n        if (vcurrent_sum > _mm256_set1_epi64x(0)) {\n            vcurrent_sum = _mm256_set1_epi64x(0);\n            vmin_ending_here = _mm256_set1_epi64x(0);\n        }\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vglobal_min);\n    for (int j = 0; j < 4; ++j) {\n        if (temp[j] < global_min) {\n            global_min = temp[j];\n        }\n    }\n\n    for (; i < nums.size(); ++i) {\n        current_sum += nums[i];\n        min_ending_here = std::min(min_ending_here, current_sum);\n        global_min = std::min(global_min, min_ending_here);\n\n        if (current_sum > 0) {\n            current_sum = 0;\n            min_ending_here = 0;\n        }\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n    __m256i vMinSum = _mm256_set1_epi64x(LLONG_MAX);\n    __m256i vCurrentSum = _mm256_set1_epi64x(0);\n    __m256i vNums;\n\n    size_t i = 0;\n    for (; i <= nums.size() - 4; i += 4) {\n        vNums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        vCurrentSum = _mm256_add_epi64(vCurrentSum, vNums);\n        vMinSum = _mm256_min_epi64(vMinSum, vCurrentSum);\n\n        if (vCurrentSum < _mm256_set1_epi64x(0)) {\n            vCurrentSum = _mm256_set1_epi64x(0);\n        }\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vMinSum);\n    for (int j = 0; j < 4; ++j) {\n        if (temp[j] < minSum) {\n            minSum = temp[j];\n        }\n    }\n\n    for (; i < nums.size(); ++i) {\n        currentSum += nums[i];\n        if (currentSum < minSum) {\n            minSum = currentSum;\n        }\n        if (currentSum < 0) {\n            currentSum = 0;\n        }\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t global_min = LLONG_MAX;\n    int64_t current_sum = 0;\n    int64_t min_ending_here = 0;\n\n    __m256i vglobal_min = _mm256_set1_epi64x(LLONG_MAX);\n    __m256i vcurrent_sum = _mm256_set1_epi64x(0);\n    __m256i vmin_ending_here = _mm256_set1_epi64x(0);\n\n    size_t i = 0;\n    for (; i <= nums.size() - 4; i += 4) {\n        __m256i vnums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        vcurrent_sum = _mm256_add_epi64(vcurrent_sum, vnums);\n        vmin_ending_here = _mm256_min_epi64(vmin_ending_here, vcurrent_sum);\n        vglobal_min = _mm256_min_epi64(vglobal_min, vmin_ending_here);\n\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi64(vcurrent_sum, _mm256_set1_epi64x(0))) != 0xFFFFFFFF) {\n            vcurrent_sum = _mm256_set1_epi64x(0);\n        }\n    }\n\n    int64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, vglobal_min);\n    for (int j = 0; j < 4; ++j) {\n        if (temp[j] < global_min) {\n            global_min = temp[j];\n        }\n    }\n\n    for (; i < nums.size(); ++i) {\n        current_sum += nums[i];\n        min_ending_here = std::min(min_ending_here, current_sum);\n        global_min = std::min(global_min, min_ending_here);\n        if (current_sum > 0) {\n            current_sum = 0;\n        }\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        int j = 0;\n        while (j + 7 < cols) {\n            __m256i chunk = _mm256_loadu_si256((__m256i*)&grid[i][j]);\n            __m256i mask = _mm256_set1_epi32(1);\n            __m256i result = _mm256_and_si256(chunk, mask);\n            int water_in_chunk = _mm256_movemask_epi8(result);\n            total_water += _mm_popcnt_u32(water_in_chunk);\n            j += 8;\n        }\n        while (j < cols) {\n            total_water += grid[i][j];\n            ++j;\n        }\n    }\n\n    int num_buckets = (total_water + capacity - 1) / capacity;\n    return num_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; i += 8) {\n        int remaining_rows = rows - i;\n        int j = 0;\n\n        for (; j <= cols - 8; j += 8) {\n            __m256i sum = _mm256_setzero_si256();\n\n            for (int k = 0; k < remaining_rows && k < 8; ++k) {\n                __m256i row = _mm256_loadu_si256((__m256i*)&grid[i + k][j]);\n                sum = _mm256_add_epi32(sum, row);\n            }\n\n            int temp[8];\n            _mm256_storeu_si256((__m256i*)temp, sum);\n\n            for (int k = 0; k < 8; ++k) {\n                total_buckets += (temp[k] + capacity - 1) / capacity;\n            }\n        }\n\n        for (; j < cols; ++j) {\n            int sum = 0;\n\n            for (int k = 0; k < remaining_rows; ++k) {\n                sum += grid[i + k][j];\n            }\n\n            total_buckets += (sum + capacity - 1) / capacity;\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; i += 8) {\n        int remaining_rows = rows - i;\n        if (remaining_rows >= 8) {\n            for (int j = 0; j < cols; j++) {\n                __m256i row1 = _mm256_set1_epi32(grid[i][j]);\n                __m256i row2 = _mm256_set1_epi32(grid[i + 1][j]);\n                __m256i row3 = _mm256_set1_epi32(grid[i + 2][j]);\n                __m256i row4 = _mm256_set1_epi32(grid[i + 3][j]);\n                __m256i row5 = _mm256_set1_epi32(grid[i + 4][j]);\n                __m256i row6 = _mm256_set1_epi32(grid[i + 5][j]);\n                __m256i row7 = _mm256_set1_epi32(grid[i + 6][j]);\n                __m256i row8 = _mm256_set1_epi32(grid[i + 7][j]);\n\n                __m256i sum = _mm256_add_epi32(row1, row2);\n                sum = _mm256_add_epi32(sum, row3);\n                sum = _mm256_add_epi32(sum, row4);\n                sum = _mm256_add_epi32(sum, row5);\n                sum = _mm256_add_epi32(sum, row6);\n                sum = _mm256_add_epi32(sum, row7);\n                sum = _mm256_add_epi32(sum, row8);\n\n                int water_units = _mm256_extract_epi32(sum, 0) + _mm256_extract_epi32(sum, 1) +\n                                 _mm256_extract_epi32(sum, 2) + _mm256_extract_epi32(sum, 3) +\n                                 _mm256_extract_epi32(sum, 4) + _mm256_extract_epi32(sum, 5) +\n                                 _mm256_extract_epi32(sum, 6) + _mm256_extract_epi32(sum, 7);\n\n                total_buckets += (water_units + capacity - 1) / capacity;\n            }\n        } else {\n            for (int j = 0; j < cols; j++) {\n                int water_units = 0;\n                for (int k = 0; k < remaining_rows; k++) {\n                    water_units += grid[i + k][j];\n                }\n                total_buckets += (water_units + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; i += 8) {\n        int j = 0;\n        for (; j <= cols - 8; j += 8) {\n            __m256i sum = _mm256_setzero_si256();\n            for (int k = 0; k < 8 && (i + k) < rows; ++k) {\n                __m256i row = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i + k][j]));\n                sum = _mm256_add_epi32(sum, row);\n            }\n            int temp[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n            for (int k = 0; k < 8; ++k) {\n                total_buckets += (temp[k] + capacity - 1) / capacity;\n            }\n        }\n        for (; j < cols; ++j) {\n            int sum = 0;\n            for (int k = 0; k < 8 && (i + k) < rows; ++k) {\n                sum += grid[i + k][j];\n            }\n            total_buckets += (sum + capacity - 1) / capacity;\n        }\n    }\n\n    for (int i = (rows / 8) * 8; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] > 0) {\n                total_buckets += (grid[i][j] + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; i += 8) {\n        int remaining_rows = std::min(8, rows - i);\n        __m256i sum = _mm256_setzero_si256();\n\n        for (int j = 0; j < cols; j += 8) {\n            int remaining_cols = std::min(8, cols - j);\n            __m256i row_data[8];\n\n            for (int k = 0; k < remaining_rows; ++k) {\n                if (remaining_cols == 8) {\n                    row_data[k] = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i + k][j]));\n                } else {\n                    row_data[k] = _mm256_setzero_si256();\n                    for (int l = 0; l < remaining_cols; ++l) {\n                        row_data[k] = _mm256_insert_epi8(row_data[k], grid[i + k][j + l], l);\n                    }\n                }\n            }\n\n            for (int k = 0; k < remaining_rows; ++k) {\n                sum = _mm256_add_epi32(sum, _mm256_sad_epu8(row_data[k], _mm256_setzero_si256()));\n            }\n        }\n\n        unsigned int lane[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(lane), sum);\n        int row_total = lane[0] + lane[1] + lane[2] + lane[3] + lane[4] + lane[5] + lane[6] + lane[7];\n\n        total_buckets += (row_total + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vec, odd_mask, even_pos_mask, result;\n    __m256i zero = _mm256_setzero_si256();\n    odd_mask = _mm256_set1_epi16(0x0001);\n    even_pos_mask = _mm256_setr_epi16(0, 0x0000, 0, 0x0000, 0, 0x0000, 0, 0x0000);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        result = _mm256_and_si256(vec, odd_mask);\n        result = _mm256_and_si256(result, even_pos_mask);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n\n        sum += _mm256_extract_epi16(result, 0);\n        sum += _mm256_extract_epi16(result, 1);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vec, odd_mask, even_pos_mask, result;\n    __m256i odd_check = _mm256_set1_epi16(0x0001);\n    __m256i even_pos_check = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        odd_mask = _mm256_and_si256(vec, odd_check);\n        even_pos_mask = _mm256_cmpeq_epi16(even_pos_check, zero_vec);\n        result = _mm256_and_si256(odd_mask, even_pos_mask);\n\n        int16_t temp[16];\n        _mm256_storeu_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 16; j += 2) {\n            sum += temp[j];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vec, odd_mask, even_pos_mask, result;\n    __m256i zero = _mm256_setzero_si256();\n    odd_mask = _mm256_set1_epi16(0x0001);\n    even_pos_mask = _mm256_setr_epi16(0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        result = _mm256_and_si256(vec, odd_mask);\n        result = _mm256_and_si256(result, even_pos_mask);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n\n        sum += _mm256_extract_epi16(result, 0);\n        sum += _mm256_extract_epi16(result, 1);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i odd_mask = _mm256_set1_epi16(1);\n    __m256i even_pos_mask = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i odd_elements = _mm256_and_si256(chunk, odd_mask);\n        __m256i even_pos_elements = _mm256_and_si256(chunk, even_pos_mask);\n        __m256i odd_in_even_pos = _mm256_and_si256(odd_elements, even_pos_elements);\n        sum = _mm256_add_epi16(sum, odd_in_even_pos);\n    }\n\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n\n    int64_t result = 0;\n    for (int i = 0; i < 16; ++i) {\n        result += temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vec, odd_mask, even_pos_mask, result;\n    __m256i zero = _mm256_setzero_si256();\n    odd_mask = _mm256_set1_epi16(0x0001);\n    even_pos_mask = _mm256_setr_epi16(0, 0x0001, 0, 0x0001, 0, 0x0001, 0, 0x0001, 0, 0x0001, 0, 0x0001, 0, 0x0001, 0, 0x0001);\n\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        vec = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        result = _mm256_and_si256(vec, odd_mask);\n        result = _mm256_and_si256(result, even_pos_mask);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n        result = _mm256_hadd_epi16(result, zero);\n\n        sum += _mm256_extract_epi16(result, 0);\n        sum += _mm256_extract_epi16(result, 1);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i masked_vec = _mm256_and_si256(vec, vec_mask);\n        __m256i cmp_mask = _mm256_cmpeq_epi32(vec, masked_vec);\n        __m256i filtered_vec = _mm256_and_si256(vec, cmp_mask);\n        vec_sum = _mm256_add_epi32(vec_sum, filtered_vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec_sum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vk = _mm256_set1_epi32(k);\n    __m256i v99 = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i velements = _mm256_loadu_si256(reinterpret_cast<__m256i*>(&arr[i]));\n        __m256i vmask = _mm256_cmpgt_epi32(v99, velements);\n        __m256i vselected = _mm256_and_si256(velements, vmask);\n        vsum = _mm256_add_epi32(vsum, vselected);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i vec_cmp = _mm256_and_si256(vec_data, vec_mask);\n        vec_cmp = _mm256_cmpeq_epi32(vec_cmp, vec_data);\n        vec_data = _mm256_and_si256(vec_data, vec_cmp);\n        vec_sum = _mm256_add_epi32(vec_sum, vec_data);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec_sum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (int i = k; i < (k + 7) / 8 * 8; ++i) {\n        if (arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i tens = _mm256_set1_epi32(10);\n    __m256i hundreds = _mm256_set1_epi32(100);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i chunk = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask1 = _mm256_cmpgt_epi32(chunk, ones);\n        __m256i mask2 = _mm256_cmpgt_epi32(hundreds, chunk);\n        __m256i mask = _mm256_and_si256(mask1, mask2);\n        __m256i valid = _mm256_and_si256(chunk, mask);\n        sum = _mm256_add_epi32(sum, valid);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_k = _mm256_set1_epi32(k);\n    __m256i vec_99 = _mm256_set1_epi32(99);\n    __m256i vec_10 = _mm256_set1_epi32(10);\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec_arr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec_cmp = _mm256_cmpgt_epi32(vec_99, vec_arr);\n        __m256i vec_mask = _mm256_and_si256(vec_arr, vec_cmp);\n        vec_sum = _mm256_add_epi32(vec_sum, vec_mask);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vec_sum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i dup = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        __m256i dup_curr = _mm256_cmpeq_epi32(prev, curr);\n        dup = _mm256_or_si256(dup, _mm256_and_si256(dup_curr, _mm256_slli_si256(dup_curr, 4)));\n\n        if (_mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp)) != 0) {\n            return false;\n        }\n        prev = curr;\n    }\n\n    if (_mm256_movemask_epi8(_mm256_packs_epi32(dup, dup)) != 0) {\n        return false;\n    }\n\n    for (int i = (n / 8) * 8 + 1; i < n; ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (lst[i] == lst[i - 1] && (i > 1 && lst[i] == lst[i - 2])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i curr;\n    __m256i cmp_res;\n    __m256i dup_res;\n    __m256i dup_count = _mm256_setzero_si256();\n    int mask;\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        cmp_res = _mm256_cmpgt_epi32(curr, prev);\n        dup_res = _mm256_cmpeq_epi32(curr, prev);\n        dup_count = _mm256_add_epi32(dup_count, dup_res);\n\n        mask = _mm256_movemask_ps(reinterpret_cast<__m256>(cmp_res));\n        if (mask != 0) return false;\n\n        prev = curr;\n    }\n\n    int dup_mask = _mm256_movemask_ps(reinterpret_cast<__m256>(dup_count));\n    if (dup_mask != 0) return false;\n\n    for (int i = n - (n % 8); i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) return false;\n        if (lst[i] == lst[i + 1] && (i < n - 2 && lst[i + 1] == lst[i + 2])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n < 2) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i dup = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        if (_mm256_movemask_epi8(cmp) != 0) return false;\n\n        __m256i eq = _mm256_cmpeq_epi32(prev, curr);\n        dup = _mm256_or_si256(dup, eq);\n\n        prev = curr;\n    }\n\n    if (_mm256_testz_si256(dup, dup) == 0) return false;\n\n    for (int i = (n & ~7) + 1; i < n; ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (lst[i] == lst[i - 1]) {\n            int count = 0;\n            while (i < n && lst[i] == lst[i - 1]) {\n                ++count;\n                ++i;\n            }\n            if (count > 1) return false;\n            --i;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() < 2) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    __m256i curr;\n    __m256i dup_mask = _mm256_setzero_si256();\n    __m256i cmp_mask;\n\n    for (size_t i = 1; i < lst.size(); i += 8) {\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        cmp_mask = _mm256_cmpgt_epi32(curr, prev);\n        if (!_mm256_testz_si256(cmp_mask, cmp_mask)) {\n            return false;\n        }\n        dup_mask = _mm256_or_si256(dup_mask, _mm256_cmpeq_epi32(curr, prev));\n        prev = curr;\n    }\n\n    if (_mm256_testc_si256(dup_mask, _mm256_cmpeq_epi32(dup_mask, _mm256_set1_epi32(1)))) {\n        return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() < 2) return true;\n\n    __m256i prev = _mm256_set1_epi32(lst[0]);\n    int duplicates = 1;\n\n    for (size_t i = 1; i <= lst.size() - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(current, prev);\n\n        if (_mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp)) != 0) {\n            return false;\n        }\n\n        __m256i eq = _mm256_cmpeq_epi32(current, prev);\n        duplicates += _mm256_movemask_epi8(_mm256_packs_epi32(eq, eq)) != 0 ? 1 : 0;\n\n        if (duplicates > 1) return false;\n\n        prev = current;\n    }\n\n    for (size_t i = (lst.size() / 8) * 8; i < lst.size(); ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (lst[i] == lst[i - 1]) {\n            if (++duplicates > 1) return false;\n        } else {\n            duplicates = 1;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i sign = _mm256_and_si256(vec, sign_mask);\n        __m256i sign_shifted = _mm256_srli_epi32(sign, 31);\n        __m256i sign_prod = _mm256_xor_si256(prod, sign_shifted);\n        prod = sign_prod;\n    }\n\n    alignas(32) int temp[8] = {0};\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += temp[j];\n    }\n\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), prod);\n    int total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_prod *= (temp[j] == 0) ? 1 : ((temp[j] < 0) ? -1 : 1);\n    }\n\n    for (; i < arr.size(); ++i) {\n        total_sum += std::abs(arr[i]);\n        total_prod *= (arr[i] == 0) ? 1 : ((arr[i] < 0) ? -1 : 1);\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_and_si256(vec, sign_mask);\n        __m256i sign = _mm256_or_si256(sign_vec, _mm256_set1_epi32(1));\n\n        sum = _mm256_add_epi32(sum, abs_vec);\n        prod = _mm256_mullo_epi32(prod, sign);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int total_sum = 0;\n    int total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += temp[j];\n    }\n\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), prod);\n    for (int j = 0; j < 8; ++j) {\n        total_prod *= temp[j];\n    }\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        total_sum += std::abs(val);\n        total_prod *= (val == 0) ? 0 : (val < 0 ? -1 : 1);\n    }\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_and_si256(vec, sign_mask);\n        __m256i sign = _mm256_or_si256(_mm256_cmpeq_epi32(vec, zero), _mm256_srli_epi32(sign_vec, 31));\n\n        sum = _mm256_add_epi32(sum, abs_vec);\n        prod = _mm256_mullo_epi32(prod, sign);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int sum_total = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum_total += temp[j];\n    }\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), prod);\n    int prod_total = 1;\n    for (int j = 0; j < 8; ++j) {\n        prod_total *= temp[j];\n    }\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        int sign = (val == 0) ? 0 : (val < 0) ? -1 : 1;\n        sum_total += std::abs(val);\n        prod_total *= sign;\n    }\n\n    return sum_total * prod_total;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_and_si256(vec, sign_mask);\n        __m256i sign = _mm256_or_si256(sign_vec, _mm256_set1_epi32(1));\n\n        sum = _mm256_add_epi32(sum, abs_vec);\n        prod = _mm256_mullo_epi32(prod, sign);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int sum_total = 0;\n    int prod_total = 1;\n    for (int j = 0; j < 8; ++j) {\n        sum_total += temp[j];\n    }\n\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), prod);\n    for (int j = 0; j < 8; ++j) {\n        prod_total *= temp[j];\n    }\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        int sign = (val > 0) - (val < 0);\n        sum_total += abs(val);\n        prod_total *= sign;\n    }\n\n    return sum_total * prod_total;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i sign = _mm256_blendv_epi8(one, neg_one, _mm256_cmpgt_epi32(zero, vec));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        prod = _mm256_mullo_epi32(prod, sign);\n        sum = _mm256_add_epi32(sum, abs_vec);\n    }\n\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int sum_total = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), prod);\n    int prod_total = temp[0] * temp[1] * temp[2] * temp[3] * temp[4] * temp[5] * temp[6] * temp[7];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        int sign = (val > 0) - (val < 0);\n        prod_total *= sign;\n        sum_total += std::abs(val);\n    }\n\n    return sum_total * prod_total;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath = {grid[i][j]};\n            dfs_simd(grid, i, j, k, 1, currentPath, minPath);\n        }\n    }\n\n    minPath.resize(k);\n    return minPath;\n}\n\nvoid dfs_simd(const std::vector<std::vector<int>> &grid, int x, int y, int k, int len, std::vector<int> &currentPath, std::vector<int> &minPath) {\n    if (len == k) {\n        if (std::lexicographical_compare(currentPath.begin(), currentPath.end(), minPath.begin(), minPath.end())) {\n            minPath = currentPath;\n        }\n        return;\n    }\n\n    int N = grid.size();\n    int directions[4][2] = {{0, 1}, {1, 0}, {0, -1}, {-1, 0}};\n\n    __m256i vCurrentPath = _mm256_loadu_si256((__m256i*)currentPath.data());\n    __m256i vMinPath = _mm256_loadu_si256((__m256i*)minPath.data());\n\n    for (int d = 0; d < 4; ++d) {\n        int nx = x + directions[d][0];\n        int ny = y + directions[d][1];\n        if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n            currentPath.push_back(grid[nx][ny]);\n            dfs_simd(grid, nx, ny, k, len + 1, currentPath, minPath);\n            currentPath.pop_back();\n        }\n    }\n\n    _mm256_storeu_si256((__m256i*)currentPath.data(), vCurrentPath);\n    _mm256_storeu_si256((__m256i*)minPath.data(), vMinPath);\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            __m256i path = _mm256_set1_epi32(grid[i][j]);\n            __m256i minPathVec = _mm256_loadu_si256((__m256i*)minPath.data());\n\n            for (int step = 1; step < k; ++step) {\n                int minVal = INT_MAX;\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) != abs(dj) && i + di >= 0 && i + di < N && j + dj >= 0 && j + dj < N) {\n                            minVal = std::min(minVal, grid[i + di][j + dj]);\n                        }\n                    }\n                }\n                currentPath[step] = minVal;\n                path = _mm256_insert_epi32(path, minVal, step);\n            }\n\n            minPathVec = _mm256_min_epi32(minPathVec, path);\n            _mm256_storeu_si256((__m256i*)minPath.data(), minPathVec);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath = {grid[i][j]};\n            __m256i path = _mm256_set1_epi32(grid[i][j]);\n            __m256i minPathVec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(minPath.data()));\n\n            for (int step = 1; step < k; ++step) {\n                int minValue = INT_MAX;\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) != abs(dj) && i + di >= 0 && i + di < N && j + dj >= 0 && j + dj < N) {\n                            int neighbor = grid[i + di][j + dj];\n                            if (neighbor < minValue) {\n                                minValue = neighbor;\n                            }\n                        }\n                    }\n                }\n                currentPath.push_back(minValue);\n                path = _mm256_insert_epi32(path, minValue, step % 8);\n            }\n\n            minPathVec = _mm256_min_epi32(minPathVec, path);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(minPath.data()), minPathVec);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            for (int step = 1; step < k; ++step) {\n                int minValue = INT_MAX;\n                __m256i vmin = _mm256_set1_epi32(INT_MAX);\n                __m256i vcurrent = _mm256_set1_epi32(currentPath[step - 1]);\n\n                if (i > 0) {\n                    __m256i vneighbor = _mm256_set1_epi32(grid[i - 1][j]);\n                    vmin = _mm256_min_epi32(vmin, vneighbor);\n                }\n                if (i < N - 1) {\n                    __m256i vneighbor = _mm256_set1_epi32(grid[i + 1][j]);\n                    vmin = _mm256_min_epi32(vmin, vneighbor);\n                }\n                if (j > 0) {\n                    __m256i vneighbor = _mm256_set1_epi32(grid[i][j - 1]);\n                    vmin = _mm256_min_epi32(vmin, vneighbor);\n                }\n                if (j < N - 1) {\n                    __m256i vneighbor = _mm256_set1_epi32(grid[i][j + 1]);\n                    vmin = _mm256_min_epi32(vmin, vneighbor);\n                }\n\n                int minIndices[8];\n                _mm256_storeu_si256((__m256i*)minIndices, vmin);\n                minValue = minIndices[0];\n\n                for (int l = 1; l < 8; ++l) {\n                    if (minIndices[l] < minValue) {\n                        minValue = minIndices[l];\n                    }\n                }\n\n                currentPath[step] = minValue;\n            }\n\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            for (int step = 1; step < k; ++step) {\n                int minValue = INT_MAX;\n                __m256i values = _mm256_set1_epi32(INT_MAX);\n                __m256i indices = _mm256_set1_epi32(0);\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) == abs(dj)) continue;\n                        int ni = i + di * step;\n                        int nj = j + dj * step;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            int value = grid[ni][nj];\n                            __m256i v = _mm256_set1_epi32(value);\n                            __m256i mask = _mm256_cmpgt_epi32(values, v);\n                            values = _mm256_blendv_epi8(values, v, mask);\n                            indices = _mm256_blendv_epi8(indices, _mm256_set1_epi32(value), mask);\n                        }\n                    }\n                }\n\n                int minIndex = _mm256_extract_epi32(indices, 0);\n                for (int l = 1; l < 8; ++l) {\n                    minIndex = std::min(minIndex, _mm256_extract_epi32(indices, l));\n                }\n\n                currentPath[step] = minIndex;\n            }\n\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    __m256i temp = _mm256_and_si256(n_vec, odd_mask);\n    __m256i is_odd = _mm256_cmpeq_epi8(temp, ones);\n    __m256i odd_digits = _mm256_and_si256(n_vec, is_odd);\n\n    while (_mm256_movemask_epi8(_mm256_cmpeq_epi8(odd_digits, zero)) != 0xFFFFFFFF) {\n        odd_digits = _mm256_srli_epi64(odd_digits, 4);\n        temp = _mm256_and_si256(odd_digits, odd_mask);\n        is_odd = _mm256_cmpeq_epi8(temp, ones);\n        odd_digits = _mm256_and_si256(odd_digits, is_odd);\n        product = _mm256_mullo_epi64(product, odd_digits);\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, product);\n\n    return result[0] | result[1] | result[2] | result[3];\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i digits = _mm256_set1_epi64x(n);\n    __m256i odd_mask = _mm256_set1_epi8(1);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n\n    for (int i = 0; i < 8; ++i) {\n        __m256i digit = _mm256_and_si256(digits, _mm256_set1_epi64x(0xF));\n        __m256i is_odd = _mm256_and_si256(digit, ones);\n        __m256i odd_digits = _mm256_cmpeq_epi8(is_odd, odd_mask);\n        product = _mm256_blendv_epi8(product, _mm256_mullo_epi64(product, digit), odd_digits);\n        digits = _mm256_srli_epi64(digits, 4);\n    }\n\n    __m256i mask = _mm256_cmpeq_epi64(product, zero);\n    uint64_t result = _mm256_movemask_epi8(mask) == 0xFFFFFFFF ? 0 : _mm256_extract_epi64(product, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i odds = _mm256_set1_epi8(1);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n\n    uint64_t result = 1;\n    bool has_odd = false;\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        if (digit % 2 == 1) {\n            has_odd = true;\n            result *= digit;\n        }\n    }\n\n    if (!has_odd) {\n        return 0;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n    uint64_t result = 1;\n    bool has_odd = false;\n\n    while (n > 0) {\n        uint64_t digits = n % 10000;\n        n /= 10000;\n\n        __m128i digits_vec = _mm_set1_epi32((int)digits);\n        __m256i digits_vec_256 = _mm256_cvtepu8_epi64(digits_vec);\n        __m256i odd_digits = _mm256_and_si256(digits_vec_256, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi8(odd_digits, ones);\n\n        __m256i odd_product = _mm256_blendv_epi8(zero, digits_vec_256, is_odd);\n        product = _mm256_mul_epu32(product, odd_product);\n\n        uint64_t temp_result = _mm256_extract_epi64(product, 0);\n        if (temp_result > 1) {\n            result *= temp_result;\n            has_odd = true;\n        }\n    }\n\n    return has_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i odd_mask = _mm256_set1_epi8(0x01);\n    __m256i product = _mm256_set1_epi64x(1);\n    __m256i zero = _mm256_set1_epi64x(0);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    __m256i temp = _mm256_and_si256(n_vec, odd_mask);\n    __m256i is_odd = _mm256_cmpeq_epi8(temp, ones);\n    __m256i odd_digits = _mm256_and_si256(n_vec, is_odd);\n\n    while (_mm256_testz_si256(odd_digits, odd_digits) == 0) {\n        __m256i digit = _mm256_and_si256(odd_digits, _mm256_set1_epi8(0x0F));\n        product = _mm256_mullo_epi64(product, digit);\n        odd_digits = _mm256_srli_epi64(odd_digits, 4);\n    }\n\n    __m256i result = _mm256_extract_epi64(product, 0);\n    return _mm256_extract_epi64(result, 0);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    size_t i = 0;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sqr_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum_vec = _mm256_add_ps(sum_vec, sqr_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (; i < size; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    uint64_t sum = 0;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256 temp_vec;\n\n    for (size_t i = 0; i < n; i += 8) {\n        size_t remaining = n - i;\n        __m256 data_vec = _mm256_loadu_ps(&lst[i]);\n\n        // Ceiling operation\n        temp_vec = _mm256_ceil_ps(data_vec);\n\n        // Square operation\n        temp_vec = _mm256_mul_ps(temp_vec, temp_vec);\n\n        // Accumulate the sum\n        sum_vec = _mm256_add_ps(sum_vec, temp_vec);\n    }\n\n    // Horizontal sum of the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    size_t i = 0;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256 temp_vec;\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sqr_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum_vec = _mm256_add_ps(sum_vec, sqr_vec);\n    }\n\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (; i < size; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m256 sum = _mm256_setzero_ps();\n    __m256 temp;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 8; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        temp = _mm256_ceil_ps(data);\n        temp = _mm256_mul_ps(temp, temp);\n        sum = _mm256_add_ps(sum, temp);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, sum);\n    uint64_t total = 0;\n    for (int j = 0; j < 8; ++j) {\n        total += static_cast<uint64_t>(result[j]);\n    }\n\n    for (; i < lst.size(); ++i) {\n        float val = std::ceil(lst[i]);\n        total += static_cast<uint64_t>(val * val);\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    __m256 sum_vec = _mm256_setzero_ps();\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sqr_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        sum_vec = _mm256_add_ps(sum_vec, sqr_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (; i < size; ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int result = -1;\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n\n        if (mask != 0) {\n            result = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n\n        prev = curr;\n    }\n\n    if (result == -1) {\n        for (int i = n - (n % 8); i < n; ++i) {\n            if (arr[i] < arr[i - 1]) {\n                result = i;\n                break;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int result = -1;\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (arr[i + j] < arr[i + j - 1]) {\n                    result = i + j;\n                }\n            }\n            return result;\n        }\n        prev = curr;\n    }\n\n    for (int i = (n & ~7) + 1; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int result = -1;\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n        if (mask != 0) {\n            result = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n\n        prev = current;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int result_index = -1;\n\n    for (size_t i = 1; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n        if (mask != 0) {\n            result_index = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n\n        prev = current;\n    }\n\n    if (result_index == -1) {\n        for (size_t i = (n & ~7) + 1; i < n; ++i) {\n            if (arr[i] < arr[i - 1]) {\n                result_index = i;\n                break;\n            }\n        }\n    }\n\n    return result_index;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int index = -1;\n\n    for (int i = 1; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n        if (mask != 0) {\n            index = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n\n        prev = current;\n    }\n\n    if (index == -1) {\n        for (int i = n - (n % 8); i < n; ++i) {\n            if (arr[i] < arr[i - 1]) {\n                index = i;\n                break;\n            }\n        }\n    }\n\n    return index;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m256i vmin = _mm256_set1_epi32(INT_MIN);\n    __m256i vmax = _mm256_set1_epi32(INT_MAX);\n    __m256i vzero = _mm256_set1_epi32(0);\n    __m256i vneg = _mm256_set1_epi32(-1);\n\n    int largest_negative = INT_MIN;\n    int smallest_positive = INT_MAX;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i vneg_mask = _mm256_cmpgt_epi32(vzero, v);\n        __m256i vpos_mask = _mm256_cmpgt_epi32(v, vzero);\n\n        __m256i vneg = _mm256_blendv_epi8(v, vmin, vneg_mask);\n        __m256i vpos = _mm256_blendv_epi8(v, vmax, vpos_mask);\n\n        vmin = _mm256_max_epi32(vmin, vneg);\n        vmax = _mm256_min_epi32(vmax, vpos);\n    }\n\n    int temp_neg[8];\n    int temp_pos[8];\n    _mm256_storeu_si256((__m256i*)temp_neg, vmin);\n    _mm256_storeu_si256((__m256i*)temp_pos, vmax);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp_neg[j] < 0 && temp_neg[j] > largest_negative) {\n            largest_negative = temp_neg[j];\n        }\n        if (temp_pos[j] > 0 && temp_pos[j] < smallest_positive) {\n            smallest_positive = temp_pos[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    if (largest_negative == INT_MIN) largest_negative = 0;\n    if (smallest_positive == INT_MAX) smallest_positive = 0;\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m256i vmin = _mm256_set1_epi32(INT_MIN);\n    __m256i vmax = _mm256_set1_epi32(INT_MAX);\n    __m256i vzero = _mm256_set1_epi32(0);\n    int largest_negative = INT_MIN;\n    int smallest_positive = INT_MAX;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i vneg = _mm256_cmpgt_epi32(vzero, v);\n        __m256i vpos = _mm256_cmpgt_epi32(v, vzero);\n\n        __m256i vmax_neg = _mm256_blendv_epi8(v, vmin, vneg);\n        __m256i vmin_pos = _mm256_blendv_epi8(v, vmax, vpos);\n\n        vmin = _mm256_max_epi32(vmin, vmax_neg);\n        vmax = _mm256_min_epi32(vmax, vmin_pos);\n    }\n\n    int temp_neg[8];\n    int temp_pos[8];\n    _mm256_storeu_si256((__m256i*)temp_neg, vmin);\n    _mm256_storeu_si256((__m256i*)temp_pos, vmax);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp_neg[j] > largest_negative && temp_neg[j] < 0) {\n            largest_negative = temp_neg[j];\n        }\n        if (temp_pos[j] < smallest_positive && temp_pos[j] > 0) {\n            smallest_positive = temp_pos[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    if (largest_negative == INT_MIN) largest_negative = 0;\n    if (smallest_positive == INT_MAX) smallest_positive = 0;\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i vmin = _mm256_set1_epi32(INT_MIN);\n    __m256i vmax = _mm256_set1_epi32(INT_MAX);\n    __m256i vzero = _mm256_set1_epi32(0);\n    __m256i vneg = _mm256_set1_epi32(-1);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i vneg_mask = _mm256_cmpgt_epi32(vzero, v);\n        __m256i vpos_mask = _mm256_cmpgt_epi32(v, vzero);\n\n        __m256i vneg = _mm256_blendv_epi8(v, vmax, vneg_mask);\n        __m256i vpos = _mm256_blendv_epi8(vmin, v, vpos_mask);\n\n        vmax = _mm256_max_epi32(vmax, vneg);\n        vmin = _mm256_min_epi32(vmin, vpos);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vmax);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < 0 && temp[j] > largest_neg) {\n            largest_neg = temp[j];\n        }\n    }\n\n    _mm256_storeu_si256((__m256i*)temp, vmin);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > 0 && temp[j] < smallest_pos) {\n            smallest_pos = temp[j];\n        }\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_neg) {\n            largest_neg = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_pos) {\n            smallest_pos = lst[i];\n        }\n    }\n\n    if (largest_neg == INT_MIN) largest_neg = 0;\n    if (smallest_pos == INT_MAX) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m256i vmin = _mm256_set1_epi32(INT_MIN);\n    __m256i vmax = _mm256_set1_epi32(INT_MAX);\n    __m256i vzero = _mm256_set1_epi32(0);\n    int min_val = INT_MIN;\n    int max_val = INT_MAX;\n    bool has_negative = false;\n    bool has_positive = false;\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i vneg = _mm256_cmpgt_epi32(vzero, v);\n        __m256i vpos = _mm256_cmpgt_epi32(v, vzero);\n\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(vneg, _mm256_setzero_si256()))) {\n            vmin = _mm256_max_epi32(vmin, _mm256_and_si256(vneg, v));\n            has_negative = true;\n        }\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(vpos, _mm256_setzero_si256()))) {\n            vmax = _mm256_min_epi32(vmax, _mm256_and_si256(vpos, v));\n            has_positive = true;\n        }\n    }\n\n    if (has_negative) {\n        int extract[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(extract), vmin);\n        for (int val : extract) {\n            if (val > min_val) min_val = val;\n        }\n    }\n\n    if (has_positive) {\n        int extract[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(extract), vmax);\n        for (int val : extract) {\n            if (val < max_val) max_val = val;\n        }\n    }\n\n    if (!has_negative) min_val = 0;\n    if (!has_positive) max_val = 0;\n\n    return {min_val, max_val};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m256i vmin = _mm256_set1_epi32(INT_MIN);\n    __m256i vmax = _mm256_set1_epi32(INT_MAX);\n    __m256i vzero = _mm256_set1_epi32(0);\n    __m256i vneg = _mm256_set1_epi32(-1);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i vneg_mask = _mm256_cmpgt_epi32(vzero, v);\n        __m256i vpos_mask = _mm256_cmpgt_epi32(v, vzero);\n\n        __m256i vneg = _mm256_blendv_epi8(v, vmax, vneg_mask);\n        __m256i vpos = _mm256_blendv_epi8(vmin, v, vpos_mask);\n\n        vmax = _mm256_max_epi32(vmax, vneg);\n        vmin = _mm256_min_epi32(vmin, vpos);\n    }\n\n    int temp_neg[8];\n    int temp_pos[8];\n    _mm256_storeu_si256((__m256i*)temp_neg, vmax);\n    _mm256_storeu_si256((__m256i*)temp_pos, vmin);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp_neg[j] != INT_MIN) largest_neg = std::max(largest_neg, temp_neg[j]);\n        if (temp_pos[j] != INT_MAX) smallest_pos = std::min(smallest_pos, temp_pos[j]);\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0) largest_neg = std::max(largest_neg, lst[i]);\n        if (lst[i] > 0) smallest_pos = std::min(smallest_pos, lst[i]);\n    }\n\n    if (largest_neg == INT_MIN) largest_neg = 0;\n    if (smallest_pos == INT_MAX) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    uint64_t factorial = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        factorial *= i;\n        result *= factorial;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    uint64_t factorial = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        factorial *= i;\n        result *= factorial;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    uint64_t factorial = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        factorial *= i;\n        result *= factorial;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    uint64_t factorial = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        factorial *= i;\n        result *= factorial;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    uint64_t factorial = 1;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        factorial *= i;\n        result *= factorial;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int sum = 0;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)&lst[i]);\n        __m256i v_sq = _mm256_mul_epi32(v, v);\n        __m256i v_cb = _mm256_mul_epi32(v_sq, v);\n\n        __m256i mask_3 = _mm256_setr_epi32(i % 3 == 0 ? -1 : 0, (i + 1) % 3 == 0 ? -1 : 0, (i + 2) % 3 == 0 ? -1 : 0, (i + 3) % 3 == 0 ? -1 : 0,\n                                          (i + 4) % 3 == 0 ? -1 : 0, (i + 5) % 3 == 0 ? -1 : 0, (i + 6) % 3 == 0 ? -1 : 0, (i + 7) % 3 == 0 ? -1 : 0);\n        __m256i mask_4 = _mm256_setr_epi32(i % 4 == 0 ? -1 : 0, (i + 1) % 4 == 0 ? -1 : 0, (i + 2) % 4 == 0 ? -1 : 0, (i + 3) % 4 == 0 ? -1 : 0,\n                                          (i + 4) % 4 == 0 ? -1 : 0, (i + 5) % 4 == 0 ? -1 : 0, (i + 6) % 4 == 0 ? -1 : 0, (i + 7) % 4 == 0 ? -1 : 0);\n\n        __m256i mask_3_not_4 = _mm256_andnot_si256(mask_3, mask_4);\n\n        __m256i result = _mm256_blendv_epi8(v, v_sq, mask_3);\n        result = _mm256_blendv_epi8(result, v_cb, mask_3_not_4);\n\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i*)temp, result);\n\n        for (int j = 0; j < 8 && i + j < n; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i values = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i mult_three = _mm256_set1_epi32(0);\n        __m256i mult_four = _mm256_set1_epi32(0);\n\n        __m256i rem_three = _mm256_set1_epi32(0);\n        __m256i rem_four = _mm256_set1_epi32(0);\n\n        rem_three = _mm256_rem_epi32(indices, three);\n        rem_four = _mm256_rem_epi32(indices, four);\n\n        mult_three = _mm256_cmpeq_epi32(rem_three, _mm256_setzero_si256());\n        mult_four = _mm256_cmpeq_epi32(rem_four, _mm256_setzero_si256());\n\n        mult_four = _mm256_andnot_si256(mult_three, mult_four);\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(values, values), values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, mult_three);\n        result = _mm256_blendv_epi8(result, cubed, mult_four);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i values = _mm256_loadu_si256((__m256i const*)&lst[i]);\n\n        __m256i mod3 = _mm256_and_si256(_mm256_set1_epi32(3), indices);\n        __m256i mod4 = _mm256_and_si256(_mm256_set1_epi32(4), indices);\n\n        __m256i is_mult_3 = _mm256_cmpeq_epi32(mod3, zero);\n        __m256i is_mult_4 = _mm256_cmpeq_epi32(mod4, zero);\n        __m256i is_mult_4_not_3 = _mm256_andnot_si256(is_mult_3, is_mult_4);\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mul_epi32(values, _mm256_mul_epi32(values, values));\n\n        __m256i result = _mm256_blendv_epi8(values, squared, is_mult_3);\n        result = _mm256_blendv_epi8(result, cubed, is_mult_4_not_3);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    return temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i mult3 = _mm256_set1_epi32(i / 3 * 3);\n        __m256i mult4 = _mm256_set1_epi32(i / 4 * 4);\n\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_and_si256(indices, three), mult3);\n        __m256i mask4 = _mm256_cmpeq_epi32(_mm256_and_si256(indices, four), mult4);\n\n        __m256i sq_values = _mm256_mullo_epi32(values, values);\n        __m256i cb_values = _mm256_mul_epi32(sq_values, values);\n\n        __m256i result = _mm256_blendv_epi8(values, sq_values, mask3);\n        result = _mm256_blendv_epi8(result, cb_values, mask4);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += temp[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i mult3 = _mm256_set1_epi32(3);\n        __m256i mult4 = _mm256_set1_epi32(4);\n\n        __m256i mask3 = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, mult3));\n        __m256i mask4 = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_rem_epi32(indices, mult4));\n\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(values, values), values);\n\n        __m256i result = _mm256_blendv_epi8(values, squared, mask3);\n        result = _mm256_blendv_epi8(result, cubed, mask4);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t size = nums.size();\n    const int* data = nums.data();\n\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v3 = _mm256_set1_epi32(3);\n    __m256i v5 = _mm256_set1_epi32(5);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v9 = _mm256_set1_epi32(9);\n    __m256i vodd_mask = _mm256_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i vabs = _mm256_abs_epi32(v);\n        __m256i vgt10 = _mm256_cmpgt_epi32(vabs, v10);\n\n        __m256i vfirst_digit = _mm256_and_si256(vabs, _mm256_set1_epi32(0xF));\n        __m256i vlast_digit = _mm256_and_si256(_mm256_srli_epi32(vabs, 28), _mm256_set1_epi32(0xF));\n\n        __m256i vodd_first = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(vfirst_digit, v1),\n                _mm256_cmpeq_epi32(vfirst_digit, v3)),\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(vfirst_digit, v5),\n                _mm256_cmpeq_epi32(vfirst_digit, v7)));\n        vodd_first = _mm256_or_si256(vodd_first, _mm256_cmpeq_epi32(vfirst_digit, v9));\n\n        __m256i vodd_last = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(vlast_digit, v1),\n                _mm256_cmpeq_epi32(vlast_digit, v3)),\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(vlast_digit, v5),\n                _mm256_cmpeq_epi32(vlast_digit, v7)));\n        vodd_last = _mm256_or_si256(vodd_last, _mm256_cmpeq_epi32(vlast_digit, v9));\n\n        __m256i vodd_both = _mm256_and_si256(vodd_first, vodd_last);\n        __m256i vmask = _mm256_and_si256(vgt10, vodd_both);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(vmask, _mm256_setzero_si256()));\n        count += _mm_popcnt_u32(mask & vodd_mask);\n    }\n\n    for (size_t i = (size & ~(size_t)7); i < size; ++i) {\n        int num = abs(data[i]);\n        if (num > 10) {\n            int first_digit = num % 10;\n            int last_digit = num ? num : 1;\n            while (num >= 10) {\n                last_digit = num % 10;\n                num /= 10;\n            }\n            if ((first_digit == 1 || first_digit == 3 || first_digit == 5 || first_digit == 7 || first_digit == 9) &&\n                (last_digit == 1 || last_digit == 3 || last_digit == 5 || last_digit == 7 || last_digit == 9)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t size = nums.size();\n    const int* data = nums.data();\n\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v3 = _mm256_set1_epi32(3);\n    __m256i v5 = _mm256_set1_epi32(5);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v9 = _mm256_set1_epi32(9);\n    __m256i vOddFirst = _mm256_setr_epi32(1, 3, 5, 7, 9, 0, 0, 0);\n    __m256i vOddLast = _mm256_setr_epi32(1, 3, 5, 7, 9, 0, 0, 0);\n\n    for (size_t i = 0; i < size - 7; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i vAbs = _mm256_abs_epi32(v);\n        __m256i vFirstDigit = _mm256_and_si256(vAbs, _mm256_set1_epi32(0xF));\n        __m256i vLastDigit = _mm256_and_si256(_mm256_srli_epi32(vAbs, 28), _mm256_set1_epi32(0xF));\n\n        __m256i vGreater10 = _mm256_cmpgt_epi32(vAbs, v10);\n        __m256i vOddFirstDigit = _mm256_cmpeq_epi32(vFirstDigit, vOddFirst);\n        __m256i vOddLastDigit = _mm256_cmpeq_epi32(vLastDigit, vOddLast);\n\n        __m256i vMask = _mm256_and_si256(_mm256_and_si256(vGreater10, vOddFirstDigit), vOddLastDigit);\n        int mask = _mm256_movemask_ps((__m256)vMask);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (size_t i = (size & ~7); i < size; ++i) {\n        int num = nums[i];\n        int absNum = abs(num);\n        int firstDigit = absNum % 10;\n        int lastDigit = absNum ? absNum : firstDigit;\n        while (absNum >= 10) absNum /= 10;\n        lastDigit = absNum;\n\n        if (absNum > 10 && (firstDigit == 1 || firstDigit == 3 || firstDigit == 5 || firstDigit == 7 || firstDigit == 9) &&\n            (lastDigit == 1 || lastDigit == 3 || lastDigit == 5 || lastDigit == 7 || lastDigit == 9)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t n = nums.size();\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v3 = _mm256_set1_epi32(3);\n    __m256i v5 = _mm256_set1_epi32(5);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v9 = _mm256_set1_epi32(9);\n    __m256i vodd1 = _mm256_set1_epi32(0xAAAAAAAA);\n    __m256i vodd2 = _mm256_set1_epi32(0xCCCCCCCC);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)&nums[i]);\n        __m256i vabs = _mm256_abs_epi32(v);\n        __m256i vcmp = _mm256_cmpgt_epi32(vabs, v10);\n\n        __m256i vfirst = _mm256_and_si256(v, vodd1);\n        __m256i vlast = _mm256_and_si256(v, vodd2);\n        __m256i vfirstodd = _mm256_cmpeq_epi32(vfirst, v1);\n        __m256i vlastodd = _mm256_cmpeq_epi32(vlast, v1);\n        vfirstodd = _mm256_or_si256(vfirstodd, _mm256_cmpeq_epi32(vfirst, v3));\n        vlastodd = _mm256_or_si256(vlastodd, _mm256_cmpeq_epi32(vlast, v3));\n        vfirstodd = _mm256_or_si256(vfirstodd, _mm256_cmpeq_epi32(vfirst, v5));\n        vlastodd = _mm256_or_si256(vlastodd, _mm256_cmpeq_epi32(vlast, v5));\n        vfirstodd = _mm256_or_si256(vfirstodd, _mm256_cmpeq_epi32(vfirst, v7));\n        vlastodd = _mm256_or_si256(vlastodd, _mm256_cmpeq_epi32(vlast, v7));\n        vfirstodd = _mm256_or_si256(vfirstodd, _mm256_cmpeq_epi32(vfirst, v9));\n        vlastodd = _mm256_or_si256(vlastodd, _mm256_cmpeq_epi32(vlast, v9));\n\n        __m256i vodd = _mm256_and_si256(vfirstodd, vlastodd);\n        __m256i vresult = _mm256_and_si256(vcmp, vodd);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(vresult, vresult));\n        count += _mm_popcnt_u32(mask & 0b11110000111100001111000011110000);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t n = nums.size();\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v3 = _mm256_set1_epi32(3);\n    __m256i v5 = _mm256_set1_epi32(5);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v9 = _mm256_set1_epi32(9);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(v, v10);\n\n        __m256i first_digit = _mm256_and_si256(v, _mm256_set1_epi32(0xF));\n        __m256i last_digit = _mm256_and_si256(_mm256_srli_epi32(v, _mm256_set1_epi32(28)), _mm256_set1_epi32(0xF));\n\n        __m256i odd_first = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(first_digit, v1),\n                _mm256_cmpeq_epi32(first_digit, v3)),\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(first_digit, v5),\n                _mm256_cmpeq_epi32(first_digit, v7)));\n        odd_first = _mm256_or_si256(odd_first, _mm256_cmpeq_epi32(first_digit, v9));\n\n        __m256i odd_last = _mm256_or_si256(\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(last_digit, v1),\n                _mm256_cmpeq_epi32(last_digit, v3)),\n            _mm256_or_si256(\n                _mm256_cmpeq_epi32(last_digit, v5),\n                _mm256_cmpeq_epi32(last_digit, v7)));\n        odd_last = _mm256_or_si256(odd_last, _mm256_cmpeq_epi32(last_digit, v9));\n\n        __m256i odd_both = _mm256_and_si256(odd_first, odd_last);\n        __m256i result = _mm256_and_si256(cmp, odd_both);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(result, _mm256_setzero_si256()));\n        count += _mm_popcnt_u32(mask & 0xFF);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t n = nums.size();\n    __m256i v10 = _mm256_set1_epi32(10);\n    __m256i v1 = _mm256_set1_epi32(1);\n    __m256i v3 = _mm256_set1_epi32(3);\n    __m256i v5 = _mm256_set1_epi32(5);\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i v9 = _mm256_set1_epi32(9);\n    __m256i vOddFirst = _mm256_setr_epi32(1, 3, 5, 7, 9, 0, 0, 0);\n    __m256i vOddLast = _mm256_setr_epi32(1, 3, 5, 7, 9, 0, 0, 0);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i vAbs = _mm256_abs_epi32(v);\n        __m256i vGreater = _mm256_cmpgt_epi32(vAbs, v10);\n\n        __m256i vFirstDigit = _mm256_and_si256(vAbs, _mm256_set1_epi32(0xF));\n        __m256i vLastDigit = _mm256_and_si256(_mm256_srli_epi32(vAbs, 28), _mm256_set1_epi32(0xF));\n\n        __m256i vFirstOdd = _mm256_cmpeq_epi32(vFirstDigit, vOddFirst);\n        __m256i vLastOdd = _mm256_cmpeq_epi32(vLastDigit, vOddLast);\n\n        __m256i vOdd = _mm256_and_si256(vFirstOdd, vLastOdd);\n        __m256i vResult = _mm256_and_si256(vGreater, vOdd);\n\n        int mask = _mm256_movemask_epi8(_mm256_packs_epi32(vResult, _mm256_setzero_si256()));\n        count += _mm_popcnt_u32(mask & 0xFF);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    uint64_t count = 0;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; k += 4) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i sum = _mm256_add_epi64(_mm256_add_epi64(vi, vj), vk);\n                __m256i mod3 = _mm256_and_si256(sum, _mm256_set1_epi64x(3));\n                uint64_t mask = _mm256_movemask_epi8(_mm256_cmpeq_epi64(mod3, _mm256_setzero_si256()));\n                count += _mm_popcnt_u64(mask);\n            }\n        }\n    }\n\n    free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi64x(a[j]);\n            __m256i vij = _mm256_add_epi64(vi, vj);\n            for (uint64_t k = j + 1; k < n; k += 4) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i vijk = _mm256_add_epi64(vij, vk);\n                __m256i vmod = _mm256_and_si256(vijk, _mm256_set1_epi64x(2));\n                __m256i vzero = _mm256_setzero_si256();\n                __m256i vcmp = _mm256_cmpeq_epi64(vmod, vzero);\n                uint64_t mask = _mm256_movemask_epi8(_mm256_packs_epi16(_mm256_packs_epi32(vcmp, vzero), vzero));\n                count += _mm_popcnt_u64(mask);\n            }\n        }\n    }\n\n    free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    uint64_t count = 0;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; k += 4) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i sum = _mm256_add_epi64(_mm256_add_epi64(vi, vj), vk);\n                __m256i mod3 = _mm256_and_si256(sum, _mm256_set1_epi64x(3));\n                uint64_t mask = _mm256_movemask_pd(_mm256_castsi256_pd(_mm256_cmpeq_epi64(mod3, _mm256_setzero_si256())));\n                count += _mm_popcnt_u64(mask);\n            }\n        }\n    }\n\n    free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)aligned_alloc(32, n * sizeof(uint64_t));\n    uint64_t count = 0;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; k += 4) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i sum = _mm256_add_epi64(_mm256_add_epi64(vi, vj), vk);\n                __m256i mod3 = _mm256_and_si256(sum, _mm256_set1_epi64x(3));\n                uint64_t mask = _mm256_movemask_epi8(_mm256_cmpeq_epi64(mod3, _mm256_setzero_si256()));\n                count += _mm_popcnt_u64(mask);\n            }\n        }\n    }\n\n    free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 32);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; k += 4) {\n                __m256i vk = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i sum = _mm256_add_epi64(_mm256_add_epi64(vi, vj), vk);\n                __m256i mod3 = _mm256_and_si256(sum, _mm256_set1_epi64x(3));\n                uint64_t mask = _mm256_movemask_pd(_mm256_castsi256_pd(_mm256_cmpeq_epi64(mod3, _mm256_setzero_si256())));\n                count += _mm_popcnt_u64(mask);\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i primes = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i x_vec = _mm256_set1_epi32(x);\n    __m256i y_vec = _mm256_set1_epi32(y);\n\n    __m256i cmp = _mm256_cmpeq_epi32(primes, n_vec);\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(cmp, cmp));\n\n    if (mask != 0) {\n        return x;\n    } else {\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i primes = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i x_vec = _mm256_set1_epi32(x);\n    __m256i y_vec = _mm256_set1_epi32(y);\n\n    __m256i is_prime = _mm256_cmpeq_epi32(primes, n_vec);\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(is_prime, is_prime));\n\n    return mask ? _mm256_extract_epi32(x_vec, 0) : _mm256_extract_epi32(y_vec, 0);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i two_vec = _mm256_set1_epi32(2);\n    __m256i i_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i is_prime_vec = _mm256_set1_epi32(1);\n\n    for (__m256i j_vec = _mm256_set1_epi32(2); ; j_vec = _mm256_add_epi32(j_vec, two_vec)) {\n        __m256i j_sq_vec = _mm256_mul_epu32(j_vec, j_vec);\n        __m256i cmp_vec = _mm256_cmpgt_epi32(j_sq_vec, n_vec);\n        if (_mm256_movemask_epi8(cmp_vec) == 0xFFFFFFFF) {\n            break;\n        }\n        __m256i mod_vec = _mm256_rem_epu32(n_vec, j_vec);\n        __m256i is_prime_mask = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n        is_prime_vec = _mm256_andnot_si256(is_prime_mask, is_prime_vec);\n    }\n\n    int is_prime = _mm256_movemask_epi8(is_prime_vec) == 0xFFFFFFFF;\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i primes = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i x_vec = _mm256_set1_epi32(x);\n    __m256i y_vec = _mm256_set1_epi32(y);\n\n    __m256i is_prime = _mm256_cmpeq_epi32(primes, n_vec);\n    int mask = _mm256_movemask_epi8(_mm256_packs_epi32(is_prime, is_prime));\n\n    return mask ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i x_vec = _mm256_set1_epi32(x);\n    __m256i y_vec = _mm256_set1_epi32(y);\n    __m256i two_vec = _mm256_set1_epi32(2);\n    __m256i i_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i prime_vec = _mm256_set1_epi32(1);\n\n    for (__m256i j_vec = _mm256_set1_epi32(2); ; j_vec = _mm256_add_epi32(j_vec, one_vec)) {\n        __m256i mod_vec = _mm256_rem_epu32(n_vec, j_vec);\n        __m256i cmp_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n        if (_mm256_testz_si256(cmp_vec, cmp_vec)) {\n            prime_vec = _mm256_set1_epi32(0);\n            break;\n        }\n        __m256i cmp2_vec = _mm256_cmpgt_epi32(j_vec, i_vec);\n        if (_mm256_testc_si256(cmp2_vec, cmp2_vec)) {\n            break;\n        }\n    }\n\n    __m256i result_vec = _mm256_blendv_epi8(y_vec, x_vec, prime_vec);\n    return _mm256_extract_epi32(result_vec, 0);\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t i = 0;\n    for (; i <= lst.size() - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256i intVec = _mm256_cvtps_epi32(vec);\n        __m256 oddMask = _mm256_cmp_ps(vec, _mm256_set1_ps(0.0f), _CMP_GT_OQ);\n        oddMask = _mm256_and_ps(oddMask, _mm256_cmp_ps(vec, _mm256_set1_ps(static_cast<float>(INT_MAX)), _CMP_LE_OQ));\n        oddMask = _mm256_and_ps(oddMask, _mm256_castsi256_ps(_mm256_set1_epi32(1)));\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 maskedSquared = _mm256_and_ps(squared, oddMask);\n        __m128 maskedSquaredLow = _mm256_extractf128_ps(maskedSquared, 0);\n        __m128 maskedSquaredHigh = _mm256_extractf128_ps(maskedSquared, 1);\n        maskedSquaredLow = _mm_hadd_ps(maskedSquaredLow, maskedSquaredLow);\n        maskedSquaredLow = _mm_hadd_ps(maskedSquaredLow, maskedSquaredLow);\n        maskedSquaredHigh = _mm_hadd_ps(maskedSquaredHigh, maskedSquaredHigh);\n        maskedSquaredHigh = _mm_hadd_ps(maskedSquaredHigh, maskedSquaredHigh);\n        sum += _mm_cvtss_f32(maskedSquaredLow) + _mm_cvtss_f32(maskedSquaredHigh);\n    }\n    for (; i < lst.size(); ++i) {\n        int intVal = static_cast<int>(lst[i]);\n        if (intVal > 0 && intVal % 2 == 1) {\n            sum += intVal * intVal;\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    const float* data = lst.data();\n\n    __m256 sum_v = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 v = _mm256_loadu_ps(&data[i]);\n        __m256 int_v = _mm256_cvttps_epi32(v);\n        __m256 float_v = _mm256_cvtepi32_ps(int_v);\n        __m256 odd_mask = _mm256_and_ps(_mm256_cvtepi32_ps(_mm256_set1_epi32(1)), float_v);\n        __m256 is_odd = _mm256_cmp_ps(odd_mask, one, _CMP_EQ_OQ);\n        __m256 is_positive = _mm256_cmp_ps(v, _mm256_setzero_ps(), _CMP_GT_OQ);\n        __m256 mask = _mm256_and_ps(is_odd, is_positive);\n        __m256 squared = _mm256_mul_ps(float_v, float_v);\n        squared = _mm256_and_ps(mask, squared);\n        sum_v = _mm256_add_ps(sum_v, squared);\n    }\n\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, sum_v);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float>& lst) {\n    uint64_t sum = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 int_vec = _mm256_cvttps_epi32(vec);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 is_odd = _mm256_and_ps(_mm256_cvtepi32_ps(_mm256_set1_epi32(1)), float_vec);\n        __m256 is_positive = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        __m256 mask = _mm256_and_ps(is_odd, is_positive);\n        __m256 squared = _mm256_mul_ps(float_vec, float_vec);\n        squared = _mm256_and_ps(mask, squared);\n        sum_vec = _mm256_add_ps(sum_vec, squared);\n    }\n\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (; i < size; ++i) {\n        int num = static_cast<int>(lst[i]);\n        if (num > 0 && num % 2 == 1) {\n            sum += num * num;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t n = lst.size();\n\n    __m256 sum_v = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m256 v = _mm256_loadu_ps(&lst[i]);\n        __m256 int_v = _mm256_cvttps_epi32(v);\n        __m256 float_v = _mm256_cvtepi32_ps(int_v);\n        __m256 odd_mask = _mm256_and_ps(_mm256_cmp_ps(float_v, _mm256_mul_ps(two, _mm256_floor_ps(_mm256_div_ps(float_v, two))), _CMP_NEQ_OQ), _mm256_cmp_ps(v, _mm256_setzero_ps(), _CMP_GT_OQ));\n        __m256 squared = _mm256_mul_ps(float_v, float_v);\n        sum_v = _mm256_add_ps(sum_v, _mm256_and_ps(odd_mask, squared));\n    }\n\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, sum_v);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    for (size_t i = (n & ~7); i < n; ++i) {\n        int int_v = static_cast<int>(lst[i]);\n        if (int_v > 0 && int_v % 2 == 1) {\n            sum += static_cast<uint64_t>(int_v * int_v);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    __m256 sum_vec = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 int_vec = _mm256_cvttps_epi32(vec);\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        __m256 is_odd = _mm256_and_ps(_mm256_castsi256_ps(_mm256_set1_epi32(1)), float_vec);\n        __m256 is_positive = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_GT_OQ);\n        __m256 mask = _mm256_and_ps(is_odd, is_positive);\n        __m256 squared = _mm256_mul_ps(float_vec, float_vec);\n        sum_vec = _mm256_add_ps(sum_vec, _mm256_and_ps(mask, squared));\n    }\n\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += static_cast<uint64_t>(temp[i]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    for (size_t i = 0; i < game.size(); i += 8) {\n        __m256i game_vec = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i guess_vec = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i diff = _mm256_abs_epi32(_mm256_sub_epi32(game_vec, guess_vec));\n        _mm256_storeu_si256((__m256i*)&result[i], diff);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    for (size_t i = 0; i < game.size(); i += 8) {\n        __m256i vgame = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i vguess = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i vdiff = _mm256_abs_epi32(_mm256_sub_epi32(vgame, vguess));\n        _mm256_storeu_si256((__m256i*)&result[i], vdiff);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    for (size_t i = 0; i < game.size(); i += 8) {\n        __m256i g = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i u = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i diff = _mm256_abs_epi32(_mm256_sub_epi32(g, u));\n        _mm256_storeu_si256((__m256i*)&result[i], diff);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    for (size_t i = 0; i < game.size(); i += 8) {\n        __m256i g = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i u = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i diff = _mm256_abs_epi32(_mm256_sub_epi32(g, u));\n        _mm256_storeu_si256((__m256i*)&result[i], diff);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    int i = 0;\n    for (; i <= game.size() - 8; i += 8) {\n        __m256i g = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i u = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i diff = _mm256_abs_epi32(_mm256_sub_epi32(g, u));\n        _mm256_storeu_si256((__m256i*)&result[i], diff);\n    }\n    for (; i < game.size(); ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n    return result;\n}\n```"}

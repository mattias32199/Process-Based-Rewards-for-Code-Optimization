{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_val = _mm_loadu_si128((__m128i*)(dst + i));\n        __m128i mask_val = _mm_set_epi64x((mask[i+1] ? -1 : 0), (mask[i] ? -1 : 0));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask_val, src_val), _mm_andnot_si128(mask_val, dst_val));\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_val = _mm_loadu_si128((__m128i*)(dst + i));\n        __m128i mask_val = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask_val, src_val), _mm_andnot_si128(mask_val, dst_val));\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        dst[i] = mask[i] ? src[i] : dst[i];\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_val = _mm_loadu_si128((__m128i*)(dst + i));\n        __m128i mask_val = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask_val, src_val), _mm_andnot_si128(mask_val, dst_val));\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        dst[i] = mask[i] ? src[i] : dst[i];\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    __m128i *dstV = (__m128i *)dst;\n    const __m128i *srcV = (const __m128i *)src;\n    const __m128i *maskV = (const __m128i *)mask;\n\n    for (i = 0; i + 1 < length / 2; i += 2) {\n        __m128i srcVal = _mm_loadu_si128(&srcV[i]);\n        __m128i maskVal = _mm_loadu_si128(&maskV[i]);\n        __m128i dstVal = _mm_loadu_si128(&dstV[i]);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(maskVal, srcVal), _mm_andnot_si128(maskVal, dstVal));\n\n        _mm_storeu_si128(&dstV[i], result);\n    }\n\n    for (i *= 2; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i dst_val = _mm_loadu_si128((__m128i*)(dst + i));\n        __m128i mask_val = _mm_set_epi64x(mask[i + 1] ? -1 : 0, mask[i] ? -1 : 0);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask_val, src_val), _mm_andnot_si128(mask_val, dst_val));\n\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    if (i < length) {\n        dst[i] = mask[i] ? src[i] : dst[i];\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i vscale = _mm_set1_epi32(scale);\n    __m128i vseven = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vsrc = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i vtmp = _mm_mullo_epi32(vsrc, vscale);\n\n        __m128i vtmp1 = _mm_srai_epi32(vtmp, 3);\n        __m128i vtmp2 = _mm_slli_epi32(vtmp, 3);\n\n        __m128i vtmp3 = _mm_mullo_epi32(vtmp1, vtmp2);\n        __m128i vtmp4 = _mm_add_epi32(vtmp, vtmp3);\n        __m128i vresult = _mm_div_epi32(vtmp4, vseven);\n\n        _mm_storeu_si128((__m128i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i vscale = _mm_set1_epi32(scale);\n    __m128i vseven = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vsrc = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i vtmp = _mm_mullo_epi32(vsrc, vscale);\n\n        __m128i vtmp1 = _mm_srai_epi32(vtmp, 3);\n        __m128i vtmp2 = _mm_slli_epi32(vtmp, 3);\n\n        __m128i vtmp3 = _mm_mullo_epi32(vtmp1, vtmp2);\n        __m128i vresult = _mm_add_epi32(vtmp, vtmp3);\n        vresult = _mm_div_epi32(vresult, vseven);\n\n        _mm_storeu_si128((__m128i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i vscale = _mm_set1_epi32(scale);\n    __m128i vseven = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vsrc = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i vscaled = _mm_mullo_epi32(vsrc, vscale);\n\n        __m128i vtmp1 = _mm_srai_epi32(vscaled, 3);\n        __m128i vtmp2 = _mm_slli_epi32(vscaled, 3);\n\n        __m128i vtmp3 = _mm_mullo_epi32(vtmp1, vtmp2);\n        __m128i vresult = _mm_add_epi32(vscaled, vtmp3);\n        vresult = _mm_div_epi32(vresult, vseven);\n\n        _mm_storeu_si128((__m128i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i vscale = _mm_set1_epi32(scale);\n    __m128i vseven = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vsrc = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i vtmp = _mm_mullo_epi32(vsrc, vscale);\n\n        __m128i vtmp1 = _mm_srai_epi32(vtmp, 3);\n        __m128i vtmp2 = _mm_slli_epi32(vtmp, 3);\n\n        __m128i vtmp3 = _mm_mullo_epi32(vtmp1, vtmp2);\n        __m128i vtmp4 = _mm_add_epi32(vtmp, vtmp3);\n        __m128i vresult = _mm_div_epi32(vtmp4, vseven);\n\n        _mm_storeu_si128((__m128i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i vscale = _mm_set1_epi32(scale);\n    __m128i vseven = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vsrc = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i vtmp = _mm_mullo_epi32(vsrc, vscale);\n\n        __m128i vtmp1 = _mm_srai_epi32(vtmp, 3);\n        __m128i vtmp2 = _mm_slli_epi32(vtmp, 3);\n\n        __m128i vtmp3 = _mm_mullo_epi32(vtmp1, vtmp2);\n        __m128i vresult = _mm_add_epi32(vtmp, vtmp3);\n        vresult = _mm_div_epi32(vresult, vseven);\n\n        _mm_storeu_si128((__m128i*)(dst + i), vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d value = _mm_load_sd(&src[r * cols + stride]);\n        _mm_store_sd(&dst[r], value);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d value = _mm_load_sd(&src[r * cols + stride]);\n        _mm_store_sd(&dst[r], value);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d value = _mm_load_sd(&src[r * cols + stride]);\n        _mm_store_sd(&dst[r], value);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i <= rows - 2; i += 2) {\n        __m128d val = _mm_loadu_pd(&src[(i + 0) * cols + stride]);\n        _mm_storeu_pd(&dst[i], val);\n    }\n\n    if (i < rows) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d value = _mm_load_sd(&src[r * cols + stride]);\n        _mm_store_sd(&dst[r], value);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src1 = _mm_set_ps(src[indices[i+3]], src[indices[i+2]], src[indices[i+1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i], src1);\n\n        __m128i idx2 = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src2 = _mm_loadu_ps(&src[i]);\n        _mm_store_ss(&dst[indices[i]], src2);\n        _mm_store_ss(&dst[indices[i+1]], _mm_shuffle_ps(src2, src2, _MM_SHUFFLE(1,1,1,1)));\n        _mm_store_ss(&dst[indices[i+2]], _mm_shuffle_ps(src2, src2, _MM_SHUFFLE(2,2,2,2)));\n        _mm_store_ss(&dst[indices[i+3]], _mm_shuffle_ps(src2, src2, _MM_SHUFFLE(3,3,3,3)));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src0 = _mm_set_ps(src[indices[i+3]], src[indices[i+2]], src[indices[i+1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i], src0);\n\n        __m128i idx_scatter = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src1 = _mm_loadu_ps(&src[i]);\n        _mm_storeu_ps(&dst[indices[i]], src1);\n        _mm_store_ss(&dst[indices[i+1]], _mm_load_ss(&src[i+1]));\n        _mm_store_ss(&dst[indices[i+2]], _mm_load_ss(&src[i+2]));\n        _mm_store_ss(&dst[indices[i+3]], _mm_load_ss(&src[i+3]));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 srcVals = _mm_set_ps(src[indices[i+3]], src[indices[i+2]], src[indices[i+1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i], srcVals);\n\n        __m128i idxScatter = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 srcValsScatter = _mm_loadu_ps(&src[i]);\n        _mm_storeu_ps(&dst[indices[i]], srcValsScatter);\n        _mm_storeu_ps(&dst[indices[i+1]], srcValsScatter);\n        _mm_storeu_ps(&dst[indices[i+2]], srcValsScatter);\n        _mm_storeu_ps(&dst[indices[i+3]], srcValsScatter);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src0 = _mm_set_ps(src[indices[i+3]], src[indices[i+2]], src[indices[i+1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i], src0);\n\n        __m128i idx_scatter = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 src1 = _mm_loadu_ps(&src[i]);\n        _mm_storeu_ps(&dst[indices[i]], src1);\n        _mm_store_ss(&dst[indices[i+1]], _mm_load_ss(&src[i+1]));\n        _mm_store_ss(&dst[indices[i+2]], _mm_load_ss(&src[i+2]));\n        _mm_store_ss(&dst[indices[i+3]], _mm_load_ss(&src[i+3]));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 srcVals = _mm_set_ps(src[indices[i+3]], src[indices[i+2]], src[indices[i+1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i], srcVals);\n\n        __m128i idxScatter = _mm_loadu_si128((__m128i*)&indices[i]);\n        __m128 srcValsScatter = _mm_loadu_ps(&src[i]);\n        _mm_storeu_ps(&dst[indices[i]], srcValsScatter);\n        _mm_store_ss(&dst[indices[i+1]], _mm_load_ss(&src[i+1]));\n        _mm_store_ss(&dst[indices[i+2]], _mm_load_ss(&src[i+2]));\n        _mm_store_ss(&dst[indices[i+3]], _mm_load_ss(&src[i+3]));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t len = end - start + 1;\n    uint64_t i = 0;\n\n    for (i = 0; i + 7 < len; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[start + len - i - 8]);\n\n        v1 = _mm_shufflehi_epi16(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        v1 = _mm_shufflelo_epi16(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm_shufflehi_epi16(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        v2 = _mm_shufflelo_epi16(v2, _MM_SHUFFLE(0, 1, 2, 3));\n\n        _mm_storeu_si128((__m128i*)&arr[start + i], v2);\n        _mm_storeu_si128((__m128i*)&arr[start + len - i - 8], v1);\n    }\n\n    for (; i < len / 2; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[start + len - i - 1];\n        arr[start + len - i - 1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    for (i = 0; i < length - 7; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[end - i - 7]);\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        v1 = _mm_shuffle_epi32(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm_storeu_si128((__m128i*)&arr[start + i], v2);\n        _mm_storeu_si128((__m128i*)&arr[end - i - 7], v1);\n    }\n\n    for (; i < length; i++) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t len = end - start + 1;\n    uint64_t i = 0;\n\n    for (i = 0; i + 7 < len; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[start + len - i - 8]);\n\n        v2 = _mm_shufflehi_epi16(v2, _MM_SHUFFLE(2, 3, 0, 1));\n        v2 = _mm_shufflelo_epi16(v2, _MM_SHUFFLE(2, 3, 0, 1));\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(1, 0, 3, 2));\n\n        _mm_storeu_si128((__m128i*)&arr[start + i], v2);\n        _mm_storeu_si128((__m128i*)&arr[start + len - i - 8], v1);\n    }\n\n    for (; i < len; i++) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[start + len - i - 1];\n        arr[start + len - i - 1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t length = end - start + 1;\n    uint64_t i = 0;\n\n    // Reverse elements using SIMD\n    for (i = 0; i < length - 7; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[start + length - i - 8]);\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        v1 = _mm_shuffle_epi32(v1, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm_storeu_si128((__m128i*)&arr[start + i], v2);\n        _mm_storeu_si128((__m128i*)&arr[start + length - i - 8], v1);\n    }\n\n    // Reverse remaining elements if any\n    for (; i < length / 2; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[start + length - i - 1];\n        arr[start + length - i - 1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    uint64_t len = end - start + 1;\n    uint64_t i = 0;\n\n    for (; i < (len >> 3); ++i) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&arr[start + i * 8]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&arr[end - i * 8]);\n        _mm_storeu_si128((__m128i*)&arr[start + i * 8], v2);\n        _mm_storeu_si128((__m128i*)&arr[end - i * 8], v1);\n    }\n\n    for (i *= 8; i < (len >> 1); ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t offset = slice_idx * slice_size;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 16) {\n            __m128i data = _mm_loadu_si128((__m128i const*)(tensor + offset + i * dim2 + j));\n            _mm_storeu_si128((__m128i*)(slice + i * dim2 + j), data);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i, j;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 16) {\n            __m128i data = _mm_loadu_si128((__m128i*)&tensor_slice[i * dim2 + j]);\n            _mm_storeu_si128((__m128i*)&slice[i * dim2 + j], data);\n        }\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = tensor_slice[i * dim2 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i;\n    for (i = 0; i + 15 < slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(tensor_slice + i));\n        _mm_storeu_si128((__m128i*)(slice + i), data);\n    }\n\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor_slice[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i;\n    for (i = 0; i + 15 < slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(tensor_slice + i));\n        _mm_storeu_si128((__m128i*)(slice + i), data);\n    }\n\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor_slice[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    const uint8_t *tensor_slice = tensor + slice_idx * slice_size;\n\n    size_t i;\n    for (i = 0; i <= slice_size - 16; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(tensor_slice + i));\n        _mm_storeu_si128((__m128i*)(slice + i), data);\n    }\n\n    for (; i < slice_size; ++i) {\n        slice[i] = tensor_slice[i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, l;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = i; k < i + block_size && k < rows; ++k) {\n                for (l = j; l < j + block_size && l < cols; l += 8) {\n                    __m128i row = _mm_loadu_si128((__m128i*)&src[k * cols + l]);\n                    __m128i row1 = _mm_loadu_si128((__m128i*)&src[k * cols + l + 8]);\n\n                    __m128i tmp0 = _mm_unpacklo_epi16(row, row1);\n                    __m128i tmp1 = _mm_unpackhi_epi16(row, row1);\n\n                    row = _mm_unpacklo_epi16(tmp0, tmp1);\n                    row1 = _mm_unpackhi_epi16(tmp0, tmp1);\n\n                    _mm_storeu_si128((__m128i*)&dst[l * rows + k], _mm_unpacklo_epi32(row, row1));\n                    _mm_storeu_si128((__m128i*)&dst[l * rows + k + 4], _mm_unpackhi_epi32(row, row1));\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, ii, jj;\n    __m128i row0, row1, row2, row3, row4, row5, row6, row7;\n    __m128i col0, col1, col2, col3, col4, col5, col6, col7;\n    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                for (jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    row0 = _mm_loadu_si128((__m128i*)&src[(ii + 0) * cols + jj]);\n                    row1 = _mm_loadu_si128((__m128i*)&src[(ii + 1) * cols + jj]);\n                    row2 = _mm_loadu_si128((__m128i*)&src[(ii + 2) * cols + jj]);\n                    row3 = _mm_loadu_si128((__m128i*)&src[(ii + 3) * cols + jj]);\n                    row4 = _mm_loadu_si128((__m128i*)&src[(ii + 4) * cols + jj]);\n                    row5 = _mm_loadu_si128((__m128i*)&src[(ii + 5) * cols + jj]);\n                    row6 = _mm_loadu_si128((__m128i*)&src[(ii + 6) * cols + jj]);\n                    row7 = _mm_loadu_si128((__m128i*)&src[(ii + 7) * cols + jj]);\n\n                    tmp0 = _mm_unpacklo_epi16(row0, row1);\n                    tmp1 = _mm_unpackhi_epi16(row0, row1);\n                    tmp2 = _mm_unpacklo_epi16(row2, row3);\n                    tmp3 = _mm_unpackhi_epi16(row2, row3);\n                    tmp4 = _mm_unpacklo_epi16(row4, row5);\n                    tmp5 = _mm_unpackhi_epi16(row4, row5);\n                    tmp6 = _mm_unpacklo_epi16(row6, row7);\n                    tmp7 = _mm_unpackhi_epi16(row6, row7);\n\n                    col0 = _mm_unpacklo_epi32(tmp0, tmp2);\n                    col1 = _mm_unpackhi_epi32(tmp0, tmp2);\n                    col2 = _mm_unpacklo_epi32(tmp1, tmp3);\n                    col3 = _mm_unpackhi_epi32(tmp1, tmp3);\n                    col4 = _mm_unpacklo_epi32(tmp4, tmp6);\n                    col5 = _mm_unpackhi_epi32(tmp4, tmp6);\n                    col6 = _mm_unpacklo_epi32(tmp5, tmp7);\n                    col7 = _mm_unpackhi_epi32(tmp5, tmp7);\n\n                    tmp0 = _mm_unpacklo_epi64(col0, col4);\n                    tmp1 = _mm_unpackhi_epi64(col0, col4);\n                    tmp2 = _mm_unpacklo_epi64(col1, col5);\n                    tmp3 = _mm_unpackhi_epi64(col1, col5);\n                    tmp4 = _mm_unpacklo_epi64(col2, col6);\n                    tmp5 = _mm_unpackhi_epi64(col2, col6);\n                    tmp6 = _mm_unpacklo_epi64(col3, col7);\n                    tmp7 = _mm_unpackhi_epi64(col3, col7);\n\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 0], tmp0);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 1], tmp1);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 2], tmp2);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 3], tmp3);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 4], tmp4);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 5], tmp5);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 6], tmp6);\n                    _mm_storeu_si128((__m128i*)&dst[ii * cols + jj + 7], tmp7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    __m128i row0, row1, row2, row3, row4, row5, row6, row7;\n    __m128i col0, col1, col2, col3, col4, col5, col6, col7;\n    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t k = i; k < i + block_size && k < rows; k += 8) {\n                row0 = _mm_loadu_si128((__m128i *)&src[k * cols + j]);\n                row1 = _mm_loadu_si128((__m128i *)&src[(k + 1) * cols + j]);\n                row2 = _mm_loadu_si128((__m128i *)&src[(k + 2) * cols + j]);\n                row3 = _mm_loadu_si128((__m128i *)&src[(k + 3) * cols + j]);\n                row4 = _mm_loadu_si128((__m128i *)&src[(k + 4) * cols + j]);\n                row5 = _mm_loadu_si128((__m128i *)&src[(k + 5) * cols + j]);\n                row6 = _mm_loadu_si128((__m128i *)&src[(k + 6) * cols + j]);\n                row7 = _mm_loadu_si128((__m128i *)&src[(k + 7) * cols + j]);\n\n                tmp0 = _mm_unpacklo_epi16(row0, row1);\n                tmp1 = _mm_unpackhi_epi16(row0, row1);\n                tmp2 = _mm_unpacklo_epi16(row2, row3);\n                tmp3 = _mm_unpackhi_epi16(row2, row3);\n                tmp4 = _mm_unpacklo_epi16(row4, row5);\n                tmp5 = _mm_unpackhi_epi16(row4, row5);\n                tmp6 = _mm_unpacklo_epi16(row6, row7);\n                tmp7 = _mm_unpackhi_epi16(row6, row7);\n\n                col0 = _mm_unpacklo_epi32(tmp0, tmp2);\n                col1 = _mm_unpackhi_epi32(tmp0, tmp2);\n                col2 = _mm_unpacklo_epi32(tmp1, tmp3);\n                col3 = _mm_unpackhi_epi32(tmp1, tmp3);\n                col4 = _mm_unpacklo_epi32(tmp4, tmp6);\n                col5 = _mm_unpackhi_epi32(tmp4, tmp6);\n                col6 = _mm_unpacklo_epi32(tmp5, tmp7);\n                col7 = _mm_unpackhi_epi32(tmp5, tmp7);\n\n                col0 = _mm_unpacklo_epi64(col0, col4);\n                col1 = _mm_unpackhi_epi64(col0, col4);\n                col2 = _mm_unpacklo_epi64(col1, col5);\n                col3 = _mm_unpackhi_epi64(col1, col5);\n                col4 = _mm_unpacklo_epi64(col2, col6);\n                col5 = _mm_unpackhi_epi64(col2, col6);\n                col6 = _mm_unpacklo_epi64(col3, col7);\n                col7 = _mm_unpackhi_epi64(col3, col7);\n\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k], col0);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 1], col1);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 2], col2);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 3], col3);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 4], col4);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 5], col5);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 6], col6);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + k + 7], col7);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, l;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = i; k < i + block_size && k < rows; ++k) {\n                for (l = j; l < j + block_size && l < cols; l += 8) {\n                    __m128i row = _mm_loadu_si128((__m128i*)&src[k * cols + l]);\n                    __m128i row1 = _mm_loadu_si128((__m128i*)&src[k * cols + l + 8]);\n\n                    __m128i tmp0 = _mm_unpacklo_epi16(row, row1);\n                    __m128i tmp1 = _mm_unpackhi_epi16(row, row1);\n\n                    row = _mm_unpacklo_epi16(tmp0, tmp1);\n                    row1 = _mm_unpackhi_epi16(tmp0, tmp1);\n\n                    _mm_storeu_si128((__m128i*)&dst[l * rows + k], _mm_unpacklo_epi32(row, row1));\n                    _mm_storeu_si128((__m128i*)&dst[l * rows + k + 4], _mm_unpackhi_epi32(row, row1));\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    __m128i row0, row1, row2, row3, row4, row5, row6, row7;\n    __m128i col0, col1, col2, col3, col4, col5, col6, col7;\n    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t k = 0; k < block_size; ++k) {\n                size_t idx = i + k;\n                if (idx >= rows) break;\n\n                row0 = _mm_loadu_si128((__m128i *)&src[(idx * cols) + j]);\n                row1 = _mm_loadu_si128((__m128i *)&src[(idx * cols) + j + 8]);\n                row2 = _mm_loadu_si128((__m128i *)&src[(idx + 1 * cols) + j]);\n                row3 = _mm_loadu_si128((__m128i *)&src[(idx + 1 * cols) + j + 8]);\n                row4 = _mm_loadu_si128((__m128i *)&src[(idx + 2 * cols) + j]);\n                row5 = _mm_loadu_si128((__m128i *)&src[(idx + 2 * cols) + j + 8]);\n                row6 = _mm_loadu_si128((__m128i *)&src[(idx + 3 * cols) + j]);\n                row7 = _mm_loadu_si128((__m128i *)&src[(idx + 3 * cols) + j + 8]);\n\n                tmp0 = _mm_unpacklo_epi16(row0, row1);\n                tmp1 = _mm_unpackhi_epi16(row0, row1);\n                tmp2 = _mm_unpacklo_epi16(row2, row3);\n                tmp3 = _mm_unpackhi_epi16(row2, row3);\n                tmp4 = _mm_unpacklo_epi16(row4, row5);\n                tmp5 = _mm_unpackhi_epi16(row4, row5);\n                tmp6 = _mm_unpacklo_epi16(row6, row7);\n                tmp7 = _mm_unpackhi_epi16(row6, row7);\n\n                col0 = _mm_unpacklo_epi32(tmp0, tmp2);\n                col1 = _mm_unpackhi_epi32(tmp0, tmp2);\n                col2 = _mm_unpacklo_epi32(tmp1, tmp3);\n                col3 = _mm_unpackhi_epi32(tmp1, tmp3);\n                col4 = _mm_unpacklo_epi32(tmp4, tmp6);\n                col5 = _mm_unpackhi_epi32(tmp4, tmp6);\n                col6 = _mm_unpacklo_epi32(tmp5, tmp7);\n                col7 = _mm_unpackhi_epi32(tmp5, tmp7);\n\n                row0 = _mm_unpacklo_epi64(col0, col4);\n                row1 = _mm_unpackhi_epi64(col0, col4);\n                row2 = _mm_unpacklo_epi64(col1, col5);\n                row3 = _mm_unpackhi_epi64(col1, col5);\n                row4 = _mm_unpacklo_epi64(col2, col6);\n                row5 = _mm_unpackhi_epi64(col2, col6);\n                row6 = _mm_unpacklo_epi64(col3, col7);\n                row7 = _mm_unpackhi_epi64(col3, col7);\n\n                _mm_storeu_si128((__m128i *)&dst[(j + k) * rows + i], row0);\n                _mm_storeu_si128((__m128i *)&dst[(j + k) * rows + i + 8], row1);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 1) * rows + i], row2);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 1) * rows + i + 8], row3);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 2) * rows + i], row4);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 2) * rows + i + 8], row5);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 3) * rows + i], row6);\n                _mm_storeu_si128((__m128i *)&dst[(j + k + 3) * rows + i + 8], row7);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_v = _mm_setzero_ps();\n    __m128 temp_v;\n    float temp[4] __attribute__((aligned(16)));\n    float result = 0.0f;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t idx1 = i * dim * dim + i * dim + i;\n        size_t idx2 = (i + 1) * dim * dim + (i + 1) * dim + (i + 1);\n        size_t idx3 = (i + 2) * dim * dim + (i + 2) * dim + (i + 2);\n        size_t idx4 = (i + 3) * dim * dim + (i + 3) * dim + (i + 3);\n\n        if (i + 3 < dim) {\n            temp_v = _mm_set_ps(array[idx4], array[idx3], array[idx2], array[idx1]);\n        } else {\n            temp[0] = array[idx1];\n            if (i + 1 < dim) temp[1] = array[idx2];\n            else temp[1] = 0.0f;\n            if (i + 2 < dim) temp[2] = array[idx3];\n            else temp[2] = 0.0f;\n            if (i + 3 < dim) temp[3] = array[idx4];\n            else temp[3] = 0.0f;\n            temp_v = _mm_load_ps(temp);\n        }\n\n        sum_v = _mm_add_ps(sum_v, temp_v);\n    }\n\n    _mm_store_ps(temp, sum_v);\n    result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_v = _mm_setzero_ps();\n    __m128 temp_v;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp_v = _mm_loadu_ps(&array[index]);\n        sum_v = _mm_add_ps(sum_v, temp_v);\n    }\n\n    float sum[4];\n    _mm_storeu_ps(sum, sum_v);\n\n    return sum[0] + sum[1] + sum[2] + sum[3];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_v = _mm_setzero_ps();\n    __m128 temp_v;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp_v = _mm_loadu_ps(&array[index]);\n        sum_v = _mm_add_ps(sum_v, temp_v);\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum_v);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = dim - (dim % 4); i < dim; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_v = _mm_setzero_ps();\n    __m128 temp_v;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        size_t index = i * (dim * dim + dim + 1);\n        temp_v = _mm_loadu_ps(&array[index]);\n        sum_v = _mm_add_ps(sum_v, temp_v);\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum_v);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = dim - (dim % 4); i < dim; ++i) {\n        size_t index = i * (dim * dim + dim + 1);\n        sum += array[index];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    __m128 temp_vec;\n    float sum = 0.0f;\n\n    size_t i = 0;\n    size_t j = 0;\n    size_t step = dim * dim;\n\n    for (; i <= dim - 4; i += 4, j += 4 * step) {\n        temp_vec = _mm_loadu_ps(&array[j]);\n        sum_vec = _mm_add_ps(sum_vec, temp_vec);\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < dim; ++i, j += step) {\n        sum += array[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d vthreshold = _mm_set1_pd(threshold);\n    __m128d vscale = _mm_set1_pd(scale);\n    __m128d vone = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vsrc = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d vmask = _mm_cmpgt_pd(vsrc, vthreshold);\n            __m128d vscaled = _mm_mul_pd(vsrc, vscale);\n            __m128d vresult = _mm_or_pd(_mm_and_pd(vmask, vscaled), _mm_andnot_pd(vmask, vsrc));\n            _mm_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d vthreshold = _mm_set1_pd(threshold);\n    __m128d vscale = _mm_set1_pd(scale);\n    __m128d vone = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vsrc = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d vmask = _mm_cmpgt_pd(vsrc, vthreshold);\n            __m128d vscaled = _mm_mul_pd(vsrc, vscale);\n            __m128d vresult = _mm_or_pd(_mm_and_pd(vmask, vscaled), _mm_andnot_pd(vmask, vsrc));\n            _mm_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d vthreshold = _mm_set1_pd(threshold);\n    __m128d vscale = _mm_set1_pd(scale);\n    size_t totalElements = rows * cols;\n\n    for (size_t i = 0; i < totalElements; i += 2) {\n        __m128d vsrc = _mm_loadu_pd(&src[i]);\n        __m128d vmask = _mm_cmpgt_pd(vsrc, vthreshold);\n        __m128d vscaled = _mm_mul_pd(vsrc, vscale);\n        __m128d vresult = _mm_or_pd(_mm_and_pd(vmask, vscaled), _mm_andnot_pd(vmask, vsrc));\n        _mm_storeu_pd(&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d vthreshold = _mm_set1_pd(threshold);\n    __m128d vscale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vsrc = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d vmask = _mm_cmpgt_pd(vsrc, vthreshold);\n            __m128d vscaled = _mm_mul_pd(vsrc, vscale);\n            __m128d vresult = _mm_or_pd(_mm_and_pd(vmask, vscaled), _mm_andnot_pd(vmask, vsrc));\n            _mm_storeu_pd(&dst[i * cols + j], vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d _threshold = _mm_set1_pd(threshold);\n    __m128d _scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d _src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d _mask = _mm_cmpgt_pd(_src, _threshold);\n            __m128d _scaled = _mm_mul_pd(_src, _scale);\n            __m128d _result = _mm_or_pd(_mm_and_pd(_mask, _scaled), _mm_andnot_pd(_mask, _src));\n            _mm_storeu_pd(&dst[i * cols + j], _result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        size_t j = 0;\n\n        for (; j <= cols - 2; j += 2) {\n            __m128d data = _mm_loadu_pd(&src[src_row + j]);\n            _mm_storeu_pd(&dst[dst_row + j], data);\n        }\n\n        if (j < cols) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        size_t j = 0;\n\n        for (; j <= cols - 2; j += 2) {\n            __m128d data = _mm_loadu_pd(&src[src_row + j]);\n            _mm_storeu_pd(&dst[dst_row + j], data);\n        }\n\n        if (j < cols) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        size_t j = 0;\n\n        for (; j <= cols - 2; j += 2) {\n            __m128d data = _mm_loadu_pd(&src[src_row + j]);\n            _mm_storeu_pd(&dst[dst_row + j], data);\n        }\n\n        if (j < cols) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row = indices[i] * cols;\n        size_t dst_row = i * cols;\n        size_t j = 0;\n\n        for (; j <= cols - 2; j += 2) {\n            __m128d data = _mm_loadu_pd(&src[src_row + j]);\n            _mm_storeu_pd(&dst[dst_row + j], data);\n        }\n\n        if (j < cols) {\n            dst[dst_row + j] = src[src_row + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        size_t j = 0;\n\n        for (; j <= cols - 2; j += 2) {\n            __m128d src_val = _mm_loadu_pd(&src[idx * cols + j]);\n            _mm_storeu_pd(&dst[i * cols + j], src_val);\n        }\n\n        for (; j < cols; ++j) {\n            dst[i * cols + j] = src[idx * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i v_scalar = _mm_set1_epi64x(scalar);\n    __m128i v_two = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_sum = _mm_add_epi64(v_src1, v_src2);\n        v_sum = _mm_add_epi64(v_sum, v_scalar);\n        v_sum = _mm_srai_epi64(_mm_slli_epi64(v_sum, 1), 1);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i vscalar = _mm_set1_epi64x(scalar);\n    __m128i vtwo = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vsrc1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vsrc2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i vsum = _mm_add_epi64(vsrc1, vsrc2);\n        vsum = _mm_add_epi64(vsum, vscalar);\n\n        __m128i vresult = _mm_srai_epi64(_mm_add_epi64(vsum, vtwo), 1);\n\n        _mm_storeu_si128((__m128i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i half_vec = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum, half_vec), 1);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i v_scalar = _mm_set1_epi64x(scalar);\n    __m128i v_two = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_add = _mm_add_epi64(v_src1, v_src2);\n        v_add = _mm_add_epi64(v_add, v_scalar);\n\n        __m128i v_result = _mm_srai_epi64(_mm_add_epi64(v_add, v_two), 1);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    __m128i half_vec = _mm_set1_epi64x(2);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n\n        __m128i result = _mm_srai_epi64(_mm_add_epi64(sum, half_vec), 1);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_vec = _mm_and_si128(vec, even_mask);\n        even_vec = _mm_cmpeq_epi32(even_vec, _mm_setzero_si128());\n        min_vec = _mm_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[4];\n    _mm_storeu_si128((__m128i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (size_t i = 0; i < 4; ++i) {\n        if (min_vals[i] < min_value) {\n            min_value = min_vals[i];\n        }\n    }\n\n    for (size_t i = length - (length % 4); i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_vec = _mm_and_si128(vec, even_mask);\n        even_vec = _mm_cmpeq_epi32(even_vec, _mm_setzero_si128());\n        min_vec = _mm_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[4];\n    _mm_storeu_si128((__m128i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (int i = 0; i < 4; ++i) {\n        if (min_vals[i] < min_value) {\n            min_value = min_vals[i];\n        }\n    }\n\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_chunk = _mm_and_si128(chunk, even_mask);\n        __m128i cmp = _mm_cmplt_epi32(even_chunk, min_val);\n\n        min_val = _mm_or_si128(_mm_and_si128(cmp, even_chunk), _mm_andnot_si128(cmp, min_val));\n    }\n\n    int min_vals[4];\n    _mm_storeu_si128((__m128i*)min_vals, min_val);\n\n    int min_even = INT_MAX;\n    for (int i = 0; i < 4; ++i) {\n        if (min_vals[i] < min_even && (min_vals[i] & 0xAAAAAAAA) == min_vals[i]) {\n            min_even = min_vals[i];\n        }\n    }\n\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_even) {\n            min_even = src[i];\n        }\n    }\n\n    return min_even == INT_MAX ? -1 : min_even;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m128i min_vec = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_vec = _mm_and_si128(vec, even_mask);\n        even_vec = _mm_cmpeq_epi32(even_vec, _mm_setzero_si128());\n        min_vec = _mm_min_epi32(min_vec, even_vec);\n    }\n\n    int min_vals[4];\n    _mm_storeu_si128((__m128i*)min_vals, min_vec);\n\n    int min_value = INT_MAX;\n    for (int i = 0; i < 4; ++i) {\n        if (min_vals[i] < min_value) {\n            min_value = min_vals[i];\n        }\n    }\n\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i even_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_chunk = _mm_and_si128(chunk, even_mask);\n        __m128i cmp = _mm_cmplt_epi32(even_chunk, min_val);\n\n        min_val = _mm_or_si128(_mm_and_si128(cmp, even_chunk), _mm_andnot_si128(cmp, min_val));\n    }\n\n    int min_vals[4];\n    _mm_storeu_si128((__m128i*)min_vals, min_val);\n\n    int min_even = INT_MAX;\n    for (int i = 0; i < 4; ++i) {\n        if (min_vals[i] < min_even && min_vals[i] != 0xAAAAAAAA) {\n            min_even = min_vals[i];\n        }\n    }\n\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_even) {\n            min_even = src[i];\n        }\n    }\n\n    return min_even == INT_MAX ? -1 : min_even;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i arith_shift_vec = _mm_sra_epi32(src_vec, shift_vec);\n        __m128i logical_shift_vec = _mm_srl_epi32(_mm_xor_si128(src_vec, _mm_set1_epi32(0x80000000)), shift_vec);\n        __m128i result_vec = _mm_add_epi32(arith_shift_vec, logical_shift_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i arith_shift_vec = _mm_srai_epi32(src_vec, shift_vec);\n        __m128i logical_shift_vec = _mm_srl_epi32(src_vec, shift_vec);\n        __m128i result_vec = _mm_add_epi32(arith_shift_vec, logical_shift_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (int)((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i arith_shift_vec = _mm_sra_epi32(src_vec, shift_vec);\n        __m128i logical_shift_vec = _mm_srl_epi32(src_vec, shift_vec);\n        __m128i result_vec = _mm_add_epi32(arith_shift_vec, logical_shift_vec);\n        _mm_storeu_si128((__m128i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n\n        __m128i arith_shift_vec = _mm_sra_epi32(src_vec, shift_vec);\n        __m128i logi_shift_vec = _mm_srl_epi32(_mm_and_si128(src_vec, _mm_set1_epi32(0x7FFFFFFF)), shift_vec);\n\n        __m128i result_vec = _mm_add_epi32(arith_shift_vec, logi_shift_vec);\n\n        _mm_storeu_si128((__m128i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i;\n    __m128i shift_vec = _mm_set1_epi32(shift);\n    __m128i zero = _mm_setzero_si128();\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const*)(src + i));\n\n        __m128i arith_shift_vec = _mm_sra_epi32(src_vec, shift_vec);\n\n        __m128i logical_shift_vec = _mm_srl_epi32(_mm_andnot_si128(_mm_cmplt_epi32(src_vec, zero), src_vec), shift_vec);\n\n        __m128i result_vec = _mm_add_epi32(arith_shift_vec, logical_shift_vec);\n\n        _mm_storeu_si128((__m128i*)(dst + i), result_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i const*)(matrix + i));\n        sum_vec = _mm_add_epi32(sum_vec, temp);\n    }\n\n    int32_t sums[4];\n    _mm_storeu_si128((__m128i*)sums, sum_vec);\n    int64_t sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    for (; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t totalElements = rows * cols;\n    __m128i sum128 = _mm_setzero_si128();\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= totalElements - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i const*)(matrix + i));\n        sum128 = _mm_add_epi32(sum128, temp);\n    }\n\n    int32_t sum[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)sum, sum128);\n\n    int64_t totalSum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < totalElements; ++i) {\n        totalSum += matrix[i];\n    }\n\n    return totalSum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i temp;\n    int64_t sum = 0;\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i const *)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, temp);\n    }\n\n    int32_t sums[4];\n    _mm_storeu_si128((__m128i *)sums, sum_vec);\n    sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    for (; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i const*)(matrix + i));\n        sum_vec = _mm_add_epi32(sum_vec, temp);\n    }\n\n    int32_t sums[4];\n    _mm_storeu_si128((__m128i*)sums, sum_vec);\n\n    int64_t sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    for (; i < size; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t totalElements = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= totalElements - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i const *)&matrix[i]);\n        sum = _mm_add_epi32(sum, temp);\n    }\n\n    int32_t result[4];\n    _mm_storeu_si128((__m128i *)result, sum);\n\n    int64_t finalSum = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < totalElements; ++i) {\n        finalSum += matrix[i];\n    }\n\n    return finalSum;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m128i current_val;\n\n    for (size_t i = 0; i < length - 15; i += 16) {\n        current_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i cmp = _mm_cmpgt_epi8(current_val, max_val);\n        int mask = _mm_movemask_epi8(cmp);\n\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            max_val = current_val;\n            max_idx = i + bit;\n            mask &= ~(1 << bit);\n        }\n    }\n\n    for (size_t i = (length - 15) * 16; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m128i idx = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_cmpgt_epi8(chunk, max_val);\n\n        if (_mm_movemask_epi8(mask) != 0) {\n            max_val = chunk;\n            max_idx = i + (size_t)_mm_extract_epi8(idx, _mm_bsrli_si128(mask, 0));\n        }\n    }\n\n    int8_t max_val_scalar = _mm_extract_epi8(max_val, 0);\n    for (size_t i = 1; i < 16; ++i) {\n        int8_t val = _mm_extract_epi8(max_val, i);\n        if (val > max_val_scalar) {\n            max_val_scalar = val;\n            max_idx = (max_idx & ~0xF) | i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m128i current_val;\n\n    for (size_t i = 0; i < length - 15; i += 16) {\n        current_val = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i cmp = _mm_cmpgt_epi8(current_val, max_val);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            max_val = current_val;\n            max_idx = i;\n        }\n    }\n\n    int8_t max_scalar = _mm_extract_epi8(max_val, 0);\n    for (size_t i = max_idx; i < length; ++i) {\n        if (src[i] > max_scalar) {\n            max_scalar = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m128i current_val;\n\n    for (size_t i = 0; i < length - 15; i += 16) {\n        current_val = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_cmpgt_epi8(current_val, max_val);\n\n        if (_mm_movemask_epi8(mask) != 0) {\n            max_val = current_val;\n            max_idx = i;\n        }\n    }\n\n    int8_t max_val_scalar = _mm_extract_epi8(max_val, 0);\n    for (size_t i = (length & ~15); i < length; ++i) {\n        if (src[i] > max_val_scalar) {\n            max_val_scalar = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_set1_epi8(INT8_MIN);\n    size_t max_idx = 0;\n    __m128i index = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_cmpgt_epi8(chunk, max_val);\n\n        if (_mm_movemask_epi8(mask) != 0) {\n            max_val = chunk;\n            max_idx = i + (size_t)_mm_extract_epi8(index, _mm_bsrli_si128(mask, 0));\n        }\n    }\n\n    int8_t max_val_scalar = _mm_extract_epi8(max_val, 0);\n    for (size_t i = (length & ~(size_t)15); i < length; ++i) {\n        if (src[i] > max_val_scalar) {\n            max_val_scalar = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi16(7, 6, 5, 4, 3, 2, 1, 0);\n        __m128i idxMask = _mm_set1_epi16(mask);\n        __m128i andResult = _mm_and_si128(idx, idxMask);\n        __m128i cmpResult = _mm_cmpgt_epi16(andResult, _mm_setzero_si128());\n\n        __m128i src1Val = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2Val = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(cmpResult, src1Val), _mm_andnot_si128(cmpResult, src2Val));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m128i mask_vector = _mm_set1_epi32(mask);\n    __m128i index_vector = _mm_set_epi32(6, 4, 2, 0);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i index_add = _mm_add_epi32(index_vector, _mm_set1_epi32(i));\n        __m128i condition = _mm_and_si128(index_add, mask_vector);\n        __m128i blend_mask = _mm_cmpeq_epi32(condition, _mm_setzero_si128());\n        __m128i result = _mm_blendv_epi8(src2_vector, src1_vector, blend_mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m128i mask_vector = _mm_set1_epi32(mask);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i index_vector = _mm_set_epi16(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i+0);\n        __m128i and_result = _mm_and_si128(index_vector, mask_vector);\n        __m128i compare_result = _mm_cmpeq_epi16(and_result, _mm_setzero_si128());\n\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i result = _mm_blendv_epi8(src2_vector, src1_vector, compare_result);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m128i mask_vector = _mm_set1_epi32(mask);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i index_vector = _mm_set_epi16(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i+0);\n        __m128i and_result = _mm_and_si128(index_vector, mask_vector);\n        __m128i compare_result = _mm_cmpeq_epi16(and_result, _mm_setzero_si128());\n\n        __m128i src1_vector = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_vector = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i result_vector = _mm_blendv_epi8(src2_vector, src1_vector, compare_result);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result_vector);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i;\n    __m128i mask_v = _mm_set1_epi32(mask);\n    __m128i index_v = _mm_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i index_v = _mm_add_epi32(index_v, _mm_set1_epi32(8));\n        __m128i src1_v = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_v = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i cond_v = _mm_and_si128(index_v, mask_v);\n        __m128i result_v = _mm_or_si128(_mm_and_si128(_mm_cmpeq_epi32(cond_v, _mm_setzero_si128()), src1_v),\n                                         _mm_andnot_si128(_mm_cmpeq_epi32(cond_v, _mm_setzero_si128()), src2_v));\n        _mm_storeu_si128((__m128i*)&dst[i], result_v);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m128i v, v1, v2, v3, v4, tmp1, tmp2, tmp3, tmp4, mask;\n    const __m128i m1 = _mm_set1_epi32(0x55555555);\n    const __m128i m2 = _mm_set1_epi32(0x33333333);\n    const __m128i m4 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i m8 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i m16 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&src[i]);\n\n        v1 = _mm_and_si128(v, m1);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 1), m1);\n        v = _mm_add_epi32(v1, v2);\n\n        v1 = _mm_and_si128(v, m2);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 2), m2);\n        v = _mm_add_epi32(v1, v2);\n\n        v = _mm_add_epi32(_mm_and_si128(v, m4), _mm_and_si128(_mm_srli_epi32(v, 4), m4));\n        v = _mm_add_epi32(_mm_and_si128(v, m8), _mm_and_si128(_mm_srli_epi32(v, 8), m8));\n        v = _mm_add_epi32(_mm_and_si128(v, m16), _mm_and_si128(_mm_srli_epi32(v, 16), m16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        val = (val & 0x55555555) + ((val >> 1) & 0x55555555);\n        val = (val & 0x33333333) + ((val >> 2) & 0x33333333);\n        val = (val & 0x0F0F0F0F) + ((val >> 4) & 0x0F0F0F0F);\n        val = (val & 0x00FF00FF) + ((val >> 8) & 0x00FF00FF);\n        val = (val & 0x0000FFFF) + ((val >> 16) & 0x0000FFFF);\n        dst[i] = val;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m128i v, v1, v2, v3, v4, tmp1, tmp2, tmp3, tmp4;\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&src[i]);\n\n        v1 = _mm_and_si128(v, mask1);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 1), mask1);\n        v = _mm_add_epi32(v1, v2);\n\n        v1 = _mm_and_si128(v, mask2);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 2), mask2);\n        v = _mm_add_epi32(v1, v2);\n\n        v = _mm_add_epi32(_mm_and_si128(v, mask3), _mm_and_si128(_mm_srli_epi32(v, 4), mask3));\n\n        v1 = _mm_and_si128(v, mask4);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 8), mask4);\n        v = _mm_add_epi32(v1, v2);\n\n        v1 = _mm_and_si128(v, _mm_set1_epi32(0x0000FFFF));\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 16), _mm_set1_epi32(0x0000FFFF));\n        v = _mm_add_epi32(v1, v2);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t x = src[i];\n        x = (x & 0x55555555) + ((x >> 1) & 0x55555555);\n        x = (x & 0x33333333) + ((x >> 2) & 0x33333333);\n        x = (x & 0x0F0F0F0F) + ((x >> 4) & 0x0F0F0F0F);\n        x = (x & 0x00FF00FF) + ((x >> 8) & 0x00FF00FF);\n        x = (x & 0x0000FFFF) + ((x >> 16) & 0x0000FFFF);\n        dst[i] = x;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m128i v, v1, v2, v3, v4, tmp1, tmp2, tmp3, tmp4, mask;\n    const __m128i m1 = _mm_set1_epi32(0x55555555);\n    const __m128i m2 = _mm_set1_epi32(0x33333333);\n    const __m128i m4 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i m8 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i m16 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&src[i]);\n\n        v1 = _mm_and_si128(v, m1);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 1), m1);\n        v = _mm_add_epi32(v1, v2);\n\n        v1 = _mm_and_si128(v, m2);\n        v2 = _mm_and_si128(_mm_srli_epi32(v, 2), m2);\n        v = _mm_add_epi32(v1, v2);\n\n        v = _mm_add_epi32(_mm_and_si128(v, m4), _mm_and_si128(_mm_srli_epi32(v, 4), m4));\n        v = _mm_add_epi32(_mm_and_si128(v, m8), _mm_and_si128(_mm_srli_epi32(v, 8), m8));\n        v = _mm_add_epi32(_mm_and_si128(v, m16), _mm_and_si128(_mm_srli_epi32(v, 16), m16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <tmmintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m128i v, tmp;\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask4 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i mask8 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i mask16 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&src[i]);\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 4), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask8), _mm_and_si128(_mm_srli_epi32(v, 8), mask8));\n        v = _mm_add_epi32(_mm_and_si128(v, mask16), _mm_and_si128(_mm_srli_epi32(v, 16), mask16));\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (; i < length; ++i) {\n        v = _mm_cvtsi32_si128(src[i]);\n        tmp = _mm_and_si128(v, mask1);\n        tmp = _mm_add_epi32(tmp, _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        tmp = _mm_add_epi32(_mm_and_si128(tmp, mask2), _mm_and_si128(_mm_srli_epi32(tmp, 2), mask2));\n        tmp = _mm_add_epi32(_mm_and_si128(tmp, mask4), _mm_and_si128(_mm_srli_epi32(tmp, 4), mask4));\n        tmp = _mm_add_epi32(_mm_and_si128(tmp, mask8), _mm_and_si128(_mm_srli_epi32(tmp, 8), mask8));\n        tmp = _mm_add_epi32(_mm_and_si128(tmp, mask16), _mm_and_si128(_mm_srli_epi32(tmp, 16), mask16));\n        dst[i] = _mm_cvtsi128_si32(tmp);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i;\n    __m128i v, v1, v2, v3, v4, tmp1, tmp2, tmp3, tmp4, mask;\n    const __m128i one = _mm_set1_epi8(1);\n    const __m128i two = _mm_set1_epi8(2);\n    const __m128i four = _mm_set1_epi8(4);\n    const __m128i eight = _mm_set1_epi8(8);\n    const __m128i sixteen = _mm_set1_epi8(16);\n    const __m128i thirtytwo = _mm_set1_epi8(32);\n    const __m128i sixtyfour = _mm_set1_epi8(64);\n    const __m128i onetwentyeight = _mm_set1_epi8(128);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&src[i]);\n\n        tmp1 = _mm_srli_epi32(v, 1);\n        tmp1 = _mm_and_si128(tmp1, one);\n        tmp2 = _mm_srli_epi32(v, 2);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 3);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 4);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 5);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 6);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 7);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 8);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 9);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 10);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 11);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 12);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 13);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 14);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 15);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 16);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 17);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 18);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 19);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 20);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 21);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 22);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 23);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 24);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 25);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 26);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 27);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 28);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 29);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 30);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n        tmp2 = _mm_srli_epi32(v, 31);\n        tmp2 = _mm_and_si128(tmp2, one);\n        tmp1 = _mm_add_epi32(tmp1, tmp2);\n\n        _mm_storeu_si128((__m128i*)&dst[i], tmp1);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i + 7 < length; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        dst[i] = (result < src1[i]) ? UINT16_MAX : result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <immintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (; i < length; ++i) {\n        uint16_t result = src1[i] + src2[i];\n        if (result < src1[i]) {\n            result = UINT16_MAX;\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d maskA = _mm_cmpgt_pd(_mm_set1_pd(100.0), _mm_abs_pd(a));\n                __m128d maskB = _mm_cmpgt_pd(_mm_set1_pd(100.0), _mm_abs_pd(b));\n                __m128d mask = _mm_and_pd(maskA, maskB);\n\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n\n                sum = _mm_add_pd(sum, prod);\n            }\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d maskA = _mm_and_pd(_mm_cmpgt_pd(a, _mm_set1_pd(-100)), _mm_cmplt_pd(a, _mm_set1_pd(100)));\n                __m128d maskB = _mm_and_pd(_mm_cmpgt_pd(b, _mm_set1_pd(-100)), _mm_cmplt_pd(b, _mm_set1_pd(100)));\n\n                __m128d mask = _mm_and_pd(maskA, maskB);\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n\n                sum = _mm_add_pd(sum, prod);\n            }\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d _min = _mm_set1_pd(-100.0);\n    __m128d _max = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d _sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d _a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d _b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d _mask_a = _mm_and_pd(_mm_cmpge_pd(_a, _min), _mm_cmple_pd(_a, _max));\n                __m128d _mask_b = _mm_and_pd(_mm_cmpge_pd(_b, _min), _mm_cmple_pd(_b, _max));\n\n                __m128d _mask = _mm_and_pd(_mask_a, _mask_b);\n                __m128d _prod = _mm_mul_pd(_a, _b);\n                _prod = _mm_and_pd(_prod, _mask);\n\n                _sum = _mm_add_pd(_sum, _prod);\n            }\n            _sum = _mm_hadd_pd(_sum, _sum);\n            _mm_storel_pd(&C[i * p + j], _sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d maskA = _mm_and_pd(_mm_cmpgt_pd(a, _mm_set1_pd(-100)), _mm_cmplt_pd(a, _mm_set1_pd(100)));\n                __m128d maskB = _mm_and_pd(_mm_cmpgt_pd(b, _mm_set1_pd(-100)), _mm_cmplt_pd(b, _mm_set1_pd(100)));\n\n                __m128d mask = _mm_and_pd(maskA, maskB);\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n\n                sum = _mm_add_pd(sum, prod);\n            }\n            double result[2];\n            _mm_storeu_pd(result, sum);\n            C[i * p + j] = result[0] + result[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d c = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d maskA = _mm_and_pd(_mm_cmpgt_pd(_mm_set1_pd(-100.0), a), _mm_cmplt_pd(a, _mm_set1_pd(100.0)));\n                __m128d maskB = _mm_and_pd(_mm_cmpgt_pd(_mm_set1_pd(-100.0), b), _mm_cmplt_pd(b, _mm_set1_pd(100.0)));\n\n                __m128d mask = _mm_and_pd(maskA, maskB);\n                __m128d prod = _mm_mul_pd(a, b);\n                prod = _mm_and_pd(prod, mask);\n\n                c = _mm_add_pd(c, prod);\n            }\n            __m128d sum = _mm_hadd_pd(c, c);\n            _mm_storel_pd(&C[i * p + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_size - 2; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n\n    if (i < total_size) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 1 < total_size; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    if (i < total_size) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i <= total_size - 2; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n\n    if (i < total_size) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 1 < total_size; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n\n    if (i < total_size) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 1 < total_size; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), c);\n    }\n\n    if (i < total_size) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        dst[i] = roundf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        dst[i] = roundf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        dst[i] = roundf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        dst[i] = roundf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(v1, v2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < length; ++i) {\n        float result = src1[i] + src2[i];\n        dst[i] = roundf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 absMask = _mm_set1_ps(-0.0f);\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 negativeOne = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 absDiff = _mm_and_ps(diff, absMask);\n\n        __m128 cmp1 = _mm_cmple_ps(absDiff, fifty);\n\n        __m128 mulResult = _mm_mul_ps(v1, v2);\n        __m128 roundResult = _mm_ceil_ps(mulResult);\n\n        __m128 mask = _mm_and_ps(cmp1, _mm_set_ps(\n            (i + 3) % 2 == 0 ? -1.0f : 0.0f,\n            (i + 2) % 2 == 0 ? -1.0f : 0.0f,\n            (i + 1) % 2 == 0 ? -1.0f : 0.0f,\n            i % 2 == 0 ? -1.0f : 0.0f\n        ));\n\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, roundResult), _mm_andnot_ps(mask, negativeOne));\n\n        _mm_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 absMask = _mm_set1_ps(0x7FFFFFFF);\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 negativeOne = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 absDiff = _mm_and_ps(_mm_sub_ps(v1, v2), absMask);\n        __m128 cmpMask = _mm_cmple_ps(absDiff, fifty);\n\n        __m128 mulResult = _mm_mul_ps(v1, v2);\n        __m128 roundResult = _mm_cvtepi32_ps(_mm_cvttps_epi32(_mm_add_ps(mulResult, _mm_set1_ps(0.5f))));\n\n        __m128 evenIndexMask = _mm_setr_ps(\n            (i % 2 == 0) ? 1.0f : 0.0f,\n            ((i + 1) % 2 == 0) ? 1.0f : 0.0f,\n            ((i + 2) % 2 == 0) ? 1.0f : 0.0f,\n            ((i + 3) % 2 == 0) ? 1.0f : 0.0f\n        );\n\n        __m128 finalMask = _mm_and_ps(cmpMask, evenIndexMask);\n        __m128 result = _mm_or_ps(_mm_and_ps(finalMask, roundResult), _mm_andnot_ps(finalMask, negativeOne));\n\n        _mm_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 absMask = _mm_set1_ps(-0.0f);\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 negativeOne = _mm_set1_ps(-1.0f);\n    __m128i indexMask = _mm_set1_epi32(2);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 src1Vec = _mm_loadu_ps(&src1[i]);\n        __m128 src2Vec = _mm_loadu_ps(&src2[i]);\n        __m128 diff = _mm_sub_ps(src1Vec, src2Vec);\n        __m128 absDiff = _mm_and_ps(diff, absMask);\n\n        __m128 cmp1 = _mm_cmplt_ps(absDiff, fifty);\n        __m128i index = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i cmp2 = _mm_cmpeq_epi32(_mm_and_si128(index, indexMask), _mm_setzero_si128());\n        __m128 cmpMask = _mm_and_ps(_mm_castsi128_ps(cmp2), cmp1);\n\n        __m128 result = _mm_mul_ps(src1Vec, src2Vec);\n        result = _mm_or_ps(_mm_and_ps(cmpMask, result), _mm_andnot_ps(cmpMask, negativeOne));\n\n        result = _mm_round_ps(result, _MM_FROUND_CEIL);\n\n        _mm_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 absMask = _mm_set1_ps(-0.0f);\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 negativeOne = _mm_set1_ps(-1.0f);\n    __m128i index = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 src1V = _mm_loadu_ps(&src1[i]);\n        __m128 src2V = _mm_loadu_ps(&src2[i]);\n        __m128 diff = _mm_sub_ps(src1V, src2V);\n        __m128 absDiff = _mm_andnot_ps(absMask, diff);\n        __m128 cmpMask = _mm_cmplt_ps(absDiff, fifty);\n\n        __m128 mulResult = _mm_mul_ps(src1V, src2V);\n        __m128 roundedResult = _mm_ceil_ps(mulResult);\n\n        __m128i idx = _mm_add_epi32(index, _mm_set1_epi32(i));\n        __m128i mask = _mm_set1_epi32(2);\n        __m128i cmpIndex = _mm_cmpeq_epi32(_mm_and_si128(idx, mask), _mm_setzero_si128());\n        __m128 cmpIndexMask = _mm_castsi128_ps(_mm_and_si128(cmpIndex, _mm_castps_si128(cmpMask)));\n\n        roundedResult = _mm_or_ps(_mm_and_ps(cmpIndexMask, roundedResult), _mm_andnot_ps(cmpIndexMask, negativeOne));\n\n        _mm_storeu_ps(&dst[i], roundedResult);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 absMask = _mm_set1_ps(-0.0f);\n    __m128 fifty = _mm_set1_ps(50.0f);\n    __m128 negativeOne = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&src1[i]);\n        __m128 v2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 diff = _mm_sub_ps(v1, v2);\n        __m128 absDiff = _mm_andnot_ps(absMask, diff);\n        __m128 cmpMask = _mm_cmple_ps(absDiff, fifty);\n\n        __m128 mulResult = _mm_mul_ps(v1, v2);\n        __m128 roundedResult = _mm_cvtepi32_ps(_mm_cvttps_epi32(_mm_add_ps(mulResult, _mm_set1_ps(0.5f))));\n\n        __m128 indexMask = _mm_set_ps(\n            (i + 3) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 2) % 2 == 0 ? 1.0f : 0.0f,\n            (i + 1) % 2 == 0 ? 1.0f : 0.0f,\n            i % 2 == 0 ? 1.0f : 0.0f\n        );\n\n        __m128 finalMask = _mm_and_ps(cmpMask, indexMask);\n        __m128 result = _mm_or_ps(_mm_and_ps(finalMask, roundedResult), _mm_andnot_ps(finalMask, negativeOne));\n\n        _mm_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalarVec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d matVec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result = _mm_mul_pd(matVec, scalarVec);\n            _mm_storeu_pd(&dst[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        __m128 sqrSum = _mm_setzero_ps();\n\n        size_t j = 0;\n        for (; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 sqr = _mm_mul_ps(row, row);\n            sqrSum = _mm_add_ps(sqrSum, sqr);\n        }\n\n        float temp[4];\n        _mm_storeu_ps(temp, sqrSum);\n        float rowSum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            rowSum += val * val;\n        }\n\n        float invNorm = 1.0f / sqrtf(rowSum);\n        __m128 norm = _mm_set1_ps(invNorm);\n\n        for (j = 0; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_mul_ps(row, norm);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= invNorm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        __m128 sqrSum = _mm_setzero_ps();\n\n        size_t j = 0;\n        for (; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 sqr = _mm_mul_ps(row, row);\n            sqrSum = _mm_add_ps(sqrSum, sqr);\n        }\n\n        float temp[4];\n        _mm_storeu_ps(temp, sqrSum);\n        float rowSum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            rowSum += val * val;\n        }\n\n        float invNorm = 1.0f / sqrtf(rowSum);\n        __m128 norm = _mm_set1_ps(invNorm);\n\n        j = 0;\n        for (; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_mul_ps(row, norm);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= invNorm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        size_t j = 0;\n\n        for (; j + 3 < n; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 sqr = _mm_mul_ps(row, row);\n            sum = _mm_add_ps(sum, sqr);\n        }\n\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            row_sum += val * val;\n        }\n\n        float inv_norm = 1.0f / sqrtf(row_sum);\n        __m128 norm = _mm_set1_ps(inv_norm);\n\n        for (j = 0; j + 3 < n; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_mul_ps(row, norm);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <pmmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        __m128 sqrSum = _mm_setzero_ps();\n        size_t j = 0;\n\n        for (; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 sqr = _mm_mul_ps(row, row);\n            sqrSum = _mm_add_ps(sqrSum, sqr);\n        }\n\n        float temp[4];\n        _mm_storeu_ps(temp, sqrSum);\n        float rowSum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            rowSum += val * val;\n        }\n\n        float invNorm = 1.0f / sqrtf(rowSum);\n        __m128 norm = _mm_set1_ps(invNorm);\n\n        for (j = 0; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_mul_ps(row, norm);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= invNorm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <pmmintrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        __m128 sqrSum = _mm_setzero_ps();\n\n        size_t j;\n        for (j = 0; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 sqr = _mm_mul_ps(row, row);\n            sqrSum = _mm_add_ps(sqrSum, sqr);\n        }\n\n        float temp[4];\n        _mm_storeu_ps(temp, sqrSum);\n        float rowSum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            rowSum += val * val;\n        }\n\n        float invNorm = 1.0f / sqrtf(rowSum);\n        __m128 norm = _mm_set1_ps(invNorm);\n\n        for (j = 0; j <= n - 4; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized = _mm_mul_ps(row, norm);\n            _mm_storeu_ps(&mat[i * n + j], normalized);\n        }\n\n        for (; j < n; ++j) {\n            mat[i * n + j] *= invNorm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            dst[i * p + j] = nearbyint(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            temp = _mm_add_sd(temp, _mm_set_sd(0.5));\n            dst[i * p + j] = _mm_cvtsd_f64(temp);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            temp = _mm_add_sd(temp, _mm_set_sd(0.5));\n            _mm_storel_pd(&dst[i * p + j], temp);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            dst[i * p + j] = nearbyint(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            dst[i * p + j] = nearbyint(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 srcVal = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 scaled = _mm_mul_ps(srcVal, quarter);\n            __m128 rounded = _mm_add_ps(scaled, half);\n            rounded = _mm_floor_ps(rounded);\n            rounded = _mm_mul_ps(rounded, quarter);\n            _mm_storeu_ps(&dst[j * rows + i], rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 data = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 rounded = _mm_round_ps(data, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            __m128 quartered = _mm_mul_ps(rounded, quarter);\n            quartered = _mm_round_ps(quartered, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            quartered = _mm_mul_ps(quartered, half);\n            _mm_storeu_ps(&dst[j * rows + i], quartered);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 data = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 rounded = _mm_round_ps(data, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            __m128 quartered = _mm_mul_ps(rounded, quarter);\n            __m128 result = _mm_add_ps(quartered, half);\n            _mm_storeu_ps(&dst[j * rows + i], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 srcVal = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 scaled = _mm_mul_ps(srcVal, quarter);\n            __m128 rounded = _mm_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m128 result = _mm_mul_ps(rounded, half);\n            _mm_storeu_ps(&dst[j * rows + i], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 half = _mm_set1_ps(0.5f);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += 4) {\n            __m128 data = _mm_loadu_ps(&src[i * cols + j]);\n            __m128 added = _mm_add_ps(data, data);\n            __m128 rounded = _mm_add_ps(added, half);\n            rounded = _mm_mul_ps(rounded, quarter);\n            rounded = _mm_floor_ps(rounded);\n            rounded = _mm_mul_ps(rounded, quarter);\n            _mm_storeu_ps(&dst[j * rows + i], rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m128 t1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub = _mm_sub_ps(t1, t2);\n        __m128 rounded = _mm_cvtepi32_ps(_mm_cvttps_epi32(sub));\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n\n    for (; i < total_size; ++i) {\n        float diff = tensor1[i] - tensor2[i];\n        dst[i] = (diff > 0) ? floorf(diff) : ceilf(diff);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128 xmm1, xmm2, xmm3, xmm4;\n\n    for (i = 0; i <= total_size - 4; i += 4) {\n        xmm1 = _mm_loadu_ps(&tensor1[i]);\n        xmm2 = _mm_loadu_ps(&tensor2[i]);\n        xmm3 = _mm_sub_ps(xmm1, xmm2);\n        xmm4 = _mm_setzero_ps();\n        xmm3 = _mm_xor_ps(xmm3, xmm4);\n        _mm_storeu_ps(&dst[i], xmm3);\n    }\n\n    for (; i < total_size; ++i) {\n        dst[i] = (float)((int)((tensor1[i] - tensor2[i])));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128 xmm1, xmm2, xmm3, xmm4;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        xmm1 = _mm_loadu_ps(&tensor1[i]);\n        xmm2 = _mm_loadu_ps(&tensor2[i]);\n        xmm3 = _mm_sub_ps(xmm1, xmm2);\n        xmm4 = _mm_set1_ps(0.5f);\n        xmm3 = _mm_add_ps(xmm3, xmm4);\n        xmm3 = _mm_floor_ps(xmm3);\n        xmm3 = _mm_sub_ps(xmm3, xmm4);\n        _mm_storeu_ps(&dst[i], xmm3);\n    }\n\n    for (; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        if (result > 0) {\n            dst[i] = floor(result);\n        } else {\n            dst[i] = ceil(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i + 3 < total_size; i += 4) {\n        __m128 t1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub = _mm_sub_ps(t1, t2);\n        __m128i rounded = _mm_cvtps_epi32(sub);\n        __m128 result = _mm_cvtepi32_ps(rounded);\n        _mm_storeu_ps(&dst[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        float diff = tensor1[i] - tensor2[i];\n        dst[i] = (diff > 0) ? floorf(diff) : ceilf(diff);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128 t1, t2, res;\n    __m128i res_int;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        t1 = _mm_loadu_ps(&tensor1[i]);\n        t2 = _mm_loadu_ps(&tensor2[i]);\n        res = _mm_sub_ps(t1, t2);\n        res_int = _mm_cvtps_epi32(res);\n        res = _mm_cvtepi32_ps(res_int);\n        _mm_storeu_ps(&dst[i], res);\n    }\n\n    for (; i < total_size; ++i) {\n        float temp = tensor1[i] - tensor2[i];\n        dst[i] = (temp > 0) ? floorf(temp) : ceilf(temp);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 1; j += 2) {\n            __m128d a = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d b = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d c = _mm_mul_pd(a, b);\n            _mm_storeu_pd(&dst[i * n + j], c);\n        }\n        if (j < n) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 1; j += 2) {\n            __m128d a = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d b = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d c = _mm_mul_pd(a, b);\n            _mm_storeu_pd(&dst[i * n + j], c);\n        }\n        if (j < n) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 1; j += 2) {\n            __m128d a = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d b = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d c = _mm_mul_pd(a, b);\n            _mm_storeu_pd(&dst[i * n + j], c);\n        }\n        if (j < n) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d a = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d b = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d c = _mm_mul_pd(a, b);\n            _mm_storeu_pd(&dst[i * n + j], c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 1; j += 2) {\n            __m128d a = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d b = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d c = _mm_mul_pd(a, b);\n            _mm_storeu_pd(&dst[i * n + j], c);\n        }\n        if (j < n) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 mat_val = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 vec_val = _mm_loadu_ps(&vec[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(mat_val, vec_val));\n        }\n        _mm_store_ss(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (j = 0; j < n; j += 4) {\n            __m128 vecVals = _mm_loadu_ps(&vec[j]);\n            __m128 matVals = _mm_loadu_ps(&mat[i * n + j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(matVals, vecVals));\n        }\n        float result[4];\n        _mm_storeu_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 vecVals = _mm_loadu_ps(&vec[j]);\n            __m128 matVals = _mm_loadu_ps(&mat[i * n + j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(matVals, vecVals));\n        }\n        _mm_store_ss(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 vec_chunk = _mm_loadu_ps(&vec[j]);\n            __m128 mat_chunk = _mm_loadu_ps(&mat[i * n + j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(mat_chunk, vec_chunk));\n        }\n        _mm_store_ss(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 vecVals = _mm_loadu_ps(&vec[j]);\n            __m128 matVals = _mm_loadu_ps(&mat[i * n + j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(matVals, vecVals));\n        }\n        float result[4];\n        _mm_storeu_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    size_t totalElements = m * n;\n    size_t i;\n\n    for (i = 0; i <= totalElements - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&mat[i]);\n        sum1 = _mm_add_pd(sum1, data);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n\n    __m128d temp = _mm_hadd_pd(sum1, sum1);\n\n    double sum[2];\n    _mm_storeu_pd(sum, temp);\n\n    double totalSum = sum[0] + sum[1];\n\n    if (i < totalElements) {\n        totalSum += mat[i];\n    }\n\n    return totalSum / totalElements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <emmintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    size_t totalElements = m * n;\n    size_t i;\n\n    for (i = 0; i <= totalElements - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&mat[i]);\n        sum1 = _mm_add_pd(sum1, data);\n    }\n\n    if (i < totalElements) {\n        __m128d data = _mm_loadh_pd(&mat[i], _mm_setzero_pd());\n        sum2 = _mm_add_sd(sum2, data);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    __m128d sum = _mm_hadd_pd(sum1, sum1);\n\n    double result[2];\n    _mm_storeu_pd(result, sum);\n\n    return (result[0] + result[1]) / totalElements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < n - 3; j += 4) {\n            __m128d row1 = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d row2 = _mm_loadu_pd(&mat[i * n + j + 2]);\n            sum1 = _mm_add_pd(sum1, row1);\n            sum2 = _mm_add_pd(sum2, row2);\n        }\n        if (j < n) {\n            __m128d row = _mm_setzero_pd();\n            for (; j < n; ++j) {\n                row = _mm_add_sd(row, _mm_load_sd(&mat[i * n + j]));\n            }\n            sum3 = _mm_add_pd(sum3, row);\n        }\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum3 = _mm_add_pd(sum3, sum4);\n    sum1 = _mm_add_pd(sum1, sum3);\n\n    __m128d temp = _mm_hadd_pd(sum1, sum1);\n    double sum = _mm_cvtsd_f64(temp);\n\n    return sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    size_t totalElements = m * n;\n    size_t i;\n\n    for (i = 0; i <= totalElements - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&mat[i]);\n        sum1 = _mm_add_pd(sum1, data);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    __m128d temp = _mm_hadd_pd(sum1, sum1);\n\n    double sum[2];\n    _mm_storeu_pd(sum, temp);\n\n    double totalSum = sum[0] + sum[1];\n\n    if (i < totalElements) {\n        totalSum += mat[i];\n    }\n\n    return totalSum / totalElements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\n#include <emmintrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t totalElements = m * n;\n    size_t i;\n\n    for (i = 0; i <= totalElements - 2; i += 2) {\n        __m128d data = _mm_loadu_pd(&mat[i]);\n        sum = _mm_add_pd(sum, data);\n    }\n\n    if (i < totalElements) {\n        __m128d data = _mm_loadh_pd(_mm_load_sd(&mat[i]), &mat[i]);\n        sum = _mm_add_sd(sum, data);\n    }\n\n    double temp[2];\n    _mm_storeu_pd(temp, sum);\n    double result = temp[0] + temp[1];\n\n    return result / totalElements;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i const*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i const*)&matrix[i * cols + j + 1]);\n\n            if (ascending) {\n                __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i const*)(matrix + i * cols + j));\n            __m128i v2 = _mm_loadu_si128((__m128i const*)(matrix + i * cols + j + 1));\n\n            if (direction) {\n                __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (direction) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool direction = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i const*)(matrix + i * cols + j));\n            __m128i v2 = _mm_loadu_si128((__m128i const*)(matrix + i * cols + j + 1));\n\n            if (direction) {\n                __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (direction) {\n                if (matrix[i * cols + j] > matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            } else {\n                if (matrix[i * cols + j] < matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool ascending = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n\n            __m128i cmp;\n            if (ascending) {\n                cmp = _mm_cmpgt_epi32(v2, v1);\n            } else {\n                cmp = _mm_cmpgt_epi32(v1, v2);\n            }\n\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool dir = directions[i];\n        for (size_t j = 0; j < cols - 1; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i const*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i const*)&matrix[i * cols + j + 1]);\n\n            if (dir) {\n                __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp = _mm_cmpgt_epi32(v1, v2);\n                if (_mm_movemask_epi8(cmp) != 0) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base);\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);\n        __m128i mul_vec = _mm_mullo_epi16(div_vec, base_vec);\n        _mm_storeu_si128((__m128i*)&dst[i], mul_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i _base = _mm_set1_epi16(base);\n    __m128i _zero = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i _src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i _rem = _mm_rem_epi16(_src, _base);\n        __m128i _dst = _mm_sub_epi16(_src, _rem);\n        _mm_storeu_si128((__m128i*)&dst[i], _dst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - src[i] % base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i _base = _mm_set1_epi16(base);\n    __m128i _zero = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i _src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i _rem = _mm_rem_epi16(_src, _base);\n        __m128i _div = _mm_sub_epi16(_src, _rem);\n        _mm_storeu_si128((__m128i*)&dst[i], _div);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i _base = _mm_set1_epi16(base);\n    __m128i _zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i _src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i _rem = _mm_rem_epi16(_src, _base);\n        __m128i _sub = _mm_sub_epi16(_src, _rem);\n        _mm_storeu_si128((__m128i*)&dst[i], _sub);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i base_vec = _mm_set1_epi16(base);\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i rem_vec = _mm_rem_epi16(src_vec, base_vec);\n        __m128i sub_vec = _mm_sub_epi16(src_vec, rem_vec);\n        _mm_storeu_si128((__m128i*)&dst[i], sub_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] - (src[i] % base);\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec0, vec1, mat0, mat1;\n    vec0 = _mm_loadu_pd(&vector[0]);\n    vec1 = _mm_loadu_pd(&vector[2]);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            mat0 = _mm_loadu_pd(&matrix[i * cols + j]);\n            mat1 = _mm_loadu_pd(&matrix[i * cols + j + 2]);\n\n            __m128d cmp0 = _mm_cmpeq_pd(mat0, vec0);\n            __m128d cmp1 = _mm_cmpeq_pd(mat1, vec1);\n\n            if (!_mm_movemask_pd(cmp0) == 3 || !_mm_movemask_pd(cmp1) == 3) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            for (size_t j = (cols / 4) * 4; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n            if (match) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec0, vec1, mat0, mat1;\n    vec0 = _mm_loadu_pd(vector);\n    vec1 = _mm_loadu_pd(vector + 2);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            mat0 = _mm_loadu_pd(matrix + i * cols + j);\n            mat1 = _mm_loadu_pd(matrix + i * cols + j + 2);\n\n            __m128d cmp0 = _mm_cmpeq_pd(mat0, vec0);\n            __m128d cmp1 = _mm_cmpeq_pd(mat1, vec1);\n\n            if (!_mm_movemask_pd(cmp0) || !_mm_movemask_pd(cmp1)) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match) {\n            for (size_t j = (cols / 4) * 4; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n            if (match) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&vector[j]);\n            __m128d mat = _mm_loadu_pd(&matrix[i * cols + j]);\n            __m128d cmp = _mm_cmpeq_pd(mat, vec);\n            if (!_mm_movemask_pd(cmp)) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec0, vec1, mat0, mat1, cmp0, cmp1, result;\n    vec0 = _mm_loadu_pd(vector);\n    vec1 = _mm_loadu_pd(vector + 2);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            mat0 = _mm_loadu_pd(&matrix[i * cols + j]);\n            mat1 = _mm_loadu_pd(&matrix[i * cols + j + 2]);\n            cmp0 = _mm_cmpeq_pd(mat0, vec0);\n            cmp1 = _mm_cmpeq_pd(mat1, vec1);\n            result = _mm_and_pd(cmp0, cmp1);\n            if (!_mm_movemask_pd(result)) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            for (size_t j = (cols / 4) * 4; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n            if (match) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec0 = _mm_loadu_pd(vector);\n    __m128d vec1 = _mm_loadu_pd(vector + 2);\n\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols - 1; j += 2) {\n            __m128d mat = _mm_loadu_pd(&matrix[i * cols + j]);\n            __m128d cmp0 = _mm_cmpeq_pd(mat, vec0);\n            if (_mm_movemask_pd(cmp0) != 3) {\n                match = false;\n                break;\n            }\n            if (j + 3 <= cols) {\n                vec0 = vec1;\n                vec1 = _mm_loadu_pd(vector + j + 4);\n            }\n        }\n        if (match && (cols % 2 != 0)) {\n            if (matrix[i * cols + cols - 1] != vector[cols - 1]) {\n                match = false;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stddef.h>\n#include <stdint.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < totalElements / 16; ++i) {\n        __m128i a = _mm_loadu_si128(&A_vec[i]);\n        __m128i b = _mm_loadu_si128(&B_vec[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_movemask_epi8(eq);\n\n        count += _mm_popcnt_u32(mask);\n    }\n\n    size_t remaining = totalElements % 16;\n    if (remaining > 0) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[totalElements - remaining]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[totalElements - remaining]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_movemask_epi8(eq);\n\n        count += _mm_popcnt_u32(mask & ((1 << remaining) - 1));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < totalElements / 16; ++i) {\n        __m128i a = _mm_loadu_si128(&A_vec[i]);\n        __m128i b = _mm_loadu_si128(&B_vec[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_movemask_epi8(eq);\n\n        count += _mm_popcnt_u32(_mm_extract_epi32(mask, 0)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(mask, 1)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(mask, 2)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(mask, 3));\n    }\n\n    size_t remaining = totalElements % 16;\n    for (size_t i = totalElements - remaining; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_set1_epi8(0xFF);\n        eq = _mm_and_si128(eq, mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(eq));\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_set1_epi8(0xFF);\n        eq = _mm_and_si128(eq, mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(eq));\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t totalElements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    for (; i <= totalElements - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_set1_epi8(0x01);\n        eq = _mm_and_si128(eq, mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(eq));\n    }\n\n    for (; i < totalElements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i prod = _mm_mul_epi32(va, vx);\n        __m128i abs_vy = _mm_abs_epi64(vy);\n        __m128i result = _mm_sub_epi64(prod, abs_vy);\n\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i prod = _mm_mul_epi32(va, vx);\n        __m128i abs_vy = _mm_abs_epi64(vy);\n        __m128i result = _mm_sub_epi64(prod, abs_vy);\n\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i prod = _mm_mul_epi32(va, vx);\n        __m128i abs_vy = _mm_abs_epi64(vy);\n        __m128i result = _mm_sub_epi64(prod, abs_vy);\n\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i vax = _mm_mul_epi32(va, vx);\n        __m128i vabs_y = _mm_abs_epi64(vy);\n        __m128i vresult = _mm_sub_epi64(vax, vabs_y);\n\n        _mm_storeu_si128((__m128i*)&y[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i prod = _mm_mul_epi32(va, vx);\n        __m128i abs_vy = _mm_abs_epi64(vy);\n        __m128i result = _mm_sub_epi64(prod, abs_vy);\n\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(SHRT_MAX);\n    __m128i max_val = _mm_set1_epi16(SHRT_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, chunk);\n        max_val = _mm_max_epi16(max_val, chunk);\n    }\n\n    int16_t min_arr[8], max_arr[8];\n    _mm_storeu_si128((__m128i*)min_arr, min_val);\n    _mm_storeu_si128((__m128i*)max_arr, max_val);\n\n    int16_t min_num = SHRT_MAX;\n    int16_t max_num = SHRT_MIN;\n\n    for (int i = 0; i < 8; ++i) {\n        if (min_arr[i] < min_num) min_num = min_arr[i];\n        if (max_arr[i] > max_num) max_num = max_arr[i];\n    }\n\n    return (MinMaxPair){min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, chunk);\n        max_val = _mm_max_epi16(max_val, chunk);\n    }\n\n    int16_t min_array[8], max_array[8];\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n    _mm_storeu_si128((__m128i*)max_array, max_val);\n\n    int16_t min_num = INT16_MAX;\n    int16_t max_num = INT16_MIN;\n\n    for (int i = 0; i < 8; ++i) {\n        if (min_array[i] < min_num) min_num = min_array[i];\n        if (max_array[i] > max_num) max_num = max_array[i];\n    }\n\n    return (MinMaxPair){min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, chunk);\n        max_val = _mm_max_epi16(max_val, chunk);\n    }\n\n    int16_t min_arr[8], max_arr[8];\n    _mm_storeu_si128((__m128i*)min_arr, min_val);\n    _mm_storeu_si128((__m128i*)max_arr, max_val);\n\n    int16_t min_num = min_arr[0], max_num = max_arr[0];\n    for (int i = 1; i < 8; ++i) {\n        if (min_arr[i] < min_num) min_num = min_arr[i];\n        if (max_arr[i] > max_num) max_num = max_arr[i];\n    }\n\n    return (MinMaxPair){min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, chunk);\n        max_val = _mm_max_epi16(max_val, chunk);\n    }\n\n    int16_t min_arr[8], max_arr[8];\n    _mm_storeu_si128((__m128i*)min_arr, min_val);\n    _mm_storeu_si128((__m128i*)max_arr, max_val);\n\n    int16_t min_num = min_arr[0], max_num = max_arr[0];\n    for (int i = 1; i < 8; ++i) {\n        if (min_arr[i] < min_num) min_num = min_arr[i];\n        if (max_arr[i] > max_num) max_num = max_arr[i];\n    }\n\n    return (MinMaxPair){min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    size_t i;\n    for (i = 0; i <= length - 8; i += 8) {\n        __m128i data = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, data);\n        max_val = _mm_max_epi16(max_val, data);\n    }\n\n    int16_t temp_min[8];\n    int16_t temp_max[8];\n    _mm_storeu_si128((__m128i*)temp_min, min_val);\n    _mm_storeu_si128((__m128i*)temp_max, max_val);\n\n    int16_t min_num = temp_min[0];\n    int16_t max_num = temp_max[0];\n\n    for (size_t j = 1; j < 8; ++j) {\n        if (temp_min[j] < min_num) min_num = temp_min[j];\n        if (temp_max[j] > max_num) max_num = temp_max[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    MinMaxPair result = {min_num, max_num};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n            __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n            __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j;\n        for (j = 0; j <= cols - 4; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&row[j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&row[j + 1]);\n            __m128i cmp = _mm_cmplt_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (; j < cols - 1; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n            __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&matrix[i * cols + j + 1]);\n            __m128i cmp = _mm_cmpgt_epi32(v2, v1);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n        }\n        for (size_t j = (cols / 4) * 4; j < cols - 1; ++j) {\n            if (matrix[i * cols + j] >= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n    size_t block_elements_vec = block_elements / 4;\n\n    __m128i first_block[block_elements_vec];\n    for (size_t i = 0; i < block_elements_vec; ++i) {\n        first_block[i] = _mm_loadu_si128((__m128i const*)(matrix + i * 4));\n    }\n\n    for (size_t b_row = 0; b_row < num_blocks_row; ++b_row) {\n        for (size_t b_col = 0; b_col < num_blocks_col; ++b_col) {\n            if (b_row == 0 && b_col == 0) continue;\n\n            for (size_t i = 0; i < block_elements_vec; ++i) {\n                size_t index = (b_row * block_size * cols) + (b_col * block_size) + (i * 4);\n                __m128i current_block = _mm_loadu_si128((__m128i const*)(matrix + index));\n\n                __m128i cmp_result = _mm_cmpeq_epi32(first_block[i], current_block);\n                if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_area = block_size * block_size;\n    size_t num_blocks_row = cols / block_size;\n    size_t num_blocks_col = rows / block_size;\n\n    __m128i first_block[4];\n    for (size_t i = 0; i < block_size; ++i) {\n        first_block[i] = _mm_loadu_si128((__m128i const*)(matrix + i * cols));\n    }\n\n    for (size_t b_row = 0; b_row < num_blocks_col; ++b_row) {\n        for (size_t b_col = 0; b_col < num_blocks_row; ++b_col) {\n            if (b_row == 0 && b_col == 0) continue;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                __m128i current_block = _mm_loadu_si128((__m128i const*)(matrix + (b_row * block_size + i) * cols + b_col * block_size));\n                __m128i cmp = _mm_cmpeq_epi32(first_block[i], current_block);\n                if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp, _mm_set1_epi32(-1))) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_area = block_size * block_size;\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t num_blocks = num_blocks_row * num_blocks_col;\n\n    __m128i first_block[block_area / 4];\n    for (size_t i = 0; i < block_area / 4; ++i) {\n        first_block[i] = _mm_loadu_si128((__m128i*)&matrix[i * 4]);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t row_start = (b / num_blocks_col) * block_size;\n        size_t col_start = (b % num_blocks_col) * block_size;\n        for (size_t i = 0; i < block_size; ++i) {\n            for (size_t j = 0; j < block_size / 4; ++j) {\n                __m128i current_block = _mm_loadu_si128((__m128i*)&matrix[(row_start + i) * cols + col_start + j * 4]);\n                __m128i first_block_val = first_block[i * (block_size / 4) + j];\n                __m128i cmp = _mm_cmpeq_epi32(first_block_val, current_block);\n                if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    __m128i first_block[block_elements / 4];\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            first_block[(i * block_size + j) / 4] = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n        }\n    }\n\n    for (size_t b_row = 0; b_row < num_blocks_row; ++b_row) {\n        for (size_t b_col = 0; b_col < num_blocks_col; ++b_col) {\n            if (b_row == 0 && b_col == 0) continue;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 4) {\n                    size_t index = (b_row * block_size + i) * cols + (b_col * block_size + j);\n                    __m128i current_block = _mm_loadu_si128((__m128i*)&matrix[index]);\n                    __m128i cmp_result = _mm_cmpeq_epi32(first_block[(i * block_size + j) / 4], current_block);\n                    if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t block_rows = rows / block_size;\n    size_t block_cols = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    __m128i first_block[4];\n    for (size_t i = 0; i < block_size; ++i) {\n        first_block[i] = _mm_loadu_si128((__m128i const*)(matrix + i * cols));\n    }\n\n    for (size_t b_row = 0; b_row < block_rows; ++b_row) {\n        for (size_t b_col = 0; b_col < block_cols; ++b_col) {\n            if (b_row == 0 && b_col == 0) continue;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                __m128i current_block = _mm_loadu_si128((__m128i const*)(matrix + (b_row * block_size + i) * cols + b_col * block_size));\n                __m128i cmp = _mm_cmpeq_epi32(first_block[i], current_block);\n                if (!_mm_movemask_epi8(_mm_cmpeq_epi32(cmp, _mm_set1_epi32(-1)))) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || block_size > length) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks == 0) {\n        return false;\n    }\n\n    __m128d tolerance_v = _mm_set1_pd(tolerance);\n    __m128d zero_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < block_size; i += 2) {\n        __m128d first_block_v = _mm_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m128d current_block_v = _mm_loadu_pd(&vec[j * block_size + i]);\n            __m128d diff_v = _mm_sub_pd(first_block_v, current_block_v);\n            __m128d abs_diff_v = _mm_or_pd(_mm_andnot_pd(_mm_cmplt_pd(diff_v, zero_v), _mm_sub_pd(zero_v, diff_v)),\n                                           _mm_and_pd(_mm_cmpge_pd(diff_v, zero_v), diff_v));\n\n            __m128d cmp_v = _mm_cmple_pd(abs_diff_v, tolerance_v);\n            if (!_mm_movemask_pd(_mm_cmpneq_pd(cmp_v, _mm_set1_pd(1.0))) == 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <cstddef>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < 2 * block_size) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d zero = _mm_setzero_pd();\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = block_size; i < num_blocks * block_size; i += block_size) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d a = _mm_loadu_pd(&vec[j]);\n            __m128d b = _mm_loadu_pd(&vec[i + j]);\n            __m128d diff = _mm_sub_pd(a, b);\n            __m128d abs_diff = _mm_max_pd(_mm_sub_pd(zero, diff), diff);\n            __m128d cmp = _mm_cmpgt_pd(abs_diff, tol);\n\n            if (_mm_movemask_pd(cmp) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || block_size > length) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks < 2) {\n        return false;\n    }\n\n    __m128d tolerance_v = _mm_set1_pd(tolerance);\n    __m128d zero_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < block_size; i += 2) {\n        __m128d first_block_v = _mm_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m128d current_block_v = _mm_loadu_pd(&vec[j * block_size + i]);\n            __m128d diff_v = _mm_sub_pd(first_block_v, current_block_v);\n            __m128d abs_diff_v = _mm_andnot_pd(_mm_set1_pd(-0.0), diff_v);\n\n            __m128d cmp_v = _mm_cmplt_pd(abs_diff_v, tolerance_v);\n            if (!_mm_movemask_pd(_mm_cmpunord_pd(cmp_v, cmp_v))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || block_size > length) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = block_size; i < full_blocks * block_size; i += block_size) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d diff = _mm_sub_pd(_mm_loadu_pd(&vec[i + j]), _mm_loadu_pd(&vec[j]));\n            __m128d abs_diff = _mm_andnot_pd(_mm_set_sd(-0.0), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, tol);\n            if (!_mm_movemask_pd(_mm_cmpneq_pd(cmp, zero))) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <emmintrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < 2 * block_size) {\n        return false;\n    }\n\n    size_t full_blocks = length / block_size;\n    if (full_blocks < 2) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < block_size; i += 2) {\n        __m128d first_block = _mm_loadu_pd(&vec[i]);\n\n        for (size_t j = 1; j < full_blocks; ++j) {\n            __m128d current_block = _mm_loadu_pd(&vec[j * block_size + i]);\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_or_pd(_mm_andnot_pd(_mm_cmplt_pd(diff, zero), diff), _mm_and_pd(_mm_cmplt_pd(zero, diff), _mm_xor_pd(diff, _mm_set1_pd(-0.0))));\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, tol);\n            if (!_mm_movemask_pd(_mm_cmpneq_pd(cmp, _mm_set1_pd(1.0))) == 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(row + j);\n            data = _mm_mul_pd(data, data);\n            sum = _mm_add_pd(sum, data);\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(row + j);\n            data = _mm_mul_pd(data, data);\n            sum = _mm_add_pd(sum, data);\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(row + j);\n            data = _mm_mul_pd(data, data);\n            sum = _mm_add_pd(sum, data);\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(row + j);\n            sum = _mm_add_pd(sum, _mm_mul_pd(data, data));\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d data = _mm_loadu_pd(row + j);\n            data = _mm_mul_pd(data, data);\n            sum = _mm_add_pd(sum, data);\n        }\n\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double row_norm = sqrt(temp[0] + temp[1]);\n\n        result[i] = row_norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 3 < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_or_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_sq = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(a, _mm_setzero_si128()));\n        __m128i b_sq = _mm_madd_epi16(_mm_unpacklo_epi8(b, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n        __m128i prod = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpacklo_epi8(b, _mm_setzero_si128()));\n\n        sum_sq = _mm_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(2, 3, 0, 1))));\n    int64_t sum_prod_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(2, 3, 0, 1))));\n\n    for (; i < length; ++i) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_sq = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(a, _mm_setzero_si128()));\n        __m128i b_sq = _mm_madd_epi16(_mm_unpacklo_epi8(b, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n        __m128i prod = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpacklo_epi8(b, _mm_setzero_si128()));\n\n        sum_sq = _mm_add_epi32(sum_sq, _mm_add_epi32(a_sq, b_sq));\n        sum_prod = _mm_add_epi32(sum_prod, prod);\n    }\n\n    int64_t result_sq = _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(2, 3, 0, 1)))) +\n                        _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(2, 3, 0, 1))));\n    int64_t result_prod = _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(2, 3, 0, 1)))) +\n                         _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(2, 3, 0, 1))));\n\n    for (; i < length; ++i) {\n        result_sq += A[i] * A[i] + B[i] * B[i];\n        result_prod += A[i] * B[i];\n    }\n\n    return result_sq - result_prod;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n\n        __m128i a_sq = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(a, _mm_setzero_si128()));\n        __m128i b_sq = _mm_madd_epi16(_mm_unpacklo_epi8(b, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n        __m128i prod = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n\n        sum_sq = _mm_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(2, 3, 0, 1)))) +\n                           _mm_cvtsi128_si64(_mm_add_epi32(_mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(1, 0, 3, 2)), _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(3, 2, 1, 0))));\n    int64_t sum_prod_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(2, 3, 0, 1)))) +\n                             _mm_cvtsi128_si64(_mm_add_epi32(_mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(1, 0, 3, 2)), _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(3, 2, 1, 0))));\n\n    for (; i < length; ++i) {\n        int64_t a = A[i];\n        int64_t b = B[i];\n        sum_sq_total += a * a + b * b;\n        sum_prod_total += a * b;\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_sq = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(a, _mm_setzero_si128()));\n        __m128i b_sq = _mm_madd_epi16(_mm_unpacklo_epi8(b, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n        __m128i prod = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n\n        sum_sq = _mm_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(1, 0, 3, 2)))) +\n                           _mm_cvtsi128_si64(_mm_add_epi32(_mm_unpackhi_epi64(sum_sq, _mm_setzero_si128()), _mm_shuffle_epi32(_mm_unpackhi_epi64(sum_sq, _mm_setzero_si128()), _MM_SHUFFLE(1, 0, 3, 2))));\n\n    int64_t sum_prod_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(1, 0, 3, 2)))) +\n                             _mm_cvtsi128_si64(_mm_add_epi32(_mm_unpackhi_epi64(sum_prod, _mm_setzero_si128()), _mm_shuffle_epi32(_mm_unpackhi_epi64(sum_prod, _mm_setzero_si128()), _MM_SHUFFLE(1, 0, 3, 2))));\n\n    for (; i < length; ++i) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - 2 * sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_sq = _mm_setzero_si128();\n    __m128i sum_prod = _mm_setzero_si128();\n\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n\n        __m128i a_sq = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(a, _mm_setzero_si128()));\n        __m128i b_sq = _mm_madd_epi16(_mm_unpacklo_epi8(b, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n        __m128i prod = _mm_madd_epi16(_mm_unpacklo_epi8(a, _mm_setzero_si128()), _mm_unpackhi_epi8(b, _mm_setzero_si128()));\n\n        sum_sq = _mm_add_epi32(sum_sq, a_sq);\n        sum_sq = _mm_add_epi32(sum_sq, b_sq);\n        sum_prod = _mm_add_epi32(sum_prod, prod);\n    }\n\n    int64_t sum_sq_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_sq, _mm_shuffle_epi32(sum_sq, _MM_SHUFFLE(2, 3, 0, 1))));\n    int64_t sum_prod_total = _mm_cvtsi128_si64(_mm_add_epi32(sum_prod, _mm_shuffle_epi32(sum_prod, _MM_SHUFFLE(2, 3, 0, 1))));\n\n    for (; i < length; ++i) {\n        sum_sq_total += A[i] * A[i] + B[i] * B[i];\n        sum_prod_total += A[i] * B[i];\n    }\n\n    return sum_sq_total - sum_prod_total;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i not_b = _mm_xor_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i not_b = _mm_xor_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i not_b = _mm_xor_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i not_b = _mm_andnot_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    if (i < length) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i not_b = _mm_xor_si128(b, _mm_set1_epi64x(-1));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_mask = _mm_and_si128(a, b);\n        __m128i neg_mask = _mm_and_si128(_mm_xor_si128(a, _mm_set1_epi8(-1)), _mm_xor_si128(b, _mm_set1_epi8(-1)));\n\n        __m128i pos_result = _mm_and_si128(pos_mask, _mm_set1_epi8(1));\n        __m128i neg_result = _mm_and_si128(neg_mask, _mm_set1_epi8(-1));\n\n        __m128i result = _mm_or_si128(pos_result, neg_result);\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_mask = _mm_and_si128(a, b);\n        __m128i neg_mask = _mm_and_si128(_mm_xor_si128(a, _mm_set1_epi8(-1)), _mm_xor_si128(b, _mm_set1_epi8(-1)));\n\n        __m128i pos_result = _mm_and_si128(pos_mask, _mm_set1_epi8(1));\n        __m128i neg_result = _mm_and_si128(neg_mask, _mm_set1_epi8(-1));\n\n        __m128i result = _mm_or_si128(pos_result, neg_result);\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i negative_one = _mm_set1_epi8(-1);\n\n    for (i = 0; i < length - 15; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_mask = _mm_and_si128(_mm_cmpgt_epi8(a, zero), _mm_cmpgt_epi8(b, zero));\n        __m128i neg_mask = _mm_and_si128(_mm_cmpgt_epi8(zero, a), _mm_cmpgt_epi8(zero, b));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(pos_mask, one),\n            _mm_andnot_si128(pos_mask, _mm_and_si128(neg_mask, negative_one))\n        );\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_mask = _mm_and_si128(a, b);\n        __m128i neg_mask = _mm_andnot_si128(a, b);\n\n        __m128i pos_result = _mm_cmpeq_epi8(pos_mask, pos_mask);\n        __m128i neg_result = _mm_cmpeq_epi8(neg_mask, neg_mask);\n\n        __m128i result = _mm_or_si128(pos_result, neg_result);\n        result = _mm_add_epi8(result, neg_result);\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i negative_one = _mm_set1_epi8(-1);\n\n    for (i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_mask = _mm_and_si128(_mm_cmpgt_epi8(a, zero), _mm_cmpgt_epi8(b, zero));\n        __m128i neg_mask = _mm_and_si128(_mm_cmpgt_epi8(zero, a), _mm_cmpgt_epi8(zero, b));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(pos_mask, one),\n            _mm_andnot_si128(neg_mask, zero)\n        );\n\n        result = _mm_or_si128(\n            result,\n            _mm_and_si128(neg_mask, negative_one)\n        );\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m128i* A_simd = (__m128i*)A;\n    __m128i* B_simd = (__m128i*)B;\n    __m128i* C_simd = (__m128i*)C;\n\n    size_t simd_length = length / 8;\n\n    for (i = 0; i < simd_length; ++i) {\n        if (i < mid / 8) {\n            __m128i a = _mm_loadu_si128(&A_simd[i]);\n            __m128i b = _mm_loadu_si128(&B_simd[i]);\n            __m128i result = _mm_xor_si128(a, b);\n            _mm_storeu_si128(&C_simd[i], result);\n        } else if (i > mid / 8) {\n            __m128i a = _mm_loadu_si128(&A_simd[i]);\n            __m128i b = _mm_loadu_si128(&B_simd[i]);\n            __m128i result = _mm_and_si128(a, b);\n            _mm_storeu_si128(&C_simd[i], result);\n        } else {\n            __m128i a = _mm_loadu_si128(&A_simd[i]);\n            _mm_storeu_si128(&C_simd[i], a);\n        }\n    }\n\n    i *= 8;\n    for (; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > mid) {\n            C[i] = A[i] & B[i];\n        } else {\n            C[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Handle elements before the midpoint\n    for (; i < mid - 7; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], xor_result);\n    }\n    for (; i < mid; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the element exactly at the midpoint\n    C[mid] = A[mid];\n\n    // Handle elements after the midpoint\n    for (; i < length - 7; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i and_result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], and_result);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SSE2 intrinsics\n    for (; i <= mid - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], xor_result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Keep the value in A at the midpoint\n    if (i < length) {\n        C[i] = A[i];\n        ++i;\n    }\n\n    // Process elements after the midpoint using SSE2 intrinsics\n    for (; i <= length - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i and_result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], and_result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SSE2 intrinsics\n    for (; i <= mid - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], xor_result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Keep the value in A at the midpoint\n    if (i < length) {\n        C[i] = A[i];\n        ++i;\n    }\n\n    // Process elements after the midpoint using SSE2 intrinsics\n    for (; i <= length - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i and_result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], and_result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using SSE2 intrinsics\n    for (; i <= mid - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], xor_result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < mid; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Keep the value in A at the midpoint\n    if (i < length) {\n        C[i] = A[i];\n        i++;\n    }\n\n    // Process elements after the midpoint using SSE2 intrinsics\n    for (; i <= length - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i and_result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], and_result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128 mA = _mm_loadu_ps(&A[i]);\n        __m128 mB = _mm_loadu_ps(&B[i]);\n        __m128 mC = _mm_loadu_ps(&C[i]);\n        __m128 mD = _mm_loadu_ps(&D[i]);\n\n        __m128i mCond1 = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i mCond2 = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128i mCond1And2 = _mm_and_si128(mCond1, mCond2);\n        __m128i mCond1AndNot2 = _mm_andnot_si128(mCond2, mCond1);\n        __m128i mNotCond1And2 = _mm_andnot_si128(mCond1, mCond2);\n        __m128i mNotCond1AndNot2 = _mm_andnot_si128(mCond1, _mm_xor_si128(mCond2, _mm_set1_epi32(-1)));\n\n        __m128 mOut = _mm_or_ps(\n            _mm_and_ps(_mm_castsi128_ps(mCond1And2), mA),\n            _mm_or_ps(\n                _mm_and_ps(_mm_castsi128_ps(mCond1AndNot2), mB),\n                _mm_or_ps(\n                    _mm_and_ps(_mm_castsi128_ps(mNotCond1And2), mC),\n                    _mm_and_ps(_mm_castsi128_ps(mNotCond1AndNot2), mD)\n                )\n            )\n        );\n\n        _mm_storeu_ps(&out[i], mOut);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128i mask1, mask2, mask3, mask4;\n    __m128 vecA, vecB, vecC, vecD, vecOut;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        // Load conditions\n        mask1 = _mm_set_epi32(\n            -(cond1[i+3] && cond2[i+3]),\n            -(cond1[i+2] && cond2[i+2]),\n            -(cond1[i+1] && cond2[i+1]),\n            -(cond1[i] && cond2[i])\n        );\n        mask2 = _mm_set_epi32(\n            -(cond1[i+3] && !cond2[i+3]),\n            -(cond1[i+2] && !cond2[i+2]),\n            -(cond1[i+1] && !cond2[i+1]),\n            -(cond1[i] && !cond2[i])\n        );\n        mask3 = _mm_set_epi32(\n            -(!cond1[i+3] && cond2[i+3]),\n            -(!cond1[i+2] && cond2[i+2]),\n            -(!cond1[i+1] && cond2[i+1]),\n            -(!cond1[i] && cond2[i])\n        );\n        mask4 = _mm_set_epi32(\n            -(!cond1[i+3] && !cond2[i+3]),\n            -(!cond1[i+2] && !cond2[i+2]),\n            -(!cond1[i+1] && !cond2[i+1]),\n            -(!cond1[i] && !cond2[i])\n        );\n\n        // Load data\n        vecA = _mm_loadu_ps(&A[i]);\n        vecB = _mm_loadu_ps(&B[i]);\n        vecC = _mm_loadu_ps(&C[i]);\n        vecD = _mm_loadu_ps(&D[i]);\n\n        // Blend data based on conditions\n        vecOut = _mm_or_ps(\n            _mm_and_ps(_mm_castsi128_ps(mask1), vecA),\n            _mm_or_ps(\n                _mm_and_ps(_mm_castsi128_ps(mask2), vecB),\n                _mm_or_ps(\n                    _mm_and_ps(_mm_castsi128_ps(mask3), vecC),\n                    _mm_and_ps(_mm_castsi128_ps(mask4), vecD)\n                )\n            )\n        );\n\n        // Store result\n        _mm_storeu_ps(&out[i], vecOut);\n    }\n\n    // Handle remaining elements if total_size is not a multiple of 4\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_mask = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_mask = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128 cond1_float = _mm_castsi128_ps(_mm_and_si128(cond1_mask, _mm_set1_epi32(1)));\n        __m128 cond2_float = _mm_castsi128_ps(_mm_and_si128(cond2_mask, _mm_set1_epi32(1)));\n\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(_mm_and_ps(cond1_float, cond2_float), a),\n            _mm_andnot_ps(cond1_float, _mm_and_ps(cond2_float, c))\n        );\n\n        result = _mm_or_ps(\n            result,\n            _mm_and_ps(_mm_andnot_ps(cond2_float, cond1_float), b)\n        );\n\n        result = _mm_or_ps(\n            result,\n            _mm_andnot_ps(_mm_or_ps(cond1_float, cond2_float), d)\n        );\n\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    for (i = 0; i <= total_size - 4; i += 4) {\n        __m128 mA = _mm_loadu_ps(&A[i]);\n        __m128 mB = _mm_loadu_ps(&B[i]);\n        __m128 mC = _mm_loadu_ps(&C[i]);\n        __m128 mD = _mm_loadu_ps(&D[i]);\n\n        __m128i mCond1 = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i mCond2 = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128i mCond1And2 = _mm_and_si128(mCond1, mCond2);\n        __m128i mCond1AndNotCond2 = _mm_andnot_si128(mCond2, mCond1);\n        __m128i mNotCond1AndCond2 = _mm_andnot_si128(mCond1, mCond2);\n\n        __m128 mOut = _mm_blendv_ps(mD, _mm_blendv_ps(mC, _mm_blendv_ps(mB, mA, _mm_castsi128_ps(mCond1And2)), _mm_castsi128_ps(mCond1AndNotCond2)), _mm_castsi128_ps(mNotCond1AndCond2));\n\n        _mm_storeu_ps(&out[i], mOut);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_mask = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_mask = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128i cond1_and_cond2 = _mm_and_si128(cond1_mask, cond2_mask);\n        __m128i cond1_and_not_cond2 = _mm_andnot_si128(cond2_mask, cond1_mask);\n        __m128i not_cond1_and_cond2 = _mm_andnot_si128(cond1_mask, cond2_mask);\n        __m128i not_cond1_and_not_cond2 = _mm_andnot_si128(cond1_mask, _mm_andnot_si128(cond1_mask, cond2_mask));\n\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(a, _mm_castsi128_ps(cond1_and_cond2)),\n            _mm_or_ps(\n                _mm_and_ps(b, _mm_castsi128_ps(cond1_and_not_cond2)),\n                _mm_or_ps(\n                    _mm_and_ps(c, _mm_castsi128_ps(not_cond1_and_cond2)),\n                    _mm_and_ps(d, _mm_castsi128_ps(not_cond1_and_not_cond2))\n                )\n            )\n        );\n\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask4 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i mask8 = _mm_set1_epi32(0x00FF00FF);\n\n    for (size_t i = 0; i < total_size; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 4), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask8), _mm_and_si128(_mm_srli_epi32(v, 8), mask8));\n        v = _mm_add_epi32(_mm_srli_epi32(v, 16), _mm_and_si128(v, _mm_set1_epi32(0x0000FFFF)));\n\n        uint32_t result[4];\n        _mm_storeu_si128((__m128i*)result, v);\n\n        for (size_t j = 0; j < 4 && (i + j) < total_size; ++j) {\n            out[i + j] = (uint8_t)result[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask4 = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask8 = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask16 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 4), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask8), _mm_and_si128(_mm_srli_epi32(v, 8), mask8));\n        v = _mm_add_epi32(_mm_and_si128(v, mask16), _mm_and_si128(_mm_srli_epi32(v, 16), mask16));\n\n        uint32_t result[4];\n        _mm_storeu_si128((__m128i*)result, v);\n\n        out[i] = (uint8_t)result[0];\n        out[i + 1] = (uint8_t)result[1];\n        out[i + 2] = (uint8_t)result[2];\n        out[i + 3] = (uint8_t)result[3];\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        v = (v & 0x55555555) + ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v & 0x0F0F0F0F) + ((v >> 4) & 0x0F0F0F0F);\n        v = (v & 0x00FF00FF) + ((v >> 8) & 0x00FF00FF);\n        v = (v & 0x0000FFFF) + ((v >> 16) & 0x0000FFFF);\n        out[i] = (uint8_t)v;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask3), _mm_and_si128(_mm_srli_epi32(v, 4), mask3));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 8), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask5), _mm_srli_epi32(_mm_and_si128(v, mask5), 16));\n\n        _mm_storeu_si128((__m128i*)&out[i], v);\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        v = (v & 0x55555555) + ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v & 0x0F0F0F0F) + ((v >> 4) & 0x0F0F0F0F);\n        v = (v & 0x00FF00FF) + ((v >> 8) & 0x00FF00FF);\n        v = (v & 0x0000FFFF) + ((v >> 16) & 0x0000FFFF);\n        out[i] = v;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask4 = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask8 = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask16 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 4), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask8), _mm_and_si128(_mm_srli_epi32(v, 8), mask8));\n        v = _mm_add_epi32(_mm_and_si128(v, mask16), _mm_and_si128(_mm_srli_epi32(v, 16), mask16));\n\n        _mm_storeu_si128((__m128i*)&out[i], v);\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        v = (v & 0x55555555) + ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v & 0x0F0F0F0F) + ((v >> 4) & 0x0F0F0F0F);\n        v = (v & 0x00FF00FF) + ((v >> 8) & 0x00FF00FF);\n        v = (v & 0x0000FFFF) + ((v >> 16) & 0x0000FFFF);\n        out[i] = v;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i;\n\n    __m128i mask1 = _mm_set1_epi32(0x55555555);\n    __m128i mask2 = _mm_set1_epi32(0x33333333);\n    __m128i mask3 = _mm_set1_epi32(0x0F0F0F0F);\n    __m128i mask4 = _mm_set1_epi32(0x00FF00FF);\n    __m128i mask5 = _mm_set1_epi32(0x0000FFFF);\n\n    for (i = 0; i < total_size - 3; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask3), _mm_and_si128(_mm_srli_epi32(v, 4), mask3));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 8), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask5), _mm_and_si128(_mm_srli_epi32(v, 16), mask5));\n\n        _mm_storeu_si128((__m128i*)&out[i], _mm_shuffle_epi8(v, _mm_setzero_si128()));\n    }\n\n    for (; i < total_size; ++i) {\n        uint32_t v = A[i];\n        v = (v & 0x55555555) + ((v >> 1) & 0x55555555);\n        v = (v & 0x33333333) + ((v >> 2) & 0x33333333);\n        v = (v & 0x0F0F0F0F) + ((v >> 4) & 0x0F0F0F0F);\n        v = (v & 0x00FF00FF) + ((v >> 8) & 0x00FF00FF);\n        v = (v & 0x0000FFFF) + ((v >> 16) & 0x0000FFFF);\n        out[i] = v;\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128i three = _mm_set1_epi32(3);\n    __m128i zero = _mm_setzero_si128();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 3; j += 4) {\n            __m128i a = _mm_loadu_si128((__m128i const*)(A + i * cols + j));\n            __m128i rem = _mm_and_si128(_mm_add_epi32(a, _mm_sub_epi32(zero, _mm_mul_epu32(a, three))), _mm_set1_epi32(2));\n            __m128i mask = _mm_cmpeq_epi32(rem, zero);\n            __m128i abs_a = _mm_abs_epi32(a);\n            __m128i result = _mm_or_si128(_mm_and_si128(mask, abs_a), _mm_andnot_si128(mask, a));\n            _mm_storeu_si128((__m128i*)(B + i * cols + j), result);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128i three = _mm_set1_epi32(3);\n    __m128i zero = _mm_setzero_si128();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 3; j += 4) {\n            __m128i a = _mm_loadu_si128((__m128i const*)(A + i * cols + j));\n            __m128i rem = _mm_and_si128(_mm_add_epi32(a, _mm_sub_epi32(zero, _mm_mul_epu32(a, three))), _mm_set1_epi32(2));\n            __m128i mask = _mm_cmpeq_epi32(rem, zero);\n            __m128i abs_a = _mm_abs_epi32(a);\n            __m128i result = _mm_or_si128(_mm_and_si128(mask, abs_a), _mm_andnot_si128(mask, a));\n            _mm_storeu_si128((__m128i*)(B + i * cols + j), result);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128i *A_vec = (__m128i*)A;\n    __m128i *B_vec = (__m128i*)B;\n    __m128i mask = _mm_set1_epi32(0x00000002);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += 4) {\n            __m128i vec = _mm_loadu_si128(&A_vec[j]);\n            __m128i rem = _mm_and_si128(_mm_add_epi32(vec, _mm_sub_epi32(_mm_srli_epi32(_mm_add_epi32(vec, three), 2), three)), mask);\n            __m128i is_div3 = _mm_cmpeq_epi32(rem, _mm_setzero_si128());\n            __m128i abs_vec = _mm_xor_si128(_mm_add_epi32(vec, sign_mask), sign_mask);\n            __m128i result = _mm_or_si128(_mm_and_si128(is_div3, abs_vec), _mm_andnot_si128(is_div3, vec));\n            _mm_storeu_si128(&B_vec[j], result);\n        }\n        A_vec += cols;\n        B_vec += cols;\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128i three = _mm_set1_epi32(3);\n    __m128i zero = _mm_setzero_si128();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols - 3; j += 4) {\n            __m128i a = _mm_loadu_si128((__m128i const*)&A[i * cols + j]);\n            __m128i rem = _mm_and_si128(_mm_add_epi32(a, _mm_sub_epi32(zero, _mm_mul_epu32(a, three))), _mm_set1_epi32(2));\n            __m128i mask = _mm_cmpeq_epi32(rem, zero);\n            __m128i abs_a = _mm_abs_epi32(a);\n            __m128i result = _mm_or_si128(_mm_and_si128(mask, abs_a), _mm_andnot_si128(mask, a));\n            _mm_storeu_si128((__m128i*)&B[i * cols + j], result);\n        }\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            B[i * cols + j] = (val % 3 == 0) ? abs(val) : val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128i *A_vec = (__m128i*)A;\n    __m128i *B_vec = (__m128i*)B;\n    __m128i three = _mm_set1_epi32(3);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols / 4; ++j) {\n            __m128i a = _mm_loadu_si128(&A_vec[j]);\n            __m128i rem = _mm_and_si128(_mm_add_epi32(a, _mm_srli_epi32(a, 31)), _mm_set1_epi32(2));\n            __m128i div3 = _mm_cmpeq_epi32(rem, zero);\n            __m128i abs_val = _mm_xor_si128(_mm_add_epi32(a, sign_mask), sign_mask);\n            __m128i result = _mm_or_si128(_mm_and_si128(div3, abs_val), _mm_andnot_si128(div3, a));\n            _mm_storeu_si128(&B_vec[j], result);\n        }\n\n        for (j = (cols / 4) * 4; j < cols; ++j) {\n            int a = A[i * cols + j];\n            B[i * cols + j] = (a % 3 == 0) ? abs(a) : a;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i rotate = _mm_set1_epi32(rotate_bits);\n    __m128i allOnes = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i srcVec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i lowBits = _mm_and_si128(srcVec, mask);\n        __m128i compare = _mm_cmpeq_epi32(lowBits, mask);\n\n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(srcVec, rotate), _mm_srli_epi32(srcVec, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m128i negated = _mm_xor_si128(srcVec, allOnes);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(compare, rotated), _mm_andnot_si128(compare, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i rotate = _mm_set1_epi32(rotate_bits);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i srcVec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i lowBits = _mm_and_si128(srcVec, mask);\n        __m128i cmpRes = _mm_cmpeq_epi32(lowBits, mask);\n\n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(srcVec, rotate), _mm_srli_epi32(srcVec, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m128i negated = _mm_xor_si128(srcVec, _mm_set1_epi32(-1));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(cmpRes, rotated), _mm_andnot_si128(cmpRes, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i rotate = _mm_set1_epi32(rotate_bits);\n    __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_chunk = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_bits = _mm_and_si128(src_chunk, mask);\n        __m128i compare = _mm_cmpeq_epi32(low_bits, mask);\n\n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(src_chunk, rotate), _mm_srli_epi32(src_chunk, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m128i negated = _mm_xor_si128(src_chunk, all_ones);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(compare, rotated), _mm_andnot_si128(compare, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i rotate = _mm_set1_epi32(rotate_bits);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i srcVec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i lowBits = _mm_and_si128(srcVec, mask);\n        __m128i compare = _mm_cmpeq_epi32(lowBits, mask);\n\n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(srcVec, rotate), _mm_srli_epi32(srcVec, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m128i negated = _mm_xor_si128(srcVec, _mm_set1_epi32(-1));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(compare, rotated), _mm_andnot_si128(compare, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i rotate = _mm_set1_epi32(rotate_bits);\n    __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_bits = _mm_and_si128(src_vec, mask);\n        __m128i compare = _mm_cmpeq_epi32(low_bits, mask);\n\n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(src_vec, rotate), _mm_srli_epi32(src_vec, _mm_sub_epi32(_mm_set1_epi32(32), rotate)));\n        __m128i negated = _mm_xor_si128(src_vec, all_ones);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(compare, rotated), _mm_andnot_si128(compare, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff);\n        __m128 cmp_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 cmp_lt = _mm_cmplt_ps(diff, _mm_sub_ps(zero, eps));\n        __m128 cmp_eq = _mm_cmple_ps(abs_diff, eps);\n\n        __m128 result = _mm_or_ps(_mm_and_ps(cmp_gt, one), _mm_andnot_ps(cmp_gt, _mm_or_ps(_mm_and_ps(cmp_lt, neg_one), _mm_and_ps(cmp_eq, zero))));\n\n        _mm_storeu_ps((float*)&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 neg_eps = _mm_set1_ps(-epsilon);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 cmp_lt = _mm_cmplt_ps(diff, neg_eps);\n\n        __m128 result = _mm_or_ps(_mm_and_ps(cmp_gt, _mm_set1_ps(1.0f)),\n                                  _mm_and_ps(cmp_lt, _mm_set1_ps(-1.0f)));\n\n        __m128 mask = _mm_or_ps(cmp_gt, cmp_lt);\n        result = _mm_andnot_ps(mask, zero);\n\n        __m128i int_result = _mm_cvtps_epi32(result);\n        _mm_storeu_si128((__m128i*)&out[i], int_result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 neg_eps = _mm_set1_ps(-epsilon);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 cmp_lt = _mm_cmplt_ps(diff, neg_eps);\n\n        __m128 result = _mm_or_ps(_mm_and_ps(cmp_gt, _mm_set1_ps(1.0f)), _mm_and_ps(cmp_lt, _mm_set1_ps(-1.0f)));\n\n        __m128i int_result = _mm_cvtps_epi32(result);\n        _mm_storeu_si128((__m128i*)&out[i], int_result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 neg_eps = _mm_set1_ps(-epsilon);\n    __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 cmp_lt = _mm_cmplt_ps(diff, neg_eps);\n\n        __m128i result = _mm_setzero_si128();\n        result = _mm_or_si128(result, _mm_and_si128(_mm_cvtps_epi30(cmp_gt), _mm_set1_epi32(1)));\n        result = _mm_or_si128(result, _mm_and_si128(_mm_cvtps_epi30(cmp_lt), _mm_set1_epi32(-1)));\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 neg_eps = _mm_set1_ps(-epsilon);\n    __m128 zero = _mm_setzero_ps();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n\n        __m128 cmp_gt = _mm_cmpgt_ps(diff, eps);\n        __m128 cmp_lt = _mm_cmplt_ps(diff, neg_eps);\n\n        __m128 result = _mm_add_ps(_mm_and_ps(cmp_gt, _mm_set1_ps(1.0f)), _mm_and_ps(cmp_lt, _mm_set1_ps(-1.0f)));\n\n        __m128i result_int = _mm_cvtps_epi32(result);\n        result_int = _mm_packs_epi32(result_int, result_int);\n        result_int = _mm_packs_epi16(result_int, result_int);\n\n        _mm_storel_epi64((__m128i*)&out[i], result_int);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps(INT32_MAX);\n    const __m128 float_min = _mm_set1_ps(INT32_MIN);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 float_vals = _mm_loadu_ps(&A[i]);\n        __m128i int_vals = _mm_cvtps_epi32(float_vals);\n\n        __m128 mask_max = _mm_cmpgt_ps(float_vals, float_max);\n        __m128 mask_min = _mm_cmplt_ps(float_vals, float_min);\n\n        __m128i mask_max_i = _mm_castps_si128(mask_max);\n        __m128i mask_min_i = _mm_castps_si128(mask_min);\n\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_max_i, int_vals), _mm_and_si128(mask_max_i, int32_max));\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_min_i, int_vals), _mm_and_si128(mask_min_i, int32_min));\n\n        _mm_storeu_si128((__m128i*)&B[i], int_vals);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int32_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int32_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps(INT32_MAX);\n    const __m128 float_min = _mm_set1_ps(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 float_vals = _mm_loadu_ps(&A[i]);\n\n        __m128 max_vals = _mm_min_ps(float_vals, float_max);\n        __m128 min_vals = _mm_max_ps(max_vals, float_min);\n\n        __m128i int_vals = _mm_cvtps_epi32(min_vals);\n\n        __m128i max_int_vals = _mm_min_epi32(int_vals, int32_max);\n        __m128i min_int_vals = _mm_max_epi32(max_int_vals, int32_min);\n\n        _mm_storeu_si128((__m128i*)&B[i], min_int_vals);\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128 float_max = _mm_set1_ps((float)INT32_MAX);\n    const __m128 float_min = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 float_vals = _mm_loadu_ps(&A[i]);\n\n        __m128 cmp_max = _mm_cmpgt_ps(float_vals, float_max);\n        __m128 cmp_min = _mm_cmplt_ps(float_vals, float_min);\n\n        float_vals = _mm_max_ps(float_vals, float_min);\n        float_vals = _mm_min_ps(float_vals, float_max);\n\n        __m128i int_vals = _mm_cvtps_epi32(float_vals);\n\n        __m128i mask_max = _mm_castps_si128(cmp_max);\n        __m128i mask_min = _mm_castps_si128(cmp_min);\n\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_max, int_vals), _mm_and_si128(mask_max, int_max));\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_min, int_vals), _mm_and_si128(mask_min, int_min));\n\n        _mm_storeu_si128((__m128i*)&B[i], int_vals);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    const __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    const __m128 float_max = _mm_set1_ps(INT32_MAX);\n    const __m128 float_min = _mm_set1_ps(INT32_MIN);\n\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 float_vals = _mm_loadu_ps(&A[i]);\n\n        __m128 cmp_max = _mm_cmpgt_ps(float_vals, float_max);\n        __m128 cmp_min = _mm_cmplt_ps(float_vals, float_min);\n\n        float_vals = _mm_min_ps(float_vals, _mm_and_ps(cmp_max, float_max));\n        float_vals = _mm_max_ps(float_vals, _mm_and_ps(cmp_min, float_min));\n\n        __m128i int_vals = _mm_cvtps_epi32(float_vals);\n\n        int_vals = _mm_min_epi32(int_vals, int_max);\n        int_vals = _mm_max_epi32(int_vals, int_min);\n\n        _mm_storeu_si128((__m128i*)&B[i], int_vals);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m128i int_max = _mm_set1_epi32(INT32_MAX);\n    __m128i int_min = _mm_set1_epi32(INT32_MIN);\n    __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 float_vals = _mm_loadu_ps(&A[i]);\n        __m128i int_vals = _mm_cvtps_epi32(float_vals);\n\n        __m128 cmp_max = _mm_cmpgt_ps(float_vals, _mm_cvtepi32_ps(int_max));\n        __m128 cmp_min = _mm_cmplt_ps(float_vals, _mm_cvtepi32_ps(int_min));\n\n        __m128i mask_max = _mm_castps_si128(cmp_max);\n        __m128i mask_min = _mm_castps_si128(cmp_min);\n\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_max, int_vals), _mm_and_si128(mask_max, int_max));\n        int_vals = _mm_or_si128(_mm_andnot_si128(mask_min, int_vals), _mm_and_si128(mask_min, int_min));\n\n        _mm_storeu_si128((__m128i*)&B[i], int_vals);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vector = _mm_set1_ps((float)scale);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i int_vector = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_vector = _mm_cvtepi32_ps(int_vector);\n        float_vector = _mm_mul_ps(float_vector, scale_vector);\n        _mm_storeu_ps(&B[i], float_vector);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vector = _mm_set1_ps((float)scale);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i int_vector = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_vector = _mm_cvtepi32_ps(int_vector);\n        float_vector = _mm_mul_ps(float_vector, scale_vector);\n        _mm_storeu_ps(&B[i], float_vector);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vector = _mm_set1_ps((float)scale);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i int_vector = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_vector = _mm_cvtepi32_ps(int_vector);\n        float_vector = _mm_mul_ps(float_vector, scale_vector);\n        _mm_storeu_ps(&B[i], float_vector);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vector = _mm_set1_ps((float)scale);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i int_vector = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_vector = _mm_cvtepi32_ps(int_vector);\n        float_vector = _mm_mul_ps(float_vector, scale_vector);\n        _mm_storeu_ps(&B[i], float_vector);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128 scale_vector = _mm_set1_ps((float)scale);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i int_vector = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_vector = _mm_cvtepi32_ps(int_vector);\n        float_vector = _mm_mul_ps(float_vector, scale_vector);\n        _mm_storeu_ps(&B[i], float_vector);\n    }\n\n    for (size_t i = size - (size % 4); i < size; ++i) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vc = _mm_div_ps(va, vb);\n        __m128i vi = _mm_cvtps_epi32(vc);\n        _mm_storeu_si128((__m128i*)&C[i], vi);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i c = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128i rounded = _mm_cvtps_epi32(div);\n        _mm_storeu_si128((__m128i*)&C[i], rounded);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)((A[i] / B[i]) + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_val_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_val_d = _mm_set1_pd(-FLT_MAX);\n    __m128 max_val_f = _mm_set1_ps(FLT_MAX);\n    __m128 min_val_f = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i < size - 1; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n\n        __m128d max_mask = _mm_cmpgt_pd(a, max_val_d);\n        __m128d min_mask = _mm_cmplt_pd(a, min_val_d);\n\n        a = _mm_max_pd(a, min_val_d);\n        a = _mm_min_pd(a, max_val_d);\n\n        __m128 f = _mm_cvtpd_ps(a);\n\n        f = _mm_blendv_ps(f, max_val_f, _mm_castpd_ps(max_mask));\n        f = _mm_blendv_ps(f, min_val_f, _mm_castpd_ps(min_mask));\n\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    if (size % 2 != 0) {\n        __m128d a = _mm_load_sd(&A[size - 1]);\n\n        __m128d max_mask = _mm_cmpgt_sd(a, max_val_d);\n        __m128d min_mask = _mm_cmplt_sd(a, min_val_d);\n\n        a = _mm_max_sd(a, min_val_d);\n        a = _mm_min_sd(a, max_val_d);\n\n        __m128 f = _mm_cvtsd_ss(_mm_setzero_ps(), a);\n\n        f = _mm_blendv_ps(f, max_val_f, _mm_castpd_ps(max_mask));\n        f = _mm_blendv_ps(f, min_val_f, _mm_castpd_ps(min_mask));\n\n        _mm_store_ss(&B[size - 1], f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_val_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_val_d = _mm_set1_pd(-FLT_MAX);\n    __m128 max_val_f = _mm_set1_ps(FLT_MAX);\n    __m128 min_val_f = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i < size - 1; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n\n        __m128d max_cmp = _mm_cmpgt_pd(a, max_val_d);\n        __m128d min_cmp = _mm_cmplt_pd(a, min_val_d);\n\n        a = _mm_blendv_pd(a, max_val_d, max_cmp);\n        a = _mm_blendv_pd(a, min_val_d, min_cmp);\n\n        __m128 f = _mm_cvtpd_ps(a);\n\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    if (size % 2 != 0) {\n        double last_val = A[size - 1];\n        if (last_val > FLT_MAX) {\n            last_val = FLT_MAX;\n        } else if (last_val < -FLT_MAX) {\n            last_val = -FLT_MAX;\n        }\n        B[size - 1] = (float)last_val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_val_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_val_d = _mm_set1_pd(-FLT_MAX);\n    __m128 max_val_f = _mm_set1_ps(FLT_MAX);\n    __m128 min_val_f = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i < size - 1; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n\n        __m128d max_cmp = _mm_cmpgt_pd(a, max_val_d);\n        __m128d min_cmp = _mm_cmplt_pd(a, min_val_d);\n\n        a = _mm_blendv_pd(a, max_val_d, max_cmp);\n        a = _mm_blendv_pd(a, min_val_d, min_cmp);\n\n        __m128 a_f = _mm_cvtpd_ps(a);\n\n        _mm_storeu_ps(&B[i], a_f);\n    }\n\n    if (size % 2 != 0) {\n        __m128d a = _mm_load_sd(&A[size - 1]);\n\n        __m128d max_cmp = _mm_cmpgt_pd(a, max_val_d);\n        __m128d min_cmp = _mm_cmplt_pd(a, min_val_d);\n\n        a = _mm_blendv_pd(a, max_val_d, max_cmp);\n        a = _mm_blendv_pd(a, min_val_d, min_cmp);\n\n        __m128 a_f = _mm_cvtpd_ps(a);\n\n        _mm_store_ss(&B[size - 1], a_f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_val = _mm_set1_pd(FLT_MAX);\n    __m128d min_val = _mm_set1_pd(-FLT_MAX);\n\n    size_t i;\n    for (i = 0; i <= size - 2; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n        a = _mm_min_pd(a, max_val);\n        a = _mm_max_pd(a, min_val);\n\n        __m128 f = _mm_cvtpd_ps(a);\n        _mm_storeu_ps(&B[i], f);\n    }\n\n    if (i < size) {\n        __m128d a = _mm_load_sd(&A[i]);\n        a = _mm_min_sd(a, max_val);\n        a = _mm_max_sd(a, min_val);\n\n        __m128 f = _mm_cvtsd_ss(f, a);\n        _mm_store_ss(&B[i], f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m128d max_val_d = _mm_set1_pd(FLT_MAX);\n    __m128d min_val_d = _mm_set1_pd(-FLT_MAX);\n    __m128 max_val_f = _mm_set1_ps(FLT_MAX);\n    __m128 min_val_f = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i < size - 1; i += 2) {\n        __m128d a = _mm_loadu_pd(&A[i]);\n\n        __m128d max_cmp = _mm_cmpgt_pd(a, max_val_d);\n        __m128d min_cmp = _mm_cmplt_pd(a, min_val_d);\n\n        a = _mm_blendv_pd(a, max_val_d, max_cmp);\n        a = _mm_blendv_pd(a, min_val_d, min_cmp);\n\n        __m128 a_f = _mm_cvtpd_ps(a);\n\n        max_cmp = _mm_cmpgt_ps(a_f, max_val_f);\n        min_cmp = _mm_cmplt_ps(a_f, min_val_f);\n\n        a_f = _mm_blendv_ps(a_f, max_val_f, max_cmp);\n        a_f = _mm_blendv_ps(a_f, min_val_f, min_cmp);\n\n        _mm_storeu_ps(&B[i], a_f);\n    }\n\n    if (size % 2 != 0) {\n        double last_val = A[size - 1];\n        if (last_val > FLT_MAX) {\n            B[size - 1] = FLT_MAX;\n        } else if (last_val < -FLT_MAX) {\n            B[size - 1] = -FLT_MAX;\n        } else {\n            B[size - 1] = (float)last_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_lo = _mm_unpacklo_epi16(a, _mm_setzero_si128());\n        __m128i b_hi = _mm_unpackhi_epi16(a, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)&B[i], b_lo);\n        _mm_storeu_si128((__m128i*)&B[i + 4], b_hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i lo = _mm_unpacklo_epi16(v, _mm_setzero_si128());\n        __m128i hi = _mm_unpackhi_epi16(v, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)(B + i), lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_lo = _mm_unpacklo_epi16(a, _mm_setzero_si128());\n        __m128i b_hi = _mm_unpackhi_epi16(a, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)&B[i], b_lo);\n        _mm_storeu_si128((__m128i*)&B[i + 4], b_hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_lo = _mm_unpacklo_epi16(a, _mm_setzero_si128());\n        __m128i b_hi = _mm_unpackhi_epi16(a, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)&B[i], b_lo);\n        _mm_storeu_si128((__m128i*)&B[i + 4], b_hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b_lo = _mm_unpacklo_epi16(a, _mm_setzero_si128());\n        __m128i b_hi = _mm_unpackhi_epi16(a, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)(B + i), b_lo);\n        _mm_storeu_si128((__m128i*)(B + i + 4), b_hi);\n    }\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_v = _mm_set1_ps(min_val);\n    __m128 max_val_v = _mm_set1_ps(max_val);\n    __m128 range_v = _mm_sub_ps(max_val_v, min_val_v);\n    __m128 zero_v = _mm_setzero_ps();\n    __m128 one_v = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 A_v = _mm_loadu_ps(&A[i]);\n        __m128i control_v = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 control_mask = _mm_castsi128_ps(_mm_cmpgt_epi32(control_v, _mm_setzero_si128()));\n\n        __m128 norm_v = _mm_sub_ps(A_v, min_val_v);\n        norm_v = _mm_div_ps(norm_v, range_v);\n        norm_v = _mm_max_ps(norm_v, zero_v);\n        norm_v = _mm_min_ps(norm_v, one_v);\n\n        __m128 result_v = _mm_or_ps(_mm_and_ps(control_mask, norm_v), _mm_andnot_ps(control_mask, A_v));\n\n        _mm_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_v = _mm_set1_ps(min_val);\n    __m128 max_val_v = _mm_set1_ps(max_val);\n    __m128 range_v = _mm_sub_ps(max_val_v, min_val_v);\n    __m128 zero_v = _mm_setzero_ps();\n    __m128 one_v = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 A_v = _mm_loadu_ps(&A[i]);\n        __m128i control_v = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 control_mask = _mm_castsi128_ps(_mm_cmpgt_epi32(control_v, _mm_setzero_si128()));\n\n        __m128 norm_v = _mm_sub_ps(A_v, min_val_v);\n        norm_v = _mm_div_ps(norm_v, range_v);\n        norm_v = _mm_max_ps(norm_v, zero_v);\n        norm_v = _mm_min_ps(norm_v, one_v);\n\n        __m128 result_v = _mm_blendv_ps(A_v, norm_v, control_mask);\n        _mm_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 minVal = _mm_set1_ps(min_val);\n    __m128 maxVal = _mm_set1_ps(max_val);\n    __m128 range = _mm_sub_ps(maxVal, minVal);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128i ctrl = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 ctrl_mask = _mm_castsi128_ps(_mm_cmpgt_epi32(ctrl, _mm_setzero_si128()));\n\n        __m128 norm = _mm_sub_ps(a, minVal);\n        norm = _mm_div_ps(norm, range);\n        norm = _mm_max_ps(norm, zero);\n        norm = _mm_min_ps(norm, one);\n\n        __m128 result = _mm_blendv_ps(a, norm, ctrl_mask);\n        _mm_storeu_ps(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_v = _mm_set1_ps(min_val);\n    __m128 max_val_v = _mm_set1_ps(max_val);\n    __m128 range_v = _mm_sub_ps(max_val_v, min_val_v);\n    __m128 zero_v = _mm_setzero_ps();\n    __m128 one_v = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 A_v = _mm_loadu_ps(&A[i]);\n        __m128i control_v = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128i mask = _mm_cmpgt_epi32(control_v, _mm_setzero_si128());\n        __m128 mask_v = _mm_castsi128_ps(mask);\n\n        __m128 norm_v = _mm_sub_ps(A_v, min_val_v);\n        norm_v = _mm_div_ps(norm_v, range_v);\n        norm_v = _mm_max_ps(norm_v, zero_v);\n        norm_v = _mm_min_ps(norm_v, one_v);\n\n        __m128 result_v = _mm_or_ps(_mm_and_ps(mask_v, norm_v), _mm_andnot_ps(mask_v, A_v));\n\n        _mm_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 min_val_v = _mm_set1_ps(min_val);\n    __m128 max_val_v = _mm_set1_ps(max_val);\n    __m128 range_v = _mm_sub_ps(max_val_v, min_val_v);\n    __m128 zero_v = _mm_set1_ps(0.0f);\n    __m128 one_v = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 A_v = _mm_loadu_ps(&A[i]);\n        __m128i control_v = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 control_f_v = _mm_cvtepi32_ps(control_v);\n        __m128 mask_v = _mm_cmpgt_ps(control_f_v, _mm_setzero_ps());\n\n        __m128 norm_v = _mm_sub_ps(A_v, min_val_v);\n        norm_v = _mm_div_ps(norm_v, range_v);\n        norm_v = _mm_max_ps(norm_v, zero_v);\n        norm_v = _mm_min_ps(norm_v, one_v);\n\n        __m128 result_v = _mm_blendv_ps(A_v, norm_v, mask_v);\n        _mm_storeu_ps(&B[i], result_v);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vHigh = _mm_srli_epi32(vA, 16);\n        __m128 vFloat = _mm_cvtepi32_ps(vHigh);\n        __m128 vSqrt = _mm_sqrt_ps(vFloat);\n        _mm_storeu_ps(&B[i], vSqrt);\n    }\n    for (; i < size; ++i) {\n        uint32_t high = A[i] >> 16;\n        float f = (float)high;\n        B[i] = sqrtf(f);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i highBits = _mm_srli_epi32(vA, 16);\n        __m128 vB = _mm_cvtepi32_ps(highBits);\n        vB = _mm_sqrt_ps(vB);\n        _mm_storeu_ps(&B[i], vB);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t value = A[i];\n        float highBits = (value >> 16);\n        B[i] = sqrtf(highBits);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i high16 = _mm_srli_epi32(vA, 16);\n        __m128 vB = _mm_cvtepi32_ps(high16);\n        vB = _mm_sqrt_ps(vB);\n        _mm_storeu_ps(&B[i], vB);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t value = A[i];\n        float high16 = (float)((value >> 16) & 0xFFFF);\n        B[i] = sqrtf(high16);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i high16 = _mm_srli_epi32(vA, 16);\n        __m128 vB = _mm_cvtepi32_ps(high16);\n        vB = _mm_sqrt_ps(vB);\n        _mm_storeu_ps(&B[i], vB);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t value = A[i] >> 16;\n        float floatValue = (float)value;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i high16 = _mm_srli_epi32(vA, 16);\n        __m128 vB = _mm_cvtepi32_ps(high16);\n        vB = _mm_sqrt_ps(vB);\n        _mm_storeu_ps(&B[i], vB);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t value = A[i] >> 16;\n        float floatValue = (float)value;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_cvtsi32_si128(A[i]);\n        float f = _mm_cvtss_f32(_mm_castsi128_ps(v));\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_cvtsi32_si128(A[i]);\n        float f = _mm_cvtss_f32(_mm_castsi128_ps(v));\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i + 3 < size; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_cvtsi32_si128(A[i]);\n        float f = _mm_cvtss_f32(_mm_castsi128_ps(v));\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(B + i, f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_cvtsi32_si128(A[i]);\n        float f = _mm_cvtss_f32(_mm_castsi128_ps(v));\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 f = _mm_castsi128_ps(v);\n        _mm_storeu_ps(&B[i], f);\n    }\n    for (; i < size; ++i) {\n        __m128i v = _mm_cvtsi32_si128(A[i]);\n        float f = _mm_cvtsi128_ss(v);\n        B[i] = f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i* A_vec = (__m128i*)A;\n    __m128* diff_vec = (__m128*)diff;\n\n    __m128i prev_vec = _mm_set1_epi32(0);\n    __m128 zero_vec = _mm_set1_ps(0.0f);\n\n    diff[0] = 0.0f;\n\n    for (size_t i = 1; i < size; i += 4) {\n        __m128i curr_vec = _mm_loadu_si128(&A_vec[i]);\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        __m128i diff_int_vec = _mm_sub_epi32(curr_vec, prev_vec);\n        __m128 diff_float_vec = _mm_cvtepi32_ps(diff_int_vec);\n\n        __m128 cond_mask = _mm_castsi128_ps(_mm_srai_epi32(cond_vec, 31));\n        __m128 result_vec = _mm_andnot_ps(cond_mask, diff_float_vec);\n        result_vec = _mm_or_ps(result_vec, _mm_and_ps(cond_mask, zero_vec));\n\n        _mm_storeu_ps(&diff[i], result_vec);\n\n        prev_vec = curr_vec;\n    }\n\n    for (size_t i = size - (size % 4); i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i *A_vec = (__m128i*)A;\n    __m128 *diff_vec = (__m128*)diff;\n\n    // Set the first element to 0.0f\n    diff[0] = 0.0f;\n\n    // Initialize previous value\n    __m128i prev = _mm_set1_epi32(A[0]);\n\n    for (size_t i = 1; i < size - 3; i += 4) {\n        // Load the current values from A\n        __m128i curr = _mm_loadu_si128(&A_vec[i]);\n\n        // Load the condition values\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Compute the difference\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n\n        // Convert the difference to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Mask the results based on the condition\n        __m128 mask = _mm_castsi128_ps(_mm_and_si128(cond_vec, _mm_set1_epi32(0xFFFFFFFF)));\n        __m128 zero = _mm_setzero_ps();\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, diff_float), _mm_andnot_ps(mask, zero));\n\n        // Store the result in diff\n        _mm_storeu_ps(&diff_vec[i], result);\n\n        // Update the previous value\n        prev = curr;\n    }\n\n    // Handle the remaining elements\n    for (size_t i = size - (size % 4); i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i* A_vec = (__m128i*)A;\n    __m128* diff_vec = (__m128*)diff;\n\n    // Set the first element to 0.0f\n    diff[0] = 0.0f;\n\n    // Initialize previous element\n    __m128i prev = _mm_set1_epi32(A[0]);\n\n    for (size_t i = 1; i < size - 3; i += 4) {\n        // Load 4 elements from A\n        __m128i curr = _mm_loadu_si128(&A_vec[i]);\n\n        // Load 4 condition values and convert bool to int\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i cond_mask = _mm_cmpeq_epi32(cond_vec, _mm_set1_epi32(1));\n\n        // Calculate difference\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n\n        // Convert difference to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Mask the results based on condition\n        __m128 mask = _mm_castsi128_ps(cond_mask);\n        __m128 zero = _mm_set1_ps(0.0f);\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, diff_float), _mm_andnot_ps(mask, zero));\n\n        // Store the result\n        _mm_storeu_ps(&diff[i], result);\n\n        // Update previous element\n        prev = curr;\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size - 3) + 1; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i* A_vec = (__m128i*)A;\n    __m128* diff_vec = (__m128*)diff;\n\n    // Set the first element to 0.0f\n    diff[0] = 0.0f;\n\n    // Initialize previous value\n    __m128i prev = _mm_set1_epi32(A[0]);\n\n    for (size_t i = 1; i < size; i += 4) {\n        // Load the current values of A and cond\n        __m128i current = _mm_loadu_si128(&A_vec[i]);\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Calculate the difference\n        __m128i diff_int = _mm_sub_epi32(current, prev);\n\n        // Convert the difference to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Create a mask for the condition\n        __m128 zero_mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, _mm_setzero_si128()));\n\n        // Set diff_float to 0.0f where cond is false\n        __m128 result = _mm_andnot_ps(zero_mask, diff_float);\n\n        // Store the result in diff\n        _mm_storeu_ps(&diff[i], result);\n\n        // Update previous value\n        prev = current;\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (size_t i = (size & ~(size_t)3); i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i* A_vec = (__m128i*)A;\n    __m128* diff_vec = (__m128*)diff;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128 zerof = _mm_setzero_ps();\n\n    diff[0] = 0.0f;\n\n    for (size_t i = 1; i < size; i += 4) {\n        __m128i A_prev = _mm_loadu_si128(A_vec);\n        __m128i A_curr = _mm_loadu_si128(A_vec + 1);\n\n        __m128i cond_mask = _mm_loadu_si128((__m128i*)(cond + i));\n        __m128i diff_int = _mm_sub_epi32(A_curr, A_prev);\n\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(cond_mask, zero));\n        diff_float = _mm_andnot_ps(mask, diff_float);\n\n        _mm_storeu_ps(diff + i, diff_float);\n\n        A_vec++;\n    }\n\n    for (size_t i = size; i < size + 3; ++i) {\n        if (i < size) {\n            diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m128i mask = _mm_set1_epi64x(0x00000001FFFFFFFF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vec = _mm_unpacklo_epi32(src_vec, src_vec);\n        dst_vec = _mm_or_si128(dst_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], dst_vec);\n\n        dst_vec = _mm_unpackhi_epi32(src_vec, src_vec);\n        dst_vec = _mm_or_si128(dst_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i + 2], dst_vec);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0x0000000100000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m128i mask = _mm_set1_epi64x(0x0000000100000000);\n\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_val = _mm_unpacklo_epi32(src_val, src_val);\n        dst_val = _mm_or_si128(dst_val, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], dst_val);\n    }\n\n    if (i < length) {\n        __m128i src_val = _mm_loadl_epi64((__m128i*)&src[i]);\n        __m128i dst_val = _mm_unpacklo_epi32(src_val, src_val);\n        dst_val = _mm_or_si128(dst_val, mask);\n        _mm_storel_epi64((__m128i*)&dst[i], dst_val);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m128i high32bits = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vals = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vals_low = _mm_unpacklo_epi32(src_vals, high32bits);\n        __m128i dst_vals_high = _mm_unpackhi_epi32(src_vals, high32bits);\n\n        _mm_storeu_si128((__m128i*)&dst[i * 2], dst_vals_low);\n        _mm_storeu_si128((__m128i*)&dst[i * 2 + 2], dst_vals_high);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m128i mask = _mm_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i src_vals = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vals_low = _mm_cvtepu32_epi64(_mm_castsi128_si64(src_vals));\n        __m128i dst_vals_high = _mm_cvtepu32_epi64(_mm_srli_si128(src_vals, 8));\n\n        dst_vals_low = _mm_or_si128(dst_vals_low, mask);\n        dst_vals_high = _mm_or_si128(dst_vals_high, mask);\n\n        _mm_storeu_si128((__m128i*)&dst[i], dst_vals_low);\n        _mm_storeu_si128((__m128i*)&dst[i + 2], dst_vals_high);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i;\n    __m128i mask = _mm_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i src_val = _mm_loadl_epi64((__m128i*)&src[i]);\n        __m128i dst_val = _mm_unpacklo_epi32(src_val, src_val);\n        dst_val = _mm_or_si128(dst_val, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], dst_val);\n    }\n\n    if (i < length) {\n        __m128i src_val = _mm_cvtsi32_si128(src[i]);\n        __m128i dst_val = _mm_unpacklo_epi32(src_val, src_val);\n        dst_val = _mm_or_si128(dst_val, mask);\n        _mm_storel_epi64((__m128i*)&dst[i], dst_val);\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_set_epi16(index[i+3], index[i+2], index[i+1], index[i]);\n        __m128d vec1 = _mm_set_pd(vec[idx[i+1]], vec[idx[i]]);\n        __m128d vec2 = _mm_set_pd(vec[idx[i+3]], vec[idx[i+2]]);\n        sum1 = _mm_add_pd(sum1, vec1);\n        sum2 = _mm_add_pd(sum2, vec2);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum1 = _mm_add_pd(sum1, sum3);\n    sum1 = _mm_add_pd(sum1, sum4);\n\n    double result[2];\n    _mm_storeu_pd(result, sum1);\n\n    double final_sum = result[0] + result[1];\n\n    for (; i < length; ++i) {\n        final_sum += vec[index[i]];\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_set_epi16(index[i+3], index[i+2], index[i+1], index[i]);\n        __m128d vec1 = _mm_set_pd(vec[idx[3]], vec[idx[1]]);\n        __m128d vec2 = _mm_set_pd(vec[idx[2]], vec[idx[0]]);\n        sum1 = _mm_add_pd(sum1, vec1);\n        sum2 = _mm_add_pd(sum2, vec2);\n\n        idx = _mm_set_epi16(index[i+7], index[i+6], index[i+5], index[i+4]);\n        __m128d vec3 = _mm_set_pd(vec[idx[3]], vec[idx[1]]);\n        __m128d vec4 = _mm_set_pd(vec[idx[2]], vec[idx[0]]);\n        sum3 = _mm_add_pd(sum3, vec3);\n        sum4 = _mm_add_pd(sum4, vec4);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum3 = _mm_add_pd(sum3, sum4);\n    sum1 = _mm_add_pd(sum1, sum3);\n\n    double result[2];\n    _mm_storeu_pd(result, sum1);\n\n    double final_sum = result[0] + result[1];\n\n    for (; i < length; ++i) {\n        final_sum += vec[index[i]];\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128d vec1 = _mm_set_pd(vec[index[i+1]], vec[index[i]]);\n        __m128d vec2 = _mm_set_pd(vec[index[i+3]], vec[index[i+2]]);\n        sum1 = _mm_add_pd(sum1, vec1);\n        sum2 = _mm_add_pd(sum2, vec2);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum3 = _mm_add_pd(sum3, sum4);\n    sum1 = _mm_add_pd(sum1, sum3);\n\n    double result[2];\n    _mm_storeu_pd(result, sum1);\n\n    double final_sum = result[0] + result[1];\n\n    for (; i < length; ++i) {\n        final_sum += vec[index[i]];\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128i idx = _mm_set_epi16(index[i+3], index[i+2], index[i+1], index[i]);\n        __m128d vec1 = _mm_set_pd(vec[index[i+1]], vec[index[i]]);\n        __m128d vec2 = _mm_set_pd(vec[index[i+3]], vec[index[i+2]]);\n        sum1 = _mm_add_pd(sum1, vec1);\n        sum2 = _mm_add_pd(sum2, vec2);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum3 = _mm_add_pd(sum3, sum4);\n    sum1 = _mm_add_pd(sum1, sum3);\n\n    double result[2];\n    _mm_storeu_pd(result, sum1);\n\n    double total = result[0] + result[1];\n\n    for (; i < length; ++i) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum1 = _mm_setzero_pd();\n    __m128d sum2 = _mm_setzero_pd();\n    __m128d sum3 = _mm_setzero_pd();\n    __m128d sum4 = _mm_setzero_pd();\n\n    size_t i;\n    for (i = 0; i <= length - 4; i += 4) {\n        __m128d vec1 = _mm_set_pd(vec[index[i+1]], vec[index[i]]);\n        __m128d vec2 = _mm_set_pd(vec[index[i+3]], vec[index[i+2]]);\n        sum1 = _mm_add_pd(sum1, vec1);\n        sum2 = _mm_add_pd(sum2, vec2);\n    }\n\n    sum1 = _mm_add_pd(sum1, sum2);\n    sum3 = _mm_add_pd(sum3, sum4);\n    sum1 = _mm_add_pd(sum1, sum3);\n\n    double result[2];\n    _mm_storeu_pd(result, sum1);\n\n    double final_sum = result[0] + result[1];\n\n    for (; i < length; ++i) {\n        final_sum += vec[index[i]];\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m128d input_val = _mm_load1_pd(&input[input_index]);\n                    __m128d kernel_val = _mm_load1_pd(&kernel[kernel_index]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d zero = _mm_setzero_pd();\n            __m128d relu_mask = _mm_cmpgt_pd(sum, zero);\n            sum = _mm_and_pd(sum, relu_mask);\n\n            _mm_storel_pd(&output[i * output_stride + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m128d input_val = _mm_load1_pd(&input[input_index]);\n                    __m128d kernel_val = _mm_load1_pd(&kernel[kernel_index]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d result = _mm_hadd_pd(sum, sum);\n            double temp[2];\n            _mm_storeu_pd(temp, result);\n            double final_result = temp[0] + temp[1];\n            final_result = fmax(0, final_result);\n            output[i * output_stride + j] = final_result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    double in_val = input[(i + k) * input_stride + (j + l)];\n                    double ker_val = kernel[k * kernel_size + l];\n                    __m128d in_vec = _mm_set1_pd(in_val);\n                    __m128d ker_vec = _mm_set1_pd(ker_val);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(in_vec, ker_vec));\n                }\n            }\n\n            __m128d zero = _mm_setzero_pd();\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_storel_pd(&output[i * output_stride + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_size + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m128d input_val = _mm_load1_pd(&input[input_index]);\n                    __m128d kernel_val = _mm_load1_pd(&kernel[kernel_index]);\n\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_storel_pd(&output[i * output_size + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double* output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    size_t output_stride = output_size;\n    size_t input_stride = input_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < kernel_size; ++k) {\n                for (size_t l = 0; l < kernel_size; ++l) {\n                    size_t input_index = (i + k) * input_stride + (j + l);\n                    size_t kernel_index = k * kernel_size + l;\n\n                    __m128d input_val = _mm_load1_pd(&input[input_index]);\n                    __m128d kernel_val = _mm_load1_pd(&kernel[kernel_index]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d result = _mm_max_pd(sum, _mm_setzero_pd());\n            _mm_storel_pd(&output[i * output_stride + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    int n = numbers.size();\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (int i = 0; i < n; ++i) {\n        __m128d num1 = _mm_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < n; ++j) {\n            __m128d num2 = _mm_set1_pd(numbers[j]);\n            __m128d diff = _mm_sub_pd(num1, num2);\n            __m128d abs_diff = _mm_andnot_pd(_mm_cmplt_pd(diff, zero_vec), diff);\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n            int mask = _mm_movemask_pd(cmp);\n\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <emmintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    int n = numbers.size();\n    __m128d threshold_v = _mm_set1_pd(threshold);\n    __m128d zero_v = _mm_setzero_pd();\n\n    for (int i = 0; i < n; ++i) {\n        __m128d num_v = _mm_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < n; ++j) {\n            __m128d diff = _mm_sub_pd(num_v, _mm_set1_pd(numbers[j]));\n            __m128d abs_diff = _mm_andnot_pd(_mm_set_sd(-0.0), diff);\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_v);\n            int mask = _mm_movemask_pd(cmp);\n\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    int n = numbers.size();\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (int i = 0; i < n; ++i) {\n        __m128d num1 = _mm_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < n; ++j) {\n            __m128d num2 = _mm_set1_pd(numbers[j]);\n            __m128d diff = _mm_sub_pd(num1, num2);\n            __m128d abs_diff = _mm_andnot_pd(_mm_cmplt_pd(diff, zero_vec), diff);\n\n            __m128d cmp_res = _mm_cmplt_pd(abs_diff, threshold_vec);\n            int mask = _mm_movemask_pd(cmp_res);\n\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    int n = numbers.size();\n    __m128d threshold_v = _mm_set1_pd(threshold);\n    __m128d zero_v = _mm_setzero_pd();\n\n    for (int i = 0; i < n; ++i) {\n        __m128d num_v = _mm_set1_pd(numbers[i]);\n\n        for (int j = i + 1; j < n; ++j) {\n            __m128d num2_v = _mm_set1_pd(numbers[j]);\n            __m128d diff_v = _mm_sub_pd(num_v, num2_v);\n            __m128d abs_diff_v = _mm_andnot_pd(_mm_set_sd(-0.0), diff_v);\n\n            __m128d cmp_v = _mm_cmplt_pd(abs_diff_v, threshold_v);\n            int mask = _mm_movemask_pd(cmp_v);\n\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <emmintrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double>& numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    __m128d threshold_v = _mm_set1_pd(threshold);\n    __m128d zero_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128d num1 = _mm_load1_pd(&numbers[i]);\n\n        for (size_t j = i + 1; j < n; ++j) {\n            __m128d num2 = _mm_load1_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(num1, num2);\n            __m128d abs_diff = _mm_andnot_pd(_mm_cmplt_pd(zero_v, diff), diff);\n\n            __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_v);\n            int mask = _mm_movemask_pd(cmp);\n\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int size = operations.size();\n    int i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        sum = _mm_add_epi32(sum, chunk);\n        __m128i mask = _mm_cmplt_epi32(sum, zero);\n        int mask_int = _mm_movemask_epi8(mask);\n        if (mask_int != 0) {\n            return true;\n        }\n    }\n\n    int remaining_sum = _mm_cvtsi128_si32(sum);\n    for (; i < size; ++i) {\n        remaining_sum += operations[i];\n        if (remaining_sum < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i < operations.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        sum = _mm_add_epi32(sum, chunk);\n        min_val = _mm_min_epi32(min_val, sum);\n        sum = _mm_add_epi32(sum, chunk);\n    }\n\n    __m128i cmp = _mm_cmplt_epi32(min_val, zero);\n    int mask = _mm_movemask_epi8(cmp);\n\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int size = operations.size();\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < size - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&operations[i]);\n        sum = _mm_add_epi32(sum, chunk);\n\n        __m128i cmp = _mm_cmplt_epi32(sum, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    int remainder = size % 4;\n    if (remainder > 0) {\n        int temp[4] = {0, 0, 0, 0};\n        for (int i = 0; i < remainder; ++i) {\n            temp[i] = operations[size - remainder + i];\n        }\n        __m128i chunk = _mm_loadu_si128((__m128i*)temp);\n        sum = _mm_add_epi32(sum, chunk);\n\n        __m128i cmp = _mm_cmplt_epi32(sum, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < operations.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        sum = _mm_add_epi32(sum, chunk);\n\n        __m128i mask = _mm_cmplt_epi32(sum, zero);\n        int result = _mm_movemask_epi8(mask);\n\n        if (result != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    int size = operations.size();\n\n    for (int i = 0; i <= size - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&operations[i]);\n        sum = _mm_add_epi32(sum, chunk);\n\n        __m128i cmp = _mm_cmplt_epi32(sum, zero);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    int remaining = size % 4;\n    if (remaining > 0) {\n        __m128i chunk = _mm_setzero_si128();\n        for (int i = 0; i < remaining; ++i) {\n            chunk = _mm_insert_epi32(chunk, operations[size - remaining + i], i);\n        }\n        sum = _mm_add_epi32(sum, chunk);\n\n        __m128i cmp = _mm_cmplt_epi32(sum, zero);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    for (size_t i = 0; i < n; ++i) {\n        sum += numbers[i];\n    }\n    double mean = sum / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d abs_mask = _mm_set1_pd(-0.0);\n\n    for (size_t i = 0; i < n - 1; i += 2) {\n        __m128d num_vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_vec = _mm_sub_pd(num_vec, mean_vec);\n        __m128d abs_diff_vec = _mm_andnot_pd(abs_mask, diff_vec);\n        sum_vec = _mm_add_pd(sum_vec, abs_diff_vec);\n    }\n\n    if (n % 2 != 0) {\n        __m128d num_vec = _mm_loadh_pd(_mm_setzero_pd(), &numbers[n - 1]);\n        __m128d diff_vec = _mm_sub_pd(num_vec, mean_vec);\n        __m128d abs_diff_vec = _mm_andnot_pd(abs_mask, diff_vec);\n        sum_vec = _mm_add_pd(sum_vec, abs_diff_vec);\n    }\n\n    __m128d sum_vec_hor = _mm_hadd_pd(sum_vec, sum_vec);\n    double sum_abs_diff[2];\n    _mm_storeu_pd(sum_abs_diff, sum_vec_hor);\n\n    double mad = (sum_abs_diff[0] + sum_abs_diff[1]) / n;\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    __m128d sum_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        sum_v = _mm_add_pd(sum_v, num_v);\n    }\n\n    _mm_storeu_pd(&sum, sum_v);\n    double mean = (sum + (n % 2 == 0 ? 0.0 : numbers.back())) / n;\n\n    __m128d mean_v = _mm_set1_pd(mean);\n    __m128d mad_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_v = _mm_sub_pd(num_v, mean_v);\n        __m128d abs_diff_v = _mm_andnot_pd(_mm_set1_pd(-0.0), diff_v);\n        mad_v = _mm_add_pd(mad_v, abs_diff_v);\n    }\n\n    double mad_sum = 0.0;\n    _mm_storeu_pd(&mad_sum, mad_v);\n\n    return (mad_sum + (n % 2 == 0 ? 0.0 : std::abs(numbers.back() - mean))) / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    __m128d sum_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        sum_v = _mm_add_pd(sum_v, num_v);\n    }\n\n    _mm_storeu_pd(&sum, sum_v);\n    double mean = (sum / n);\n\n    __m128d mean_v = _mm_set1_pd(mean);\n    __m128d mad_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_v = _mm_sub_pd(num_v, mean_v);\n        __m128d abs_diff_v = _mm_andnot_pd(_mm_set1_pd(-0.0), diff_v);\n        mad_v = _mm_add_pd(mad_v, abs_diff_v);\n    }\n\n    double mad_sum[2];\n    _mm_storeu_pd(mad_sum, mad_v);\n    double mad = (mad_sum[0] + mad_sum[1]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    for (size_t i = 0; i < n; ++i) {\n        sum += numbers[i];\n    }\n    double mean = sum / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d abs_mask = _mm_set1_pd(-0.0);\n\n    size_t i;\n    for (i = 0; i <= n - 2; i += 2) {\n        __m128d data_vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_vec = _mm_sub_pd(data_vec, mean_vec);\n        __m128d abs_diff_vec = _mm_andnot_pd(abs_mask, diff_vec);\n        sum_vec = _mm_add_pd(sum_vec, abs_diff_vec);\n    }\n\n    double sum_abs_dev[2];\n    _mm_storeu_pd(sum_abs_dev, sum_vec);\n    double total_abs_dev = sum_abs_dev[0] + sum_abs_dev[1];\n\n    for (; i < n; ++i) {\n        total_abs_dev += std::abs(numbers[i] - mean);\n    }\n\n    return total_abs_dev / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    double sum = 0.0;\n    __m128d sum_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        sum_v = _mm_add_pd(sum_v, num_v);\n    }\n\n    _mm_storeu_pd(&sum, sum_v);\n    double mean = (sum / n);\n\n    __m128d mean_v = _mm_set1_pd(mean);\n    __m128d mad_v = _mm_setzero_pd();\n\n    for (size_t i = 0; i < n; i += 2) {\n        __m128d num_v = _mm_loadu_pd(&numbers[i]);\n        __m128d diff_v = _mm_sub_pd(num_v, mean_v);\n        __m128d abs_diff_v = _mm_andnot_pd(_mm_set1_pd(-0.0), diff_v);\n        mad_v = _mm_add_pd(mad_v, abs_diff_v);\n    }\n\n    double mad_sum[2];\n    _mm_storeu_pd(mad_sum, mad_v);\n    double mad = (mad_sum[0] + mad_sum[1]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    size_t n = numbers.size();\n    std::vector<int> result((n * 2) - 1);\n\n    if (n == 0) return result;\n\n    __m128i delim = _mm_set1_epi32(delimeter);\n    __m128i *out = reinterpret_cast<__m128i*>(&result[0]);\n    __m128i *in = reinterpret_cast<__m128i*>(&numbers[0]);\n\n    size_t i;\n    for (i = 0; i <= n - 4; i += 4) {\n        __m128i num = _mm_loadu_si128(&in[i / 4]);\n        __m128i inter = _mm_setr_epi32(\n            _mm_extract_epi32(num, 0), _mm_extract_epi32(delim, 0),\n            _mm_extract_epi32(num, 1), _mm_extract_epi32(delim, 0)\n        );\n        _mm_storeu_si128(&out[i / 2], inter);\n\n        inter = _mm_setr_epi32(\n            _mm_extract_epi32(num, 2), _mm_extract_epi32(delim, 0),\n            _mm_extract_epi32(num, 3), _mm_extract_epi32(delim, 0)\n        );\n        _mm_storeu_si128(&out[(i / 2) + 2], inter);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != n - 1) {\n            result[i * 2 + 1] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m128i delim = _mm_set1_epi32(delimeter);\n    __m128i* numPtr = reinterpret_cast<__m128i*>(numbers.data());\n    __m128i* resPtr = reinterpret_cast<__m128i*>(result.data());\n\n    size_t i = 0;\n    for (; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_loadu_si128(&numPtr[i / 4]);\n        __m128i inter = _mm_setr_epi32(\n            _mm_extract_epi16(num, 0), _mm_extract_epi16(delim, 0),\n            _mm_extract_epi16(num, 2), _mm_extract_epi16(delim, 0));\n        _mm_storeu_si128(&resPtr[i / 4 * 2], inter);\n\n        inter = _mm_setr_epi32(\n            _mm_extract_epi16(num, 4), _mm_extract_epi16(delim, 0),\n            _mm_extract_epi16(num, 6), _mm_extract_epi16(delim, 0));\n        _mm_storeu_si128(&resPtr[i / 4 * 2 + 1], inter);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i != numbers.size() - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimiter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m128i delim = _mm_set1_epi32(delimiter);\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128i nums = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n\n        __m128i interspersed1 = _mm_unpacklo_epi32(nums, delim);\n        __m128i interspersed2 = _mm_unpackhi_epi32(nums, delim);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i * 2]), interspersed1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i * 2 + 4]), interspersed2);\n    }\n\n    size_t remaining = numbers.size() % 4;\n    if (remaining > 0) {\n        __m128i nums = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[numbers.size() - remaining]));\n        __m128i mask = _mm_setr_epi32(0xFFFFFFFF, remaining > 1 ? 0xFFFFFFFF : 0, remaining > 2 ? 0xFFFFFFFF : 0, 0);\n        nums = _mm_and_si128(nums, mask);\n\n        __m128i interspersed = _mm_unpacklo_epi32(nums, delim);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[result.size() - remaining * 2]), interspersed);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m128i delim = _mm_set1_epi32(delimeter);\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128i num = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        __m128i mask = _mm_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n\n        __m128i interspersed = _mm_or_si128(_mm_and_si128(num, mask), _mm_andnot_si128(mask, delim));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i * 2]), interspersed);\n\n        if (i + 4 < numbers.size()) {\n            __m128i next_num = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i + 4]));\n            __m128i next_interspersed = _mm_or_si128(_mm_and_si128(next_num, mask), _mm_andnot_si128(mask, delim));\n\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i * 2 + 4]), next_interspersed);\n        }\n    }\n\n    if (numbers.size() % 2 == 1) {\n        result.push_back(numbers.back());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimiter) {\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    if (numbers.empty()) {\n        return result;\n    }\n\n    __m128i delim = _mm_set1_epi32(delimiter);\n    __m128i *nums = reinterpret_cast<__m128i*>(numbers.data());\n    __m128i *res = reinterpret_cast<__m128i*>(result.data());\n\n    size_t i = 0;\n    for (; i < numbers.size() - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128(&nums[i / 4]);\n        __m128i interspersed = _mm_unpacklo_epi32(chunk, delim);\n        _mm_storeu_si128(&res[i / 4 * 2], interspersed);\n        __m128i interspersed2 = _mm_unpackhi_epi32(chunk, delim);\n        _mm_storeu_si128(&res[i / 4 * 2 + 1], interspersed2);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i < numbers.size() - 1) {\n            result.push_back(delimiter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi16(1);\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 8; i += 8) {\n        __m128i num = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        sum = _mm_add_epi16(sum, num);\n        temp = _mm_mullo_epi16(prod, num);\n        prod = _mm_mulhi_epu16(temp, num);\n    }\n\n    alignas(16) int16_t sum_array[8] = {0};\n    _mm_store_si128(reinterpret_cast<__m128i*>(sum_array), sum);\n    int64_t total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n    }\n\n    alignas(16) int16_t prod_array[8] = {1};\n    _mm_store_si128(reinterpret_cast<__m128i*>(prod_array), prod);\n    int64_t total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_prod *= prod_array[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_setr_epi16(numbers[i], numbers[i+1], numbers[i+2], numbers[i+3], 0, 0, 0, 0);\n        sum = _mm_add_epi32(sum, _mm_unpacklo_epi16(num, _mm_setzero_si128()));\n\n        temp = _mm_unpacklo_epi16(num, _mm_setzero_si128());\n        prod = _mm_mul_epu32(prod, temp);\n    }\n\n    int32_t sum_array[4];\n    int32_t prod_array[4] = {1, 1, 1, 1};\n    _mm_storeu_si128((__m128i*)sum_array, sum);\n    _mm_storeu_si128((__m128i*)prod_array, prod);\n\n    int64_t total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    int64_t total_prod = 1;\n    for (int j = 0; j < 4; ++j) {\n        total_prod *= prod_array[j];\n    }\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        sum = _mm_add_epi32(sum, _mm_cvtepi16_epi32(chunk));\n        temp = _mm_mullo_epi32(prod, _mm_cvtepi16_epi32(chunk));\n        prod = temp;\n    }\n\n    alignas(16) int32_t sum_array[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(sum_array), sum);\n    int64_t total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    alignas(16) int32_t prod_array[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(prod_array), prod);\n    int64_t total_prod = prod_array[0] * prod_array[1] * prod_array[2] * prod_array[3];\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i temp;\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 4; i += 4) {\n        __m128i num = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        sum = _mm_add_epi32(sum, _mm_cvtepi16_epi32(num));\n\n        temp = _mm_mullo_epi32(prod, _mm_cvtepi16_epi32(num));\n        prod = _mm_add_epi32(temp, _mm_srli_si128(temp, 8));\n    }\n\n    int32_t sum_arr[4];\n    int32_t prod_arr[4] = {1, 1, 1, 1};\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_arr), sum);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(prod_arr), prod);\n\n    int64_t total_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n    int64_t total_prod = prod_arr[0] * prod_arr[1] * prod_arr[2] * prod_arr[3];\n\n    for (; i < numbers.size(); ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    if (numbers.empty()) {\n        return {0, 1};\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi16(1);\n\n    size_t i;\n    for (i = 0; i <= numbers.size() - 8; i += 8) {\n        __m128i num = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        sum = _mm_add_epi16(sum, num);\n        prod = _mm_mullo_epi16(prod, num);\n    }\n\n    alignas(16) int16_t temp[8] = {0};\n    for (; i < numbers.size(); ++i) {\n        temp[i % 8] = numbers[i];\n    }\n\n    __m128i num = _mm_load_si128(reinterpret_cast<const __m128i*>(temp));\n    sum = _mm_add_epi16(sum, num);\n    prod = _mm_mullo_epi16(prod, num);\n\n    alignas(16) int16_t sum_array[8];\n    alignas(16) int16_t prod_array[8];\n    _mm_store_si128(reinterpret_cast<__m128i*>(sum_array), sum);\n    _mm_store_si128(reinterpret_cast<__m128i*>(prod_array), prod);\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_array[j];\n        total_prod *= prod_array[j];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        current_max = _mm_max_epi32(current_max, chunk);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current_max);\n\n        int temp_max = _mm_extract_epi32(current_max, 3);\n        current_max = _mm_set1_epi32(temp_max);\n    }\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        result[i] = std::max(result[i], result[i - 1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m128i max_val = _mm_set1_epi32(INT_MIN);\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        max_val = _mm_max_epi32(max_val, chunk);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), max_val);\n\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), max_val);\n        int local_max = *std::max_element(temp, temp + 4);\n        max_val = _mm_set1_epi32(local_max);\n    }\n\n    for (size_t i = 0; i < numbers.size() % 4; ++i) {\n        size_t index = numbers.size() - (numbers.size() % 4) + i;\n        result[index] = std::max(result[index - 1], numbers[index]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m128i current_max = _mm_set1_epi32(INT_MIN);\n    __m128i shuffle_mask = _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        current_max = _mm_max_epi32(current_max, chunk);\n        current_max = _mm_shuffle_epi32(current_max, _MM_SHUFFLE(3, 3, 3, 3));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current_max);\n    }\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        if (i > 0) {\n            result[i] = std::max(result[i], result[i - 1]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m128i current_max = _mm_set1_epi32(INT_MIN);\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        current_max = _mm_max_epi32(current_max, chunk);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current_max);\n\n        int max_val = std::max({_mm_extract_epi32(current_max, 0), _mm_extract_epi32(current_max, 1),\n                                _mm_extract_epi32(current_max, 2), _mm_extract_epi32(current_max, 3)});\n        current_max = _mm_set1_epi32(max_val);\n    }\n\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        if (i >= 4) {\n            result[i] = std::max(result[i], result[i - 4]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n        current_max = _mm_max_epi32(current_max, chunk);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current_max);\n\n        int temp_max = _mm_extract_epi32(current_max, 3);\n        current_max = _mm_set1_epi32(temp_max);\n    }\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        result[i] = std::max(result[i], result[i - 1]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    // Ensure the length is a multiple of 16 for SSE operations\n    size_t padded_length = (length + 15) & ~15;\n    char* a_padded = (char*)_mm_malloc(padded_length, 16);\n    char* b_padded = (char*)_mm_malloc(padded_length, 16);\n    char* result_padded = (char*)_mm_malloc(padded_length, 16);\n\n    std::memcpy(a_padded, a.c_str(), length);\n    std::memcpy(b_padded, b.c_str(), length);\n\n    for (size_t i = 0; i < padded_length; i += 16) {\n        __m128i va = _mm_load_si128((__m128i*)(a_padded + i));\n        __m128i vb = _mm_load_si128((__m128i*)(b_padded + i));\n        __m128i vxor = _mm_xor_si128(va, vb);\n        _mm_store_si128((__m128i*)(result_padded + i), vxor);\n    }\n\n    std::memcpy(&result[0], result_padded, length);\n\n    _mm_free(a_padded);\n    _mm_free(b_padded);\n    _mm_free(result_padded);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    size_t padded_length = (length + 15) / 16 * 16;\n\n    char* a_padded = new char[padded_length];\n    char* b_padded = new char[padded_length];\n    char* result = new char[padded_length];\n\n    std::memcpy(a_padded, a.c_str(), length);\n    std::memcpy(b_padded, b.c_str(), length);\n\n    __m128i* a_vec = (__m128i*)a_padded;\n    __m128i* b_vec = (__m128i*)b_padded;\n    __m128i* res_vec = (__m128i*)result;\n\n    for (size_t i = 0; i < padded_length / 16; ++i) {\n        __m128i av = _mm_loadu_si128(&a_vec[i]);\n        __m128i bv = _mm_loadu_si128(&b_vec[i]);\n        __m128i xor_result = _mm_xor_si128(av, bv);\n        _mm_storeu_si128(&res_vec[i], xor_result);\n    }\n\n    std::string xor_result_str(result, length);\n\n    delete[] a_padded;\n    delete[] b_padded;\n    delete[] result;\n\n    return xor_result_str;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    // Ensure the length is a multiple of 16 for SSE operations\n    size_t padded_length = (length + 15) & ~15;\n    char* padded_a = new char[padded_length];\n    char* padded_b = new char[padded_length];\n    char* padded_result = new char[padded_length];\n\n    std::memcpy(padded_a, a.c_str(), length);\n    std::memcpy(padded_b, b.c_str(), length);\n\n    for (size_t i = 0; i < padded_length; i += 16) {\n        __m128i va = _mm_loadu_si128((__m128i*)(padded_a + i));\n        __m128i vb = _mm_loadu_si128((__m128i*)(padded_b + i));\n        __m128i vxor = _mm_xor_si128(va, vb);\n        _mm_storeu_si128((__m128i*)(padded_result + i), vxor);\n    }\n\n    std::memcpy(&result[0], padded_result, length);\n\n    delete[] padded_a;\n    delete[] padded_b;\n    delete[] padded_result;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.length();\n    std::string result(len, ' ');\n\n    size_t i = 0;\n    for (; i <= len - 16; i += 16) {\n        __m128i va = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i vb = _mm_loadu_si128((__m128i*)&b[i]);\n        __m128i vx = _mm_xor_si128(va, vb);\n        _mm_storeu_si128((__m128i*)&result[i], vx);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t length = a.length();\n    std::string result(length, ' ');\n\n    // Ensure the length is a multiple of 16 for SIMD operations\n    size_t alignedLength = (length + 15) & ~15;\n    char* alignedA = (char*)_mm_malloc(alignedLength, 16);\n    char* alignedB = (char*)_mm_malloc(alignedLength, 16);\n    char* alignedResult = (char*)_mm_malloc(alignedLength, 16);\n\n    std::memcpy(alignedA, a.c_str(), length);\n    std::memcpy(alignedB, b.c_str(), length);\n\n    for (size_t i = 0; i < alignedLength; i += 16) {\n        __m128i va = _mm_loadu_si128((__m128i*)(alignedA + i));\n        __m128i vb = _mm_loadu_si128((__m128i*)(alignedB + i));\n        __m128i vxor = _mm_xor_si128(va, vb);\n        _mm_storeu_si128((__m128i*)(alignedResult + i), vxor);\n    }\n\n    std::memcpy(&result[0], alignedResult, length);\n\n    _mm_free(alignedA);\n    _mm_free(alignedB);\n    _mm_free(alignedResult);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    __m128i v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m128i v_lengths = _mm_setzero_si128();\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t length = strings[i + j].length();\n            v_lengths = _mm_insert_epi32(v_lengths, static_cast<int>(length), j);\n        }\n\n        __m128i v_cmp = _mm_cmpgt_epi32(v_lengths, v_max_length);\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(v_cmp, v_cmp));\n\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n            v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) return {};\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    __m128i v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m128i v_lengths = _mm_setzero_si128();\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t length = strings[i + j].length();\n            v_lengths = _mm_insert_epi32(v_lengths, static_cast<int>(length), j);\n        }\n\n        __m128i v_cmp = _mm_cmpgt_epi32(v_lengths, v_max_length);\n        int mask = _mm_movemask_epi8(v_cmp);\n\n        if (mask != 0) {\n            for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    max_length = strings[i + j].length();\n                    longest = strings[i + j];\n                    v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n                    break;\n                }\n            }\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    for (size_t i = 1; i < strings.size(); i += 2) {\n        __m128i len1 = _mm_set1_epi32(static_cast<int>(strings[i].length()));\n        __m128i len2 = _mm_set1_epi32(i + 1 < strings.size() ? static_cast<int>(strings[i + 1].length()) : 0);\n\n        __m128i max_len = _mm_max_epi32(len1, len2);\n\n        int max_len_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(max_len_arr), max_len);\n\n        if (max_len_arr[0] > static_cast<int>(max_length)) {\n            max_length = max_len_arr[0];\n            longest = strings[i];\n        }\n        if (i + 1 < strings.size() && max_len_arr[2] > static_cast<int>(max_length)) {\n            max_length = max_len_arr[2];\n            longest = strings[i + 1];\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) return {};\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    __m128i v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m128i v_lengths = _mm_setzero_si128();\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t length = strings[i + j].length();\n            v_lengths = _mm_insert_epi32(v_lengths, static_cast<int>(length), j);\n        }\n\n        __m128i v_cmp = _mm_cmpgt_epi32(v_lengths, v_max_length);\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(v_cmp, v_cmp));\n\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            max_length = strings[i + index].length();\n            longest = strings[i + index];\n            v_max_length = _mm_set1_epi32(static_cast<int>(max_length));\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) {\n        return {};\n    }\n\n    std::string longest = strings[0];\n    size_t max_length = longest.length();\n\n    __m128i max_len_vec = _mm_set1_epi32(static_cast<int>(max_length));\n\n    for (size_t i = 1; i < strings.size(); i += 4) {\n        __m128i lengths = _mm_setzero_si128();\n\n        for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n            size_t len = strings[i + j].length();\n            lengths = _mm_insert_epi32(lengths, static_cast<int>(len), j);\n        }\n\n        __m128i cmp_res = _mm_cmpgt_epi32(lengths, max_len_vec);\n        int mask = _mm_movemask_epi8(cmp_res);\n\n        if (mask != 0) {\n            for (size_t j = 0; j < 4 && (i + j) < strings.size(); ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    max_length = strings[i + j].length();\n                    longest = strings[i + j];\n                    max_len_vec = _mm_set1_epi32(static_cast<int>(max_length));\n                    break;\n                }\n            }\n        }\n    }\n\n    return longest;\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i _a = _mm_set1_epi64x(a);\n    __m128i _b = _mm_set1_epi64x(b);\n    __m128i _zero = _mm_setzero_si128();\n\n    while (1) {\n        __m128i _rem = _mm_sub_epi64(_a, _mm_mul_epu32(_mm_srli_epi64(_a, 32), _b));\n        _rem = _mm_sub_epi64(_rem, _mm_and_si128(_mm_cmpgt_epi64(_zero, _rem), _b));\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(_rem, _zero)) == 0xFF) {\n            return (uint64_t)_b;\n        }\n\n        _a = _b;\n        _b = _rem;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i _a = _mm_set1_epi64x(a);\n    __m128i _b = _mm_set1_epi64x(b);\n\n    while (true) {\n        __m128i _rem = _mm_sub_epi64(_a, _mm_mul_epu32(_mm_srli_epi64(_a, 32), _b));\n        if (_mm_extract_epi64(_rem, 0) == 0) {\n            return _mm_extract_epi64(_b, 0);\n        }\n        _a = _b;\n        _b = _rem;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i _a = _mm_set1_epi64x(a);\n    __m128i _b = _mm_set1_epi64x(b);\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi64(_b, _mm_setzero_si128())) != 0xFFFF) {\n        __m128i tmp = _mm_andnot_si128(_mm_cmpeq_epi64(_mm_setzero_si128(), _mm_and_si128(_a, _mm_set1_epi64x(1))) , _b);\n        _b = _mm_andnot_si128(_mm_cmpeq_epi64(_mm_setzero_si128(), _mm_and_si128(_b, _mm_set1_epi64x(1))) , _a);\n        _a = tmp;\n\n        __m128i q = _mm_sub_epi64(_a, _mm_and_si128(_mm_srli_epi64(_a, 1), _mm_set1_epi64x(3)));\n        q = _mm_add_epi64(q, _mm_and_si128(_mm_srli_epi64(q, 2), _mm_set1_epi64x(3)));\n        q = _mm_srli_epi64(q, 3);\n\n        __m128i r = _mm_sub_epi64(_a, _mm_slli_epi64(_mm_mul_epu32(_b, q), 1));\n\n        _a = _b;\n        _b = r;\n    }\n\n    return _mm_cvtsi128_si64(_a);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i a_vec = _mm_set1_epi64x(a);\n    __m128i b_vec = _mm_set1_epi64x(b);\n\n    while (true) {\n        __m128i remainder = _mm_sub_epi64(_mm_min_epu64(a_vec, b_vec), _mm_and_si128(_mm_set_epi64x(1), _mm_xor_si128(a_vec, b_vec)));\n        if (_mm_testz_si128(remainder, remainder)) {\n            return (uint64_t)_mm_cvtsi128_si64(b_vec);\n        }\n        a_vec = b_vec;\n        b_vec = remainder;\n    }\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i _a = _mm_set1_epi64x(a);\n    __m128i _b = _mm_set1_epi64x(b);\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi64(_b, _mm_setzero_si128())) == 0) {\n        __m128i _tmp = _b;\n        _b = _mm_and_si128(_a, _mm_set1_epi64x(_mm_cvtsi128_si64(_mm_and_si128(_b, _mm_set1_epi64x(1)))));\n        _a = _tmp;\n    }\n\n    return _mm_cvtsi128_si64(_a);\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f;\n    float closest2 = 0.0f;\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n\n        for (size_t j = i + 4; j < numbers.size(); j += 4) {\n            __m128 vec2 = _mm_loadu_ps(&numbers[j]);\n\n            __m128 diff1 = _mm_sub_ps(vec1, vec2);\n            __m128 diff2 = _mm_sub_ps(vec2, vec1);\n            __m128 abs_diff = _mm_max_ps(_mm_andnot_ps(_mm_set1_ps(-0.0f), diff1), _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2));\n\n            float diff_array[4];\n            _mm_storeu_ps(diff_array, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diff_array[k] < min_diff) {\n                    min_diff = diff_array[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n\n        for (size_t j = i + 4; j < numbers.size(); j += 4) {\n            __m128 vec2 = _mm_loadu_ps(&numbers[j]);\n\n            __m128 diff1 = _mm_sub_ps(vec1, vec2);\n            __m128 diff2 = _mm_sub_ps(vec2, vec1);\n            __m128 abs_diff = _mm_max_ps(_mm_abs_ps(diff1), _mm_abs_ps(diff2));\n\n            float diff_array[4];\n            _mm_storeu_ps(diff_array, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diff_array[k] < min_diff) {\n                    min_diff = diff_array[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n\n        for (size_t j = i + 4; j < numbers.size(); j += 4) {\n            __m128 vec2 = _mm_loadu_ps(&numbers[j]);\n\n            __m128 diff1 = _mm_sub_ps(vec1, vec2);\n            __m128 diff2 = _mm_sub_ps(vec2, vec1);\n            __m128 abs_diff = _mm_max_ps(_mm_andnot_ps(_mm_cmplt_ps(diff1, _mm_setzero_ps()), diff2), _mm_andnot_ps(_mm_cmplt_ps(diff2, _mm_setzero_ps()), diff1));\n\n            float diff_array[4];\n            _mm_storeu_ps(diff_array, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diff_array[k] < min_diff) {\n                    min_diff = diff_array[k];\n                    closest1 = fminf(numbers[i + k], numbers[j + k]);\n                    closest2 = fmaxf(numbers[i + k], numbers[j + k]);\n                }\n            }\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) {\n        return {};\n    }\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f;\n    float closest2 = 0.0f;\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n\n        for (size_t j = i + 4; j < numbers.size(); j += 4) {\n            __m128 vec2 = _mm_loadu_ps(&numbers[j]);\n\n            __m128 diff1 = _mm_sub_ps(vec1, vec2);\n            __m128 diff2 = _mm_sub_ps(vec2, vec1);\n            __m128 abs_diff = _mm_max_ps(_mm_andnot_ps(_mm_set1_ps(-0.0f), diff1), _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2));\n\n            float diff_array[4];\n            _mm_storeu_ps(diff_array, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diff_array[k] < min_diff) {\n                    min_diff = diff_array[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    if (numbers.size() < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    for (size_t i = 0; i < numbers.size(); i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n\n        for (size_t j = i + 4; j < numbers.size(); j += 4) {\n            __m128 vec2 = _mm_loadu_ps(&numbers[j]);\n\n            __m128 diff1 = _mm_sub_ps(vec1, vec2);\n            __m128 diff2 = _mm_sub_ps(vec2, vec1);\n            __m128 abs_diff = _mm_or_ps(_mm_and_ps(diff1, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF))),\n                                         _mm_and_ps(diff2, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF))));\n\n            float diffs[4];\n            _mm_storeu_ps(diffs, abs_diff);\n\n            for (int k = 0; k < 4; ++k) {\n                if (diffs[k] < min_diff) {\n                    min_diff = diffs[k];\n                    closest1 = numbers[i + k];\n                    closest2 = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    if (closest1 > closest2) {\n        std::swap(closest1, closest2);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 sub_vec = _mm_sub_ps(num_vec, min_vec);\n        __m128 div_vec = _mm_div_ps(sub_vec, range_vec);\n        __m128 res_vec = _mm_min_ps(_mm_max_ps(div_vec, _mm_setzero_ps()), one_vec);\n        _mm_storeu_ps(&result[i], res_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 sub_vec = _mm_sub_ps(num_vec, min_vec);\n        __m128 div_vec = _mm_div_ps(sub_vec, range_vec);\n        __m128 res_vec = _mm_mul_ps(div_vec, one_vec);\n        _mm_storeu_ps(&numbers[i], res_vec);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 sub_vec = _mm_sub_ps(num_vec, min_vec);\n        __m128 div_vec = _mm_div_ps(sub_vec, range_vec);\n        _mm_storeu_ps(&result[i], div_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t n = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(n);\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 sub_vec = _mm_sub_ps(num_vec, min_vec);\n        __m128 div_vec = _mm_div_ps(sub_vec, range_vec);\n        __m128 res_vec = _mm_mul_ps(div_vec, one_vec);\n        _mm_storeu_ps(&result[i], res_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    size_t size = numbers.size();\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    std::vector<float> result(size);\n    __m128 min_vec = _mm_set1_ps(min_val);\n    __m128 range_vec = _mm_set1_ps(range);\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 num_vec = _mm_loadu_ps(&numbers[i]);\n        __m128 sub_vec = _mm_sub_ps(num_vec, min_vec);\n        __m128 div_vec = _mm_div_ps(sub_vec, range_vec);\n        __m128 res_vec = _mm_min_ps(_mm_max_ps(div_vec, _mm_setzero_ps()), one_vec);\n        _mm_storeu_ps(&result[i], res_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i divisors = _mm_set_epi64x(7, 5, 3, 2);\n    __m128i rem = _mm_setzero_si128();\n    __m128i max_divisor = _mm_setzero_si128();\n\n    for (int64_t i = 2; i < n; i += 4) {\n        divisors = _mm_set_epi64x(i+3, i+2, i+1, i);\n        rem = _mm_setzero_si128();\n        rem = _mm_or_si128(rem, _mm_and_si128(_mm_cmpgt_epi64(n_vec, divisors), _mm_set1_epi64x(1)));\n        __m128i mod = _mm_and_si128(_mm_sub_epi64(n_vec, _mm_mul_epu32(divisors, _mm_div_epi64(n_vec, divisors))), rem);\n        __m128i is_divisor = _mm_cmpeq_epi64(mod, _mm_setzero_si128());\n        max_divisor = _mm_or_si128(max_divisor, _mm_and_si128(is_divisor, divisors));\n    }\n\n    uint64_t result = (uint64_t)_mm_extract_epi64(max_divisor, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i divisors = _mm_set_epi64x(n-1, n-2, n-3, n-4);\n    __m128i remainders = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi64x(1);\n    __m128i mask = _mm_set1_epi64x(0);\n\n    remainders = _mm_sub_epi64(n_vec, _mm_mul_epu32(divisors, _mm_div_epu32(n_vec, divisors)));\n    mask = _mm_cmpeq_epi64(remainders, _mm_setzero_si128());\n\n    uint64_t results[4];\n    _mm_storeu_si128((__m128i*)results, mask);\n\n    for (int i = 0; i < 4; ++i) {\n        if (results[i] != 0) {\n            return n - 4 + i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i one_vec = _mm_set1_epi64x(1);\n    __m128i half_n_vec = _mm_srli_epi64(n_vec, 1);\n\n    uint64_t largest_divisor = 1;\n\n    for (uint64_t i = 2; i <= n / 2; i += 2) {\n        __m128i i_vec = _mm_set1_epi64x(i);\n        __m128i i_plus_one_vec = _mm_add_epi64(i_vec, one_vec);\n\n        __m128i rem_vec = _mm_rem_epu64(n_vec, i_vec);\n        __m128i rem_plus_one_vec = _mm_rem_epu64(n_vec, i_plus_one_vec);\n\n        uint64_t rem[2], rem_plus_one[2];\n        _mm_storeu_si128((__m128i*)rem, rem_vec);\n        _mm_storeu_si128((__m128i*)rem_plus_one, rem_plus_one_vec);\n\n        if (rem[0] == 0) {\n            largest_divisor = i;\n        }\n\n        if (rem_plus_one[0] == 0) {\n            largest_divisor = i + 1;\n        }\n    }\n\n    return largest_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i divisors = _mm_set_epi64x(n-1, n-2, n-3, n-4);\n    __m128i remainders = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi64x(1);\n    __m128i mask = _mm_set1_epi64x(0);\n\n    for (uint64_t i = n - 5; i > 0; i -= 4) {\n        divisors = _mm_add_epi64(divisors, _mm_set1_epi64x(-4));\n        remainders = _mm_and_si128(_mm_sub_epi64(n_vec, _mm_mul_epu32(divisors, _mm_srli_epi64(n_vec, 32))), mask);\n        mask = _mm_cmpeq_epi64(remainders, _mm_setzero_si128());\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(mask, mask), mask)) != 0) {\n            uint64_t divisor = _mm_extract_epi64(divisors, 0);\n            return divisor;\n        }\n    }\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i divisors = _mm_set_epi64x(n-1, n-2, n-3, n-4);\n    __m128i remainders = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi64x(1);\n    __m128i max_divisor = _mm_setzero_si128();\n\n    for (uint64_t i = n-1; i > 0; i -= 4) {\n        divisors = _mm_set_epi64x(i, i-1, i-2, i-3);\n        remainders = _mm_and_si128(_mm_cmpgt_epi64(n_vec, divisors), _mm_sub_epi64(n_vec, _mm_mul_epu32(divisors, _mm_div_epu32(n_vec, divisors))));\n        max_divisor = _mm_or_si128(_mm_andnot_si128(remainders, divisors), _mm_and_si128(remainders, max_divisor));\n    }\n\n    uint64_t results[2];\n    _mm_storeu_si128((__m128i*)results, max_divisor);\n    return results[0] > results[1] ? results[0] : results[1];\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m128i _n = _mm_set1_epi32(n);\n    __m128i _two = _mm_set1_epi32(2);\n    __m128i _three = _mm_set1_epi32(3);\n    __m128i _five = _mm_set1_epi32(5);\n    __m128i _seven = _mm_set1_epi32(7);\n\n    __m128i _rem_two = _mm_set1_epi32(n % 2);\n    __m128i _rem_three = _mm_set1_epi32(n % 3);\n    __m128i _rem_five = _mm_set1_epi32(n % 5);\n    __m128i _rem_seven = _mm_set1_epi32(n % 7);\n\n    __m128i _zero = _mm_set1_epi32(0);\n\n    __m128i _cmp_two = _mm_cmpeq_epi32(_rem_two, _zero);\n    __m128i _cmp_three = _mm_cmpeq_epi32(_rem_three, _zero);\n    __m128i _cmp_five = _mm_cmpeq_epi32(_rem_five, _zero);\n    __m128i _cmp_seven = _mm_cmpeq_epi32(_rem_seven, _zero);\n\n    int mask_two = _mm_movemask_epi8(_cmp_two);\n    int mask_three = _mm_movemask_epi8(_cmp_three);\n    int mask_five = _mm_movemask_epi8(_cmp_five);\n    int mask_seven = _mm_movemask_epi8(_cmp_seven);\n\n    if (mask_two == 0xFFFF) {\n        while (n % 2 == 0) {\n            factors.push_back(2);\n            n /= 2;\n        }\n    }\n    if (mask_three == 0xFFFF) {\n        while (n % 3 == 0) {\n            factors.push_back(3);\n            n /= 3;\n        }\n    }\n    if (mask_five == 0xFFFF) {\n        while (n % 5 == 0) {\n            factors.push_back(5);\n            n /= 5;\n        }\n    }\n    if (mask_seven == 0xFFFF) {\n        while (n % 7 == 0) {\n            factors.push_back(7);\n            n /= 7;\n        }\n    }\n\n    for (int i = 11; i <= std::sqrt(n); i += 2) {\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i _2 = _mm_set1_epi32(2);\n    __m128i _3 = _mm_set1_epi32(3);\n    __m128i _n = _mm_set1_epi32(n);\n    __m128i _1 = _mm_set1_epi32(1);\n    __m128i _inc = _mm_set_epi32(4, 3, 2, 1);\n\n    __m128i _i = _mm_set_epi32(3, 2, 2, 2);\n    __m128i _mask = _mm_set1_epi32(1);\n\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    for (__m128i _i = _mm_set_epi32(5, 3, 3, 3); _mm_movemask_epi8(_mm_cmplt_epi32(_i, _mm_set1_epi32(std::sqrt(n) + 1))) != 0; _i = _mm_add_epi32(_i, _inc)) {\n        __m128i _rem = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            int i = _mm_extract_epi32(_i, j);\n            if (i <= std::sqrt(n) && n % i == 0) {\n                _mm_store_si128(&_rem, _mm_insert_epi32(_rem, n / i, j));\n                while (n % i == 0) {\n                    factors.push_back(i);\n                    n /= i;\n                }\n            }\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i _2 = _mm_set1_epi32(2);\n    __m128i _3 = _mm_set1_epi32(3);\n    __m128i _n = _mm_set1_epi32(n);\n    __m128i _1 = _mm_set1_epi32(1);\n    __m128i _i = _mm_set1_epi32(2);\n\n    __m128i _limit = _mm_set1_epi32(static_cast<int>(sqrt(n)) + 1);\n\n    while (1) {\n        __m128i _mod = _mm_set1_epi32(n % _mm_cvtsi128_si32(_i));\n        __m128i _zero = _mm_setzero_si128();\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(_mod, _zero)) == 0xF) {\n            factors.push_back(_mm_cvtsi128_si32(_i));\n            _n = _mm_div_epi32(_n, _i);\n            n = _mm_cvtsi128_si32(_n);\n        } else {\n            _i = _mm_add_epi32(_i, _1);\n            if (_mm_cvtsi128_si32(_i) > _mm_cvtsi128_si32(_limit)) {\n                if (n > 1) {\n                    factors.push_back(n);\n                }\n                break;\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <smmintrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i _n = _mm_set1_epi32(n);\n    __m128i _two = _mm_set1_epi32(2);\n    __m128i _three = _mm_set1_epi32(3);\n    __m128i _four = _mm_set1_epi32(4);\n\n    __m128i _mask = _mm_set1_epi32(1);\n    __m128i _result;\n\n    while (1) {\n        _result = _mm_and_si128(_n, _mask);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(_result, _mm_setzero_si128())) == 0xFFFF) {\n            break;\n        }\n        factors.push_back(2);\n        _n = _mm_srli_epi32(_n, _two);\n    }\n\n    int remainder = _mm_cvtsi128_si32(_n);\n    int limit = static_cast<int>(sqrt(remainder)) + 1;\n\n    for (int i = 3; i <= limit; i += 2) {\n        __m128i _i = _mm_set1_epi32(i);\n        __m128i _rem = _mm_set1_epi32(remainder);\n        __m128i _mod = _mm_set1_epi32(remainder % i);\n\n        while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mod, _mm_setzero_si128())) == 0xFFFF) {\n            factors.push_back(i);\n            _rem = _mm_div_epi32(_rem, _i);\n            _mod = _mm_set1_epi32(_mm_cvtsi128_si32(_rem) % i);\n        }\n\n        remainder = _mm_cvtsi128_si32(_rem);\n        limit = static_cast<int>(sqrt(remainder)) + 1;\n    }\n\n    if (remainder > 2) {\n        factors.push_back(remainder);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i _n = _mm_set1_epi32(n);\n    __m128i _two = _mm_set1_epi32(2);\n    __m128i _three = _mm_set1_epi32(3);\n    __m128i _four = _mm_set1_epi32(4);\n\n    __m128i _mask = _mm_set1_epi32(1);\n    __m128i _result;\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi32(_mm_setzero_si128(), _mm_and_si128(_n, _mask))) != 0) {\n        _result = _mm_cvtsi32_si128(n % 2);\n        if (_mm_extract_epi32(_result, 0) == 0) {\n            factors.push_back(2);\n            n /= 2;\n            _n = _mm_set1_epi32(n);\n        } else {\n            break;\n        }\n    }\n\n    for (int i = 3; i <= std::sqrt(n); i += 2) {\n        __m128i _i = _mm_set1_epi32(i);\n        __m128i _rem = _mm_set1_epi32(n % i);\n        __m128i _zero = _mm_setzero_si128();\n\n        while (_mm_movemask_epi8(_mm_cmpeq_epi32(_rem, _zero)) != 0) {\n            factors.push_back(i);\n            n /= i;\n            _n = _mm_set1_epi32(n);\n            _rem = _mm_set1_epi32(n % i);\n        }\n    }\n\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(v, zero);\n\n        float temp[4];\n        _mm_storeu_ps(temp, v);\n\n        if (_mm_movemask_ps(mask) & 1) result.push_back(temp[0]);\n        if (_mm_movemask_ps(mask) & 2) result.push_back(temp[1]);\n        if (_mm_movemask_ps(mask) & 4) result.push_back(temp[2]);\n        if (_mm_movemask_ps(mask) & 8) result.push_back(temp[3]);\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(v, _mm_setzero_ps());\n        float temp[4];\n        _mm_storeu_ps(temp, v);\n\n        for (int j = 0; j < 4; ++j) {\n            if (((int*)&mask)[j] < 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(v, _mm_setzero_ps());\n        float temp[4];\n        _mm_storeu_ps(temp, v);\n\n        for (int j = 0; j < 4; ++j) {\n            if (((int*)&mask)[j] < 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(data, zero);\n\n        float temp[4];\n        _mm_storeu_ps(temp, data);\n\n        if (_mm_movemask_ps(mask)) {\n            for (int j = 0; j < 4; ++j) {\n                if (temp[j] > 0) {\n                    result.push_back(temp[j]);\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    size_t size = l.size();\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128 v = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(v, _mm_setzero_ps());\n        __m128 positive_vals = _mm_and_ps(v, mask);\n\n        float temp[4];\n        _mm_storeu_ps(temp, positive_vals);\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    int64_t i = 5;\n\n    __m128i v_i = _mm_set1_epi64x(i);\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_sqrt_n = _mm_set1_epi64x(sqrt_n);\n    __m128i v_two = _mm_set1_epi64x(2);\n    __m128i v_four = _mm_set1_epi64x(4);\n\n    for (; i <= sqrt_n; i += 4) {\n        __m128i v_i_plus_two = _mm_add_epi64(v_i, v_two);\n        __m128i v_mod_i = _mm_rem_epi64(v_n, v_i);\n        __m128i v_mod_i_plus_two = _mm_rem_epi64(v_n, v_i_plus_two);\n\n        __m128i v_zero = _mm_setzero_si128();\n        __m128i v_cmp_i = _mm_cmpeq_epi64(v_mod_i, v_zero);\n        __m128i v_cmp_i_plus_two = _mm_cmpeq_epi64(v_mod_i_plus_two, v_zero);\n\n        int mask_i = _mm_movemask_epi8(_mm_packs_epi16(v_cmp_i, v_cmp_i));\n        int mask_i_plus_two = _mm_movemask_epi8(_mm_packs_epi16(v_cmp_i_plus_two, v_cmp_i_plus_two));\n\n        if (mask_i != 0 || mask_i_plus_two != 0) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, v_four);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    int64_t i = 5;\n\n    __m128i v_i = _mm_set1_epi64x(i);\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_sqrt_n = _mm_set1_epi64x(sqrt_n);\n    __m128i v_two = _mm_set1_epi64x(2);\n    __m128i v_three = _mm_set1_epi64x(3);\n\n    __m128i v_step = _mm_set1_epi64x(6);\n    __m128i v_one = _mm_set1_epi64x(1);\n\n    while (i <= sqrt_n) {\n        __m128i v_mod_two = _mm_set1_epi64x(n % i);\n        __m128i v_mod_three = _mm_set1_epi64x(n % (i + 2));\n\n        __m128i v_cmp_two = _mm_cmpeq_epi64(v_mod_two, _mm_setzero_si128());\n        __m128i v_cmp_three = _mm_cmpeq_epi64(v_mod_three, _mm_setzero_si128());\n\n        int mask_two = _mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(v_cmp_two, v_cmp_two), _mm_setzero_si128()));\n        int mask_three = _mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(v_cmp_three, v_cmp_three), _mm_setzero_si128()));\n\n        if (mask_two != 0 || mask_three != 0) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, v_step);\n        i += 6;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    int64_t i = 5;\n\n    __m128i v_i = _mm_set1_epi64x(i);\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_sqrt_n = _mm_set1_epi64x(sqrt_n);\n    __m128i v_two = _mm_set1_epi64x(2);\n    __m128i v_three = _mm_set1_epi64x(3);\n\n    __m128i v_step = _mm_set1_epi64x(6);\n\n    while (i <= sqrt_n) {\n        __m128i v_mod2 = _mm_rem_epi64(v_n, _mm_add_epi64(v_i, v_two));\n        __m128i v_mod3 = _mm_rem_epi64(v_n, _mm_add_epi64(v_i, v_three));\n\n        __m128i v_cmp2 = _mm_cmpeq_epi64(v_mod2, _mm_setzero_si128());\n        __m128i v_cmp3 = _mm_cmpeq_epi64(v_mod3, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(v_cmp2, v_cmp2), _mm_packs_epi32(v_cmp3, v_cmp3)))) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, v_step);\n        i += 6;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    int64_t i = 5;\n\n    __m128i v_i = _mm_set1_epi64x(i);\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_sqrt_n = _mm_set1_epi64x(sqrt_n);\n    __m128i v_two = _mm_set1_epi64x(2);\n    __m128i v_four = _mm_set1_epi64x(4);\n\n    for (; i <= sqrt_n; i += 6) {\n        __m128i v_i_plus_two = _mm_add_epi64(v_i, v_two);\n        __m128i v_rem_i = _mm_rem_epi64(v_n, v_i);\n        __m128i v_rem_i_plus_two = _mm_rem_epi64(v_n, v_i_plus_two);\n\n        if (_mm_testz_si128(v_rem_i, v_rem_i) || _mm_testz_si128(v_rem_i_plus_two, v_rem_i_plus_two)) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, v_four);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <math.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = (int64_t)sqrt(n);\n    int64_t i = 5;\n\n    __m128i v_i = _mm_set1_epi64x(i);\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_sqrt_n = _mm_set1_epi64x(sqrt_n);\n    __m128i v_two = _mm_set1_epi64x(2);\n    __m128i v_three = _mm_set1_epi64x(3);\n\n    __m128i v_step = _mm_set1_epi64x(6);\n    __m128i v_one = _mm_set1_epi64x(1);\n\n    while (i <= sqrt_n) {\n        __m128i v_mod_two = _mm_set1_epi64x(n % i);\n        __m128i v_mod_three = _mm_set1_epi64x(n % (i + 2));\n\n        __m128i v_cmp_two = _mm_cmpeq_epi64(v_mod_two, _mm_setzero_si128());\n        __m128i v_cmp_three = _mm_cmpeq_epi64(v_mod_three, _mm_setzero_si128());\n\n        int mask_two = _mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(v_cmp_two, v_cmp_two), _mm_setzero_si128()));\n        int mask_three = _mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(v_cmp_three, v_cmp_three), _mm_setzero_si128()));\n\n        if (mask_two || mask_three) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, v_step);\n        i += 6;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    __m128i lowerCase = _mm_set1_epi8('a' - 'A');\n    __m128i lowerCaseMask = _mm_set1_epi8('a');\n    __m128i upperCaseMask = _mm_set1_epi8('Z');\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i isLower = _mm_cmpgt_epi8(_mm_subs_epu8(chunk, lowerCaseMask), _mm_setzero_si128());\n        __m128i isUpper = _mm_cmpgt_epi8(_mm_subs_epu8(upperCaseMask, chunk), _mm_setzero_si128());\n        __m128i isLetter = _mm_or_si128(isLower, isUpper);\n\n        if (_mm_movemask_epi8(isLetter) != 0) {\n            hasLetter = true;\n            __m128i toggledCase = _mm_xor_si128(chunk, _mm_and_si128(isLetter, lowerCase));\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), toggledCase);\n        }\n    }\n\n    for (size_t i = len - (len % 16); i < len; ++i) {\n        if (std::isalpha(s[i])) {\n            hasLetter = true;\n            result[i] = std::isupper(s[i]) ? std::tolower(s[i]) : std::toupper(s[i]);\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    __m128i lowerCase = _mm_set1_epi8('a' - 'A');\n    __m128i upperCase = _mm_set1_epi8('A' - 'a');\n    __m128i lowerMask = _mm_set1_epi8('a');\n    __m128i upperMask = _mm_set1_epi8('Z');\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i isLower = _mm_and_si128(_mm_cmpgt_epi8(_mm_subs_epu8(chunk, lowerMask), _mm_setzero_si128()), lowerCase);\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epu8(_mm_subs_epu8(upperMask, chunk), _mm_setzero_si128()), upperCase);\n        __m128i toggled = _mm_or_si128(isLower, isUpper);\n        __m128i resultChunk = _mm_xor_si128(chunk, toggled);\n\n        hasLetter |= !_mm_test_all_zeros(_mm_or_si128(isLower, isUpper), _mm_or_si128(isLower, isUpper));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(const_cast<char*>(result.data() + i)), resultChunk);\n    }\n\n    for (size_t i = (len / 16) * 16; i < len; ++i) {\n        if (std::isalpha(s[i])) {\n            result[i] = std::isupper(s[i]) ? std::tolower(s[i]) : std::toupper(s[i]);\n            hasLetter = true;\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (size_t i = 0; i < len; ++i) {\n        if (isalpha(s[i])) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 16 bytes (4 characters)\n    for (size_t i = 0; i < len; i += 16) {\n        size_t chunk_size = std::min(len - i, size_t(16));\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m128i lower_mask = _mm_set1_epi8('a');\n        __m128i upper_mask = _mm_set1_epi8('A');\n        __m128i diff = _mm_set1_epi8('a' - 'A');\n\n        // Check for lowercase letters\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, lower_mask), _mm_cmplt_epi8(chunk, _mm_add_epi8(lower_mask, _mm_set1_epi8(26))));\n        // Check for uppercase letters\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upper_mask), _mm_cmplt_epi8(chunk, _mm_add_epi8(upper_mask, _mm_set1_epi8(26))));\n\n        // Convert lowercase to uppercase\n        __m128i lower_to_upper = _mm_and_si128(is_lower, _mm_sub_epi8(chunk, diff));\n        // Convert uppercase to lowercase\n        __m128i upper_to_lower = _mm_and_si128(is_upper, _mm_add_epi8(chunk, diff));\n\n        // Combine the results\n        __m128i result_chunk = _mm_or_si128(_mm_or_si128(lower_to_upper, upper_to_lower), _mm_andnot_si128(_mm_or_si128(is_lower, is_upper), chunk));\n\n        // Store the result back into the string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), result_chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (size_t i = 0; i < len; ++i) {\n        if (isalpha(s[i])) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 16 bytes (4 characters)\n    for (size_t i = 0; i < len; i += 16) {\n        size_t remaining = std::min(len - i, size_t(16));\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Create masks for lowercase and uppercase letters\n        __m128i lowerMask = _mm_set1_epi8('a');\n        __m128i upperMask = _mm_set1_epi8('A');\n        __m128i diff = _mm_set1_epi8('a' - 'A');\n\n        // Check for lowercase letters\n        __m128i isLower = _mm_and_si128(_mm_cmpgt_epi8(chunk, lowerMask), _mm_cmplt_epi8(chunk, _mm_add_epi8(lowerMask, _mm_set1_epi8(26))));\n        // Check for uppercase letters\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upperMask), _mm_cmplt_epi8(chunk, _mm_add_epi8(upperMask, _mm_set1_epi8(26))));\n\n        // Convert lowercase to uppercase\n        __m128i lowerToUpper = _mm_sub_epi8(chunk, _mm_and_si128(isLower, diff));\n        // Convert uppercase to lowercase\n        __m128i upperToLower = _mm_add_epi8(chunk, _mm_and_si128(isUpper, diff));\n\n        // Combine the results\n        __m128i resultChunk = _mm_or_si128(_mm_and_si128(isLower, lowerToUpper), _mm_andnot_si128(isLower, upperToLower));\n\n        // Store the result back into the string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(const_cast<char*>(result.data() + i)), resultChunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    std::string result = s;\n    size_t len = s.length();\n    bool hasLetter = false;\n\n    __m128i lowerCase = _mm_set1_epi8('a' - 'A');\n    __m128i upperCase = _mm_set1_epi8('A' - 'a');\n    __m128i lowerMask = _mm_set1_epi8('a');\n    __m128i upperMask = _mm_set1_epi8('Z');\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i isLower = _mm_and_si128(_mm_cmpgt_epi8(_mm_subs_epu8(chunk, lowerMask), _mm_setzero_si128()), lowerCase);\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epu8(_mm_subs_epu8(upperMask, chunk), _mm_setzero_si128()), upperCase);\n        __m128i toggled = _mm_or_si128(isLower, isUpper);\n        __m128i mask = _mm_or_si128(_mm_cmpeq_epi8(isLower, _mm_setzero_si128()), _mm_cmpeq_epi8(isUpper, _mm_setzero_si128()));\n        toggled = _mm_xor_si128(chunk, toggled);\n        toggled = _mm_and_si128(mask, toggled);\n        chunk = _mm_andnot_si128(mask, chunk);\n        chunk = _mm_or_si128(chunk, toggled);\n        _mm_storeu_si128((__m128i*)&result[i], chunk);\n\n        hasLetter |= _mm_movemask_epi8(_mm_or_si128(isLower, isUpper)) != 0;\n    }\n\n    for (size_t i = len - (len % 16); i < len; ++i) {\n        if (std::isalpha(s[i])) {\n            result[i] = std::isupper(s[i]) ? std::tolower(s[i]) : std::toupper(s[i]);\n            hasLetter = true;\n        }\n    }\n\n    if (!hasLetter) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> result = l;\n    std::vector<int> to_sort;\n\n    for (size_t i = 2; i < l.size(); i += 3) {\n        to_sort.push_back(l[i]);\n    }\n\n    std::sort(to_sort.begin(), to_sort.end());\n\n    size_t sort_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        result[i] = to_sort[sort_index++];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> result = l;\n    std::vector<int> to_sort;\n\n    for (size_t i = 2; i < l.size(); i += 3) {\n        to_sort.push_back(l[i]);\n    }\n\n    std::sort(to_sort.begin(), to_sort.end());\n\n    size_t sort_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        result[i] = to_sort[sort_index++];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    std::vector<int> sorted_third;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        sorted_third.push_back(l[i]);\n    }\n    std::sort(sorted_third.begin(), sorted_third.end());\n\n    size_t sorted_index = 0;\n    for (size_t i = 2; i < l.size(); i += 3) {\n        l[i] = sorted_third[sorted_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    float max_vals[4];\n    _mm_storeu_ps(max_vals, max_vec);\n\n    float max_val = max_vals[0];\n    for (int i = 1; i < 4; ++i) {\n        if (max_vals[i] > max_val) {\n            max_val = max_vals[i];\n        }\n    }\n\n    for (size_t i = (l.size() / 4) * 4; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max_vec = _mm_set1_ps(std::numeric_limits<float>::lowest());\n    __m128 current_vec;\n\n    size_t i;\n    for (i = 0; i <= l.size() - 4; i += 4) {\n        current_vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, current_vec);\n    }\n\n    float max_vals[4];\n    _mm_storeu_ps(max_vals, max_vec);\n\n    float max_val = max_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_vals[j] > max_val) {\n            max_val = max_vals[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max_vec = _mm_set1_ps(-std::numeric_limits<float>::infinity());\n    __m128 curr_vec;\n\n    size_t i;\n    for (i = 0; i <= l.size() - 4; i += 4) {\n        curr_vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, curr_vec);\n    }\n\n    float max_vals[4];\n    _mm_storeu_ps(max_vals, max_vec);\n\n    float max_elem = max_vals[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_vals[j] > max_elem) {\n            max_elem = max_vals[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_elem) {\n            max_elem = l[i];\n        }\n    }\n\n    return max_elem;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m128 max_vec = _mm_set1_ps(-std::numeric_limits<float>::infinity());\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    float max_vals[4];\n    _mm_storeu_ps(max_vals, max_vec);\n    float max_val = max_vals[0];\n    for (int i = 1; i < 4; ++i) {\n        if (max_vals[i] > max_val) {\n            max_val = max_vals[i];\n        }\n    }\n\n    for (size_t i = (l.size() / 4) * 4; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m128 max_val = _mm_set1_ps(std::numeric_limits<float>::lowest());\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128 chunk = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, chunk);\n    }\n\n    float max_arr[4];\n    _mm_storeu_ps(max_arr, max_val);\n\n    float max_elem = max_arr[0];\n    for (int i = 1; i < 4; ++i) {\n        if (max_arr[i] > max_elem) {\n            max_elem = max_arr[i];\n        }\n    }\n\n    return max_elem;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i _11 = _mm_set1_epi32(11);\n    __m128i _13 = _mm_set1_epi32(13);\n    __m128i _7 = _mm_set1_epi32(7);\n    __m128i _0 = _mm_set1_epi32(0);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i div11 = _mm_div_epi32(nums, _11);\n        __m128i div13 = _mm_div_epi32(nums, _13);\n        __m128i mul11 = _mm_mullo_epi32(div11, _11);\n        __m128i mul13 = _mm_mullo_epi32(div13, _13);\n        __m128i cmp11 = _mm_cmpeq_epi32(nums, mul11);\n        __m128i cmp13 = _mm_cmpeq_epi32(nums, mul13);\n        __m128i cmp7 = _mm_cmpeq_epi32(nums, _7);\n\n        __m128i mask11 = _mm_and_si128(cmp11, _mm_set1_epi32(1));\n        __m128i mask13 = _mm_and_si128(cmp13, _mm_set1_epi32(1));\n        __m128i mask7 = _mm_and_si128(cmp7, _mm_set1_epi32(1));\n\n        __m128i mask = _mm_or_si128(mask11, mask13);\n        mask = _mm_and_si128(mask, mask7);\n\n        int mask_values[4];\n        _mm_storeu_si128((__m128i*)mask_values, mask);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_values[j]) {\n                int num = i + j;\n                while (num > 0) {\n                    if (num % 10 == 7) {\n                        count++;\n                    }\n                    num /= 10;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i eleven = _mm_set1_epi32(11);\n    __m128i thirteen = _mm_set1_epi32(13);\n    __m128i seven = _mm_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i div11 = _mm_div_epi32(nums, eleven);\n        __m128i div13 = _mm_div_epi32(nums, thirteen);\n        __m128i mask11 = _mm_cmpeq_epi32(div11, _mm_setzero_si128());\n        __m128i mask13 = _mm_cmpeq_epi32(div13, _mm_setzero_si128());\n        __m128i mask = _mm_or_si128(mask11, mask13);\n\n        for (int j = 0; j < 4; j++) {\n            if (_mm_extract_epi32(mask, j)) {\n                int num = i + j;\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (int k = 0; str[k] != '\\0'; k++) {\n                    if (str[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i eleven = _mm_set1_epi32(11);\n    __m128i thirteen = _mm_set1_epi32(13);\n    __m128i seven = _mm_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i div11 = _mm_div_epi32(numbers, eleven);\n        __m128i div13 = _mm_div_epi32(numbers, thirteen);\n        __m128i mask11 = _mm_cmpeq_epi32(div11, _mm_setzero_si128());\n        __m128i mask13 = _mm_cmpeq_epi32(div13, _mm_setzero_si128());\n        __m128i mask = _mm_or_si128(mask11, mask13);\n\n        int mask_array[4];\n        _mm_storeu_si128((__m128i*)mask_array, mask);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_array[j]) {\n                int num = i + j;\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (int k = 0; str[k] != '\\0'; k++) {\n                    if (str[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i eleven = _mm_set1_epi32(11);\n    __m128i thirteen = _mm_set1_epi32(13);\n    __m128i sevens = _mm_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i numbers = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i div11 = _mm_div_epi32(numbers, eleven);\n        __m128i div13 = _mm_div_epi32(numbers, thirteen);\n        __m128i mask11 = _mm_cmpeq_epi32(div11, _mm_setzero_si128());\n        __m128i mask13 = _mm_cmpeq_epi32(div13, _mm_setzero_si128());\n        __m128i mask = _mm_or_si128(mask11, mask13);\n\n        int mask_array[4];\n        _mm_storeu_si128((__m128i*)mask_array, mask);\n\n        for (int j = 0; j < 4; j++) {\n            if (mask_array[j]) {\n                int num = i + j;\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (int k = 0; str[k] != '\\0'; k++) {\n                    if (str[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m128i eleven = _mm_set1_epi32(11);\n    __m128i thirteen = _mm_set1_epi32(13);\n    __m128i seven = _mm_set1_epi8('7');\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i nums = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i div11 = _mm_div_epi32(nums, eleven);\n        __m128i div13 = _mm_div_epi32(nums, thirteen);\n        __m128i mask11 = _mm_cmpeq_epi32(div11, _mm_setzero_si128());\n        __m128i mask13 = _mm_cmpeq_epi32(div13, _mm_setzero_si128());\n        __m128i mask = _mm_or_si128(mask11, mask13);\n\n        for (int j = 0; j < 4; j++) {\n            if (_mm_extract_epi32(mask, j)) {\n                int num = i + j;\n                char str[12];\n                sprintf(str, \"%d\", num);\n                for (int k = 0; str[k] != '\\0'; k++) {\n                    if (str[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    std::sort(even_elements.begin(), even_elements.end());\n\n    size_t even_index = 0;\n    for (size_t i = 0; i < l.size(); i += 2) {\n        l[i] = even_elements[even_index++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < n; ++i) {\n        seen.insert(l[i]);\n    }\n\n    for (int i = 0; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            int target = -(l[i] + l[j]);\n            if (seen.count(target)) {\n                if (target != l[i] && target != l[j]) {\n                    return true;\n                } else if (target == l[i] && seen.count(target) > 1) {\n                    return true;\n                } else if (target == l[j] && seen.count(target) > 1) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < n; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i vij = _mm_add_epi32(vi, vj);\n            int ij[4];\n            _mm_storeu_si128((__m128i*)ij, vij);\n\n            for (int k = 0; k < 4; ++k) {\n                if (seen.find(-ij[k]) != seen.end()) {\n                    return true;\n                }\n            }\n\n            seen.insert(l[j]);\n        }\n        seen.insert(l[i]);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    std::unordered_set<int> seen;\n\n    for (int i = 0; i < n; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i sum = _mm_add_epi32(vi, vj);\n            for (int k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi32(l[k]);\n                __m128i total = _mm_add_epi32(sum, vk);\n                int result[4];\n                _mm_storeu_si128((__m128i*)result, total);\n                if (result[0] == 0 || result[1] == 0 || result[2] == 0 || result[3] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < n; ++i) {\n        seen.insert(l[i]);\n    }\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m128i vi = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        for (int j = i + 1; j < n; j += 4) {\n            __m128i vj = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[j]));\n            for (int k = j + 1; k < n; k += 4) {\n                __m128i vk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[k]));\n\n                __m128i sum1 = _mm_add_epi32(vi, vj);\n                __m128i sum2 = _mm_add_epi32(sum1, vk);\n\n                int sums[4];\n                _mm_storeu_si128(reinterpret_cast<__m128i*>(sums), sum2);\n\n                for (int s = 0; s < 4; ++s) {\n                    if (sums[s] == 0 && l[i + s] != l[j + s] && l[j + s] != l[k + s] && l[i + s] != l[k + s]) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < n; ++i) {\n        __m128i vi = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j < n; ++j) {\n            __m128i vj = _mm_set1_epi32(l[j]);\n            __m128i vsum = _mm_add_epi32(vi, vj);\n            int sums[4];\n            _mm_storeu_si128((__m128i*)sums, vsum);\n            for (int k = 0; k < 4; ++k) {\n                if (seen.find(-sums[k]) != seen.end()) {\n                    return true;\n                }\n            }\n            seen.insert(l[j]);\n        }\n        seen.insert(l[i]);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n\n    for (; i <= l.size() - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&l[i]);\n        v = _mm_add_epi32(v, _mm_set1_epi32(1));\n        _mm_storeu_si128((__m128i*)&result[i], v);\n    }\n\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m128i increment = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i incremented = _mm_add_epi32(chunk, increment);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), incremented);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        chunk = _mm_add_epi32(chunk, one);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m128i increment = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i incremented = _mm_add_epi32(chunk, increment);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), incremented);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    std::vector<int> result(l.size());\n    __m128i increment = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i incremented = _mm_add_epi32(chunk, increment);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), incremented);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i* vec = (__m128i*)&l[0];\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v = _mm_loadu_si128(&vec[i/4]);\n        for (int j = i + 4; j < n; j += 4) {\n            __m128i u = _mm_loadu_si128(&vec[j/4]);\n            __m128i sum = _mm_add_epi32(v, u);\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i* vec = (__m128i*)&l[0];\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(&vec[i / 4]);\n        for (int j = i + 4; j < n; j += 4) {\n            __m128i chunk2 = _mm_loadu_si128(&vec[j / 4]);\n            __m128i sum = _mm_add_epi32(chunk, chunk2);\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int size = l.size();\n    if (size < 2) return false;\n\n    std::unordered_set<int> seen;\n    for (int i = 0; i < size; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), v);\n\n        for (int j = 0; j < 4; ++j) {\n            if (i + j >= size) break;\n            if (seen.find(temp[j]) != seen.end() || (temp[j] == 0 && seen.find(0) != seen.end())) {\n                return true;\n            }\n            seen.insert(-temp[j]);\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    if (l.size() < 2) return false;\n\n    std::unordered_set<int> seen;\n    __m128i *ptr = (__m128i*)&l[0];\n    __m128i zero = _mm_setzero_si128();\n    __m128i signMask = _mm_set1_epi32(1 << 31);\n\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(&ptr[i/4]);\n        __m128i negVec = _mm_xor_si128(vec, signMask);\n\n        int extract[4];\n        _mm_storeu_si128((__m128i*)extract, vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (seen.find(extract[j]) != seen.end() || seen.find(-extract[j]) != seen.end()) {\n                return true;\n            }\n            seen.insert(extract[j]);\n        }\n    }\n\n    for (size_t i = (l.size() & ~3); i < l.size(); ++i) {\n        if (seen.find(l[i]) != seen.end() || seen.find(-l[i]) != seen.end()) {\n            return true;\n        }\n        seen.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i signMask = _mm_set1_epi32(1 << 31);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i vi = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        for (int j = i + 4; j < n; j += 4) {\n            __m128i vj = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[j]));\n            __m128i sum = _mm_add_epi32(vi, vj);\n            __m128i sign = _mm_xor_si128(sum, _mm_and_si128(sum, signMask));\n            __m128i cmp = _mm_cmpeq_epi32(sign, zero);\n            if (_mm_movemask_epi8(_mm_packs_epi32(cmp, cmp)) != 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    char buffer[64];\n    int pos = sizeof(buffer) - 1;\n    buffer[pos] = '\\0';\n\n    __m128i _base = _mm_set1_epi8(base);\n    __m128i _x = _mm_set1_epi64x(x);\n    __m128i _zero = _mm_setzero_si128();\n    __m128i _remainders = _mm_setzero_si128();\n    __m128i _quotients = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(_x, _zero)) != 0) {\n        _remainders = _mm_and_si128(_x, _mm_set1_epi64x(base - 1));\n        _quotients = _mm_srli_epi64(_mm_sub_epi64(_x, _remainders), 1);\n\n        _x = _mm_div_epi64(_x, _base);\n\n        int remainders[2];\n        _mm_storeu_si128((__m128i*)remainders, _remainders);\n\n        for (int i = 0; i < 2; ++i) {\n            if (remainders[i] < 10) {\n                buffer[--pos] = remainders[i] + '0';\n            }\n        }\n    }\n\n    return std::string(&buffer[pos]);\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result = \"\";\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n\n        __m128i vec_remainder = _mm_set1_epi8(remainder + '0');\n        __m128i vec_base = _mm_set1_epi8(base);\n\n        alignas(16) char temp[16];\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), vec_remainder);\n\n        result = temp[0] + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <smmintrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result = \"\";\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x = x / base;\n\n        result = std::to_string(remainder) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result = \"\";\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n        result = std::to_string(remainder) + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n#include <string>\n#include <cstdint>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    std::string result;\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n        result = char('0' + remainder) + result;\n    }\n\n    // Use SIMD to process the string in parallel\n    size_t len = result.length();\n    size_t alignedLen = (len + 15) & ~15; // Align to 16 bytes\n    char* alignedResult = (char*)_mm_malloc(alignedLen, 16);\n    memcpy(alignedResult, result.c_str(), len);\n    memset(alignedResult + len, 0, alignedLen - len);\n\n    for (size_t i = 0; i < alignedLen; i += 16) {\n        __m128i chunk = _mm_load_si128((__m128i*)(alignedResult + i));\n        // Perform any SIMD operations if needed\n        _mm_store_si128((__m128i*)(alignedResult + i), chunk);\n    }\n\n    result = std::string(alignedResult, len);\n    _mm_free(alignedResult);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    char* str = const_cast<char*>(text.c_str());\n\n    int i = 0;\n    for (; i <= len - 16; i += 16) {\n        __m128i front = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i back = _mm_loadu_si128((__m128i*)&str[len - i - 16]);\n        back = _mm_shuffle_epi8(back, _mm_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        __m128i cmp = _mm_cmpeq_epi8(front, back);\n        if (!_mm_movemask_epi8(_mm_cmpeq_epi8(cmp, _mm_set1_epi8(-1)))) {\n            return false;\n        }\n    }\n\n    for (; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    char* str = const_cast<char*>(text.c_str());\n    size_t i = 0;\n    size_t j = len - 1;\n\n    while (i < j) {\n        // Load 16 bytes from the start and end of the string\n        __m128i front = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i back = _mm_loadu_si128((__m128i*)&str[j - 15]);\n\n        // Reverse the back segment\n        back = _mm_shuffle_epi8(back, _mm_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare the front and back segments\n        __m128i cmp = _mm_cmpeq_epi8(front, back);\n\n        // Check if all bytes are equal\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n\n        i += 16;\n        if (j < 16) j = 0;\n        else j -= 16;\n    }\n\n    // Check remaining characters if any\n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len == 0) return true;\n\n    char* str = &text[0];\n    int i = 0;\n    int j = len - 16;\n\n    __m128i* p = (__m128i*)str;\n    __m128i* q = (__m128i*)&str[j];\n\n    __m128i zero = _mm_setzero_si128();\n\n    while (p < q) {\n        __m128i a = _mm_loadu_si128(p);\n        __m128i b = _mm_loadu_si128(q);\n        __m128i cmp = _mm_cmpeq_epi8(a, _mm_shuffle_epi8(b, _mm_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi8(cmp, zero)) != 0) {\n            return false;\n        }\n        p++;\n        q--;\n    }\n\n    i = (len / 16) * 16;\n    j = len - 1;\n\n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len == 0) return true;\n\n    char* str = const_cast<char*>(text.c_str());\n    int i = 0;\n    int j = len - 1;\n\n    while (i < j) {\n        // Load 16 bytes from the start and end of the string\n        __m128i start = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i end = _mm_loadu_si128((__m128i*)&str[j - 15]);\n\n        // Reverse the end segment\n        end = _mm_shuffle_epi8(end, _mm_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare the start and reversed end segments\n        __m128i cmp = _mm_cmpeq_epi8(start, end);\n\n        // Check if all bytes are equal\n        if (!_mm_movemask_epi8(_mm_cmpeq_epi8(cmp, _mm_set1_epi8(-1)))) {\n            return false;\n        }\n\n        i += 16;\n        j -= 16;\n    }\n\n    // Handle any remaining characters\n    while (i < j) {\n        if (str[i] != str[j]) {\n            return false;\n        }\n        i++;\n        j--;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    char* str = const_cast<char*>(text.c_str());\n\n    int i = 0;\n    for (; i <= len - 16; i += 16) {\n        __m128i left = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&str[len - 16 - i]);\n        right = _mm_or_si128(right, _mm_set1_epi8(0x20)); // Convert to lowercase\n        left = _mm_or_si128(left, _mm_set1_epi8(0x20)); // Convert to lowercase\n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        if (!_mm_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n\n    for (; i < len / 2; ++i) {\n        if (tolower(str[i]) != tolower(str[len - 1 - i])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    const int shift = 5;\n    const __m128i alpha_start = _mm_set1_epi8('a');\n    const __m128i alpha_end = _mm_set1_epi8('z' + 1);\n    const __m128i shift_vec = _mm_set1_epi8(26 - shift);\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(chunk, alpha_start), _mm_cmplt_epi8(chunk, alpha_end));\n        __m128i shifted = _mm_subs_epu8(chunk, shift_vec);\n        __m128i result = _mm_or_si128(_mm_and_si128(is_lower, shifted), _mm_andnot_si128(is_lower, chunk));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <emmintrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), ' ');\n    __m128i shift = _mm_set1_epi8(-5);\n    __m128i lowercase_a = _mm_set1_epi8('a');\n    __m128i mod_26 = _mm_set1_epi8(26);\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i shifted = _mm_add_epi8(chunk, shift);\n\n        __m128i adjusted = _mm_sub_epi8(shifted, lowercase_a);\n        __m128i modulo = _mm_and_si128(_mm_add_epi8(adjusted, mod_26), _mm_set1_epi8(0x1F));\n        __m128i result = _mm_add_epi8(modulo, lowercase_a);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(const_cast<char*>(out.data() + i)), result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    const int shift = 5;\n    const __m128i alpha_start = _mm_set1_epi8('a');\n    const __m128i alpha_end = _mm_set1_epi8('z');\n    const __m128i shift_val = _mm_set1_epi8(shift);\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i decoded = _mm_subs_epu8(chunk, shift_val);\n\n        __m128i mask = _mm_cmpgt_epi8(decoded, alpha_end);\n        decoded = _mm_or_si128(_mm_and_si128(mask, _mm_add_epi8(decoded, _mm_set1_epi8(26))), _mm_andnot_si128(mask, decoded));\n\n        mask = _mm_cmplt_epi8(decoded, alpha_start);\n        decoded = _mm_or_si128(_mm_and_si128(mask, _mm_sub_epi8(decoded, _mm_set1_epi8(26))), _mm_andnot_si128(mask, decoded));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), decoded);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    const int shift = 5;\n    const __m128i alpha_start = _mm_set1_epi8('a');\n    const __m128i alpha_end = _mm_set1_epi8('z' + 1);\n    const __m128i shift_vec = _mm_set1_epi8(shift);\n    const __m128i mod_vec = _mm_set1_epi8(26);\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i decoded = _mm_subs_epu8(chunk, shift_vec);\n\n        __m128i mask = _mm_cmpgt_epi8(decoded, alpha_end);\n        __m128i adjusted = _mm_add_epi8(decoded, mod_vec);\n        decoded = _mm_or_si128(_mm_and_si128(mask, adjusted), _mm_andnot_si128(mask, decoded));\n\n        _mm_storeu_si128((__m128i*)&out[i], decoded);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string decode_shift_simd(std::string s) {\n    std::string out(s.length(), '\\0');\n    const int shift = 5;\n    const int alphabet_size = 26;\n    const char base = 'a';\n\n    __m128i vec_shift = _mm_set1_epi8(-shift);\n    __m128i vec_base = _mm_set1_epi8(base);\n    __m128i vec_alphabet_size = _mm_set1_epi8(alphabet_size);\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i vec_chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i vec_decoded = _mm_add_epi8(vec_chars, vec_shift);\n        vec_decoded = _mm_sub_epi8(vec_decoded, vec_base);\n        vec_decoded = _mm_rem_epi8(vec_decoded, vec_alphabet_size);\n        vec_decoded = _mm_add_epi8(vec_decoded, vec_base);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), vec_decoded);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    __m128i mask = _mm_setzero_si128();\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(chunk, threshold);\n        mask = _mm_and_si128(mask, cmp);\n    }\n\n    return _mm_movemask_epi8(mask) == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    __m128i zero = _mm_setzero_si128();\n    __m128i compare_result;\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        compare_result = _mm_cmplt_epi32(chunk, threshold);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(compare_result, zero)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = zero;\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(chunk, threshold);\n        result = _mm_and_si128(result, cmp);\n    }\n\n    int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(result, zero));\n    return mask == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    __m128i zero = _mm_setzero_si128();\n    int size = l.size();\n    int i;\n\n    for (i = 0; i <= size - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(chunk, threshold);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp, zero)) != 0) {\n            return false;\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < l.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(chunk, threshold);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp, zero)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('<');\n    __m128i closing = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        __m128i open_count = _mm_sub_epi8(zero, open_mask);\n        __m128i close_count = _mm_sub_epi8(zero, close_mask);\n\n        open_count = _mm_sad_epu8(open_count, zero);\n        close_count = _mm_sad_epu8(close_count, zero);\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(2, 3, 0, 1)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(2, 3, 0, 1)));\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(1, 0, 3, 2)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(1, 0, 3, 2)));\n\n        int open = _mm_cvtsi128_si32(open_count);\n        int close = _mm_cvtsi128_si32(close_count);\n\n        balance += open - close;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.length();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('<');\n    __m128i closing = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        int open_count = _mm_movemask_epi8(_mm_or_si128(open_mask, zero));\n        int close_count = _mm_movemask_epi8(_mm_or_si128(close_mask, zero));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int len = brackets.length();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('<');\n    __m128i closing = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        int open_count = _mm_movemask_epi8(_mm_cmpeq_epi8(open_mask, zero)) & 0xFFFF;\n        int close_count = _mm_movemask_epi8(_mm_cmpeq_epi8(close_mask, zero)) & 0xFFFF;\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i zeros = _mm_setzero_si128();\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, open);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, close);\n\n        int open_count = _mm_movemask_epi8(_mm_or_si128(open_mask, zeros));\n        int close_count = _mm_movemask_epi8(_mm_or_si128(close_mask, zeros));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, open);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, close);\n\n        int open_count = _mm_movemask_epi8(_mm_or_si128(open_mask, zero));\n        int close_count = _mm_movemask_epi8(_mm_or_si128(close_mask, zero));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n % 16 != 0) {\n        brackets.append(16 - n % 16, ' ');\n        n = brackets.size();\n    }\n\n    __m128i opening = _mm_set1_epi8('(');\n    __m128i closing = _mm_set1_epi8(')');\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        __m128i open_count = _mm_sad_epu8(open_mask, zero);\n        __m128i close_count = _mm_sad_epu8(close_mask, zero);\n\n        open_count = _mm_add_epi64(open_count, _mm_srli_si128(open_count, 8));\n        close_count = _mm_add_epi64(close_count, _mm_srli_si128(close_count, 8));\n\n        int open = _mm_cvtsi128_si32(open_count);\n        int close = _mm_cvtsi128_si32(close_count);\n\n        balance += open - close;\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('(');\n    __m128i closing = _mm_set1_epi8(')');\n    __m128i zeros = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        __m128i open_count = _mm_sub_epi8(zeros, open_mask);\n        __m128i close_count = _mm_sub_epi8(zeros, close_mask);\n\n        open_count = _mm_sad_epu8(open_count, zeros);\n        close_count = _mm_sad_epu8(close_count, zeros);\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(2, 3, 0, 1)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(2, 3, 0, 1)));\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(1, 0, 3, 2)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(1, 0, 3, 2)));\n\n        int open = _mm_cvtsi128_si32(open_count);\n        int close = _mm_cvtsi128_si32(close_count);\n\n        balance += open - close;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('(');\n    __m128i closing = _mm_set1_epi8(')');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        int open_count = _mm_movemask_epi8(_mm_cmpeq_epi8(open_mask, zero));\n        int close_count = _mm_movemask_epi8(_mm_cmpeq_epi8(close_mask, zero));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n % 16 != 0) {\n        brackets.append(16 - n % 16, ' ');\n        n = brackets.size();\n    }\n\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    __m128i zero = _mm_setzero_si128();\n    int balance = 0;\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, open);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, close);\n\n        __m128i open_count = _mm_sub_epi8(zero, open_mask);\n        __m128i close_count = _mm_sub_epi8(zero, close_mask);\n\n        open_count = _mm_sad_epu8(open_count, zero);\n        close_count = _mm_sad_epu8(close_count, zero);\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(2, 3, 0, 1)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(2, 3, 0, 1)));\n\n        open_count = _mm_add_epi32(open_count, _mm_shuffle_epi32(open_count, _MM_SHUFFLE(1, 0, 3, 2)));\n        close_count = _mm_add_epi32(close_count, _mm_shuffle_epi32(close_count, _MM_SHUFFLE(1, 0, 3, 2)));\n\n        int open_sum = _mm_cvtsi128_si32(open_count);\n        int close_sum = _mm_cvtsi128_si32(close_count);\n\n        balance += open_sum - close_sum;\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    int balance = 0;\n\n    __m128i opening = _mm_set1_epi8('(');\n    __m128i closing = _mm_set1_epi8(')');\n    __m128i zeros = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, opening);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, closing);\n\n        int open_count = _mm_movemask_epi8(_mm_cmpeq_epi8(open_mask, zeros));\n        int close_count = _mm_movemask_epi8(_mm_cmpeq_epi8(close_mask, zeros));\n\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n\n    __m128 coeff, mult, res;\n    float multipliers[4];\n    int i;\n\n    for (i = 0; i <= xs.size() - 4; i += 4) {\n        coeff = _mm_loadu_ps(&xs[i]);\n        multipliers[0] = i + 1;\n        multipliers[1] = i + 2;\n        multipliers[2] = i + 3;\n        multipliers[3] = i + 4;\n        mult = _mm_loadu_ps(multipliers);\n        res = _mm_mul_ps(coeff, mult);\n        _mm_storeu_ps(&result[i], res);\n    }\n\n    for (; i < xs.size() - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n\n    for (size_t i = 1; i < xs.size(); i += 4) {\n        __m128 coeff = _mm_loadu_ps(&xs[i]);\n        __m128 factor = _mm_set_ps(i+3, i+2, i+1, i);\n        __m128 deriv = _mm_mul_ps(coeff, factor);\n        _mm_storeu_ps(&result[i-1], deriv);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result(xs.size() - 1);\n\n    __m128 coeff, mult, result_vec;\n    float multipliers[4] = {1.0, 2.0, 3.0, 4.0};\n\n    for (size_t i = 0; i < xs.size() - 1; i += 4) {\n        coeff = _mm_loadu_ps(&xs[i + 1]);\n        mult = _mm_loadu_ps(multipliers);\n\n        result_vec = _mm_mul_ps(coeff, mult);\n\n        _mm_storeu_ps(&result[i], result_vec);\n\n        multipliers[0] += 4.0;\n        multipliers[1] += 4.0;\n        multipliers[2] += 4.0;\n        multipliers[3] += 4.0;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result;\n    if (xs.size() < 2) return result;\n\n    __m128 coeff, mult, res;\n    float temp[4];\n    int i;\n\n    for (i = 1; i < xs.size() - 3; i += 4) {\n        coeff = _mm_loadu_ps(&xs[i]);\n        mult = _mm_set_ps(i+3, i+2, i+1, i);\n        res = _mm_mul_ps(coeff, mult);\n        _mm_storeu_ps(temp, res);\n        result.insert(result.end(), temp, temp + 4);\n    }\n\n    for (; i < xs.size(); ++i) {\n        result.push_back(xs[i] * i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    std::vector<float> result;\n    if (xs.size() < 2) return result;\n\n    __m128 coeffs, derivs;\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128 factor = _mm_cvtepi32_ps(_mm_add_epi32(indices, one));\n\n    for (size_t i = 0; i < xs.size() - 1; i += 4) {\n        coeffs = _mm_loadu_ps(&xs[i + 1]);\n        derivs = _mm_mul_ps(coeffs, factor);\n\n        _mm_storeu_ps(&xs[i], derivs);\n    }\n\n    result.resize(xs.size() - 1);\n    for (size_t i = 0; i < xs.size() - 1; ++i) {\n        result[i] = xs[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n    int count = 0;\n\n    // Define vowel masks\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_e = _mm_set1_epi8('e');\n    __m128i vowels_i = _mm_set1_epi8('i');\n    __m128i vowels_o = _mm_set1_epi8('o');\n    __m128i vowels_u = _mm_set1_epi8('u');\n    __m128i vowels_y = _mm_set1_epi8('y');\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(str + i));\n\n        __m128i mask_a = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i mask_e = _mm_cmpeq_epi8(chunk, vowels_e);\n        __m128i mask_i = _mm_cmpeq_epi8(chunk, vowels_i);\n        __m128i mask_o = _mm_cmpeq_epi8(chunk, vowels_o);\n        __m128i mask_u = _mm_cmpeq_epi8(chunk, vowels_u);\n\n        mask_a = _mm_or_si128(mask_a, mask_e);\n        mask_a = _mm_or_si128(mask_a, mask_i);\n        mask_a = _mm_or_si128(mask_a, mask_o);\n        mask_a = _mm_or_si128(mask_a, mask_u);\n\n        int mask = _mm_movemask_epi8(mask_a);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Handle remaining characters\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == len - 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    if (len == 0) return 0;\n\n    // Convert the string to lowercase for uniformity\n    std::transform(s.begin(), s.end(), s.begin(), ::tolower);\n\n    // Prepare the vowel mask\n    __m128i vowels = _mm_set1_epi8('a');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('e'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('i'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('o'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('u'));\n\n    int count = 0;\n    int i = 0;\n\n    // Process 16 bytes at a time\n    for (; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i mask = _mm_cmpeq_epi8(chunk, vowels);\n        mask = _mm_and_si128(mask, _mm_set1_epi8(1));\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u') {\n            count++;\n        }\n    }\n\n    // Check if 'y' is at the end of the string\n    if (s.back() == 'y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nint vowels_count_simd(std::string s) {\n    int count = 0;\n    int len = s.length();\n    char *str = const_cast<char*>(s.c_str());\n\n    __m128i vowels = _mm_set1_epi8('a');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('e'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('i'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('o'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('u'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('A'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i result = _mm_and_si128(chunk, vowels);\n        result = _mm_cmpeq_epi8(result, vowels);\n        int mask = _mm_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        if (strchr(\"aeiouAEIOU\", str[i]) != nullptr) {\n            count++;\n        }\n    }\n\n    if (len > 0 && (str[len - 1] == 'y' || str[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n    int count = 0;\n\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_e = _mm_set1_epi8('e');\n    __m128i vowels_i = _mm_set1_epi8('i');\n    __m128i vowels_o = _mm_set1_epi8('o');\n    __m128i vowels_u = _mm_set1_epi8('u');\n    __m128i vowels_y = _mm_set1_epi8('y');\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(str + i));\n\n        __m128i mask_a = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i mask_e = _mm_cmpeq_epi8(chunk, vowels_e);\n        __m128i mask_i = _mm_cmpeq_epi8(chunk, vowels_i);\n        __m128i mask_o = _mm_cmpeq_epi8(chunk, vowels_o);\n        __m128i mask_u = _mm_cmpeq_epi8(chunk, vowels_u);\n\n        __m128i mask = _mm_or_si128(mask_a, _mm_or_si128(mask_e, _mm_or_si128(mask_i, _mm_or_si128(mask_o, mask_u))));\n\n        int local_count = _mm_movemask_epi8(mask);\n        count += _mm_popcnt_u32(local_count);\n    }\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        char c = tolower(str[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == len - 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nint vowels_count_simd(std::string s) {\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n    int count = 0;\n\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_e = _mm_set1_epi8('e');\n    __m128i vowels_i = _mm_set1_epi8('i');\n    __m128i vowels_o = _mm_set1_epi8('o');\n    __m128i vowels_u = _mm_set1_epi8('u');\n    __m128i vowels_y = _mm_set1_epi8('y');\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(str + i));\n\n        __m128i mask_a = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i mask_e = _mm_cmpeq_epi8(chunk, vowels_e);\n        __m128i mask_i = _mm_cmpeq_epi8(chunk, vowels_i);\n        __m128i mask_o = _mm_cmpeq_epi8(chunk, vowels_o);\n        __m128i mask_u = _mm_cmpeq_epi8(chunk, vowels_u);\n\n        mask_a = _mm_or_si128(mask_a, mask_e);\n        mask_a = _mm_or_si128(mask_a, mask_i);\n        mask_a = _mm_or_si128(mask_a, mask_o);\n        mask_a = _mm_or_si128(mask_a, mask_u);\n\n        int mask = _mm_movemask_epi8(mask_a);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        if (str[i] == 'a' || str[i] == 'e' || str[i] == 'i' || str[i] == 'o' || str[i] == 'u') {\n            count++;\n        }\n    }\n\n    if (s.back() == 'y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n\n    __m128i* ptr = (__m128i*)str;\n    __m128i upperCaseMask = _mm_set1_epi8('A');\n    __m128i lowerCaseMask = _mm_set1_epi8('a');\n    __m128i offset = _mm_set1_epi8('a' - 'A');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(ptr++);\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epi8(chunk, upperCaseMask), _mm_cmplt_epi8(chunk, lowerCaseMask));\n        __m128i upperChunk = _mm_or_si128(_mm_and_si128(isUpper, chunk), _mm_andnot_si128(isUpper, zero));\n        __m128i sums = _mm_sad_epu8(upperChunk, zero);\n        sum += _mm_extract_epi16(sums, 0) + _mm_extract_epi16(sums, 4);\n    }\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n\n    __m128i* ptr = (__m128i*)str;\n    __m128i upperCaseMask = _mm_set1_epi8('A');\n    __m128i lowerCaseMask = _mm_set1_epi8('a');\n    __m128i offset = _mm_set1_epi8('a' - 'A');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(ptr + (i / 16));\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epi8(_mm_subs_epu8(chunk, upperCaseMask)), _mm_cmplt_epi8(chunk, lowerCaseMask));\n        __m128i upperChars = _mm_and_si128(chunk, isUpper);\n        __m128i upperSum = _mm_sad_epu8(upperChars, zero);\n        sum += _mm_extract_epi16(upperSum, 0) + _mm_extract_epi16(upperSum, 4);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 16 bytes at a time\n    for (; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i upperMask = _mm_set1_epi8('A');\n        __m128i lowerMask = _mm_set1_epi8('Z');\n        __m128i upperCheck = _mm_and_si128(_mm_cmpge_epi8(chunk, upperMask), _mm_cmple_epi8(chunk, lowerMask));\n\n        int mask = _mm_movemask_epi8(upperCheck);\n        while (mask) {\n            int index = __builtin_ctz(mask);\n            sum += s[i + index];\n            mask &= ~(1 << index);\n        }\n    }\n\n    // Process remaining bytes\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = zero;\n    __m128i upper_mask = _mm_set1_epi8('A');\n    __m128i lower_mask = _mm_set1_epi8('Z');\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<__m128i*>(str + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(_mm_subs_epu8(chunk, upper_mask), zero), _mm_cmplt_epi8(_mm_subs_epu8(chunk, lower_mask), zero));\n        __m128i upper_chars = _mm_and_si128(chunk, is_upper);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_sad_epu8(upper_chars, zero));\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum_vec);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.length();\n    char* str = const_cast<char*>(s.c_str());\n\n    __m128i* ptr = (__m128i*)str;\n    __m128i upperCaseMask = _mm_set1_epi8('A');\n    __m128i lowerCaseMask = _mm_set1_epi8('a');\n    __m128i offset = _mm_set1_epi8('a' - 'A');\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(ptr++);\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epi8(_mm_subs_epu8(chunk, upperCaseMask), zero), _mm_cmplt_epi8(_mm_subs_epu8(chunk, lowerCaseMask), zero));\n        __m128i upperChars = _mm_and_si128(chunk, isUpper);\n        __m128i upperSum = _mm_sad_epu8(upperChars, zero);\n        sum += _mm_extract_epi16(upperSum, 0) + _mm_extract_epi16(upperSum, 4);\n    }\n\n    for (int i = (len / 16) * 16; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i min_idx = _mm_set1_epi32(INT_MAX);\n    __m128i idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i even_mask = _mm_andnot_si128(_mm_set1_epi32(1), chunk);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n\n        __m128i min_chunk = _mm_min_epu32(chunk, min_val);\n        __m128i min_chunk_idx = _mm_add_epi32(_mm_set1_epi32(i), idx);\n\n        min_val = _mm_min_epu32(min_chunk, min_val);\n        min_idx = _mm_min_epu32(min_chunk_idx, min_idx);\n    }\n\n    int min_value = _mm_extract_epi32(min_val, 0);\n    int min_index = _mm_extract_epi32(min_idx, 0);\n\n    if (min_value == INT_MAX) return {};\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    __m128i minVal = _mm_set1_epi32(INT_MAX);\n    __m128i minIdx = _mm_set1_epi32(INT_MAX);\n    __m128i idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i evenMask = _mm_andnot_si128(_mm_set1_epi32(1), chunk);\n        __m128i isEven = _mm_cmpeq_epi32(evenMask, _mm_setzero_si128());\n\n        __m128i currentIdx = _mm_add_epi32(idx, _mm_set1_epi32(i));\n        __m128i currentMin = _mm_min_epi32(chunk, minVal);\n\n        __m128i updateMask = _mm_and_si128(isEven, _mm_cmplt_epi32(chunk, minVal));\n        minVal = _mm_blendv_epi8(minVal, currentMin, updateMask);\n        minIdx = _mm_blendv_epi8(minIdx, currentIdx, updateMask);\n    }\n\n    int minValues[4];\n    int minIndices[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(minValues), minVal);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(minIndices), minIdx);\n\n    int smallestValue = INT_MAX;\n    int smallestIndex = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (minValues[j] < smallestValue) {\n            smallestValue = minValues[j];\n            smallestIndex = minIndices[j];\n        }\n    }\n\n    if (smallestValue == INT_MAX) return {};\n    return {smallestValue, smallestIndex};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i min_idx = _mm_set1_epi32(INT_MAX);\n    __m128i idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i even_mask = _mm_andnot_si128(_mm_set1_epi32(1), chunk);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n\n        __m128i min_chunk = _mm_min_epu32(chunk, min_val);\n        __m128i min_idx_chunk = _mm_add_epi32(_mm_and_si128(is_even, idx), _mm_set1_epi32(i));\n        min_idx = _mm_min_epu32(min_idx_chunk, min_idx);\n\n        min_val = min_chunk;\n        idx = _mm_add_epi32(idx, _mm_set1_epi32(4));\n    }\n\n    int min_vals[4];\n    int min_idxs[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(min_vals), min_val);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(min_idxs), min_idx);\n\n    int smallest_val = INT_MAX;\n    int smallest_idx = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (min_vals[j] < smallest_val) {\n            smallest_val = min_vals[j];\n            smallest_idx = min_idxs[j];\n        }\n    }\n\n    if (smallest_val == INT_MAX) return {};\n    return {smallest_val, smallest_idx};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i min_idx = _mm_set1_epi32(INT_MAX);\n    __m128i idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i even_mask = _mm_andnot_si128(_mm_set1_epi32(1), chunk);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n\n        __m128i curr_idx = _mm_add_epi32(idx, _mm_set1_epi32(i));\n        __m128i curr_min = _mm_min_epi32(chunk, min_val);\n\n        min_val = _mm_or_si128(_mm_and_si128(is_even, curr_min), _mm_andnot_si128(is_even, min_val));\n        min_idx = _mm_or_si128(_mm_and_si128(is_even, curr_idx), _mm_andnot_si128(is_even, min_idx));\n\n        idx = _mm_add_epi32(idx, _mm_set1_epi32(4));\n    }\n\n    int min_vals[4];\n    int min_idxs[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(min_vals), min_val);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(min_idxs), min_idx);\n\n    int smallest_val = INT_MAX;\n    int smallest_idx = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (min_vals[j] < smallest_val) {\n            smallest_val = min_vals[j];\n            smallest_idx = min_idxs[j];\n        }\n    }\n\n    if (smallest_val == INT_MAX) return {};\n    return {smallest_val, smallest_idx};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    __m128i minVal = _mm_set1_epi32(INT_MAX);\n    __m128i minIdx = _mm_set1_epi32(INT_MAX);\n    __m128i idx = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i evenMask = _mm_and_si128(_mm_set1_epi32(1), _mm_srli_epi32(chunk, 31));\n        __m128i evenChunk = _mm_and_si128(chunk, evenMask);\n\n        __m128i cmp = _mm_cmplt_epi32(evenChunk, minVal);\n        minVal = _mm_blendv_epi8(minVal, evenChunk, cmp);\n        minIdx = _mm_blendv_epi8(minIdx, _mm_add_epi32(idx, _mm_set1_epi32(i)), cmp);\n    }\n\n    int minValues[4];\n    int minIndices[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(minValues), minVal);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(minIndices), minIdx);\n\n    int smallestValue = INT_MAX;\n    int smallestIndex = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (minValues[j] < smallestValue) {\n            smallestValue = minValues[j];\n            smallestIndex = minIndices[j];\n        }\n    }\n\n    if (smallestValue == INT_MAX) return {};\n    return {smallestValue, smallestIndex};\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_freq = -1;\n    int max_value = -1;\n\n    for (int i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i freq = _mm_setzero_si128();\n\n        for (int j = 0; j < lst.size(); j += 4) {\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[j]));\n            __m128i eq = _mm_cmpeq_epi32(v, v2);\n            freq = _mm_add_epi32(freq, eq);\n        }\n\n        int freq_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(freq_arr), freq);\n\n        for (int k = 0; k < 4; ++k) {\n            if (i + k < lst.size() && freq_arr[k] >= lst[i + k] && freq_arr[k] > max_freq) {\n                max_freq = freq_arr[k];\n                max_value = lst[i + k];\n            }\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int size = lst.size();\n    int max_freq = -1;\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i freq = _mm_setzero_si128();\n\n        for (int j = 0; j < size; j += 4) {\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[j]));\n            __m128i eq = _mm_cmpeq_epi32(v, v2);\n            freq = _mm_add_epi32(freq, eq);\n        }\n\n        int freq_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(freq_arr), freq);\n\n        for (int k = 0; k < 4; ++k) {\n            if (i + k < size && freq_arr[k] >= lst[i + k] && freq_arr[k] > max_freq) {\n                max_freq = lst[i + k];\n            }\n        }\n    }\n\n    return max_freq;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int size = lst.size();\n    const int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> frequency(max_val + 1, 0);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i ones = _mm_set1_epi32(1);\n\n        __m128i mask0 = _mm_cmpeq_epi32(v, _mm_set1_epi32(1));\n        __m128i mask1 = _mm_cmpeq_epi32(v, _mm_set1_epi32(2));\n        __m128i mask2 = _mm_cmpeq_epi32(v, _mm_set1_epi32(3));\n        __m128i mask3 = _mm_cmpeq_epi32(v, _mm_set1_epi32(4));\n\n        int count0 = _mm_movemask_epi8(_mm_cmpeq_epi32(v, _mm_set1_epi32(1)));\n        int count1 = _mm_movemask_epi8(_mm_cmpeq_epi32(v, _mm_set1_epi32(2)));\n        int count2 = _mm_movemask_epi8(_mm_cmpeq_epi32(v, _mm_set1_epi32(3)));\n        int count3 = _mm_movemask_epi8(_mm_cmpeq_epi32(v, _mm_set1_epi32(4)));\n\n        frequency[1] += __builtin_popcount(count0);\n        frequency[2] += __builtin_popcount(count1);\n        frequency[3] += __builtin_popcount(count2);\n        frequency[4] += __builtin_popcount(count3);\n    }\n\n    for (int i = max_val; i >= 1; --i) {\n        if (frequency[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_freq = -1;\n    int max_value = -1;\n\n    for (int i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i freq = _mm_setzero_si128();\n\n        for (int j = 0; j < lst.size(); j += 4) {\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[j]));\n            __m128i cmp = _mm_cmpeq_epi32(v, v2);\n            freq = _mm_add_epi32(freq, cmp);\n        }\n\n        int freq_arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(freq_arr), freq);\n\n        for (int k = 0; k < 4; ++k) {\n            if (i + k < lst.size() && freq_arr[k] >= lst[i + k] && freq_arr[k] > max_freq) {\n                max_freq = freq_arr[k];\n                max_value = lst[i + k];\n            }\n        }\n    }\n\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = -1;\n    int n = lst.size();\n\n    // Initialize frequency array\n    int freq[5] = {0}; // Assuming the maximum value in the list is 4\n\n    for (int i = 0; i < n; i += 4) {\n        // Load 4 integers from the list\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        // Increment frequency count for each integer\n        for (int j = 0; j < 4; ++j) {\n            int val = _mm_extract_epi32(v, j);\n            if (val > 0 && val <= 4) {\n                freq[val]++;\n            }\n        }\n    }\n\n    // Find the greatest integer that meets the criteria\n    for (int i = 4; i >= 1; --i) {\n        if (freq[i] >= i) {\n            max_val = i;\n            break;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using SIMD\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 0; i < n - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum = _mm_add_epi32(sum, chunk);\n    }\n\n    // Horizontally add the elements of the sum vector\n    __m128i shuf = _mm_shuffle_epi32(sum, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sum2 = _mm_add_epi32(sum, shuf);\n    shuf = _mm_shuffle_epi32(sum2, _MM_SHUFFLE(0, 1, 2, 3));\n    sum2 = _mm_add_epi32(sum2, shuf);\n\n    // Add any remaining elements\n    int remainder_sum = 0;\n    for (int i = n - (n % 4); i < n; ++i) {\n        remainder_sum += q[i];\n    }\n\n    // Combine the SIMD sum with the remainder sum\n    int total_sum = _mm_cvtsi128_si32(sum2) + remainder_sum;\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    // Calculate the sum of elements using SSE\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 0; i < n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum = _mm_add_epi32(sum, chunk);\n    }\n\n    // Horizontal add to get the final sum\n    int sums[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sums), sum);\n    int total_sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    // Calculate the sum of the elements using SSE\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 0; i < n; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum = _mm_add_epi32(sum, chunk);\n    }\n\n    // Horizontally add the elements of the sum vector\n    int sum_elements[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum_elements), sum);\n    int total_sum = sum_elements[0] + sum_elements[1] + sum_elements[2] + sum_elements[3];\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    // Calculate the sum of the elements using SSE\n    __m128i sum = _mm_setzero_si128();\n    __m128i* ptr = reinterpret_cast<__m128i*>(q.data());\n    int numElements = n / 4;\n    int remainder = n % 4;\n\n    for (int i = 0; i < numElements; ++i) {\n        sum = _mm_add_epi32(sum, ptr[i]);\n    }\n\n    // Horizontally add the elements of the sum vector\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int totalSum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add the remainder elements\n    for (int i = 0; i < remainder; ++i) {\n        totalSum += q[numElements * 4 + i];\n    }\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    return totalSum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is palindromic\n    for (int i = 0; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) return false;\n    }\n\n    // Calculate the sum of the elements using SSE\n    __m128i sum = _mm_setzero_si128();\n    for (int i = 0; i < n - 3; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum = _mm_add_epi32(sum, chunk);\n    }\n\n    // Horizontally add the elements of the sum vector\n    __m128i shuf = _mm_shuffle_epi32(sum, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128i sum2 = _mm_add_epi32(sum, shuf);\n    shuf = _mm_shuffle_epi32(sum2, _MM_SHUFFLE(0, 1, 2, 3));\n    sum2 = _mm_add_epi32(sum2, shuf);\n\n    // Add any remaining elements\n    int remaining_sum = _mm_cvtsi128_si32(sum2);\n    for (int i = n - (n % 4); i < n; ++i) {\n        remaining_sum += q[i];\n    }\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    return remaining_sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    for (; i < j && (j - i + 1) >= 4; i += 4, j -= 4) {\n        __m128i vi = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[i]));\n        __m128i vj = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[j]));\n        vj = _mm_shuffle_epi32(vj, _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m128i cmp = _mm_cmpeq_epi32(vi, vj);\n        int mask = _mm_movemask_epi8(cmp);\n        changes += _mm_popcnt_u32(~mask & 0xFFFF);\n    }\n\n    for (; i < j; ++i, --j) {\n        if (arr[i] != arr[j]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 4) {\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[i]));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[n - i - 1]));\n        __m128i cmp = _mm_cmpeq_epi32(v1, _mm_setr_epi32(arr[n - i - 4], arr[n - i - 3], arr[n - i - 2], arr[n - i - 1]));\n\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n        changes += _mm_popcnt_u32(~mask & 0xFFFF);\n    }\n\n    for (int i = (n / 2) * 4; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 4) {\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[i]));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[n - i - 1]));\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n        int mask = _mm_movemask_epi8(cmp);\n\n        changes += _mm_popcnt_u32(~mask & 0xFFFF);\n    }\n\n    if (n % 4 != 0) {\n        int remaining = n % 4;\n        for (int i = n / 2 - remaining; i < n / 2; ++i) {\n            if (arr[i] != arr[n - i - 1]) {\n                ++changes;\n            }\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 4) {\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[i]));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&arr[n - i - 4]));\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n        int mask = _mm_movemask_epi8(cmp);\n\n        changes += _mm_popcnt_u32(~mask & 0xFFFF);\n    }\n\n    for (int i = (n / 2) * 2; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; i += 4) {\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[n - i - 4]));\n        v2 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(0, 1, 2, 3));\n\n        __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n        int mask = _mm_movemask_epi8(cmp);\n\n        changes += _mm_popcnt_u32(~mask & 0xFFFF);\n    }\n\n    for (int i = (n / 2) * 4; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (const auto& str : lst1) {\n        int len = str.length();\n        totalChars1 += len;\n        sum1 = _mm_add_epi32(sum1, _mm_set1_epi32(len));\n    }\n\n    for (const auto& str : lst2) {\n        int len = str.length();\n        totalChars2 += len;\n        sum2 = _mm_add_epi32(sum2, _mm_set1_epi32(len));\n    }\n\n    sum1 = _mm_hadd_epi32(sum1, sum1);\n    sum2 = _mm_hadd_epi32(sum2, sum2);\n\n    int sum1_total = _mm_extract_epi32(sum1, 0) + _mm_extract_epi32(sum1, 1) + _mm_extract_epi32(sum1, 2) + _mm_extract_epi32(sum1, 3);\n    int sum2_total = _mm_extract_epi32(sum2, 0) + _mm_extract_epi32(sum2, 1) + _mm_extract_epi32(sum2, 2) + _mm_extract_epi32(sum2, 3);\n\n    if (totalChars1 < totalChars2) {\n        return lst1;\n    } else if (totalChars2 < totalChars1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    if (totalChars1 < totalChars2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m128i vec1 = _mm_set1_epi32(totalChars1);\n    __m128i vec2 = _mm_set1_epi32(totalChars2);\n\n    __m128i result = _mm_cmpgt_epi32(vec1, vec2);\n\n    int mask = _mm_movemask_epi8(result);\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i total1 = _mm_setzero_si128();\n    __m128i total2 = _mm_setzero_si128();\n\n    for (const auto& str : lst1) {\n        __m128i len = _mm_set1_epi32(str.length());\n        total1 = _mm_add_epi32(total1, len);\n    }\n\n    for (const auto& str : lst2) {\n        __m128i len = _mm_set1_epi32(str.length());\n        total2 = _mm_add_epi32(total2, len);\n    }\n\n    __m128i sum1 = _mm_add_epi32(total1, _mm_shuffle_epi32(total1, _MM_SHUFFLE(2, 3, 0, 1)));\n    __m128i sum2 = _mm_add_epi32(total2, _mm_shuffle_epi32(total2, _MM_SHUFFLE(2, 3, 0, 1)));\n\n    sum1 = _mm_add_epi32(sum1, _mm_shuffle_epi32(sum1, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum2 = _mm_add_epi32(sum2, _mm_shuffle_epi32(sum2, _MM_SHUFFLE(1, 0, 3, 2)));\n\n    int totalChars1 = _mm_cvtsi128_si32(sum1);\n    int totalChars2 = _mm_cvtsi128_si32(sum2);\n\n    if (totalChars1 < totalChars2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    int totalChars1 = 0;\n    int totalChars2 = 0;\n\n    for (const auto& str : lst1) {\n        totalChars1 += str.length();\n    }\n\n    for (const auto& str : lst2) {\n        totalChars2 += str.length();\n    }\n\n    __m128i vec1 = _mm_set1_epi32(totalChars1);\n    __m128i vec2 = _mm_set1_epi32(totalChars2);\n\n    __m128i result = _mm_cmpgt_epi32(vec1, vec2);\n    int mask = _mm_movemask_epi8(_mm_packs_epi32(result, result));\n\n    if (mask == 0) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i x_vec = _mm_set1_epi64x(x);\n    __m128i one_vec = _mm_set1_epi64x(1);\n    __m128i result = one_vec;\n\n    for (int i = 0; i < 64; i++) {\n        if (_mm_extract_epi64(result, 0) == x) return true;\n        result = _mm_mul_epu32(result, n_vec);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i x_vec = _mm_set1_epi64x(x);\n    __m128i one_vec = _mm_set1_epi64x(1);\n    __m128i power_vec = _mm_set1_epi64x(1);\n\n    for (int i = 0; i < 64; i++) {\n        power_vec = _mm_mul_epu32(power_vec, n_vec);\n        __m128i cmp_vec = _mm_cmpeq_epi64(power_vec, x_vec);\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(cmp_vec, cmp_vec), cmp_vec)) == 0xFFFF) {\n            return true;\n        }\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(_mm_cmpgt_epi64(power_vec, x_vec), _mm_cmpgt_epi64(power_vec, x_vec)), _mm_cmpgt_epi64(power_vec, x_vec))) == 0xFFFF) {\n            return false;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m128d n_vec = _mm_set1_pd((double)n);\n    __m128d x_vec = _mm_set1_pd((double)x);\n    __m128d one_vec = _mm_set1_pd(1.0);\n    __m128d result_vec = one_vec;\n\n    for (int i = 0; i < 2; ++i) {\n        result_vec = _mm_mul_pd(result_vec, n_vec);\n        __m128d cmp_vec = _mm_cmpeq_pd(result_vec, x_vec);\n        int mask = _mm_movemask_pd(cmp_vec);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (x == 0 || n == 0 || n == 1) return false;\n\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i x_vec = _mm_set1_epi64x(x);\n    __m128i one_vec = _mm_set1_epi64x(1);\n    __m128i power_vec = _mm_set1_epi64x(n);\n\n    for (int i = 0; i < 64; i++) {\n        __m128i cmp_result = _mm_cmpeq_epi64(power_vec, x_vec);\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(cmp_result, cmp_result), _mm_setzero_si128())) == 0xFFFF) {\n            return true;\n        }\n        power_vec = _mm_mul_epu32(power_vec, n_vec);\n        if (_mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(_mm_cmplt_epu64(power_vec, x_vec), _mm_setzero_si128()), _mm_setzero_si128())) == 0) {\n            break;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdbool.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (x == 1 && n == 1) return true;\n    if (n == 1) return false;\n\n    __m128i n_vec = _mm_set1_epi64x(n);\n    __m128i x_vec = _mm_set1_epi64x(x);\n    __m128i one_vec = _mm_set1_epi64x(1);\n    __m128i result = one_vec;\n    __m128i power = one_vec;\n\n    for (int i = 0; i < 64; i++) {\n        power = _mm_mul_epu32(power, n_vec);\n        result = _mm_or_si128(result, _mm_cmpeq_epi64(power, x_vec));\n    }\n\n    uint64_t mask = _mm_movemask_epi8(_mm_packs_epi16(_mm_packs_epi32(result, result), result));\n    return (mask != 0);\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    int len = num.length();\n    int count = 0;\n\n    __m128i primeMask = _mm_set1_epi8(0);\n    primeMask = _mm_insert_epi8(primeMask, '2', 0);\n    primeMask = _mm_insert_epi8(primeMask, '3', 1);\n    primeMask = _mm_insert_epi8(primeMask, '5', 2);\n    primeMask = _mm_insert_epi8(primeMask, '7', 3);\n    primeMask = _mm_insert_epi8(primeMask, 'B', 4);\n    primeMask = _mm_insert_epi8(primeMask, 'D', 5);\n\n    for (int i = 0; i < len; i += 16) {\n        int remaining = len - i;\n        int chunkSize = (remaining >= 16) ? 16 : remaining;\n\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(hexChars + i));\n        __m128i result = _mm_cmpeq_epi8(chunk, primeMask);\n\n        unsigned int mask = _mm_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    int len = num.length();\n    int count = 0;\n\n    __m128i primeMask = _mm_set1_epi8(0);\n    primeMask = _mm_insert_epi8(primeMask, '2', 0);\n    primeMask = _mm_insert_epi8(primeMask, '3', 1);\n    primeMask = _mm_insert_epi8(primeMask, '5', 2);\n    primeMask = _mm_insert_epi8(primeMask, '7', 3);\n    primeMask = _mm_insert_epi8(primeMask, 'B', 4);\n    primeMask = _mm_insert_epi8(primeMask, 'D', 5);\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(hexChars + i));\n        __m128i result = _mm_cmpeq_epi8(chunk, primeMask);\n\n        unsigned int mask = _mm_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m128i prime_hex = _mm_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.length();\n    size_t i;\n\n    for (i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&num[i]);\n        __m128i mask = _mm_cmpeq_epi8(chunk, prime_hex);\n        mask = _mm_and_si128(mask, _mm_set1_epi8(1));\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const __m128i prime_hex = _mm_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.length();\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(num.data() + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, prime_hex);\n        mask = _mm_and_si128(mask, _mm_set1_epi8(1));\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    for (size_t i = len - (len % 16); i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const char* hexChars = num.c_str();\n    int len = num.length();\n    int count = 0;\n\n    __m128i primeMask = _mm_set1_epi8(0);\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('2'));\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('3'));\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('5'));\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('7'));\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('B'));\n    primeMask = _mm_add_epi8(primeMask, _mm_set1_epi8('D'));\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(hexChars + i));\n        __m128i result = _mm_cmpeq_epi8(chunk, primeMask);\n        result = _mm_and_si128(result, _mm_set1_epi8(1));\n        count += _mm_popcnt_u32(_mm_movemask_epi8(result));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i prev_prev, prev, current;\n    prev_prev = _mm_set1_epi8(s[0]);\n    prev = _mm_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < s.length(); ++i) {\n        current = _mm_set1_epi8(s[i]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(prev_prev, prev);\n        __m128i cmp2 = _mm_cmpeq_epi8(prev, current);\n        __m128i cmp3 = _mm_cmpeq_epi8(prev_prev, current);\n\n        __m128i result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n\n        if (_mm_movemask_epi8(result) != 0) {\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i prev_prev, prev, current;\n    prev_prev = _mm_set1_epi8(s[0]);\n    prev = _mm_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < s.length(); ++i) {\n        current = _mm_set1_epi8(s[i]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(prev_prev, prev);\n        __m128i cmp2 = _mm_cmpeq_epi8(prev, current);\n        __m128i cmp3 = _mm_cmpeq_epi8(prev_prev, current);\n\n        __m128i result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), cmp3);\n\n        if (_mm_movemask_epi8(result) != 0) {\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i prev_prev, prev, current;\n    prev_prev = _mm_set1_epi8(s[0]);\n    prev = _mm_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < s.length(); ++i) {\n        current = _mm_set1_epi8(s[i]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(prev_prev, prev);\n        __m128i cmp2 = _mm_cmpeq_epi8(prev, current);\n        __m128i cmp3 = _mm_cmpeq_epi8(prev_prev, current);\n\n        __m128i result = _mm_or_si128(cmp1, cmp2);\n        result = _mm_or_si128(result, cmp3);\n\n        if (_mm_movemask_epi8(result) != 0) {\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i prev2 = _mm_set1_epi8(s[0]);\n    __m128i prev1 = _mm_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < s.length(); i += 16) {\n        __m128i current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i distinct1 = _mm_cmpeq_epi8(prev1, current);\n        __m128i distinct2 = _mm_cmpeq_epi8(prev2, current);\n        __m128i distinct3 = _mm_cmpeq_epi8(prev2, prev1);\n\n        __m128i distinct = _mm_or_si128(distinct1, distinct2);\n        distinct = _mm_or_si128(distinct, distinct3);\n\n        if (_mm_movemask_epi8(distinct) != 0) {\n            return false;\n        }\n\n        prev2 = _mm_srli_si128(prev2, 1);\n        prev2 = _mm_insert_epi8(prev2, s[i], 15);\n        prev1 = _mm_srli_si128(prev1, 1);\n        prev1 = _mm_insert_epi8(prev1, s[i + 1], 15);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i prev_prev, prev, current;\n    prev_prev = _mm_set1_epi8(s[0]);\n    prev = _mm_set1_epi8(s[1]);\n\n    for (size_t i = 2; i < s.length(); ++i) {\n        current = _mm_set1_epi8(s[i]);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(prev_prev, prev);\n        __m128i cmp2 = _mm_cmpeq_epi8(prev, current);\n        __m128i cmp3 = _mm_cmpeq_epi8(prev_prev, current);\n\n        __m128i result = _mm_or_si128(cmp1, cmp2);\n        result = _mm_or_si128(result, cmp3);\n\n        if (_mm_movemask_epi8(result) != 0) {\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    __m128i v_len = _mm_set1_epi32(len);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n    __m128i v_i = _mm_set_epi32(5, 4, 3, 2);\n\n    for (int i = 5; i <= std::sqrt(len); i += 4) {\n        __m128i v_i_plus_4 = _mm_add_epi32(v_i, _mm_set1_epi32(4));\n        __m128i v_mod_two = _mm_and_si128(_mm_set1_epi32(i), v_two);\n        __m128i v_mod_three = _mm_and_si128(_mm_set1_epi32(i), v_three);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_setzero_si128(), _mm_and_si128(v_mod_two, v_mod_three)))) {\n            return false;\n        }\n\n        v_i = v_i_plus_4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    __m128i v_len = _mm_set1_epi32(len);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n    __m128i v_i = _mm_set_epi32(5, 4, 3, 2);\n\n    for (int i = 5; i <= std::sqrt(len); i += 4) {\n        __m128i v_i_plus = _mm_add_epi32(v_i, v_two);\n        __m128i v_mod = _mm_set_epi32(\n            len % (i + 3),\n            len % (i + 2),\n            len % (i + 1),\n            len % i\n        );\n\n        __m128i v_zero = _mm_setzero_si128();\n        __m128i v_cmp = _mm_cmpeq_epi32(v_mod, v_zero);\n\n        int mask = _mm_movemask_epi8(v_cmp);\n        if (mask != 0) {\n            return false;\n        }\n\n        v_i = _mm_add_epi32(v_i, _mm_set1_epi32(4));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    __m128i v = _mm_set1_epi32(len);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i i = _mm_set_epi32(5, 7, 11, 13);\n    __m128i rem;\n\n    for (int j = 5; j <= std::sqrt(len); j += 4) {\n        rem = _mm_set_epi32(len % (j+2), len % (j+3), len % j, len % (j+1));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(rem, _mm_setzero_si128())) != 0) {\n            return false;\n        }\n        i = _mm_add_epi32(i, _mm_set1_epi32(4));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    __m128i v_len = _mm_set1_epi32(len);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n    __m128i v_six = _mm_set1_epi32(6);\n\n    __m128i v_mod_two = _mm_and_si128(_mm_set1_epi32(len), _mm_set1_epi32(1));\n    __m128i v_mod_three = _mm_and_si128(_mm_set1_epi32(len), _mm_set1_epi32(3));\n\n    if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_mod_two, _mm_setzero_si128())) &&\n        _mm_movemask_epi8(_mm_cmpeq_epi32(v_mod_three, _mm_setzero_si128()))) {\n        return false;\n    }\n\n    for (int i = 5; i * i <= len; i += 6) {\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i v_i_plus_two = _mm_set1_epi32(i + 2);\n        __m128i v_mod_i = _mm_and_si128(v_len, v_i);\n        __m128i v_mod_i_plus_two = _mm_and_si128(v_len, v_i_plus_two);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_mod_i, _mm_setzero_si128())) ||\n            _mm_movemask_epi8(_mm_cmpeq_epi32(v_mod_i_plus_two, _mm_setzero_si128()))) {\n            return false;\n        }\n    }\n\n    return is_prime(len);\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    __m128i v_len = _mm_set1_epi32(len);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n    __m128i v_i = _mm_set_epi32(5, 4, 3, 2);\n    __m128i v_step = _mm_set1_epi32(6);\n\n    int i = 5;\n    for (; i * i <= len; i += 6) {\n        __m128i v_mod_two = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_mod_three = _mm_set_epi32(i+5, i+4, i+3, i+2);\n\n        __m128i v_rem_two = _mm_rem_epi32(v_len, v_mod_two);\n        __m128i v_rem_three = _mm_rem_epi32(v_len, v_mod_three);\n\n        __m128i v_zero = _mm_setzero_si128();\n        __m128i v_cmp_two = _mm_cmpeq_epi32(v_rem_two, v_zero);\n        __m128i v_cmp_three = _mm_cmpeq_epi32(v_rem_three, v_zero);\n\n        int mask_two = _mm_movemask_epi8(v_cmp_two);\n        int mask_three = _mm_movemask_epi8(v_cmp_three);\n\n        if (mask_two != 0 || mask_three != 0) return false;\n\n        v_i = _mm_add_epi32(v_i, v_step);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n    __m128i mask = _mm_set1_epi8(0x0F);\n\n    while (N > 0) {\n        temp = _mm_set1_epi8(N & 0xFF);\n        temp = _mm_and_si128(temp, mask);\n        sum = _mm_add_epi8(sum, temp);\n        N >>= 4;\n    }\n\n    uint16_t sum_digits = _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 1) +\n                          _mm_extract_epi16(sum, 2) + _mm_extract_epi16(sum, 3) +\n                          _mm_extract_epi16(sum, 4) + _mm_extract_epi16(sum, 5) +\n                          _mm_extract_epi16(sum, 6) + _mm_extract_epi16(sum, 7);\n\n    std::ostringstream oss;\n    while (sum_digits > 0) {\n        oss << (sum_digits & 1);\n        sum_digits >>= 1;\n    }\n\n    std::string result = oss.str();\n    std::reverse(result.begin(), result.end());\n    return result.empty() ? \"0\" : result;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n#include <iomanip>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i digits = _mm_setzero_si128();\n\n    while (N > 0) {\n        uint64_t dig = N % 10;\n        N /= 10;\n        digits = _mm_add_epi64(digits, _mm_set1_epi64x(dig));\n    }\n\n    sum = _mm_sad_epu8(sum, digits);\n\n    uint64_t result = _mm_cvtsi128_si64(sum);\n\n    std::stringstream ss;\n    ss << std::bitset<64>(result);\n    std::string binary_string = ss.str();\n\n    binary_string = binary_string.substr(binary_string.find('1'));\n\n    return binary_string;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n#include <iomanip>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i digits = _mm_setzero_si128();\n\n    while (N > 0) {\n        uint64_t digit = N % 10;\n        N /= 10;\n\n        __m128i digit_vec = _mm_set1_epi8(static_cast<char>(digit));\n        sum = _mm_add_epi8(sum, digit_vec);\n    }\n\n    uint64_t total_sum = 0;\n    uint64_t* sum_array = (uint64_t*)&sum;\n    total_sum = sum_array[0] + sum_array[1];\n\n    std::ostringstream oss;\n    oss << std::bitset<64>(total_sum);\n    std::string binary_string = oss.str();\n\n    size_t pos = binary_string.find('1');\n    if (pos != std::string::npos) {\n        return binary_string.substr(pos);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i n = _mm_set1_epi64x(N);\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i shuffle = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1);\n\n    for (int i = 0; i < 8; ++i) {\n        __m128i digits = _mm_and_si128(n, mask);\n        digits = _mm_shuffle_epi8(digits, shuffle);\n        sum = _mm_add_epi64(sum, digits);\n        n = _mm_srli_epi64(n, 4);\n    }\n\n    uint64_t result = _mm_cvtsi128_si64(sum);\n\n    std::ostringstream oss;\n    while (result > 0) {\n        oss << (result % 2);\n        result /= 2;\n    }\n\n    std::string binary = oss.str();\n    std::reverse(binary.begin(), binary.end());\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp = _mm_setzero_si128();\n\n    while (N > 0) {\n        uint64_t digits = N % 10;\n        N /= 10;\n\n        temp = _mm_set1_epi32(digits);\n        sum = _mm_add_epi32(sum, temp);\n    }\n\n    uint64_t result = _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1) + _mm_extract_epi32(sum, 2) + _mm_extract_epi32(sum, 3);\n\n    std::ostringstream oss;\n    while (result > 0) {\n        oss << (result % 2);\n        result /= 2;\n    }\n\n    std::string binary = oss.str();\n    std::reverse(binary.begin(), binary.end());\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i evenMask = _mm_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i evenElements = _mm_and_si128(chunk, evenMask);\n        sum = _mm_add_epi32(sum, evenElements);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i evenMask = _mm_andnot_si128(_mm_set1_epi32(1), chunk);\n        __m128i evenElements = _mm_and_si128(chunk, evenMask);\n        sum = _mm_add_epi32(sum, evenElements);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i evenMask = _mm_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i evenElements = _mm_and_si128(chunk, evenMask);\n        sum = _mm_add_epi32(sum, evenElements);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i even_mask = _mm_andnot_si128(_mm_set1_epi32(1), data);\n        __m128i even_elements = _mm_and_si128(even_mask, data);\n        sum = _mm_add_epi32(sum, even_elements);\n    }\n\n    int tmp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(tmp), sum);\n    return tmp[0] + tmp[1] + tmp[2] + tmp[3];\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(1);\n\n    for (size_t i = 1; i < lst.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i even = _mm_andnot_si128(mask, chunk);\n        sum = _mm_add_epi32(sum, even);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int blockSize = 16;\n    const __m128i shiftVec = _mm_set1_epi8(shift);\n    const __m128i mask = _mm_set1_epi8(0x1F); // 0x1F = 31, to wrap around within the alphabet range\n\n    for (size_t i = 0; i < s.size(); i += blockSize) {\n        size_t remaining = s.size() - i;\n        size_t chunkSize = (remaining >= blockSize) ? blockSize : remaining;\n\n        // Load the chunk of characters\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Add the shift value\n        __m128i encryptedChunk = _mm_add_epi8(chunk, shiftVec);\n\n        // Apply the mask to wrap around within the alphabet range\n        encryptedChunk = _mm_and_si128(encryptedChunk, mask);\n\n        // Store the result back into the result string\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(const_cast<char*>(result.data() + i)), encryptedChunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int chunk_size = 16;\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i encrypted_chunk = _mm_add_epi8(chunk, _mm_set1_epi8(shift));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), encrypted_chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int blockSize = 16;\n    const __m128i alphaStart = _mm_set1_epi8('a');\n    const __m128i alphaEnd = _mm_set1_epi8('z' + 1);\n    const __m128i shiftVector = _mm_set1_epi8(shift);\n\n    for (size_t i = 0; i < s.size(); i += blockSize) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i isLower = _mm_and_si128(_mm_cmpgt_epi8(chunk, alphaStart), _mm_cmplt_epi8(chunk, alphaEnd));\n        __m128i shifted = _mm_add_epi8(chunk, shiftVector);\n        __m128i wrapped = _mm_or_si128(_mm_and_si128(isLower, shifted), _mm_andnot_si128(isLower, chunk));\n        __m128i corrected = _mm_or_si128(_mm_and_si128(_mm_cmpge_epi8(wrapped, alphaEnd), _mm_sub_epi8(wrapped, _mm_set1_epi8(26))), _mm_andnot_si128(_mm_cmpge_epi8(wrapped, alphaEnd), wrapped));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), corrected);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int chunk_size = 16;\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i encrypted_chunk = _mm_add_epi8(chunk, _mm_set1_epi8(shift));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(const_cast<char*>(result.data() + i)), encrypted_chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    std::string result(s.size(), '\\0');\n    const int shift = 4;\n    const int chunk_size = 16;\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n        __m128i encrypted_chunk = _mm_add_epi8(chunk, _mm_set1_epi8(shift));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), encrypted_chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string &message) {\n    std::string encoded(message.size(), ' ');\n    const char *input = message.c_str();\n    char *output = &encoded[0];\n    size_t length = message.size();\n\n    __m128i vowels = _mm_set1_epi8('a');\n    vowels = _mm_insert_epi8(vowels, 'e', 1);\n    vowels = _mm_insert_epi8(vowels, 'i', 2);\n    vowels = _mm_insert_epi8(vowels, 'o', 3);\n    vowels = _mm_insert_epi8(vowels, 'u', 4);\n    vowels = _mm_insert_epi8(vowels, 'A', 5);\n    vowels = _mm_insert_epi8(vowels, 'E', 6);\n    vowels = _mm_insert_epi8(vowels, 'I', 7);\n    vowels = _mm_insert_epi8(vowels, 'O', 8);\n    vowels = _mm_insert_epi8(vowels, 'U', 9);\n\n    __m128i replacements = _mm_set1_epi8('c');\n    replacements = _mm_insert_epi8(replacements, 'g', 1);\n    replacements = _mm_insert_epi8(replacements, 'k', 2);\n    replacements = _mm_insert_epi8(replacements, 'q', 3);\n    replacements = _mm_insert_epi8(replacements, 'w', 4);\n    replacements = _mm_insert_epi8(replacements, 'C', 5);\n    replacements = _mm_insert_epi8(replacements, 'G', 6);\n    replacements = _mm_insert_epi8(replacements, 'K', 7);\n    replacements = _mm_insert_epi8(replacements, 'Q', 8);\n    replacements = _mm_insert_epi8(replacements, 'W', 9);\n\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(input + i));\n        __m128i is_vowel = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i replaced = _mm_or_si128(_mm_andnot_si128(is_vowel, chunk), _mm_and_si128(is_vowel, replacements));\n\n        __m128i is_lower = _mm_and_si128(chunk, case_mask);\n        __m128i swapped_case = _mm_xor_si128(chunk, case_mask);\n\n        __m128i result = _mm_or_si128(_mm_andnot_si128(is_lower, swapped_case), _mm_and_si128(is_lower, replaced));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(output + i), result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string &message) {\n    std::string encoded(message.size(), ' ');\n    const char* input = message.c_str();\n    char* output = &encoded[0];\n\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_mask = _mm_set1_epi8(0x01 << 5);\n    __m128i case_mask = _mm_set1_epi8(0x20);\n    __m128i two_places = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i < message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(input + i));\n\n        __m128i is_vowel = _mm_and_si128(_mm_cmpeq_epi8(chunk, vowels), vowels_mask);\n        __m128i is_upper = _mm_andnot_si128(chunk, case_mask);\n        __m128i is_lower = _mm_and_si128(chunk, case_mask);\n\n        __m128i replaced_vowels = _mm_add_epi8(chunk, two_places);\n        chunk = _mm_or_si128(_mm_andnot_si128(is_vowel, chunk), _mm_and_si128(is_vowel, replaced_vowels));\n\n        __m128i swapped_case = _mm_xor_si128(chunk, case_mask);\n        chunk = _mm_or_si128(_mm_and_si128(is_lower, swapped_case), _mm_andnot_si128(is_lower, chunk));\n        chunk = _mm_or_si128(_mm_and_si128(is_upper, swapped_case), _mm_andnot_si128(is_upper, chunk));\n\n        _mm_storeu_si128((__m128i*)(output + i), chunk);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string &message) {\n    std::string encoded(message.size(), '\\0');\n    const char* input = message.c_str();\n    char* output = &encoded[0];\n\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_mask = _mm_set1_epi8(0x02);\n    __m128i case_mask = _mm_set1_epi8(0x20);\n    __m128i two_places_ahead = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i < message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(input + i));\n\n        __m128i is_vowel = _mm_and_si128(_mm_cmpeq_epi8(chunk, vowels), vowels_mask);\n        __m128i is_lower = _mm_and_si128(chunk, case_mask);\n        __m128i is_upper = _mm_andnot_si128(chunk, case_mask);\n\n        __m128i shifted = _mm_add_epi8(chunk, two_places_ahead);\n        __m128i swapped_case = _mm_xor_si128(chunk, case_mask);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(is_vowel, shifted), _mm_andnot_si128(is_vowel, swapped_case));\n\n        _mm_storeu_si128((__m128i*)(output + i), result);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    const char *input = message.c_str();\n    char *output = &encoded[0];\n    size_t length = message.size();\n\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_mask = _mm_set1_epi8(0x02);\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(input + i));\n\n        __m128i is_vowel = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i is_vowel_masked = _mm_and_si128(is_vowel, vowels_mask);\n        __m128i shifted = _mm_add_epi8(chunk, _mm_set1_epi8(2));\n        chunk = _mm_or_si128(_mm_andnot_si128(is_vowel_masked, chunk), _mm_and_si128(is_vowel_masked, shifted));\n\n        __m128i is_lower = _mm_and_si128(chunk, case_mask);\n        __m128i toggled_case = _mm_xor_si128(chunk, case_mask);\n        chunk = _mm_or_si128(_mm_andnot_si128(is_lower, chunk), _mm_and_si128(is_lower, toggled_case));\n\n        _mm_storeu_si128((__m128i*)(output + i), chunk);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    std::string encoded(message.size(), ' ');\n    __m128i vowels = _mm_set1_epi8('a');\n    __m128i vowels_replace = _mm_set1_epi8('c');\n    __m128i case_mask = _mm_set1_epi8(0x20);\n\n    for (size_t i = 0; i < message.size(); i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        __m128i is_vowel = _mm_cmpeq_epi8(chunk, vowels);\n        __m128i replaced_vowels = _mm_or_si128(_mm_andnot_si128(is_vowel, chunk), _mm_and_si128(is_vowel, vowels_replace));\n        __m128i swapped_case = _mm_xor_si128(replaced_vowels, case_mask);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&encoded[i]), swapped_case);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            __m128i upperMask = _mm_set1_epi8('A');\n            __m128i lowerMask = _mm_set1_epi8('a');\n            __m128i upperCompare = _mm_cmpeq_epi8(_mm_and_si128(chunk, _mm_set1_epi8(~0x20)), upperMask);\n            __m128i lowerCompare = _mm_cmpeq_epi8(_mm_or_si128(chunk, _mm_set1_epi8(0x20)), lowerMask);\n\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(upperCompare, _mm_setzero_si128())) != 0xFFFF) {\n                allLower = false;\n            }\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(lowerCompare, _mm_setzero_si128())) != 0xFFFF) {\n                allUpper = false;\n            }\n        }\n\n        if (!allLower && !allUpper) return false;\n    }\n\n    return allLower || allUpper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool first_upper = true;\n    bool first_lower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        size_t i = 0;\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&key[i]));\n            __m128i upper_mask = _mm_set1_epi8('A');\n            __m128i lower_mask = _mm_set1_epi8('a');\n            __m128i upper_diff = _mm_subs_epu8(chunk, upper_mask);\n            __m128i lower_diff = _mm_subs_epu8(chunk, lower_mask);\n            __m128i upper_check = _mm_cmpeq_epi8(upper_diff, _mm_setzero_si128());\n            __m128i lower_check = _mm_cmpeq_epi8(lower_diff, _mm_setzero_si128());\n\n            if (_mm_movemask_epi8(upper_check) != 0) {\n                first_lower = false;\n            }\n            if (_mm_movemask_epi8(lower_check) != 0) {\n                first_upper = false;\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (std::isupper(key[i])) {\n                first_lower = false;\n            }\n            if (std::islower(key[i])) {\n                first_upper = false;\n            }\n        }\n    }\n\n    return first_upper || first_lower;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        size_t i = 0;\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128((__m128i*)(key.data() + i));\n            __m128i lower_mask = _mm_set1_epi8('a' - 1);\n            __m128i upper_mask = _mm_set1_epi8('A' - 1);\n            __m128i is_lower = _mm_and_si128(_mm_subs_epu8(chunk, lower_mask), _mm_set1_epi8(0x20));\n            __m128i is_upper = _mm_and_si128(_mm_subs_epu8(chunk, upper_mask), _mm_set1_epi8(0x20));\n\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(is_lower, _mm_setzero_si128())) != 0xFFFF) {\n                all_upper = false;\n            }\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(is_upper, _mm_setzero_si128())) != 0xFFFF) {\n                all_lower = false;\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (std::islower(key[i])) {\n                all_upper = false;\n            } else if (std::isupper(key[i])) {\n                all_lower = false;\n            } else {\n                return false;\n            }\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool allUpper = true;\n    bool allLower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        size_t i = 0;\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            __m128i isUpper = _mm_cmpgt_epi8(_mm_set1_epi8('A'-1), chunk);\n            __m128i isLower = _mm_cmpgt_epi8(_mm_set1_epi8('a'-1), chunk);\n            __m128i isAlpha = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('Z')), _mm_cmpgt_epi8(_mm_set1_epi8('z'), chunk));\n            isUpper = _mm_and_si128(isUpper, isAlpha);\n            isLower = _mm_and_si128(isLower, isAlpha);\n\n            if (_mm_movemask_epi8(isUpper) != 0) {\n                allLower = false;\n            }\n            if (_mm_movemask_epi8(isLower) != 0) {\n                allUpper = false;\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (std::isupper(key[i])) {\n                allLower = false;\n            }\n            if (std::islower(key[i])) {\n                allUpper = false;\n            }\n        }\n\n        if (!allLower && !allUpper) return false;\n    }\n\n    return allLower || allUpper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_map_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    bool first_upper = true;\n    bool first_lower = true;\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        size_t i = 0;\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.data() + i));\n            __m128i upper_mask = _mm_set1_epi8('A');\n            __m128i lower_mask = _mm_set1_epi8('a');\n            __m128i diff_upper = _mm_subs_epu8(chunk, upper_mask);\n            __m128i diff_lower = _mm_subs_epu8(chunk, lower_mask);\n            __m128i is_upper = _mm_cmpeq_epi8(diff_upper, _mm_setzero_si128());\n            __m128i is_lower = _mm_cmpeq_epi8(diff_lower, _mm_setzero_si128());\n\n            if (_mm_movemask_epi8(is_upper) != 0) {\n                first_lower = false;\n            } else if (_mm_movemask_epi8(is_lower) != 0) {\n                first_upper = false;\n            } else {\n                all_upper = false;\n                all_lower = false;\n                break;\n            }\n        }\n\n        for (; i < len; ++i) {\n            if (std::isupper(key[i])) {\n                first_lower = false;\n            } else if (std::islower(key[i])) {\n                first_upper = false;\n            } else {\n                all_upper = false;\n                all_lower = false;\n                break;\n            }\n        }\n\n        if (!all_upper && !all_lower) return false;\n    }\n\n    return first_upper || first_lower;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    bool *is_prime = new bool[n];\n    std::fill_n(is_prime, n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i < std::sqrt(n); ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    bool *is_prime = new bool[n];\n    for (int i = 2; i < n; ++i) is_prime[i] = true;\n\n    for (int i = 2; i < std::sqrt(n); ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    bool *is_prime = new bool[n];\n    for (int i = 2; i < n; ++i) is_prime[i] = true;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    bool *is_prime = new bool[n];\n    std::fill_n(is_prime, n, true);\n    is_prime[0] = is_prime[1] = false;\n\n    for (int i = 2; i < std::sqrt(n); ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n < 2) return primes;\n\n    bool *is_prime = new bool[n];\n    for (int i = 2; i < n; ++i) is_prime[i] = true;\n\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i]) {\n            for (int j = i * i; j < n; j += i) {\n                is_prime[j] = false;\n            }\n        }\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i]) {\n            primes.push_back(i);\n        }\n    }\n\n    delete[] is_prime;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.c_str();\n\n    __m128i vowels = _mm_set1_epi8('A');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    size_t i = 0;\n    for (; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i even_mask = _mm_setr_epi8(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i even_chunk = _mm_and_si128(chunk, even_mask);\n        __m128i cmp = _mm_and_si128(_mm_cmpeq_epi8(even_chunk, vowels), even_mask);\n\n        unsigned int mask = _mm_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < len; i += 2) {\n        if (i % 2 == 0 && strchr(\"AEIOU\", data[i]) != nullptr) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    const char* data = s.c_str();\n    size_t length = s.length();\n\n    __m128i vowels = _mm_set1_epi8('A');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, vowels);\n\n        __m128i even_mask = _mm_setr_epi8(\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF,\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF\n        );\n\n        mask = _mm_and_si128(mask, even_mask);\n\n        int mask_value = _mm_movemask_epi8(mask);\n        count += _mm_popcnt_u32(mask_value);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    const char* data = s.c_str();\n\n    __m128i vowels = _mm_set1_epi8('A');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, vowels);\n\n        __m128i even_mask = _mm_setr_epi8(\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF,\n            0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF\n        );\n\n        mask = _mm_and_si128(mask, even_mask);\n\n        int mask_values[16];\n        _mm_storeu_si128((__m128i*)mask_values, mask);\n\n        for (int j = 0; j < 16; j += 2) {\n            if (mask_values[j] != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    int len = s.length();\n    const char *str = s.c_str();\n\n    __m128i vowels = _mm_set1_epi8('A');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, vowels);\n\n        int local_count = _mm_movemask_epi8(mask);\n\n        for (int j = 0; j < 16 && i + j < len; j += 2) {\n            if (local_count & (1 << j)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    const char *str = s.c_str();\n    size_t len = s.length();\n\n    __m128i vowels = _mm_set1_epi8('A');\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('E'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('I'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('O'));\n    vowels = _mm_or_si128(vowels, _mm_set1_epi8('U'));\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i mask = _mm_cmpeq_epi8(chunk, vowels);\n\n        __m128i even_mask = _mm_setr_epi8(\n            0x80, 0, 0x80, 0, 0x80, 0, 0x80, 0,\n            0x80, 0, 0x80, 0, 0x80, 0, 0x80, 0\n        );\n\n        mask = _mm_and_si128(mask, even_mask);\n\n        int local_count = _mm_movemask_epi8(mask);\n        local_count = (local_count & 0x55555555) + ((local_count >> 1) & 0x55555555);\n        local_count = (local_count & 0x33333333) + ((local_count >> 2) & 0x33333333);\n        local_count = (local_count & 0x0F0F0F0F) + ((local_count >> 4) & 0x0F0F0F0F);\n        local_count = (local_count & 0x00FF00FF) + ((local_count >> 8) & 0x00FF00FF);\n        local_count = (local_count & 0x0000FFFF) + ((local_count >> 16) & 0x0000FFFF);\n\n        count += local_count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i current = _mm_set1_epi32(n);\n    __m128i increment = _mm_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current);\n        current = _mm_add_epi32(current, increment);\n        increment = _mm_add_epi32(increment, _mm_set1_epi32(2));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i current = _mm_set1_epi32(n);\n    __m128i increment = _mm_set1_epi32((n % 2 == 0) ? 2 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), current);\n        current = _mm_add_epi32(current, increment);\n        increment = _mm_add_epi32(increment, _mm_set1_epi32(2));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> pile(n);\n    __m128i current = _mm_set1_epi32(n);\n    __m128i increment = _mm_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&pile[i]), current);\n        current = _mm_add_epi32(current, increment);\n        increment = _mm_add_epi32(increment, _mm_set1_epi32(2));\n    }\n\n    return pile;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> pile(n);\n    __m128i current = _mm_set1_epi32(n);\n    __m128i increment = _mm_set1_epi32(n % 2 == 0 ? 2 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&pile[i]), current);\n        current = _mm_add_epi32(current, increment);\n        increment = _mm_add_epi32(increment, _mm_set1_epi32(2));\n    }\n\n    return pile;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> pile(n);\n    __m128i current = _mm_set1_epi32(n);\n    __m128i increment = _mm_set1_epi32((n % 2 == 0) ? 2 : 1);\n\n    for (int i = 0; i < n; i += 4) {\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&pile[i]), current);\n        current = _mm_add_epi32(current, increment);\n        increment = _mm_add_epi32(increment, _mm_set1_epi32(2));\n    }\n\n    return pile;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i count_vec = _mm_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i values = _mm_set_epi64x(i + 1, i);\n        sum = _mm_add_epi64(sum, values);\n    }\n\n    __m128i total_sum = _mm_hadd_epi64(sum, sum);\n    int64_t total = _mm_cvtsi128_si64(total_sum);\n\n    int64_t avg = total / count;\n    if (total % count != 0) {\n        avg++;\n    }\n\n    std::bitset<64> binary(avg);\n    std::string binary_string = binary.to_string();\n\n    size_t pos = binary_string.find('1');\n    if (pos != std::string::npos) {\n        return binary_string.substr(pos);\n    }\n\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    int64_t sum = 0;\n\n    int64_t i;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i count_vec = _mm_setzero_si128();\n\n    for (i = n; i <= m - 1; i += 2) {\n        __m128i vec = _mm_set_epi64x(i + 1, i);\n        sum_vec = _mm_add_epi64(sum_vec, vec);\n        count_vec = _mm_add_epi64(count_vec, _mm_set1_epi64x(1));\n    }\n\n    int64_t sum_parts[2];\n    int64_t count_parts[2];\n    _mm_storeu_si128((__m128i*)sum_parts, sum_vec);\n    _mm_storeu_si128((__m128i*)count_parts, count_vec);\n\n    sum = sum_parts[0] + sum_parts[1];\n    count = count_parts[0] + count_parts[1];\n\n    if (i <= m) {\n        sum += i;\n        count += 1;\n    }\n\n    int64_t avg = sum / count;\n    if (sum % count != 0) {\n        avg += 1;\n    }\n\n    std::bitset<64> binary(avg);\n    std::string binary_string = binary.to_string();\n\n    size_t pos = binary_string.find('1');\n    if (pos != std::string::npos) {\n        return binary_string.substr(pos);\n    }\n\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i count_vec = _mm_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i vec = _mm_set_epi64x(i + 1, i);\n        sum = _mm_add_epi64(sum, vec);\n    }\n\n    __m128i total_sum = _mm_add_epi64(sum, _mm_srli_si128(sum, 8));\n    int64_t total = _mm_cvtsi128_si64(total_sum);\n\n    int64_t avg = total / count;\n    if (total % count != 0) {\n        avg++;\n    }\n\n    std::bitset<64> binary(avg);\n    return binary.to_string().substr(binary.to_string().find('1'));\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i count_vec = _mm_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i values = _mm_set_epi64x(i + 1, i);\n        sum = _mm_add_epi64(sum, values);\n    }\n\n    __m128i total_sum = _mm_add_epi64(sum, _mm_srli_si128(sum, 8));\n    int64_t total = _mm_cvtsi128_si64(total_sum);\n\n    int64_t avg = total / count;\n    if (total % count != 0) {\n        avg++;\n    }\n\n    std::bitset<64> binary(avg);\n    return binary.to_string();\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    int64_t count = m - n + 1;\n    __m128i sum = _mm_setzero_si128();\n    __m128i count_vec = _mm_set1_epi64x(count);\n\n    for (int64_t i = n; i <= m; i += 2) {\n        __m128i values = _mm_set_epi64x(i + 1, i);\n        sum = _mm_add_epi64(sum, values);\n    }\n\n    __m128i total_sum = _mm_hadd_epi64(sum, sum);\n    int64_t sum_scalar = _mm_cvtsi128_si64(total_sum);\n\n    int64_t avg = sum_scalar / count;\n    int64_t rounded_avg = (sum_scalar + count / 2) / count;\n\n    std::bitset<64> binary(rounded_avg);\n    std::string binary_str = binary.to_string();\n\n    auto pos = binary_str.find('1');\n    if (pos != std::string::npos) {\n        return binary_str.substr(pos);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum = _mm_set1_epi32(0);\n    __m128i fact = _mm_set1_epi32(1);\n    __m128i mod = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i index = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_set_epi32((i+3)%2==0 ? -1 : 0, (i+2)%2==0 ? -1 : 0, (i+1)%2==0 ? -1 : 0, i%2==0 ? -1 : 0);\n        __m128i add = _mm_add_epi32(index, one);\n\n        __m128i temp_fact = fact;\n        for (int j = 1; j <= i+3; ++j) {\n            temp_fact = _mm_mullo_epi32(temp_fact, _mm_set1_epi32(j));\n            temp_fact = _mm_and_si128(temp_fact, mask);\n            temp_fact = _mm_and_si128(temp_fact, mod);\n        }\n\n        __m128i temp_sum = _mm_add_epi32(sum, add);\n        temp_sum = _mm_andnot_si128(mask, temp_sum);\n\n        __m128i temp_result = _mm_or_si128(temp_fact, temp_sum);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), temp_result);\n\n        fact = temp_fact;\n        sum = temp_sum;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum = _mm_set1_epi32(0);\n    __m128i fact = _mm_set1_epi32(1);\n    __m128i mod = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i idx = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m128i mask = _mm_setr_epi32(i % 2 == 0 ? -1 : 0, (i + 1) % 2 == 0 ? -1 : 0, (i + 2) % 2 == 0 ? -1 : 0, (i + 3) % 2 == 0 ? -1 : 0);\n\n        __m128i even = _mm_and_si128(idx, mask);\n        __m128i odd = _mm_andnot_si128(mask, idx);\n\n        fact = _mm_mullo_epi32(fact, idx);\n        fact = _mm_and_si128(fact, mask);\n        fact = _mm_add_epi32(fact, one);\n        fact = _mm_srai_epi32(fact, mod);\n\n        sum = _mm_add_epi32(sum, odd);\n        sum = _mm_add_epi32(sum, one);\n\n        __m128i res = _mm_or_si128(_mm_and_si128(fact, mask), _mm_andnot_si128(mask, sum));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i - 1]), res);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum = _mm_set1_epi32(0);\n    __m128i fact = _mm_set1_epi32(1);\n    __m128i mod = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 1; i <= n; ++i) {\n        __m128i idx = _mm_set1_epi32(i);\n        __m128i even_mask = _mm_set1_epi32(~1);\n\n        if (i % 2 == 0) {\n            fact = _mm_mullo_epi32(fact, idx);\n            fact = _mm_and_si128(fact, mod);\n            result[i - 1] = _mm_cvtsi128_si32(fact);\n        } else {\n            sum = _mm_add_epi32(sum, idx);\n            result[i - 1] = _mm_cvtsi128_si32(sum);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum = _mm_set1_epi32(0);\n    __m128i fact = _mm_set1_epi32(1);\n    __m128i mod = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 1; i <= n; i++) {\n        if (i % 2 == 0) {\n            fact = _mm_mullo_epi32(fact, _mm_set1_epi32(i));\n            fact = _mm_and_si128(fact, _mm_set1_epi32(0xFFFF));\n            __m128i temp = _mm_mul_epu32(fact, mod);\n            temp = _mm_srli_epi64(temp, 32);\n            result[i - 1] = _mm_cvtsi128_si32(temp);\n        } else {\n            sum = _mm_add_epi32(sum, _mm_set1_epi32(i));\n            result[i - 1] = _mm_cvtsi128_si32(sum);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum = _mm_set1_epi32(0);\n    __m128i fact = _mm_set1_epi32(1);\n    __m128i mod = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i index = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m128i mask = _mm_setr_epi32((i % 2 == 0) ? -1 : 0, ((i + 1) % 2 == 0) ? -1 : 0, ((i + 2) % 2 == 0) ? -1 : 0, ((i + 3) % 2 == 0) ? -1 : 0);\n\n        __m128i even = _mm_and_si128(index, mask);\n        __m128i odd = _mm_andnot_si128(mask, index);\n\n        even = _mm_mullo_epi32(even, fact);\n        fact = _mm_add_epi32(fact, _mm_and_si128(_mm_cmpeq_epi32(even, _mm_setzero_si128()), one));\n        even = _mm_and_si128(_mm_cmpgt_epi32(even, mod), mod);\n\n        odd = _mm_add_epi32(odd, one);\n        sum = _mm_add_epi32(sum, odd);\n\n        __m128i temp = _mm_or_si128(_mm_and_si128(mask, even), _mm_andnot_si128(mask, sum));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i - 1]), temp);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i evens = _mm_set1_epi32(2);\n    __m128i odds = _mm_set1_epi32(1);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(0, 0, 0, 0);\n        __m128i palindromes = _mm_set1_epi32(1);\n\n        for (int j = 0; j < 4 && i + j <= n; ++j) {\n            int num = i + j;\n            int reversed = 0;\n            int temp = num;\n            while (temp != 0) {\n                reversed = reversed * 10 + temp % 10;\n                temp /= 10;\n            }\n            if (num == reversed) {\n                palindromes = _mm_insert_epi32(palindromes, num, j);\n            }\n        }\n\n        __m128i even_mask = _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(palindromes, ones), evens), palindromes);\n        __m128i odd_mask = _mm_andnot_si128(even_mask, palindromes);\n\n        even_count += _mm_movemask_epi8(_mm_cmpeq_epi32(even_mask, evens));\n        odd_count += _mm_movemask_epi8(_mm_cmpeq_epi32(odd_mask, odds));\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i evens = _mm_set1_epi32(0);\n    __m128i odds = _mm_set1_epi32(0);\n    __m128i twos = _mm_set1_epi32(2);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(0, 0, 0, i);\n        nums = _mm_add_epi32(nums, ones);\n        nums = _mm_add_epi32(nums, twos);\n\n        __m128i palindromes = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            int num = _mm_extract_epi32(nums, j);\n            int reversed = 0;\n            int temp = num;\n            while (temp != 0) {\n                int digit = temp % 10;\n                reversed = reversed * 10 + digit;\n                temp /= 10;\n            }\n            if (num == reversed) {\n                palindromes = _mm_insert_epi32(palindromes, num, j);\n            }\n        }\n\n        __m128i even_mask = _mm_set1_epi32(0x00000001);\n        __m128i odd_mask = _mm_set1_epi32(0x00000002);\n\n        __m128i even_count = _mm_and_si128(palindromes, even_mask);\n        __m128i odd_count = _mm_and_si128(palindromes, odd_mask);\n\n        evens = _mm_add_epi32(evens, even_count);\n        odds = _mm_add_epi32(odds, odd_count);\n    }\n\n    int even_count = _mm_extract_epi32(evens, 0) + _mm_extract_epi32(evens, 1) + _mm_extract_epi32(evens, 2) + _mm_extract_epi32(evens, 3);\n    int odd_count = _mm_extract_epi32(odds, 0) + _mm_extract_epi32(odds, 1) + _mm_extract_epi32(odds, 2) + _mm_extract_epi32(odds, 3);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i evens = _mm_set1_epi32(0);\n    __m128i odds = _mm_set1_epi32(0);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(0, i + 3, i + 2, i + 1);\n        __m128i divs = _mm_set_epi32(0, (i + 3) % 10, (i + 2) % 10, (i + 1) % 10);\n        __m128i rems = _mm_set_epi32(0, (i + 3) / 10, (i + 2) / 10, (i + 1) / 10);\n\n        __m128i is_palindrome = _mm_cmpeq_epi32(divs, rems);\n        __m128i is_even = _mm_andnot_si128(_mm_set1_epi32(1), nums);\n        __m128i is_odd = _mm_and_si128(_mm_set1_epi32(1), nums);\n\n        evens = _mm_add_epi32(evens, _mm_and_si128(is_palindrome, is_even));\n        odds = _mm_add_epi32(odds, _mm_and_si128(is_palindrome, is_odd));\n    }\n\n    int even_count = _mm_extract_epi32(evens, 0) + _mm_extract_epi32(evens, 1) + _mm_extract_epi32(evens, 2) + _mm_extract_epi32(evens, 3);\n    int odd_count = _mm_extract_epi32(odds, 0) + _mm_extract_epi32(odds, 1) + _mm_extract_epi32(odds, 2) + _mm_extract_epi32(odds, 3);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n#include <cstring>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i evens = _mm_set1_epi32(2);\n    __m128i odds = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(0, 0, 0, i);\n        __m128i nums_rev = _mm_set_epi32(0, 0, 0, reverse_int(i));\n\n        __m128i is_palindrome = _mm_cmpeq_epi32(nums, nums_rev);\n        __m128i is_even = _mm_cmpeq_epi32(_mm_and_si128(nums, ones), zero);\n        __m128i is_odd = _mm_cmpeq_epi32(_mm_and_si128(nums, ones), ones);\n\n        __m128i even_mask = _mm_and_si128(is_palindrome, is_even);\n        __m128i odd_mask = _mm_and_si128(is_palindrome, is_odd);\n\n        even_count += _mm_movemask_epi8(_mm_cmpeq_epi32(even_mask, evens));\n        odd_count += _mm_movemask_epi8(_mm_cmpeq_epi32(odd_mask, odds));\n    }\n\n    return {even_count, odd_count};\n}\n\nint reverse_int(int num) {\n    int reversed = 0;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return reversed;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i evens = _mm_set1_epi32(0);\n    __m128i odds = _mm_set1_epi32(0);\n\n    for (int i = 1; i <= n; i += 4) {\n        __m128i nums = _mm_set_epi32(0, i + 3 <= n ? i + 3 : 0, i + 2 <= n ? i + 2 : 0, i + 1 <= n ? i + 1 : 0, i);\n        __m128i rev_nums = _mm_set_epi32(0, reverse_int(i + 3 <= n ? i + 3 : 0), reverse_int(i + 2 <= n ? i + 2 : 0), reverse_int(i + 1 <= n ? i + 1 : 0), reverse_int(i));\n\n        __m128i is_palindrome = _mm_cmpeq_epi32(nums, rev_nums);\n        __m128i is_even = _mm_andnot_si128(_mm_set1_epi32(1), nums);\n\n        evens = _mm_add_epi32(evens, _mm_and_si128(is_palindrome, is_even));\n        odds = _mm_add_epi32(odds, _mm_andnot_si128(is_even, is_palindrome));\n    }\n\n    int even_count = _mm_extract_epi32(evens, 0) + _mm_extract_epi32(evens, 1) + _mm_extract_epi32(evens, 2) + _mm_extract_epi32(evens, 3);\n    int odd_count = _mm_extract_epi32(odds, 0) + _mm_extract_epi32(odds, 1) + _mm_extract_epi32(odds, 2) + _mm_extract_epi32(odds, 3);\n\n    return {even_count, odd_count};\n}\n\nint reverse_int(int num) {\n    int reversed = 0;\n    while (num > 0) {\n        reversed = reversed * 10 + num % 10;\n        num /= 10;\n    }\n    return reversed;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i signMask = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i absNums = _mm_xor_si128(nums, _mm_and_si128(signMask, _mm_cmplt_epi32(nums, zero)));\n\n        __m128i sumDigits = _mm_setzero_si128();\n        for (int j = 0; j < 10; ++j) {\n            __m128i rem = _mm_and_si128(absNums, _mm_set1_epi32(9));\n            sumDigits = _mm_add_epi32(sumDigits, rem);\n            absNums = _mm_srli_epi32(_mm_sub_epi32(absNums, rem), 1);\n        }\n\n        __m128i mask = _mm_cmpgt_epi32(sumDigits, zero);\n        int maskInt = _mm_movemask_epi8(_mm_packs_epi32(mask, mask));\n        count += _mm_popcnt_u32(maskInt & 0b00000000000000000000000011110000);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i signMask = _mm_set1_epi32(0x80000000);\n    __m128i digitMask = _mm_set1_epi32(0x0000000F);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&n[i]));\n        __m128i absChunk = _mm_xor_si128(_mm_and_si128(chunk, signMask), chunk);\n        __m128i sumDigits = _mm_setzero_si128();\n\n        for (int j = 0; j < 4; ++j) {\n            __m128i digits = _mm_and_si128(absChunk, digitMask);\n            sumDigits = _mm_add_epi32(sumDigits, digits);\n            absChunk = _mm_srli_epi32(absChunk, 4);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sumDigits, zero);\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i signMask = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i sum;\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&n[i]));\n        __m128i absChunk = _mm_xor_si128(chunk, _mm_and_si128(signMask, _mm_cmplt_epi32(chunk, zero)));\n\n        __m128i digitSum = _mm_setzero_si128();\n        for (int j = 0; j < 10; ++j) {\n            __m128i remainder = _mm_and_si128(absChunk, _mm_set1_epi32(9));\n            digitSum = _mm_add_epi32(digitSum, remainder);\n            absChunk = _mm_srli_epi32(_mm_sub_epi32(absChunk, remainder), 3);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(digitSum, zero);\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i signMask = _mm_set1_epi32(0x80000000);\n    __m128i digitMask = _mm_set1_epi32(0x0000000F);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&n[i]));\n        __m128i absChunk = _mm_xor_si128(chunk, _mm_and_si128(chunk, signMask));\n\n        __m128i sumDigits = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            __m128i digits = _mm_and_si128(absChunk, digitMask);\n            sumDigits = _mm_add_epi32(sumDigits, digits);\n            absChunk = _mm_srli_epi32(absChunk, 4);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sumDigits, zero);\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n        count += _mm_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i signMask = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&n[i]));\n        __m128i absV = _mm_xor_si128(v, _mm_and_si128(signMask, _mm_cmplt_epi32(v, zero)));\n\n        __m128i sumDigits = _mm_setzero_si128();\n        __m128i temp = _mm_setzero_si128();\n\n        for (int j = 0; j < 10; ++j) {\n            temp = _mm_and_si128(absV, _mm_set1_epi32(9));\n            sumDigits = _mm_add_epi32(sumDigits, temp);\n            absV = _mm_srli_epi32(_mm_sub_epi32(absV, temp), 3);\n        }\n\n        __m128i mask = _mm_cmpgt_epi32(sumDigits, zero);\n        int localCount = _mm_movemask_epi8(_mm_packs_epi32(mask, mask));\n        count += _mm_popcnt_u32(localCount & 0x1111);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&arr[j]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&sorted_arr[(j + i) % N]);\n            __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n        std::rotate(arr.rbegin(), arr.rbegin() + 1, arr.rend());\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    for (int i = 0; i < N; ++i) {\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&arr[(j + i) % N]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&sorted_arr[j]);\n            __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n        if (is_sorted) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    std::vector<int> temp(N);\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            temp[(j + i) % N] = arr[j];\n        }\n\n        bool is_sorted = true;\n        for (int j = 0; j < N; j += 4) {\n            __m128i v1 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&temp[j]));\n            __m128i v2 = _mm_loadu_si128(reinterpret_cast<__m128i*>(&sorted_arr[j]));\n            __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    std::vector<int> temp(N);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            temp[(j + i) % N] = arr[j];\n        }\n\n        bool is_sorted = true;\n        for (int k = 0; k < N; k += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&temp[k]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&sorted_arr[k]);\n            __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    std::vector<int> sorted_arr = arr;\n    std::sort(sorted_arr.begin(), sorted_arr.end());\n\n    std::vector<int> temp(N);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            temp[(j + i) % N] = arr[j];\n        }\n\n        bool is_sorted = true;\n        for (int k = 0; k < N; k += 4) {\n            __m128i v1 = _mm_loadu_si128((__m128i*)&temp[k]);\n            __m128i v2 = _mm_loadu_si128((__m128i*)&sorted_arr[k]);\n            __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size1 = lst1.size();\n    int size2 = lst2.size();\n\n    __m128i mask1 = _mm_set1_epi32(1);\n    __m128i mask2 = _mm_set1_epi32(1);\n\n    for (int i = 0; i < size1; i += 4) {\n        __m128i chunk1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i even1 = _mm_andnot_si128(mask1, chunk1);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(even1, chunk1)) != 0xF) {\n            for (int j = 0; j < size2; j += 4) {\n                __m128i chunk2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[j]));\n                __m128i even2 = _mm_andnot_si128(mask2, chunk2);\n                if (_mm_movemask_epi8(_mm_cmpeq_epi32(even2, chunk2)) == 0xF) {\n                    return \"YES\";\n                }\n            }\n            return \"NO\";\n        }\n    }\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    bool possible = true;\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i v1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i v2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i]));\n\n        __m128i even1 = _mm_andnot_si128(mask, v1);\n        __m128i even2 = _mm_andnot_si128(mask, v2);\n\n        __m128i cmp1 = _mm_cmpeq_epi32(even1, v1);\n        __m128i cmp2 = _mm_cmpeq_epi32(even2, v2);\n\n        __m128i result = _mm_or_si128(cmp1, cmp2);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(result, zero)) != 0xFFFF) {\n            possible = false;\n            break;\n        }\n    }\n\n    return possible ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size1 = lst1.size();\n    int size2 = lst2.size();\n\n    __m128i mask1 = _mm_set1_epi32(1);\n    __m128i mask2 = _mm_set1_epi32(1);\n\n    for (int i = 0; i < size1 - 3; i += 4) {\n        __m128i vec1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i vec2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i]));\n\n        __m128i even1 = _mm_andnot_si128(mask1, vec1);\n        __m128i even2 = _mm_andnot_si128(mask2, vec2);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(even1, vec1)) != 0xFFFF) {\n            return \"NO\";\n        }\n    }\n\n    for (int i = size1 - (size1 % 4); i < size1; ++i) {\n        if (lst1[i] % 2 != 0 && lst2[i] % 2 != 0) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size1 = lst1.size();\n    int size2 = lst2.size();\n\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    bool possible = true;\n\n    for (int i = 0; i < size1; i += 4) {\n        __m128i chunk1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i chunk2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i % size2]));\n\n        __m128i even1 = _mm_andnot_si128(mask, chunk1);\n        __m128i even2 = _mm_andnot_si128(mask, chunk2);\n\n        __m128i cmp1 = _mm_cmpeq_epi32(even1, chunk1);\n        __m128i cmp2 = _mm_cmpeq_epi32(even2, chunk2);\n\n        __m128i result = _mm_or_si128(cmp1, cmp2);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(result, zero)) != 0xFFFF) {\n            possible = false;\n            break;\n        }\n    }\n\n    return possible ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size1 = lst1.size();\n    int size2 = lst2.size();\n\n    __m128i mask = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    bool lst1_has_odd = false;\n    bool lst2_has_even = false;\n\n    for (int i = 0; i < size1 - 3; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i is_odd = _mm_and_si128(v, mask);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(is_odd, zero)) != 0xFFFF) {\n            lst1_has_odd = true;\n            break;\n        }\n    }\n\n    for (int i = 0; i < size2 - 3; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i]));\n        __m128i is_even = _mm_andnot_si128(v, mask);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(is_even, zero)) != 0xFFFF) {\n            lst2_has_even = true;\n            break;\n        }\n    }\n\n    if (!lst1_has_odd) {\n        return \"YES\";\n    }\n\n    if (lst2_has_even) {\n        return \"YES\";\n    }\n\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 16 bytes at a time\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n            __m128i odd_mask = _mm_set1_epi8('1');\n            __m128i cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            int mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('3');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('5');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('7');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('9');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 16 bytes at a time using SSE\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n            __m128i odd_mask = _mm_set1_epi8('1');\n            __m128i cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            int mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('3');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('5');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('7');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('9');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 16 bytes at a time\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n            __m128i odd_mask = _mm_set1_epi8('1');\n            __m128i cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            int mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('3');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('5');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('7');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('9');\n            cmp_mask = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_mask);\n            count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 16 bytes at a time\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[i]));\n            __m128i odd_mask = _mm_set1_epi8('1');\n            __m128i cmp = _mm_cmpeq_epi8(chunk, odd_mask);\n            int mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('3');\n            cmp = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('5');\n            cmp = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('7');\n            cmp = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('9');\n            cmp = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <sstream>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t len = str.length();\n        size_t i = 0;\n\n        // Process 16 bytes at a time\n        for (; i <= len - 16; i += 16) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.data() + i));\n            __m128i odd_mask = _mm_set1_epi8('1');\n            __m128i cmp_result = _mm_cmpeq_epi8(chunk, odd_mask);\n            int mask = _mm_movemask_epi8(cmp_result);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('3');\n            cmp_result = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_result);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('5');\n            cmp_result = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_result);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('7');\n            cmp_result = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_result);\n            count += _mm_popcnt_u32(mask);\n\n            odd_mask = _mm_set1_epi8('9');\n            cmp_result = _mm_cmpeq_epi8(chunk, odd_mask);\n            mask = _mm_movemask_epi8(cmp_result);\n            count += _mm_popcnt_u32(mask);\n        }\n\n        // Process remaining bytes\n        for (; i < len; ++i) {\n            if (str[i] == '1' || str[i] == '3' || str[i] == '5' || str[i] == '7' || str[i] == '9') {\n                count++;\n            }\n        }\n\n        std::ostringstream oss;\n        oss << \"the number of odd elements \" << count << \"n the str\" << count << \"ng \" << count << \" of the \" << count << \"nput.\";\n        result.push_back(oss.str());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n\n    __m128i vMinSum = _mm_set1_epi64x(LLONG_MAX);\n    __m128i vCurrentSum = _mm_setzero_si128();\n\n    for (size_t i = 0; i < nums.size(); i += 2) {\n        __m128i vNums = _mm_set_epi64x(i + 1 < nums.size() ? nums[i + 1] : 0, nums[i]);\n        vCurrentSum = _mm_add_epi64(vCurrentSum, vNums);\n\n        vMinSum = _mm_min_epi64(vMinSum, vCurrentSum);\n\n        if (vCurrentSum.m128i_i64[0] < 0) {\n            vCurrentSum = _mm_setzero_si128();\n        }\n\n        int64_t tempMin[2];\n        _mm_storeu_si128((__m128i*)tempMin, vMinSum);\n        minSum = std::min(minSum, tempMin[0]);\n        minSum = std::min(minSum, tempMin[1]);\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n\n    __m128i vMinSum = _mm_set1_epi64x(LLONG_MAX);\n    __m128i vCurrentSum = _mm_set1_epi64x(0);\n\n    for (size_t i = 0; i < nums.size(); i += 2) {\n        __m128i vNums = _mm_set_epi64x(i + 1 < nums.size() ? nums[i + 1] : 0, nums[i]);\n\n        vCurrentSum = _mm_add_epi64(vCurrentSum, vNums);\n        vMinSum = _mm_min_epi64(vMinSum, vCurrentSum);\n\n        if (vCurrentSum.m128i_i64[0] < 0) {\n            vCurrentSum = _mm_set1_epi64x(0);\n        }\n\n        int64_t tempMin[2];\n        _mm_storeu_si128((__m128i*)tempMin, vMinSum);\n        minSum = std::min(minSum, std::min(tempMin[0], tempMin[1]));\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int n = nums.size();\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n\n    __m128i vMinSum = _mm_set1_epi64x(LLONG_MAX);\n    __m128i vCurrentSum = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 2) {\n        __m128i vNums = _mm_set_epi64x(0, 0);\n        if (i < n) vNums = _mm_insert_epi64(vNums, nums[i], 0);\n        if (i + 1 < n) vNums = _mm_insert_epi64(vNums, nums[i + 1], 1);\n\n        vCurrentSum = _mm_add_epi64(vCurrentSum, vNums);\n        vMinSum = _mm_min_epi64(vMinSum, vCurrentSum);\n\n        if (i + 2 <= n) {\n            int64_t temp[2];\n            _mm_storeu_si128((__m128i*)temp, vCurrentSum);\n            currentSum = temp[0] + temp[1];\n            minSum = std::min(minSum, currentSum);\n            vCurrentSum = _mm_setzero_si128();\n        }\n    }\n\n    int64_t temp[2];\n    _mm_storeu_si128((__m128i*)temp, vMinSum);\n    minSum = std::min(minSum, std::min(temp[0], temp[1]));\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n\n    __m128i vMinSum = _mm_set1_epi64x(LLONG_MAX);\n    __m128i vCurrentSum = _mm_set1_epi64x(0);\n    __m128i vZero = _mm_set1_epi64x(0);\n\n    for (size_t i = 0; i < nums.size(); i += 2) {\n        __m128i vNums = _mm_set_epi64x(i + 1 < nums.size() ? nums[i + 1] : 0, nums[i]);\n\n        vCurrentSum = _mm_add_epi64(vCurrentSum, vNums);\n        vMinSum = _mm_min_epi64(vMinSum, vCurrentSum);\n\n        if (vCurrentSum.m128i_i64[0] < 0) {\n            vCurrentSum = vZero;\n        }\n        if (i + 1 < nums.size() && vCurrentSum.m128i_i64[1] < 0) {\n            vCurrentSum.m128i_i64[1] = 0;\n        }\n\n        minSum = std::min(minSum, _mm_extract_epi64(vMinSum, 0));\n        if (i + 1 < nums.size()) {\n            minSum = std::min(minSum, _mm_extract_epi64(vMinSum, 1));\n        }\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    int n = nums.size();\n    int64_t minSum = LLONG_MAX;\n    int64_t currentSum = 0;\n\n    __m128i vMinSum = _mm_set1_epi64x(LLONG_MAX);\n    __m128i vCurrentSum = _mm_setzero_si128();\n    __m128i vNums;\n\n    for (int i = 0; i < n; i += 2) {\n        if (i + 1 < n) {\n            vNums = _mm_set_epi64x(nums[i + 1], nums[i]);\n        } else {\n            vNums = _mm_set_epi64x(0, nums[i]);\n        }\n\n        vCurrentSum = _mm_add_epi64(vCurrentSum, vNums);\n        vMinSum = _mm_min_epi64(vMinSum, vCurrentSum);\n\n        if (vCurrentSum.m128i_i64[0] < 0) {\n            vCurrentSum = _mm_setzero_si128();\n        }\n\n        int64_t tempMinSum[2];\n        _mm_storeu_si128((__m128i*)tempMinSum, vMinSum);\n        minSum = std::min(minSum, std::min(tempMinSum[0], tempMinSum[1]));\n    }\n\n    return minSum;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_buckets = 0;\n    size_t rows = grid.size();\n    size_t cols = grid[0].size();\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (; j <= cols - 4; j += 4) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[i][j]));\n            sum = _mm_add_epi32(sum, chunk);\n        }\n\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n        int row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n\n        total_buckets += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n    int size = grid.size() * grid[0].size();\n    int *data = new int[size];\n\n    for (int i = 0, k = 0; i < grid.size(); ++i) {\n        for (int j = 0; j < grid[i].size(); ++j, ++k) {\n            data[k] = grid[i][j];\n        }\n    }\n\n    __m128i *ptr = (__m128i*)data;\n    __m128i capacity_vec = _mm_set1_epi32(capacity);\n    __m128i accumulator = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += 16) {\n        __m128i chunk1 = _mm_loadu_si128(&ptr[i]);\n        __m128i chunk2 = _mm_loadu_si128(&ptr[i + 4]);\n        __m128i chunk3 = _mm_loadu_si128(&ptr[i + 8]);\n        __m128i chunk4 = _mm_loadu_si128(&ptr[i + 12]);\n\n        __m128i sum1 = _mm_add_epi32(chunk1, chunk2);\n        __m128i sum2 = _mm_add_epi32(chunk3, chunk4);\n        __m128i sum = _mm_add_epi32(sum1, sum2);\n\n        __m128i div = _mm_div_epi32(sum, capacity_vec);\n        accumulator = _mm_add_epi32(accumulator, div);\n    }\n\n    alignas(16) int result[4];\n    _mm_store_si128((__m128i*)result, accumulator);\n\n    total_buckets = result[0] + result[1] + result[2] + result[3];\n\n    delete[] data;\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint max_fill_simd(const std::vector<std::vector<int>> & grid, int capacity) {\n    int total_buckets = 0;\n    int well_length = grid[0].size();\n    int num_wells = grid.size();\n\n    for (int i = 0; i < num_wells; i += 4) {\n        __m128i sum = _mm_setzero_si128();\n        for (int j = 0; j < well_length; ++j) {\n            __m128i values = _mm_setr_epi32(\n                (i + 0 < num_wells) ? grid[i + 0][j] : 0,\n                (i + 1 < num_wells) ? grid[i + 1][j] : 0,\n                (i + 2 < num_wells) ? grid[i + 2][j] : 0,\n                (i + 3 < num_wells) ? grid[i + 3][j] : 0\n            );\n            sum = _mm_add_epi32(sum, values);\n        }\n\n        alignas(16) int results[4];\n        _mm_store_si128((__m128i*)results, sum);\n\n        for (int k = 0; k < 4; ++k) {\n            if (i + k < num_wells) {\n                total_buckets += (results[k] + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_buckets = 0;\n    size_t rows = grid.size();\n    size_t cols = grid[0].size();\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        __m128i sum = _mm_setzero_si128();\n\n        for (; j <= cols - 4; j += 4) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[i][j]));\n            sum = _mm_add_epi32(sum, chunk);\n        }\n\n        alignas(16) int temp[4] = {0, 0, 0, 0};\n        _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum);\n        int row_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        for (; j < cols; ++j) {\n            row_sum += grid[i][j];\n        }\n\n        total_buckets += (row_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        int j = 0;\n        while (j < cols) {\n            __m128i sum = _mm_setzero_si128();\n            int k = 0;\n            for (; k < 4 && j < cols; ++j, ++k) {\n                sum = _mm_add_epi32(sum, _mm_set1_epi32(grid[i][j]));\n            }\n            int partial_sum;\n            _mm_store_ss(&partial_sum, _mm_cvtsi128_ss(sum));\n            total_buckets += (partial_sum + capacity - 1) / capacity;\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i oddMask = _mm_set1_epi16(1);\n    __m128i evenPosMask = _mm_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i oddCheck = _mm_and_si128(chunk, oddMask);\n        __m128i evenPosCheck = _mm_and_si128(evenPosMask, oddCheck);\n        __m128i filtered = _mm_and_si128(chunk, evenPosCheck);\n        sum = _mm_add_epi16(sum, filtered);\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int64_t result = 0;\n    for (int i = 0; i < 8; ++i) {\n        result += temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i oddMask = _mm_set1_epi16(1);\n    __m128i evenPosMask = _mm_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i oddCheck = _mm_and_si128(chunk, oddMask);\n        __m128i evenPosCheck = _mm_and_si128(evenPosMask, oddCheck);\n        __m128i filtered = _mm_and_si128(chunk, evenPosCheck);\n        sum = _mm_add_epi16(sum, filtered);\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int64_t result = 0;\n    for (int i = 0; i < 8; ++i) {\n        result += temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i oddMask = _mm_set1_epi16(1);\n    __m128i evenPosMask = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i isOdd = _mm_and_si128(chunk, oddMask);\n        __m128i isEvenPos = _mm_and_si128(evenPosMask, _mm_cmpeq_epi16(chunk, chunk));\n        __m128i mask = _mm_and_si128(isOdd, isEvenPos);\n        __m128i oddElements = _mm_and_si128(chunk, mask);\n        sum = _mm_add_epi32(sum, _mm_srai_epi32(_mm_unpacklo_epi16(oddElements, oddElements), 16));\n        sum = _mm_add_epi32(sum, _mm_srai_epi32(_mm_unpackhi_epi16(oddElements, oddElements), 16));\n    }\n\n    sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(1, 0, 3, 2)));\n    sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));\n\n    int64_t result = _mm_cvtsi128_si32(sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i oddMask = _mm_set1_epi16(1);\n    __m128i evenMask = _mm_set1_epi16(2);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i oddCheck = _mm_and_si128(chunk, oddMask);\n        __m128i evenPosCheck = _mm_and_si128(_mm_set_epi16(0, 7, 0, 5, 0, 3, 0, 1), evenMask);\n        __m128i oddInEvenPos = _mm_and_si128(oddCheck, evenPosCheck);\n        __m128i filtered = _mm_and_si128(chunk, oddInEvenPos);\n        sum = _mm_add_epi32(sum, _mm_srai_epi32(_mm_unpacklo_epi16(filtered, filtered), 16));\n        sum = _mm_add_epi32(sum, _mm_srai_epi32(_mm_unpackhi_epi16(filtered, filtered), 16));\n    }\n\n    sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(2, 3, 0, 1)));\n    sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));\n\n    return _mm_cvtsi128_si32(sum);\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i oddMask = _mm_set1_epi16(0x0001);\n    __m128i evenPosMask = _mm_setr_epi16(0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF, 0, 0xFFFF);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i oddCheck = _mm_and_si128(chunk, oddMask);\n        __m128i evenPosCheck = _mm_and_si128(chunk, evenPosMask);\n        __m128i validOdds = _mm_and_si128(oddCheck, evenPosCheck);\n        sum = _mm_add_epi16(sum, validOdds);\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n\n    int64_t result = 0;\n    for (int i = 0; i < 8; ++i) {\n        result += temp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(99);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i hundred = _mm_set1_epi32(100);\n\n    int i = 0;\n    for (; i <= k - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i cmp1 = _mm_cmpgt_epi32(chunk, mask);\n        __m128i cmp2 = _mm_cmplt_epi32(chunk, hundred);\n        __m128i valid = _mm_and_si128(cmp1, cmp2);\n        __m128i filtered = _mm_and_si128(chunk, valid);\n        sum = _mm_add_epi32(sum, filtered);\n    }\n\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < k; ++i) {\n        if (arr[i] > 9 && arr[i] < 100) {\n            result += arr[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i limit = _mm_set1_epi32(100);\n    __m128i mask;\n    __m128i temp;\n\n    int i = 0;\n    for (; i <= k - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        mask = _mm_cmplt_epi32(chunk, limit);\n        temp = _mm_and_si128(chunk, mask);\n        sum = _mm_add_epi32(sum, temp);\n    }\n\n    alignas(16) int result[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(result), sum);\n    int total_sum = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < k; ++i) {\n        if (arr[i] < 100) {\n            total_sum += arr[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i limit = _mm_set1_epi32(100);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i mask = _mm_cmplt_epi32(chunk, limit);\n        __m128i filtered = _mm_and_si128(chunk, mask);\n        sum = _mm_add_epi32(sum, filtered);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i max_two_digits = _mm_set1_epi32(99);\n    __m128i mask = _mm_set1_epi32(0xFF);\n\n    int i = 0;\n    for (; i <= k - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_chunk = _mm_abs_epi32(chunk);\n        __m128i cmp_mask = _mm_cmplt_epi32(abs_chunk, max_two_digits);\n        __m128i filtered_chunk = _mm_and_si128(chunk, cmp_mask);\n        sum = _mm_add_epi32(sum, filtered_chunk);\n    }\n\n    alignas(16) int temp[4] = {0, 0, 0, 0};\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < k; ++i) {\n        if (arr[i] <= 99 && arr[i] >= -99) {\n            result += arr[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i limit = _mm_set1_epi32(100);\n    __m128i mask;\n    __m128i temp;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        mask = _mm_cmpgt_epi32(limit, chunk);\n        temp = _mm_and_si128(chunk, mask);\n        sum = _mm_add_epi32(sum, temp);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i curr;\n    __m128i dup_check = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (int i = 1; i < n; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i cmp = _mm_cmpgt_epi32(curr, prev);\n        __m128i eq = _mm_cmpeq_epi32(curr, prev);\n        __m128i gt_two = _mm_sub_epi32(_mm_add_epi32(curr, two), prev);\n        __m128i gt_one = _mm_sub_epi32(_mm_add_epi32(curr, one), prev);\n\n        dup_check = _mm_or_si128(dup_check, _mm_and_si128(eq, gt_two));\n        dup_check = _mm_or_si128(dup_check, _mm_and_si128(gt_one, _mm_andnot_si128(eq, cmp)));\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp, _mm_setzero_si128())) != 0xFFFF) {\n            return false;\n        }\n\n        prev = curr;\n    }\n\n    if (_mm_movemask_epi8(_mm_cmpeq_epi32(dup_check, _mm_setzero_si128())) != 0xFFFF) {\n        return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i curr;\n    __m128i mask;\n    __m128i dup_mask = _mm_setzero_si128();\n\n    for (size_t i = 1; i <= n - 4; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        mask = _mm_cmplt_epi32(prev, curr);\n        if (_mm_movemask_epi8(mask) != 0xFFFF) return false;\n\n        dup_mask = _mm_or_si128(dup_mask, _mm_cmpeq_epi32(prev, curr));\n        if (_mm_movemask_epi8(dup_mask) != 0) return false;\n\n        prev = curr;\n    }\n\n    for (size_t i = (n & ~3); i < n; ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (lst[i] == lst[i - 1] && (i + 1 < n && lst[i + 1] == lst[i])) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i curr;\n    __m128i mask;\n    __m128i dup_mask = _mm_setzero_si128();\n    int dup_count = 0;\n\n    for (int i = 1; i <= n - 4; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        mask = _mm_cmpgt_epi32(curr, prev);\n        if (_mm_movemask_epi8(mask) != 0) return false;\n\n        __m128i eq_mask = _mm_cmpeq_epi32(curr, prev);\n        dup_mask = _mm_or_si128(dup_mask, eq_mask);\n        dup_count += _mm_movemask_epi8(eq_mask) != 0 ? 1 : 0;\n        if (dup_count > 1) return false;\n\n        prev = curr;\n    }\n\n    for (int i = (n & ~3); i < n; ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (lst[i] == lst[i - 1]) {\n            if (++dup_count > 1) return false;\n        } else {\n            dup_count = 0;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i curr;\n    __m128i mask;\n    __m128i dup_mask = _mm_setzero_si128();\n\n    for (int i = 1; i < n; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        mask = _mm_cmplt_epi32(prev, curr);\n        if (_mm_movemask_epi8(mask) != 0xFFFF) return false;\n\n        dup_mask = _mm_or_si128(dup_mask, _mm_cmpeq_epi32(prev, curr));\n        if (_mm_movemask_epi8(dup_mask) != 0x0000) return false;\n\n        prev = curr;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    if (n <= 1) return true;\n\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i curr;\n    __m128i mask;\n    __m128i dup_mask = _mm_setzero_si128();\n\n    for (int i = 1; i < n; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        mask = _mm_cmpgt_epi32(curr, prev);\n        dup_mask = _mm_or_si128(dup_mask, _mm_cmpeq_epi32(curr, prev));\n\n        if (_mm_movemask_epi8(mask) != 0xFFFF) return false;\n        if (_mm_movemask_epi8(dup_mask) == 0xFFFF) return false;\n\n        prev = curr;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_chunk = _mm_andnot_si128(sign_mask, chunk);\n        __m128i sign_chunk = _mm_and_si128(chunk, sign_mask);\n\n        sum = _mm_add_epi32(sum, abs_chunk);\n        prod = _mm_mul_epi32(prod, sign_chunk);\n    }\n\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int sum_total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), prod);\n    int prod_total = temp[0] & temp[1] & temp[2] & temp[3];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        sum_total += (val >= 0) ? val : -val;\n        prod_total &= (val >= 0) ? 1 : -1;\n    }\n\n    return sum_total * ((prod_total & 0x80000000) ? -1 : 1);\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_chunk = _mm_abs_epi32(chunk);\n        __m128i sign_chunk = _mm_and_si128(chunk, sign_mask);\n        __m128i sign_bits = _mm_or_si128(sign_chunk, _mm_set1_epi32(1));\n\n        sum = _mm_add_epi32(sum, abs_chunk);\n        prod = _mm_mul_epi32(prod, sign_bits);\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int sum_total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), prod);\n    int prod_total = temp[0] & temp[1] & temp[2] & temp[3];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        sum_total += std::abs(val);\n        prod_total &= (val < 0) ? -1 : 1;\n    }\n\n    return sum_total * prod_total;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_chunk = _mm_abs_epi32(chunk);\n        __m128i sign_chunk = _mm_and_si128(chunk, sign_mask);\n        sign_chunk = _mm_or_si128(sign_chunk, _mm_set1_epi32(1));\n\n        sum = _mm_add_epi32(sum, abs_chunk);\n        prod = _mm_mul_epi32(prod, sign_chunk);\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int sum_total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), prod);\n    int prod_total = temp[0] & temp[1] & temp[2] & temp[3];\n\n    for (; i < arr.size(); ++i) {\n        int val = arr[i];\n        int sign = (val > 0) - (val < 0);\n        sum_total += std::abs(val);\n        prod_total &= sign;\n    }\n\n    return sum_total * prod_total;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    for (size_t i = 0; i < arr.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_chunk = _mm_abs_epi32(chunk);\n        __m128i sign_chunk = _mm_and_si128(chunk, sign_mask);\n        __m128i sign_bits = _mm_or_si128(sign_chunk, _mm_set1_epi32(1));\n\n        sum = _mm_add_epi32(sum, abs_chunk);\n        prod = _mm_mul_epi32(prod, sign_bits);\n    }\n\n    int sums[4];\n    int prods[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sums), sum);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(prods), prod);\n\n    int total_sum = sums[0] + sums[1] + sums[2] + sums[3];\n    int total_prod = prods[0] * prods[1] * prods[2] * prods[3];\n\n    return total_sum * total_prod;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    size_t i;\n    for (i = 0; i <= arr.size() - 4; i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i signs = _mm_and_si128(chunk, sign_mask);\n        __m128i magnitudes = _mm_andnot_si128(sign_mask, chunk);\n\n        prod = _mm_mul_epi32(prod, signs);\n        sum = _mm_add_epi32(sum, magnitudes);\n    }\n\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum);\n    int sum_total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), prod);\n    int prod_total = temp[0] & temp[1] & temp[2] & temp[3];\n\n    for (; i < arr.size(); ++i) {\n        int sign = arr[i] & 0x80000000;\n        int magnitude = arr[i] & ~0x80000000;\n        prod_total &= sign;\n        sum_total += magnitude;\n    }\n\n    return sum_total * (prod_total ? -1 : 1);\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            for (int l = 1; l < k; ++l) {\n                int minValue = INT_MAX;\n                __m128i vMin = _mm_set1_epi32(INT_MAX);\n\n                if (i > 0) {\n                    __m128i v = _mm_set1_epi32(grid[i-1][j]);\n                    vMin = _mm_min_epi32(vMin, v);\n                }\n                if (i < N-1) {\n                    __m128i v = _mm_set1_epi32(grid[i+1][j]);\n                    vMin = _mm_min_epi32(vMin, v);\n                }\n                if (j > 0) {\n                    __m128i v = _mm_set1_epi32(grid[i][j-1]);\n                    vMin = _mm_min_epi32(vMin, v);\n                }\n                if (j < N-1) {\n                    __m128i v = _mm_set1_epi32(grid[i][j+1]);\n                    vMin = _mm_min_epi32(vMin, v);\n                }\n\n                _mm_storeu_si128((__m128i*)&minValue, vMin);\n                currentPath[l] = minValue;\n            }\n\n            if (std::lexicographical_compare(currentPath.begin(), currentPath.end(), minPath.begin(), minPath.end())) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            for (int step = 1; step < k; ++step) {\n                int minValue = INT_MAX;\n                __m128i vMin = _mm_set1_epi32(INT_MAX);\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) == abs(dj)) continue;\n                        int ni = i + di * step;\n                        int nj = j + dj * step;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            int value = grid[ni][nj];\n                            __m128i vValue = _mm_set1_epi32(value);\n                            vMin = _mm_min_epi32(vMin, vValue);\n                            minValue = std::min(minValue, value);\n                        }\n                    }\n                }\n\n                currentPath[step] = minValue;\n            }\n\n            if (currentPath < minPath) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            for (int step = 1; step < k; ++step) {\n                int minValue = INT_MAX;\n                __m128i vMin = _mm_set1_epi32(INT_MAX);\n\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (abs(di) != abs(dj) && i + di >= 0 && i + di < N && j + dj >= 0 && j + dj < N) {\n                            int value = grid[i + di][j + dj];\n                            __m128i vValue = _mm_set1_epi32(value);\n                            vMin = _mm_min_epi32(vMin, vValue);\n                            minValue = std::min(minValue, value);\n                        }\n                    }\n                }\n\n                currentPath[step] = minValue;\n                i += (minValue / N);\n                j += (minValue % N);\n            }\n\n            if (std::lexicographical_compare(currentPath.begin(), currentPath.end(), minPath.begin(), minPath.end())) {\n                minPath = currentPath;\n            }\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            __m128i pathVec = _mm_set1_epi32(grid[i][j]);\n            __m128i minPathVec = _mm_loadu_si128((__m128i*)&minPath[0]);\n\n            for (int step = 1; step < k; ++step) {\n                int minNeighbor = INT_MAX;\n                if (i > 0) minNeighbor = std::min(minNeighbor, grid[i-1][j]);\n                if (i < N-1) minNeighbor = std::min(minNeighbor, grid[i+1][j]);\n                if (j > 0) minNeighbor = std::min(minNeighbor, grid[i][j-1]);\n                if (j < N-1) minNeighbor = std::min(minNeighbor, grid[i][j+1]);\n\n                currentPath[step] = minNeighbor;\n                pathVec = _mm_set_epi32(currentPath[step], currentPath[step-1], currentPath[step-2], currentPath[step-3]);\n                minPathVec = _mm_min_epi32(minPathVec, pathVec);\n            }\n\n            _mm_storeu_si128((__m128i*)&minPath[0], minPathVec);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k) {\n    int N = grid.size();\n    std::vector<int> minPath(k, INT_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath(k, 0);\n            currentPath[0] = grid[i][j];\n\n            __m128i pathVec = _mm_set1_epi32(grid[i][j]);\n            __m128i minPathVec = _mm_loadu_si128((__m128i*)&minPath[0]);\n\n            for (int step = 1; step < k; ++step) {\n                int minNeighbor = INT_MAX;\n                if (i > 0) minNeighbor = std::min(minNeighbor, grid[i-1][j]);\n                if (i < N-1) minNeighbor = std::min(minNeighbor, grid[i+1][j]);\n                if (j > 0) minNeighbor = std::min(minNeighbor, grid[i][j-1]);\n                if (j < N-1) minNeighbor = std::min(minNeighbor, grid[i][j+1]);\n\n                currentPath[step] = minNeighbor;\n                pathVec = _mm_insert_epi16(pathVec, minNeighbor, step);\n            }\n\n            minPathVec = _mm_min_epu32(minPathVec, pathVec);\n            _mm_storeu_si128((__m128i*)&minPath[0], minPathVec);\n        }\n    }\n\n    return minPath;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    uint64_t temp = n;\n    uint64_t digits[4] = {0};\n    int count = 0;\n\n    while (temp > 0) {\n        digits[count++] = temp % 10;\n        temp /= 10;\n    }\n\n    __m128i digits_vec = _mm_loadu_si128((__m128i*)digits);\n    __m128i odd_digits = _mm_and_si128(digits_vec, odd_mask);\n    __m128i is_odd = _mm_cmpeq_epi8(odd_digits, odd_mask);\n\n    for (int i = 0; i < count; ++i) {\n        if (_mm_extract_epi8(is_odd, i)) {\n            product = _mm_mullo_epi32(product, _mm_set1_epi32(digits[i]));\n        }\n    }\n\n    uint64_t result = _mm_extract_epi32(product, 0) * _mm_extract_epi32(product, 1) * _mm_extract_epi32(product, 2) * _mm_extract_epi32(product, 3);\n\n    return result == 1 ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i digit_vec = _mm_set1_epi32(digit);\n        __m128i is_odd = _mm_and_si128(digit_vec, odd_mask);\n        __m128i is_odd_mask = _mm_cmpeq_epi8(is_odd, odd_mask);\n\n        product = _mm_and_si128(product, is_odd_mask);\n        product = _mm_mullo_epi32(product, digit_vec);\n    }\n\n    __m128i result = _mm_hadd_epi32(product, product);\n    result = _mm_hadd_epi32(result, result);\n\n    return _mm_cvtsi128_si32(result);\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i digit_vec = _mm_set1_epi32(digit);\n        __m128i is_odd = _mm_and_si128(digit_vec, odd_mask);\n        __m128i odd_digit = _mm_and_si128(digit_vec, is_odd);\n\n        product = _mm_mullo_epi32(product, odd_digit);\n    }\n\n    uint64_t result = _mm_extract_epi32(product, 0) * _mm_extract_epi32(product, 1) * _mm_extract_epi32(product, 2) * _mm_extract_epi32(product, 3);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i digit_vec = _mm_set1_epi32(digit);\n        __m128i is_odd = _mm_and_si128(digit_vec, odd_mask);\n        __m128i odd_digit = _mm_and_si128(digit_vec, is_odd);\n\n        product = _mm_mullo_epi32(product, odd_digit);\n    }\n\n    __m128i result = _mm_packus_epi32(product, zero);\n    result = _mm_packus_epi16(result, zero);\n\n    uint64_t final_result = _mm_cvtsi128_si64(result);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    while (n > 0) {\n        uint64_t digit = n % 10;\n        n /= 10;\n\n        __m128i digit_vec = _mm_set1_epi32(digit);\n        __m128i is_odd = _mm_and_si128(digit_vec, odd_mask);\n        __m128i odd_digit = _mm_and_si128(digit_vec, is_odd);\n\n        product = _mm_mullo_epi32(product, _mm_or_si128(odd_digit, _mm_set1_epi32(1)));\n    }\n\n    uint64_t result = _mm_extract_epi32(product, 0) * _mm_extract_epi32(product, 1) * _mm_extract_epi32(product, 2) * _mm_extract_epi32(product, 3);\n\n    return (result == 1) ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128 temp;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        temp = _mm_ceil_ps(data);\n        temp = _mm_mul_ps(temp, temp);\n        sum = _mm_add_ps(sum, temp);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    uint64_t total = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < lst.size(); ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total += ceil_val * ceil_val;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128 temp;\n\n    size_t i = 0;\n    for (; i <= lst.size() - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        temp = _mm_cvtps_epi32(_mm_ceil_ps(data));\n        temp = _mm_cvtepi32_ps(temp);\n        temp = _mm_mul_ps(temp, temp);\n        sum = _mm_add_ps(sum, temp);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    uint64_t total = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < lst.size(); ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total += ceil_val * ceil_val;\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128 temp;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        temp = _mm_ceil_ps(data);\n        temp = _mm_mul_ps(temp, temp);\n        sum = _mm_add_ps(sum, temp);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n\n    return static_cast<uint64_t>(result[0] + result[1] + result[2] + result[3]);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128 temp;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128 ceil_data = _mm_ceil_ps(data);\n        __m128 sq_data = _mm_mul_ps(ceil_data, ceil_data);\n        sum = _mm_add_ps(sum, sq_data);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    uint64_t total_sum = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < lst.size(); ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        total_sum += ceil_val * ceil_val;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128 temp;\n\n    size_t i;\n    for (i = 0; i <= lst.size() - 4; i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128 ceil_data = _mm_ceil_ps(data);\n        __m128 sq_data = _mm_mul_ps(ceil_data, ceil_data);\n        sum = _mm_add_ps(sum, sq_data);\n    }\n\n    float temp_result[4];\n    _mm_storeu_ps(temp_result, sum);\n    uint64_t result = temp_result[0] + temp_result[1] + temp_result[2] + temp_result[3];\n\n    for (; i < lst.size(); ++i) {\n        float ceil_val = std::ceil(lst[i]);\n        result += ceil_val * ceil_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    __m128i prev, curr, cmp;\n    int mask, index = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i + 1]));\n        cmp = _mm_cmpgt_epi32(prev, curr);\n        mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n\n        if (mask != 0) {\n            index = i + 4 - __builtin_ctz(mask) / 4;\n        }\n    }\n\n    if (n % 4 != 0) {\n        int remainder = n % 4;\n        int temp[4] = {INT_MAX, INT_MAX, INT_MAX, INT_MAX};\n        for (int i = 0; i < remainder; ++i) {\n            temp[i] = arr[n - remainder + i];\n        }\n\n        prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[n - remainder - 1]));\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        cmp = _mm_cmpgt_epi32(prev, curr);\n        mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n\n        if (mask != 0) {\n            index = n - remainder + 3 - __builtin_ctz(mask) / 4;\n        }\n    }\n\n    return index;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m128i prev = _mm_set1_epi32(INT_MIN);\n    __m128i current;\n    __m128i mask = _mm_set1_epi32(1);\n    int result = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        int cmp_mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n\n        if (cmp_mask != 0) {\n            int index = __builtin_ctz(cmp_mask) / 4;\n            result = i + index;\n        }\n\n        prev = current;\n    }\n\n    if (n % 4 != 0) {\n        int remaining = n % 4;\n        int temp[4] = {INT_MIN, INT_MIN, INT_MIN, INT_MIN};\n        for (int i = 0; i < remaining; ++i) {\n            temp[i] = arr[n - remaining + i];\n        }\n\n        current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        int cmp_mask = _mm_movemask_epi8(_mm_packs_epi32(cmp, cmp));\n\n        if (cmp_mask != 0) {\n            int index = __builtin_ctz(cmp_mask) / 4;\n            result = n - remaining + index;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    __m128i prev, current;\n    int max_index = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        if (i == 0) {\n            prev = _mm_set1_epi32(INT_MIN);\n        } else {\n            prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i - 1]));\n            prev = _mm_alignr_epi8(prev, _mm_set1_epi32(INT_MIN), 12);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < 4; ++j) {\n                if (mask & (0xF << (j * 4))) {\n                    max_index = i + j;\n                }\n            }\n        }\n    }\n\n    if (n % 4 != 0) {\n        int remainder = n % 4;\n        int temp[4] = {INT_MIN, INT_MIN, INT_MIN, INT_MIN};\n        for (int i = 0; i < remainder; ++i) {\n            temp[i] = arr[n - remainder + i];\n        }\n\n        current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(temp));\n        prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[n - remainder - 1]));\n        prev = _mm_alignr_epi8(prev, _mm_set1_epi32(INT_MIN), 12);\n\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < remainder; ++j) {\n                if (mask & (0xF << (j * 4))) {\n                    max_index = n - remainder + j;\n                }\n            }\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    __m128i prev, current;\n    int result = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        current = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        if (i == 0) {\n            prev = _mm_srli_si128(current, 4);\n        } else {\n            prev = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i - 1]));\n            prev = _mm_srli_si128(prev, 12);\n            prev = _mm_or_si128(prev, _mm_slli_si128(current, 12));\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            for (int j = 0; j < 4; ++j) {\n                if (arr[i + j] < arr[i + j - 1]) {\n                    result = i + j;\n                }\n            }\n        }\n    }\n\n    if (n % 4 != 0) {\n        int start = n - (n % 4);\n        for (int i = start; i < n; ++i) {\n            if (arr[i] < arr[i - 1]) {\n                result = i;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    __m128i prev, curr;\n    int result = -1;\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        curr = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        if (i > 0) {\n            __m128i cmp = _mm_cmpgt_epi32(curr, _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i - 1])));\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0xFFFF) {\n                for (int j = 0; j < 4; ++j) {\n                    if (!(mask & (0xF << (j * 4)))) {\n                        result = i + j;\n                    }\n                }\n            }\n        }\n        prev = curr;\n    }\n\n    for (int i = (n & ~3); i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m128i vmin = _mm_set1_epi32(INT_MIN);\n    __m128i vmax = _mm_set1_epi32(INT_MAX);\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128i vneg = _mm_set1_epi32(0);\n    __m128i vpos = _mm_set1_epi32(0);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vcmp_neg = _mm_cmplt_epi32(v, vzero);\n        __m128i vcmp_pos = _mm_cmpgt_epi32(v, vzero);\n\n        vneg = _mm_or_si128(_mm_and_si128(vcmp_neg, v), _mm_andnot_si128(vcmp_neg, vneg));\n        vpos = _mm_or_si128(_mm_and_si128(vcmp_pos, v), _mm_andnot_si128(vcmp_pos, vpos));\n\n        vmin = _mm_max_epi32(vmin, vneg);\n        vmax = _mm_min_epi32(vmax, vpos);\n    }\n\n    int temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vmin);\n    for (int i = 0; i < 4; ++i) {\n        if (temp[i] > largest_neg && temp[i] < 0) {\n            largest_neg = temp[i];\n        }\n    }\n\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vmax);\n    for (int i = 0; i < 4; ++i) {\n        if (temp[i] < smallest_pos && temp[i] > 0) {\n            smallest_pos = temp[i];\n        }\n    }\n\n    if (largest_neg == INT_MIN) largest_neg = 0;\n    if (smallest_pos == INT_MAX) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m128i vmin = _mm_set1_epi32(INT_MIN);\n    __m128i vmax = _mm_set1_epi32(INT_MAX);\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128i vneg = _mm_set1_epi32(0);\n    __m128i vpos = _mm_set1_epi32(0);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vmask_neg = _mm_cmplt_epi32(v, vzero);\n        __m128i vmask_pos = _mm_cmpgt_epi32(v, vzero);\n\n        vneg = _mm_or_si128(_mm_and_si128(vmask_neg, v), vneg);\n        vpos = _mm_or_si128(_mm_and_si128(vmask_pos, v), vpos);\n\n        vmax = _mm_max_epi32(vmax, _mm_and_si128(vmask_neg, v));\n        vmin = _mm_min_epi32(vmin, _mm_and_si128(vmask_pos, v));\n    }\n\n    int temp_neg[4], temp_pos[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_neg), vneg);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_pos), vpos);\n\n    for (int i = 0; i < 4; ++i) {\n        if (temp_neg[i] > largest_neg) largest_neg = temp_neg[i];\n        if (temp_pos[i] != 0 && temp_pos[i] < smallest_pos) smallest_pos = temp_pos[i];\n    }\n\n    if (largest_neg == INT_MIN) largest_neg = 0;\n    if (smallest_pos == INT_MAX) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m128i vmin = _mm_set1_epi32(INT_MIN);\n    __m128i vmax = _mm_set1_epi32(INT_MAX);\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128i vpos_min = _mm_set1_epi32(INT_MAX);\n    __m128i vneg_max = _mm_set1_epi32(INT_MIN);\n\n    int size = lst.size();\n    int i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i vpos = _mm_max_epi32(v, vzero);\n        __m128i vneg = _mm_min_epi32(v, vzero);\n\n        vpos_min = _mm_min_epi32(vpos_min, vpos);\n        vneg_max = _mm_max_epi32(vneg_max, vneg);\n    }\n\n    int pos_min = INT_MAX;\n    int neg_max = INT_MIN;\n\n    int pos_mask = _mm_movemask_epi8(_mm_cmpgt_epi32(vpos_min, _mm_set1_epi32(INT_MAX - 1)));\n    int neg_mask = _mm_movemask_epi8(_mm_cmplt_epi32(vneg_max, _mm_set1_epi32(INT_MIN + 1)));\n\n    int* pos_min_ptr = (int*)&vpos_min;\n    int* neg_max_ptr = (int*)&vneg_max;\n\n    if (pos_mask != 0) {\n        for (int j = 0; j < 4; ++j) {\n            if (pos_min_ptr[j] < pos_min) {\n                pos_min = pos_min_ptr[j];\n            }\n        }\n    }\n\n    if (neg_mask != 0) {\n        for (int j = 0; j < 4; ++j) {\n            if (neg_max_ptr[j] > neg_max) {\n                neg_max = neg_max_ptr[j];\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (lst[i] > 0 && lst[i] < pos_min) {\n            pos_min = lst[i];\n        }\n        if (lst[i] < 0 && lst[i] > neg_max) {\n            neg_max = lst[i];\n        }\n    }\n\n    if (pos_min == INT_MAX) pos_min = 0;\n    if (neg_max == INT_MIN) neg_max = 0;\n\n    return {neg_max, pos_min};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m128i vmin = _mm_set1_epi32(INT_MIN);\n    __m128i vmax = _mm_set1_epi32(INT_MAX);\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128i vneg_mask = _mm_set1_epi32(0x80000000);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n    bool has_neg = false;\n    bool has_pos = false;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vneg = _mm_and_si128(v, vneg_mask);\n        __m128i vpos = _mm_xor_si128(v, vneg_mask);\n\n        __m128i vcmp_neg = _mm_cmpgt_epi32(vzero, v);\n        __m128i vcmp_pos = _mm_cmpgt_epi32(v, vzero);\n\n        __m128i vmax_neg = _mm_or_si128(_mm_and_si128(vcmp_neg, v), _mm_andnot_si128(vcmp_neg, vmin));\n        __m128i vmin_pos = _mm_or_si128(_mm_and_si128(vcmp_pos, v), _mm_andnot_si128(vcmp_pos, vmax));\n\n        vmin = _mm_max_epi32(vmin, vmax_neg);\n        vmax = _mm_min_epi32(vmax, vmin_pos);\n\n        int temp_neg[4], temp_pos[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_neg), vmax_neg);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_pos), vmin_pos);\n\n        for (int j = 0; j < 4; ++j) {\n            if (temp_neg[j] > largest_neg) {\n                largest_neg = temp_neg[j];\n                has_neg = true;\n            }\n            if (temp_pos[j] < smallest_pos && temp_pos[j] > 0) {\n                smallest_pos = temp_pos[j];\n                has_pos = true;\n            }\n        }\n    }\n\n    if (!has_neg) largest_neg = 0;\n    if (!has_pos) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) return {0, 0};\n\n    __m128i vmin = _mm_set1_epi32(INT_MIN);\n    __m128i vmax = _mm_set1_epi32(INT_MAX);\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128i vneg = _mm_set1_epi32(0);\n    __m128i vpos = _mm_set1_epi32(0);\n\n    int largest_neg = INT_MIN;\n    int smallest_pos = INT_MAX;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vcmp_neg = _mm_cmplt_epi32(v, vzero);\n        __m128i vcmp_pos = _mm_cmpgt_epi32(v, vzero);\n\n        vneg = _mm_or_si128(_mm_and_si128(vcmp_neg, v), vneg);\n        vpos = _mm_or_si128(_mm_and_si128(vcmp_pos, v), vpos);\n\n        vmax = _mm_max_epi32(vmax, _mm_and_si128(vcmp_neg, v));\n        vmin = _mm_min_epi32(vmin, _mm_and_si128(vcmp_pos, v));\n    }\n\n    int extract_neg[4];\n    int extract_pos[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(extract_neg), vneg);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(extract_pos), vpos);\n\n    for (int i = 0; i < 4; ++i) {\n        if (extract_neg[i] != 0 && extract_neg[i] > largest_neg) {\n            largest_neg = extract_neg[i];\n        }\n        if (extract_pos[i] != 0 && extract_pos[i] < smallest_pos) {\n            smallest_pos = extract_pos[i];\n        }\n    }\n\n    if (largest_neg == INT_MIN) largest_neg = 0;\n    if (smallest_pos == INT_MAX) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i factorial = _mm_set1_epi64x(1);\n    __m128i multiplier = _mm_set1_epi64x(1);\n    __m128i result = _mm_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        multiplier = _mm_set1_epi64x(i);\n        factorial = _mm_mul_epu64(factorial, multiplier);\n        result = _mm_mul_epu64(result, factorial);\n    }\n\n    uint64_t final_result;\n    _mm_storel_epi64((__m128i*)&final_result, result);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i factorial = _mm_set1_epi64x(1);\n    __m128i multiplier = _mm_set1_epi64x(1);\n    __m128i increment = _mm_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        multiplier = _mm_add_epi64(multiplier, increment);\n        factorial = _mm_mul_epu32(factorial, multiplier);\n        increment = _mm_add_epi64(increment, _mm_set1_epi64x(1));\n    }\n\n    uint64_t result = _mm_cvtsi128_si64(factorial);\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i factorial = _mm_set1_epi64x(1);\n    __m128i multiplier = _mm_set1_epi64x(1);\n    __m128i result = _mm_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        multiplier = _mm_set1_epi64x(i);\n        factorial = _mm_mul_epu32(factorial, multiplier);\n        result = _mm_mul_epu32(result, factorial);\n    }\n\n    uint64_t final_result = _mm_cvtsi128_si64(result);\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i factorial = _mm_set1_epi64x(1);\n    __m128i multiplier = _mm_set1_epi64x(1);\n    __m128i increment = _mm_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        multiplier = _mm_add_epi64(multiplier, increment);\n        factorial = _mm_mul_epu32(factorial, multiplier);\n        factorial = _mm_add_epi64(factorial, _mm_srli_si128(factorial, 8));\n    }\n\n    uint64_t result;\n    _mm_storel_epi64((__m128i*)&result, factorial);\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i factorial = _mm_set1_epi32(1);\n    __m128i multiplier = _mm_set1_epi32(1);\n    __m128i increment = _mm_set1_epi32(1);\n    __m128i limit = _mm_set1_epi32(n);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        multiplier = _mm_add_epi32(multiplier, increment);\n        factorial = _mm_mul_epu32(factorial, multiplier);\n    }\n\n    uint64_t result = 1;\n    uint64_t temp = 1;\n    for (uint64_t i = 1; i <= n; ++i) {\n        temp *= i;\n        result *= temp;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    int sum = 0;\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i v_sq = _mm_mullo_epi32(v, v);\n        __m128i v_cb = _mm_mullo_epi32(v_sq, v);\n\n        __m128i mask_mul_3 = _mm_set_epi32(\n            (i + 3) % 3 == 0 ? -1 : 0,\n            (i + 2) % 3 == 0 ? -1 : 0,\n            (i + 1) % 3 == 0 ? -1 : 0,\n            i % 3 == 0 ? -1 : 0\n        );\n        __m128i mask_mul_4 = _mm_set_epi32(\n            (i + 3) % 4 == 0 && (i + 3) % 3 != 0 ? -1 : 0,\n            (i + 2) % 4 == 0 && (i + 2) % 3 != 0 ? -1 : 0,\n            (i + 1) % 4 == 0 && (i + 1) % 3 != 0 ? -1 : 0,\n            i % 4 == 0 && i % 3 != 0 ? -1 : 0\n        );\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask_mul_3, v_sq),\n            _mm_and_si128(mask_mul_4, v_cb)\n        );\n\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), result);\n\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i squares = _mm_mullo_epi32(values, values);\n        __m128i cubes = _mm_mullo_epi32(values, _mm_mullo_epi32(values, values));\n\n        __m128i mask3 = _mm_setr_epi32(i % 3 == 0 ? -1 : 0, (i + 1) % 3 == 0 ? -1 : 0, (i + 2) % 3 == 0 ? -1 : 0, (i + 3) % 3 == 0 ? -1 : 0);\n        __m128i mask4 = _mm_setr_epi32(i % 4 == 0 ? -1 : 0, (i + 1) % 4 == 0 ? -1 : 0, (i + 2) % 4 == 0 ? -1 : 0, (i + 3) % 4 == 0 ? -1 : 0);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask3, squares), _mm_andnot_si128(mask3, _mm_and_si128(mask4, cubes)));\n\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i squares = _mm_mullo_epi32(values, values);\n        __m128i cubes = _mm_mullo_epi32(_mm_mullo_epi32(values, values), values);\n\n        __m128i mask3 = _mm_set_epi32(0, (i+3) % 3 == 0 ? -1 : 0, (i+2) % 3 == 0 ? -1 : 0, (i+1) % 3 == 0 ? -1 : 0);\n        __m128i mask4 = _mm_set_epi32(0, (i+3) % 4 == 0 ? -1 : 0, (i+2) % 4 == 0 ? -1 : 0, (i+1) % 4 == 0 ? -1 : 0);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask3, squares), _mm_andnot_si128(mask3, values));\n        result = _mm_or_si128(_mm_and_si128(mask4, cubes), _mm_andnot_si128(mask4, result));\n\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    alignas(16) int temp_sum[4];\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp_sum), sum);\n\n    return temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    int sum = 0;\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i v_sq = _mm_mullo_epi32(v, v);\n        __m128i v_cb = _mm_mullo_epi32(v_sq, v);\n\n        __m128i mask_mul_3 = _mm_set_epi32(\n            (i + 3) % 3 == 0 ? -1 : 0,\n            (i + 2) % 3 == 0 ? -1 : 0,\n            (i + 1) % 3 == 0 ? -1 : 0,\n            i % 3 == 0 ? -1 : 0\n        );\n        __m128i mask_mul_4 = _mm_set_epi32(\n            (i + 3) % 4 == 0 && (i + 3) % 3 != 0 ? -1 : 0,\n            (i + 2) % 4 == 0 && (i + 2) % 3 != 0 ? -1 : 0,\n            (i + 1) % 4 == 0 && (i + 1) % 3 != 0 ? -1 : 0,\n            i % 4 == 0 && i % 3 != 0 ? -1 : 0\n        );\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask_mul_3, v_sq),\n            _mm_and_si128(mask_mul_4, v_cb)\n        );\n\n        __m128i mask_not_mul = _mm_andnot_si128(_mm_or_si128(mask_mul_3, mask_mul_4), v);\n        result = _mm_or_si128(result, mask_not_mul);\n\n        int temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), result);\n\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128i values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask3 = _mm_set_epi32(0, (i+3) % 3 == 0 ? -1 : 0, (i+2) % 3 == 0 ? -1 : 0, (i+1) % 3 == 0 ? -1 : 0);\n        __m128i mask4 = _mm_set_epi32(0, (i+3) % 4 == 0 ? -1 : 0, (i+2) % 4 == 0 ? -1 : 0, (i+1) % 4 == 0 ? -1 : 0);\n\n        __m128i squared = _mm_mullo_epi32(values, values);\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(values, values), values);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(mask3, squared), _mm_andnot_si128(mask3, values));\n        result = _mm_or_si128(_mm_and_si128(mask4, cubed), _mm_andnot_si128(mask4, result));\n\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i ten = _mm_set1_epi32(10);\n    __m128i nine = _mm_set1_epi32(9);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i odd_mask = _mm_set1_epi32(0x55555555);\n\n    for (size_t i = 0; i <= nums.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i gt_ten = _mm_cmpgt_epi32(abs_vec, ten);\n\n        __m128i first_digit = _mm_and_si128(_mm_srli_epi32(abs_vec, 28), one);\n        __m128i last_digit = _mm_and_si128(abs_vec, one);\n        __m128i odd_first_last = _mm_and_si128(_mm_or_si128(first_digit, last_digit), odd_mask);\n\n        __m128i mask = _mm_and_si128(gt_ten, odd_first_last);\n        int mask_value = _mm_movemask_epi8(_mm_packs_epi32(mask, mask));\n\n        count += _mm_popcnt_u32(mask_value & 0x1111);\n    }\n\n    for (size_t i = (nums.size() / 4) * 4; i < nums.size(); ++i) {\n        int num = nums[i];\n        int abs_num = abs(num);\n        if (abs_num > 10) {\n            int first_digit = (abs_num >> 28) & 1;\n            int last_digit = abs_num & 1;\n            if ((first_digit | last_digit) & 1) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i v10 = _mm_set1_epi32(10);\n    __m128i v1 = _mm_set1_epi32(1);\n    __m128i v3 = _mm_set1_epi32(3);\n    __m128i v5 = _mm_set1_epi32(5);\n    __m128i v7 = _mm_set1_epi32(7);\n    __m128i v9 = _mm_set1_epi32(9);\n    __m128i vOddFirst = _mm_set_epi32(9, 7, 5, 3);\n    __m128i vOddLast = _mm_set_epi32(9, 7, 5, 3);\n\n    for (size_t i = 0; i < nums.size(); i += 4) {\n        __m128i vNums = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i vAbsNums = _mm_abs_epi32(vNums);\n        __m128i vFirstDigit = _mm_and_si128(vAbsNums, _mm_set1_epi32(0xF));\n        __m128i vLastDigit = _mm_and_si128(_mm_srli_epi32(vAbsNums, 28), _mm_set1_epi32(0xF));\n\n        __m128i vGreaterThan10 = _mm_cmpgt_epi32(vAbsNums, v10);\n        __m128i vOddFirstDigit = _mm_cmpeq_epi32(_mm_and_si128(vFirstDigit, vOddFirst), vFirstDigit);\n        __m128i vOddLastDigit = _mm_cmpeq_epi32(_mm_and_si128(vLastDigit, vOddLast), vLastDigit);\n\n        __m128i vResult = _mm_and_si128(_mm_and_si128(vGreaterThan10, vOddFirstDigit), vOddLastDigit);\n\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(vResult, vResult));\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i ten = _mm_set1_epi32(10);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i five = _mm_set1_epi32(5);\n    __m128i seven = _mm_set1_epi32(7);\n    __m128i nine = _mm_set1_epi32(9);\n\n    for (size_t i = 0; i < nums.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i absChunk = _mm_abs_epi32(chunk);\n\n        __m128i gtTen = _mm_cmpgt_epi32(absChunk, ten);\n\n        __m128i firstDigit = _mm_and_si128(absChunk, _mm_set1_epi32(0xF));\n        __m128i lastDigit = _mm_and_si128(_mm_srli_epi32(absChunk, 28), _mm_set1_epi32(0xF));\n\n        __m128i firstOdd = _mm_or_si128(\n            _mm_or_si128(\n                _mm_cmpeq_epi32(firstDigit, one),\n                _mm_cmpeq_epi32(firstDigit, three)\n            ),\n            _mm_or_si128(\n                _mm_cmpeq_epi32(firstDigit, five),\n                _mm_cmpeq_epi32(firstDigit, seven)\n            )\n        );\n        firstOdd = _mm_or_si128(firstOdd, _mm_cmpeq_epi32(firstDigit, nine));\n\n        __m128i lastOdd = _mm_or_si128(\n            _mm_or_si128(\n                _mm_cmpeq_epi32(lastDigit, one),\n                _mm_cmpeq_epi32(lastDigit, three)\n            ),\n            _mm_or_si128(\n                _mm_cmpeq_epi32(lastDigit, five),\n                _mm_cmpeq_epi32(lastDigit, seven)\n            )\n        );\n        lastOdd = _mm_or_si128(lastOdd, _mm_cmpeq_epi32(lastDigit, nine));\n\n        __m128i bothOdd = _mm_and_si128(firstOdd, lastOdd);\n        __m128i result = _mm_and_si128(gtTen, bothOdd);\n\n        int mask = _mm_movemask_epi8(_mm_packs_epi32(result, result));\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i v10 = _mm_set1_epi32(10);\n    __m128i v1 = _mm_set1_epi32(1);\n    __m128i v3 = _mm_set1_epi32(3);\n    __m128i v5 = _mm_set1_epi32(5);\n    __m128i v7 = _mm_set1_epi32(7);\n    __m128i v9 = _mm_set1_epi32(9);\n    __m128i vOddFirst = _mm_set_epi32(9, 7, 5, 3);\n    __m128i vOddLast = _mm_set_epi32(9, 7, 5, 3);\n\n    for (size_t i = 0; i < nums.size(); i += 4) {\n        __m128i vNums = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i vAbsNums = _mm_abs_epi32(vNums);\n        __m128i vGreaterThan10 = _mm_cmpgt_epi32(vAbsNums, v10);\n\n        __m128i vFirstDigit = _mm_and_si128(vAbsNums, _mm_set1_epi32(0xF));\n        __m128i vLastDigit = _mm_and_si128(_mm_srli_epi32(vAbsNums, 28), _mm_set1_epi32(0xF));\n\n        __m128i vFirstDigitOdd = _mm_or_si128(\n            _mm_or_si128(\n                _mm_cmpeq_epi32(vFirstDigit, v1),\n                _mm_cmpeq_epi32(vFirstDigit, v3)),\n            _mm_or_si128(\n                _mm_cmpeq_epi32(vFirstDigit, v5),\n                _mm_cmpeq_epi32(vFirstDigit, v7)));\n        vFirstDigitOdd = _mm_or_si128(vFirstDigitOdd, _mm_cmpeq_epi32(vFirstDigit, v9));\n\n        __m128i vLastDigitOdd = _mm_or_si128(\n            _mm_or_si128(\n                _mm_cmpeq_epi32(vLastDigit, v1),\n                _mm_cmpeq_epi32(vLastDigit, v3)),\n            _mm_or_si128(\n                _mm_cmpeq_epi32(vLastDigit, v5),\n                _mm_cmpeq_epi32(vLastDigit, v7)));\n        vLastDigitOdd = _mm_or_si128(vLastDigitOdd, _mm_cmpeq_epi32(vLastDigit, v9));\n\n        __m128i vOddDigits = _mm_and_si128(vFirstDigitOdd, vLastDigitOdd);\n        __m128i vResult = _mm_and_si128(vGreaterThan10, vOddDigits);\n\n        int mask = _mm_movemask_epi8(vResult);\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i ten = _mm_set1_epi32(10);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i five = _mm_set1_epi32(5);\n    __m128i seven = _mm_set1_epi32(7);\n    __m128i nine = _mm_set1_epi32(9);\n\n    for (size_t i = 0; i < nums.size(); i += 4) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i absChunk = _mm_abs_epi32(chunk);\n        __m128i gtTen = _mm_cmpgt_epi32(absChunk, ten);\n\n        __m128i firstDigit = _mm_and_si128(chunk, _mm_set1_epi32(0xF));\n        __m128i lastDigit = _mm_and_si128(_mm_srli_epi32(chunk, 28), one);\n\n        __m128i oddFirst = _mm_or_si128(\n            _mm_or_si128(_mm_cmpeq_epi32(firstDigit, one), _mm_cmpeq_epi32(firstDigit, three)),\n            _mm_or_si128(_mm_cmpeq_epi32(firstDigit, five), _mm_cmpeq_epi32(firstDigit, seven))\n        );\n        oddFirst = _mm_or_si128(oddFirst, _mm_cmpeq_epi32(firstDigit, nine));\n\n        __m128i oddLast = _mm_or_si128(\n            _mm_or_si128(_mm_cmpeq_epi32(lastDigit, one), _mm_cmpeq_epi32(lastDigit, three)),\n            _mm_or_si128(_mm_cmpeq_epi32(lastDigit, five), _mm_cmpeq_epi32(lastDigit, seven))\n        );\n        oddLast = _mm_or_si128(oddLast, _mm_cmpeq_epi32(lastDigit, nine));\n\n        __m128i oddBoth = _mm_and_si128(oddFirst, oddLast);\n        __m128i result = _mm_and_si128(gtTen, oddBoth);\n\n        int mask = _mm_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask & 0x0F0F0F0F);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi64x(a[k]);\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(vi, vj), vk);\n                __m128i mod3 = _mm_and_si128(_mm_add_epi64(sum, _mm_set1_epi64x(1)), _mm_set1_epi64x(3));\n                if (_mm_extract_epi64(mod3, 0) == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi64x(a[j]);\n            __m128i vij = _mm_add_epi64(vi, vj);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi64x(a[k]);\n                __m128i vsum = _mm_add_epi64(vij, vk);\n                __m128i vmod = _mm_and_si128(vsum, _mm_set1_epi64x(3));\n                if (_mm_extract_epi64(vmod, 0) == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi64x(a[k]);\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(vi, vj), vk);\n                __m128i mod3 = _mm_and_si128(_mm_add_epi64(sum, _mm_set1_epi64x(1)), _mm_set1_epi64x(3));\n                if (_mm_extract_epi64(mod3, 0) == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi64x(a[k]);\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(vi, vj), vk);\n                __m128i mod3 = _mm_and_si128(_mm_add_epi64(sum, _mm_set1_epi64x(1)), _mm_set1_epi64x(3));\n                if (_mm_extract_epi64(mod3, 0) == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stdlib.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    if (n < 3) return 0;\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 16);\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i-1] = i * i - i + 1;\n    }\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        __m128i vi = _mm_set1_epi64x(a[i]);\n        for (uint64_t j = i + 1; j < n - 1; ++j) {\n            __m128i vj = _mm_set1_epi64x(a[j]);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i vk = _mm_set1_epi64x(a[k]);\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(vi, vj), vk);\n                __m128i mod3 = _mm_and_si128(_mm_add_epi64(sum, _mm_set1_epi64x(1)), _mm_set1_epi64x(3));\n                if (_mm_extract_epi64(mod3, 0) == 0) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i two_vec = _mm_set1_epi32(2);\n    __m128i i_vec = _mm_set_epi32(0, 1, 2, 3);\n    __m128i prime_vec = _mm_set1_epi32(1);\n\n    for (__m128i j_vec = _mm_set1_epi32(2); ; j_vec = _mm_add_epi32(j_vec, two_vec)) {\n        __m128i mul_vec = _mm_mullo_epi32(j_vec, j_vec);\n        __m128i cmp_vec = _mm_cmplt_epi32(mul_vec, n_vec);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp_vec, _mm_setzero_si128())) == 0xFFFF) {\n            break;\n        }\n        __m128i mod_vec = _mm_set_epi32(0, n % _mm_extract_epi32(j_vec, 1), n % _mm_extract_epi32(j_vec, 2), n % _mm_extract_epi32(j_vec, 3));\n        __m128i zero_vec = _mm_setzero_si128();\n        __m128i cmp_mod_vec = _mm_cmpeq_epi32(mod_vec, zero_vec);\n        if (_mm_movemask_epi8(cmp_mod_vec) != 0) {\n            prime_vec = _mm_set1_epi32(0);\n            break;\n        }\n    }\n\n    int is_prime = _mm_extract_epi32(prime_vec, 0);\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i x_vec = _mm_set1_epi32(x);\n    __m128i y_vec = _mm_set1_epi32(y);\n    __m128i two_vec = _mm_set1_epi32(2);\n    __m128i one_vec = _mm_set1_epi32(1);\n\n    __m128i i_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (__m128i j_vec = two_vec; ; j_vec = _mm_add_epi32(j_vec, one_vec)) {\n        __m128i div_vec = _mm_div_epi32(n_vec, j_vec);\n        __m128i mul_vec = _mm_mul_epi32(div_vec, j_vec);\n        __m128i cmp_vec = _mm_cmpeq_epi32(mul_vec, n_vec);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp_vec, mask)) == 0xFFFF) {\n            return _mm_extract_epi32(y_vec, 0);\n        }\n\n        __m128i sqrt_vec = _mm_sqrt_epi32(j_vec);\n        __m128i sqrt_cmp_vec = _mm_cmplt_epi32(sqrt_vec, j_vec);\n\n        if (_mm_movemask_epi8(_mm_cmplt_epi32(sqrt_cmp_vec, mask)) == 0xFFFF) {\n            break;\n        }\n    }\n\n    return _mm_extract_epi32(x_vec, 0);\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i x_vec = _mm_set1_epi32(x);\n    __m128i y_vec = _mm_set1_epi32(y);\n    __m128i two_vec = _mm_set1_epi32(2);\n    __m128i i_vec = _mm_set_epi32(0, 1, 2, 3);\n    __m128i prime_vec = _mm_set1_epi32(1);\n\n    for (__m128i j_vec = _mm_set1_epi32(2); _mm_movemask_epi8(_mm_cmplt_epi32(j_vec, n_vec)) == 0xF; j_vec = _mm_add_epi32(j_vec, two_vec)) {\n        __m128i rem_vec = _mm_setzero_si128();\n        __m128i mod_vec = _mm_setzero_si128();\n\n        for (__m128i k_vec = _mm_set1_epi32(2); _mm_movemask_epi8(_mm_cmplt_epi32(k_vec, j_vec)) == 0xF; k_vec = _mm_add_epi32(k_vec, two_vec)) {\n            mod_vec = _mm_set_epi32(0, 0, 0, _mm_cvtsi128_si32(_mm_set_epi32(0, 0, 0, n % _mm_cvtsi128_si32(k_vec))));\n            rem_vec = _mm_or_si128(rem_vec, _mm_cmpeq_epi32(mod_vec, _mm_setzero_si128()));\n        }\n\n        prime_vec = _mm_and_si128(prime_vec, _mm_cmpeq_epi32(rem_vec, _mm_setzero_si128()));\n    }\n\n    __m128i result_vec = _mm_blendv_epi8(y_vec, x_vec, prime_vec);\n    return _mm_cvtsi128_si32(result_vec);\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i two_vec = _mm_set1_epi32(2);\n    __m128i i_vec = _mm_set_epi32(0, 1, 2, 3);\n    __m128i prime_vec = _mm_set1_epi32(1);\n    __m128i x_vec = _mm_set1_epi32(x);\n    __m128i y_vec = _mm_set1_epi32(y);\n\n    for (__m128i i = _mm_set_epi32(0, 1, 2, 3);\n         _mm_movemask_epi8(_mm_cmplt_epi32(i, n_vec)) != 0;\n         i = _mm_add_epi32(i, _mm_set1_epi32(4))) {\n\n        __m128i mod_vec = _mm_set_epi32(\n            n % _mm_extract_epi32(i, 3),\n            n % _mm_extract_epi32(i, 2),\n            n % _mm_extract_epi32(i, 1),\n            n % _mm_extract_epi32(i, 0)\n        );\n\n        __m128i cmp_vec = _mm_cmpeq_epi32(mod_vec, _mm_setzero_si128());\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(cmp_vec, _mm_setzero_si128())) != 0) {\n            prime_vec = _mm_set1_epi32(0);\n            break;\n        }\n    }\n\n    __m128i result_vec = _mm_blendv_epi8(y_vec, x_vec, prime_vec);\n    return _mm_extract_epi32(result_vec, 0);\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    __m128i n_vec = _mm_set1_epi32(n);\n    __m128i x_vec = _mm_set1_epi32(x);\n    __m128i y_vec = _mm_set1_epi32(y);\n    __m128i two_vec = _mm_set1_epi32(2);\n    __m128i one_vec = _mm_set1_epi32(1);\n\n    __m128i i_vec = _mm_set_epi32(3, 2, 1, 0);\n    __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (__m128i j = _mm_set1_epi32(2); ; j = _mm_add_epi32(j, one_vec)) {\n        __m128i quotient = _mm_div_epi32(n_vec, j);\n        __m128i product = _mm_mul_epi32(j, quotient);\n        __m128i remainder = _mm_sub_epi32(n_vec, product);\n\n        __m128i compare = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n        __m128i is_factor = _mm_and_si128(compare, mask);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(is_factor, mask)) == 0xFFFF) {\n            return _mm_extract_epi32(y_vec, 0);\n        }\n\n        __m128i compare_limit = _mm_cmpeq_epi32(j, _mm_sqrt_epi32(n_vec));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(compare_limit, mask)) == 0xFFFF) {\n            break;\n        }\n    }\n\n    return _mm_extract_epi32(x_vec, 0);\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m128i vSum = _mm_setzero_si128();\n    __m128i vOne = _mm_set1_epi32(1);\n    __m128i vNegOne = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 vData = _mm_loadu_ps(&lst[i]);\n        __m128i vIntData = _mm_cvtps_epi32(vData);\n        __m128i vMask = _mm_and_si128(_mm_cmpgt_epi32(vIntData, _mm_setzero_si128()), _mm_cmplt_epi32(vIntData, vNegOne));\n        __m128i vOddMask = _mm_and_si128(_mm_set1_epi32(1), vIntData);\n        __m128i vValidOdd = _mm_and_si128(vMask, vOddMask);\n        __m128i vSquare = _mm_mullo_epi32(vIntData, vIntData);\n        vSquare = _mm_and_si128(vSquare, vValidOdd);\n        vSum = _mm_add_epi32(vSum, vSquare);\n    }\n\n    alignas(16) int temp[4];\n    _mm_store_si128((__m128i*)temp, vSum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128i intData = _mm_cvttps_epi32(data);\n        __m128i mask = _mm_and_si128(intData, one);\n        __m128i cmp = _mm_cmpgt_epi32(intData, zero);\n        mask = _mm_and_si128(mask, cmp);\n\n        __m128 squared = _mm_mul_ps(data, data);\n        squared = _mm_and_ps(squared, _mm_castsi128_ps(mask));\n        sum = _mm_add_ps(sum, squared);\n    }\n\n    __m128 shuf = _mm_shuffle_ps(sum, sum, _MM_SHUFFLE(2, 3, 0, 1));\n    __m128 sum2 = _mm_add_ps(sum, shuf);\n    shuf = _mm_shuffle_ps(sum2, sum2, _MM_SHUFFLE(0, 1, 2, 3));\n    sum2 = _mm_add_ps(sum2, shuf);\n\n    uint64_t result;\n    _mm_store_ss(&result, sum2);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128i intData = _mm_cvtps_epi32(data);\n\n        __m128i isOdd = _mm_and_si128(_mm_cmpgt_epi32(_mm_and_si128(intData, one), _mm_setzero_si128()), one);\n        __m128i isPositive = _mm_cmpgt_epi32(intData, _mm_setzero_si128());\n\n        __m128i mask = _mm_and_si128(isOdd, isPositive);\n        __m128i squared = _mm_mullo_epi32(intData, intData);\n\n        sum = _mm_add_epi64(sum, _mm_and_si128(squared, _mm_cmpeq_epi32(mask, one)));\n    }\n\n    uint64_t result[2];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), sum);\n\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    __m128i sum = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128i intData = _mm_cvtps_epi32(data);\n\n        __m128i mask = _mm_and_si128(_mm_cmplt_epi32(_mm_setzero_si128(), intData), _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(intData, one), one), _mm_cmpgt_epi32(intData, _mm_setzero_si128())));\n        __m128i squared = _mm_mullo_epi32(intData, intData);\n        sum = _mm_add_epi32(sum, _mm_and_si128(squared, mask));\n    }\n\n    uint32_t sums[4];\n    _mm_storeu_si128((__m128i*)sums, sum);\n\n    return sums[0] + sums[1] + sums[2] + sums[3];\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```c++\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float>& lst) {\n    __m128 sum = _mm_setzero_ps();\n    __m128i mask = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m128 data = _mm_loadu_ps(&lst[i]);\n        __m128i intData = _mm_cvtps_epi32(data);\n        __m128i oddMask = _mm_and_si128(intData, mask);\n        __m128i zero = _mm_setzero_si128();\n        __m128i posMask = _mm_cmpgt_epi32(intData, zero);\n        __m128i finalMask = _mm_and_si128(oddMask, posMask);\n\n        __m128 squared = _mm_mul_ps(data, data);\n        squared = _mm_and_ps(squared, _mm_castsi128_ps(finalMask));\n        sum = _mm_add_ps(sum, squared);\n    }\n\n    __m128 shuf = _mm_movehl_ps(sum, sum);\n    __m128 sums = _mm_add_ps(sum, shuf);\n    shuf = _mm_shuffle_ps(sums, sums, _MM_SHUFFLE(1, 0, 3, 2));\n    sums = _mm_add_ss(sums, shuf);\n\n    uint64_t result;\n    _mm_store_ss(&result, sums);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    int i = 0;\n    for (; i <= game.size() - 4; i += 4) {\n        __m128i g = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i u = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n        __m128i diff = _mm_sub_epi32(g, u);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n    for (; i < game.size(); ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128i game_vals = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i guess_vals = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n\n        __m128i diff = _mm_sub_epi32(game_vals, guess_vals);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n\n    for (size_t i = 0; i < game.size(); i += 4) {\n        __m128i game_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i guess_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    int i = 0;\n\n    for (; i <= game.size() - 4; i += 4) {\n        __m128i g = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i x = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n        __m128i diff = _mm_sub_epi32(g, x);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    for (; i < game.size(); ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m128i game_vals = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i guess_vals = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n\n        __m128i diff = _mm_sub_epi32(game_vals, guess_vals);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    return result;\n}\n```"}

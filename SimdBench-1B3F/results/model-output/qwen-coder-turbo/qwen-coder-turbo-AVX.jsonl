{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i v_src;\n    __m256i v_dst;\n    __m256i v_mask;\n\n    for (size_t i = 0; i < length; i += 4) {\n        v_src = _mm256_loadu_si256((__m256i *)&src[i]);\n        v_mask = _mm256_loadu_si256((__m256i *)&mask[i]);\n\n        // Convert bool mask to integer mask\n        v_mask = _mm256_castps_si256(_mm256_and_ps(_mm256_castsi256_ps(v_mask), _mm256_set1_ps(1.0f)));\n\n        // Perform conditional move\n        v_dst = _mm256_blendv_epi64(v_dst, v_src, v_mask);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i v_src, v_dst, v_mask;\n    size_t i;\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v_src = _mm256_loadu_si256((__m256i *)&src[i]);\n        v_mask = _mm256_cvtepi32_epi64(_mm256_castsi128_si256(_mm256_loadu_si128((__m256i *)&mask[i])));\n        v_dst = _mm256_blendv_epi64(v_dst, v_src, v_mask);\n        _mm256_storeu_si256((__m256i *)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i v_src;\n    __m256i v_dst;\n    __m256i v_mask;\n\n    for (size_t i = 0; i < length; i += 4) {\n        v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        v_mask = _mm256_cvtepi32_epi64(_mm256_castsi128_si256(_mm256_loadu_si128((__m128i*)&mask[i])));\n\n        v_dst = _mm256_blendv_epi64(v_dst, v_src, v_mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i v_src, v_dst, v_mask;\n    size_t i;\n\n    for (i = 0; i <= length - 4; i += 4) {\n        v_src = _mm256_loadu_si256((__m256i *)&src[i]);\n        v_mask = _mm256_loadu_si256((__m256i *)&mask[i]);\n\n        // Convert boolean mask to integer mask\n        v_mask = _mm256_castps_si256(_mm256_and_ps(_mm256_castsi256_ps(v_mask), _mm256_set1_ps(1.0f)));\n\n        // Perform conditional move\n        v_dst = _mm256_blendv_epi64(v_dst, v_src, v_mask);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i v_src;\n    __m256i v_dst;\n    __m256i v_mask;\n\n    for (size_t i = 0; i < length; i += 4) {\n        v_src = _mm256_loadu_si256((__m256i *)&src[i]);\n        v_mask = _mm256_cvtepi32_epi64(_mm256_castsi128_si256(_mm256_loadu_si128((__m256i *)&mask[i])));\n\n        v_dst = _mm256_blendv_epi64(v_dst, v_src, v_mask);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256i seven_vec = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i scaled_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1_vec = _mm256_srai_epi32(scaled_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(scaled_vec, 3);\n        __m256i result_vec = _mm256_add_epi32(scaled_vec, _mm256_mullo_epi32(tmp1_vec, tmp2_vec));\n        result_vec = _mm256_div_epi32(result_vec, seven_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        int src_val = src[i];\n        int scaled_val = src_val * scale;\n        int tmp1 = scaled_val >> 3;\n        int tmp2 = scaled_val << 3;\n        int result = (scaled_val + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256i seven_vec = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i scaled_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        \n        __m256i tmp1 = _mm256_srai_epi32(scaled_vec, 3);\n        __m256i tmp2 = _mm256_slli_epi32(scaled_vec, 3);\n        \n        __m256i result = _mm256_add_epi32(scaled_vec, _mm256_mullo_epi32(tmp1, tmp2));\n        result = _mm256_div_epi32(result, seven_vec);\n        \n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int result = (element * scale + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256i seven_vec = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i scaled_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1_vec = _mm256_srai_epi32(scaled_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(scaled_vec, 3);\n        __m256i sum_vec = _mm256_add_epi32(scaled_vec, _mm256_mullo_epi32(tmp1_vec, tmp2_vec));\n        __m256i result_vec = _mm256_div_epi32(sum_vec, seven_vec);\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        int src_val = src[i];\n        int tmp1 = (src_val * scale) >> 3;\n        int tmp2 = (src_val * scale) << 3;\n        int result = (src_val * scale + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256i seven_vec = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n\n        // Step 1: Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m256i tmp1 = _mm256_srai_epi32(_mm256_mullo_epi32(src_vec, scale_vec), 3);\n\n        // Step 2: Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m256i tmp2 = _mm256_slli_epi32(_mm256_mullo_epi32(src_vec, scale_vec), 3);\n\n        // Step 3: Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7\n        __m256i numerator = _mm256_add_epi32(_mm256_mullo_epi32(src_vec, scale_vec), _mm256_mullo_epi32(tmp1, tmp2));\n        __m256i result = _mm256_div_epi32(numerator, seven_vec);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        int src_val = src[i];\n        int tmp1 = (src_val * scale) >> 3;\n        int tmp2 = (src_val * scale) << 3;\n        int result = (src_val * scale + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256i seven_vec = _mm256_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i scaled_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n\n        __m256i tmp1_vec = _mm256_srai_epi32(scaled_vec, 3);\n        __m256i tmp2_vec = _mm256_slli_epi32(scaled_vec, 3);\n\n        __m256i result_vec = _mm256_add_epi32(scaled_vec, _mm256_mullo_epi32(tmp1_vec, tmp2_vec));\n        result_vec = _mm256_div_epi32(result_vec, seven_vec);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int result = (element * scale + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 2; // Number of doubles per vector register\n    const size_t aligned_cols = cols - cols % vec_size;\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d vec;\n        for (size_t c = 0; c < aligned_cols; c += vec_size) {\n            size_t idx = r * cols + c + stride;\n            vec = _mm_loadu_pd(src + idx);\n            _mm_storeu_pd(dst + r, vec);\n        }\n        // Handle remaining elements that don't fit into a vector\n        for (size_t c = aligned_cols; c < cols; ++c) {\n            size_t idx = r * cols + c + stride;\n            dst[r] = src[idx];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t vec_bytes = vec_size * sizeof(double);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d vec;\n        size_t idx = r * cols + stride;\n\n        // Load the element into a vector register\n        vec = _mm_loadu_pd(&src[idx]);\n\n        // Store the vector register into the destination array\n        _mm_storeu_pd(&dst[r], vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t aligned_rows = rows / vec_size * vec_size;\n\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t r = 0; r < aligned_rows; r += vec_size) {\n        __m128d src_vec = _mm_loadu_pd(src + r * cols + stride);\n        _mm_storeu_pd(dst + r, src_vec);\n    }\n\n    for (size_t r = aligned_rows; r < rows; ++r) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t vec_bytes = vec_size * sizeof(double);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m128d vec;\n        size_t idx = r * cols + stride;\n\n        // Load the element into a vector register\n        vec = _mm_loadu_pd(&src[idx]);\n\n        // Store the vector register into the destination array\n        _mm_storeu_pd(&dst[r], vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t aligned_rows = rows - rows % vec_size;\n\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t r = 0; r < aligned_rows; r += vec_size) {\n        __m128d src_vec = _mm_loadu_pd(src + r * cols + stride);\n        _mm_storeu_pd(dst + r, src_vec);\n    }\n\n    for (size_t r = aligned_rows; r < rows; ++r) {\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256 v_src;\n    __m256i v_indices;\n    __m256 v_dst;\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 floats from src into v_src\n        v_src = _mm256_loadu_ps(&src[i]);\n\n        // Load 8 ints from indices into v_indices\n        v_indices = _mm256_loadu_si256((__m256i *)&indices[i]);\n\n        // Gather 8 floats from dst using v_indices as offsets\n        v_dst = _mm256_i32gather_ps(dst, v_indices, 4);\n\n        // Store the gathered values back to dst\n        _mm256_storeu_ps(&dst[i], v_dst);\n\n        // Scatter 8 floats from v_src to dst using v_indices as offsets\n        _mm256_i32scatter_ps(dst, v_indices, v_src, 4);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i *)&indices[i]);\n        __m256 src_vec1 = _mm256_i32gather_ps(src, idx_vec, 4);\n        __m256 src_vec2 = _mm256_i32gather_ps(src + 8, idx_vec, 4);\n\n        _mm256_storeu_ps(&dst[i], src_vec1);\n        _mm256_storeu_ps(&dst[i + 8], src_vec2);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i *)&indices[i]);\n        __m256 src_vec1 = _mm256_i32gather_ps(src, idx_vec, 4);\n        __m256 src_vec2 = _mm256_i32gather_ps(src + 8, idx_vec, 4);\n\n        _mm256_storeu_ps(&dst[i], src_vec1);\n        _mm256_storeu_ps(&dst[i + 8], src_vec2);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256 v_src;\n    __m256i v_indices;\n    __m256 v_dst;\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 elements from src into v_src\n        v_src = _mm256_loadu_ps(src + i);\n\n        // Load 8 elements from indices into v_indices\n        v_indices = _mm256_loadu_si256((__m256i *)(indices + i));\n\n        // Gather 8 elements from dst using v_indices as offsets\n        v_dst = _mm256_i32gather_ps(dst, v_indices, 4);\n\n        // Store the gathered values back to dst\n        _mm256_storeu_ps(dst + i, v_dst);\n\n        // Scatter 8 elements from src to dst using v_indices as offsets\n        _mm256_maskstore_ps(dst, v_indices, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256 v_src;\n    __m256i v_indices;\n    __m256 v_dst;\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 floats from src into v_src\n        v_src = _mm256_loadu_ps(&src[i]);\n\n        // Load 8 integers from indices into v_indices\n        v_indices = _mm256_loadu_si256((__m256i *)&indices[i]);\n\n        // Gather 8 floats from dst using v_indices as offsets\n        v_dst = _mm256_i32gather_ps(dst, v_indices, 4);\n\n        // Store the gathered values back to dst\n        _mm256_storeu_ps(&dst[i], v_dst);\n\n        // Scatter 8 floats from v_src to dst using v_indices as offsets\n        _mm256_maskstore_ps(&dst[0], v_indices, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 32) {\n        // Handle small ranges without AVX\n        for (uint64_t i = start, j = end; i < j; ++i, --j) {\n            int16_t temp = arr[i];\n            arr[i] = arr[j];\n            arr[j] = temp;\n        }\n        return;\n    }\n\n    __m256i *ptr_start = (__m256i *)(arr + start);\n    __m256i *ptr_end = (__m256i *)(arr + end);\n\n    while (ptr_start < ptr_end) {\n        __m256i temp = _mm256_loadu_si256(ptr_start);\n        _mm256_storeu_si256(ptr_start, _mm256_loadu_si256(ptr_end));\n        _mm256_storeu_si256(ptr_end, temp);\n        ++ptr_start;\n        --ptr_end;\n    }\n\n    // Handle remaining elements if the range length is not a multiple of 32\n    for (uint64_t i = start + ((end - start + 1) % 32), j = end; i < j; ++i, --j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 32) {\n        // Handle small ranges without AVX2\n        for (uint64_t i = start, j = end; i < j; ++i, --j) {\n            std::swap(arr[i], arr[j]);\n        }\n        return;\n    }\n\n    __m256i temp;\n    int16_t *ptr = &arr[start];\n    int16_t *end_ptr = &arr[end];\n\n    while (ptr + 16 <= end_ptr) {\n        temp = _mm256_loadu_si256((__m256i *)ptr);\n        _mm256_storeu_si256((__m256i *)end_ptr, temp);\n        ptr += 16;\n        end_ptr -= 16;\n    }\n\n    // Handle remaining elements\n    for (; ptr < end_ptr; ++ptr, --end_ptr) {\n        std::swap(*ptr, *end_ptr);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 32) {\n        // Handle small ranges without AVX\n        for (uint64_t i = start, j = end; i < j; ++i, --j) {\n            int16_t temp = arr[i];\n            arr[i] = arr[j];\n            arr[j] = temp;\n        }\n        return;\n    }\n\n    __m256i v0, v1, v2, v3;\n    uint64_t len = end - start + 1;\n    uint64_t half_len = len / 2;\n\n    for (uint64_t i = start, j = end; i < j; i += 16, j -= 16) {\n        v0 = _mm256_loadu_si256((__m256i *)(arr + i));\n        v1 = _mm256_loadu_si256((__m256i *)(arr + j));\n\n        v2 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        v3 = _mm256_permutevar8x32_epi32(v1, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        _mm256_storeu_si256((__m256i *)(arr + i), v3);\n        _mm256_storeu_si256((__m256i *)(arr + j), v2);\n    }\n\n    // Handle remaining elements\n    for (uint64_t i = start + (len & ~0xF); i < end; i++, j--) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 32) {\n        // Handle small ranges without AVX\n        for (uint64_t i = start, j = end; i < j; ++i, --j) {\n            std::swap(arr[i], arr[j]);\n        }\n        return;\n    }\n\n    __m256i temp;\n    int16_t *ptr_start = &arr[start];\n    int16_t *ptr_end = &arr[end];\n\n    while (ptr_start < ptr_end) {\n        __m256i vec_start = _mm256_loadu_si256((__m256i *)ptr_start);\n        __m256i vec_end = _mm256_loadu_si256((__m256i *)ptr_end);\n\n        temp = vec_start;\n        vec_start = _mm256_permutevar8x32_epi32(vec_end, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        vec_end = temp;\n\n        _mm256_storeu_si256((__m256i *)ptr_start, vec_start);\n        _mm256_storeu_si256((__m256i *)ptr_end, vec_end);\n\n        ptr_start += 16;\n        ptr_end -= 16;\n    }\n\n    // Handle remaining elements if the range length is not a multiple of 32\n    for (; ptr_start < ptr_end; ++ptr_start, --ptr_end) {\n        std::swap(*ptr_start, *ptr_end);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 32) {\n        // Handle small ranges without AVX\n        for (uint64_t i = start; i < (start + end) / 2; ++i) {\n            std::swap(arr[i], arr[end - i]);\n        }\n        return;\n    }\n\n    __m256i temp;\n    int16_t *ptr_start = &arr[start];\n    int16_t *ptr_end = &arr[end];\n\n    while (ptr_start < ptr_end) {\n        temp = _mm256_loadu_si256((__m256i *)ptr_start);\n        _mm256_storeu_si256((__m256i *)ptr_end, temp);\n\n        ptr_start += 16;\n        ptr_end -= 16;\n    }\n\n    // Handle remaining elements if the range is not a multiple of 32\n    for (; ptr_start < ptr_end; ++ptr_start, --ptr_end) {\n        std::swap(*ptr_start, *ptr_end);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_offset = slice_idx * dim1 * dim2;\n    const size_t row_size = dim1 * sizeof(uint8_t);\n\n    for (size_t i = 0; i < dim2; ++i) {\n        const size_t src_row_offset = (slice_offset + i * dim1) * sizeof(uint8_t);\n        const uint8_t *src_row = &tensor[src_row_offset];\n        uint8_t *dst_row = &slice[i * dim1];\n\n        __m256i v0, v1, v2, v3;\n\n        for (size_t j = 0; j <= dim1 - 32; j += 32) {\n            v0 = _mm256_loadu_si256((__m256i *)&src_row[j]);\n            v1 = _mm256_loadu_si256((__m256i *)&src_row[j + 32]);\n            v2 = _mm256_loadu_si256((__m256i *)&src_row[j + 64]);\n            v3 = _mm256_loadu_si256((__m256i *)&src_row[j + 96]);\n\n            _mm256_storeu_si256((__m256i *)&dst_row[j], v0);\n            _mm256_storeu_si256((__m256i *)&dst_row[j + 32], v1);\n            _mm256_storeu_si256((__m256i *)&dst_row[j + 64], v2);\n            _mm256_storeu_si256((__m256i *)&dst_row[j + 96], v3);\n        }\n\n        // Handle remaining elements\n        for (size_t j = dim1 - dim1 % 32; j < dim1; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_offset = slice_idx * dim1 * dim2;\n    const size_t stride_dim1 = dim2;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        const size_t row_offset = i * stride_dim1;\n        const size_t src_offset = slice_offset + row_offset;\n\n        __m256i v0, v1, v2, v3;\n        for (size_t j = 0; j <= dim2 - 32; j += 32) {\n            v0 = _mm256_loadu_si256((__m256i *)(tensor + src_offset + j));\n            v1 = _mm256_loadu_si256((__m256i *)(tensor + src_offset + j + 32));\n            v2 = _mm256_loadu_si256((__m256i *)(tensor + src_offset + j + 64));\n            v3 = _mm256_loadu_si256((__m256i *)(tensor + src_offset + j + 96));\n\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j), v0);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 32), v1);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 64), v2);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 96), v3);\n        }\n\n        // Handle remaining elements\n        for (size_t j = dim2 - dim2 % 32; j < dim2; ++j) {\n            slice[row_offset + j] = tensor[src_offset + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_offset = slice_idx * dim1 * dim2;\n    const size_t stride_dim1 = dim2;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        const size_t row_offset = i * stride_dim1;\n        const size_t src_row_offset = row_offset + slice_offset;\n\n        __m256i v0, v1, v2, v3;\n        for (size_t j = 0; j <= dim2 - 32; j += 32) {\n            v0 = _mm256_loadu_si256((__m256i *)(tensor + src_row_offset + j));\n            v1 = _mm256_loadu_si256((__m256i *)(tensor + src_row_offset + j + 32));\n            v2 = _mm256_loadu_si256((__m256i *)(tensor + src_row_offset + j + 64));\n            v3 = _mm256_loadu_si256((__m256i *)(tensor + src_row_offset + j + 96));\n\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j), v0);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 32), v1);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 64), v2);\n            _mm256_storeu_si256((__m256i *)(slice + row_offset + j + 96), v3);\n        }\n\n        // Handle remaining elements\n        for (size_t j = dim2 - dim2 % 32; j < dim2; ++j) {\n            slice[row_offset + j] = tensor[src_row_offset + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_offset = slice_idx * dim1 * dim2;\n    const size_t stride = dim1 * dim2;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i * stride + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i *)(tensor + slice_offset + i * stride + j + 32));\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), vec1);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 32), vec2);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_offset = slice_idx * dim1 * dim2;\n    const size_t row_size = dim1 * sizeof(uint8_t);\n\n    for (size_t i = 0; i < dim2; ++i) {\n        const size_t src_row_offset = (slice_offset + i * dim1) * sizeof(uint8_t);\n        const uint8_t *src_row = tensor + src_row_offset;\n\n        __m256i v0, v1, v2, v3, v4, v5, v6, v7;\n\n        for (size_t j = 0; j < dim1 / 32; ++j) {\n            v0 = _mm256_loadu_si256((__m256i *)(src_row + j * 32 * sizeof(uint8_t)));\n            v1 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 32) * sizeof(uint8_t)));\n            v2 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 64) * sizeof(uint8_t)));\n            v3 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 96) * sizeof(uint8_t)));\n            v4 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 128) * sizeof(uint8_t)));\n            v5 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 160) * sizeof(uint8_t)));\n            v6 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 192) * sizeof(uint8_t)));\n            v7 = _mm256_loadu_si256((__m256i *)(src_row + (j * 32 + 224) * sizeof(uint8_t)));\n\n            _mm256_storeu_si256((__m256i *)(slice + j * 32 * sizeof(uint8_t)), v0);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 32) * sizeof(uint8_t)), v1);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 64) * sizeof(uint8_t)), v2);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 96) * sizeof(uint8_t)), v3);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 128) * sizeof(uint8_t)), v4);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 160) * sizeof(uint8_t)), v5);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 192) * sizeof(uint8_t)), v6);\n            _mm256_storeu_si256((__m256i *)(slice + (j * 32 + 224) * sizeof(uint8_t)), v7);\n        }\n\n        // Handle remaining elements\n        for (size_t j = dim1 - dim1 % 32; j < dim1; ++j) {\n            slice[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m256i zero = _mm256_setzero_si256();\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    if (row_block + i < rows && col_block + j < cols) {\n                        __m256i src_val = _mm256_loadu_si256((__m256i*)&src[(row_block + i) * cols + col_block + j]);\n                        _mm256_storeu_si256((__m256i*)&dst[(col_block + j) * rows + row_block + i], src_val);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi16(1);\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    __m256i src_val = _mm256_loadu_si256((__m256i *)&src[(row_block + i) * cols + col_block]);\n                    __m256i dst_val = _mm256_loadu_si256((__m256i *)&dst[col_block * rows + (row_block + i)]);\n                    dst_val = _mm256_insert_epi16(dst_val, _mm256_extract_epi16(src_val, j), j);\n                    _mm256_storeu_si256((__m256i *)&dst[col_block * rows + (row_block + i)], dst_val);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi16(1);\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    __m256i src_val = _mm256_loadu_si256((__m256i*)&src[(row_block + i) * cols + col_block]);\n                    __m256i dst_val = _mm256_loadu_si256((__m256i*)&dst[col_block * rows + (row_block + i)]);\n                    dst_val = _mm256_add_epi16(dst_val, src_val);\n                    _mm256_storeu_si256((__m256i*)&dst[col_block * rows + (row_block + i)], dst_val);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi16(1);\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    const size_t src_index = (row_block + i) * cols + (col_block + j);\n                    const size_t dst_index = (col_block + j) * rows + (row_block + i);\n                    dst[dst_index] = src[src_index];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m256i zero = _mm256_setzero_si256();\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    if (row_block + i < rows && col_block + j < cols) {\n                        __m256i val = _mm256_loadu_si256((__m256i *)&src[(row_block + i) * cols + col_block + j]);\n                        _mm256_storeu_si256((__m256i *)&dst[(col_block + j) * rows + row_block + i], val);\n                    } else {\n                        _mm256_storeu_si256((__m256i *)&dst[(col_block + j) * rows + row_block + i], zero);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m256 v = _mm256_loadu_ps(&array[i * dim * dim + i * dim + i]);\n        sum = _mm256_add_ps(sum, v);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum);\n\n    float result = 0.0f;\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    for (; i < dim; ++i) {\n        result += array[i * dim * dim + i * dim + i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m256 vec = _mm256_loadu_ps(&array[i * dim * dim + i * dim + i]);\n        sum = _mm256_add_ps(sum, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum);\n\n    float result = 0.0f;\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j] + temp[j + 4];\n    }\n\n    for (; i < dim; ++i) {\n        result += array[i * dim * dim + i * dim + i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    while (i + 7 < dim * dim * dim) {\n        __m256 v0 = _mm256_loadu_ps(&array[i]);\n        __m256 v1 = _mm256_loadu_ps(&array[i + dim]);\n        __m256 v2 = _mm256_loadu_ps(&array[i + dim * dim]);\n\n        __m256 d0 = _mm256_permutevar8x32_ps(v0, _mm256_set_epi32(0, 4, 8, 12, 1, 5, 9, 13));\n        __m256 d1 = _mm256_permutevar8x32_ps(v1, _mm256_set_epi32(0, 4, 8, 12, 1, 5, 9, 13));\n        __m256 d2 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(0, 4, 8, 12, 1, 5, 9, 13));\n\n        sum = _mm256_add_ps(sum, d0);\n        sum = _mm256_add_ps(sum, d1);\n        sum = _mm256_add_ps(sum, d2);\n\n        i += 8 * dim * dim;\n    }\n\n    float s[8];\n    _mm256_storeu_ps(s, sum);\n\n    float total_sum = 0.0f;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += s[j];\n    }\n\n    for (; i < dim * dim * dim; ++i) {\n        if ((i / dim / dim) == (i / dim % dim) && (i / dim % dim) == (i % dim)) {\n            total_sum += array[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m256 vec = _mm256_loadu_ps(&array[i * dim * dim + i * dim + i]);\n        sum = _mm256_add_ps(sum, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum);\n\n    float result = 0.0f;\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j] + temp[j + 4];\n    }\n\n    for (; i < dim; ++i) {\n        result += array[i * dim * dim + i * dim + i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m256 vec = _mm256_loadu_ps(&array[i * dim * dim + i * dim + i]);\n        sum = _mm256_add_ps(sum, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, sum);\n\n    float result = 0.0f;\n    for (int j = 0; j < 4; ++j) {\n        result += temp[j] + temp[j + 4];\n    }\n\n    for (; i < dim; ++i) {\n        result += array[i * dim * dim + i * dim + i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double *src_row = src + i * cols;\n        double *dst_row = dst + i * cols;\n\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(src_row + j);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(dst_row + j, v_result);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src_row[j] > threshold) {\n                dst_row[j] = src_row[j] * scale;\n            } else {\n                dst_row[j] = src_row[j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double *src_row = src + i * cols;\n        double *dst_row = dst + i * cols;\n\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(src_row + j);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(dst_row + j, v_result);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src_row[j] > threshold) {\n                dst_row[j] = src_row[j] * scale;\n            } else {\n                dst_row[j] = src_row[j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double *src_row = src + i * cols;\n        double *dst_row = dst + i * cols;\n\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(src_row + j);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OS);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(dst_row + j, v_result);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src_row[j] > threshold) {\n                dst_row[j] = src_row[j] * scale;\n            } else {\n                dst_row[j] = src_row[j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(&dst[i * cols + j], v_result);\n        }\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m256d v_src = _mm256_loadu_pd(&src[i * cols + j]);\n            __m256d v_mask = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OS);\n            __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n            __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_mask);\n            _mm256_storeu_pd(&dst[i * cols + j], v_result);\n        }\n        // Handle remaining elements that do not fit into a full vector\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i += 2) {\n        __m256d row1 = _mm256_loadu_pd(src + indices[i] * cols);\n        __m256d row2 = _mm256_loadu_pd(src + indices[i + 1] * cols);\n\n        _mm256_storeu_pd(dst + i * cols, row1);\n        _mm256_storeu_pd(dst + (i + 1) * cols, row2);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i += 2) {\n        __m256d row1 = _mm256_loadu_pd(src + indices[i] * cols);\n        __m256d row2 = _mm256_loadu_pd(src + indices[i + 1] * cols);\n\n        _mm256_storeu_pd(dst + i * cols, row1);\n        _mm256_storeu_pd(dst + (i + 1) * cols, row2);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 4) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n        size_t idx2 = indices[i + 2];\n        size_t idx3 = indices[i + 3];\n\n        v0 = _mm256_loadu_pd(src + idx0 * cols);\n        v1 = _mm256_loadu_pd(src + idx1 * cols);\n        v2 = _mm256_loadu_pd(src + idx2 * cols);\n        v3 = _mm256_loadu_pd(src + idx3 * cols);\n\n        _mm256_storeu_pd(dst + i * cols, v0);\n        _mm256_storeu_pd(dst + (i + 1) * cols, v1);\n        _mm256_storeu_pd(dst + (i + 2) * cols, v2);\n        _mm256_storeu_pd(dst + (i + 3) * cols, v3);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i += 2) {\n        __m256d row1 = _mm256_loadu_pd(src + indices[i] * cols);\n        __m256d row2 = _mm256_loadu_pd(src + indices[i + 1] * cols);\n\n        _mm256_storeu_pd(dst + i * cols, row1);\n        _mm256_storeu_pd(dst + (i + 1) * cols, row2);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    for (size_t i = 0; i < rows; i += 2) {\n        __m256d row1 = _mm256_loadu_pd(src + indices[i] * cols);\n        __m256d row2 = _mm256_loadu_pd(src + indices[i + 1] * cols);\n\n        _mm256_storeu_pd(dst + i * cols, row1);\n        _mm256_storeu_pd(dst + (i + 1) * cols, row2);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i v_result = _mm256_add_epi64(v_src1, v_src2);\n        v_result = _mm256_add_epi64(v_result, v_scalar);\n        v_result = _mm256_srai_epi64(v_result, 1);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i v_scalar = _mm256_set1_epi64x(scalar);\n        __m256i v_result = _mm256_add_epi64(v_src1, v_src2);\n        v_result = _mm256_add_epi64(v_result, v_scalar);\n        v_result = _mm256_srai_epi64(v_result, 1);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n        __m256i v_scalar = _mm256_set1_epi64x(scalar);\n        __m256i v_result = _mm256_add_epi64(v_src1, v_src2);\n        v_result = _mm256_add_epi64(v_result, v_scalar);\n        v_result = _mm256_srai_epi64(v_result, 1);\n        _mm256_storeu_si256((__m256i *)&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n        __m256i v_result = _mm256_add_epi64(v_src1, v_src2);\n        v_result = _mm256_add_epi64(v_result, v_scalar);\n        v_result = _mm256_srai_epi64(v_result, 1);\n        _mm256_storeu_si256((__m256i *)&dst[i], v_result);\n    }\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i *)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i *)&src2[i]);\n        __m256i v_result = _mm256_add_epi64(v_src1, v_src2);\n        v_result = _mm256_add_epi64(v_result, v_scalar);\n        v_result = _mm256_srai_epi64(v_result, 1);\n        _mm256_storeu_si256((__m256i *)&dst[i], v_result);\n    }\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 7 < length) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_and_si256(src_vec, _mm256_set1_epi32(0xAAAAAAAA));\n        min_val = _mm256_min_epi32(min_val, mask);\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < length) {\n        if ((src[i] & 1) == 0 && src[i] < _mm256_extract_epi32(min_val, 0)) {\n            min_val = _mm256_set1_epi32(src[i]);\n        }\n        i++;\n    }\n\n    return _mm256_extract_epi32(min_val, 0);\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_and_si256(src_vec, _mm256_set1_epi32(0x55555555));\n        min_val = _mm256_min_epi32(min_val, mask);\n    }\n\n    int result[8];\n    _mm256_storeu_si256((__m256i *)result, min_val);\n\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if ((result[j] & 0x55555555) != 0 && result[j] < min_value) {\n            min_value = result[j];\n        }\n    }\n\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vals = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_and_si256(src_vals, _mm256_set1_epi32(0xFFFFFFFE));\n        min_val = _mm256_min_epi32(min_val, mask);\n    }\n\n    int result[8];\n    _mm256_storeu_si256((__m256i *)result, min_val);\n\n    int min_value = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if ((result[j] & 1) == 0 && result[j] < min_value) {\n            min_value = result[j];\n        }\n    }\n\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if ((src[i] & 1) == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 7 < length) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_and_si256(_mm256_srai_epi32(vec, 31), _mm256_set1_epi32(1));\n        vec = _mm256_andnot_si256(mask, vec);\n        min_val = _mm256_min_epi32(min_val, vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < length) {\n        if ((src[i] & 1) == 0 && src[i] < _mm256_extract_epi32(min_val, 0)) {\n            min_val = _mm256_set1_epi32(src[i]);\n        }\n        i++;\n    }\n\n    return _mm256_extract_epi32(min_val, 0);\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_val = _mm256_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i mask = _mm256_and_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFE)); // Mask out odd indices\n        min_val = _mm256_min_epi32(min_val, mask);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if ((i & 1) == 0) { // Check if index is even\n            min_val = _mm256_min_epi32(min_val, _mm256_set1_epi32(src[i]));\n        }\n    }\n\n    // Extract the minimum value from the result\n    int result[8];\n    _mm256_storeu_si256((__m256i *)result, min_val);\n\n    // Find the minimum value in the result array\n    int min_result = INT_MAX;\n    for (int j = 0; j < 8; ++j) {\n        if (result[j] != INT_MAX && result[j] < min_result) {\n            min_result = result[j];\n        }\n    }\n\n    return min_result;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i arith_shifted = _mm256_sra_epi32(src_vec, _mm256_set1_epi32(shift));\n        __m256i logic_shifted = _mm256_srli_epi32(_mm256_and_si256(src_vec, mask), shift);\n        __m256i result = _mm256_add_epi32(arith_shifted, logic_shifted);\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    const __m256i shift_mask = _mm256_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_sra_epi32(src_vec, shift_mask);\n\n        // Logical right shift\n        __m256i logical_shifted = _mm256_srli_epi32(_mm256_and_si256(src_vec, mask), shift);\n\n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n    const __m256i shift_vec = _mm256_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_sra_epi32(src_vec, shift_vec);\n\n        // Logical right shift\n        __m256i logic_shifted = _mm256_srli_epi32(_mm256_and_si256(src_vec, mask), shift);\n\n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shifted, logic_shifted);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_srai_epi32(src_vec, shift); // Arithmetic right shift\n\n        __m256i src_uint_vec = _mm256_and_si256(src_vec, mask); // Convert to unsigned\n        __m256i dst_uint_vec = _mm256_srli_epi32(src_uint_vec, shift); // Logical right shift\n\n        dst_vec = _mm256_add_epi32(dst_vec, dst_uint_vec); // Add results\n\n        _mm256_storeu_si256((__m256i *)&dst[i], dst_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_sra_epi32(src_vec, _mm256_set1_epi32(shift));\n\n        // Logical right shift\n        __m256i logical_shifted = _mm256_srli_epi32(_mm256_and_si256(src_vec, mask), shift);\n\n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n        _mm256_storeu_si256((__m256i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i row_vec = _mm256_loadu_si256((__m256i *)&row_start[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            total_sum += temp[k];\n        }\n        sum_vec = _mm256_setzero_si256();\n    }\n\n    // Handle remaining elements that don't fit into a full vector\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = cols - cols % 8; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i row_vec = _mm256_loadu_si256((__m256i *)&row_start[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            total_sum += temp[k];\n        }\n        sum_vec = _mm256_setzero_si256();\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i row_vec = _mm256_loadu_si256((__m256i *)&row_start[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            total_sum += temp[k];\n        }\n        sum_vec = _mm256_setzero_si256();\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i row_vec = _mm256_loadu_si256((__m256i *)&row_start[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            total_sum += temp[k];\n        }\n        sum_vec = _mm256_setzero_si256();\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i row_vec = _mm256_loadu_si256((__m256i *)&row_start[j]);\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, sum_vec);\n        for (int k = 0; k < 8; ++k) {\n            total_sum += temp[k];\n        }\n        sum_vec = _mm256_setzero_si256();\n    }\n\n    // Handle remaining elements that don't fit into a full vector\n    for (size_t i = rows * cols - (rows * cols % 8); i < rows * cols; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const size_t vec_len = 32; // AVX2 vector length for int8_t\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + vec_len <= length; i += vec_len) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(current_vals, max_val);\n        max_val = _mm256_blendv_epi8(max_val, current_vals, mask);\n\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i idx_mask = _mm256_movemask_epi8(mask);\n        while (idx_mask) {\n            int bit_pos = __builtin_ctz(idx_mask);\n            idx_mask &= ~(1 << bit_pos);\n            if (_mm256_extract_epi8(current_vals, bit_pos) > _mm256_extract_epi8(max_val, bit_pos)) {\n                max_idx = i + bit_pos;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % vec_len; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const size_t vec_len = 32; // AVX2 vector length in bytes\n    const size_t num_vecs = (length + vec_len - 1) / vec_len;\n\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i < num_vecs; ++i) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(src + i * vec_len));\n        __m256i cmp = _mm256_cmpgt_epi8(vec, max_val);\n\n        max_val = _mm256_blendv_epi8(max_val, vec, cmp);\n        max_idx += _mm256_movemask_epi8(cmp) << (i * vec_len);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vecs * vec_len; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8(max_value));\n\n        int8_t local_max_value = max_value;\n        uint32_t local_max_index = max_index;\n\n        for (int j = 0; j < simd_width; ++j) {\n            if (_mm256_extract_epi8(mask, j)) {\n                local_max_value = src[i + j];\n                local_max_index = i + j;\n            }\n        }\n\n        if (local_max_value > max_value) {\n            max_value = local_max_value;\n            max_index = local_max_index;\n        }\n    }\n\n    // Handle remaining elements that do not fit into a full SIMD width\n    for (size_t i = length - length % simd_width; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const size_t vec_len = 32; // AVX2 vector length for int8_t\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + vec_len <= length; i += vec_len) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(current_vals, max_val);\n        max_val = _mm256_blendv_epi8(max_val, current_vals, mask);\n\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i idx_mask = _mm256_movemask_epi8(mask);\n        while (idx_mask) {\n            if (idx_mask & 1) {\n                max_idx = idx_vec.m256i_i32[0];\n            }\n            idx_vec = _mm256_srli_si256(idx_vec, 4);\n            idx_mask >>= 4;\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % vec_len; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    const size_t vec_len = 32; // AVX2 vector length for int8_t\n    __m256i max_val = _mm256_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + vec_len <= length; i += vec_len) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i greater_mask = _mm256_cmpgt_epi8(current_vals, max_val);\n        max_val = _mm256_blendv_epi8(max_val, current_vals, greater_mask);\n\n        __m256i idx_vec = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i selected_idx = _mm256_blendv_epi32(_mm256_setzero_si256(), idx_vec, greater_mask);\n        max_idx = _mm256_movemask_epi8(selected_idx) & 0xFF;\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % vec_len; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i v_mask = _mm256_set1_epi32(mask);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i v_index = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                           i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i v_and_result = _mm256_and_si256(v_index, v_mask);\n\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_cond = _mm256_cmpgt_epi32(v_and_result, v_zero);\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, v_cond);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i v_mask = _mm256_set1_epi32(mask);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi16(1);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i v_index = _mm256_setr_epi32(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i v_and_result = _mm256_and_epi32(v_index, v_mask);\n        __m256i v_non_zero = _mm256_cmpgt_epi32(v_and_result, v_zero);\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i v_dst = _mm256_blendv_epi16(v_src2, v_src1, v_non_zero);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i v_mask = _mm256_set1_epi32(mask);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i v_index = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                           i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i v_and_result = _mm256_and_epi32(v_index, v_mask);\n\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_cond = _mm256_cmpgt_epi32(v_and_result, v_zero);\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, v_cond);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i v_mask = _mm256_set1_epi32(mask);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i v_index = _mm256_setr_epi32(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                           i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i v_and_result = _mm256_and_si256(v_index, v_mask);\n\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_cond = _mm256_cmpgt_epi32(v_and_result, v_zero);\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, v_cond);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i v_mask = _mm256_set1_epi32(mask);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i v_index = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7,\n                                           i + 8, i + 9, i + 10, i + 11, i + 12, i + 13, i + 14, i + 15);\n        __m256i v_and_result = _mm256_and_epi32(v_index, v_mask);\n\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_condition = _mm256_cmpgt_epi32(v_and_result, v_zero);\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, v_condition);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_popcount = _mm256_popcnt_epi32(v_src);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_popcount = _mm256_popcnt_epi32(v_src);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i;\n\n    // Process elements in chunks of 8 using AVX2\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_popcount = _mm256_popcnt_epi32(v_src);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_popcount);\n    }\n\n    // Handle remaining elements that do not fit into a full chunk\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_popcount = _mm256_popcnt_epi32(v_src);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_popcount = _mm256_popcnt_epi32(v_src);\n        _mm256_storeu_si256((__m256i*)&dst[i], v_popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements per iteration\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_sum = _mm256_adds_epu16(v_src1, v_src2);\n        v_sum = _mm256_min_epu16(v_sum, max_val);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements per iteration\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_sum = _mm256_adds_epu16(v_src1, v_src2);\n        __m256i v_clamped = _mm256_min_epu16(v_sum, max_val);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_clamped);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements per iteration\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_sum = _mm256_adds_epu16(v_src1, v_src2);\n        __m256i v_clamped = _mm256_min_epu16(v_sum, max_val);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_clamped);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements per iteration\n    __m256i mask = _mm256_set1_epi16(0xFFFF); // Mask for saturation\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_result = _mm256_adds_epu16(v_src1, v_src2);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements per iteration\n    __m256i mask = _mm256_set1_epi16(0xFFFF); // Mask for saturation\n\n    for (size_t i = 0; i <= length - simd_width; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i v_sum = _mm256_adds_epu16(v_src1, v_src2);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_sum);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % simd_width; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d v_zero = _mm256_setzero_pd();\n    __m256d v_neg_100 = _mm256_set1_pd(-100.0);\n    __m256d v_100 = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d v_c = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v_a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d v_b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d v_mask_a = _mm256_cmp_pd(v_a, v_neg_100, _CMP_GE_OQ);\n                __m256d v_mask_b = _mm256_cmp_pd(v_b, v_100, _CMP_LE_OQ);\n                __m256d v_mask = _mm256_and_pd(v_mask_a, v_mask_b);\n\n                v_a = _mm256_and_pd(v_a, v_mask);\n                v_b = _mm256_and_pd(v_b, v_mask);\n\n                v_c = _mm256_add_pd(v_c, _mm256_mul_pd(v_a, v_b));\n            }\n\n            _mm256_storeu_pd(&C[i * p + j], v_c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d v_zero = _mm256_setzero_pd();\n    __m256d v_neg_100 = _mm256_set1_pd(-100.0);\n    __m256d v_pos_100 = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d v_c = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v_a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d v_b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d v_mask_a = _mm256_cmp_pd(v_a, v_neg_100, _CMP_GE_OQ);\n                __m256d v_mask_b = _mm256_cmp_pd(v_b, v_pos_100, _CMP_LE_OQ);\n                __m256d v_mask = _mm256_and_pd(v_mask_a, v_mask_b);\n\n                v_a = _mm256_and_pd(v_a, v_mask);\n                v_b = _mm256_and_pd(v_b, v_mask);\n\n                v_c = _mm256_add_pd(v_c, _mm256_mul_pd(v_a, v_b));\n            }\n\n            _mm256_storeu_pd(&C[i * p + j], v_c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d v_zero = _mm256_setzero_pd();\n    __m256d v_neg_100 = _mm256_set1_pd(-100.0);\n    __m256d v_pos_100 = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d v_c = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d v_a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d v_b = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check if elements are within the range [-100, 100]\n                __m256d v_mask_a = _mm256_and_pd(_mm256_cmp_pd(v_a, v_neg_100, _CMP_GE_OQ), _mm256_cmp_pd(v_a, v_pos_100, _CMP_LE_OQ));\n                __m256d v_mask_b = _mm256_and_pd(_mm256_cmp_pd(v_b, v_neg_100, _CMP_GE_OQ), _mm256_cmp_pd(v_b, v_pos_100, _CMP_LE_OQ));\n\n                // Multiply and accumulate only if both elements are within the range\n                __m256d v_product = _mm256_mul_pd(v_a, v_b);\n                v_c = _mm256_add_pd(v_c, _mm256_and_pd(v_product, _mm256_and_pd(v_mask_a, v_mask_b)));\n            }\n\n            _mm256_storeu_pd(&C[i * p + j], v_c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d v_zero = _mm256_setzero_pd();\n    __m256d v_neg_100 = _mm256_set1_pd(-100.0);\n    __m256d v_100 = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d v_c = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v_a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d v_b = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d v_mask_a = _mm256_cmp_pd(v_a, v_neg_100, _CMP_GE_OQ);\n                __m256d v_mask_b = _mm256_cmp_pd(v_b, v_100, _CMP_LE_OQ);\n                __m256d v_mask = _mm256_and_pd(v_mask_a, v_mask_b);\n\n                v_a = _mm256_and_pd(v_a, v_mask);\n                v_b = _mm256_and_pd(v_b, v_mask);\n\n                v_c = _mm256_add_pd(v_c, _mm256_mul_pd(v_a, v_b));\n            }\n\n            _mm256_storeu_pd(&C[i * p + j], v_c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d v_zero = _mm256_setzero_pd();\n    __m256d v_min = _mm256_set1_pd(-100.0);\n    __m256d v_max = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d v_c = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d v_a = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d v_b = _mm256_loadu_pd(&B[k * p + j]);\n\n                // Check if elements are within the range [-100, 100]\n                __m256d v_mask = _mm256_and_pd(_mm256_cmp_pd(v_a, v_min, _CMP_GE_OQ), _mm256_cmp_pd(v_a, v_max, _CMP_LE_OQ));\n                v_mask = _mm256_and_pd(v_mask, _mm256_cmp_pd(v_b, v_min, _CMP_GE_OQ));\n                v_mask = _mm256_and_pd(v_mask, _mm256_cmp_pd(v_b, v_max, _CMP_LE_OQ));\n\n                // Multiply and accumulate only if elements are within the range\n                v_c = _mm256_add_pd(v_c, _mm256_mul_pd(v_a, v_b));\n                v_c = _mm256_blendv_pd(v_c, v_zero, v_mask);\n            }\n\n            _mm256_storeu_pd(&C[i * p + j], v_c);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m256i vecC = _mm256_add_epi64(vecA, vecB);\n            _mm256_storeu_si256((__m256i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m256i vecC = _mm256_add_epi64(vecA, vecB);\n            _mm256_storeu_si256((__m256i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m256i vecC = _mm256_add_epi64(vecA, vecB);\n            _mm256_storeu_si256((__m256i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m256i vecC = _mm256_add_epi64(vecA, vecB);\n            _mm256_storeu_si256((__m256i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m256i vecC = _mm256_add_epi64(vecA, vecB);\n            _mm256_storeu_si256((__m256i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n        __m256 v_result = _mm256_add_ps(v_src1, v_src2);\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n        __m256 v_result = _mm256_add_ps(v_src1, v_src2);\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(round(src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n        __m256 v_result = _mm256_add_ps(v_src1, v_src2);\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(std::round(src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n        __m256 v_result = _mm256_add_ps(v_src1, v_src2);\n        __m256i v_int_result = _mm256_cvtps_epi32(v_result);\n        __m256 v_rounded_result = _mm256_cvtepi32_ps(v_int_result);\n        _mm256_storeu_ps(&dst[i], v_rounded_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n        __m256 v_result = _mm256_add_ps(v_src1, v_src2);\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(std::nearbyint(src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 v_threshold = _mm256_set1_ps(50.0f);\n    const __m256i v_neg_one = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_and_ps(_mm256_or_ps(v_diff, _mm256_setzero_ps()), _mm256_set1_ps(0x7FFFFFFF));\n        __m256i v_mask = _mm256_cmp_ps_epi32(_mm256_cvtps_epi32(v_abs_diff), _mm256_cvtps_epi32(v_threshold), _CMP_LE_OS);\n\n        __m256 v_index = _mm256_setr_ps(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i v_index_mask = _mm256_slli_epi32(_mm256_srli_epi32(_mm256_and_si256(v_index, _mm256_set1_epi32(2)), 31), 31);\n\n        __m256 v_result = _mm256_mul_ps(v_src1, v_src2);\n        v_result = _mm256_add_ps(v_result, _mm256_set1_ps(0.5f));\n        v_result = _mm256_max_ps(v_result, _mm256_setzero_ps());\n\n        v_result = _mm256_blendv_ps(v_result, _mm256_castsi256_ps(v_neg_one), v_mask);\n        v_result = _mm256_blendv_ps(v_result, _mm256_castsi256_ps(v_index_mask), v_index_mask);\n\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const int vec_len = 8; // AVX2 processes 8 floats at a time\n    __m256 v_threshold = _mm256_set1_ps(50.0f);\n    __m256i v_mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - vec_len; i += vec_len) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_and_ps(_mm256_add_ps(v_diff, _mm256_set1_ps(0.0f)), _mm256_set1_ps(0x7FFFFFFF));\n        __m256i v_index = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i v_mod_2 = _mm256_and_si256(v_index, _mm256_set1_epi32(1));\n\n        __m256 v_condition = _mm256_cmp_ps(v_abs_diff, v_threshold, _CMP_LE_OS);\n        __m256i v_condition_mask = _mm256_and_si256(_mm256_castps_si256(v_condition), v_mod_2);\n\n        __m256 v_result = _mm256_mul_ps(v_src1, v_src2);\n        __m256 v_neg_one = _mm256_set1_ps(-1.0f);\n        v_result = _mm256_blendv_ps(v_result, v_neg_one, _mm256_castsi256_ps(v_condition_mask));\n\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_UPWARD | _MM_FROUND_NO_EXC);\n\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % vec_len; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 v_threshold = _mm256_set1_ps(50.0f);\n    __m256i v_mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_and_ps(_mm256_add_ps(v_diff, _mm256_setzero_ps()), _mm256_set1_ps(0x7FFFFFFF));\n        __m256 v_condition = _mm256_cmp_ps(v_abs_diff, v_threshold, _CMP_LE_OQ);\n        __m256 v_index = _mm256_set_ps(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i + 0);\n        __m256i v_index_int = _mm256_cvtps_epi32(v_index);\n        __m256i v_is_multiple_of_2 = _mm256_and_si256(v_index_int, _mm256_set1_epi32(1));\n\n        __m256 v_result = _mm256_mul_ps(v_src1, v_src2);\n        __m256 v_neg_one = _mm256_set1_ps(-1.0f);\n        v_result = _mm256_blendv_ps(v_result, v_neg_one, _mm256_or_ps(v_condition, v_is_multiple_of_2));\n\n        v_result = _mm256_max_ps(v_result, _mm256_setzero_ps());\n        v_result = _mm256_add_ps(v_result, _mm256_set1_ps(0.5f));\n        v_result = _mm256_floor_ps(v_result);\n\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 v_threshold = _mm256_set1_ps(50.0f);\n    __m256i v_mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_and_ps(_mm256_add_ps(v_diff, _mm256_permutevar8x32_ps(v_diff, v_mask)), _mm256_set1_ps(0x7FFFFFFF));\n        __m256i v_index = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i v_is_valid = _mm256_cmpgt_epi32(_mm256_cvtps_epi32(v_abs_diff), _mm256_setzero_si256());\n        __m256i v_is_even = _mm256_and_si256(_mm256_srli_epi32(v_index, 1), _mm256_set1_epi32(1));\n\n        __m256 v_result = _mm256_mul_ps(v_src1, v_src2);\n        v_result = _mm256_blendv_ps(v_result, _mm256_set1_ps(-1.0f), _mm256_or_si256(v_is_valid, v_is_even));\n        v_result = _mm256_max_ps(v_result, _mm256_set1_ps(-1.0f));\n\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 v_threshold = _mm256_set1_ps(50.0f);\n    const __m256i v_neg_one = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_and_ps(_mm256_or_ps(v_diff, _mm256_setzero_ps()), _mm256_set1_ps(0.0f));\n        __m256 v_condition = _mm256_cmp_ps(v_abs_diff, v_threshold, _CMP_LE_OQ);\n\n        __m256 v_product = _mm256_mul_ps(v_src1, v_src2);\n        __m256 v_rounded = _mm256_add_ps(v_product, _mm256_set1_ps(0.5f));\n        v_rounded = _mm256_max_ps(v_rounded, _mm256_setzero_ps());\n\n        __m256i v_mask = _mm256_cvtps_epi32(v_condition);\n        v_mask = _mm256_and_si256(v_mask, _mm256_set1_epi32(0x7FFFFFFF));\n\n        __m256i v_result = _mm256_blendv_epi32(_mm256_castps_si256(v_rounded), v_neg_one, v_mask);\n        v_result = _mm256_or_si256(v_result, _mm256_and_si256(v_mask, _mm256_set1_epi32(0x80000000)));\n\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t num_vecs_per_row = n / vec_size;\n\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < num_vecs_per_row; ++j) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j * vec_size]);\n            __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j * vec_size], result_vec);\n        }\n\n        // Handle remaining elements that do not fit into a vector register\n        for (size_t j = num_vecs_per_row * vec_size; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t num_vecs = n / vec_size;\n\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < num_vecs; j += vec_size) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result_vec);\n        }\n\n        // Handle remaining elements that don't fit into a vector\n        for (size_t j = num_vecs * vec_size; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t vec_size = 2; // Number of elements per vector register (AVX2)\n    const size_t aligned_m = m & ~1; // Align m to the nearest even number\n\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Broadcast scalar to all elements in the vector\n\n    // Process aligned part of the matrix\n    for (size_t i = 0; i < aligned_m; i += vec_size) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result back to dst\n    }\n\n    // Process remaining unaligned elements\n    for (size_t i = aligned_m; i < m; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t vec_size = 2; // Number of elements per vector register (AVX2)\n    const size_t aligned_m = m & ~1; // Align m to the nearest even number\n\n    __m128d scalar_vec = _mm_set1_pd(scalar); // Broadcast scalar to all elements of the vector\n\n    // Process aligned part of the matrix\n    for (size_t i = 0; i < aligned_m; i += vec_size) {\n        __m128d mat_vec = _mm_loadu_pd(&mat[i]); // Load 2 elements from mat\n        __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n        _mm_storeu_pd(&dst[i], result_vec); // Store the result back to dst\n    }\n\n    // Process remaining unaligned elements\n    for (size_t i = aligned_m; i < m; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t vec_size = 2; // Number of elements per vector register\n    const size_t num_vecs = n / vec_size;\n\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < num_vecs; j += vec_size) {\n            __m128d mat_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result_vec);\n        }\n\n        // Handle remaining elements that do not fit into a full vector\n        for (size_t j = num_vecs * vec_size; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        float sum_squares = 0.0f;\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_row_vec = _mm256_mul_ps(row_vec, row_vec);\n            sum_squares += _mm256_reduce_add_ps(squared_row_vec);\n        }\n\n        float norm = sqrt(sum_squares);\n        if (norm != 0.0f) {\n            for (size_t j = 0; j < n; j += 8) {\n                __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n                __m256 normalized_row_vec = _mm256_div_ps(row_vec, _mm256_set1_ps(norm));\n                _mm256_storeu_ps(&mat[i * n + j], normalized_row_vec);\n            }\n        } else {\n            // Handle zero norm case if needed\n            for (size_t j = 0; j < n; j += 8) {\n                __m256 row_vec = _mm256_loadu_ps(&mat[i * n + j]);\n                _mm256_storeu_ps(&mat[i * n + j], row_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_elements = _mm256_mul_ps(row_elements, row_elements);\n            sum_squares = _mm256_add_ps(sum_squares, squared_elements);\n        }\n\n        float sums[8];\n        _mm256_storeu_ps(sums, sum_squares);\n        float l2_norm = 0.0f;\n        for (int k = 0; k < 8; ++k) {\n            l2_norm += sums[k];\n        }\n        l2_norm = sqrt(l2_norm);\n\n        if (l2_norm != 0.0f) {\n            for (size_t j = 0; j < n; j += 8) {\n                __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n                __m256 normalized_elements = _mm256_div_ps(row_elements, _mm256_set1_ps(l2_norm));\n                _mm256_storeu_ps(&mat[i * n + j], normalized_elements);\n            }\n        } else {\n            // Handle case where the row has zero L2 norm\n            for (size_t j = 0; j < n; j += 8) {\n                __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n                _mm256_storeu_ps(&mat[i * n + j], row_elements);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        float sum_squares = 0.0f;\n\n        // Process 8 elements at a time using AVX2\n        for (size_t j = 0; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 row_squared = _mm256_mul_ps(row, row);\n            sum_squares += _mm256_reduce_add_ps(row_squared);\n        }\n\n        // Process remaining elements (if any)\n        for (size_t j = n - n % 8; j < n; ++j) {\n            sum_squares += mat[i * n + j] * mat[i * n + j];\n        }\n\n        // Calculate the L^2 norm\n        float norm = sqrt(sum_squares);\n\n        // Normalize the row\n        for (size_t j = 0; j <= n - 8; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_row = _mm256_div_ps(row, _mm256_set1_ps(norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_row);\n        }\n\n        // Normalize remaining elements (if any)\n        for (size_t j = n - n % 8; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_elements = _mm256_mul_ps(row_elements, row_elements);\n            sum_squares = _mm256_add_ps(sum_squares, squared_elements);\n        }\n\n        // Horizontal add to get the sum of squares for the row\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_squares);\n        float sum = temp[0] + temp[1] + temp[2] + temp[3] +\n                    temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Calculate the L^2 norm\n        float norm = sqrt(sum);\n\n        // Normalize the row\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_elements = _mm256_div_ps(row_elements, _mm256_set1_ps(norm));\n            _mm256_storeu_ps(&mat[i * n + j], normalized_elements);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m256 one = _mm256_set1_ps(1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_squares = zero;\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 squared_elements = _mm256_mul_ps(row_elements, row_elements);\n            sum_squares = _mm256_add_ps(sum_squares, squared_elements);\n        }\n\n        // Handle remaining elements\n        for (size_t j = n - n % 8; j < n; ++j) {\n            float element = mat[i * n + j];\n            sum_squares = _mm256_add_ss(sum_squares, _mm256_set1_ps(element * element));\n        }\n\n        __m256 l2_norm = _mm256_sqrt_ps(sum_squares);\n        l2_norm = _mm256_max_ps(l2_norm, _mm256_set1_ps(1e-7f)); // Avoid division by zero\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            __m256 row_elements = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 normalized_row = _mm256_div_ps(row_elements, l2_norm);\n            _mm256_storeu_ps(&mat[i * n + j], normalized_row);\n        }\n\n        // Handle remaining elements\n        for (size_t j = n - n % 8; j < n; ++j) {\n            mat[i * n + j] /= l2_norm[0];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d sum = zero;\n            for (size_t k = 0; k < n; ++k) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n            __m256d rounded_sum = _mm256_roundscale_pd(sum, 3); // Round to nearest integer\n            _mm256_storeu_pd(&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m256d a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d b = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(a, b));\n            }\n\n            __m256d rounded_sum = _mm256_roundscale_pd(sum, 3); // Round to nearest integer\n            _mm256_storeu_pd(&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d v2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(v1, v2));\n            }\n            __m256d rounded_sum = _mm256_roundscale_pd(sum, 3); // Round to nearest integer\n            _mm256_storeu_pd(&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d v2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(v1, v2));\n            }\n            __m256d rounded_sum = _mm256_roundscale_pd(sum, 3); // Round to nearest integer\n            _mm256_storeu_pd(&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m256d v1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d v2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(v1, v2));\n            }\n\n            __m256d rounded_sum = _mm256_add_pd(sum, half);\n            rounded_sum = _mm256_andnot_si256(_mm256_cmp_pd(rounded_sum, zero, _CMP_LE_OS), rounded_sum);\n            rounded_sum = _mm256_or_si256(_mm256_cmp_pd(rounded_sum, zero, _CMP_GT_OS), rounded_sum);\n\n            _mm256_storeu_pd(&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const size_t simd_width = 8; // AVX2 can process 8 floats at once\n    for (size_t i = 0; i < rows; i += simd_width) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_vec = _mm256_loadu_ps(src + i * cols + j);\n            __m256 rounded_row_vec = _mm256_add_ps(_mm256_mul_ps(row_vec, _mm256_set1_ps(4.0f)), _mm256_set1_ps(0.5f));\n            rounded_row_vec = _mm256_floor_ps(rounded_row_vec);\n            rounded_row_vec = _mm256_div_ps(rounded_row_vec, _mm256_set1_ps(4.0f));\n\n            _mm256_storeu_ps(dst + j * rows + i, rounded_row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 floats at a time\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_vec = _mm256_loadu_ps(src + i * cols + j);\n            __m256 rounded_vec = _mm256_add_ps(_mm256_mul_ps(row_vec, _mm256_set1_ps(4.0f)), _mm256_set1_ps(0.5f));\n            rounded_vec = _mm256_floor_ps(rounded_vec);\n            rounded_vec = _mm256_div_ps(rounded_vec, _mm256_set1_ps(4.0f));\n\n            _mm256_storeu_ps(dst + j * rows + i, rounded_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const size_t simd_width = 8; // AVX2 can process 8 floats at once\n\n    for (size_t i = 0; i < rows; i += simd_width) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_vec = _mm256_loadu_ps(src + i * cols + j);\n\n            // Round each element to the nearest quarter (0.25)\n            __m256 rounded = _mm256_add_ps(_mm256_mul_ps(row_vec, _mm256_set1_ps(4.0f)), _mm256_set1_ps(0.5f));\n            rounded = _mm256_floor_ps(rounded);\n            rounded = _mm256_div_ps(rounded, _mm256_set1_ps(4.0f));\n\n            // Store the transposed and rounded values in the destination matrix\n            _mm256_storeu_ps(dst + j * rows + i, rounded);\n        }\n    }\n\n    // Handle remaining elements that do not fit into the SIMD width\n    for (size_t i = rows - rows % simd_width; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            float value = src[i * cols + j];\n            value = std::round(value / 0.25f) * 0.25f;\n            dst[j * rows + i] = value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 8; // AVX2 can process 8 floats at a time\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_vec = _mm256_loadu_ps(src + i * cols + j);\n            __m256 rounded_row_vec = _mm256_add_ps(_mm256_mul_ps(row_vec, _mm256_set1_ps(4.0f)), _mm256_set1_ps(0.5f));\n            rounded_row_vec = _mm256_floor_ps(rounded_row_vec);\n            rounded_row_vec = _mm256_div_ps(rounded_row_vec, _mm256_set1_ps(4.0f));\n\n            _mm256_storeu_ps(dst + j * rows + i, rounded_row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const size_t simd_width = 8; // AVX2 can process 8 floats at once\n    for (size_t i = 0; i < rows; i += simd_width) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256 row_vec = _mm256_loadu_ps(src + i * cols + j);\n            __m256 rounded_row = _mm256_mul_ps(row_vec, _mm256_set1_ps(4.0f));\n            rounded_row = _mm256_add_ps(rounded_row, _mm256_set1_ps(0.5f));\n            rounded_row = _mm256_floor_ps(rounded_row);\n            rounded_row = _mm256_div_ps(rounded_row, _mm256_set1_ps(4.0f));\n\n            for (int k = 0; k < simd_width && i + k < rows; ++k) {\n                dst[(i + k) * cols + j] = rounded_row.m256_f32[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 8; i += 8) {\n        __m256 v_tensor1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v_tensor2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 v_result = _mm256_sub_ps(v_tensor1, v_tensor2);\n        __m256i v_rounded = _mm256_cvtps_epi32(v_result);\n        __m256 v_final = _mm256_cvtepi32_ps(v_rounded);\n        _mm256_storeu_ps(&dst[i], v_final);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 8; i += 8) {\n        __m256 v_tensor1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v_tensor2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 v_result = _mm256_sub_ps(v_tensor1, v_tensor2);\n        v_result = _mm256_round_ps(v_result, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], v_result);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(v1, v2);\n        __m256i rounded = _mm256_cvtps_epi32(sub);\n        __m256 result = _mm256_cvtepi32_ps(rounded);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(v1, v2);\n        __m256i rounded = _mm256_cvtps_epi32(sub);\n        __m256 rsub = _mm256_cvtepi32_ps(rounded);\n        _mm256_storeu_ps(&dst[i], rsub);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 8; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub = _mm256_sub_ps(v1, v2);\n        __m256i rounded = _mm256_cvtps_epi32(sub);\n        __m256 rsub = _mm256_cvtepi32_ps(rounded);\n        _mm256_storeu_ps(&dst[i], rsub);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256d v_mat1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v_mat2 = _mm256_loadu_pd(&mat2[i * n]);\n        __m256d v_dst = _mm256_mul_pd(v_mat1, v_mat2);\n        _mm256_storeu_pd(&dst[i * n], v_dst);\n    }\n    for (; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256d v1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v2 = _mm256_loadu_pd(&mat2[i * n]);\n        __m256d v3 = _mm256_mul_pd(v1, v2);\n        _mm256_storeu_pd(&dst[i * n], v3);\n    }\n    for (; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256d v_mat1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v_mat2 = _mm256_loadu_pd(&mat2[i * n]);\n        __m256d v_dst = _mm256_mul_pd(v_mat1, v_mat2);\n        _mm256_storeu_pd(&dst[i * n], v_dst);\n    }\n    for (; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256d v1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v2 = _mm256_loadu_pd(&mat2[i * n]);\n        __m256d v3 = _mm256_mul_pd(v1, v2);\n        _mm256_storeu_pd(&dst[i * n], v3);\n    }\n    for (; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256d v1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v2 = _mm256_loadu_pd(&mat2[i * n]);\n        __m256d v3 = _mm256_mul_pd(v1, v2);\n        _mm256_storeu_pd(&dst[i * n], v3);\n    }\n    for (; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 row = _mm256_loadu_ps(&mat[i * n]);\n        __m256 col = _mm256_loadu_ps(vec);\n        __m256 result = _mm256_mul_ps(row, col);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 col = _mm256_loadu_ps(&vec[j]);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(row, col));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        for (int k = 0; k < 8; ++k) {\n            if (j + k < n) {\n                dst[i] += temp[k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n / 8; ++j) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j * 8]);\n            __m256 col = _mm256_loadu_ps(&vec[j * 8]);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(row, col));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        for (size_t j = 0; j < 8; ++j) {\n            if (j < n % 8) {\n                dst[i] += temp[j];\n            } else {\n                break;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n / 8; ++j) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j * 8]);\n            __m256 col = _mm256_loadu_ps(&vec[j * 8]);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(row, col));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        for (size_t j = 0; j < 8; ++j) {\n            if (j < n % 8) {\n                dst[i] += temp[j];\n            } else {\n                break;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        for (size_t j = 0; j < n / 8; ++j) {\n            __m256 row = _mm256_loadu_ps(&mat[i * n + j * 8]);\n            __m256 col = _mm256_loadu_ps(&vec[j * 8]);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(row, col));\n        }\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        for (size_t j = 0; j < 8; ++j) {\n            if (j < n % 8) {\n                dst[i] += temp[j];\n            } else {\n                break;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    // Sum up the remaining elements\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double partial_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Handle remaining elements if there are less than 4\n    for (; i < total_elements; ++i) {\n        partial_sum += mat[i];\n    }\n\n    // Calculate the average\n    return partial_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    // Sum up the remaining elements\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double partial_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements if there are less than 4\n    for (; i < total_elements; ++i) {\n        partial_sum += mat[i];\n    }\n\n    // Calculate the average\n    return partial_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    // Sum up the remaining elements\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double partial_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements that don't fit into a block of 4\n    for (; i < total_elements; ++i) {\n        partial_sum += mat[i];\n    }\n\n    // Calculate the average\n    return partial_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    // Sum up the remaining elements\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double partial_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements that don't fit into a block of 4\n    for (; i < total_elements; ++i) {\n        partial_sum += mat[i];\n    }\n\n    return partial_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum = _mm256_add_pd(sum, vec);\n    }\n\n    // Sum up the remaining elements\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double partial_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements that don't fit into a block of 4\n    for (; i < total_elements; ++i) {\n        partial_sum += mat[i];\n    }\n\n    return partial_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(matrix + i * cols));\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(matrix + i * cols + 8));\n\n        __m256i mask_asc = _mm256_cmpgt_epi32(prev, curr);\n        __m256i mask_desc = _mm256_cmplt_epi32(prev, curr);\n\n        if (directions[i]) {\n            if (_mm256_movemask_epi8(mask_asc)) return false;\n        } else {\n            if (_mm256_movemask_epi8(mask_desc)) return false;\n        }\n\n        for (size_t j = 16; j < cols; j += 8) {\n            prev = curr;\n            curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(matrix + i * cols + j));\n\n            mask_asc = _mm256_cmpgt_epi32(prev, curr);\n            mask_desc = _mm256_cmplt_epi32(prev, curr);\n\n            if (directions[i]) {\n                if (_mm256_movemask_epi8(mask_asc)) return false;\n            } else {\n                if (_mm256_movemask_epi8(mask_desc)) return false;\n            }\n        }\n\n        for (size_t j = cols - cols % 8; j < cols; ++j) {\n            if (directions[i] && matrix[i * cols + j] > matrix[i * cols + j + 1]) return false;\n            if (!directions[i] && matrix[i * cols + j] < matrix[i * cols + j + 1]) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i prev = _mm256_loadu_si256((__m256i*)(matrix + i * cols));\n        __m256i curr = _mm256_loadu_si256((__m256i*)(matrix + i * cols + 8));\n        __m256i mask = _mm256_set1_epi32(directions[i] ? -1 : 1);\n\n        for (size_t j = 16; j < cols; j += 32) {\n            __m256i next = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j));\n            __m256i cmp = _mm256_cmpgt_epi32(prev, next);\n            __m256i cmp_masked = _mm256_and_si256(cmp, mask);\n            if (_mm256_movemask_epi8(cmp_masked) != 0) return false;\n\n            prev = next;\n        }\n\n        // Handle remaining elements that don't fit into a full 32-byte chunk\n        for (size_t j = cols - cols % 32; j < cols; ++j) {\n            if ((directions[i] && matrix[i * cols + j] < matrix[i * cols + j - 1]) ||\n                (!directions[i] && matrix[i * cols + j] > matrix[i * cols + j - 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i prev = _mm256_loadu_si256((__m256i*)(matrix + i * cols));\n        __m256i curr = _mm256_loadu_si256((__m256i*)(matrix + i * cols + 8));\n\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n\n        if (directions[i]) {\n            // Ascending order\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                continue;\n            }\n        } else {\n            // Descending order\n            if (_mm256_movemask_epi8(cmp) == 0) {\n                continue;\n            }\n        }\n\n        return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const bool direction = directions[i];\n        const int* row = &matrix[i * cols];\n\n        for (size_t j = 0; j <= cols - 8; j += 8) {\n            __m256i prev = _mm256_loadu_si256((__m256i*)&row[j]);\n            __m256i curr = _mm256_loadu_si256((__m256i*)&row[j + 8]);\n\n            if (direction) {\n                __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n                if (_mm256_movemask_epi8(cmp) != 0) return false;\n            } else {\n                __m256i cmp = _mm256_cmpgt_epi32(curr, prev);\n                if (_mm256_movemask_epi8(cmp) != 0) return false;\n            }\n        }\n\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - cols % 8; j < cols; ++j) {\n            if ((direction && row[j] > row[j + 1]) || (!direction && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i prev = _mm256_setzero_si256();\n        __m256i curr = _mm256_setzero_si256();\n        __m256i mask = _mm256_setzero_si256();\n\n        for (size_t j = 0; j + 7 < cols; j += 8) {\n            prev = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j));\n            curr = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j + 8));\n\n            if (directions[i]) {\n                mask = _mm256_cmpgt_epi32(prev, curr);\n            } else {\n                mask = _mm256_cmplt_epi32(prev, curr);\n            }\n\n            if (_mm256_movemask_epi8(mask)) {\n                return false;\n            }\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 8; j < cols; ++j) {\n            int prev_val = matrix[i * cols + j];\n            int curr_val = matrix[i * cols + j + 1];\n\n            if (directions[i] && prev_val > curr_val) {\n                return false;\n            } else if (!directions[i] && prev_val < curr_val) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i base_vec = _mm256_set1_epi16(base);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i quotients = _mm256_div_epi16(src_vec, base_vec);\n        __m256i multiples = _mm256_mul_epi16(quotients, base_vec);\n        __m256i remainders = _mm256_sub_epi16(src_vec, multiples);\n\n        __m256i mask = _mm256_cmpgt_epi16(remainders, zero_vec);\n        __m256i next_multiples = _mm256_add_epi16(multiples, base_vec);\n        __m256i result = _mm256_blendv_epi16(next_multiples, multiples, mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i base_vec = _mm256_set1_epi16(base);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i quotients = _mm256_div_epi16(src_vec, base_vec);\n        __m256i multiples = _mm256_mul_epi16(quotients, base_vec);\n\n        __m256i remainders = _mm256_sub_epi16(src_vec, multiples);\n        __m256i next_multiples = _mm256_add_epi16(multiples, base_vec);\n\n        __m256i mask = _mm256_cmpgt_epi16(remainders, zero_vec);\n        __m256i result = _mm256_blendv_epi16(next_multiples, multiples, mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i base_vec = _mm256_set1_epi16(base);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i div_result = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul_result = _mm256_mul_epi16(div_result, base_vec);\n        __m256i sub_result = _mm256_sub_epi16(src_vec, mul_result);\n        __m256i cmp_mask = _mm256_cmpgt_epi16(sub_result, zero_vec);\n        __m256i add_one = _mm256_add_epi16(mul_result, base_vec);\n        __m256i final_result = _mm256_blendv_epi16(add_one, mul_result, cmp_mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i base_vec = _mm256_set1_epi16(base);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i div_result = _mm256_div_epi16(src_vec, base_vec);\n        __m256i mul_result = _mm256_mul_epi16(div_result, base_vec);\n        __m256i sub_result = _mm256_sub_epi16(src_vec, mul_result);\n        __m256i cmp_mask = _mm256_cmpgt_epi16(sub_result, zero_vec);\n        __m256i add_one = _mm256_add_epi16(mul_result, base_vec);\n        __m256i final_result = _mm256_blendv_epi16(add_one, mul_result, cmp_mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i base_vec = _mm256_set1_epi16(base);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i quotients = _mm256_div_epi16(src_vec, base_vec);\n        __m256i multiples = _mm256_mul_epi16(quotients, base_vec);\n        __m256i remainders = _mm256_sub_epi16(src_vec, multiples);\n\n        __m256i mask = _mm256_cmpgt_epi16(remainders, zero_vec);\n        __m256i next_multiples = _mm256_add_epi16(multiples, base_vec);\n        __m256i result = _mm256_blendv_epi16(next_multiples, multiples, mask);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v0 = _mm256_loadu_pd(vector);\n    __m256d v1 = _mm256_loadu_pd(vector + 4);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d m0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d m1 = _mm256_loadu_pd(matrix + i * cols + 4);\n\n        __m256d cmp0 = _mm256_cmp_pd(m0, v0, _CMP_EQ_OQ);\n        __m256d cmp1 = _mm256_cmp_pd(m1, v1, _CMP_EQ_OQ);\n\n        __m256d mask0 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp0));\n        __m256d mask1 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp1));\n\n        if ((mask0 | mask1) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v0 = _mm256_loadu_pd(vector);\n    __m256d v1 = _mm256_loadu_pd(vector + 4);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d m0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d m1 = _mm256_loadu_pd(matrix + i * cols + 4);\n\n        __m256d cmp0 = _mm256_cmp_pd(m0, v0, _CMP_EQ_OQ);\n        __m256d cmp1 = _mm256_cmp_pd(m1, v1, _CMP_EQ_OQ);\n\n        __m256d mask0 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp0));\n        __m256d mask1 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp1));\n\n        if ((mask0 | mask1) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v0 = _mm256_loadu_pd(vector);\n    __m256d v1 = _mm256_loadu_pd(vector + 4);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d m0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d m1 = _mm256_loadu_pd(matrix + i * cols + 4);\n\n        __m256d cmp0 = _mm256_cmp_pd(m0, v0, _CMP_EQ_OQ);\n        __m256d cmp1 = _mm256_cmp_pd(m1, v1, _CMP_EQ_OQ);\n\n        __m256d mask0 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp0));\n        __m256d mask1 = _mm256_movemask_epi8(_mm256_castpd_si256(cmp1));\n\n        if ((mask0 | mask1) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v = _mm256_loadu_pd(vector);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d m = _mm256_loadu_pd(matrix + i * cols);\n\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m256d eq = _mm256_cmp_pd(m, v, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_epi8(eq);\n            if (mask != 0) {\n                return true;\n            }\n            m = _mm256_loadu_pd(matrix + i * cols + 4 * j + 4);\n        }\n\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (matrix[i * cols + j] == vector[j]) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v = _mm256_loadu_pd(vector);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d m = _mm256_loadu_pd(matrix + i * cols);\n        for (size_t j = 1; j < cols / 4; ++j) {\n            m = _mm256_or_pd(m, _mm256_loadu_pd(matrix + i * cols + j * 4));\n        }\n        __m256d mask = _mm256_cmp_pd(m, v, _CMP_EQ_OQ);\n        int mask_int = _mm256_movemask_epi8(_mm256_castpd_si256(mask));\n        if (mask_int != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(va, vb);\n        __m256i mask = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i sum = _mm256_and_si256(eq, mask);\n        int result[8];\n        _mm256_storeu_si256((__m256i*)result, sum);\n        count += result[0] + result[1] + result[2] + result[3] +\n                 result[4] + result[5] + result[6] + result[7];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq_mask = _mm256_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq_mask = _mm256_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(va, vb);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i sum = _mm256_mullo_epi32(eq, ones);\n        int result[8];\n        _mm256_storeu_si256((__m256i*)result, sum);\n        count += result[0] + result[1] + result[2] + result[3] +\n                 result[4] + result[5] + result[6] + result[7];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i eq = _mm256_cmpeq_epi8(va, vb);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i sum = _mm256_mullo_epi32(eq, ones);\n        int result[8];\n        _mm256_storeu_si256((__m256i*)result, sum);\n        count += result[0] + result[1] + result[2] + result[3] +\n                 result[4] + result[5] + result[6] + result[7];\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_mul = _mm256_mul_epi64(vec_x, vec_a);\n\n        __m256i vec_abs = _mm256_andnot_si256(_mm256_cmpgt_epi64(vec_mul, _mm256_setzero_si256()), vec_mul);\n        vec_abs = _mm256_or_si256(vec_abs, _mm256_srli_epi64(vec_abs, 63));\n\n        _mm256_storeu_si256((__m256i *)&y[i], vec_abs);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_mul = _mm256_mul_epi64(vec_x, vec_a);\n\n        __m256i vec_abs = _mm256_andnot_si256(_mm256_cmpgt_epi64(vec_mul, _mm256_setzero_si256()), vec_mul);\n        _mm256_storeu_si256((__m256i *)&y[i], vec_abs);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_mul = _mm256_mul_epi64(vec_x, vec_a);\n\n        __m256i vec_abs = _mm256_andnot_si256(_mm256_cmpgt_epi64(vec_mul, _mm256_setzero_si256()), vec_mul);\n        _mm256_storeu_si256((__m256i *)&y[i], vec_abs);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_mul = _mm256_mul_epi64(vec_x, vec_a);\n        __m256i vec_abs = _mm256_andnot_si256(_mm256_cmpgt_epi64(vec_mul, _mm256_setzero_si256()), vec_mul);\n        _mm256_storeu_si256((__m256i *)&y[i], vec_abs);\n    }\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)&x[i]);\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_mul = _mm256_mul_epi64(vec_x, vec_a);\n        __m256i vec_abs = _mm256_andnot_si256(_mm256_cmpgt_epi64(vec_mul, _mm256_setzero_si256()), vec_mul);\n        _mm256_storeu_si256((__m256i *)&y[i], vec_abs);\n    }\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_val = _mm256_min_epi16(min_val, current_vals);\n        max_val = _mm256_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[16];\n    int16_t local_max[16];\n\n    _mm256_storeu_si256((__m256i*)local_min, min_val);\n    _mm256_storeu_si256((__m256i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX;\n    int16_t global_max = INT16_MIN;\n\n    for (int j = 0; j < 16; ++j) {\n        if (local_min[j] < global_min) {\n            global_min = local_min[j];\n        }\n        if (local_max[j] > global_max) {\n            global_max = local_max[j];\n        }\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_val = _mm256_min_epi16(min_val, current_vals);\n        max_val = _mm256_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[16];\n    int16_t local_max[16];\n    _mm256_storeu_si256((__m256i*)local_min, min_val);\n    _mm256_storeu_si256((__m256i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX;\n    int16_t global_max = INT16_MIN;\n\n    for (int j = 0; j < 16; ++j) {\n        if (local_min[j] < global_min) global_min = local_min[j];\n        if (local_max[j] > global_max) global_max = local_max[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < global_min) global_min = vec[i];\n        if (vec[i] > global_max) global_max = vec[i];\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_val = _mm256_min_epi16(min_val, current_vals);\n        max_val = _mm256_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[16];\n    int16_t local_max[16];\n\n    _mm256_storeu_si256((__m256i*)local_min, min_val);\n    _mm256_storeu_si256((__m256i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX;\n    int16_t global_max = INT16_MIN;\n\n    for (int j = 0; j < 16; ++j) {\n        if (local_min[j] < global_min) global_min = local_min[j];\n        if (local_max[j] > global_max) global_max = local_max[j];\n    }\n\n    return {global_min, global_max};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_val = _mm256_min_epi16(min_val, current_vals);\n        max_val = _mm256_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[16], local_max[16];\n    _mm256_storeu_si256((__m256i*)local_min, min_val);\n    _mm256_storeu_si256((__m256i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX, global_max = INT16_MIN;\n    for (int j = 0; j < 16; ++j) {\n        if (local_min[j] < global_min) global_min = local_min[j];\n        if (local_max[j] > global_max) global_max = local_max[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < global_min) global_min = vec[i];\n        if (vec[i] > global_max) global_max = vec[i];\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i*)&vec[i]);\n        min_val = _mm256_min_epi16(min_val, current_vals);\n        max_val = _mm256_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[16];\n    int16_t local_max[16];\n    _mm256_storeu_si256((__m256i*)local_min, min_val);\n    _mm256_storeu_si256((__m256i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX;\n    int16_t global_max = INT16_MIN;\n\n    for (int j = 0; j < 16; ++j) {\n        if (local_min[j] < global_min) global_min = local_min[j];\n        if (local_max[j] > global_max) global_max = local_max[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < global_min) global_min = vec[i];\n        if (vec[i] > global_max) global_max = vec[i];\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m256i* row_ptr = reinterpret_cast<const __m256i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 8; ++j) {\n            __m256i current_row = _mm256_loadu_si256(row_ptr++);\n            __m256i next_row = _mm256_loadu_si256(row_ptr++);\n\n            __m256i diff = _mm256_sub_epi32(next_row, current_row);\n            __m256i mask = _mm256_cmpgt_epi32(diff, zero);\n\n            if (_mm256_movemask_epi8(mask) != 0xFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 8 != 0) {\n            const int* remaining_elements = reinterpret_cast<const int*>(row_ptr);\n            for (size_t j = 0; j < cols % 8; ++j) {\n                if (remaining_elements[j] <= remaining_elements[j + 1]) {\n                    return false;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m256i* row_ptr = reinterpret_cast<const __m256i*>(matrix + i * cols);\n        bool is_increasing = true;\n\n        for (size_t j = 0; j < cols / 8; ++j) {\n            __m256i current_row = _mm256_loadu_si256(row_ptr + j);\n            __m256i next_row = _mm256_loadu_si256(row_ptr + j + 1);\n\n            __m256i diff = _mm256_sub_epi32(next_row, current_row);\n            __m256i mask = _mm256_cmpgt_epi32(diff, zero);\n\n            if (_mm256_movemask_epi8(mask) != 0xFF) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m256i* row_ptr = reinterpret_cast<const __m256i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 8; ++j) {\n            __m256i current_row = _mm256_loadu_si256(row_ptr++);\n            __m256i next_row = _mm256_loadu_si256(row_ptr++);\n\n            __m256i diff = _mm256_sub_epi32(next_row, current_row);\n            __m256i mask = _mm256_cmpgt_epi32(diff, zero);\n\n            if (_mm256_movemask_epi8(mask) != 0xFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 8 != 0) {\n            const int* remaining_elements = reinterpret_cast<const int*>(row_ptr);\n            for (size_t j = 0; j < cols % 8; ++j) {\n                if (remaining_elements[j] <= remaining_elements[j + 1]) {\n                    return false;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m256i* row_ptr = reinterpret_cast<const __m256i*>(matrix + i * cols);\n        bool is_increasing = true;\n\n        for (size_t j = 0; j < cols / 8; ++j) {\n            __m256i current_row = _mm256_loadu_si256(row_ptr + j);\n            __m256i next_row = _mm256_loadu_si256(row_ptr + j + 1);\n\n            __m256i diff = _mm256_sub_epi32(next_row, current_row);\n            __m256i mask = _mm256_cmpgt_epi32(diff, zero);\n\n            if (_mm256_movemask_epi8(mask) != 0xFF) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m256i* row_ptr = reinterpret_cast<const __m256i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 8; ++j) {\n            __m256i current_row = _mm256_loadu_si256(row_ptr++);\n            __m256i next_row = _mm256_loadu_si256(row_ptr++);\n\n            __m256i diff = _mm256_sub_epi32(next_row, current_row);\n            __m256i mask = _mm256_cmpgt_epi32(diff, zero);\n\n            if (_mm256_movemask_epi8(mask) != 0xFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 8 != 0) {\n            const int* remaining_elements = reinterpret_cast<const int*>(row_ptr);\n            for (size_t j = 0; j < cols % 8; ++j) {\n                if (remaining_elements[j] <= remaining_elements[j + 1]) {\n                    return false;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m256i first_block = _mm256_loadu_si256((__m256i*)(matrix));\n\n    for (size_t i = block_size; i < rows; i += block_size) {\n        for (size_t j = block_size; j < cols; j += block_size) {\n            __m256i current_block = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j));\n            __m256i cmp_result = _mm256_cmpeq_epi32(first_block, current_block);\n            uint32_t mask = _mm256_movemask_epi8(cmp_result);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m256i first_block = _mm256_loadu_si256((__m256i*)(matrix));\n\n    for (size_t i = 1; i < rows / block_size; ++i) {\n        for (size_t j = 0; j < cols / block_size; ++j) {\n            __m256i current_block = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j * block_size));\n            if (!_mm256_cmpeq_epi32(current_block, first_block).mm256_movemask_epi8()) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m256i first_block = _mm256_loadu_si256((__m256i*)(matrix));\n\n    for (size_t i = block_size; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256i current_block = _mm256_loadu_si256((__m256i*)(matrix + i * cols + j));\n            __m256i cmp_result = _mm256_cmpeq_epi32(current_block, first_block);\n            uint32_t mask = _mm256_movemask_epi8(cmp_result);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m256i first_block = _mm256_loadu_si256((__m256i*)(matrix));\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += 4 * block_size) {\n            __m256i current_block = _mm256_loadu_si256((__m256i*)(matrix + row * cols + col));\n            __m256i cmp_result = _mm256_cmpeq_epi32(first_block, current_block);\n\n            uint32_t mask = _mm256_movemask_epi8(cmp_result);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m256i first_block = _mm256_loadu_si256((__m256i*)(matrix));\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = block_size; col < cols; col += block_size) {\n            __m256i current_block = _mm256_loadu_si256((__m256i*)(matrix + row * cols + col));\n            __m256i cmp_result = _mm256_cmpeq_epi32(first_block, current_block);\n            uint32_t mask = _mm256_movemask_epi8(cmp_result);\n            if (mask != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 8 != 0) {\n        return false;\n    }\n\n    __m256d first_block = _mm256_loadu_pd(vec);\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = block_size; i < length; i += block_size) {\n        __m256d current_block = _mm256_loadu_pd(&vec[i]);\n        __m256d diff = _mm256_sub_pd(current_block, first_block);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        __m256d max_abs_diff = _mm256_max_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(1, 0, 7, 6, 5, 4, 3, 2)));\n\n        if (_mm256_movemask_pd(_mm256_cmp_pd(max_abs_diff, tolerance_vec, _CMP_GT_OS)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 8 != 0) {\n        return false;\n    }\n\n    __m256d first_block = _mm256_loadu_pd(&vec[0]);\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = block_size; i < length - block_size + 1; i += block_size) {\n        __m256d current_block = _mm256_loadu_pd(&vec[i]);\n        __m256d diff = _mm256_sub_pd(current_block, first_block);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        __m256d max_abs_diff = _mm256_max_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)));\n\n        if (_mm256_movemask_pd(_mm256_cmp_pd(max_abs_diff, tolerance_vec, _CMP_GT_OS)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 8 != 0) {\n        return false;\n    }\n\n    __m256d first_block = _mm256_loadu_pd(vec);\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = block_size; i < length; i += block_size) {\n        __m256d current_block = _mm256_loadu_pd(&vec[i]);\n        __m256d diff = _mm256_sub_pd(current_block, first_block);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        __m256d max_abs_diff = _mm256_max_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(1, 0, 7, 6, 5, 4, 3, 2)));\n\n        if (_mm256_movemask_pd(_mm256_cmp_pd(max_abs_diff, tolerance_vec, _CMP_GT_OS)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 8 != 0) {\n        return false;\n    }\n\n    __m256d first_block = _mm256_loadu_pd(vec);\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = block_size; i < length; i += block_size) {\n        __m256d current_block = _mm256_loadu_pd(&vec[i]);\n        __m256d diff = _mm256_sub_pd(current_block, first_block);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        __m256d max_abs_diff = _mm256_max_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)));\n        max_abs_diff = _mm256_max_pd(max_abs_diff, _mm256_permutevar8x32_ps(max_abs_diff, _mm256_set_epi32(1, 0, 7, 6, 5, 4, 3, 2)));\n\n        if (_mm256_movemask_pd(_mm256_cmp_pd(max_abs_diff, tolerance_vec, _CMP_GT_OS)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 8 != 0) {\n        return false;\n    }\n\n    __m256d first_block = _mm256_loadu_pd(&vec[0]);\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = block_size; i < length; i += block_size) {\n        __m256d current_block = _mm256_loadu_pd(&vec[i]);\n\n        __m256d diff = _mm256_sub_pd(current_block, first_block);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        __m256d max_abs_diff = _mm256_max_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n\n        if (_mm256_movemask_epi8(_mm256_cmp_pd(max_abs_diff, tolerance_vec, _CMP_GT_OS)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = zero;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n        for (; j < cols; ++j) {\n            sum = _mm256_add_sd(sum, _mm256_mul_pd(_mm256_set1_pd(matrix[i * cols + j]), _mm256_set1_pd(matrix[i * cols + j])));\n        }\n\n        __m256d sqrt_sum = _mm256_sqrt_pd(sum);\n        __m256d threshold = _mm256_loadu_pd(&thresholds[i]);\n\n        __m256d cmp = _mm256_cmp_pd(sqrt_sum, threshold, _CMP_GT_OS);\n        __m256d mask = _mm256_castsi256_pd(_mm256_cmpeq_epi32(_mm256_castpd_si256(cmp), _mm256_setzero_si256()));\n\n        for (int k = 0; k < 4; ++k) {\n            result[i] |= (_mm256_movemask_epi8(mask) & (1 << k)) != 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = zero;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n\n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(_mm256_set1_pd(matrix[i * cols + j]), _mm256_set1_pd(matrix[i * cols + j])));\n        }\n\n        __m256d norm = _mm256_sqrt_pd(sum);\n        __m256d threshold = _mm256_loadu_pd(&thresholds[i]);\n\n        __m256d cmp = _mm256_cmp_pd(norm, threshold, _CMP_GT_OS);\n        __m256d mask = _mm256_castsi256_pd(_mm256_cmpeq_epi32(_mm256_castpd_si256(cmp), _mm256_setzero_si256()));\n\n        for (int k = 0; k < 4; ++k) {\n            result[i * cols + k] = _mm256_cvtsd_f64(_mm256_and_pd(mask, _mm256_insertf128_pd(zero, _mm256_extractf128_pd(cmp, 0), 0)));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = zero;\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n        __m256d norm = _mm256_sqrt_pd(sum);\n        __m256d threshold = _mm256_broadcast_sd(&thresholds[i]);\n        __m256d cmp = _mm256_cmp_pd(norm, threshold, _CMP_GT_OS);\n        uint32_t mask = _mm256_movemask_epi8(cmp);\n        result[i] = (mask == 0);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = zero;\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n\n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(_mm256_set1_pd(matrix[i * cols + j]), _mm256_set1_pd(matrix[i * cols + j])));\n        }\n\n        __m256d norm = _mm256_sqrt_pd(sum);\n        __m256d threshold = _mm256_loadu_pd(&thresholds[i]);\n\n        __m256d cmp = _mm256_cmp_pd(norm, threshold, _CMP_GT_OS);\n        result[i] = _mm256_movemask_epi8(cmp) != 0;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = zero;\n        for (size_t j = 0; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n\n        // Handle remaining elements\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(_mm256_set1_pd(val), _mm256_set1_pd(val)));\n        }\n\n        __m256d norm = _mm256_sqrt_pd(sum);\n        __m256d threshold = _mm256_loadu_pd(&thresholds[i]);\n\n        __m256d cmp = _mm256_cmp_pd(norm, threshold, _CMP_GT_OS);\n        result[i] = _mm256_movemask_epi8(cmp) != 0;\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n        __m256i vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i sq_A = _mm256_mullo_epi16(vec_A, vec_A);\n        __m256i sq_B = _mm256_mullo_epi16(vec_B, vec_B);\n\n        __m256i prod_AB = _mm256_mullo_epi16(vec_A, vec_B);\n\n        sum_squares = _mm256_add_epi32(sum_squares, sq_A);\n        sum_squares = _mm256_add_epi32(sum_squares, sq_B);\n\n        sum_products = _mm256_add_epi32(sum_products, prod_AB);\n    }\n\n    int32_t result_squares[8];\n    int32_t result_products[8];\n\n    _mm256_storeu_si256((__m256i *)result_squares, sum_squares);\n    _mm256_storeu_si256((__m256i *)result_products, sum_products);\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    for (int j = 0; j < 8; ++j) {\n        total_sum_squares += result_squares[j];\n        total_sum_products += result_products[j];\n    }\n\n    for (size_t i = length - length % 32; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        total_sum_squares += a * a + b * b;\n        total_sum_products += a * b;\n    }\n\n    return total_sum_squares - 2 * total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i sq_A = _mm256_mullo_epi16(vec_A, vec_A);\n        __m256i sq_B = _mm256_mullo_epi16(vec_B, vec_B);\n\n        __m256i prod_AB = _mm256_mullo_epi16(vec_A, vec_B);\n\n        sum_squares = _mm256_add_epi32(sum_squares, sq_A);\n        sum_squares = _mm256_add_epi32(sum_squares, sq_B);\n\n        sum_products = _mm256_add_epi32(sum_products, prod_AB);\n    }\n\n    int32_t result_squares[8];\n    int32_t result_products[8];\n\n    _mm256_storeu_si256((__m256i *)result_squares, sum_squares);\n    _mm256_storeu_si256((__m256i *)result_products, sum_products);\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    for (int j = 0; j < 8; ++j) {\n        total_sum_squares += result_squares[j];\n        total_sum_products += result_products[j];\n    }\n\n    int64_t diff = total_sum_squares - 2 * total_sum_products;\n\n    for (size_t i = length - 32; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        diff += a * a + b * b - 2 * a * b;\n    }\n\n    return diff;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n        __m256i ab_product = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(a_squared));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(b_squared));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_cvtepi16_epi32(ab_product));\n    }\n\n    int32_t s_s[8], s_p[8];\n    _mm256_storeu_si256((__m256i *)s_s, sum_squares);\n    _mm256_storeu_si256((__m256i *)s_p, sum_products);\n\n    int64_t result = 0;\n    for (int j = 0; j < 8; ++j) {\n        result += static_cast<int64_t>(s_s[j]) + static_cast<int64_t>(s_p[j]);\n    }\n\n    for (size_t i = length - 32; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        result += static_cast<int64_t>(a * a) + static_cast<int64_t>(b * b) - static_cast<int64_t>(a * b);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i sq_A = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(vec_A), _mm256_cvtepi8_epi16(vec_A));\n        __m256i sq_B = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(vec_B), _mm256_cvtepi8_epi16(vec_B));\n        __m256i prod_AB = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(vec_A), _mm256_cvtepi8_epi16(vec_B));\n\n        sum_squares = _mm256_add_epi32(sum_squares, sq_A);\n        sum_squares = _mm256_add_epi32(sum_squares, sq_B);\n        sum_products = _mm256_add_epi32(sum_products, prod_AB);\n    }\n\n    int32_t result[8];\n    _mm256_storeu_si256((__m256i *)result, sum_squares);\n    _mm256_storeu_si256((__m256i *)(result + 4), sum_products);\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    for (int j = 0; j < 8; ++j) {\n        total_sum_squares += result[j];\n        total_sum_products += result[j + 4];\n    }\n\n    for (size_t i = length - length % 32; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        total_sum_squares += a * a + b * b;\n        total_sum_products += a * b;\n    }\n\n    return total_sum_squares - 2 * total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i sq_A = _mm256_mullo_epi16(vec_A, vec_A);\n        __m256i sq_B = _mm256_mullo_epi16(vec_B, vec_B);\n\n        __m256i prod_AB = _mm256_mullo_epi16(vec_A, vec_B);\n\n        sum_squares = _mm256_add_epi32(sum_squares, sq_A);\n        sum_squares = _mm256_add_epi32(sum_squares, sq_B);\n\n        sum_products = _mm256_add_epi32(sum_products, prod_AB);\n    }\n\n    int32_t result_squares[8];\n    int32_t result_products[8];\n\n    _mm256_storeu_si256((__m256i *)result_squares, sum_squares);\n    _mm256_storeu_si256((__m256i *)result_products, sum_products);\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    for (int j = 0; j < 8; ++j) {\n        total_sum_squares += result_squares[j];\n        total_sum_products += result_products[j];\n    }\n\n    int64_t diff = total_sum_squares - 2 * total_sum_products;\n\n    for (size_t i = length - 32; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        diff += a * a + b * b - 2 * a * b;\n    }\n\n    return diff;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vn = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i*)&C[i], vn);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vc = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vn = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i*)&C[i], vn);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vn = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i*)&C[i], vn);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i minus_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_positive = _mm256_cmpgt_epi8(a_vec, zero);\n        __m256i b_positive = _mm256_cmpgt_epi8(b_vec, zero);\n        __m256i a_negative = _mm256_cmplt_epi8(a_vec, zero);\n        __m256i b_negative = _mm256_cmplt_epi8(b_vec, zero);\n\n        __m256i result = _mm256_or_si256(_mm256_and_si256(a_positive, b_positive), _mm256_and_si256(a_negative, b_negative));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(_mm256_or_si256(a_positive, b_positive), _mm256_or_si256(a_negative, b_negative)));\n\n        result = _mm256_or_si256(result, _mm256_and_si256(result, one));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(result, minus_one));\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i pos_mask = _mm256_cmpgt_epi8(va, _mm256_setzero_si256());\n        __m256i neg_mask = _mm256_cmplt_epi8(va, _mm256_setzero_si256());\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(pos_mask, _mm256_and_si256(neg_mask, _mm256_set1_epi8(1))),\n            _mm256_andnot_si256(_mm256_or_si256(pos_mask, neg_mask), _mm256_setzero_si256())\n        );\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i minus_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_positive = _mm256_cmpgt_epi8(a, zero);\n        __m256i b_positive = _mm256_cmpgt_epi8(b, zero);\n        __m256i a_negative = _mm256_cmplt_epi8(a, zero);\n        __m256i b_negative = _mm256_cmplt_epi8(b, zero);\n\n        __m256i result = _mm256_or_si256(_mm256_and_si256(a_positive, b_positive), _mm256_and_si256(a_negative, b_negative));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(_mm256_or_si256(a_positive, b_positive), _mm256_or_si256(a_negative, b_negative)));\n\n        result = _mm256_or_si256(result, _mm256_and_si256(result, one));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(result, minus_one));\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i minus_one = _mm256_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i a_positive = _mm256_cmpgt_epi8(a_vec, zero);\n        __m256i b_positive = _mm256_cmpgt_epi8(b_vec, zero);\n        __m256i a_negative = _mm256_cmplt_epi8(a_vec, zero);\n        __m256i b_negative = _mm256_cmplt_epi8(b_vec, zero);\n\n        __m256i result = _mm256_or_si256(_mm256_and_si256(a_positive, b_positive), _mm256_and_si256(a_negative, b_negative));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(_mm256_or_si256(a_positive, b_positive), _mm256_or_si256(a_negative, b_negative)));\n\n        result = _mm256_or_si256(result, _mm256_and_si256(result, one));\n        result = _mm256_or_si256(result, _mm256_andnot_si256(result, minus_one));\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 32; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set1_epi8(1);\n    __m256i mask_neg = _mm256_set1_epi8(-1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i pos_mask = _mm256_and_si256(vec_A, vec_B);\n        __m256i neg_mask = _mm256_or_si256(_mm256_andnot_si256(vec_A, vec_B), _mm256_andnot_si256(vec_B, vec_A));\n\n        __m256i result = _mm256_or_si256(_mm256_or_si256(pos_mask, neg_mask), zero);\n\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 32; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i mid_mask = _mm256_set1_epi32(0x80000000); // Midpoint mask\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        __m256i select_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(i + 8), mid_mask);\n        __m256i final_result = _mm256_blendv_epi8(xor_result, and_result, select_mask);\n\n        if (length - i > 16) {\n            _mm256_storeu_si256((__m256i*)&C[i], final_result);\n        } else {\n            for (int j = 0; j < length - i; ++j) {\n                C[i + j] = final_result.m256i_i16[j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i mid_mask = _mm256_set1_epi32(0x80000000); // Assuming 32-bit integers for simplicity\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i mid_point = _mm256_and_si256(a, mid_mask);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        __m256i final_result = _mm256_or_si256(mid_point, _mm256_or_si256(xor_result, and_result));\n\n        _mm256_storeu_si256((__m256i*)&C[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i mid_mask = _mm256_set1_epi32(0x80000000); // Midpoint mask\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        __m256i select_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(i + 8), mid_mask);\n        __m256i final_result = _mm256_blendv_epi8(xor_result, and_result, select_mask);\n\n        if (length - i >= 16) {\n            _mm256_storeu_si256((__m256i*)&C[i], final_result);\n        } else {\n            for (int j = 0; j < length - i; ++j) {\n                C[i + j] = final_result.m256i_i16[j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i mid_mask = _mm256_set1_epi32(0x80000000); // Midpoint mask\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        __m256i select_mask = _mm256_cmpgt_epi32(_mm256_setr_epi32(i / 4, i / 4, i / 4, i / 4), mid_mask);\n        __m256i final_result = _mm256_blendv_epi8(and_result, xor_result, select_mask);\n\n        _mm256_storeu_si256((__m256i*)&C[i], final_result);\n    }\n\n    for (; i < length; ++i) {\n        if (i == length / 2) {\n            C[i] = A[i];\n        } else if (i < length / 2) {\n            C[i] = A[i] ^ B[i];\n        } else {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i mid_mask = _mm256_set1_epi32(0x80000000);\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        __m256i and_result = _mm256_and_si256(a, b);\n\n        __m256i select_mask = _mm256_cmpgt_epi32(_mm256_srli_epi32(mid_mask, 1), zero);\n        __m256i final_result = _mm256_blendv_epi8(and_result, xor_result, select_mask);\n\n        _mm256_storeu_si256((__m256i*)&C[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (i == length / 2) {\n            C[i] = A[i];\n        } else if (i < length / 2) {\n            C[i] = A[i] ^ B[i];\n        } else {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256 va, vb, vc, vd;\n    __m256i vcond1, vcond2, vtrue = _mm256_set1_epi32(0xFFFFFFFF), vfalse = _mm256_setzero_si256();\n    size_t i, j, k;\n\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 8) {\n            va = _mm256_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm256_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm256_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm256_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm256_loadu_si256((__m256i*)(cond1 + i * dim2 * dim3 + j * dim3));\n            vcond2 = _mm256_loadu_si256((__m256i*)(cond2 + i * dim2 * dim3 + j * dim3));\n\n            __m256i mask1 = _mm256_and_si256(vcond1, vcond2);\n            __m256i mask2 = _mm256_andnot_si256(vcond2, vcond1);\n            __m256i mask3 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i mask4 = _mm256_andnot_si256(mask1, mask2);\n\n            va = _mm256_blendv_ps(va, vb, _mm256_castsi256_ps(mask2));\n            va = _mm256_blendv_ps(va, vc, _mm256_castsi256_ps(mask3));\n            va = _mm256_blendv_ps(va, vd, _mm256_castsi256_ps(mask4));\n\n            _mm256_storeu_ps(out + i * dim2 * dim3 + j * dim3, va);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256 va, vb, vc, vd;\n    __m256i vcond1, vcond2, vtrue = _mm256_set1_epi32(0xFFFFFFFF), vfalse = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 8) {\n            va = _mm256_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm256_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm256_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm256_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm256_loadu_si256((__m256i*)(cond1 + i * dim2 * dim3 + j * dim3));\n            vcond2 = _mm256_loadu_si256((__m256i*)(cond2 + i * dim2 * dim3 + j * dim3));\n\n            __m256i mask1 = _mm256_and_si256(vcond1, vcond2);\n            __m256i mask2 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i mask3 = _mm256_andnot_si256(mask1, vcond2);\n            __m256i mask4 = _mm256_andnot_si256(mask1, mask2);\n\n            va = _mm256_blendv_ps(va, vb, _mm256_castsi256_ps(mask2));\n            va = _mm256_blendv_ps(va, vc, _mm256_castsi256_ps(mask3));\n            va = _mm256_blendv_ps(va, vd, _mm256_castsi256_ps(mask4));\n\n            _mm256_storeu_ps(out + i * dim2 * dim3 + j * dim3, va);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256 va, vb, vc, vd;\n    __m256i vcond1, vcond2, vtrue = _mm256_set1_epi32(0xFFFFFFFF), vfalse = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 8) {\n            va = _mm256_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm256_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm256_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm256_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm256_loadu_si256((__m256i*)(cond1 + i * dim2 * dim3 + j * dim3));\n            vcond2 = _mm256_loadu_si256((__m256i*)(cond2 + i * dim2 * dim3 + j * dim3));\n\n            __m256i mask1 = _mm256_and_si256(vcond1, vcond2);\n            __m256i mask2 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i mask3 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i mask4 = _mm256_andnot_si256(vcond1, vcond2);\n\n            va = _mm256_blendv_ps(va, vb, _mm256_castsi256_ps(mask1));\n            vc = _mm256_blendv_ps(vc, vd, _mm256_castsi256_ps(mask2));\n            va = _mm256_blendv_ps(va, vc, _mm256_castsi256_ps(mask3));\n            va = _mm256_blendv_ps(va, vd, _mm256_castsi256_ps(mask4));\n\n            _mm256_storeu_ps(out + i * dim2 * dim3 + j * dim3, va);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256 va, vb, vc, vd;\n    __m256i vcond1, vcond2, vtrue = _mm256_set1_epi32(0xFFFFFFFF), vfalse = _mm256_setzero_si256();\n    __m256 result;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 8) {\n            va = _mm256_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm256_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm256_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm256_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm256_loadu_si256((__m256i*)(cond1 + i * dim2 * dim3 + j * dim3));\n            vcond2 = _mm256_loadu_si256((__m256i*)(cond2 + i * dim2 * dim3 + j * dim3));\n\n            vcond1 = _mm256_and_si256(vcond1, vcond2);\n            vcond2 = _mm256_andnot_si256(vcond1, vcond2);\n\n            result = _mm256_or_ps(_mm256_and_ps(va, vcond1), _mm256_and_ps(vb, vcond2));\n            result = _mm256_or_ps(result, _mm256_andnot_ps(vcond1, _mm256_and_ps(vc, vcond2)));\n            result = _mm256_or_ps(result, _mm256_andnot_ps(vcond1, _mm256_andnot_ps(vcond2, vd)));\n\n            _mm256_storeu_ps(out + i * dim2 * dim3 + j * dim3, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256 va, vb, vc, vd;\n    __m256i vcond1, vcond2, vtrue = _mm256_set1_epi32(0xFFFFFFFF), vfalse = _mm256_setzero_si256();\n    size_t i, j, k;\n\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 8) {\n            va = _mm256_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm256_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm256_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm256_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm256_loadu_si256((__m256i*)(cond1 + i * dim2 * dim3 + j * dim3));\n            vcond2 = _mm256_loadu_si256((__m256i*)(cond2 + i * dim2 * dim3 + j * dim3));\n\n            __m256i vselect1 = _mm256_and_si256(vcond1, vcond2);\n            __m256i vselect2 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i vselect3 = _mm256_andnot_si256(vcond1, vcond2);\n            __m256i vselect4 = _mm256_andnot_si256(vcond1, vcond2);\n\n            vselect1 = _mm256_or_si256(vselect1, _mm256_andnot_si256(vcond2, vcond1));\n            vselect2 = _mm256_or_si256(vselect2, _mm256_andnot_si256(vcond1, vcond2));\n            vselect3 = _mm256_or_si256(vselect3, _mm256_andnot_si256(vcond1, vcond2));\n            vselect4 = _mm256_or_si256(vselect4, _mm256_andnot_si256(vcond1, vcond2));\n\n            va = _mm256_blendv_ps(va, vb, _mm256_castsi256_ps(vselect1));\n            vc = _mm256_blendv_ps(vc, vd, _mm256_castsi256_ps(vselect2));\n            va = _mm256_blendv_ps(va, vc, _mm256_castsi256_ps(vselect3));\n            va = _mm256_blendv_ps(va, vd, _mm256_castsi256_ps(vselect4));\n\n            _mm256_storeu_ps(out + i * dim2 * dim3 + j * dim3, va);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 8;\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m256i vA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vCount = _mm256_setzero_si256();\n            for (int k = 0; k < dim3; ++k) {\n                __m256i vBitCount = _mm256_popcnt_epi32(vA);\n                vCount = _mm256_add_epi8(vCount, vBitCount);\n                vA = _mm256_srli_epi32(vA, 32);\n            }\n            _mm256_storeu_si256((__m256i*)&out[i * dim2 * dim3 + j * dim3], vCount);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 8; // AVX2 processes 8 32-bit integers at a time\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m256i vA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vCount = _mm256_setzero_si256();\n            for (int k = 0; k < dim3; ++k) {\n                __m256i vBitCount = _mm256_popcnt_epi32(vA);\n                vCount = _mm256_add_epi8(vCount, vBitCount);\n                vA = _mm256_srli_epi32(vA, 32);\n            }\n            _mm256_storeu_si256((__m256i*)&out[i * dim2 * dim3 + j * dim3], vCount);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 8;\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m256i vA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vCount = _mm256_setzero_si256();\n            for (int k = 0; k < dim3; ++k) {\n                __m256i vBitCount = _mm256_popcnt_epi32(vA);\n                vCount = _mm256_add_epi8(vCount, vBitCount);\n                vA = _mm256_srli_epi32(vA, 32);\n            }\n            _mm256_storeu_si256((__m256i*)&out[i * dim2 + j], vCount);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 8;\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m256i vA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vCount = _mm256_setzero_si256();\n            for (int k = 0; k < dim3; ++k) {\n                __m256i vBitCount = _mm256_popcnt_epi32(vA);\n                vCount = _mm256_add_epi8(vCount, vBitCount);\n                vA = _mm256_srli_epi32(vA, 32);\n            }\n            _mm256_storeu_si256((__m256i*)&out[i * dim2 * dim3 + j * dim3], vCount);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 8;\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m256i vA = _mm256_loadu_si256((__m256i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m256i vCount = _mm256_setzero_si256();\n            for (int k = 0; k < dim3; ++k) {\n                __m256i vBitCount = _mm256_popcnt_epi32(vA);\n                vCount = _mm256_add_epi8(vCount, vBitCount);\n                vA = _mm256_srli_epi32(vA, 32);\n            }\n            _mm256_storeu_si256((__m256i*)&out[i * dim2 + j], vCount);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 integers at a time\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - vec_size + 1; j += vec_size) {\n            __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i * cols + j]);\n            __m256i vec_B = _mm256_and_si256(vec_A, _mm256_set1_epi32(0xFFFFFFFE)); // Clear the least significant bit\n            __m256i mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec_A, _mm256_set1_epi32(0x00000003)), _mm256_setzero_si256());\n            __m256i abs_vec = _mm256_or_si256(vec_B, _mm256_andnot_si256(mask, vec_A));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], abs_vec);\n        }\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - vec_size + 1; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 integers at a time\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - vec_size + 1; j += vec_size) {\n            __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i * cols + j]);\n            __m256i div_by_3_mask = _mm256_and_si256(_mm256_srai_epi32(vec_A, 31), _mm256_set1_epi32(1));\n            __m256i abs_vec_A = _mm256_or_si256(_mm256_andnot_si256(div_by_3_mask, vec_A), _mm256_and_si256(div_by_3_mask, _mm256_sub_epi32(_mm256_setzero_si256(), vec_A)));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], abs_vec_A);\n        }\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - cols % vec_size; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 integers at a time\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - vec_size + 1; j += vec_size) {\n            __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i * cols + j]);\n            __m256i vec_B = _mm256_setzero_si256();\n            __m256i mask = _mm256_and_si256(vec_A, _mm256_set1_epi32(0xFFFFFFFE)); // Clear the least significant bit\n            __m256i div_by_3 = _mm256_add_epi32(mask, _mm256_set1_epi32(1));\n            __m256i abs_vec = _mm256_sub_epi32(_mm256_or_si256(div_by_3, vec_B), _mm256_and_si256(div_by_3, vec_B));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], abs_vec);\n        }\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - cols % vec_size; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 integers at a time\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - vec_size + 1; j += vec_size) {\n            __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i * cols + j]);\n            __m256i div_by_3_mask = _mm256_and_si256(vec_A, _mm256_set1_epi32(0xFFFFFFFE));\n            __m256i abs_vec_A = _mm256_or_si256(div_by_3_mask, _mm256_srli_epi32(vec_A, 31));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], abs_vec_A);\n        }\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - cols % vec_size; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const size_t vec_size = 8; // AVX2 can process 8 integers at a time\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - vec_size + 1; j += vec_size) {\n            __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i * cols + j]);\n            __m256i vec_B = _mm256_and_si256(vec_A, _mm256_set1_epi32(0xFFFFFFFE)); // Clear the least significant bit\n            __m256i vec_C = _mm256_or_si256(vec_B, _mm256_and_si256(_mm256_srai_epi32(vec_A, 31), _mm256_set1_epi32(3)));\n            _mm256_storeu_si256((__m256i*)&B[i * cols + j], vec_C);\n        }\n        // Handle remaining elements that do not fit into a full vector\n        for (size_t j = cols - cols % vec_size; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i rotate_mask = _mm256_set1_epi32(0xF); // Mask to extract low 4 bits\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, rotate_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, rotate_mask);\n        __m256i rotated = _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits));\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi32(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i rotate_mask = _mm256_set1_epi32(0xF); // Mask to extract the low 4 bits\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, rotate_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, rotate_mask);\n        __m256i rotated = _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits));\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi32(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i rotate_mask = _mm256_set1_epi32(0xF); // Mask to extract the low 4 bits\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, rotate_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, rotate_mask);\n        __m256i rotated = _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits));\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi32(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i rotate_mask = _mm256_set1_epi32(0xF); // Mask to extract low 4 bits\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, rotate_mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, rotate_mask);\n        __m256i rotated = _mm256_srlv_epi32(src_vec, _mm256_set1_epi32(rotate_bits));\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi32(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i mask = _mm256_set1_epi32(0xF); // Mask to extract the low 4 bits\n    const __m256i rotate_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotation\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(src_vec, mask);\n\n        // Check if the low 4 bits are all 1\n        __m256i all_ones = _mm256_cmpeq_epi32(low_4_bits, _mm256_set1_epi32(0xF));\n        __m256i cond = _mm256_movemask_epi8(_mm256_castsi256_si128(all_ones));\n\n        if (cond == 0xFF) { // All elements satisfy the condition\n            __m256i rotated = _mm256_rotl_epi32(src_vec, rotate_bits);\n            _mm256_storeu_si256((__m256i*)&dst[i], rotated);\n        } else { // At least one element does not satisfy the condition\n            __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(-1));\n            _mm256_storeu_si256((__m256i*)&dst[i], negated);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256i v_one = _mm256_set1_epi8(1);\n    __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff = _mm256_sub_ps(va, vb);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, v_zero, _CMP_LE_OS), diff);\n        __m256 greater_than_epsilon = _mm256_cmp_ps(abs_diff, v_epsilon, _CMP_GT_OS);\n        __m256 less_than_neg_epsilon = _mm256_cmp_ps(abs_diff, v_epsilon, _CMP_LT_OS);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(_mm256_castps_si256(greater_than_epsilon), v_one),\n            _mm256_and_si256(_mm256_castps_si256(less_than_neg_epsilon), v_neg_one)\n        );\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256i v_one = _mm256_set1_epi8(1);\n    __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff = _mm256_sub_ps(va, vb);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, v_zero, _CMP_LE_OS), diff);\n        __m256 abs_epsilon = _mm256_andnot_ps(_mm256_cmp_ps(v_epsilon, v_zero, _CMP_LE_OS), v_epsilon);\n\n        __m256i mask_greater = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_GT_OS);\n        __m256i mask_less = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LT_OS);\n        __m256i result = _mm256_or_si256(_mm256_and_si256(mask_greater, v_one), _mm256_and_si256(mask_less, v_neg_one));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256i v_one = _mm256_set1_epi8(1);\n    __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff = _mm256_sub_ps(va, vb);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, v_zero, _CMP_LE_OS), diff);\n        __m256 greater_than_epsilon = _mm256_cmp_ps(abs_diff, v_epsilon, _CMP_GT_OS);\n        __m256 less_than_neg_epsilon = _mm256_cmp_ps(abs_diff, v_epsilon, _CMP_LT_OS);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(_mm256_castps_si256(greater_than_epsilon), v_one),\n            _mm256_and_si256(_mm256_castps_si256(less_than_neg_epsilon), v_neg_one)\n        );\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256i v_one = _mm256_set1_epi8(1);\n    __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 7 < length; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff = _mm256_sub_ps(va, vb);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, v_zero, _CMP_LE_OS), diff);\n        __m256 abs_epsilon = _mm256_andnot_ps(_mm256_cmp_ps(v_epsilon, v_zero, _CMP_LE_OS), v_epsilon);\n\n        __m256i mask_greater = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_GT_OS);\n        __m256i mask_less = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LT_OS);\n        __m256i result = _mm256_or_si256(_mm256_and_si256(mask_greater, v_one), _mm256_and_si256(mask_less, v_neg_one));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256i v_one = _mm256_set1_epi8(1);\n    __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff = _mm256_sub_ps(va, vb);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, v_zero, _CMP_LE_OS), diff);\n        __m256 abs_epsilon = _mm256_andnot_ps(_mm256_cmp_ps(v_epsilon, v_zero, _CMP_LE_OS), v_epsilon);\n\n        __m256i mask_greater = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_GT_OS);\n        __m256i mask_less = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LT_OS);\n        __m256i result = _mm256_or_si256(_mm256_and_si256(mask_greater, v_one), _mm256_and_si256(mask_less, v_neg_one));\n\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i INT32_MAX = _mm256_set1_epi32(INT32_MAX);\n    const __m256i INT32_MIN = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i;\n    for (i = 0; i <= size - 8; i += 8) {\n        __m256 floats = _mm256_loadu_ps(&A[i]);\n        __m256i ints = _mm256_cvtps_epi32(floats);\n        __m256i clamped = _mm256_max_epi32(_mm256_min_epi32(ints, INT32_MAX), INT32_MIN);\n        _mm256_storeu_si256((__m256i*)&B[i], clamped);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], static_cast<float>(INT32_MIN), static_cast<float>(INT32_MAX)));\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 max_float = _mm256_set1_ps(static_cast<float>(INT32_MAX));\n    const __m256 min_float = _mm256_set1_ps(static_cast<float>(INT32_MIN));\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(&A[i]);\n        __m256 cmp_max = _mm256_cmp_ps(vec_A, max_float, _CMP_LE_OS);\n        __m256 cmp_min = _mm256_cmp_ps(vec_A, min_float, _CMP_GE_OS);\n\n        __m256i vec_B = _mm256_or_si256(\n            _mm256_and_si256(_mm256_castps_si256(cmp_max), int_max),\n            _mm256_andnot_si256(_mm256_castps_si256(cmp_min), int_min)\n        );\n\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::max(std::min(A[i], static_cast<float>(INT32_MAX)), static_cast<float>(INT32_MIN)));\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(&A[i]);\n        __m256 clamped_A = _mm256_min_ps(_mm256_max_ps(vec_A, min_val), max_val);\n        __m256i vec_B = _mm256_cvtps_epi32(clamped_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], (float)INT32_MIN, (float)INT32_MAX));\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i int_max = _mm256_set1_epi32(INT32_MAX);\n    const __m256i int_min = _mm256_set1_epi32(INT32_MIN);\n    const __m256 max_float = _mm256_set1_ps(static_cast<float>(INT32_MAX));\n    const __m256 min_float = _mm256_set1_ps(static_cast<float>(INT32_MIN));\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 vec_A = _mm256_loadu_ps(&A[i]);\n        __m256 clamped_A = _mm256_min_ps(_mm256_max_ps(vec_A, min_float), max_float);\n        __m256i vec_B = _mm256_cvtps_epi32(clamped_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], static_cast<float>(INT32_MIN), static_cast<float>(INT32_MAX)));\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256i INT32_MAX = _mm256_set1_epi32(INT32_MAX);\n    const __m256i INT32_MIN = _mm256_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 floats = _mm256_loadu_ps(&A[i]);\n        __m256i ints = _mm256_cvtps_epi32(floats);\n        __m256i clamped = _mm256_max_epi32(_mm256_min_epi32(ints, INT32_MAX), INT32_MIN);\n        _mm256_storeu_si256((__m256i*)&B[i], clamped);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], static_cast<float>(INT32_MIN), static_cast<float>(INT32_MAX)));\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256 scale_factor = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 result = _mm256_mul_ps(_mm256_cvtepi32_ps(a_vec), scale_factor);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256 scale_factor = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 scaled_vec = _mm256_mul_ps(_mm256_cvtepi32_ps(a_vec), scale_factor);\n        _mm256_storeu_ps(&B[i], scaled_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256 scale_factor = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 scaled_vec = _mm256_mul_ps(_mm256_cvtepi32_ps(a_vec), scale_factor);\n        _mm256_storeu_ps(&B[i], scaled_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256 scale_factor = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 scaled_vec = _mm256_mul_ps(_mm256_cvtepi32_ps(a_vec), scale_factor);\n        _mm256_storeu_ps(&B[i], scaled_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi32(scale);\n    const __m256 scale_factor = _mm256_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 scaled_vec = _mm256_mul_ps(_mm256_cvtepi32_ps(a_vec), scale_factor);\n        _mm256_storeu_ps(&B[i], scaled_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vresult = _mm256_div_ps(va, vb);\n        __m256i viresult = _mm256_cvtps_epi32(vresult);\n        _mm256_storeu_si256((__m256i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vr = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vr);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vr = _mm256_div_ps(va, vb);\n        __m256i vi = _mm256_cvtps_epi32(vr);\n        _mm256_storeu_si256((__m256i*)&C[i], vi);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vresult = _mm256_div_ps(va, vb);\n        __m256i viresult = _mm256_cvtps_epi32(vresult);\n        _mm256_storeu_si256((__m256i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n        __m256 vresult = _mm256_div_ps(va, vb);\n        __m256i viresult = _mm256_cvtps_epi32(vresult);\n        _mm256_storeu_si256((__m256i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 maxFloat = _mm256_set1_ps(FLT_MAX);\n    const __m256 minFloat = _mm256_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256d dValues = _mm256_loadu_pd(&A[i]);\n        __m256d dMax = _mm256_max_pd(dValues, _mm256_setzero_pd());\n        __m256d dMin = _mm256_min_pd(dMax, _mm256_set1_pd(FLT_MAX));\n        __m256 sValues = _mm256_cvtpd_ps(dMin);\n        _mm256_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(value);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 maxFloat = _mm256_set1_ps(FLT_MAX);\n    const __m256 minFloat = _mm256_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256d dValues = _mm256_loadu_pd(&A[i]);\n        __m256d dMax = _mm256_max_pd(dValues, _mm256_setzero_pd());\n        __m256d dMin = _mm256_min_pd(dMax, _mm256_set1_pd(FLT_MAX));\n        __m256 sValues = _mm256_cvtpd_ps(dMin);\n        _mm256_storeu_ps(&B[i], sValues);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_float = _mm256_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n        __m256d a_high = _mm256_permutevar8x32_pd(a, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256d a_low = _mm256_permutevar8x32_pd(a, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4));\n\n        __m256d abs_a_high = _mm256_andnot_pd(_mm256_set1_pd(-0.0), a_high);\n        __m256d abs_a_low = _mm256_andnot_pd(_mm256_set1_pd(-0.0), a_low);\n\n        __m256d cmp_max_high = _mm256_cmp_pd(abs_a_high, max_float, _CMP_LE_OS);\n        __m256d cmp_min_high = _mm256_cmp_pd(abs_a_high, min_float, _CMP_GE_OS);\n        __m256d cmp_max_low = _mm256_cmp_pd(abs_a_low, max_float, _CMP_LE_OS);\n        __m256d cmp_min_low = _mm256_cmp_pd(abs_a_low, min_float, _CMP_GE_OS);\n\n        __m256d result_high = _mm256_blendv_pd(max_float, min_float, cmp_min_high);\n        __m256d result_low = _mm256_blendv_pd(max_float, min_float, cmp_min_low);\n\n        result_high = _mm256_or_pd(result_high, _mm256_and_pd(cmp_max_high, a_high));\n        result_low = _mm256_or_pd(result_low, _mm256_and_pd(cmp_max_low, a_low));\n\n        __m256 result_high_f = _mm256_cvtpd_ps(result_high);\n        __m256 result_low_f = _mm256_cvtpd_ps(result_low);\n\n        _mm256_storeu_ps(&B[i], result_high_f);\n        _mm256_storeu_ps(&B[i + 4], result_low_f);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(val);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 maxFloat = _mm256_set1_ps(FLT_MAX);\n    const __m256 minFloat = _mm256_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256d a = _mm256_loadu_pd(&A[i]);\n        __m256d a_high = _mm256_permutevar8x32_pd(a, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        \n        __m256d abs_a = _mm256_andnot_pd(_mm256_set1_pd(-0.0), a);\n        __m256d abs_a_high = _mm256_andnot_pd(_mm256_set1_pd(-0.0), a_high);\n        \n        __m256d clamp_mask = _mm256_cmp_pd(abs_a, maxFloat, _CMP_LE_OS);\n        __m256d clamp_mask_high = _mm256_cmp_pd(abs_a_high, maxFloat, _CMP_LE_OS);\n        \n        __m256d clamped_a = _mm256_blendv_pd(maxFloat, a, clamp_mask);\n        __m256d clamped_a_high = _mm256_blendv_pd(maxFloat, a_high, clamp_mask_high);\n        \n        __m256d neg_clamp_mask = _mm256_cmp_pd(abs_a, minFloat, _CMP_GE_OS);\n        __m256d neg_clamp_mask_high = _mm256_cmp_pd(abs_a_high, minFloat, _CMP_GE_OS);\n        \n        __m256d clamped_a_neg = _mm256_blendv_pd(minFloat, clamped_a, neg_clamp_mask);\n        __m256d clamped_a_high_neg = _mm256_blendv_pd(minFloat, clamped_a_high, neg_clamp_mask_high);\n        \n        __m256 result = _mm256_castpd_ps(clamped_a_neg);\n        __m256 result_high = _mm256_castpd_ps(clamped_a_high_neg);\n        \n        _mm256_storeu_ps(&B[i], result);\n        _mm256_storeu_ps(&B[i + 4], result_high);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 maxFloat = _mm256_set1_ps(FLT_MAX);\n    const __m256 minFloat = _mm256_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256d dValues = _mm256_loadu_pd(&A[i]);\n        __m256d dMax = _mm256_max_pd(dValues, _mm256_set1_pd(FLT_MAX));\n        __m256d dMin = _mm256_min_pd(dMax, _mm256_set1_pd(-FLT_MAX));\n\n        __m256 sValues = _mm256_cvtpd_ps(dMin);\n        _mm256_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 can process 8 int16_t elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 can process 8 int16_t elements at once\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 can process 8 int16_t elements at once\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of int16 elements per AVX2 register\n    size_t i;\n\n    // Process elements in chunks of AVX2 width\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of elements per AVX2 register\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vcontrol = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 vcond = _mm256_cmp_ps(vcontrol, v_zero, _CMP_GT_OS);\n        __m256 vresult = _mm256_sub_ps(va, v_min_val);\n        vresult = _mm256_div_ps(vresult, _mm256_sub_ps(v_max_val, v_min_val));\n        vresult = _mm256_max_ps(vresult, v_zero);\n        vresult = _mm256_min_ps(vresult, v_one);\n\n        vresult = _mm256_blendv_ps(vresult, va, vcond);\n\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vcontrol = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 vcond = _mm256_cmp_ps(vcontrol, v_zero, _CMP_GT_OS);\n        __m256 vresult = _mm256_sub_ps(va, v_min_val);\n        vresult = _mm256_div_ps(vresult, _mm256_sub_ps(v_max_val, v_min_val));\n        vresult = _mm256_max_ps(vresult, v_zero);\n        vresult = _mm256_min_ps(vresult, v_one);\n\n        vresult = _mm256_blendv_ps(vresult, va, vcond);\n\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vcontrol = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 vcond = _mm256_cmp_ps(vcontrol, v_zero, _CMP_GT_OS);\n        __m256 vnorm = _mm256_sub_ps(va, v_min_val);\n        vnorm = _mm256_div_ps(vnorm, _mm256_sub_ps(v_max_val, v_min_val));\n        vnorm = _mm256_max_ps(vnorm, v_zero);\n        vnorm = _mm256_min_ps(vnorm, v_one);\n\n        __m256 vresult = _mm256_blendv_ps(vnorm, va, vcond);\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vcontrol = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 vcond = _mm256_cmp_ps(vcontrol, v_zero, _CMP_GT_OS);\n        __m256 vresult = _mm256_sub_ps(va, v_min_val);\n        vresult = _mm256_div_ps(vresult, _mm256_sub_ps(v_max_val, v_min_val));\n        vresult = _mm256_max_ps(vresult, v_zero);\n        vresult = _mm256_min_ps(vresult, v_one);\n\n        vresult = _mm256_blendv_ps(vresult, va, vcond);\n\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 v_min_val = _mm256_set1_ps(min_val);\n    __m256 v_max_val = _mm256_set1_ps(max_val);\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vcontrol = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        __m256 vcond = _mm256_cmp_ps(vcontrol, v_zero, _CMP_GT_OS);\n        __m256 vnorm = _mm256_sub_ps(va, v_min_val);\n        vnorm = _mm256_div_ps(vnorm, _mm256_sub_ps(v_max_val, v_min_val));\n        vnorm = _mm256_max_ps(vnorm, v_zero);\n        vnorm = _mm256_min_ps(vnorm, v_one);\n\n        __m256 vresult = _mm256_blendv_ps(vnorm, va, vcond);\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_cvtepi32_ps(vec_A);\n        vec_B = _mm256_sqrt_ps(vec_B);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrtf(static_cast<float>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_cvtepi32_ps(vec_A); // Convert int32 to float\n        vec_B = _mm256_sqrt_ps(vec_B); // Take square root\n        _mm256_storeu_ps(&B[i], vec_B); // Store results\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrt(static_cast<float>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_cvtepi32_ps(vec_A); // Convert int32 to float32\n        vec_B = _mm256_sqrt_ps(vec_B); // Take square root\n        _mm256_storeu_ps(&B[i], vec_B); // Store results\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrt(static_cast<float>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B;\n\n        // Extract high 16 bits from each 32-bit integer\n        __m256i high_bits = _mm256_srli_epi32(vec_A, 16);\n\n        // Convert high 16 bits to float\n        vec_B = _mm256_cvtepi32_ps(high_bits);\n\n        // Take square root of the float values\n        vec_B = _mm256_sqrt_ps(vec_B);\n\n        // Store the results back to the destination array\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrt(static_cast<float>(A[i] >> 16));\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    for (size_t i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_cvtepi32_ps(vec_A);\n        vec_B = _mm256_sqrt_ps(vec_B);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n    // Handle remaining elements\n    for (size_t i = size - size % simd_width; i < size; ++i) {\n        B[i] = sqrt(static_cast<float>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_castsi256_ps(vec_A);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8;\n    size_t i;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_castsi256_ps(vec_A);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_castsi256_ps(vec_A);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_castsi256_ps(vec_A);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 8; // AVX2 processes 8 floats at a time\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i <= size - simd_width; i += simd_width) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_B = _mm256_castsi256_ps(vec_A);\n        _mm256_storeu_ps(&B[i], vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i zero_i = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_setzero_ps();\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n\n        __m256i mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(prev, zero_i));\n        __m256 diff_vec = _mm256_sub_ps(_mm256_cvtepi32_ps(curr), _mm256_cvtepi32_ps(prev));\n\n        __m256 result = _mm256_and_ps(diff_vec, mask);\n        _mm256_storeu_ps(&diff[i], result);\n\n        if (i == 0) {\n            _mm256_storeu_ps(&diff[0], zero_f);\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i + 1] - A[i]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i zero_i32 = _mm256_setzero_si256();\n    __m256 zero_f32 = _mm256_setzero_ps();\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n\n        __m256i diff_i32 = _mm256_sub_epi32(curr, prev);\n\n        __m256 mask = _mm256_castsi256_ps(_mm256_and_si256(_mm256_castps_si256(_mm256_loadu_ps((const float*)&cond[i])), _mm256_cmpgt_epi32(diff_i32, zero_i32)));\n\n        __m256 diff_f32 = _mm256_cvtepi32_ps(diff_i32);\n        diff_f32 = _mm256_and_ps(diff_f32, mask);\n\n        _mm256_storeu_ps(&diff[i], diff_f32);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i + 1] - A[i]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i zero_i = _mm256_setzero_si256();\n    __m256 zero_f = _mm256_setzero_ps();\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i prev = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i curr = _mm256_loadu_si256((__m256i*)&A[i + 1]);\n\n        __m256i mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(prev, zero_i));\n        __m256i diff_int = _mm256_sub_epi32(curr, prev);\n\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n        diff_float = _mm256_and_ps(diff_float, mask);\n\n        _mm256_storeu_ps(&diff[i], diff_float);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i + 1] - A[i]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i a_vec, a_prev_vec;\n    __m256 diff_vec, zero_vec = _mm256_setzero_ps();\n    __m256i cond_vec;\n\n    // Initialize the previous value\n    a_prev_vec = _mm256_loadu_si256((__m256i*)(A + 1));\n    diff[0] = 0.0f;\n\n    for (size_t i = 1; i <= size - 4; i += 4) {\n        a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Calculate differences\n        diff_vec = _mm256_sub_epi32(a_vec, a_prev_vec);\n\n        // Convert to floats\n        diff_vec = _mm256_cvtepi32_ps(diff_vec);\n\n        // Load conditions\n        cond_vec = _mm256_loadu_si256((__m256i*)(cond + i));\n\n        // Mask differences based on conditions\n        diff_vec = _mm256_and_ps(diff_vec, _mm256_castsi256_ps(cond_vec));\n\n        // Store results\n        _mm256_storeu_ps(&diff[i], diff_vec);\n\n        // Update previous value\n        a_prev_vec = a_vec;\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i a_vec, a_prev_vec;\n    __m256 diff_vec, zero_vec;\n    __m256i cond_vec;\n\n    zero_vec = _mm256_setzero_ps();\n    a_prev_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        cond_vec = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Calculate differences\n        __m256i diff_int = _mm256_sub_epi32(a_vec, a_prev_vec);\n\n        // Convert to floats\n        diff_vec = _mm256_cvtepi32_ps(diff_int);\n\n        // Mask where cond is false\n        __m256 mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_vec, _mm256_setzero_si256()));\n\n        // Apply mask\n        diff_vec = _mm256_andnot_ps(mask, diff_vec);\n        diff_vec = _mm256_or_ps(diff_vec, _mm256_and_ps(mask, zero_vec));\n\n        // Store results\n        _mm256_storeu_ps(&diff[i], diff_vec);\n\n        // Update a_prev_vec for next iteration\n        a_prev_vec = a_vec;\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i result_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 32-bit integers at a time\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i dst_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], dst_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i result_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i result_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 8; // AVX2 processes 8 elements at a time\n    __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i result_vec = _mm256_or_si256(src_vec, ones);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d v = _mm256_i32gather_pd(vec, idx, 8);\n        sum = _mm256_add_pd(sum, v);\n    }\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = length - length % 4; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d v = _mm256_i32gather_pd(vec, idx, 8);\n        sum = _mm256_add_pd(sum, v);\n    }\n    double partial_sum[4];\n    _mm256_storeu_pd(partial_sum, sum);\n    double total_sum = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n    for (size_t i = length - length % 4; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d v = _mm256_i32gather_pd(vec, idx, 8);\n        sum = _mm256_add_pd(sum, v);\n    }\n    double partial_sum[4];\n    _mm256_storeu_pd(partial_sum, sum);\n    double total_sum = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n\n    for (size_t i = length - length % 4; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d v = _mm256_i32gather_pd(vec, idx, 8);\n        sum = _mm256_add_pd(sum, v);\n    }\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = length - length % 4; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d v = _mm256_i32gather_pd(vec, idx, 8);\n        sum = _mm256_add_pd(sum, v);\n    }\n    double temp[4];\n    _mm256_storeu_pd(temp, sum);\n    double total = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = length - length % 4; i < length; ++i) {\n        total += vec[index[i]];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t kernel_area = kernel_size * kernel_size;\n\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < output_size; ++i) {\n        size_t row_start = i * input_size;\n        size_t kernel_row_start = 0;\n\n        __m256d sum = _mm256_setzero_pd();\n\n        for (size_t j = 0; j < kernel_size; ++j) {\n            size_t col_start = row_start + j;\n            size_t kernel_col_start = j * kernel_size;\n\n            for (size_t k = 0; k < kernel_size; k += 4) {\n                __m256d input_vec = _mm256_loadu_pd(input + col_start + k);\n                __m256d kernel_vec = _mm256_loadu_pd(kernel + kernel_col_start + k);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(input_vec, kernel_vec));\n            }\n        }\n\n        __m256d relu_mask = _mm256_cmp_pd(sum, zero, _CMP_GE_OQ);\n        sum = _mm256_max_pd(sum, zero);\n\n        _mm256_storeu_pd(output + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t kernel_area = kernel_size * kernel_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < kernel_area; j += 4) {\n            __m256d input_vec = _mm256_loadu_pd(&input[i + j]);\n            __m256d kernel_vec = _mm256_loadu_pd(&kernel[j]);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(input_vec, kernel_vec));\n        }\n        double temp[4];\n        _mm256_storeu_pd(temp, sum);\n        double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Apply ReLU activation\n        if (result < 0) {\n            result = 0;\n        }\n\n        output[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t kernel_half_size = kernel_size / 2;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < kernel_size; ++j) {\n            for (size_t k = 0; k < kernel_size; k += 4) {\n                __m256d input_vec = _mm256_loadu_pd(&input[(i + j) * input_size + k]);\n                __m256d kernel_vec = _mm256_loadu_pd(&kernel[j * kernel_size + k]);\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(input_vec, kernel_vec));\n            }\n        }\n        double result[4];\n        _mm256_storeu_pd(result, sum);\n        output[i] = std::max(0.0, result[0] + result[1] + result[2] + result[3]);\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki += 4) {\n                for (size_t kj = 0; kj < kernel_size; kj++) {\n                    __m256d kernel_val = _mm256_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    __m256d input_val = _mm256_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    sum = _mm256_add_pd(sum, _mm256_mul_pd(kernel_val, input_val));\n                }\n            }\n\n            __m256d relu_result = _mm256_max_pd(sum, zero);\n            _mm256_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m256d zero = _mm256_setzero_pd();\n    __m256d one = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki += 4) {\n                __m256d k = _mm256_loadu_pd(&kernel[ki]);\n                __m256d v = _mm256_loadu_pd(&input[(i + ki) * input_size + j]);\n\n                sum = _mm256_add_pd(sum, _mm256_mul_pd(k, v));\n            }\n\n            __m256d relu_mask = _mm256_cmp_pd(sum, zero, _CMP_GE_OS);\n            sum = _mm256_andnot_pd(zero, _mm256_or_pd(sum, relu_mask));\n\n            _mm256_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n\n        for (int j = 0; j < vec_size; ++j) {\n            __m256d diff = _mm256_sub_pd(vec, _mm256_permutevar8x32_ps(vec, _mm256_setr_epi32(j * 4, j * 4 + 1, j * 4 + 2, j * 4 + 3, j * 4, j * 4 + 1, j * 4 + 2, j * 4 + 3)));\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n            __m256d min_diff = _mm256_min_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_setr_epi32(0, 1, 2, 3, 0, 1, 2, 3)));\n            if (_mm256_movemask_pd(_mm256_cmp_pd(min_diff, threshold_vec, _CMP_LT_OS)) != 0) {\n                return true;\n            }\n        }\n    }\n\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n        for (int j = 0; j < vec_size; ++j) {\n            __m256d vec2 = _mm256_permutevar8x32_ps(vec1, _mm256_setr_epi32(j, j+1, j+2, j+3, j+4, j+5, j+6, j+7));\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LE_OS);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n        for (int j = i + 1; j < i + vec_size; ++j) {\n            __m256d vec2 = _mm256_set1_pd(numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec1, vec2);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OS);\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        for (int j = 0; j < vec_size; ++j) {\n            __m256d diff = _mm256_sub_pd(vec, _mm256_permutevar8x32_ps(vec, _mm256_setr_epi32(0, 1, 2, 3, 0, 1, 2, 3)));\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n            __m256d min_diff = _mm256_min_pd(abs_diff, _mm256_permutevar8x32_ps(abs_diff, _mm256_setr_epi32(0, 1, 2, 3, 0, 1, 2, 3)));\n            if (_mm256_movemask_pd(_mm256_cmp_pd(min_diff, threshold_vec, _CMP_LT_OS)) != 0) {\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256d vec1 = _mm256_loadu_pd(&numbers[i]);\n        __m256d vec2 = _mm256_loadu_pd(&numbers[i + 4]);\n\n        __m256d diff1 = _mm256_sub_pd(vec1, vec2);\n        __m256d abs_diff1 = _mm256_andnot_pd(_mm256_cmp_pd(diff1, _mm256_setzero_pd(), _CMP_LE_OS), diff1);\n        __m256d min_diff1 = _mm256_min_pd(abs_diff1, _mm256_add_pd(abs_diff1, _mm256_permutevar8x32_ps(diff1, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7)))));\n\n        __m256d diff2 = _mm256_sub_pd(vec2, vec1);\n        __m256d abs_diff2 = _mm256_andnot_pd(_mm256_cmp_pd(diff2, _mm256_setzero_pd(), _CMP_LE_OS), diff2);\n        __m256d min_diff2 = _mm256_min_pd(abs_diff2, _mm256_add_pd(abs_diff2, _mm256_permutevar8x32_ps(diff2, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7)))));\n\n        __m256d min_diff = _mm256_min_pd(min_diff1, min_diff2);\n        if (_mm256_movemask_pd(_mm256_cmp_pd(min_diff, threshold_vec, _CMP_LT_OS))) {\n            return true;\n        }\n    }\n\n    for (int i = n - n % 8; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&data[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        balance = _mm256_add_epi32(balance, _mm256_set1_epi32(data[i]));\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i ops = _mm256_loadu_si256((__m256i*)&data[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(below_zero_mask) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 8; i < size; ++i) {\n        if (operations[i] > 0 && balance < 0) {\n            return true;\n        }\n        balance += operations[i];\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    size_t size = operations.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i below_zero_mask = _mm256_cmplt_epi32(balance, zero);\n        if (_mm256_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (balance.m256i_i32[0] + data[i] < 0) {\n            return true;\n        }\n        balance.m256i_i32[0] += data[i];\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        balance = _mm256_add_epi32(balance, _mm256_set1_epi32(data[i]));\n        __m256i below_zero_mask = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m256i balance = _mm256_setzero_si256();\n    __m256i threshold = _mm256_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i below_zero = _mm256_cmpgt_epi32(threshold, balance);\n        if (_mm256_movemask_epi8(below_zero)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 8; i < size; ++i) {\n        if (operations[i] > 0 && balance < 0) {\n            return true;\n        }\n        balance += operations[i];\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    double mean = total_sum / n;\n\n    __m256d diff_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        diff_vec = _mm256_add_pd(diff_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(sum, diff_vec);\n    double total_abs_diff = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return total_abs_diff / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    double mean = total_sum / n;\n\n    __m256d diff_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        diff_vec = _mm256_add_pd(diff_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(sum, diff_vec);\n    double total_abs_diff = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return total_abs_diff / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    double mean = total_sum / n;\n\n    __m256d diff_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        diff_vec = _mm256_add_pd(diff_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(sum, diff_vec);\n    double total_abs_diff = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return total_abs_diff / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    double mean = total_sum / n;\n\n    __m256d diff_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        diff_vec = _mm256_add_pd(diff_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(sum, diff_vec);\n    double total_abs_diff = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return total_abs_diff / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    double mean = total_sum / n;\n\n    __m256d diff_vec = _mm256_setzero_pd();\n    for (int i = 0; i < n; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_cmp_pd(diff, _mm256_setzero_pd(), _CMP_LE_OS), diff);\n        diff_vec = _mm256_add_pd(diff_vec, abs_diff);\n    }\n\n    _mm256_storeu_pd(sum, diff_vec);\n    double total_abs_diff = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return total_abs_diff / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    const int vec_size = 8; // AVX2 can process 8 integers at once\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimiter);\n    int i = 0;\n\n    while (i + vec_size <= numbers.size()) {\n        __m256i num_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        _mm256_storeu_si256((__m256i*)&result.back(), num_vec);\n        _mm256_storeu_si256((__m256i*)&result.back() + 8, delim_vec);\n        i += vec_size;\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result.push_back(numbers[i]);\n        if (i + 1 < numbers.size()) {\n            result.push_back(delimiter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    const size_t num_elements = numbers.size();\n    if (num_elements == 0) {\n        return {};\n    }\n\n    const size_t aligned_size = (num_elements * 2 - 1) & ~7;\n    std::vector<int> result(aligned_size);\n\n    __m256i v_delimiter = _mm256_set1_epi32(delimiter);\n    size_t i = 0;\n\n    for (; i + 7 <= num_elements; i += 8) {\n        __m256i v_numbers = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i v_result = _mm256_interleave_epi32(v_numbers, v_delimiter);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i * 2]), v_result);\n    }\n\n    for (; i < num_elements; ++i) {\n        result[i * 2] = numbers[i];\n        result[i * 2 + 1] = delimiter;\n    }\n\n    // Remove extra delimiter at the end\n    result.resize(num_elements * 2 - 1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    const size_t n = numbers.size();\n    if (n == 0) return {};\n\n    const size_t simd_width = 8; // AVX2 can process 8 integers at a time\n    const size_t aligned_size = (n + simd_width - 1) / simd_width * simd_width;\n    std::vector<int> result(aligned_size * 2 - 1);\n\n    __m256i v_delimiter = _mm256_set1_epi32(delimiter);\n    __m256i v_numbers;\n\n    size_t i = 0;\n    for (; i <= n - simd_width; i += simd_width) {\n        v_numbers = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i * 2]), v_numbers);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i * 2 + simd_width]), v_delimiter);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        result[i * 2 + 1] = delimiter;\n    }\n\n    // Remove trailing delimiter\n    result.pop_back();\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    const int vec_size = 8; // AVX2 can process 8 integers at a time\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimiter);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < numbers.size(); i += vec_size) {\n        size_t remaining = std::min(vec_size, numbers.size() - i);\n        __m256i num_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        if (remaining == vec_size) {\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data()), num_vec);\n            result.resize(result.size() + vec_size);\n        } else {\n            __m256i mask = _mm256_set1_epi32((1 << remaining) - 1);\n            num_vec = _mm256_and_si256(num_vec, mask);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data()), num_vec);\n            result.resize(result.size() + remaining);\n        }\n\n        if (i + remaining < numbers.size()) {\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + result.size()), delim_vec);\n            result.resize(result.size() + vec_size);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    const int vec_size = 8; // AVX2 can process 8 integers at a time\n    std::vector<int> result;\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m256i delim_vec = _mm256_set1_epi32(delimiter);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < numbers.size(); i += vec_size) {\n        size_t remaining = std::min(vec_size, numbers.size() - i);\n        __m256i num_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        if (remaining == vec_size) {\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result.back()), num_vec);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result.back() + vec_size), delim_vec);\n            result.resize(result.size() + 2 * vec_size);\n        } else {\n            __m256i mask = _mm256_set1_epi32(0xFFFFFFFF << (32 - remaining * 32));\n            num_vec = _mm256_and_si256(num_vec, mask);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result.back()), num_vec);\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result.back() + remaining), delim_vec);\n            result.resize(result.size() + remaining + remaining);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vproduct = _mm256_set1_epi32(1);\n\n        size_t i = 0;\n        for (; i + 15 < numbers.size(); i += 16) {\n            __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n            vsum = _mm256_add_epi32(vsum, vnums);\n            vproduct = _mm256_mullo_epi32(vproduct, vnums);\n        }\n\n        int32_t s[8];\n        int32_t p[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(s), vsum);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(p), vproduct);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += s[j];\n            product *= p[j];\n        }\n\n        for (; i < numbers.size(); ++i) {\n            sum += numbers[i];\n            product *= numbers[i];\n        }\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.size() % 8 == 0) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vproduct = _mm256_set1_epi32(1);\n\n        for (size_t i = 0; i < numbers.size(); i += 8) {\n            __m256i vnum = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n            vsum = _mm256_add_epi32(vsum, vnum);\n            vproduct = _mm256_mullo_epi32(vproduct, vnum);\n        }\n\n        int32_t s[8];\n        int32_t p[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(s), vsum);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(p), vproduct);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += s[j];\n            product *= p[j];\n        }\n    } else {\n        for (const auto& num : numbers) {\n            sum += num;\n            product *= num;\n        }\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi32(1);\n\n    size_t n = numbers.size();\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 7 < n; i += 8) {\n        __m256i vnumbers = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vsum = _mm256_add_epi32(vsum, vnumbers);\n        vproduct = _mm256_mullo_epi32(vproduct, vnumbers);\n    }\n\n    // Sum up the partial sums\n    int32_t s[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(s), vsum);\n    for (int j = 0; j < 8; ++j) {\n        sum += s[j];\n    }\n\n    // Multiply the partial products\n    int32_t p[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(p), vproduct);\n    for (int j = 0; j < 8; ++j) {\n        product *= p[j];\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n\n    for (int i = 0; i + 15 < n; i += 16) {\n        __m256i nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum_vec = _mm256_add_epi16(sum_vec, nums);\n        prod_vec = _mm256_mullo_epi16(prod_vec, nums);\n    }\n\n    int16_t sum[16];\n    int16_t prod[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum), sum_vec);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(prod), prod_vec);\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n\n    for (int i = 0; i < 16; ++i) {\n        total_sum += sum[i];\n        total_prod *= prod[i];\n    }\n\n    for (; i < n; ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vproduct = _mm256_set1_epi32(1);\n\n        size_t i = 0;\n        for (; i + 15 < numbers.size(); i += 16) {\n            __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n            vsum = _mm256_add_epi32(vsum, vnums);\n            vproduct = _mm256_mullo_epi32(vproduct, vnums);\n        }\n\n        int32_t s[8];\n        int32_t p[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(s), vsum);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(p), vproduct);\n\n        for (int j = 0; j < 8; ++j) {\n            sum += s[j];\n            product *= p[j];\n        }\n\n        for (; i < numbers.size(); ++i) {\n            sum += numbers[i];\n            product *= numbers[i];\n        }\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    result[0] = numbers[0];\n\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    const int vec_size = 8;\n\n    for (int i = 1; i + vec_size - 1 < n; i += vec_size) {\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n\n        for (int j = 0; j < vec_size; ++j) {\n            result[i + j] = _mm256_extract_epi32(max_vec, j);\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % vec_size; i < n; ++i) {\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0); // Extract the first element as it contains the max value\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m256i max_vec = _mm256_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m256i current_vec = _mm256_set1_epi32(numbers[i]);\n        max_vec = _mm256_max_epi32(max_vec, current_vec);\n        result[i] = _mm256_extract_epi32(max_vec, 0); // Extract the first element as it contains the max value\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i va = _mm256_loadu_si256(pa++);\n        __m256i vb = _mm256_loadu_si256(pb++);\n        __m256i vr = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i va = _mm256_loadu_si256(pa++);\n        __m256i vb = _mm256_loadu_si256(pb++);\n        __m256i vr = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m256i* pa = (__m256i*)a.data();\n    __m256i* pb = (__m256i*)b.data();\n    __m256i* pr = (__m256i*)result.data();\n\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i va = _mm256_loadu_si256(pa++);\n        __m256i vb = _mm256_loadu_si256(pb++);\n        __m256i vr = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m256i mask_a, mask_b, xor_result;\n    size_t i = 0;\n\n    while (i + 32 <= len) {\n        mask_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        mask_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        xor_result = _mm256_xor_si256(mask_a, mask_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), xor_result);\n        i += 32;\n    }\n\n    while (i < len) {\n        result[i] = a[i] ^ b[i];\n        ++i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m256i mask_a, mask_b, xor_result;\n    size_t i = 0;\n\n    while (i + 32 <= len) {\n        mask_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        mask_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        xor_result = _mm256_xor_si256(mask_a, mask_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), xor_result);\n        i += 32;\n    }\n\n    while (i < len) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n        ++i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* max_str = nullptr;\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_str = str.c_str();\n            max_len = str.length();\n        }\n    }\n\n    return max_str ? std::string(max_str, max_len) : \"\";\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* max_str = nullptr;\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_len = str.length();\n            max_str = str.c_str();\n        }\n    }\n\n    return max_str ? std::string(max_str, max_len) : \"\";\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* max_str = nullptr;\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_len = str.length();\n            max_str = str.c_str();\n        }\n    }\n\n    return max_str ? std::string(max_str, max_len) : \"\";\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const int max_len = 256;\n    __m256i max_len_vec = _mm256_set1_epi32(0);\n    __m256i index_vec = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i < strings.size(); ++i) {\n        const auto& str = strings[i];\n        if (str.length() > max_len) {\n            continue;\n        }\n\n        __m256i len_vec = _mm256_set1_epi32(static_cast<int>(str.length()));\n        __m256i cmp_res = _mm256_cmpgt_epi32(len_vec, max_len_vec);\n        max_len_vec = _mm256_blendv_epi32(max_len_vec, len_vec, cmp_res);\n        index_vec = _mm256_blendv_epi32(index_vec, _mm256_set1_epi32(i), cmp_res);\n    }\n\n    int max_index = -1;\n    _mm256_storeu_si256((__m256i*)&max_index, index_vec);\n\n    if (max_index == -1) {\n        return \"\";\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_and_si128(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_and_si128(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    const int n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m256 min_diff = _mm256_set1_ps(std::numeric_limits<float>::max());\n    __m256 closest_a = _mm256_setzero_ps();\n    __m256 closest_b = _mm256_setzero_ps();\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 vec_a = _mm256_loadu_ps(&numbers[i]);\n        __m256 vec_b = _mm256_loadu_ps(&numbers[i + 4]);\n\n        __m256 diff = _mm256_sub_ps(vec_a, vec_b);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, _mm256_setzero_ps(), _CMP_LE_OS), diff);\n\n        __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OS);\n        min_diff = _mm256_blendv_ps(min_diff, abs_diff, mask);\n        closest_a = _mm256_blendv_ps(closest_a, vec_a, mask);\n        closest_b = _mm256_blendv_ps(closest_b, vec_b, mask);\n    }\n\n    std::vector<float> closest_a_vec(8);\n    std::vector<float> closest_b_vec(8);\n    _mm256_storeu_ps(closest_a_vec.data(), closest_a);\n    _mm256_storeu_ps(closest_b_vec.data(), closest_b);\n\n    float min_diff_val = std::numeric_limits<float>::max();\n    float closest_a_val = 0.0f;\n    float closest_b_val = 0.0f;\n\n    for (int i = 0; i < 8; ++i) {\n        if (closest_a_vec[i] < closest_b_vec[i]) {\n            if (closest_b_vec[i] - closest_a_vec[i] < min_diff_val) {\n                min_diff_val = closest_b_vec[i] - closest_a_vec[i];\n                closest_a_val = closest_a_vec[i];\n                closest_b_val = closest_b_vec[i];\n            }\n        } else {\n            if (closest_a_vec[i] - closest_b_vec[i] < min_diff_val) {\n                min_diff_val = closest_a_vec[i] - closest_b_vec[i];\n                closest_a_val = closest_b_vec[i];\n                closest_b_val = closest_a_vec[i];\n            }\n        }\n    }\n\n    for (int i = n - 8; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff_val) {\n                min_diff_val = diff;\n                closest_a_val = std::min(numbers[i], numbers[j]);\n                closest_b_val = std::max(numbers[i], numbers[j]);\n            }\n        }\n    }\n\n    return {closest_a_val, closest_b_val};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m256 vec = _mm256_loadu_ps(numbers.data());\n    __m256 min_diff = _mm256_set1_ps(std::numeric_limits<float>::max());\n    __m256 closest_pair = _mm256_setzero_ps();\n\n    for (size_t i = 0; i <= numbers.size() - 8; i += 8) {\n        __m256 diff = _mm256_sub_ps(vec, _mm256_permutevar8x32_ps(vec, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0)));\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, _mm256_setzero_ps(), _CMP_LE_OS), diff);\n        __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OS);\n        min_diff = _mm256_blendv_ps(min_diff, abs_diff, mask);\n        closest_pair = _mm256_blendv_ps(closest_pair, _mm256_add_ps(vec, _mm256_permutevar8x32_ps(vec, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0))), mask);\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    std::pair<float, float> closest;\n    for (int i = 0; i < 8; i++) {\n        for (int j = i + 1; j < 8; j++) {\n            float diff = std::abs(result[i] - result[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n                closest = {std::min(result[i], result[j]), std::max(result[i], result[j])};\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % 8; i < numbers.size(); ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n                closest = {std::min(numbers[i], numbers[j]), std::max(numbers[i], numbers[j])};\n            }\n        }\n    }\n\n    return closest;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m256 min_diff = _mm256_set1_ps(std::numeric_limits<float>::max());\n    __m256 closest_pair = _mm256_setzero_ps();\n\n    for (size_t i = 0; i <= numbers.size() - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        __m256 vec2 = _mm256_loadu_ps(&numbers[i + 4]);\n\n        __m256 diff1 = _mm256_sub_ps(vec1, vec2);\n        __m256 abs_diff1 = _mm256_andnot_ps(_mm256_cmp_ps(diff1, _mm256_setzero_ps(), _CMP_LE_OS), diff1);\n\n        __m256 diff2 = _mm256_sub_ps(vec2, vec1);\n        __m256 abs_diff2 = _mm256_andnot_ps(_mm256_cmp_ps(diff2, _mm256_setzero_ps(), _CMP_LE_OS), diff2);\n\n        __m256 min_diff_new = _mm256_min_ps(min_diff, abs_diff1);\n        min_diff = _mm256_min_ps(min_diff_new, abs_diff2);\n\n        __m256 mask1 = _mm256_cmp_ps(abs_diff1, min_diff, _CMP_EQ_OQ);\n        __m256 mask2 = _mm256_cmp_ps(abs_diff2, min_diff, _CMP_EQ_OQ);\n\n        closest_pair = _mm256_blendv_ps(closest_pair, vec1, mask1);\n        closest_pair = _mm256_blendv_ps(closest_pair, vec2, mask2);\n    }\n\n    float closest_pair_arr[8];\n    _mm256_storeu_ps(closest_pair_arr, closest_pair);\n\n    float min_diff_val = std::numeric_limits<float>::max();\n    for (int i = 0; i < 8; ++i) {\n        for (int j = i + 1; j < 8; ++j) {\n            float diff = std::abs(closest_pair_arr[i] - closest_pair_arr[j]);\n            if (diff < min_diff_val) {\n                min_diff_val = diff;\n            }\n        }\n    }\n\n    for (int i = 0; i < 7; ++i) {\n        for (int j = i + 1; j < 8; ++j) {\n            if (closest_pair_arr[i] == closest_pair_arr[j]) {\n                return {std::min(closest_pair_arr[i], closest_pair_arr[j]), std::max(closest_pair_arr[i], closest_pair_arr[j])};\n            }\n        }\n    }\n\n    return {std::min(closest_pair_arr[0], closest_pair_arr[1]), std::max(closest_pair_arr[0], closest_pair_arr[1])};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m256 min_diff = _mm256_set1_ps(std::numeric_limits<float>::max());\n    __m256 closest_pair = _mm256_setzero_ps();\n\n    int n = numbers.size();\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        for (int j = i + 1; j <= n - 8; j += 8) {\n            __m256 other_vec = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec, other_vec);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, _mm256_setzero_ps(), _CMP_LE_OS), diff);\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OS);\n            min_diff = _mm256_blendv_ps(min_diff, abs_diff, mask);\n            closest_pair = _mm256_blendv_ps(closest_pair, _mm256_min_ps(vec, other_vec), mask);\n            closest_pair = _mm256_blendv_ps(closest_pair, _mm256_max_ps(vec, other_vec), _mm256_and_ps(mask, _mm256_cmp_ps(vec, other_vec, _CMP_GT_OS)));\n        }\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    for (int i = 0; i < 8; ++i) {\n        for (int j = i + 1; j < 8; ++j) {\n            float diff = std::abs(result[i] - result[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n            }\n        }\n    }\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m256 min_diff = _mm256_set1_ps(std::numeric_limits<float>::max());\n    __m256 closest_pair = _mm256_setzero_ps();\n\n    const int n = static_cast<int>(numbers.size());\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n\n        for (int j = i + 1; j <= n - 8; j += 8) {\n            __m256 other_vec = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec, other_vec);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_cmp_ps(diff, _mm256_setzero_ps(), _CMP_LE_OS), diff);\n\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OS);\n            min_diff = _mm256_blendv_ps(min_diff, abs_diff, mask);\n            closest_pair = _mm256_blendv_ps(closest_pair, _mm256_min_ps(vec, other_vec), mask);\n            closest_pair = _mm256_blendv_ps(closest_pair, _mm256_max_ps(vec, other_vec), _mm256_and_ps(mask, _mm256_cmp_ps(vec, other_vec, _CMP_GT_OS)));\n        }\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    for (int i = 0; i < 8; ++i) {\n        for (int j = i + 1; j < 8; ++j) {\n            float diff = std::abs(result[i] - result[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n            }\n        }\n    }\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    // Find min and max values\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    std::vector<float> result(n);\n    const __m256 scale = _mm256_set1_ps(1.0f / (max_val - min_val));\n    const __m256 offset = _mm256_set1_ps(-min_val);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_add_ps(vec, offset);\n        vec = _mm256_mul_ps(vec, scale);\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    // Find min and max values\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    std::vector<float> result(n);\n    const float range = max_val - min_val;\n\n    __m256 v_range = _mm256_set1_ps(range);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256 v_min = _mm256_set1_ps(min_val);\n        __m256 v_numbers = _mm256_loadu_ps(&numbers[i]);\n\n        // Subtract min value\n        v_numbers = _mm256_sub_ps(v_numbers, v_min);\n\n        // Divide by range\n        v_numbers = _mm256_div_ps(v_numbers, v_range);\n\n        // Store results\n        _mm256_storeu_ps(&result[i], v_numbers);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    __m256 min_val = _mm256_set1_ps(FLT_MAX);\n    __m256 max_val = _mm256_set1_ps(-FLT_MAX);\n\n    // Find min and max values in parallel\n    for(int i = 0; i <= n - 8; i += 8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float min_val_scalar = _mm256_cvtss_f32(_mm256_min_ss(min_val, _mm256_permutevar8x32_ps(min_val, _mm256_set_epi32(0, 1, 2, 3))));\n    float max_val_scalar = _mm256_cvtss_f32(_mm256_max_ss(max_val, _mm256_permutevar8x32_ps(max_val, _mm256_set_epi32(0, 1, 2, 3))));\n\n    if(n % 8 != 0){\n        for(int i = n - n % 8; i < n; ++i){\n            if(numbers[i] < min_val_scalar) min_val_scalar = numbers[i];\n            if(numbers[i] > max_val_scalar) max_val_scalar = numbers[i];\n        }\n    }\n\n    float range = max_val_scalar - min_val_scalar;\n\n    // Rescale values in parallel\n    for(int i = 0; i <= n - 8; i += 8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        vec = _mm256_sub_ps(vec, _mm256_set1_ps(min_val_scalar));\n        vec = _mm256_div_ps(vec, _mm256_set1_ps(range));\n        _mm256_storeu_ps(&result[i], vec);\n    }\n\n    if(n % 8 != 0){\n        for(int i = n - n % 8; i < n; ++i){\n            result[i] = (numbers[i] - min_val_scalar) / range;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    // Find min and max values\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    std::vector<float> result(n);\n    const int vec_size = 8; // AVX2 can process 8 floats at once\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n\n    for (int i = 0; i < n - vec_size + 1; i += vec_size) {\n        __m256 input_vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 diff_vec = _mm256_sub_ps(input_vec, min_vec);\n        __m256 scale_vec = _mm256_div_ps(diff_vec, _mm256_sub_ps(max_vec, min_vec));\n        _mm256_storeu_ps(&result[i], scale_vec);\n    }\n\n    // Handle remaining elements\n    for (int i = n - vec_size + 1; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    // Find min and max values\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    std::vector<float> result(n);\n    const float range = max_val - min_val;\n\n    __m256 v_range = _mm256_set1_ps(range);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256 v_min = _mm256_set1_ps(min_val);\n        __m256 v_numbers = _mm256_loadu_ps(&numbers[i]);\n\n        __m256 v_diff = _mm256_sub_ps(v_numbers, v_min);\n        __m256 v_scaled = _mm256_div_ps(v_diff, v_range);\n\n        _mm256_storeu_ps(&result[i], v_scaled);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    \n    for (int i = 0; i < 32; ++i) {\n        __m256i v_i = _mm256_set1_epi64x(i + 1);\n        __m256i v_div = _mm256_div_epi64(v_n, v_i);\n        __m256i v_mul = _mm256_mul_epi64(v_div, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mul, v_n);\n        \n        if (_mm256_movemask_epi8(v_cmp) != 0) {\n            return i;\n        }\n    }\n    \n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    \n    for (int i = 0; i < 4; ++i) {\n        __m256i v_divisors = _mm256_sub_epi64(v_n, v_one);\n        __m256i v_mask = _mm256_cmpgt_epi64(v_divisors, v_zero);\n        v_divisors = _mm256_and_si256(v_divisors, v_mask);\n        \n        __m256i v_remainders = _mm256_rem_epi64(v_n, v_divisors);\n        v_mask = _mm256_cmpeq_epi64(v_remainders, v_zero);\n        \n        v_divisors = _mm256_and_si256(v_divisors, v_mask);\n        \n        v_n = v_divisors;\n    }\n    \n    return _mm256_extract_epi64(v_n, 0);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    \n    for (int i = 0; i < 32; ++i) {\n        __m256i v_i = _mm256_set1_epi64x(i + 1);\n        __m256i v_div = _mm256_div_epi64(v_n, v_i);\n        __m256i v_mul = _mm256_mul_epi64(v_div, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mul, v_n);\n        \n        if (_mm256_movemask_epi8(v_cmp) != 0) {\n            return i;\n        }\n    }\n    \n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_max = v_zero;\n\n    for (int i = 0; i < 32; ++i) {\n        __m256i v_i = _mm256_set1_epi64x(i + 1);\n        __m256i v_div = _mm256_div_epi64(v_n, v_i);\n        __m256i v_mul = _mm256_mul_epi64(v_div, v_i);\n        __m256i v_eq = _mm256_cmpeq_epi64(v_mul, v_n);\n        __m256i v_mask = _mm256_and_si256(v_eq, v_i);\n        v_max = _mm256_or_si256(v_max, v_mask);\n    }\n\n    uint64_t max_val;\n    _mm256_storeu_si256((__m256i*)&max_val, v_max);\n\n    return max_val - 1;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    \n    for (int i = 0; i < 32; ++i) {\n        __m256i v_i = _mm256_set1_epi64x(i + 1);\n        __m256i v_div = _mm256_div_epi64(v_n, v_i);\n        __m256i v_mul = _mm256_mul_epi64(v_div, v_i);\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mul, v_n);\n        \n        if (_mm256_movemask_epi8(v_cmp)) {\n            return i + 1;\n        }\n    }\n    \n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 4, 5);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_n, v_i)) != 0) {\n        __m256i v_div = _mm256_and_si256(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi32(v_div, v_i);\n        __m256i v_factors = _mm256_blendv_epi32(v_i, v_zero, v_mask);\n        factors.insert(factors.end(), 4, _mm256_extract_epi32(v_factors, 0));\n        v_n = _mm256_sub_epi32(v_n, v_factors);\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(1));\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 4, 5);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_n, v_i)) != 0) {\n        __m256i v_div = _mm256_and_si256(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi32(v_div, v_i);\n        __m256i v_factors = _mm256_blendv_epi32(v_i, v_factors, v_mask);\n        v_n = _mm256_sub_epi32(v_n, v_div);\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(1));\n    }\n\n    // Handle remaining factors\n    int i = 2;\n    while (n > 1) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 4, 5);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_n, v_i)) != 0) {\n        __m256i v_div = _mm256_and_si256(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi32(v_div, v_i);\n        __m256i v_factors = _mm256_blendv_epi8(v_i, v_zero, v_mask);\n        factors.insert(factors.end(), 4, _mm256_extract_epi32(v_factors, 0));\n        v_n = _mm256_sub_epi32(v_n, v_div);\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(1));\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 4, 5);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_n, v_i)) != 0) {\n        __m256i v_div = _mm256_and_si256(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi32(v_div, v_i);\n        __m256i v_factors = _mm256_blendv_epi32(v_i, v_factors, v_mask);\n        v_n = _mm256_sub_epi32(v_n, v_div);\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(1));\n    }\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_factors);\n    for (int i = 0; i < 8; ++i) {\n        if (result[i] != 0) {\n            factors.push_back(result[i]);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_i = _mm256_setr_epi32(2, 3, 4, 5);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_n, v_i)) != 0) {\n        __m256i v_div = _mm256_and_si256(v_n, v_i);\n        __m256i v_mask = _mm256_cmpeq_epi32(v_div, v_i);\n        __m256i v_factors = _mm256_blendv_epi8(v_i, v_i, v_mask);\n        factors.insert(factors.end(), 4, _mm256_extract_epi32(v_factors, 0));\n        v_n = _mm256_sub_epi32(v_n, v_div);\n        v_i = _mm256_add_epi32(v_i, _mm256_set1_epi32(1));\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int vec_size = 8; // AVX2 processes 8 floats at a time\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < l.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OS);\n        vec = _mm256_and_ps(vec, mask);\n        float temp[vec_size];\n        _mm256_storeu_ps(temp, vec);\n        for (int j = 0; j < vec_size; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int vec_size = 8; // AVX2 processes 8 floats at a time\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < l.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OS);\n        __m256 pos_vec = _mm256_and_ps(vec, mask);\n        float temp[vec_size];\n        _mm256_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < vec_size; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int vec_size = 8; // AVX2 processes 8 floats at a time\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < l.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OS);\n        __m256 pos_vec = _mm256_and_ps(vec, mask);\n        float temp[vec_size];\n        _mm256_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < vec_size && i + j < l.size(); ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int vec_size = 8; // AVX2 processes 8 floats at a time\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < l.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OS);\n        vec = _mm256_and_ps(vec, mask);\n        float temp[vec_size];\n        _mm256_storeu_ps(temp, vec);\n        for (int j = 0; j < vec_size && i + j < l.size(); ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int vec_size = 8; // AVX2 processes 8 floats at a time\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < l.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OS);\n        vec = _mm256_and_ps(vec, mask);\n        float temp[vec_size];\n        _mm256_storeu_ps(temp, vec);\n        for (int j = 0; j < vec_size && i + j < l.size(); ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i v = _mm256_setr_epi64x(5, 7, 11, 13);\n    __m256i mask = _mm256_setzero_si256();\n    int count = 0;\n\n    while (_mm256_movemask_epi8(mask) != 0xFF) {\n        __m256i divisors = _mm256_add_epi64(v, _mm256_set1_epi64x(6));\n        __m256i remainders = _mm256_rem_epi64(n, divisors);\n        mask = _mm256_cmpgt_epi64(_mm256_setzero_si256(), remainders);\n        v = _mm256_add_epi64(v, _mm256_set1_epi64x(6));\n        count++;\n    }\n\n    return count > 0 && n % count == 0;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_i = _mm256_setr_epi64x(5, 7, 9, 11);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_divisor = _mm256_set1_epi64x(v_i[i]);\n        __m256i v_remainder = _mm256_rem_epi64(v_n, v_divisor);\n        __m256i v_zero = _mm256_setzero_si256();\n        if (_mm256_testz_si256(v_remainder, v_remainder)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = static_cast<int64_t>(sqrt(n));\n    __m256i v_limit = _mm256_set1_epi64x(limit);\n    __m256i v_step = _mm256_set1_epi64x(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i v_i = _mm256_set1_epi64x(i);\n        __m256i v_i_plus_2 = _mm256_add_epi64(v_i, _mm256_set1_epi64x(2));\n\n        __m256i v_n_div_i = _mm256_div_epi64(_mm256_set1_epi64x(n), v_i);\n        __m256i v_n_mod_i = _mm256_sub_epi64(_mm256_set1_epi64x(n), _mm256_mul_epi64(v_n_div_i, v_i));\n\n        __m256i v_n_div_i_plus_2 = _mm256_div_epi64(_mm256_set1_epi64x(n), v_i_plus_2);\n        __m256i v_n_mod_i_plus_2 = _mm256_sub_epi64(_mm256_set1_epi64x(n), _mm256_mul_epi64(v_n_div_i_plus_2, v_i_plus_2));\n\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_one = _mm256_set1_epi64x(1);\n\n        __m256i v_condition_i = _mm256_cmpgt_epi64(v_n_mod_i, v_zero);\n        __m256i v_condition_i_plus_2 = _mm256_cmpgt_epi64(v_n_mod_i_plus_2, v_zero);\n\n        __m256i v_result_i = _mm256_and_si256(v_condition_i, v_one);\n        __m256i v_result_i_plus_2 = _mm256_and_si256(v_condition_i_plus_2, v_one);\n\n        __m256i v_final_result = _mm256_or_si256(v_result_i, v_result_i_plus_2);\n\n        if (_mm256_movemask_epi8(v_final_result) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = static_cast<int64_t>(sqrt(n));\n    __m256i v_limit = _mm256_set1_epi64x(limit);\n    __m256i v_step = _mm256_set1_epi64x(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i v_i = _mm256_set1_epi64x(i);\n        __m256i v_i_plus_2 = _mm256_add_epi64(v_i, _mm256_set1_epi64x(2));\n\n        __m256i v_n_div_i = _mm256_div_epi64(_mm256_set1_epi64x(n), v_i);\n        __m256i v_n_mod_i = _mm256_sub_epi64(_mm256_set1_epi64x(n), _mm256_mul_epi64(v_n_div_i, v_i));\n\n        __m256i v_n_div_i_plus_2 = _mm256_div_epi64(_mm256_set1_epi64x(n), v_i_plus_2);\n        __m256i v_n_mod_i_plus_2 = _mm256_sub_epi64(_mm256_set1_epi64x(n), _mm256_mul_epi64(v_n_div_i_plus_2, v_i_plus_2));\n\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_one = _mm256_set1_epi64x(1);\n\n        __m256i v_condition_i = _mm256_cmpgt_epi64(v_n_mod_i, v_zero);\n        __m256i v_condition_i_plus_2 = _mm256_cmpgt_epi64(v_n_mod_i_plus_2, v_zero);\n\n        __m256i v_result_i = _mm256_and_si256(v_condition_i, v_one);\n        __m256i v_result_i_plus_2 = _mm256_and_si256(v_condition_i_plus_2, v_one);\n\n        __m256i v_result = _mm256_or_si256(v_result_i, v_result_i_plus_2);\n\n        if (_mm256_movemask_epi8(v_result) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m256i v = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_setr_epi64x(5, 7, 11, 13);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i remainder = _mm256_rem_epi64(v, divisors);\n        __m256i zero = _mm256_setzero_si256();\n        if (_mm256_testz_si256(remainder, remainder)) {\n            return false;\n        }\n        divisors = _mm256_add_epi64(divisors, _mm256_set1_epi64x(6));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const size_t len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i is_alpha = _mm256_and_si256(_mm256_cmpgt_epi8(data, zero), _mm256_cmplt_epi8(data, _mm256_set1_epi8('9')));\n        __m256i is_upper = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(data, _mm256_set1_epi8('Z')));\n        __m256i is_lower = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), data));\n        __m256i upper_to_lower = _mm256_sub_epi8(data, _mm256_set1_epi8('A'));\n        __m256i lower_to_upper = _mm256_add_epi8(data, _mm256_set1_epi8('A' - 'a'));\n\n        __m256i swapped = _mm256_or_si256(\n            _mm256_andnot_si256(is_upper, lower_to_upper),\n            _mm256_andnot_si256(is_lower, upper_to_lower)\n        );\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), swapped);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            if (c >= 'a' && c <= 'z') {\n                c -= ('a' - 'A');\n            } else {\n                c += ('a' - 'A');\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    const char* input = s.c_str();\n    size_t length = s.length();\n    std::string result(length, '\\0');\n\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 31 < length; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(input + i));\n\n        // Check if the character is a letter\n        __m256i is_letter = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8('9')), _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), data));\n        is_letter = _mm256_or_si256(is_letter, _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8('z')), _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), data)));\n\n        // Reverse case for letters\n        __m256i reversed_case = _mm256_sub_epi8(_mm256_add_epi8(data, _mm256_set1_epi8(' ')), _mm256_and_si256(is_letter, _mm256_set1_epi8(' ')));\n\n        // Combine results\n        __m256i combined = _mm256_blendv_epi8(reversed_case, data, is_letter);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), combined);\n    }\n\n    // Handle remaining characters\n    for (size_t i = length - length % 32; i < length; ++i) {\n        if ((input[i] >= 'a' && input[i] <= 'z') || (input[i] >= 'A' && input[i] <= 'Z')) {\n            result[i] = isupper(input[i]) ? tolower(input[i]) : toupper(input[i]);\n        } else {\n            result[i] = input[i];\n        }\n    }\n\n    // Reverse the string if no letters were found\n    if (length > 0 && result[0] == '\\0') {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const int len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Check if the character is a letter\n        __m256i is_letter = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8(' ')), _mm256_cmplt_epi8(data, _mm256_set1_epi8('z' + 1)));\n\n        // Reverse case for letters\n        __m256i reversed_case = _mm256_sub_epi8(_mm256_add_epi8(data, _mm256_set1_epi8('a' - 'A')), _mm256_and_si256(is_letter, _mm256_set1_epi8('a' - 'A')));\n\n        // Combine results\n        __m256i final_data = _mm256_or_si256(reversed_case, _mm256_andnot_si256(is_letter, data));\n\n        // Store result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_data);\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            c = islower(c) ? toupper(c) : tolower(c);\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const size_t len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i is_alpha = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8('9')), _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), data));\n        __m256i is_lower = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(data, _mm256_set1_epi8('z')));\n        __m256i is_upper = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), data));\n\n        __m256i lower_to_upper = _mm256_sub_epi8(data, _mm256_set1_epi8('a'));\n        lower_to_upper = _mm256_add_epi8(lower_to_upper, _mm256_set1_epi8('A'));\n\n        __m256i upper_to_lower = _mm256_sub_epi8(data, _mm256_set1_epi8('A'));\n        upper_to_lower = _mm256_add_epi8(upper_to_lower, _mm256_set1_epi8('a'));\n\n        __m256i swapped = _mm256_or_si256(\n            _mm256_andnot_si256(is_lower, lower_to_upper),\n            _mm256_andnot_si256(is_upper, upper_to_lower)\n        );\n\n        swapped = _mm256_or_si256(swapped, _mm256_and_si256(is_alpha, data));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), swapped);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            if (c >= 'a' && c <= 'z') {\n                c -= 'a';\n                c += 'A';\n            } else {\n                c -= 'A';\n                c += 'a';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    const size_t len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i is_alpha = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8('9')), _mm256_cmpgt_epi8(data, _mm256_set1_epi8('Z')));\n        __m256i is_lower = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(data, _mm256_set1_epi8('a')));\n        __m256i is_upper = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(data, _mm256_set1_epi8('A')));\n\n        __m256i lower_to_upper = _mm256_sub_epi8(data, _mm256_set1_epi8('a'));\n        __m256i upper_to_lower = _mm256_add_epi8(data, _mm256_set1_epi8('a' - 'A'));\n\n        __m256i swapped = _mm256_or_si256(\n            _mm256_andnot_si256(is_lower, _mm256_andnot_si256(is_upper, data)),\n            _mm256_or_si256(\n                _mm256_and_si256(is_lower, upper_to_lower),\n                _mm256_and_si256(is_upper, lower_to_upper)\n            )\n        );\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), swapped);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            if (c >= 'a' && c <= 'z') {\n                result[i] = c - ('a' - 'A');\n            } else {\n                result[i] = c + ('a' - 'A');\n            }\n        } else {\n            result[i] = c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i v0, v1, v2;\n    int i = 0;\n\n    // Process elements in groups of 8\n    for (; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&l[i + 0]);\n        v1 = _mm256_loadu_si256((__m256i*)&l[i + 4]);\n\n        // Sort every third element\n        v2 = _mm256_setzero_si256();\n        v2 = _mm256_insert_epi32(v2, l[i + 0], 0);\n        v2 = _mm256_insert_epi32(v2, l[i + 3], 1);\n        v2 = _mm256_insert_epi32(v2, l[i + 6], 2);\n        v2 = _mm256_sort_epi32(v2, _MM_SHUFFLE(2, 1, 0, 3));\n\n        _mm256_storeu_si256((__m256i*)&result[i + 0], v0);\n        _mm256_storeu_si256((__m256i*)&result[i + 4], v1);\n        _mm256_storeu_si256((__m256i*)&result[i + 0], v2);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            std::sort(&l[i], &l[i] + 3);\n        }\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i v0, v1, v2, v3;\n    int i;\n\n    // Process elements in groups of 8\n    for (i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&l[i + 4]);\n\n        // Sort every third element\n        v2 = _mm256_permutevar8x32_epi32(v0, v1);\n        v3 = _mm256_sort_epi32(v2, _MM_SHUFFLE(3, 2, 1, 0));\n\n        _mm256_storeu_si256((__m256i*)&result[i], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            std::sort(l.begin() + i, l.begin() + std::min(i + 3, n));\n        }\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i v0, v1, v2, v3, v4, v5, v6, v7;\n    __m256i temp;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&l[i + 0]);\n        v1 = _mm256_loadu_si256((__m256i*)&l[i + 4]);\n\n        // Sort elements at indices divisible by three\n        v2 = _mm256_and_si256(v0, _mm256_set1_epi32(0xFFFFFFFE));\n        v3 = _mm256_and_si256(v1, _mm256_set1_epi32(0xFFFFFFFE));\n\n        v4 = _mm256_min_epi32(v2, v3);\n        v5 = _mm256_max_epi32(v2, v3);\n\n        v6 = _mm256_and_si256(v0, _mm256_set1_epi32(0xFFFFFFFF));\n        v7 = _mm256_and_si256(v1, _mm256_set1_epi32(0xFFFFFFFF));\n\n        v6 = _mm256_or_si256(v6, _mm256_slli_epi32(v4, 1));\n        v7 = _mm256_or_si256(v7, _mm256_slli_epi32(v5, 1));\n\n        _mm256_storeu_si256((__m256i*)&result[i + 0], v6);\n        _mm256_storeu_si256((__m256i*)&result[i + 4], v7);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        if (i % 3 == 0) {\n            std::sort(&l[i], &l[i] + 1);\n        }\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i v0, v1, v2, v3, v4, v5;\n    int i = 0;\n\n    // Process elements in groups of 6 (3 pairs)\n    for (; i <= n - 6; i += 6) {\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&l[i + 3]);\n\n        // Sort each pair\n        v2 = _mm256_min_epi32(v0, v1);\n        v3 = _mm256_max_epi32(v0, v1);\n\n        // Store the sorted pairs back into the result array\n        _mm256_storeu_si256((__m256i*)&result[i], v2);\n        _mm256_storeu_si256((__m256i*)&result[i + 3], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i v0, v1, v2, v3;\n    int i;\n\n    for (i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&l[i + 4]);\n\n        // Sort every third element\n        v2 = _mm256_permutevar8x32_epi32(v0, v1);\n        v3 = _mm256_sort_epi32(v2, _MM_SHUFFLE(3, 2, 1, 0));\n\n        _mm256_storeu_si256((__m256i*)&result[i], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            std::sort(l.begin() + i, l.begin() + std::min(i + 3, n));\n        }\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& vec) {\n    if (vec.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m256 max_vec = _mm256_set1_ps(-std::numeric_limits<float>::max());\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 7 < vec.size(); i += 8) {\n        __m256 v = _mm256_loadu_ps(&vec[i]);\n        max_vec = _mm256_max_ps(max_vec, v);\n    }\n\n    // Handle remaining elements\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n    float local_max = *std::max_element(temp, temp + 8);\n\n    // Compare with remaining elements that didn't fit into chunks of 8\n    for (; i < vec.size(); ++i) {\n        local_max = std::max(local_max, vec[i]);\n    }\n\n    return local_max;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    __m256 v = _mm256_setzero_ps();\n    int n = l.size();\n    int i;\n\n    // Process elements in blocks of 8\n    for (i = 0; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        v = _mm256_max_ps(v, vec);\n    }\n\n    // Handle remaining elements\n    float temp[8];\n    _mm256_storeu_ps(temp, v);\n    float max_val = temp[0];\n    for (int j = 1; j < 8; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    // Process single elements\n    for (; i < n; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m256 max_vec = _mm256_set1_ps(l[0]);\n    size_t i = 4;\n\n    for (; i <= l.size() - 4; i += 4) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float max_values[8];\n    _mm256_storeu_ps(max_values, max_vec);\n\n    float max_val = max_values[0];\n    for (int j = 1; j < 8; ++j) {\n        max_val = std::max(max_val, max_values[j]);\n    }\n\n    for (; i < l.size(); ++i) {\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m256 max_vec = _mm256_set1_ps(l[0]);\n    size_t i = 4;\n\n    while (i + 7 < l.size()) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n        i += 8;\n    }\n\n    float max_values[8];\n    _mm256_storeu_ps(max_values, max_vec);\n\n    float local_max = max_values[0];\n    for (size_t j = 1; j < 8; ++j) {\n        local_max = std::max(local_max, max_values[j]);\n    }\n\n    for (; i < l.size(); ++i) {\n        local_max = std::max(local_max, l[i]);\n    }\n\n    return local_max;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& vec) {\n    if (vec.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m256 max_vec = _mm256_set1_ps(-FLT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 7 < vec.size(); i += 8) {\n        __m256 v = _mm256_loadu_ps(&vec[i]);\n        max_vec = _mm256_max_ps(max_vec, v);\n    }\n\n    // Process remaining elements\n    float max_temp[8];\n    _mm256_storeu_ps(max_temp, max_vec);\n\n    float max_val = max_temp[0];\n    for (size_t j = 1; j < 8; ++j) {\n        max_val = std::max(max_val, max_temp[j]);\n    }\n\n    // Process single elements\n    for (; i < vec.size(); ++i) {\n        max_val = std::max(max_val, vec[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i vseven = _mm256_set1_epi32('7');\n    __m256i vmul11 = _mm256_set1_epi32(11);\n    __m256i vmul13 = _mm256_set1_epi32(13);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i vmultest11 = _mm256_mullo_epi32(vi, vmul11);\n        __m256i vmultest13 = _mm256_mullo_epi32(vi, vmul13);\n        __m256i vmask11 = _mm256_cmpeq_epi32(vmultest11, vi);\n        __m256i vmask13 = _mm256_cmpeq_epi32(vmultest13, vi);\n        __m256i vmask = _mm256_or_si256(vmask11, vmask13);\n        __m256i vcount = _mm256_popcnt_epi32(vmask);\n        count += _mm256_reduce_add_epi32(vcount);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i v7 = _mm256_set1_epi32(7);\n    __m256i vmask = _mm256_set1_epi32(0x0F0F0F0F); // Mask to extract lower 4 bits\n    for (int i = 0; i < n; i += 8) {\n        __m256i vi = _mm256_loadu_si256((__m256i*)&i);\n        __m256i vmod11 = _mm256_and_si256(vi, vmask);\n        __m256i vmod13 = _mm256_add_epi32(vmod11, _mm256_set1_epi32(1));\n        __m256i vand11 = _mm256_and_si256(vmod11, v7);\n        __m256i vand13 = _mm256_and_si256(vmod13, v7);\n        __m256i vsum = _mm256_add_epi32(vand11, vand13);\n        __m256i vcount = _mm256_popcnt_epi32(vsum);\n        int c[8];\n        _mm256_storeu_si256((__m256i*)c, vcount);\n        count += c[0] + c[1] + c[2] + c[3] + c[4] + c[5] + c[6] + c[7];\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i vseven = _mm256_set1_epi32('7');\n    __m256i vmask = _mm256_set1_epi32(0x3F3F3F3F);\n    __m256i vzero = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vnum = _mm256_loadu_si256((__m256i*)&i);\n        __m256i vmod11 = _mm256_and_si256(vnum, _mm256_set1_epi32(0x000000FF));\n        __m256i vmod13 = _mm256_and_si256(vnum, _mm256_set1_epi32(0x000000FF));\n        vmod11 = _mm256_add_epi32(vmod11, _mm256_set1_epi32(11));\n        vmod13 = _mm256_add_epi32(vmod13, _mm256_set1_epi32(13));\n        vmod11 = _mm256_sub_epi32(vmod11, _mm256_mullo_epi32(vmod11, _mm256_set1_epi32(11)));\n        vmod13 = _mm256_sub_epi32(vmod13, _mm256_mullo_epi32(vmod13, _mm256_set1_epi32(13)));\n        vmod11 = _mm256_and_si256(vmod11, _mm256_set1_epi32(0x000000FF));\n        vmod13 = _mm256_and_si256(vmod13, _mm256_set1_epi32(0x000000FF));\n        __m256i vand = _mm256_and_si256(_mm256_or_si256(vmod11, vmod13), _mm256_cmpgt_epi32(vmod11, vzero));\n        __m256i vcount = _mm256_popcnt_epi32(vand);\n        count += _mm256_reduce_add_epi32(vcount);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i vseven = _mm256_set1_epi32('7');\n    __m256i vmask = _mm256_set1_epi32(0x0F);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i vnum = _mm256_loadu_si256((__m256i*)&i);\n        __m256i vdiv11 = _mm256_and_si256(vnum, _mm256_set1_epi32(0x0B));\n        __m256i vdiv13 = _mm256_and_si256(vnum, _mm256_set1_epi32(0x0D));\n        __m256i vand = _mm256_or_si256(vdiv11, vdiv13);\n        __m256i veq = _mm256_cmpeq_epi32(vand, vzero);\n        __m256i vcount = _mm256_add_epi32(_mm256_and_si256(veq, vone), vcount);\n\n        __m256i vstr = _mm256_i32gather_epi32((const int*)i, vnum, 4);\n        __m256i vfind = _mm256_cmpeq_epi8(vstr, vseven);\n        __m256i vsum = _mm256_movemask_epi8(vfind);\n        count += __builtin_popcount(vsum);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i vseven = _mm256_set1_epi8('7');\n    __m256i vmul11 = _mm256_set1_epi8('1' * 11);\n    __m256i vmul13 = _mm256_set1_epi8('1' * 13);\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i vnum = _mm256_loadu_si256((__m256i*)&i);\n        __m256i vmult11 = _mm256_mullo_epi8(vnum, vmul11);\n        __m256i vmult13 = _mm256_mullo_epi8(vnum, vmul13);\n        __m256i vsum = _mm256_add_epi8(vmult11, vmult13);\n        __m256i vcmp = _mm256_cmpeq_epi8(vsum, vseven);\n        int mask = _mm256_movemask_epi8(vcmp);\n        count += __builtin_popcount(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Vector size must be even\");\n    }\n\n    __m256* vec = reinterpret_cast<__m256*>(l.data());\n    int num_elements = n / 4;\n\n    // Extract even elements into an array\n    float even_elements[8];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i * 2] = _mm256_cvtss_f32(_mm256_extractf128_ps(vec[i], 0));\n        even_elements[i * 2 + 1] = _mm256_cvtss_f32(_mm256_extractf128_ps(vec[i], 1));\n    }\n\n    // Sort the even elements\n    std::sort(even_elements, even_elements + 8);\n\n    // Place the sorted even elements back into the vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm256_set_ps(even_elements[i * 2 + 1], even_elements[i * 2], 0, 0);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    std::vector<float> result(n);\n\n    __m256 v0, v1, v2, v3;\n    int i;\n\n    // Process elements in groups of 8\n    for (i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_ps(&l[i]);\n        v1 = _mm256_permutevar8x32_ps(v0, _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        v0 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(3, 1, 0, 2, 3, 1, 0, 2));\n        v1 = _mm256_permutevar8x32_ps(v3, _mm256_set_epi32(3, 1, 0, 2, 3, 1, 0, 2));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        v0 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(1, 0, 2, 3, 1, 0, 2, 3));\n        v1 = _mm256_permutevar8x32_ps(v3, _mm256_set_epi32(1, 0, 2, 3, 1, 0, 2, 3));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        _mm256_storeu_ps(&result[i], v2);\n        _mm256_storeu_ps(&result[i + 4], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Input size must be even\");\n    }\n\n    __m256* vec_l = reinterpret_cast<__m256*>(l.data());\n    int num_elements = n / 4;\n\n    // Extract even elements into an array\n    float even_elements[8];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i * 2] = _mm256_cvtss_f32(_mm256_extractf128_ps(vec_l[i], 0));\n        even_elements[i * 2 + 1] = _mm256_cvtss_f32(_mm256_extractf128_ps(vec_l[i], 1));\n    }\n\n    // Sort the even elements\n    std::sort(even_elements, even_elements + 8);\n\n    // Insert sorted even elements back into the original vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec_l[i] = _mm256_set_ps(even_elements[i * 2 + 1], even_elements[i * 2], 0, 0);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(const std::vector<float>& l) {\n    int n = l.size();\n    std::vector<float> result(n);\n\n    __m256 v0, v1, v2, v3;\n    int i = 0;\n\n    // Process elements in groups of 8\n    for (; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_ps(&l[i]);\n        v1 = _mm256_loadu_ps(&l[i + 4]);\n\n        // Sort the first group of 4 elements\n        v0 = _mm256_sort_ps(v0, _MM_SHUFFLE(0, 1, 2, 3));\n        // Sort the second group of 4 elements\n        v1 = _mm256_sort_ps(v1, _MM_SHUFFLE(0, 1, 2, 3));\n\n        // Interleave the sorted groups\n        v2 = _mm256_unpacklo_ps(v0, v1);\n        v3 = _mm256_unpackhi_ps(v0, v1);\n\n        // Store the interleaved results\n        _mm256_storeu_ps(&result[i], v2);\n        _mm256_storeu_ps(&result[i + 4], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(const std::vector<float>& l) {\n    int n = l.size();\n    std::vector<float> result(n);\n\n    __m256 v0, v1, v2, v3;\n    int i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_ps(&l[i]);\n        v1 = _mm256_permutevar8x32_ps(v0, _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        v0 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(3, 1, 2, 0, 3, 1, 2, 0));\n        v1 = _mm256_permutevar8x32_ps(v3, _mm256_set_epi32(3, 1, 2, 0, 3, 1, 2, 0));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        v0 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0));\n        v1 = _mm256_permutevar8x32_ps(v3, _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        v0 = _mm256_permutevar8x32_ps(v2, _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0));\n        v1 = _mm256_permutevar8x32_ps(v3, _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0));\n        v2 = _mm256_min_ps(v0, v1);\n        v3 = _mm256_max_ps(v0, v1);\n        _mm256_storeu_ps(&result[i], v2);\n        _mm256_storeu_ps(&result[i + 4], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_add_epi32(vi, vj);\n            for (int k = j + 1; k < n; ++k) {\n                __m256i vkj = _mm256_set1_epi32(l[k]);\n                __m256i sum = _mm256_add_epi32(vkj, vk);\n                if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(sum, v0)) != 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_add_epi32(vi, vj);\n            __m256i vres = _mm256_sub_epi32(vk, v2);\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vres, v0)) != 0) {\n                return true;\n            }\n            v2 = _mm256_add_epi32(v2, vj);\n        }\n        v2 = _mm256_setzero_si256();\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_add_epi32(vi, vj);\n            for (int k = j + 1; k < n; ++k) {\n                __m256i vkj = _mm256_set1_epi32(l[k]);\n                __m256i sum = _mm256_add_epi32(vkj, vk);\n                if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(sum, v0)) != 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_add_epi32(vi, vj);\n            __m256i vres = _mm256_sub_epi32(vk, v2);\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vres, v0)) != 0) {\n                return true;\n            }\n            v2 = _mm256_add_epi32(v2, vj);\n        }\n        v1 = _mm256_add_epi32(v1, vi);\n        v2 = _mm256_add_epi32(v2, v1);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n - 2; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i vk = _mm256_add_epi32(vi, vj);\n            for (int k = j + 1; k < n; ++k) {\n                __m256i vkj = _mm256_set1_epi32(l[k]);\n                __m256i sum = _mm256_add_epi32(vkj, vk);\n                if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(sum, v0)) != 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i ones = _mm256_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&l[i]);\n        __m256i inc = _mm256_add_epi32(vec, ones);\n        _mm256_storeu_si256((__m256i*)&result[i], inc);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i one = _mm256_set1_epi32(1);\n    int i = 0;\n\n    // Process in chunks of 8 elements\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i inc = _mm256_add_epi32(vec, one);\n        _mm256_storeu_si256((__m256i*)&result[i], inc);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i ones = _mm256_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)&l[i]);\n        vec = _mm256_add_epi32(vec, ones);\n        _mm256_storeu_si256((__m256i*)&result[i], vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(const std::vector<int>& l) {\n    std::vector<int> result(l.size());\n    int i = 0;\n    \n    // Process elements in chunks of 8\n    for (; i <= l.size() - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        vec = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec);\n    }\n    \n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m256i ones = _mm256_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, ones);\n        _mm256_storeu_si256((__m256i*)&result[i], vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m256i* vec = reinterpret_cast<__m256i*>(const_cast<int*>(&l[0]));\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i v1 = _mm256_loadu_si256(&vec[i]);\n        __m256i v2 = _mm256_loadu_si256(&vec[i + 4]);\n        __m256i v3 = _mm256_add_epi32(v1, v2);\n        __m256i v4 = _mm256_cmpeq_epi32(v3, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(v4)) return true;\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i v3 = _mm256_add_epi32(v2, v0);\n        __m256i v4 = _mm256_cmpeq_epi32(v3, v1);\n        if (_mm256_movemask_epi8(v4)) return true;\n        v0 = _mm256_or_si256(v0, v2);\n    }\n\n    for (int i = n - n % 8; i < n; ++i) {\n        if (std::find(l.begin(), l.end(), -l[i]) != l.end()) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i mask = _mm256_cmpeq_epi32(v, v0);\n        if (_mm256_movemask_epi8(mask)) return true;\n        v0 = _mm256_add_epi32(v0, v);\n\n        v = _mm256_loadu_si256((__m256i*)&l[i + 4]);\n        mask = _mm256_cmpeq_epi32(v, v1);\n        if (_mm256_movemask_epi8(mask)) return true;\n        v1 = _mm256_add_epi32(v1, v);\n    }\n\n    for (int i = n - n % 8; i < n; ++i) {\n        if (l[i] == 0 || (i > 0 && l[i] == -l[i - 1])) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n\n        __m256i mask = _mm256_cmpgt_epi32(v, v0);\n        __m256i neg_v = _mm256_sub_epi32(v0, v);\n\n        __m256i result = _mm256_and_si256(mask, _mm256_cmpeq_epi32(neg_v, v1));\n        if (_mm256_movemask_epi8(result)) return true;\n\n        v0 = _mm256_or_si256(v0, v);\n        v1 = _mm256_or_si256(v1, neg_v);\n    }\n\n    for (int i = n - n % 8; i < n; ++i) {\n        if (l[i] != 0 && std::find(l.begin(), l.end(), -l[i]) != l.end()) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m256i* vec = reinterpret_cast<__m256i*>(const_cast<int*>(&l[0]));\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i v1 = _mm256_loadu_si256(&vec[i]);\n        __m256i v2 = _mm256_loadu_si256(&vec[i + 4]);\n        __m256i v3 = _mm256_add_epi32(v1, v2);\n        __m256i mask = _mm256_cmpeq_epi32(v3, _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(mask)) return true;\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    while (x > 0) {\n        int remainder = x % base;\n        char digit = '0' + remainder;\n        result = digit + result;\n        x /= base;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    while (x != 0) {\n        int remainder = x % base;\n        result += std::to_string(remainder);\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::vector<int> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n    std::reverse(digits.begin(), digits.end());\n    std::string result;\n    for (int digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    __m256i v = _mm256_setzero_si256();\n    while (x != 0) {\n        int remainder = x % base;\n        char digit = '0' + remainder;\n        if (remainder >= 10) {\n            digit = 'A' + (remainder - 10);\n        }\n        result += digit;\n        x /= base;\n    }\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::vector<int> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n    std::reverse(digits.begin(), digits.end());\n    std::string result;\n    for (int digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i + 31 < len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + len - i - 32));\n\n        left = _mm256_and_si256(left, mask);\n        right = _mm256_and_si256(right, mask);\n\n        left = _mm256_reverse_epi8(left);\n        right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0,\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        ));\n\n        if (!_mm256_cmpeq_epi8(left, right).movemask()) {\n            return false;\n        }\n    }\n\n    // Check remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i + 31 < len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + len - i - 32));\n\n        left = _mm256_and_si256(left, mask);\n        right = _mm256_and_si256(right, mask);\n\n        __m256i reversed_right = _mm256_reverse_epi8(right);\n        __m256i cmp = _mm256_cmpeq_epi8(left, reversed_right);\n\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i + 31 < len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + len - i - 32));\n\n        left = _mm256_and_si256(left, mask);\n        right = _mm256_and_si256(right, mask);\n\n        __m256i reversed_right = _mm256_reverse_epi8(right);\n        __m256i cmp = _mm256_cmpeq_epi8(left, reversed_right);\n\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Check remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < len / 32; ++i) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + i * 32));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(text.c_str() + len - 32 - i * 32));\n\n        left = _mm256_and_si256(left, mask);\n        right = _mm256_and_si256(right, mask);\n\n        left = _mm256_reverse_epi8(left);\n        right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1));\n\n        if (!_mm256_cmpeq_epi8(left, right).movemask()) return false;\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i + 31 < len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - 32]));\n\n        left = _mm256_and_si256(left, mask);\n        right = _mm256_and_si256(right, mask);\n\n        __m256i reversed_right = _mm256_reverse_epi8(right);\n        __m256i cmp = _mm256_cmpeq_epi8(left, reversed_right);\n\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % 32; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(const std::string& s) {\n    const size_t len = s.length();\n    std::string out(len, '\\0');\n    const __m256i shift = _mm256_set1_epi8(5);\n    const __m256i base = _mm256_set1_epi8('a');\n\n    size_t i = 0;\n    while (i + 31 < len) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_sub_epi8(input, shift);\n        shifted = _mm256_add_epi8(shifted, base);\n        shifted = _mm256_and_si256(shifted, _mm256_set1_epi8('z' - 'a' + 1));\n        shifted = _mm256_or_si256(shifted, _mm256_cmpgt_epi8(base, shifted));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), shifted);\n        i += 32;\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        out[i] = ((s[i] - 'a' - 5 + 26) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const size_t simd_width = 32; // AVX2 processes 32 bytes at a time\n    size_t len = s.length();\n    std::string out(len, '\\0');\n\n    __m256i shift_val = _mm256_set1_epi8(5);\n    __m256i base_val = _mm256_set1_epi8('a');\n    __m256i mod_val = _mm256_set1_epi8(26);\n\n    for (size_t i = 0; i <= len - simd_width; i += simd_width) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_sub_epi8(input, shift_val);\n        __m256i wrapped = _mm256_add_epi8(shifted, base_val);\n        __m256i result = _mm256_and_si256(wrapped, mod_val);\n        result = _mm256_or_si256(result, base_val);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % simd_width; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            c = ((c - 'a' + 26 - 5) % 26) + 'a';\n        }\n        out[i] = c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const int chunk_size = 32; // AVX2 processes 32 bytes at a time\n    int len = s.length();\n    std::string out(len, '\\0');\n\n    __m256i shift_val = _mm256_set1_epi8(5);\n    __m256i base_val = _mm256_set1_epi8('a');\n    __m256i mod_val = _mm256_set1_epi8(26);\n\n    for (int i = 0; i <= len - chunk_size; i += chunk_size) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shifted = _mm256_sub_epi8(input, shift_val);\n        __m256i wrapped = _mm256_add_epi8(shifted, base_val);\n        __m256i result = _mm256_and_si256(wrapped, mod_val);\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % chunk_size; i < len; ++i) {\n        out[i] = ((s[i] - 'a' + 26 - 5) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(const std::string& s) {\n    const size_t len = s.length();\n    std::string out(len, '\\0');\n    \n    __m256i shift = _mm256_set1_epi8(5);\n    __m256i base = _mm256_set1_epi8('a');\n    __m256i mod = _mm256_set1_epi8(26);\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shifted = _mm256_sub_epi8(input, shift);\n        __m256i wrapped = _mm256_add_epi8(shifted, base);\n        __m256i result = _mm256_and_si256(wrapped, mod);\n        result = _mm256_add_epi8(result, base);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            c = ((c - 'a' + 26 - 5) % 26) + 'a';\n        }\n        out[i] = c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const int chunk_size = 32; // AVX2 processes 32 bytes at a time\n    int len = s.length();\n    std::string out(len, '\\0');\n\n    __m256i shift_val = _mm256_set1_epi8(5);\n    __m256i base_val = _mm256_set1_epi8('a');\n    __m256i mod_val = _mm256_set1_epi8(26);\n\n    for (int i = 0; i <= len - chunk_size; i += chunk_size) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i shifted = _mm256_sub_epi8(input, shift_val);\n        __m256i wrapped = _mm256_add_epi8(shifted, base_val);\n        __m256i result = _mm256_and_si256(wrapped, mod_val);\n        _mm256_storeu_si256((__m256i*)&out[i], result);\n    }\n\n    // Handle remaining characters\n    for (int i = len - len % chunk_size; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            c = ((c - 'a' + 26 - 5) % 26) + 'a';\n        }\n        out[i] = c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i;\n\n    // Process elements in blocks of 8\n    for (i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    const int vec_size = 8; // AVX2 can process 8 integers at once\n    int n = l.size();\n    __m256i threshold = _mm256_set1_epi32(t);\n\n    for (int i = 0; i <= n - vec_size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) return false;\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i = 0;\n\n    // Process elements in blocks of 8\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t n = l.size();\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i mask_open = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i mask_close = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n        count = _mm_add_epi32(count, _mm_and_si128(mask_open, _mm_set1_epi32(1)));\n        count = _mm_sub_epi32(count, _mm_and_si128(mask_close, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), count);\n    return result[0] == result[1] && result[1] == result[2] && result[2] == result[3] && result[0] == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i mask_open = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i mask_close = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n        count = _mm_add_epi32(count, _mm_and_si128(mask_open, _mm_set1_epi32(1)));\n        count = _mm_sub_epi32(count, _mm_and_si128(mask_close, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), count);\n    return result[0] == result[1] && result[1] == result[2] && result[2] == result[3] && result[0] == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i mask_open = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i mask_close = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n        count = _mm_add_epi32(count, _mm_and_si128(mask_open, _mm_set1_epi32(1)));\n        count = _mm_sub_epi32(count, _mm_and_si128(mask_close, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), count);\n    return result[0] == result[1] && result[1] == result[2] && result[2] == result[3];\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const size_t length = brackets.length();\n    if (length % 32 != 0) return false;\n\n    __m256i count_open = _mm256_setzero_si256();\n    __m256i count_close = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.c_str() + i));\n\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n\n        count_open = _mm256_add_epi32(count_open, _mm256_and_si256(open_mask, _mm256_set1_epi32(1)));\n        count_close = _mm256_add_epi32(count_close, _mm256_and_si256(close_mask, _mm256_set1_epi32(1)));\n    }\n\n    int open_sum = 0;\n    int close_sum = 0;\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&open_sum), count_open);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&close_sum), count_close);\n\n    return open_sum == close_sum;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const size_t length = brackets.length();\n    if (length % 32 != 0) return false;\n\n    __m256i count_open = _mm256_setzero_si256();\n    __m256i count_close = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.c_str() + i));\n\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n\n        count_open = _mm256_add_epi32(count_open, _mm256_and_si256(open_mask, _mm256_set1_epi32(1)));\n        count_close = _mm256_add_epi32(count_close, _mm256_and_si256(close_mask, _mm256_set1_epi32(1)));\n    }\n\n    int open_sum = 0;\n    int close_sum = 0;\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&open_sum), count_open);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&close_sum), count_close);\n\n    return open_sum == close_sum;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const size_t length = brackets.length();\n    if (length % 32 != 0) return false;\n\n    __m256i count_open = _mm256_setzero_si256();\n    __m256i count_close = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.c_str() + i));\n\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n\n        count_open = _mm256_add_epi32(count_open, _mm256_and_si256(open_mask, _mm256_set1_epi32(1)));\n        count_close = _mm256_add_epi32(count_close, _mm256_and_si256(close_mask, _mm256_set1_epi32(1)));\n    }\n\n    int total_open[8];\n    int total_close[8];\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(total_open), count_open);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(total_close), count_close);\n\n    int sum_open = 0;\n    int sum_close = 0;\n\n    for (int i = 0; i < 8; ++i) {\n        sum_open += total_open[i];\n        sum_close += total_close[i];\n    }\n\n    return sum_open == sum_close;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 32 != 0) return false;\n\n    __m256i count_open = _mm256_setzero_si256();\n    __m256i count_close = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.c_str() + i));\n\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n\n        count_open = _mm256_add_epi32(count_open, _mm256_and_si256(open_mask, _mm256_set1_epi32(1)));\n        count_close = _mm256_add_epi32(count_close, _mm256_and_si256(close_mask, _mm256_set1_epi32(1)));\n    }\n\n    int total_open = 0;\n    int total_close = 0;\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&total_open), count_open);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&total_close), count_close);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 32 != 0) return false;\n\n    __m256i count_open = _mm256_setzero_si256();\n    __m256i count_close = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i open_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8('('));\n        __m256i close_mask = _mm256_cmpeq_epi8(data, _mm256_set1_epi8(')'));\n\n        count_open = _mm256_add_epi32(count_open, _mm256_and_si256(open_mask, _mm256_set1_epi32(1)));\n        count_close = _mm256_add_epi32(count_close, _mm256_and_si256(close_mask, _mm256_set1_epi32(1)));\n    }\n\n    int total_open = 0;\n    int total_close = 0;\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&total_open), count_open);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&total_close), count_close);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    int n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 vzero = _mm256_setzero_ps();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        __m256 vx = _mm256_loadu_ps(&xs[i]);\n        __m256 vcoeff = _mm256_setr_ps(1, 2, 3, 4, 5, 6, 7, 8);\n        __m256 vresult = _mm256_mul_ps(vx, vcoeff);\n        _mm256_storeu_ps(&result[i], vresult);\n    }\n\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m256 vzero = _mm256_setzero_ps();\n\n    for (int i = 0; i < n - 1; i += 8) {\n        __m256 va = _mm256_loadu_ps(&xs[i]);\n        __m256 vb = _mm256_add_ps(va, vzero);\n        __m256 vc = _mm256_mul_ps(va, _mm256_set1_ps(1.0f));\n        __m256 vd = _mm256_add_ps(vb, vc);\n        _mm256_storeu_ps(&result[i], vd);\n    }\n\n    for (int i = (n - 1) & ~7; i < n - 1; ++i) {\n        result[i] = xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    int n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 vzero = _mm256_setzero_ps();\n\n    for (int i = 0; i < n - 1; i += 8) {\n        __m256 va = _mm256_loadu_ps(&xs[i]);\n        __m256 vb = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);\n        __m256 vc = _mm256_mul_ps(va, vb);\n        __m256 vd = _mm256_add_ps(vc, vzero);\n        _mm256_storeu_ps(&result[i], vd);\n    }\n\n    // Handle remaining elements\n    for (int i = (n - 1) & ~7; i < n - 1; ++i) {\n        result[i] = xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    int n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 vzero = _mm256_setzero_ps();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        __m256 va = _mm256_loadu_ps(&xs[i]);\n        __m256 vb = _mm256_add_ps(va, vzero);\n        __m256 vc = _mm256_mul_ps(va, _mm256_set1_ps(1.0f));\n        __m256 vd = _mm256_mul_ps(va, _mm256_set1_ps(2.0f));\n        __m256 ve = _mm256_mul_ps(va, _mm256_set1_ps(3.0f));\n        __m256 vf = _mm256_mul_ps(va, _mm256_set1_ps(4.0f));\n        __m256 vg = _mm256_mul_ps(va, _mm256_set1_ps(5.0f));\n\n        _mm256_storeu_ps(&result[i], vc);\n        _mm256_storeu_ps(&result[i + 1], vd);\n        _mm256_storeu_ps(&result[i + 2], ve);\n        _mm256_storeu_ps(&result[i + 3], vf);\n        _mm256_storeu_ps(&result[i + 4], vg);\n    }\n\n    for (int i = n - 8; i < n; ++i) {\n        result[i] = xs[i] * static_cast<float>(i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m256 vzero = _mm256_setzero_ps();\n\n    for (int i = 0; i < n - 1; i += 8) {\n        __m256 va = _mm256_loadu_ps(&xs[i]);\n        __m256 vb = _mm256_add_ps(va, vzero);\n        __m256 vc = _mm256_mul_ps(vb, _mm256_set1_ps(i + 1));\n        _mm256_storeu_ps(&result[i], vc);\n    }\n\n    // Handle remaining elements\n    for (int i = (n - 1) & ~7; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m256i vowels_mask = _mm256_set1_epi8('a');\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('e'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('i'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('o'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('u'));\n\n    while (len >= 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str));\n        __m256i mask = _mm256_and_si256(chunk, vowels_mask);\n        __m256i eq_mask = _mm256_cmpeq_epi8(mask, chunk);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n\n        str += 32;\n        len -= 32;\n    }\n\n    // Handle remaining characters\n    for (size_t i = 0; i < len; ++i) {\n        if (str[i] == 'a' || str[i] == 'e' || str[i] == 'i' || str[i] == 'o' || str[i] == 'u') {\n            count++;\n        }\n        if (str[i] == 'y' && i == len - 1) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_set1_epi8('a');\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'e', 1);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'i', 2);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'o', 3);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'u', 4);\n\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i mask = _mm256_and_si256(str_vec, vowels_mask);\n        __m256i eq_mask = _mm256_cmpeq_epi8(mask, str_vec);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u') {\n            count++;\n        }\n        if (i + 1 == s.size() && s[i] == 'y') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_set1_epi8('a');\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'e', 1);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'i', 2);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'o', 3);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'u', 4);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'A', 5);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'E', 6);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'I', 7);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'O', 8);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'U', 9);\n\n    int i = 0;\n    for (; i + 31 < s.size(); i += 32) {\n        __m256i str = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i mask = _mm256_and_si256(str, vowels_mask);\n        __m256i eq = _mm256_cmpeq_epi8(mask, str);\n        __m256i sum = _mm256_sad_epu8(eq, _mm256_setzero_si256());\n        count += _mm256_extract_epi32(sum, 0) / 2;\n    }\n\n    for (; i < s.size(); ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n            s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_set1_epi8('a');\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'e', 1);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'i', 2);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'o', 3);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'u', 4);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'A', 5);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'E', 6);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'I', 7);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'O', 8);\n    vowels_mask = _mm256_insert_epi8(vowels_mask, 'U', 9);\n\n    int i = 0;\n    for (; i + 31 < s.size(); i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i mask = _mm256_and_si256(str_vec, vowels_mask);\n        __m256i eq_mask = _mm256_cmpeq_epi8(mask, str_vec);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    for (; i < s.size(); ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n            s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_set1_epi8('a');\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('e'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('i'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('o'));\n    vowels_mask = _mm256_or_si256(vowels_mask, _mm256_set1_epi8('u'));\n\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i mask = _mm256_and_si256(str_vec, vowels_mask);\n        __m256i eq_mask = _mm256_cmpeq_epi8(mask, str_vec);\n        int eq_count = _mm256_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n\n        if (i + 32 < s.size()) {\n            char last_char = s[i + 32];\n            if (last_char == 'y' || last_char == 'Y') {\n                count++;\n            }\n        }\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n            (s[i] == 'y' || s[i] == 'Y')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ascii_a = _mm256_set1_epi8('A');\n    __m256i ascii_z = _mm256_set1_epi8('Z');\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, ascii_a), _mm256_cmplt_epi8(chunk, ascii_z));\n        __m256i ascii_values = _mm256_add_epi8(chunk, _mm256_set1_epi8(-'A'));\n        __m256i masked_values = _mm256_mullo_epi8(ascii_values, mask);\n        __m256i sum_chunk = _mm256_reduce_add_epi8(masked_values);\n\n        sum += _mm256_movemask_epi8(sum_chunk);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = len - len % 32; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += static_cast<int>(str[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ascii_a = _mm256_set1_epi8('A');\n    __m256i ascii_z = _mm256_set1_epi8('Z');\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, ascii_a), _mm256_cmplt_epi8(vec, ascii_z));\n        __m256i ascii_values = _mm256_add_epi8(vec, _mm256_set1_epi8(-'A'));\n        __m256i masked_values = _mm256_and_si256(ascii_values, mask);\n        __m256i sum_vec = _mm256_reduce_add_epi8(masked_values);\n        sum += _mm256_movemask_epi8(sum_vec);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = len - len % 32; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += static_cast<int>(str[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i asciiOffset = _mm256_set1_epi8('A' - 'a');\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i mask = _mm256_cmpgt_epi8(vec, zero);\n        __m256i lowerCaseMask = _mm256_cmplt_epi8(_mm256_add_epi8(vec, asciiOffset), zero);\n        __m256i validChars = _mm256_andnot_si256(lowerCaseMask, mask);\n        __m256i asciiValues = _mm256_and_si256(validChars, vec);\n        __m256i asciiSum = _mm256_reduce_add_epi8(asciiValues);\n\n        int localSum = _mm256_movemask_epi8(asciiSum);\n        sum += localSum;\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += static_cast<int>(str[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ascii_sum = zero;\n\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i is_upper = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A'));\n        is_upper = _mm256_and_si256(is_upper, _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z')));\n        __m256i ascii_values = _mm256_add_epi8(chars, _mm256_set1_epi8(-'A' + 'a'));\n        ascii_sum = _mm256_add_epi32(ascii_sum, _mm256_maskz_mov_epi32(_mm256_movemask_epi8(is_upper), ascii_values));\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), ascii_sum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i] - 'A' + 'a';\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ascii_sum = zero;\n\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i is_upper = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A'));\n        is_upper = _mm256_and_si256(is_upper, _mm256_cmplt_epi8(chars, _mm256_set1_epi8('Z')));\n        __m256i ascii_values = _mm256_add_epi8(chars, _mm256_set1_epi8(-'A' + 'a'));\n        ascii_sum = _mm256_add_epi32(ascii_sum, _mm256_maskz_mov_epi32(_mm256_movemask_epi8(is_upper), ascii_values));\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), ascii_sum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i] - 'A' + 'a';\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m256i v_min_even_value = _mm256_set1_epi32(INT_MAX);\n    __m256i v_min_index = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i v_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i v_indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Check if the values are even\n        __m256i v_is_even = _mm256_and_si256(v_values, _mm256_set1_epi32(1));\n        v_is_even = _mm256_cmpeq_epi32(v_is_even, _mm256_setzero_si256());\n\n        // Mask out odd values\n        __m256i v_masked_values = _mm256_and_si256(v_values, v_is_even);\n        __m256i v_masked_indices = _mm256_and_si256(v_indices, v_is_even);\n\n        // Find minimum even value and index\n        __m256i v_min_value = _mm256_min_epi32(v_min_even_value, v_masked_values);\n        __m256i v_min_idx = _mm256_or_si256(_mm256_andnot_si256(v_is_even, v_min_index), v_masked_indices);\n\n        v_min_even_value = v_min_value;\n        v_min_index = v_min_idx;\n    }\n\n    // Process remaining elements\n    for (size_t i = arr.size() - arr.size() % 8; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even_value, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m256i v_min_even_value = _mm256_set1_epi32(INT_MAX);\n    __m256i v_min_index = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i v_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i v_indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i v_mask = _mm256_and_si256(v_values, _mm256_set1_epi32(1)); // Check if even\n        v_mask = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n        __m256i v_even_values = _mm256_blendv_epi32(_mm256_set1_epi32(INT_MAX), v_values, v_mask);\n        __m256i v_even_indices = _mm256_blendv_epi32(_mm256_set1_epi32(-1), v_indices, v_mask);\n\n        v_min_even_value = _mm256_min_epi32(v_min_even_value, v_even_values);\n        v_min_index = _mm256_min_epi32(v_min_index, v_even_indices);\n    }\n\n    int result[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), v_min_even_value);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result + 4), v_min_index);\n\n    for (int j = 0; j < 8; ++j) {\n        if (result[j] != INT_MAX && result[j] < min_even_value) {\n            min_even_value = result[j];\n            min_index = result[j + 4];\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    } else {\n        return {min_even_value, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m256i v_min_even_value = _mm256_set1_epi32(INT_MAX);\n    __m256i v_min_index = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i v_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i v_indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i v_mask = _mm256_and_si256(v_values, _mm256_set1_epi32(1)); // Check if even\n        __m256i v_even_values = _mm256_and_si256(v_values, _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256()));\n\n        __m256i v_min_even_values_new = _mm256_min_epi32(v_min_even_values, v_even_values);\n        __m256i v_min_indices_new = _mm256_blendv_epi32(v_min_index, v_indices, _mm256_cmpgt_epi32(v_min_even_values, v_even_values));\n\n        v_min_even_values = v_min_even_values_new;\n        v_min_index = v_min_indices_new;\n    }\n\n    for (size_t i = arr.size() - arr.size() % 8; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m256i v_min_even_value = _mm256_set1_epi32(INT_MAX);\n    __m256i v_min_index = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i v_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i v_indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        __m256i v_mask = _mm256_and_si256(v_values, _mm256_set1_epi32(1));\n        __m256i v_is_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n        __m256i v_selected_values = _mm256_blendv_epi32(_mm256_set1_epi32(INT_MAX), v_values, v_is_even);\n        __m256i v_selected_indices = _mm256_blendv_epi32(_mm256_set1_epi32(-1), v_indices, v_is_even);\n\n        v_min_even_value = _mm256_min_epi32(v_min_even_value, v_selected_values);\n        v_min_index = _mm256_min_epi32(v_min_index, v_selected_indices);\n    }\n\n    int result[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), v_min_even_value);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result + 4), v_min_index);\n\n    for (int i = 0; i < 8; ++i) {\n        if (result[i] != INT_MAX && result[i] < min_even_value) {\n            min_even_value = result[i];\n            min_index = result[i + 4];\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    } else {\n        return {min_even_value, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m256i v_min_even_value = _mm256_set1_epi32(INT_MAX);\n    __m256i v_min_index = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < arr.size(); i += 8) {\n        __m256i v_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i v_indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        // Mask for even numbers\n        __m256i v_mask = _mm256_and_si256(v_values, _mm256_set1_epi32(1));\n        __m256i v_is_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n        // Update minimum even value and index\n        __m256i v_new_min_even_value = _mm256_blendv_epi32(v_min_even_value, v_values, v_is_even);\n        __m256i v_new_min_index = _mm256_blendv_epi32(v_min_index, v_indices, v_is_even);\n\n        v_min_even_value = v_new_min_even_value;\n        v_min_index = v_new_min_index;\n    }\n\n    // Process remaining elements\n    for (size_t i = arr.size() - arr.size() % 8; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even_value, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    const int max_val = 1000; // Assuming the maximum possible value in the list is 1000\n    __m256i count_vec = _mm256_setzero_si256();\n    \n    // Count occurrences of each value up to max_val\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(values, j);\n            if (val > 0 && val <= max_val) {\n                count_vec = _mm256_add_epi32(count_vec, _mm256_slli_epi32(_mm256_set1_epi32(1), val - 1));\n            }\n        }\n    }\n\n    // Find the greatest integer with frequency >= its value\n    int result = -1;\n    for (int i = 1; i <= max_val; ++i) {\n        int freq = _mm256_extract_epi32(count_vec, i - 1);\n        if (freq >= i) {\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    const int n = lst.size();\n    if (n < 256) return -1; // Not enough elements to benefit from AVX2\n\n    std::vector<int> count(256, 0);\n    for (int i = 0; i < n; ++i) {\n        count[lst[i]]++;\n    }\n\n    __m256i v_max = _mm256_set1_epi32(-1);\n    for (int i = 0; i < 256; i += 8) {\n        __m256i v_i = _mm256_set1_epi32(i);\n        __m256i v_count = _mm256_loadu_si256((__m256i*)&count[i]);\n        __m256i v_mask = _mm256_cmpgt_epi32(v_count, v_i);\n        v_max = _mm256_blendv_epi32(v_max, v_i, v_mask);\n    }\n\n    int max_val;\n    _mm256_storeu_si256((__m256i*)&max_val, v_max);\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    const int n = lst.size();\n    if (n < 256) return -1; // Not enough elements for AVX2\n\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) return -1; // Too large values for AVX2\n\n    __m256i count[256] = {};\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v, j);\n            count[val]++;\n        }\n    }\n\n    for (int i = 0; i <= max_val; ++i) {\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(count[i], _mm256_set1_epi32(i))) != 0) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    const int n = lst.size();\n    if (n < 256) return -1; // Not enough elements for AVX2\n\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) return -1; // Too large values for AVX2\n\n    __m256i count[256] = {};\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v, j);\n            count[val]++;\n        }\n    }\n\n    for (int i = 0; i <= max_val; ++i) {\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(count[i], _mm256_set1_epi32(i))) != 0) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    const int n = lst.size();\n    if (n < 8) return -1;\n\n    __m256i count[7] = {};\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        for (int j = 0; j < 7; ++j) {\n            __m256i cmp = _mm256_cmpeq_epi32(v, _mm256_set1_epi32(j + 1));\n            count[j] = _mm256_add_epi32(count[j], _mm256_and_si256(cmp, _mm256_set1_epi32(1)));\n        }\n    }\n\n    int result = -1;\n    for (int j = 0; j < 7; ++j) {\n        __m256i mask = _mm256_cmpgt_epi32(count[j], _mm256_set1_epi32(j + 1));\n        int mask_val = _mm256_movemask_epi8(mask);\n        if (mask_val != 0) {\n            result = j + 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_weight_vec = _mm256_set1_epi32(w);\n    __m256i one_vec = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        __m256i reversed_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        if (!_mm256_cmpeq_epi32(vec, reversed_vec).mm256_movemask_epi8()) return false;\n    }\n\n    int remaining_elements = size - i;\n    int sum_remaining = 0;\n    for (; i < size; ++i) {\n        sum_remaining += q[i];\n        if (q[i] != q[size - 1 - i]) return false;\n    }\n\n    int total_sum = _mm256_extract_epi32(sum_vec, 0) +\n                    _mm256_extract_epi32(sum_vec, 1) +\n                    _mm256_extract_epi32(sum_vec, 2) +\n                    _mm256_extract_epi32(sum_vec, 3) +\n                    sum_remaining;\n\n    return total_sum <= w && remaining_elements % 2 == 0;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_vec = _mm256_set1_epi32(w);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        __m256i rev_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        if (!_mm256_cmpeq_epi32(vec, rev_vec).mm256_movemask_epi8()) return false;\n    }\n\n    int remaining = size - i;\n    __m256i partial_sum = _mm256_setzero_si256();\n    for (; i < size; ++i) {\n        partial_sum = _mm256_add_epi32(partial_sum, _mm256_set1_epi32(q[i]));\n    }\n    sum_vec = _mm256_add_epi32(sum_vec, partial_sum);\n\n    int total_sum = _mm256_reduce_add_epi32(sum_vec);\n    if (total_sum > w) return false;\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_weight_vec = _mm256_set1_epi32(w);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    bool balanced = true;\n    int i = 0;\n    while (i + 7 < n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        balanced &= (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vec, _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(0, 4, 1, 5, 2, 6, 3, 7)))) & 0xFF) == 0xFF;\n        i += 8;\n    }\n\n    while (i < n) {\n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_set1_epi32(q[i]));\n        balanced &= q[i] == q[n - 1 - i];\n        i++;\n    }\n\n    int sum[8];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n\n    int total_sum = 0;\n    for (int j = 0; j < 8; j++) {\n        total_sum += sum[j];\n    }\n\n    return total_sum <= w && balanced;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_vec = _mm256_set1_epi32(w);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        __m256i rev_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        if (!_mm256_cmpeq_epi32(vec, rev_vec).mm256_movemask_epi8()) return false;\n    }\n\n    int remaining = n - i;\n    __m256i remaining_vec = _mm256_setzero_si256();\n    for (int j = 0; j < remaining; ++j) {\n        remaining_vec = _mm256_insert_epi32(remaining_vec, q[i + j], j);\n    }\n    sum_vec = _mm256_add_epi32(sum_vec, remaining_vec);\n\n    __m256i mask = _mm256_cmpgt_epi32(sum_vec, max_vec);\n    if (_mm256_movemask_epi8(mask)) return false;\n\n    mask = _mm256_cmpeq_epi32(sum_vec, zero_vec);\n    if (_mm256_movemask_epi8(mask)) return false;\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i max_vec = _mm256_set1_epi32(w);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        __m256i rev_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        if (!_mm256_cmpeq_epi32(vec, rev_vec).mm256_movemask_epi8()) return false;\n    }\n\n    int remaining = size - i;\n    __m256i partial_sum = _mm256_setzero_si256();\n    for (; i < size; ++i) {\n        partial_sum = _mm256_add_epi32(partial_sum, _mm256_set1_epi32(q[i]));\n    }\n    sum_vec = _mm256_add_epi32(sum_vec, partial_sum);\n\n    int total_sum = _mm256_reduce_add_epi32(sum_vec);\n    if (total_sum > w) return false;\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m256i v0, v1;\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n        __m256i cmp = _mm256_cmpeq_epi32(v0, v1);\n        changes += _mm256_movemask_epi8(cmp);\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    __m256i v1, v2;\n    int i;\n\n    for (i = 0; i < n / 16; ++i) {\n        v1 = _mm256_loadu_si256((__m256i*)&arr[i * 16]);\n        v2 = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i * 16]);\n        v1 = _mm256_min_epi32(v1, v2);\n        v2 = _mm256_max_epi32(v1, v2);\n        changes += _mm256_movemask_epi8(_mm256_cmpgt_epi32(v2, v1));\n    }\n\n    for (; i < n / 4; ++i) {\n        v1 = _mm256_loadu_si256((__m256i*)&arr[i * 4]);\n        v2 = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i * 4]);\n        v1 = _mm256_min_epi32(v1, v2);\n        v2 = _mm256_max_epi32(v1, v2);\n        changes += _mm256_movemask_epi8(_mm256_cmpgt_epi32(v2, v1));\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m256i v0, v1;\n    int i = 0;\n    for (; i <= n - 16; i += 16) {\n        v0 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&arr[n - i - 16]);\n        __m256i cmp = _mm256_cmpeq_epi32(v0, v1);\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += __builtin_popcount(~mask);\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m256i v0, v1;\n    int i = 0;\n    for (; i <= n - 16; i += 16) {\n        v0 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&arr[n - i - 16]);\n        __m256i cmp = _mm256_cmpeq_epi32(v0, v1);\n        int mask = _mm256_movemask_epi8(cmp);\n        changes += __builtin_popcount(~mask);\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes / 2;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m256i v0, v1;\n    int i = 0, j = n - 1;\n\n    while (i < j) {\n        v0 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&arr[j]);\n\n        for (int k = 0; k < 8; ++k) {\n            if (_mm256_extract_epi32(v0, k) != _mm256_extract_epi32(v1, k)) {\n                ++changes;\n            }\n        }\n\n        i += 8;\n        j -= 8;\n    }\n\n    // Handle remaining elements\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            ++changes;\n        }\n        ++i;\n        --j;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        size_t len1 = lst1[i].length();\n        size_t len2 = lst2[i].length();\n\n        if (len1 > 256 || len2 > 256) {\n            // Handle strings longer than 256 characters separately\n            continue;\n        }\n\n        __m256i vlen1 = _mm256_set1_epi32(len1);\n        __m256i vlen2 = _mm256_set1_epi32(len2);\n\n        sum1 = _mm256_add_epi32(sum1, vlen1);\n        sum2 = _mm256_add_epi32(sum2, vlen2);\n    }\n\n    int s1 = _mm256_extract_epi32(sum1, 0) + _mm256_extract_epi32(sum1, 1) +\n             _mm256_extract_epi32(sum1, 2) + _mm256_extract_epi32(sum1, 3);\n    int s2 = _mm256_extract_epi32(sum2, 0) + _mm256_extract_epi32(sum2, 1) +\n             _mm256_extract_epi32(sum2, 2) + _mm256_extract_epi32(sum2, 3);\n\n    if (s1 <= s2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        size_t len1 = lst1[i].length();\n        size_t len2 = lst2[i].length();\n\n        if (len1 > 256 || len2 > 256) {\n            // Handle strings longer than 256 characters separately\n            continue;\n        }\n\n        __m256i vlen1 = _mm256_set1_epi32(len1);\n        __m256i vlen2 = _mm256_set1_epi32(len2);\n\n        sum1 = _mm256_add_epi32(sum1, vlen1);\n        sum2 = _mm256_add_epi32(sum2, vlen2);\n    }\n\n    int s1 = _mm256_extract_epi32(sum1, 0) + _mm256_extract_epi32(sum1, 1) +\n             _mm256_extract_epi32(sum1, 2) + _mm256_extract_epi32(sum1, 3);\n    int s2 = _mm256_extract_epi32(sum2, 0) + _mm256_extract_epi32(sum2, 1) +\n             _mm256_extract_epi32(sum2, 2) + _mm256_extract_epi32(sum2, 3);\n\n    if (s1 <= s2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        size_t len1 = lst1[i].length();\n        size_t len2 = lst2[i].length();\n        if (len1 > 256 || len2 > 256) {\n            // Handle strings longer than 256 characters separately\n            continue;\n        }\n        __m256i vlen1 = _mm256_set1_epi32(len1);\n        __m256i vlen2 = _mm256_set1_epi32(len2);\n        sum1 = _mm256_add_epi32(sum1, vlen1);\n        sum2 = _mm256_add_epi32(sum2, vlen2);\n    }\n\n    int s1, s2;\n    _mm256_storeu_si256((__m256i*)&s1, sum1);\n    _mm256_storeu_si256((__m256i*)&s2, sum2);\n\n    if (s1 < s2) {\n        return lst1;\n    } else if (s2 < s1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        size_t len1 = lst1[i].length();\n        size_t len2 = lst2[i].length();\n        if (len1 > 256 || len2 > 256) continue;\n\n        __m256i vlen1 = _mm256_set1_epi8(len1);\n        __m256i vlen2 = _mm256_set1_epi8(len2);\n\n        sum1 = _mm256_add_epi32(sum1, vlen1);\n        sum2 = _mm256_add_epi32(sum2, vlen2);\n    }\n\n    int s1 = _mm256_extract_epi32(sum1, 0) + _mm256_extract_epi32(sum1, 1) +\n             _mm256_extract_epi32(sum1, 2) + _mm256_extract_epi32(sum1, 3);\n    int s2 = _mm256_extract_epi32(sum2, 0) + _mm256_extract_epi32(sum2, 1) +\n             _mm256_extract_epi32(sum2, 2) + _mm256_extract_epi32(sum2, 3);\n\n    return s1 <= s2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        size_t len1 = lst1[i].length();\n        size_t len2 = lst2[i].length();\n        if (len1 > 256 || len2 > 256) {\n            // Handle strings longer than 256 characters separately\n            continue;\n        }\n        __m256i vlen1 = _mm256_set1_epi32(len1);\n        __m256i vlen2 = _mm256_set1_epi32(len2);\n        sum1 = _mm256_add_epi32(sum1, vlen1);\n        sum2 = _mm256_add_epi32(sum2, vlen2);\n    }\n\n    int s1 = _mm256_extract_epi32(sum1, 0) + _mm256_extract_epi32(sum1, 1) +\n             _mm256_extract_epi32(sum1, 2) + _mm256_extract_epi32(sum1, 3);\n    int s2 = _mm256_extract_epi32(sum2, 0) + _mm256_extract_epi32(sum2, 1) +\n             _mm256_extract_epi32(sum2, 2) + _mm256_extract_epi32(sum2, 3);\n\n    if (s1 <= s2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m256i v_x = _mm256_set1_epi64x(x);\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_setzero_si256();\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_result = _mm256_mul_epu64(v_x, v_n);\n        v_x = v_result;\n        __m256i v_mask = _mm256_cmpeq_epi64(v_x, v_one);\n        if (_mm256_movemask_epi8(v_mask) != 0) {\n            return true;\n        }\n        __m256i v_not_equal = _mm256_cmpneq_epi64(v_x, v_zero);\n        if (_mm256_movemask_epi8(v_not_equal) == 0) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m256i v_x = _mm256_set1_epi64x(x);\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_setzero_si256();\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_result = _mm256_mullo_epi64(v_n, v_x);\n        v_x = v_result;\n        __m256i v_mask = _mm256_cmpeq_epi64(v_x, v_one);\n        if (_mm256_movemask_epi8(v_mask) != 0) {\n            return true;\n        }\n        v_mask = _mm256_cmpeq_epi64(v_x, v_zero);\n        if (_mm256_movemask_epi8(v_mask) != 0) {\n            return false;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m256i v_x = _mm256_set1_epi64x(x);\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_result = _mm256_mullo_epi64(v_n, v_x);\n        v_x = v_result;\n        __m256i v_mask = _mm256_cmpeq_epi64(v_result, v_x);\n        if (_mm256_movemask_epi8(v_mask) == 0xFFFFFFFF) {\n            return true;\n        }\n        v_x = _mm256_add_epi64(v_x, v_one);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m256i v_x = _mm256_set1_epi64x(x);\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_result = _mm256_mul_epu64(v_x, v_n);\n        v_x = v_result;\n        __m256i v_mask = _mm256_cmpeq_epi64(v_x, v_one);\n        int mask_value = _mm256_movemask_epi8(v_mask);\n        if (mask_value == 0xFFFF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m256i v_x = _mm256_set1_epi64x(x);\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_one = _mm256_setzero_si256();\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i v_result = v_one;\n        for (int j = 0; j < 64; ++j) {\n            v_result = _mm256_mul_epu64(v_result, v_n);\n            __m256i v_mask = _mm256_cmpeq_epi64(v_result, v_x);\n            if (_mm256_movemask_epi8(v_mask) == 0xFFFFFFFF) {\n                return true;\n            }\n        }\n        v_x = _mm256_srli_epi64(v_x, 64);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    const __m256i mask = _mm256_set1_epi8(0xF);\n    const __m256i primes_mask = _mm256_set1_epi8((1 << '2') | (1 << '3') | (1 << '5') | (1 << '7') | (1 << 'B') | (1 << 'D'));\n    int count = 0;\n\n    for (size_t i = 0; i + 32 <= num.size(); i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        __m256i masked_data = _mm256_and_si256(data, mask);\n        __m256i result = _mm256_and_si256(masked_data, primes_mask);\n        __m256i ones = _mm256_cmpeq_epi8(result, primes_mask);\n        int local_count = _mm256_movemask_epi8(ones);\n        count += __builtin_popcount(local_count);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = num.size() - num.size() % 32; i < num.size(); ++i) {\n        if ((num[i] == '2' || num[i] == '3' || num[i] == '5' || num[i] == '7' || num[i] == 'B' || num[i] == 'D')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i primes = _mm256_set1_epi8('2');\n    primes = _mm256_insert_epi8(primes, '3', 1);\n    primes = _mm256_insert_epi8(primes, '5', 2);\n    primes = _mm256_insert_epi8(primes, '7', 3);\n    primes = _mm256_insert_epi8(primes, 'B', 4);\n    primes = _mm256_insert_epi8(primes, 'D', 5);\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i *)&num[i]);\n        __m256i cmp = _mm256_cmpeq_epi8(data, primes);\n        __m256i sum = _mm256_sad_epu8(cmp, mask);\n        count += _mm256_movemask_epi8(sum);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i primes = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', -1, -1, '2', '3', '5', '7', 'B', 'D', -1, -1);\n\n    for (size_t i = 0; i < num.size(); i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        __m256i eq = _mm256_cmpeq_epi8(data, primes);\n        __m256i sum = _mm256_movemask_epi8(eq);\n        count += __builtin_popcount(sum);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i primes_mask = _mm256_set1_epi8(0);\n\n    // Create a mask for prime hexadecimal digits\n    primes_mask = _mm256_insert_epi8(primes_mask, 2, 0); // '2'\n    primes_mask = _mm256_insert_epi8(primes_mask, 3, 1); // '3'\n    primes_mask = _mm256_insert_epi8(primes_mask, 5, 2); // '5'\n    primes_mask = _mm256_insert_epi8(primes_mask, 7, 3); // '7'\n    primes_mask = _mm256_insert_epi8(primes_mask, 11, 4); // 'B'\n    primes_mask = _mm256_insert_epi8(primes_mask, 13, 5); // 'D'\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)&num[i]);\n        __m256i masked_data = _mm256_and_si256(data, primes_mask);\n        __m256i cmp_result = _mm256_cmpeq_epi8(masked_data, data);\n        int result = _mm256_movemask_epi8(cmp_result);\n        count += __builtin_popcount(result);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m256i mask = _mm256_set1_epi8(0);\n    __m256i primes = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, '2', '3', '5', '7', 'B', 'D', 0, 0);\n\n    for (size_t i = 0; i < num.size(); i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        __m256i eq = _mm256_cmpeq_epi8(data, primes);\n        __m256i sum = _mm256_and_si256(eq, _mm256_set1_epi8(1));\n        int result = _mm256_movemask_epi8(sum);\n        count += __builtin_popcount(result);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m256i v3 = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i v4 = _mm256_and_si256(v3, _mm256_set1_epi8(0xFF));\n        __m256i v5 = _mm256_cmpeq_epi8(v4, v0);\n        __m256i v6 = _mm256_cmpeq_epi8(v4, v1);\n        __m256i v7 = _mm256_cmpeq_epi8(v4, v2);\n        __m256i v8 = _mm256_or_si256(v5, v6);\n        __m256i v9 = _mm256_or_si256(v8, v7);\n        if (_mm256_movemask_epi8(v9) != 0) return false;\n        v0 = v1;\n        v1 = v2;\n        v2 = v4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m256i v1, v2, v3;\n    v1 = _mm256_setzero_si256();\n    v2 = _mm256_setzero_si256();\n    v3 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m256i v4 = _mm256_loadu_si256((__m256i const*)&s[i]);\n        v1 = _mm256_or_si256(v1, v4);\n        v2 = _mm256_or_si256(v2, _mm256_srli_epi16(v4, 16));\n        v3 = _mm256_or_si256(v3, _mm256_slli_epi16(v4, 16));\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(v1, v2)) != 0 ||\n            _mm256_movemask_epi8(_mm256_cmpeq_epi8(v1, v3)) != 0 ||\n            _mm256_movemask_epi8(_mm256_cmpeq_epi8(v2, v3)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i v3 = _mm256_and_si256(v2, _mm256_set1_epi8(0xFF));\n        __m256i v4 = _mm256_cmpgt_epi8(v3, v0);\n        __m256i v5 = _mm256_cmpgt_epi8(v3, v1);\n\n        v0 = _mm256_or_si256(v0, v4);\n        v1 = _mm256_or_si256(v1, v5);\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_or_si256(v0, v1));\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 7 < s.length(); i += 8) {\n        __m256i v3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i v4 = _mm256_and_si256(v3, _mm256_set1_epi8(0xFF));\n        __m256i v5 = _mm256_and_si256(_mm256_srli_epi16(v4, 8), _mm256_set1_epi8(0xFF));\n        __m256i v6 = _mm256_and_si256(_mm256_srli_epi32(v4, 16), _mm256_set1_epi8(0xFF));\n\n        __m256i v7 = _mm256_or_si256(v5, v6);\n        __m256i v8 = _mm256_or_si256(v7, v0);\n\n        __m256i v9 = _mm256_cmpgt_epi8(v8, v1);\n        __m256i v10 = _mm256_cmpgt_epi8(v8, v2);\n\n        if (_mm256_movemask_epi8(v9) != 0 || _mm256_movemask_epi8(v10) != 0) {\n            return false;\n        }\n\n        v0 = v1;\n        v1 = v2;\n        v2 = v8;\n    }\n\n    for (size_t i = s.length() - s.length() % 8; i < s.length(); ++i) {\n        if (i > 0 && s[i] == s[i - 1]) return false;\n        if (i > 1 && s[i] == s[i - 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m256i v3 = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i v4 = _mm256_cmpeq_epi8(v3, v0);\n        __m256i v5 = _mm256_cmpeq_epi8(v3, v1);\n        __m256i v6 = _mm256_cmpeq_epi8(v3, v2);\n        __m256i v7 = _mm256_or_si256(v4, v5);\n        __m256i v8 = _mm256_or_si256(v7, v6);\n        if (_mm256_movemask_epi8(v8)) return false;\n        v0 = v1;\n        v1 = v2;\n        v2 = v3;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int len = str.length();\n    __m256i vlen = _mm256_set1_epi32(len);\n    __m256i primes = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n\n    __m256i mask = _mm256_cmpeq_epi32(vlen, primes);\n    int result = _mm256_movemask_epi8(mask);\n\n    return (result & 0xFF) != 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    __m256i v1 = _mm256_set1_epi32(0);\n    __m256i v2 = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= length / 2; i += 8) {\n        __m256i v3 = _mm256_set1_epi32(i);\n        __m256i v4 = _mm256_div_epi32(v2, v3);\n        __m256i v5 = _mm256_mul_epi32(v3, v4);\n        __m256i v6 = _mm256_cmpeq_epi32(v2, v6);\n        v1 = _mm256_or_si256(v1, v6);\n    }\n\n    return _mm256_movemask_epi8(v1) == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string &str) {\n    int length = str.length();\n    if (length <= 1) return false;\n    if (length == 2 || length == 3) return true;\n\n    __m256i v0 = _mm256_set1_epi32(2);\n    __m256i v1 = _mm256_set1_epi32(length);\n\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi32(v0, v1)) != 0) {\n        __m256i v2 = _mm256_add_epi32(v0, v0);\n        v0 = _mm256_min_epi32(v0, v2);\n    }\n\n    return _mm256_movemask_epi8(_mm256_cmpeq_epi32(v0, v1)) == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    __m256i v1 = _mm256_set1_epi32(0);\n    __m256i v2 = _mm256_set1_epi32(length);\n\n    for (int i = 2; i <= length / 2; i += 8) {\n        __m256i v3 = _mm256_set1_epi32(i);\n        __m256i v4 = _mm256_div_epi32(v2, v3);\n        __m256i v5 = _mm256_mul_epi32(v3, v4);\n        __m256i v6 = _mm256_cmpeq_epi32(v2, v6);\n        v1 = _mm256_or_si256(v1, v6);\n    }\n\n    return length <= 1 ? false : _mm256_movemask_epi8(_mm256_cmpgt_epi32(v1, _mm256_setzero_si256())) == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n#include <algorithm>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string &str) {\n    int length = str.length();\n    return is_prime(length);\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    while (N > 0) {\n        __m256i N_vec = _mm256_set1_epi64x(N);\n        __m256i mod_vec = _mm256_and_si256(N_vec, _mm256_set1_epi64x(9));\n        N = N / 10;\n        sum_vec = _mm256_add_epi64(sum_vec, mod_vec);\n    }\n    uint64_t sum[4];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n    uint64_t total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    std::stringstream ss;\n    ss << std::bitset<64>(total_sum).to_string();\n    std::string result = ss.str();\n    size_t first_one = result.find('1');\n    if (first_one != std::string::npos) {\n        result = result.substr(first_one);\n    } else {\n        result = \"0\";\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    while (N > 0) {\n        __m256i N_vec = _mm256_set1_epi64x(N);\n        __m256i mod_vec = _mm256_and_si256(N_vec, _mm256_set1_epi64x(9));\n        sum_vec = _mm256_add_epi64(sum_vec, mod_vec);\n        \n        N /= 10;\n    }\n    \n    uint64_t sum[4];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n    \n    std::stringstream ss;\n    for (int i = 3; i >= 0; --i) {\n        ss << std::bitset<64>(sum[i]);\n    }\n    \n    std::string result = ss.str();\n    size_t first_one = result.find('1');\n    if (first_one != std::string::npos) {\n        result = result.substr(first_one);\n    } else {\n        result = \"0\";\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum = _mm256_setzero_si256();\n    \n    while (N > 0) {\n        __m256i num = _mm256_set1_epi64x(N);\n        __m256i mod = _mm256_and_si256(num, _mm256_set1_epi64x(9));\n        N /= 10;\n        \n        __m256i carry = _mm256_cmpgt_epi64(mod, _mm256_set1_epi64x(9));\n        mod = _mm256_add_epi64(mod, _mm256_and_si256(carry, _mm256_set1_epi64x(-10)));\n        \n        sum = _mm256_add_epi64(sum, mod);\n    }\n    \n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    \n    std::stringstream ss;\n    for (int i = 3; i >= 0; --i) {\n        ss << std::bitset<64>(result[i]);\n    }\n    \n    std::string binarySum = ss.str();\n    size_t firstOne = binarySum.find('1');\n    if (firstOne != std::string::npos) {\n        binarySum = binarySum.substr(firstOne);\n    } else {\n        binarySum = \"0\";\n    }\n    \n    return binarySum;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum = _mm256_setzero_si256();\n    \n    while (N > 0) {\n        __m256i digits = _mm256_loadu_si256((__m256i*)&N);\n        digits = _mm256_and_si256(digits, _mm256_set1_epi64x(0xF));\n        sum = _mm256_add_epi64(sum, digits);\n        \n        N >>= 4;\n    }\n    \n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    \n    std::stringstream ss;\n    for (int i = 3; i >= 0; --i) {\n        ss << std::bitset<64>(result[i]);\n    }\n    \n    std::string binarySum = ss.str();\n    size_t firstOne = binarySum.find('1');\n    if (firstOne != std::string::npos) {\n        binarySum = binarySum.substr(firstOne);\n    } else {\n        binarySum = \"0\";\n    }\n    \n    return binarySum;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    while (N > 0) {\n        __m256i N_vec = _mm256_set1_epi64x(N);\n        __m256i mod_vec = _mm256_and_si256(N_vec, _mm256_set1_epi64x(9));\n        sum_vec = _mm256_add_epi64(sum_vec, mod_vec);\n        N = N / 10;\n    }\n    uint64_t sum[4];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n    uint64_t total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    std::stringstream ss;\n    ss << std::bitset<64>(total_sum).to_string();\n    std::string result = ss.str();\n    result.erase(result.find_last_not_of('0') + 1);\n    return result.empty() ? \"0\" : result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    int n = lst.size();\n\n    for (int i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m256i val = _mm256_set1_epi32(lst[i]);\n            vsum = _mm256_add_epi32(vsum, val);\n        }\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    int n = lst.size();\n\n    for(int i = 1; i < n; i += 2){\n        if(i + 7 < n){\n            __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n            __m256i mask = _mm256_and_si256(vec, _mm256_set1_epi32(0xAAAAAAAA));\n            vsum = _mm256_add_epi32(vsum, mask);\n        } else {\n            for(; i < n; i += 2){\n                if(lst[i] % 2 == 0){\n                    sum += lst[i];\n                }\n            }\n        }\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for(int i = 0; i < 8; i++){\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i result = zero;\n\n    size_t n = lst.size();\n    size_t i = 1; // Start from index 1\n\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask = _mm256_and_si256(vec, _mm256_set1_epi32(0xAAAAAAAA));\n        result = _mm256_add_epi32(result, mask);\n        i += 8;\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, result);\n\n    for (; i < n; ++i) {\n        if (i % 2 != 0 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    int n = lst.size();\n\n    for (int i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m256i val = _mm256_set1_epi32(lst[i]);\n            vsum = _mm256_add_epi32(vsum, val);\n        }\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m256i val = _mm256_set1_epi32(lst[i]);\n            vsum = _mm256_add_epi32(vsum, val);\n        }\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 32;\n    std::string result(s.size(), '\\0');\n    const __m256i shift = _mm256_set1_epi8(4); // Shift by 4 (2 * 2)\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t remaining = std::min(chunk_size, s.size() - i);\n        if (remaining == 0) break;\n\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i output = _mm256_add_epi8(input, shift);\n\n        // Wrap around the alphabet\n        __m256i mask = _mm256_cmpgt_epi8(output, _mm256_set1_epi8('z'));\n        output = _mm256_blendv_epi8(output, _mm256_sub_epi8(output, _mm256_set1_epi8(26)), mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 32;\n    std::string result(s);\n    __m256i *input_ptr = (__m256i *)s.c_str();\n    __m256i *output_ptr = (__m256i *)result.data();\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m256i input = _mm256_loadu_si256(input_ptr + i / chunk_size);\n        __m256i shifted = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n        _mm256_storeu_si256(output_ptr + i / chunk_size, shifted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 32;\n    std::string result(s);\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t end = std::min(i + chunk_size, s.size());\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i output = _mm256_add_epi8(input, _mm256_set1_epi8(4));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 32;\n    std::string result(s.size(), '\\0');\n    const __m256i shift = _mm256_set1_epi8(4); // Shift by 4\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t end = std::min(i + chunk_size, s.size());\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.c_str() + i));\n        __m256i output = _mm256_add_epi8(input, shift);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 32;\n    std::string result(s);\n    __m256i key = _mm256_set1_epi8('c'); // 'c' is 2 * 2 = 4 positions ahead in the alphabet\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t remaining = std::min(chunk_size, static_cast<size_t>(s.size() - i));\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i output = _mm256_add_epi8(input, key);\n\n        // Wrap around the alphabet\n        __m256i mask = _mm256_cmpgt_epi8(output, _mm256_set1_epi8('z'));\n        output = _mm256_blendv_epi8(output, _mm256_sub_epi8(output, _mm256_set1_epi8(26)), mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *input = message.c_str();\n    size_t length = message.length();\n    std::string result(length, '\\0');\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', -1, -1, -1, -1, -1, -1);\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i *)(input + i));\n        __m256i swapped_case = _mm256_xor_si256(input_vec, _mm256_set1_epi8(0x20));\n\n        __m256i mask = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i replaced = _mm256_blendv_epi8(swapped_case, replacements, mask);\n\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced);\n    }\n\n    // Handle remaining characters\n    for (size_t i = length - length % 32; i < length; ++i) {\n        char c = input[i];\n        if (isalpha(c)) {\n            c = islower(c) ? toupper(c) : tolower(c);\n            if (strchr(\"aeiouAEIOU\", c)) {\n                switch (c) {\n                    case 'a': c = 'c'; break;\n                    case 'e': c = 'g'; break;\n                    case 'i': c = 'k'; break;\n                    case 'o': c = 'q'; break;\n                    case 'u': c = 'w'; break;\n                    case 'A': c = 'C'; break;\n                    case 'E': c = 'G'; break;\n                    case 'I': c = 'K'; break;\n                    case 'O': c = 'Q'; break;\n                    case 'U': c = 'W'; break;\n                }\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char * msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', -1, -1, -1, -1, -1, -1);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)(msg + i));\n\n        // Swap case\n        __m256i swapped_case = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n\n        // Replace vowels\n        __m256i mask = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i replaced = _mm256_blendv_epi8(swapped_case, replacements, mask);\n\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = msg[i];\n        if (isalpha(c)) {\n            if (islower(c)) {\n                c = toupper(c);\n            } else {\n                c = tolower(c);\n            }\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                if (c >= 'a' && c <= 'y') {\n                    c += 2;\n                } else if (c >= 'A' && c <= 'Y') {\n                    c += 2;\n                } else if (c == 'z') {\n                    c = 'b';\n                } else if (c == 'Z') {\n                    c = 'B';\n                }\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *input = message.c_str();\n    size_t length = message.length();\n    std::string result(length, '\\0');\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', -1, -1, -1, -1, -1, -1);\n\n    for (size_t i = 0; i <= length - 32; i += 32) {\n        __m256i input_vec = _mm256_loadu_si256((__m256i *)(input + i));\n        __m256i swapped_case = _mm256_xor_si256(input_vec, _mm256_set1_epi8(0x20));\n\n        __m256i mask = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i replaced = _mm256_blendv_epi8(swapped_case, replacements, mask);\n\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced);\n    }\n\n    // Handle remaining characters\n    for (size_t i = length - length % 32; i < length; ++i) {\n        char c = input[i];\n        if (isalpha(c)) {\n            c = toupper(c);\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = ((c - 'A' + 2) % 26) + 'A';\n            }\n        } else {\n            c = tolower(c);\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c = ((c - 'a' + 2) % 26) + 'a';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', -1, -1, -1, -1, -1, -1);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i msg_vec = _mm256_loadu_si256((__m256i *)(msg + i));\n        __m256i swapped_case = _mm256_xor_si256(msg_vec, _mm256_set1_epi8(0x20));\n\n        __m256i mask = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i replaced = _mm256_blendv_epi8(swapped_case, replacements, mask);\n\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = msg[i];\n        if (isalpha(c)) {\n            c = toupper(c);\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = c + 2;\n                if (c > 'Z') c -= 26;\n            }\n        } else {\n            c = tolower(c);\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c = c + 2;\n                if (c > 'z') c -= 26;\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char * msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m256i replacements = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W', -1, -1, -1, -1, -1, -1);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256((__m256i *)(msg + i));\n\n        // Swap case\n        __m256i swapped_case = _mm256_xor_si256(chunk, _mm256_set1_epi8(0x20));\n\n        // Replace vowels\n        __m256i mask = _mm256_cmpeq_epi8(swapped_case, vowels);\n        __m256i replaced = _mm256_blendv_epi8(swapped_case, replacements, mask);\n\n        // Store result\n        _mm256_storeu_si256((__m256i *)(result.data() + i), replaced);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = msg[i];\n        if (isalpha(c)) {\n            if (islower(c)) {\n                c = toupper(c);\n            } else {\n                c = tolower(c);\n            }\n            if (strchr(\"aeiouAEIOU\", c) != nullptr) {\n                c = ((c - 'a' + 2) % 26 + 'a') | (isupper(c) ? 'A' : 'a');\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0);\n    __m256i upper_mask = _mm256_set1_epi8(0);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 32 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n\n            __m256i lower_check = _mm256_cmpgt_epi8(key_vec, _mm256_set1_epi8('Z'));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), key_vec);\n\n            lower_mask = _mm256_or_si256(lower_mask, lower_check);\n            upper_mask = _mm256_or_si256(upper_mask, upper_check);\n        }\n    }\n\n    return _mm256_movemask_epi8(lower_mask) == 0 || _mm256_movemask_epi8(upper_mask) == 0;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0);\n    __m256i upper_mask = _mm256_set1_epi8(0);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 32 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n\n            __m256i lower_check = _mm256_cmpgt_epi8(key_vec, _mm256_set1_epi8('Z'));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), key_vec);\n\n            lower_mask = _mm256_or_si256(lower_mask, lower_check);\n            upper_mask = _mm256_or_si256(upper_mask, upper_check);\n        }\n    }\n\n    return _mm256_testz_si256(lower_mask, upper_mask) || _mm256_testz_si256(upper_mask, lower_mask);\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0);\n    __m256i upper_mask = _mm256_set1_epi8(0);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 32 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n\n            __m256i lower_check = _mm256_cmpgt_epi8(key_vec, _mm256_set1_epi8('Z'));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), key_vec);\n\n            lower_mask = _mm256_or_si256(lower_mask, lower_check);\n            upper_mask = _mm256_or_si256(upper_mask, upper_check);\n        }\n    }\n\n    return _mm256_testz_si256(lower_mask, upper_mask) && _mm256_testz_si256(upper_mask, lower_mask);\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0);\n    __m256i upper_mask = _mm256_set1_epi8(0);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 32 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.data() + i));\n\n            __m256i lower_check = _mm256_cmpgt_epi8(key_vec, _mm256_set1_epi8('Z'));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), key_vec);\n\n            lower_mask = _mm256_or_si256(lower_mask, lower_check);\n            upper_mask = _mm256_or_si256(upper_mask, upper_check);\n        }\n    }\n\n    return _mm256_movemask_epi8(lower_mask) == 0 || _mm256_movemask_epi8(upper_mask) == 0;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_mask = _mm256_set1_epi8(0x20 | 0x20);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 32 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n\n            __m256i lower_check = _mm256_and_si256(key_vec, lower_mask);\n            __m256i upper_check = _mm256_and_si256(key_vec, upper_mask);\n\n            __m256i lower_eq = _mm256_cmpeq_epi8(lower_check, key_vec);\n            __m256i upper_eq = _mm256_cmpeq_epi8(upper_check, key_vec);\n\n            if (_mm256_movemask_epi8(lower_eq) == 0xFFFFFFFF || _mm256_movemask_epi8(upper_eq) == 0xFFFFFFFF) {\n                continue;\n            } else {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (int i = 2; i < n; ++i) {\n        __m256i num = _mm256_set1_epi32(i);\n        __m256i remainder = _mm256_rem_epi32(num, two);\n        __m256i is_even = _mm256_cmpeq_epi32(remainder, zeros);\n        __m256i is_not_prime = _mm256_or_si256(is_even, _mm256_cmpgt_epi32(num, three));\n\n        if (_mm256_movemask_epi8(is_not_prime) == 0) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n\n    int i = 3;\n    while (i < n) {\n        __m256i start = _mm256_set1_epi32(i);\n        __m256i end = _mm256_add_epi32(start, two);\n        __m256i step = _mm256_set1_epi32(2);\n\n        bool is_prime = true;\n        while (_mm256_cmpgt_epi32(end, start)) {\n            __m256i divisors = _mm256_div_epi32(_mm256_set1_epi32(i), start);\n            __m256i remainders = _mm256_mul_epi32(divisors, start);\n            __m256i cmp = _mm256_cmpeq_epi32(remainders, _mm256_set1_epi32(i));\n            if (_mm256_movemask_epi8(cmp) != 0) {\n                is_prime = false;\n                break;\n            }\n            start = _mm256_add_epi32(start, step);\n        }\n\n        if (is_prime) {\n            primes.push_back(i);\n        }\n\n        i += 2;\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    const int vec_size = 8;\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m256i mask = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n    __m256i five = _mm256_set1_epi32(5);\n    __m256i six = _mm256_set1_epi32(6);\n    __m256i seven = _mm256_set1_epi32(7);\n    __m256i eight = _mm256_set1_epi32(8);\n    __m256i nine = _mm256_set1_epi32(9);\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i eleven = _mm256_set1_epi32(11);\n    __m256i twelve = _mm256_set1_epi32(12);\n    __m256i thirteen = _mm256_set1_epi32(13);\n    __m256i fourteen = _mm256_set1_epi32(14);\n    __m256i fifteen = _mm256_set1_epi32(15);\n    __m256i sixteen = _mm256_set1_epi32(16);\n    __m256i seventeen = _mm256_set1_epi32(17);\n    __m256i eighteen = _mm256_set1_epi32(18);\n    __m256i nineteen = _mm256_set1_epi32(19);\n    __m256i twenty = _mm256_set1_epi32(20);\n\n    for (int i = 2; i < n; ++i) {\n        __m256i num = _mm256_set1_epi32(i);\n        __m256i mod2 = _mm256_and_si256(num, two);\n        __m256i mod3 = _mm256_and_si256(num, three);\n        __m256i mod5 = _mm256_and_si256(num, five);\n        __m256i mod7 = _mm256_and_si256(num, seven);\n\n        __m256i eq2 = _mm256_cmpeq_epi32(mod2, ones);\n        __m256i eq3 = _mm256_cmpeq_epi32(mod3, ones);\n        __m256i eq5 = _mm256_cmpeq_epi32(mod5, ones);\n        __m256i eq7 = _mm256_cmpeq_epi32(mod7, ones);\n\n        __m256i or_eqs = _mm256_or_si256(eq2, eq3);\n        or_eqs = _mm256_or_si256(or_eqs, eq5);\n        or_eqs = _mm256_or_si256(or_eqs, eq7);\n\n        __m256i not_primes = _mm256_or_si256(or_eqs, mask);\n        mask = _mm256_or_si256(mask, not_primes);\n\n        if (_mm256_movemask_epi8(not_primes) == 0) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m256i mask = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n    __m256i five = _mm256_set1_epi32(5);\n    __m256i six = _mm256_set1_epi32(6);\n    __m256i seven = _mm256_set1_epi32(7);\n    __m256i eight = _mm256_set1_epi32(8);\n    __m256i nine = _mm256_set1_epi32(9);\n    __m256i ten = _mm256_set1_epi32(10);\n\n    for (int i = 2; i < n; ++i) {\n        __m256i num = _mm256_set1_epi32(i);\n        __m256i mod2 = _mm256_and_si256(num, two);\n        __m256i mod3 = _mm256_and_si256(num, three);\n        __m256i mod5 = _mm256_and_si256(num, five);\n        __m256i mod7 = _mm256_and_si256(num, seven);\n\n        __m256i cond2 = _mm256_cmpeq_epi32(mod2, zero);\n        __m256i cond3 = _mm256_cmpeq_epi32(mod3, zero);\n        __m256i cond5 = _mm256_cmpeq_epi32(mod5, zero);\n        __m256i cond7 = _mm256_cmpeq_epi32(mod7, zero);\n\n        __m256i or_cond = _mm256_or_si256(cond2, cond3);\n        or_cond = _mm256_or_si256(or_cond, cond5);\n        or_cond = _mm256_or_si256(or_cond, cond7);\n\n        __m256i not_cond = _mm256_xor_si256(or_cond, all_ones);\n\n        if (_mm256_movemask_epi8(not_cond) == 0xFF) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m256i mask = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i four = _mm256_set1_epi32(4);\n    __m256i five = _mm256_set1_epi32(5);\n    __m256i six = _mm256_set1_epi32(6);\n    __m256i seven = _mm256_set1_epi32(7);\n    __m256i eight = _mm256_set1_epi32(8);\n    __m256i nine = _mm256_set1_epi32(9);\n    __m256i ten = _mm256_set1_epi32(10);\n\n    for (int i = 2; i < n; ++i) {\n        __m256i num = _mm256_set1_epi32(i);\n        __m256i mod2 = _mm256_and_si256(num, two);\n        __m256i mod3 = _mm256_and_si256(num, three);\n        __m256i mod5 = _mm256_and_si256(num, five);\n        __m256i mod7 = _mm256_and_si256(num, seven);\n\n        __m256i eq2 = _mm256_cmpeq_epi32(mod2, zero);\n        __m256i eq3 = _mm256_cmpeq_epi32(mod3, zero);\n        __m256i eq5 = _mm256_cmpeq_epi32(mod5, zero);\n        __m256i eq7 = _mm256_cmpeq_epi32(mod7, zero);\n\n        __m256i or_eq = _mm256_or_si256(eq2, eq3);\n        __m256i or_eq2 = _mm256_or_si256(or_eq, eq5);\n        __m256i or_eq3 = _mm256_or_si256(or_eq2, eq7);\n\n        if (_mm256_testz_si256(or_eq3, or_eq3)) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i upper_mask = _mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF));\n        __m256i result = _mm256_cmpeq_epi8(_mm256_and_si256(upper_mask, vowels_mask), vowels_mask);\n        uint32_t mask = _mm256_movemask_epi8(result);\n        while (mask) {\n            if (mask & 1) {\n                count++;\n            }\n            mask >>= 1;\n        }\n    }\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (i % 2 == 0 && isupper(s[i])) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i upper_mask = _mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF));\n        __m256i result = _mm256_cmpeq_epi8(_mm256_and_si256(upper_mask, vowels_mask), vowels_mask);\n        uint32_t mask = _mm256_movemask_epi8(result);\n        while (mask) {\n            if (mask & 1) {\n                count++;\n            }\n            mask >>= 1;\n        }\n    }\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (i % 2 == 0 && s[i] >= 'A' && s[i] <= 'Z' && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i upper_mask = _mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF));\n        __m256i result = _mm256_cmpeq_epi8(_mm256_and_si256(upper_mask, vowels_mask), vowels_mask);\n        uint32_t mask = _mm256_movemask_epi8(result);\n        while (mask) {\n            if (mask & 1) {\n                count++;\n            }\n            mask >>= 1;\n        }\n    }\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if ((i % 2 == 0) && (isupper(s[i]) && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U'))) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i upper_mask = _mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF));\n        __m256i result = _mm256_cmpeq_epi8(_mm256_and_si256(upper_mask, vowels_mask), vowels_mask);\n        uint32_t mask = _mm256_movemask_epi8(result);\n        while (mask) {\n            if (mask & 1) {\n                count++;\n            }\n            mask >>= 1;\n        }\n    }\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (i % 2 == 0 && isupper(s[i])) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m256i vowels_mask = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 32; i += 32) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i upper_mask = _mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF));\n        __m256i result = _mm256_cmpeq_epi8(_mm256_and_si256(upper_mask, vowels_mask), vowels_mask);\n        uint32_t mask = _mm256_movemask_epi8(result);\n        while (mask) {\n            if (mask & 1) {\n                count++;\n            }\n            mask >>= 1;\n        }\n    }\n    for (size_t i = s.size() - s.size() % 32; i < s.size(); ++i) {\n        if (i % 2 == 0 && s[i] >= 'A' && s[i] <= 'Z' && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n\n    int i = 0;\n    while (i + 7 < n) {\n        __m256i indices = _mm256_add_epi32(_mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), ones);\n        __m256i is_odd = _mm256_and_si256(indices, _mm256_set1_epi32(1));\n        __m256i next_even = _mm256_add_epi32(indices, two);\n        __m256i next_odd = _mm256_add_epi32(next_even, one);\n        __m256i result_vec = _mm256_blendv_epi32(next_even, next_odd, is_odd);\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec);\n        i += 8;\n    }\n\n    for (; i < n; ++i) {\n        result[i] = i + 1 + (i % 2 == 0 ? 1 : 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i mask = _mm256_set1_epi32(0x55555555); // Alternating 0s and 1s\n\n    int i = 0;\n    while (i + 8 <= n) {\n        __m256i indices = _mm256_add_epi32(_mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), ones);\n        __m256i parity = _mm256_and_si256(indices, mask);\n        __m256i next_numbers = _mm256_add_epi32(indices, _mm256_cmpgt_epi32(parity, _mm256_setzero_si256()));\n        _mm256_storeu_si256((__m256i*)&result[i], next_numbers);\n        i += 8;\n    }\n\n    while (i < n) {\n        result[i] = i + 1 + (i % 2 == 0 ? 0 : 1);\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n\n    int i = 0;\n    while (i + 7 < n) {\n        __m256i indices = _mm256_add_epi32(_mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), ones);\n        __m256i parity = _mm256_and_si256(indices, two);\n        __m256i next_odd = _mm256_add_epi32(indices, three);\n        __m256i next_even = _mm256_add_epi32(indices, two);\n        __m256i result_vec = _mm256_blendv_epi32(next_odd, next_even, parity);\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec);\n        i += 8;\n    }\n\n    while (i < n) {\n        result[i] = i + 1 + (i % 2 ? 2 : 1);\n        ++i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i mask = _mm256_set1_epi32(0x80000000);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256i indices = _mm256_add_epi32(_mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), ones);\n        __m256i parity = _mm256_and_si256(indices, mask);\n        __m256i next_numbers = _mm256_add_epi32(indices, _mm256_cmpgt_epi32(parity, _mm256_setzero_si256()));\n        _mm256_storeu_si256((__m256i*)&result[i], next_numbers);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = i + 1 + (n % 2 == 0 ? 1 : 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i four = _mm256_set1_epi32(4);\n\n    int i = 0;\n    while (i + 7 < n) {\n        __m256i indices = _mm256_add_epi32(_mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), ones);\n        __m256i parity = _mm256_and_si256(indices, one);\n        __m256i next_odd = _mm256_add_epi32(indices, two);\n        __m256i next_even = _mm256_add_epi32(indices, four);\n        __m256i result_vec = _mm256_blendv_epi32(next_even, next_odd, parity);\n        _mm256_storeu_si256((__m256i*)&result[i], result_vec);\n        i += 8;\n    }\n\n    while (i < n) {\n        result[i] = i + 1 + (i % 2 == 0 ? 1 : 0);\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    for (; n + 7 <= m; n += 8) {\n        __m256i values = _mm256_set_epi64x(n + 7, n + 6, n + 5, n + 4,\n                                           n + 3, n + 2, n + 1, n);\n        sum = _mm256_add_epi64(sum, values);\n        count += 8;\n    }\n\n    // Process remaining numbers\n    for (; n <= m; ++n) {\n        sum = _mm256_add_epi64(sum, _mm256_set_epi64x(n, 0, 0, 0));\n        count++;\n    }\n\n    // Calculate the average\n    __m256i avg = _mm256_srai_epi64(_mm256_add_epi64(sum, _mm256_set1_epi64x(count / 2)), 3);\n\n    // Convert the result to an integer\n    int64_t avg_int = _mm256_extract_epi64(avg, 0);\n\n    // Convert the integer to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg_int);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    for (; n + 7 <= m; n += 8) {\n        __m256i values = _mm256_set_epi64x(n + 7, n + 6, n + 5, n + 4,\n                                           n + 3, n + 2, n + 1, n);\n        sum = _mm256_add_epi64(sum, values);\n        count += 8;\n    }\n\n    // Process remaining numbers\n    for (; n <= m; ++n) {\n        sum = _mm256_add_epi64(sum, _mm256_set_epi64x(n, 0, 0, 0));\n        count++;\n    }\n\n    // Calculate the average\n    __m256i avg = _mm256_srai_epi64(_mm256_add_epi64(sum, _mm256_set1_epi64x(count / 2)), 3);\n\n    // Convert to integer and round down\n    int64_t result = _mm256_extract_epi64(avg, 0);\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(result);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    int count = 0;\n\n    // Process 8 numbers at a time\n    while (n + 7 <= m) {\n        __m256i values = _mm256_setr_epi64x(n, n + 1, n + 2, n + 3);\n        values = _mm256_add_epi64(values, _mm256_setr_epi64x(n + 4, n + 5, n + 6, n + 7));\n        sum = _mm256_add_epi64(sum, values);\n        n += 8;\n        count += 8;\n    }\n\n    // Process remaining numbers\n    while (n <= m) {\n        sum = _mm256_add_epi64(sum, _mm256_set1_epi64x(n));\n        n++;\n        count++;\n    }\n\n    // Sum up the results in the vector\n    int64_t total_sum = 0;\n    for (int i = 0; i < 4; ++i) {\n        total_sum += _mm256_extract_epi64(sum, i);\n    }\n\n    // Calculate average and round down\n    int avg = std::floor(static_cast<double>(total_sum) / count);\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    for (; n + 7 <= m; n += 8) {\n        __m256i values = _mm256_set_epi64x(n + 7, n + 6, n + 5, n + 4,\n                                           n + 3, n + 2, n + 1, n);\n        sum = _mm256_add_epi64(sum, values);\n        count += 8;\n    }\n\n    // Process remaining numbers\n    for (; n <= m; ++n) {\n        sum = _mm256_add_epi64(sum, _mm256_set_epi64x(n, 0, 0, 0));\n        count++;\n    }\n\n    // Calculate the average\n    __m256i avg = _mm256_srai_epi64(_mm256_add_epi64(sum, _mm256_set1_epi64x(count / 2)), 3);\n\n    // Convert the result to an integer\n    int64_t avg_int = _mm256_extract_epi64(avg, 0);\n\n    // Convert the integer to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg_int);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    for (; n + 7 <= m; n += 8) {\n        __m256i values = _mm256_set_epi64x(n + 7, n + 6, n + 5, n + 4,\n                                           n + 3, n + 2, n + 1, n);\n        sum = _mm256_add_epi64(sum, values);\n        count += 8;\n    }\n\n    // Process remaining numbers\n    for (; n <= m; ++n) {\n        sum = _mm256_add_epi64(sum, _mm256_set_epi64x(n, 0, 0, 0));\n        count++;\n    }\n\n    // Calculate the average\n    __m256i avg = _mm256_srai_epi64(_mm256_add_epi64(sum, _mm256_set1_epi64x(count / 2)), 3);\n\n    // Convert to integer and round down\n    int64_t result = _mm256_extract_epi64(avg, 0);\n\n    // Convert to binary string\n    std::bitset<64> binary(result);\n    return binary.to_string().substr(64 - std::ceil(std::log2(result + 1)));\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i v_index = _mm256_setr_epi32(1, 2, 3, 4);\n    __m256i v_fact = _mm256_setzero_si256();\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i v_mask = _mm256_and_si256(v_index, _mm256_set1_epi32(1));\n        __m256i v_even = _mm256_andnot_si256(v_mask, v_index);\n\n        // Calculate factorial for even indices\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi32(v_even, j);\n            if (idx % 2 == 0 && idx > 0) {\n                int fact = 1;\n                for (int k = 1; k <= idx; ++k) {\n                    fact *= k;\n                }\n                fact %= 10000;\n                v_fact = _mm256_insert_epi32(v_fact, fact, j);\n            } else {\n                v_fact = _mm256_insert_epi32(v_fact, 0, j);\n            }\n        }\n\n        // Calculate sum for odd indices\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi32(v_odd, j);\n            if (idx % 2 != 0) {\n                int sum = 0;\n                for (int k = 1; k <= idx; ++k) {\n                    sum += k;\n                }\n                v_sum = _mm256_insert_epi32(v_sum, sum, j);\n            } else {\n                v_sum = _mm256_insert_epi32(v_sum, 0, j);\n            }\n        }\n\n        // Combine results\n        __m256i v_result = _mm256_or_si256(v_fact, v_sum);\n        _mm256_storeu_si256((__m256i*)&result[i * 8], v_result);\n\n        // Update indices\n        v_index = _mm256_add_epi32(v_index, _mm256_set1_epi32(8));\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        if (i % 2 == 0 && i > 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact *= j;\n            }\n            fact %= 10000;\n            result[i] = fact;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= i; ++j) {\n                sum += j;\n            }\n            result[i] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i indices = _mm256_set_epi32(n-1, n-2, n-3, n-4, n-5, n-6, n-7, n-8);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i mask = _mm256_cmpgt_epi32(indices, zero);\n        __m256i even_mask = _mm256_and_si256(mask, _mm256_cmpeq_epi32(_mm256_and_si256(indices, _mm256_set1_epi32(1)), zero));\n        __m256i odd_mask = _mm256_andnot_si256(even_mask, mask);\n\n        __m256i factorials = _mm256_setzero_si256();\n        __m256i sums = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(indices, j);\n            if (idx % 2 == 0) {\n                __m256i fact = _mm256_set1_epi32(1);\n                for (int k = 1; k <= idx; ++k) {\n                    fact = _mm256_mullo_epi32(fact, _mm256_set1_epi32(k));\n                }\n                factorials = _mm256_add_epi32(factorials, _mm256_and_si256(even_mask, fact));\n            } else {\n                __m256i sum = _mm256_set1_epi32(0);\n                for (int k = 1; k <= idx; ++k) {\n                    sum = _mm256_add_epi32(sum, _mm256_set1_epi32(k));\n                }\n                sums = _mm256_add_epi32(sums, _mm256_and_si256(odd_mask, sum));\n            }\n        }\n\n        factorials = _mm256_and_si256(factorials, even_mask);\n        sums = _mm256_and_si256(sums, odd_mask);\n\n        factorials = _mm256_and_si256(factorials, _mm256_set1_epi32(9999));\n        sums = _mm256_and_si256(sums, _mm256_set1_epi32(9999));\n\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(indices, j);\n            if (idx >= 0 && idx < n) {\n                result[idx] = _mm256_extract_epi32(factorials, j);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i indices = _mm256_set_epi32(n-1, n-2, n-3, n-4, n-5, n-6, n-7, n-8);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i mask = _mm256_cmpgt_epi32(indices, zero);\n        __m256i even_mask = _mm256_and_si256(mask, _mm256_cmpeq_epi32(_mm256_and_si256(indices, _mm256_set1_epi32(1)), zero));\n        __m256i odd_mask = _mm256_andnot_si256(even_mask, mask);\n\n        __m256i factorials = _mm256_setzero_si256();\n        __m256i sums = _mm256_setzero_si256();\n\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(indices, j);\n            if (idx % 2 == 0) {\n                __m256i fact = _mm256_set1_epi32(idx);\n                for (int k = idx - 1; k > 1; --k) {\n                    fact = _mm256_mullo_epi32(fact, _mm256_set1_epi32(k));\n                }\n                factorials = _mm256_add_epi32(factorials, _mm256_and_si256(even_mask, fact));\n            } else {\n                __m256i sum = _mm256_set1_epi32((idx * (idx + 1)) / 2);\n                sums = _mm256_add_epi32(sums, _mm256_and_si256(odd_mask, sum));\n            }\n        }\n\n        for (int j = 0; j < 8; ++j) {\n            int idx = _mm256_extract_epi32(indices, j);\n            if (idx % 2 == 0) {\n                result[idx] = _mm256_extract_epi32(factorials, j) % 10000;\n            } else {\n                result[idx] = _mm256_extract_epi32(sums, j);\n            }\n        }\n\n        indices = _mm256_sub_epi32(indices, _mm256_set1_epi32(8));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i indices = _mm256_set_epi32(n-1, n-2, n-3, n-4, n-5, n-6, n-7, n-8);\n    __m256i factorials = _mm256_setzero_si256();\n    __m256i sums = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i mask = _mm256_cmpgt_epi32(indices, _mm256_set1_epi32(i));\n        __m256i current_indices = _mm256_and_si256(mask, indices);\n\n        __m256i factorial = _mm256_set1_epi32(1);\n        for (int j = 1; j <= 8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i temp = _mm256_mullo_epi32(factorial, j_vec);\n            factorial = temp;\n        }\n        factorials = _mm256_add_epi32(factorials, _mm256_and_si256(mask, factorial));\n\n        __m256i sum = _mm256_set1_epi32(0);\n        for (int j = 1; j <= 8; ++j) {\n            __m256i j_vec = _mm256_set1_epi32(j);\n            __m256i temp = _mm256_add_epi32(sum, j_vec);\n            sum = temp;\n        }\n        sums = _mm256_add_epi32(sums, _mm256_andnot_si256(mask, sum));\n\n        indices = _mm256_sub_epi32(indices, _mm256_set1_epi32(8));\n    }\n\n    for (int i = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            result[i] = _mm256_extract_epi32(factorials, i / 4) % 10000;\n        } else {\n            result[i] = _mm256_extract_epi32(sums, i / 4);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i v_index = _mm256_setr_epi32(1, 2, 3, 4);\n    __m256i v_fact = _mm256_setzero_si256();\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i v_mask = _mm256_and_si256(v_index, _mm256_set1_epi32(1));\n        __m256i v_even = _mm256_andnot_si256(v_mask, v_index);\n\n        // Calculate factorial for even indices\n        __m256i v_fact_temp = v_fact;\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi32(v_even, j);\n            if (idx % 2 == 0 && idx > 0) {\n                v_fact_temp = _mm256_mullo_epi32(v_fact_temp, _mm256_set1_epi32(idx));\n            }\n        }\n        v_fact = v_fact_temp;\n\n        // Calculate sum for odd indices\n        __m256i v_sum_temp = v_sum;\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi32(v_odd, j);\n            if (idx % 2 != 0) {\n                v_sum_temp = _mm256_add_epi32(v_sum_temp, _mm256_set1_epi32(idx));\n            }\n        }\n        v_sum = v_sum_temp;\n\n        // Store results\n        for (int j = 0; j < 4; ++j) {\n            int idx = _mm256_extract_epi32(v_index, j);\n            if (idx % 2 == 0) {\n                result[idx - 1] = _mm256_extract_epi32(v_fact, j) % 10000;\n            } else {\n                result[idx - 1] = _mm256_extract_epi32(v_sum, j);\n            }\n        }\n\n        v_index = _mm256_add_epi32(v_index, _mm256_set1_epi32(4));\n    }\n\n    // Handle remaining elements\n    for (int i = n / 8 * 8; i < n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact *= j;\n            }\n            result[i] = fact % 10000;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= i; ++j) {\n                sum += j;\n            }\n            result[i] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i count_even = _mm256_setzero_si256();\n    __m256i count_odd = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                count_even = _mm256_add_epi32(count_even, _mm256_set1_epi32(1));\n            } else {\n                count_odd = _mm256_add_epi32(count_odd, _mm256_set1_epi32(1));\n            }\n        }\n    }\n\n    int even = 0, odd = 0;\n    _mm256_storeu_si256((__m256i*)&even, count_even);\n    _mm256_storeu_si256((__m256i*)&odd, count_odd);\n\n    return {even, odd};\n}\n\nbool is_palindrome(int num) {\n    std::string str = std::to_string(num);\n    int len = str.length();\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i count_even = _mm256_setzero_si256();\n    __m256i count_odd = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                count_even = _mm256_add_epi32(count_even, _mm256_set1_epi32(1));\n            } else {\n                count_odd = _mm256_add_epi32(count_odd, _mm256_set1_epi32(1));\n            }\n        }\n    }\n\n    int even_count[8];\n    int odd_count[8];\n\n    _mm256_storeu_si256((__m256i*)even_count, count_even);\n    _mm256_storeu_si256((__m256i*)odd_count, count_odd);\n\n    int total_even = 0;\n    int total_odd = 0;\n\n    for (int i = 0; i < 8; ++i) {\n        total_even += even_count[i];\n        total_odd += odd_count[i];\n    }\n\n    return {total_even, total_odd};\n}\n\nbool is_palindrome(int num) {\n    std::string str = std::to_string(num);\n    int len = str.length();\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i count_even = _mm256_setzero_si256();\n    __m256i count_odd = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                count_even = _mm256_add_epi32(count_even, _mm256_set1_epi32(1));\n            } else {\n                count_odd = _mm256_add_epi32(count_odd, _mm256_set1_epi32(1));\n            }\n        }\n    }\n\n    int result_even[8];\n    int result_odd[8];\n\n    _mm256_storeu_si256((__m256i*)result_even, count_even);\n    _mm256_storeu_si256((__m256i*)result_odd, count_odd);\n\n    std::vector<int> result;\n    for (int i = 0; i < 8; ++i) {\n        result.push_back(result_even[i]);\n        result.push_back(result_odd[i]);\n    }\n\n    return result;\n}\n\nbool is_palindrome(int num) {\n    // Implement your palindrome check here\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i count = _mm256_setzero_si256();\n    int even = 0, odd = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                even += 1;\n            } else {\n                odd += 1;\n            }\n        }\n    }\n\n    std::vector<int> result = {even, odd};\n    return result;\n}\n\nbool is_palindrome(int num) {\n    char str[12];\n    sprintf(str, \"%d\", num);\n    int len = strlen(str);\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m256i count_even = _mm256_setzero_si256();\n    __m256i count_odd = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                count_even = _mm256_add_epi32(count_even, _mm256_set1_epi32(1));\n            } else {\n                count_odd = _mm256_add_epi32(count_odd, _mm256_set1_epi32(1));\n            }\n        }\n    }\n\n    int even_count[8];\n    int odd_count[8];\n\n    _mm256_storeu_si256((__m256i*)even_count, count_even);\n    _mm256_storeu_si256((__m256i*)odd_count, count_odd);\n\n    int total_even = 0;\n    int total_odd = 0;\n\n    for (int i = 0; i < 8; ++i) {\n        total_even += even_count[i];\n        total_odd += odd_count[i];\n    }\n\n    return {total_even, total_odd};\n}\n\nbool is_palindrome(int num) {\n    std::string str = std::to_string(num);\n    int len = str.length();\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    const int* data = nums.data();\n    int size = nums.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i abs_vec = _mm256_andnot_si256(_mm256_cmpgt_epi32(vec, zero), vec);\n        __m256i sign_vec = _mm256_sign_epi32(vec, abs_vec);\n        __m256i sum_vec = _mm256_add_epi32(abs_vec, sign_vec);\n        __m256i gt_zero = _mm256_cmpgt_epi32(sum_vec, zero);\n        int mask = _mm256_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_mask = _mm256_cmpgt_epi32(zero, _mm256_and_si256(nums.data(), _mm256_set1_epi32(0x80000000)));\n\n    for (size_t i = 0; i + 7 < nums.size(); i += 8) {\n        __m256i num_vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_num_vec = _mm256_or_si256(num_vec, neg_mask);\n        __m256i sign_vec = _mm256_and_si256(neg_mask, _mm256_set1_epi32(0x80000000));\n        __m256i digit_sum_vec = _mm256_setzero_si256();\n\n        for (int j = 0; j < 4; ++j) {\n            __m256i temp = abs_num_vec;\n            temp = _mm256_add_epi32(temp, _mm256_srai_epi32(temp, 16));\n            temp = _mm256_add_epi32(temp, _mm256_srli_epi32(temp, 8));\n            temp = _mm256_add_epi32(temp, _mm256_srli_epi32(temp, 4));\n            temp = _mm256_add_epi32(temp, _mm256_srli_epi32(temp, 2));\n            temp = _mm256_add_epi32(temp, _mm256_srli_epi32(temp, 1));\n            digit_sum_vec = _mm256_add_epi32(digit_sum_vec, temp);\n\n            abs_num_vec = _mm256_and_si256(abs_num_vec, _mm256_set1_epi32(0x7FFFFFFF));\n            sign_vec = _mm256_and_si256(sign_vec, _mm256_set1_epi32(0x7FFFFFFF));\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(digit_sum_vec, zero);\n        count += _mm256_movemask_epi8(_mm256_castsi256_si128(mask)) & 0xFF;\n        count += _mm256_movemask_epi8(_mm256_extractf128_si256(mask, 1)) & 0xFF;\n    }\n\n    for (size_t i = nums.size() - nums.size() % 8; i < nums.size(); ++i) {\n        if (nums[i] != 0 && (nums[i] >= 1 || (nums[i] < 0 && (nums[i] & 0x7FFFFFFF) != 0))) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i neg_one = _mm256_set1_epi32(-1);\n\n    for (size_t i = 0; i + 7 < nums.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_vec = _mm256_andnot_si256(_mm256_cmpgt_epi32(vec, zero), vec);\n        __m256i sign_vec = _mm256_sign_epi32(vec, neg_one);\n        __m256i sum_vec = _mm256_add_epi32(abs_vec, sign_vec);\n        __m256i gt_zero = _mm256_cmpgt_epi32(sum_vec, zero);\n        int mask = _mm256_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 8; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_mask = _mm256_cmpgt_epi32(zero, _mm256_and_si256(nums.data(), _mm256_set1_epi32(0x80000000)));\n\n    for (size_t i = 0; i < nums.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_vec = _mm256_or_si256(_mm256_andnot_si256(neg_mask, vec), _mm256_and_si256(neg_mask, _mm256_add_epi32(vec, ones)));\n        __m256i sum_vec = _mm256_sad_epu8(abs_vec, zero);\n        __m256i gt_zero = _mm256_cmpgt_epi32(sum_vec, zero);\n        int mask = _mm256_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_mask = _mm256_cmpgt_epi32(zero, _mm256_and_si256(nums.data(), _mm256_set1_epi32(0x80000000)));\n\n    for (size_t i = 0; i < nums.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_vec = _mm256_or_si256(vec, neg_mask);\n        __m256i mod_vec = _mm256_mod_epi32(abs_vec, ones);\n        __m256i sum_vec = _mm256_add_epi32(mod_vec, _mm256_srli_epi32(abs_vec, 1));\n        __m256i gt_zero = _mm256_cmpgt_epi32(sum_vec, zero);\n        count += _mm256_movemask_epi8(gt_zero);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m256i vecA, vecB;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        vecA = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vecB = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(vecA, vecB);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask == 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m256i vec1, vec2;\n    bool is_sorted = true;\n\n    for (int i = 0; i < N - 1; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&arr[i + 8]);\n\n        __m256i cmp1 = _mm256_cmpgt_epi32(vec1, vec2);\n        __m256i cmp2 = _mm256_cmpgt_epi32(_mm256_srli_si256(vec1, 4), vec1);\n\n        if (_mm256_movemask_epi8(cmp1) != 0 || _mm256_movemask_epi8(cmp2) != 0) {\n            is_sorted = false;\n            break;\n        }\n    }\n\n    return is_sorted;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m256i vec1, vec2, vec3;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&arr[(i + 1) % N]);\n\n        vec3 = _mm256_cmpgt_epi32(vec1, vec2);\n        int mask = _mm256_movemask_epi8(vec3);\n\n        if (mask == 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m256i vecA, vecB, vecC;\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < N - 7; i += 8) {\n        vecA = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vecB = _mm256_loadu_si256((__m256i*)&arr[i + 8]);\n\n        vecC = _mm256_cmpgt_epi32(vecA, vecB);\n        if (_mm256_movemask_epi8(vecC) != 0) return false;\n\n        vecC = _mm256_cmpgt_epi32(vecB, _mm256_permutevar8x32_epi32(vecB, vecB));\n        if (_mm256_movemask_epi8(vecC) != 0) return false;\n    }\n\n    for (int i = N - 8; i < N; ++i) {\n        if (i > 0 && arr[i] < arr[i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m256i vecA, vecB;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        vecA = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vecB = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(vecA, vecB);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask == 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m256i v_lst1 = _mm256_loadu_si256((__m256i*)&lst1[0]);\n    __m256i v_lst2 = _mm256_loadu_si256((__m256i*)&lst2[0]);\n\n    __m256i v_mask = _mm256_and_si256(v_lst1, _mm256_set1_epi32(1));\n    __m256i v_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n    int count_odd = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_even, _mm256_setzero_si256()));\n\n    if (count_odd == 0) {\n        return \"YES\";\n    }\n\n    __m256i v_lst2_mask = _mm256_and_si256(v_lst2, _mm256_set1_epi32(1));\n    __m256i v_lst2_even = _mm256_cmpeq_epi32(v_lst2_mask, _mm256_setzero_si256());\n\n    int count_even = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_lst2_even, _mm256_setzero_si256()));\n\n    if (count_even >= count_odd) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string exchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m256i v_lst1 = _mm256_loadu_si256((__m256i*)&lst1[0]);\n    __m256i v_lst2 = _mm256_loadu_si256((__m256i*)&lst2[0]);\n\n    __m256i v_mask = _mm256_and_si256(v_lst1, _mm256_set1_epi32(1));\n    __m256i v_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n    int count_odd = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_even, _mm256_setzero_si256()));\n\n    if (count_odd == 0) {\n        return \"YES\";\n    }\n\n    __m256i v_lst2_mask = _mm256_and_si256(v_lst2, _mm256_set1_epi32(1));\n    __m256i v_lst2_even = _mm256_cmpeq_epi32(v_lst2_mask, _mm256_setzero_si256());\n\n    int count_even = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_lst2_even, _mm256_setzero_si256()));\n\n    if (count_even >= count_odd) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string exchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m256i v_lst1 = _mm256_loadu_si256((__m256i*)&lst1[0]);\n    __m256i v_lst2 = _mm256_loadu_si256((__m256i*)&lst2[0]);\n\n    __m256i v_mask = _mm256_and_si256(v_lst1, _mm256_set1_epi32(1));\n    __m256i v_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n    int count_odd = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_even, _mm256_setzero_si256()));\n\n    if (count_odd == 0) {\n        return \"YES\";\n    }\n\n    __m256i v_lst2_mask = _mm256_and_si256(v_lst2, _mm256_set1_epi32(1));\n    __m256i v_lst2_even = _mm256_cmpeq_epi32(v_lst2_mask, _mm256_setzero_si256());\n\n    int count_even_in_lst2 = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_lst2_even, _mm256_setzero_si256()));\n\n    if (count_even_in_lst2 >= count_odd) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m256i v_lst1 = _mm256_loadu_si256((__m256i*)&lst1[0]);\n    __m256i v_lst2 = _mm256_loadu_si256((__m256i*)&lst2[0]);\n\n    __m256i v_mask = _mm256_and_si256(v_lst1, _mm256_set1_epi32(1));\n    __m256i v_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n    int count_odd = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_even, _mm256_setzero_si256()));\n\n    if (count_odd == 0) {\n        return \"YES\";\n    }\n\n    __m256i v_lst2_mask = _mm256_and_si256(v_lst2, _mm256_set1_epi32(1));\n    __m256i v_lst2_even = _mm256_cmpeq_epi32(v_lst2_mask, _mm256_setzero_si256());\n\n    int count_even = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_lst2_even, _mm256_setzero_si256()));\n\n    if (count_even >= count_odd) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    __m256i v_lst1 = _mm256_loadu_si256((__m256i*)&lst1[0]);\n    __m256i v_lst2 = _mm256_loadu_si256((__m256i*)&lst2[0]);\n\n    __m256i v_mask = _mm256_and_si256(v_lst1, _mm256_set1_epi32(1));\n    __m256i v_even = _mm256_cmpeq_epi32(v_mask, _mm256_setzero_si256());\n\n    int count_odd = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_even, _mm256_setzero_si256()));\n\n    if (count_odd == 0) {\n        return \"YES\";\n    }\n\n    __m256i v_lst2_mask = _mm256_and_si256(v_lst2, _mm256_set1_epi32(1));\n    __m256i v_lst2_even = _mm256_cmpeq_epi32(v_lst2_mask, _mm256_setzero_si256());\n\n    int count_even = _mm256_movemask_epi8(_mm256_cmpneq_epi32(v_lst2_even, _mm256_setzero_si256()));\n\n    if (count_even >= count_odd) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m256i count = _mm256_setzero_si256();\n        int len = str.size();\n        const char* ptr = str.c_str();\n\n        while (len >= 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0xF));\n            __m256i odd_mask = _mm256_cmpeq_epi8(_mm256_and_si256(mask, _mm256_set1_epi8(1)), _mm256_setzero_si256());\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n            ptr += 32;\n            len -= 32;\n        }\n\n        while (len > 0) {\n            if ((ptr[0] - '0') % 2 != 0) {\n                count = _mm256_add_epi32(count, _mm256_set1_epi32(1));\n            }\n            ptr++;\n            len--;\n        }\n\n        int total_odd = _mm256_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m256i count = _mm256_setzero_si256();\n        int len = str.size();\n        const char* ptr = str.c_str();\n\n        while (len >= 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(mask, _mm256_set1_epi8(1));\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n            ptr += 32;\n            len -= 32;\n        }\n\n        while (len > 0) {\n            if ((ptr[0] - '0') % 2 != 0) {\n                count = _mm256_add_epi32(count, _mm256_set1_epi32(1));\n            }\n            ptr++;\n            len--;\n        }\n\n        int total_odd = _mm256_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m256i count = _mm256_setzero_si256();\n        int len = str.size();\n        const char* ptr = str.c_str();\n\n        while (len >= 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ptr));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(mask, _mm256_set1_epi8(1));\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(_mm256_andnot_si256(odd_mask, mask), _mm256_setzero_si256()));\n            ptr += 32;\n            len -= 32;\n        }\n\n        while (len > 0) {\n            if ((ptr[0] - '0') % 2 != 0) {\n                count = _mm256_add_epi32(count, _mm256_set1_epi32(1));\n            }\n            ptr++;\n            len--;\n        }\n\n        int total_odd = _mm256_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m256i count = _mm256_setzero_si256();\n        int len = str.size();\n        for (int i = 0; i <= len - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(_mm256_and_si256(mask, _mm256_set1_epi8(0x01)), _mm256_setzero_si256());\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n        }\n        int remaining = len % 8;\n        if (remaining > 0) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + len - remaining));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(_mm256_and_si256(mask, _mm256_set1_epi8(0x01)), _mm256_setzero_si256());\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n        }\n        int total_odd = _mm256_extract_epi32(count, 0);\n        std::string temp = \"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\";\n        result.push_back(temp);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m256i count = _mm256_setzero_si256();\n        int len = str.size();\n        for (int i = 0; i <= len - 8; i += 8) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.c_str() + i));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(_mm256_and_si256(mask, _mm256_set1_epi8(0x01)), _mm256_setzero_si256());\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n        }\n        int remaining = len % 8;\n        if (remaining > 0) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.c_str() + len - remaining));\n            __m256i mask = _mm256_and_si256(data, _mm256_set1_epi8(0x0F));\n            __m256i odd_mask = _mm256_cmpeq_epi8(_mm256_and_si256(mask, _mm256_set1_epi8(0x01)), _mm256_setzero_si256());\n            count = _mm256_add_epi32(count, _mm256_sad_epu8(odd_mask, _mm256_setzero_si256()));\n        }\n        int total_odd = _mm256_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i min_sum_vec = _mm256_set1_epi64x(nums[0]);\n    __m256i current_min_vec = _mm256_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m256i num_vec = _mm256_set1_epi64x(nums[i]);\n        __m256i temp_vec = _mm256_add_epi64(current_min_vec, num_vec);\n        current_min_vec = _mm256_min_epi64(temp_vec, num_vec);\n\n        min_sum_vec = _mm256_min_epi64(min_sum_vec, current_min_vec);\n    }\n\n    int64_t min_sum = INT64_MAX;\n    for (int i = 0; i < 4; ++i) {\n        int64_t value = _mm256_extract_epi64(min_sum_vec, i);\n        if (value < min_sum) {\n            min_sum = value;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i min_sum = _mm256_set1_epi64x(nums[0]);\n    __m256i current_sum = _mm256_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m256i num = _mm256_set1_epi64x(nums[i]);\n        __m256i temp = _mm256_min_epi64(current_sum, _mm256_add_epi64(current_sum, num));\n        current_sum = _mm256_min_epi64(min_sum, temp);\n        min_sum = _mm256_min_epi64(min_sum, current_sum);\n    }\n\n    int64_t result;\n    _mm256_storeu_si256((__m256i*)&result, min_sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i min_sum_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_sum_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i < n; i += 4) {\n        __m256i num_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Update current_sum_vec\n        current_sum_vec = _mm256_min_epi64(current_sum_vec, num_vec);\n\n        // Update min_sum_vec\n        min_sum_vec = _mm256_min_epi64(min_sum_vec, current_sum_vec);\n    }\n\n    int64_t min_sum[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(min_sum), min_sum_vec);\n\n    int64_t result = INT64_MAX;\n    for (int i = 0; i < 4; ++i) {\n        result = std::min(result, min_sum[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t min_sum = INT64_MAX;\n    __m256i min_sum_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min_vec = _mm256_setzero_si256();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    size_t n = nums.size();\n    size_t i = 0;\n\n    while (i + 7 < n) {\n        __m256i nums_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        current_min_vec = _mm256_add_epi64(current_min_vec, nums_vec);\n        current_min_vec = _mm256_min_epi64(current_min_vec, zero_vec);\n        min_sum_vec = _mm256_min_epi64(min_sum_vec, current_min_vec);\n\n        i += 8;\n    }\n\n    int64_t remaining_sum[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(remaining_sum), min_sum_vec);\n\n    for (; i < n; ++i) {\n        min_sum = std::min(min_sum, static_cast<int64_t>(nums[i]) + min_sum);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i min_sum_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min_vec = _mm256_set1_epi64x(0);\n\n    for (size_t i = 0; i < n; i += 4) {\n        __m256i nums_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        __m256i temp_vec = _mm256_add_epi64(current_min_vec, nums_vec);\n        current_min_vec = _mm256_min_epi64(temp_vec, nums_vec);\n\n        min_sum_vec = _mm256_min_epi64(min_sum_vec, current_min_vec);\n    }\n\n    int64_t min_sum[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(min_sum), min_sum_vec);\n\n    int64_t result = INT64_MAX;\n    for (int j = 0; j < 4; ++j) {\n        result = std::min(result, min_sum[j]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int vec_size = 8;\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        for (size_t i = 0; i + vec_size - 1 < row.size(); i += vec_size) {\n            __m256i row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int sum[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum), sum_vec);\n\n        for (int s : sum) {\n            total_buckets += (s + capacity - 1) / capacity;\n        }\n\n        size_t remaining = row.size() % vec_size;\n        if (remaining > 0) {\n            __m256i partial_row_vec = _mm256_setzero_si256();\n            for (size_t i = 0; i < remaining; ++i) {\n                partial_row_vec = _mm256_insert_epi32(partial_row_vec, row[row.size() - remaining + i], i);\n            }\n            int partial_sum[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(partial_sum), partial_row_vec);\n\n            for (int p : partial_sum) {\n                total_buckets += (p + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int vec_size = 8;\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        for (size_t i = 0; i + vec_size - 1 < row.size(); i += vec_size) {\n            __m256i row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int sum[vec_size];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum), sum_vec);\n\n        for (int s : sum) {\n            total_buckets += (s + capacity - 1) / capacity;\n        }\n\n        size_t remaining = row.size() % vec_size;\n        if (remaining > 0) {\n            __m256i partial_row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[row.size() - remaining]));\n            __m256i mask = _mm256_set1_epi32((1 << remaining) - 1);\n            partial_row_vec = _mm256_and_si256(partial_row_vec, mask);\n            int partial_sum[vec_size];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(partial_sum), partial_row_vec);\n\n            for (int i = 0; i < remaining; ++i) {\n                total_buckets += (partial_sum[i] + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int vec_size = 8;\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        for (size_t i = 0; i + vec_size - 1 < row.size(); i += vec_size) {\n            __m256i row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n        int sum = 0;\n        for (int j = 0; j < vec_size; ++j) {\n            sum += _mm256_extract_epi32(sum_vec, j);\n        }\n\n        size_t remaining = row.size() % vec_size;\n        if (remaining > 0) {\n            __m256i partial_row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[row.size() - remaining]));\n            __m256i mask = _mm256_set1_epi32((1 << remaining) - 1);\n            partial_row_vec = _mm256_and_si256(partial_row_vec, mask);\n            sum_vec = _mm256_add_epi32(sum_vec, partial_row_vec);\n            sum = 0;\n            for (int j = 0; j < vec_size; ++j) {\n                sum += _mm256_extract_epi32(sum_vec, j);\n            }\n        }\n\n        total_buckets += (sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int vec_size = 8; // AVX2 vector size\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        __m256i sum_vec = _mm256_setzero_si256();\n        int row_sum = 0;\n\n        for (size_t i = 0; i + vec_size - 1 < row.size(); i += vec_size) {\n            __m256i row_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum_vec = _mm256_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t i = row.size() - row.size() % vec_size; i < row.size(); ++i) {\n            row_sum += row[i];\n        }\n\n        int local_sum = _mm256_reduce_add_epi32(sum_vec);\n        local_sum += row_sum;\n        total_buckets += (local_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int vec_size = 8;\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        __m256i sum = _mm256_setzero_si256();\n        for (size_t i = 0; i + vec_size - 1 < row.size(); i += vec_size) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            sum = _mm256_add_epi32(sum, vec);\n        }\n        int remainder = row.size() % vec_size;\n        if (remainder > 0) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[row.size() - remainder]));\n            vec = _mm256_maskload_epi32(vec, _mm256_set1_epi32((1 << remainder) - 1));\n            sum = _mm256_add_epi32(sum, vec);\n        }\n\n        int local_sum = 0;\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&local_sum), sum);\n\n        total_buckets += (local_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 16; i += 16) {\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i veven_mask = _mm256_and_si256(vdata, _mm256_set1_epi16(0x0001));\n        __m256i vodd_mask = _mm256_and_si256(_mm256_srli_epi16(vdata, 1), _mm256_set1_epi16(0x0001));\n        __m256i vresult = _mm256_add_epi16(veven_mask, vodd_mask);\n        vsum = _mm256_add_epi32(vsum, _mm256_cvtepi16_epi32(vresult));\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 16; i += 16) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask = _mm256_and_si256(vec, _mm256_set1_epi16(1));\n        __m256i even_mask = _mm256_and_si256(_mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1), _mm256_srli_epi16(mask, 15));\n        __m256i result = _mm256_mullo_epi16(vec, even_mask);\n        vsum = _mm256_add_epi32(vsum, _mm256_srai_epi16(result, 15));\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 16; i += 16) {\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i veven_mask = _mm256_and_si256(vdata, _mm256_set1_epi16(0x0001));\n        __m256i vodd_mask = _mm256_and_si256(_mm256_srli_epi16(vdata, 1), _mm256_set1_epi16(0x0001));\n        __m256i vresult = _mm256_add_epi16(veven_mask, vodd_mask);\n        vsum = _mm256_add_epi32(vsum, _mm256_sad_epu16(vresult, _mm256_setzero_si256()));\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = n - n % 16; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 16; i += 16) {\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i vodd = _mm256_and_si256(vdata, _mm256_set1_epi16(1));\n        __m256i veven = _mm256_and_si256(_mm256_srli_epi16(vdata, 1), _mm256_set1_epi16(1));\n        __m256i vresult = _mm256_add_epi16(vodd, veven);\n        vsum = _mm256_add_epi32(vsum, _mm256_cvtepi16_epi32(vresult));\n    }\n\n    int32_t temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 16; i < n; ++i) {\n        if ((i & 1) == 0 && (lst[i] & 1) != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask = _mm256_and_si256(vec, _mm256_set1_epi16(1));\n        __m256i even_mask = _mm256_and_si256(_mm256_setr_epi32(0, 1, 0, 1), _mm256_setr_epi32(0, 0, 0, 0));\n        __m256i result = _mm256_add_epi16(_mm256_mullo_epi16(mask, vec), vsum);\n        vsum = _mm256_add_epi16(result, _mm256_and_si256(even_mask, vec));\n    }\n\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (size_t i = 0; i < 8; ++i) {\n        if ((temp[i] & 1) == 0 && (i % 2 == 0)) {\n            sum += temp[i];\n        }\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        if ((lst[i] & 1) == 0 && (i % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi16(_mm256_and_si256(vec, _mm256_set1_epi16(99)), _mm256_setzero_si256());\n        __m256i masked_vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi16(_mm256_and_si256(vec, _mm256_set1_epi16(99)), _mm256_setzero_si256());\n        vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; i++) {\n        if (temp[i] > 99) continue;\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi16(_mm256_and_si256(vec, _mm256_set1_epi16(99)), _mm256_setzero_si256());\n        __m256i masked_vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi16(_mm256_and_si256(vec, _mm256_set1_epi16(99)), _mm256_setzero_si256());\n        __m256i masked_vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, masked_vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi16(_mm256_and_si256(vec, _mm256_set1_epi16(99)), _mm256_setzero_si256());\n        vec = _mm256_and_si256(vec, mask);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr = _mm256_setzero_si256();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        prev = curr;\n        curr = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Check remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        if (i > 0 && lst[i] < lst[i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m256i prev = _mm256_setzero_si256();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr = _mm256_setzero_si256();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        prev = curr;\n        curr = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Check remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        if (i > 0 && lst[i] < lst[i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m256i prev = _mm256_setzero_si256();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, current);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr = _mm256_setzero_si256();\n\n    for (int i = 0; i + 7 < n; i += 8) {\n        prev = curr;\n        curr = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n        if (_mm256_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign_product = _mm256_setzero_si256();\n\n    int n = arr.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        sign_mask = _mm256_and_si256(sign_mask, _mm256_set1_epi32(-1));\n        sign_product = _mm256_mullo_epi32(sign_product, sign_mask);\n\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < n) {\n        int val = arr[i];\n        sum = _mm256_add_epi32(sum, _mm256_set1_epi32(abs(val)));\n        sign_product = _mm256_mullo_epi32(sign_product, _mm256_set1_epi32((val > 0) ? -1 : ((val < 0) ? 1 : 0)));\n\n        i++;\n    }\n\n    // Sum up the results from the AVX registers\n    int result_sum = 0;\n    int result_sign = 0;\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result_sum), sum);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result_sign), sign_product);\n\n    // Combine the results\n    int final_result = result_sum * result_sign;\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int sum = 0;\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i + 8 <= arr.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec);\n        __m256i sign_vec = _mm256_cmpgt_epi32(vec, zero);\n        __m256i sign_product = _mm256_mullo_epi32(sign_vec, one);\n        __m256i magnitude_sum = _mm256_add_epi32(abs_vec, zero);\n\n        sum += _mm256_reduce_add_epi32(magnitude_sum);\n        sum *= _mm256_movemask_epi8(sign_product);\n    }\n\n    for (size_t i = arr.size() - arr.size() % 8; i < arr.size(); ++i) {\n        sum += std::abs(arr[i]) * (arr[i] > 0 ? 1 : (arr[i] < 0 ? -1 : 0));\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign_product = _mm256_setzero_si256();\n\n    int n = arr.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        sign_mask = _mm256_and_si256(sign_mask, _mm256_cmplt_epi32(vec, _mm256_setzero_si256()));\n        sign_product = _mm256_or_si256(sign_product, _mm256_and_si256(sign_mask, _mm256_set1_epi32(-1)));\n\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < n) {\n        int val = arr[i];\n        sum = _mm256_add_epi32(sum, _mm256_set1_epi32(abs(val)));\n        if (val != 0) {\n            sign_product = _mm256_or_si256(sign_product, _mm256_set1_epi32((val < 0) ? -1 : 1));\n        }\n        i++;\n    }\n\n    // Sum up the results from the AVX registers\n    int result_sum = 0;\n    int result_sign = 1;\n    for (int j = 0; j < 8; j++) {\n        int value = _mm256_extract_epi32(sum, j);\n        result_sum += value;\n        if (_mm256_extract_epi32(sign_product, j) != 0) {\n            result_sign *= _mm256_extract_epi32(sign_product, j);\n        }\n    }\n\n    return result_sum * result_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n    __m256i zero = _mm256_setzero_si256();\n\n    int n = arr.size();\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i sign_bits = _mm256_and_si256(vec, sign_mask);\n        __m256i count_negatives = _mm256_cmpeq_epi32(sign_bits, sign_mask);\n        __m256i count_positives = _mm256_cmpeq_epi32(sign_bits, zero);\n        __m256i count_zeros = _mm256_cmpeq_epi32(sign_bits, zero);\n\n        __m256i neg_count = _mm256_movemask_epi8(count_negatives);\n        __m256i pos_count = _mm256_movemask_epi8(count_positives);\n        __m256i zero_count = _mm256_movemask_epi8(count_zeros);\n\n        int total_negatives = __builtin_popcount(neg_count);\n        int total_positives = __builtin_popcount(pos_count);\n        int total_zeros = __builtin_popcount(zero_count);\n\n        if (total_zeros > 0) {\n            return 0;\n        } else if ((total_negatives % 2) != 0) {\n            sum = _mm256_sub_epi32(sum, sum);\n        }\n    }\n\n    for (; i < n; ++i) {\n        int val = arr[i];\n        if (val == 0) {\n            return 0;\n        } else if (val < 0) {\n            sum = _mm256_sub_epi32(sum, _mm256_set1_epi32(val));\n        } else {\n            sum = _mm256_add_epi32(sum, _mm256_set1_epi32(val));\n        }\n    }\n\n    int result = 0;\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result), sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign_product = _mm256_setzero_si256();\n\n    int n = arr.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        sign_product = _mm256_and_si256(sign_product, sign_mask);\n        sign_product = _mm256_or_si256(sign_product, _mm256_cmpeq_epi32(vec, _mm256_setzero_si256()));\n\n        i += 8;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = arr[i];\n        sum = _mm256_add_epi32(sum, _mm256_set1_epi32(abs(val)));\n        sign_product = _mm256_and_si256(sign_product, _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_set1_epi32(val)));\n        sign_product = _mm256_or_si256(sign_product, _mm256_cmpeq_epi32(_mm256_set1_epi32(val), _mm256_setzero_si256()));\n    }\n\n    // Sum up the results from the AVX registers\n    int result_sum = 0;\n    int result_sign = 1;\n    for (int j = 0; j < 8; ++j) {\n        result_sum += _mm256_extract_epi32(sum, j);\n        result_sign *= (_mm256_extract_epi32(sign_product, j) != 0 ? 1 : -1);\n    }\n\n    return result_sum * result_sign;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    __m256i min_path = _mm256_set1_epi32(INT_MAX);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3);\n\n    // Iterate over all possible starting points\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            __m256i path = _mm256_set1_epi32(grid[i][j]);\n            __m256i idx = _mm256_setr_epi32(i, j, i, j);\n\n            // Iterate over the remaining steps\n            for (int step = 1; step < k; ++step) {\n                __m256i min_val = _mm256_set1_epi32(INT_MAX);\n                __m256i min_idx = _mm256_setzero_si256();\n\n                // Check all neighbors\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (di == 0 && dj == 0) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i val = _mm256_set1_epi32(grid[ni][nj]);\n                            __m256i cond = _mm256_cmpgt_epi32(min_val, val);\n                            min_val = _mm256_blendv_epi32(min_val, val, cond);\n                            min_idx = _mm256_blendv_epi32(min_idx, _mm256_add_epi32(idx, _mm256_setr_epi32(di, dj, di, dj)), cond);\n                        }\n                    }\n                }\n\n                path = _mm256_add_epi32(path, min_val);\n                idx = min_idx;\n            }\n\n            // Update the minimum path\n            __m256i cond = _mm256_cmpgt_epi32(min_path, path);\n            min_path = _mm256_blendv_epi32(min_path, path, cond);\n            indices = _mm256_blendv_epi32(indices, idx, cond);\n        }\n    }\n\n    // Extract the result\n    std::vector<int> result(k);\n    for (int i = 0; i < k; ++i) {\n        int idx = _mm256_extract_epi32(indices, i % 4);\n        result[i] = grid[idx / N][idx % N];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    std::vector<int> result(k);\n\n    __m256i min_path = _mm256_set1_epi32(INT_MAX);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3);\n\n    for (int row = 0; row < N; ++row) {\n        for (int col = 0; col < N; ++col) {\n            __m256i current_value = _mm256_set1_epi32(grid[row][col]);\n            __m256i mask = _mm256_cmpgt_epi32(min_path, current_value);\n            min_path = _mm256_blendv_epi32(min_path, current_value, mask);\n            indices = _mm256_blendv_epi32(indices, _mm256_setr_epi32(row, col, row, col), mask);\n        }\n    }\n\n    for (int i = 0; i < k; ++i) {\n        int idx = _mm256_extract_epi32(indices, i % 4);\n        result[i] = grid[idx / N][idx % N];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    __m256i min_path = _mm256_set1_epi32(INT_MAX);\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3);\n\n    // Initialize min_path with the first k elements of the grid\n    for (int i = 0; i < k; ++i) {\n        for (int j = 0; j < N; ++j) {\n            for (int l = 0; l < N; ++l) {\n                __m256i val = _mm256_set1_epi32(grid[j][l]);\n                __m256i mask = _mm256_cmpgt_epi32(min_path, val);\n                min_path = _mm256_blendv_epi32(min_path, val, mask);\n                indices = _mm256_blendv_epi32(indices, _mm256_set1_epi32(j * N + l), mask);\n            }\n        }\n    }\n\n    std::vector<int> result(k);\n    for (int i = 0; i < k; ++i) {\n        result[i] = grid[indices[i] / N][indices[i] % N];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your AVX/AVX2 implementation here\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your implementation here\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x0F);\n    __m256i result = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_cvtsi64_si256(n & 0xFFFFFFFFFFFFFFFF);\n        digits = _mm256_and_si256(digits, mask);\n\n        __m256i is_odd = _mm256_cmpgt_epi8(digits, zero);\n        __m256i odd_digits = _mm256_mullo_epi8(digits, is_odd);\n\n        result = _mm256_add_epi8(result, odd_digits);\n\n        n >>= 8;\n    }\n\n    uint8_t sum[32];\n    _mm256_storeu_si256((__m256i*)sum, result);\n\n    uint64_t total = 1;\n    bool has_odd = false;\n    for (int i = 0; i < 32; ++i) {\n        if (sum[i] != 0) {\n            total *= sum[i];\n            has_odd = true;\n        }\n    }\n\n    return has_odd ? total : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x0F);\n    __m256i result = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_cvtsi64_si256(n & 0xFFFFFFFFFFFFFFFF);\n        digits = _mm256_and_si256(digits, mask);\n\n        __m256i is_odd = _mm256_cmpgt_epi8(digits, zero);\n        __m256i odd_digits = _mm256_blendv_epi8(zero, digits, is_odd);\n\n        result = _mm256_mullo_epi8(result, odd_digits);\n\n        n >>= 64;\n    }\n\n    uint64_t final_result[32];\n    _mm256_storeu_si256((__m256i*)final_result, result);\n\n    uint64_t prod = 1;\n    for (int i = 0; i < 32; ++i) {\n        prod *= final_result[i];\n    }\n\n    return prod;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x55); // Mask to extract odd digits\n    __m256i result = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_cvtsi64_si256(n);\n        digits = _mm256_and_si256(digits, mask);\n        digits = _mm256_cmpgt_epi8(digits, zero);\n        digits = _mm256_and_si256(digits, _mm256_set1_epi8(0xFF));\n        digits = _mm256_mullo_epi8(digits, _mm256_set1_epi8(1));\n        result = _mm256_add_epi8(result, digits);\n\n        n /= 10;\n    }\n\n    uint64_t sum = 0;\n    for (int i = 0; i < 32; i += 8) {\n        sum += _mm256_extract_epi8(result, i);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x55); // Mask to extract odd digits (bitwise AND with 0x55)\n    __m256i result = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_cvtsi64_si256(n);\n        digits = _mm256_and_si256(digits, mask); // Extract odd digits\n        digits = _mm256_cmpgt_epi8(digits, zero); // Compare with zero to get boolean mask\n        digits = _mm256_and_si256(digits, _mm256_set1_epi8(0xFF)); // Convert boolean mask to actual values\n        result = _mm256_add_epi8(result, digits); // Sum up the odd digits\n        n /= 10;\n    }\n\n    uint8_t sum[32];\n    _mm256_storeu_si256((__m256i*)sum, result);\n\n    uint64_t total = 1;\n    bool hasOdd = false;\n    for (int i = 0; i < 32; i++) {\n        if (sum[i] != 0) {\n            total *= sum[i];\n            hasOdd = true;\n        }\n    }\n\n    return hasOdd ? total : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x0F);\n    __m256i result = _mm256_setzero_si256();\n\n    while (n > 0) {\n        __m256i digits = _mm256_cvtsi64_si256(n & 0xFFFFFFFFFFFFFFFF);\n        digits = _mm256_and_si256(digits, mask);\n        __m256i odd_mask = _mm256_cmpgt_epi8(digits, zero);\n        __m256i odd_digits = _mm256_blendv_epi8(zero, digits, odd_mask);\n        result = _mm256_mullo_epi8(result, odd_digits);\n        n >>= 64;\n    }\n\n    uint64_t final_result[32];\n    _mm256_storeu_si256((__m256i*)final_result, result);\n\n    uint64_t prod = 1;\n    for (int i = 0; i < 32; ++i) {\n        prod *= final_result[i];\n    }\n\n    return prod;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    size_t n = lst.size();\n    size_t i = 0;\n\n    __m256 vsum = _mm256_setzero_ps();\n\n    while (i + 8 <= n) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sq_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        vsum = _mm256_add_ps(vsum, sq_vec);\n        i += 8;\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (; i < n; ++i) {\n        float num = lst[i];\n        num = ceil(num);\n        sum += static_cast<uint64_t>(num * num);\n    }\n\n    return sum + static_cast<uint64_t>(temp[0] + temp[1] + temp[2] + temp[3] +\n                                      temp[4] + temp[5] + temp[6] + temp[7]);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const size_t n = lst.size();\n    uint64_t sum = 0;\n\n    __m256 vsum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sq_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        vsum = _mm256_add_ps(vsum, sq_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        float num = lst[i];\n        num = std::ceil(num);\n        sum += static_cast<uint64_t>(num * num);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t n = lst.size();\n    size_t i = 0;\n\n    __m256 vsum = _mm256_setzero_ps();\n\n    while (i + 8 <= n) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sq_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        vsum = _mm256_add_ps(vsum, sq_vec);\n        i += 8;\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (; i < n; ++i) {\n        float num = lst[i];\n        num = ceil(num);\n        sum += static_cast<uint64_t>(num * num);\n    }\n\n    return sum + static_cast<uint64_t>(temp[0] + temp[1] + temp[2] + temp[3] +\n                                        temp[4] + temp[5] + temp[6] + temp[7]);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    size_t n = lst.size();\n    size_t i = 0;\n\n    __m256 vsum = _mm256_setzero_ps();\n\n    while (i + 8 <= n) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sq_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        vsum = _mm256_add_ps(vsum, sq_vec);\n        i += 8;\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (; i < n; ++i) {\n        float val = ceil(lst[i]);\n        sum += static_cast<uint64_t>(val * val);\n    }\n\n    return sum + static_cast<uint64_t>(temp[0] + temp[1] + temp[2] + temp[3] +\n                                       temp[4] + temp[5] + temp[6] + temp[7]);\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const size_t n = lst.size();\n    uint64_t sum = 0;\n\n    __m256 vsum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256 sq_vec = _mm256_mul_ps(ceil_vec, ceil_vec);\n        vsum = _mm256_add_ps(vsum, sq_vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(temp[j]);\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        float num = lst[i];\n        num = std::ceil(num);\n        sum += static_cast<uint64_t>(num * num);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) return -1;\n\n    int n = arr.size();\n    __m256i v0, v1;\n    int idx = -1;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        v0 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        v1 = _mm256_loadu_si256((__m256i*)&arr[i + 8]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(v0, v1);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            idx = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n    }\n\n    if (idx == -1) {\n        for (int i = n - 8; i < n - 1; ++i) {\n            if (arr[i] > arr[i + 1]) {\n                idx = i;\n                break;\n            }\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m256i vec1, vec2, mask;\n    int result = -1;\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        vec2 = _mm256_loadu_si256((__m256i*)&arr[i + 8]);\n\n        mask = _mm256_cmpgt_epi32(vec1, vec2);\n        int mask_value = _mm256_movemask_epi8(mask);\n\n        if (mask_value != 0) {\n            int idx = __builtin_ctz(mask_value) / 4 + i;\n            result = idx;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[0]));\n    __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[8]));\n\n    for (int i = 8; i < n; i += 32) {\n        __m256i v3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(v2, v3);\n        v0 = _mm256_or_si256(v0, cmp);\n        v1 = v2;\n        v2 = v3;\n    }\n\n    int result = -1;\n    for (int i = 0; i < 32; ++i) {\n        if (_mm256_extract_epi32(v0, i)) {\n            result = i + 7;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    const int* data = arr.data();\n    int size = arr.size();\n\n    if (size < 2) {\n        return -1;\n    }\n\n    __m256i v_prev = _mm256_setzero_si256();\n    __m256i v_curr = _mm256_loadu_si256((__m256i*)(data + 0));\n    __m256i v_mask = _mm256_setzero_si256();\n\n    for (int i = 1; i <= size - 2; i += 8) {\n        __m256i v_next = _mm256_loadu_si256((__m256i*)(data + i + 1));\n\n        __m256i v_cmp = _mm256_cmpgt_epi32(v_next, v_prev);\n        v_mask = _mm256_or_si256(v_mask, v_cmp);\n\n        v_prev = v_curr;\n        v_curr = v_next;\n    }\n\n    int result = -1;\n    for (int i = 0; i < 8; ++i) {\n        int idx = i * 8 + 1;\n        if ((v_mask.m256i_i32[i] & 0x80000000) != 0) {\n            result = idx;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    const int* data = arr.data();\n\n    __m256i v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data));\n    __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + 8));\n\n    int idx = -1;\n\n    for (int i = 16; i < n; i += 32) {\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i cmp = _mm256_cmpgt_epi32(v2, v1);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            idx = i + __builtin_ctz(mask) / 4;\n            break;\n        }\n\n        v0 = v1;\n        v1 = v2;\n    }\n\n    if (idx == -1) {\n        __m256i last = _mm256_set1_epi32(arr[n - 2]);\n        __m256i cmp_last = _mm256_cmpgt_epi32(last, v1);\n        int mask_last = _mm256_movemask_epi8(cmp_last);\n\n        if (mask_last != 0) {\n            idx = n - 2;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i max_val = _mm256_set1_epi32(std::numeric_limits<int>::lowest());\n\n    int n = lst.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        min_val = _mm256_min_epi32(min_val, vec);\n        max_val = _mm256_max_epi32(max_val, vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val < 0 && val > _mm256_extract_epi32(min_val, 0)) {\n            min_val = _mm256_set1_epi32(val);\n        }\n        if (val > 0 && val < _mm256_extract_epi32(max_val, 0)) {\n            max_val = _mm256_set1_epi32(val);\n        }\n    }\n\n    int min_result = _mm256_extract_epi32(min_val, 0);\n    int max_result = _mm256_extract_epi32(max_val, 0);\n\n    return {min_result, max_result};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i max_val = _mm256_set1_epi32(std::numeric_limits<int>::min());\n\n    int n = lst.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        min_val = _mm256_min_epi32(min_val, vec);\n        max_val = _mm256_max_epi32(max_val, vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < n) {\n        int val = lst[i];\n        min_val = _mm256_min_epi32(min_val, _mm256_set1_epi32(val));\n        max_val = _mm256_max_epi32(max_val, _mm256_set1_epi32(val));\n        i++;\n    }\n\n    int min_result[8], max_result[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(min_result), min_val);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(max_result), max_val);\n\n    int min_val_final = *std::min_element(min_result, min_result + 8);\n    int max_val_final = *std::min_element(max_result, max_result + 8);\n\n    return {min_val_final, max_val_final};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i max_val = _mm256_set1_epi32(std::numeric_limits<int>::lowest());\n\n    int n = lst.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        min_val = _mm256_min_epi32(min_val, vec);\n        max_val = _mm256_max_epi32(max_val, vec);\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < n) {\n        int val = lst[i];\n        if (val < 0 && val > _mm256_extract_epi32(min_val, 0)) {\n            min_val = _mm256_set1_epi32(val);\n        }\n        if (val > 0 && val < _mm256_extract_epi32(max_val, 0)) {\n            max_val = _mm256_set1_epi32(val);\n        }\n        i++;\n    }\n\n    int min_result = _mm256_extract_epi32(min_val, 0);\n    int max_result = _mm256_extract_epi32(max_val, 0);\n\n    return {min_result, max_result};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i max_val = _mm256_set1_epi32(std::numeric_limits<int>::min());\n\n    int n = lst.size();\n    int i = 0;\n\n    // Process elements in chunks of 8\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask_neg = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i mask_pos = _mm256_cmpge_epi32(_mm256_setzero_si256(), vec);\n\n        min_val = _mm256_min_epi32(min_val, _mm256_and_si256(mask_neg, vec));\n        max_val = _mm256_max_epi32(max_val, _mm256_and_si256(mask_pos, vec));\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val < 0 && val > _mm256_extract_epi32(min_val, 0)) {\n            min_val = _mm256_set1_epi32(val);\n        }\n        if (val > 0 && val < _mm256_extract_epi32(max_val, 0)) {\n            max_val = _mm256_set1_epi32(val);\n        }\n    }\n\n    int min_result = _mm256_extract_epi32(min_val, 0);\n    int max_result = _mm256_extract_epi32(max_val, 0);\n\n    return {min_result, max_result};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.size() % 8 != 0) {\n        // Handle non-multiple-of-8 case separately\n        std::vector<int> result(2, 0);\n        for (int num : lst) {\n            if (num < 0 && num > result[0]) {\n                result[0] = num;\n            } else if (num > 0 && num < result[1]) {\n                result[1] = num;\n            }\n        }\n        return result;\n    }\n\n    int* data = const_cast<int*>(&lst[0]);\n    __m256i min_neg = _mm256_set1_epi32(INT_MIN);\n    __m256i max_pos = _mm256_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n\n        __m256i neg_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_mask = _mm256_cmpge_epi32(_mm256_setzero_si256(), vec);\n\n        __m256i neg_values = _mm256_and_si256(neg_mask, vec);\n        __m256i pos_values = _mm256_and_si256(pos_mask, vec);\n\n        min_neg = _mm256_min_epi32(min_neg, neg_values);\n        max_pos = _mm256_min_epi32(max_pos, pos_values);\n    }\n\n    int result[2];\n    _mm256_storeu_si256((__m256i*)result, min_neg);\n    _mm256_storeu_si256((__m256i*)(result + 1), max_pos);\n\n    return std::vector<int>{result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i sum = _mm256_setzero_si256();\n    for (uint64_t i = 1; i <= n; ++i) {\n        __m256i fact = _mm256_set1_epi64x(i);\n        for (int j = 1; j < i; ++j) {\n            fact = _mm256_mul_epi64(fact, _mm256_set1_epi64x(j));\n        }\n        sum = _mm256_add_epi64(sum, fact);\n    }\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) return 1;\n\n    uint64_t result = 1;\n    __m256i vec_result = _mm256_setzero_si256();\n    __m256i vec_fact = _mm256_setzero_si256();\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        vec_fact = _mm256_set_epi64x(fact, fact, fact, fact);\n        vec_result = _mm256_add_epi64(vec_result, vec_fact);\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        result += _mm256_extract_epi64(vec_result, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i result = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i two = _mm256_set1_epi32(2);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        __m256i factorial = _mm256_set1_epi32(i);\n        for (int j = 0; j < 8; ++j) {\n            factorial = _mm256_mullo_epi32(factorial, _mm256_sub_epi32(_mm256_add_epi32(_mm256_set1_epi32(j), one), two));\n        }\n        result = _mm256_add_epi64(result, _mm256_cvtepi32_epi64(factorial));\n    }\n\n    uint64_t sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum += _mm256_extract_epi64(result, i);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) return 1;\n\n    uint64_t result = 1;\n    __m256i vec_result = _mm256_setzero_si256();\n    __m256i vec_fact = _mm256_setzero_si256();\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n\n        vec_fact = _mm256_set_epi64x(fact, fact, fact, fact);\n        vec_result = _mm256_add_epi64(vec_result, vec_fact);\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        result += _mm256_extract_epi64(vec_result, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 1;\n\n    uint64_t result = 1;\n    __m256i vec_result = _mm256_setzero_si256();\n    __m256i vec_fact = _mm256_setzero_si256();\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        vec_fact = _mm256_set_epi64x(fact, fact, fact, fact);\n        vec_result = _mm256_add_epi64(vec_result, vec_fact);\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        result += _mm256_extract_epi64(vec_result, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    int n = lst.size();\n\n    for(int i = 0; i <= n - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask1 = _mm256_and_si256(_mm256_set1_epi32(3), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask2 = _mm256_and_si256(_mm256_set1_epi32(4), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask3 = _mm256_andnot_si256(mask2, mask1);\n\n        __m256i squares = _mm256_mullo_epi32(vec, vec);\n        __m256i cubes = _mm256_mullo_epi32(squares, vec);\n        __m256i result = _mm256_add_epi32(squares, _mm256_add_epi32(cubes, _mm256_and_si256(mask3, squares)));\n\n        vsum = _mm256_add_epi32(vsum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(int i = n - n % 8; i < n; ++i){\n        if(i % 3 == 0){\n            sum += lst[i] * lst[i];\n        } else if(i % 4 == 0){\n            sum += lst[i] * lst[i] * lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask3 = _mm256_and_si256(_mm256_set1_epi32(3), _mm256_setr_epi32(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7));\n        __m256i mask4 = _mm256_and_si256(_mm256_set1_epi32(4), _mm256_setr_epi32(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7));\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(squared, vec);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_andnot_si256(mask4, squared),\n            _mm256_and_si256(mask3, cubed)\n        );\n\n        vsum = _mm256_add_epi32(vsum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        if (i + j < n) {\n            sum += temp[j];\n        }\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for(size_t i = 0; i <= n - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_and_si256(_mm256_set1_epi32(3), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask4 = _mm256_and_si256(_mm256_set1_epi32(4), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask_not4 = _mm256_xor_si256(mask4, _mm256_cmpeq_epi32(mask4, _mm256_setzero_si256()));\n\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(squared, vec);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(mask3, squared),\n            _mm256_and_si256(_mm256_andnot_si256(mask4, mask_not4), cubed)\n        );\n\n        vsum = _mm256_add_epi32(vsum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for(int j = 0; j < 8; ++j){\n        sum += temp[j];\n    }\n\n    for(size_t i = n - n % 8; i < n; ++i){\n        int val = lst[i];\n        if(i % 3 == 0){\n            sum += val * val;\n        } else if(i % 4 == 0){\n            sum += val * val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    size_t n = lst.size();\n\n    for(size_t i = 0; i <= n - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask1 = _mm256_and_si256(_mm256_set1_epi32(0x00000003), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask2 = _mm256_and_si256(_mm256_set1_epi32(0x0000000C), _mm256_setr_epi32(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7));\n        __m256i mask3 = _mm256_cmpeq_epi32(mask2, _mm256_setzero_si256());\n        __m256i squares = _mm256_mullo_epi32(vec, vec);\n        __m256i cubes = _mm256_mullo_epi32(squares, vec);\n        squares = _mm256_blendv_epi32(squares, cubes, mask3);\n        vsum = _mm256_add_epi32(vsum, squares);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n\n    for(size_t i = 0; i < 8; ++i){\n        if(i < n % 8){\n            sum += temp[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    __m256i vsum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        __m256i mask3 = _mm256_and_si256(_mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0), _mm256_set1_epi32(3));\n        __m256i mask4 = _mm256_and_si256(_mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0), _mm256_set1_epi32(4));\n\n        __m256i squares = _mm256_mullo_epi32(vec, vec);\n        __m256i cubes = _mm256_mullo_epi32(squares, vec);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_andnot_si256(mask3, squares),\n            _mm256_and_si256(mask4, cubes)\n        );\n\n        vsum = _mm256_add_epi32(vsum, result);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            sum += value * value;\n        } else if (i % 4 == 0) {\n            sum += value * value * value;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    const int* data = nums.data();\n    size_t size = nums.size();\n\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i mask_odd_first_last = _mm256_set1_epi32(0x55555555);\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n\n        // Check if elements are greater than 10\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m256i abs_vec = _mm256_andnot_si256(_mm256_cmplt_epi32(vec, _mm256_setzero_si256()), vec);\n        __m256i last_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(9));\n        __m256i first_digit = _mm256_srli_epi32(last_digit, 32 - 4); // Shift right by 4 bits\n\n        // Check if first and last digits are odd\n        __m256i odd_check = _mm256_and_si256(first_digit | last_digit, mask_odd_first_last);\n\n        // Combine conditions\n        __m256i result = _mm256_and_si256(gt_threshold, odd_check);\n\n        // Count true values\n        int local_count = _mm256_movemask_epi8(result);\n        count += __builtin_popcount(local_count);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (data[i] > 10 && ((abs(data[i]) / 1000) % 2 != 0 && (abs(data[i]) % 10) % 2 != 0)) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    const int* data = nums.data();\n    size_t size = nums.size();\n\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i mask_odd_first_last = _mm256_set1_epi32(0x55555555); // Binary: 01010101...\n\n    for (size_t i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n\n        // Check if elements are greater than 10\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m256i abs_vec = _mm256_andnot_si256(_mm256_cmplt_epi32(vec, _mm256_setzero_si256()), vec);\n        __m256i first_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(0xF));\n        __m256i last_digit = _mm256_srli_epi32(abs_vec, 28);\n\n        // Check if first and last digits are odd\n        __m256i odd_first_last = _mm256_and_si256(first_digit, mask_odd_first_last);\n        __m256i odd_last = _mm256_and_si256(last_digit, mask_odd_first_last);\n\n        // Combine conditions\n        __m256i combined = _mm256_and_si256(gt_threshold, _mm256_and_si256(odd_first_last, odd_last));\n\n        // Count true values\n        int local_count = _mm256_movemask_epi8(combined);\n        count += __builtin_popcount(local_count);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 8; i < size; ++i) {\n        if (data[i] > 10 && ((abs(data[i]) / 1000000000) % 2 != 0) && ((abs(data[i]) / 10) % 2 != 0)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    const int* data = nums.data();\n    int size = nums.size();\n\n    __m256i mask_gt_10 = _mm256_set1_epi32(10);\n    __m256i mask_odd_first_last = _mm256_set1_epi32(0x55555555);\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n\n        // Check if elements are greater than 10\n        __m256i gt_10 = _mm256_cmpgt_epi32(vec, mask_gt_10);\n\n        // Extract first and last digits\n        __m256i first_digits = _mm256_and_si256(_mm256_srai_epi32(vec, 28), mask_odd_first_last);\n        __m256i last_digits = _mm256_and_si256(_mm256_and_si256(vec, _mm256_set1_epi32(0xF)), mask_odd_first_last);\n\n        // Combine conditions\n        __m256i combined = _mm256_and_si256(gt_10, _mm256_and_si256(first_digits, last_digits));\n\n        // Count true values\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, combined);\n        for (int j = 0; j < 8; ++j) {\n            count += (temp[j] != 0);\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 8; i < size; ++i) {\n        if (data[i] > 10 && ((data[i] / 1000000000) % 2 != 0 || (data[i] / 10) % 2 != 0)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i mask_odd_first_last = _mm256_set1_epi32(0x55555555); // Binary: 01010101...\n\n    for (size_t i = 0; i + 8 <= nums.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Check if elements are greater than 10\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m256i abs_vec = _mm256_andnot_si256(_mm256_cmplt_epi32(vec, _mm256_setzero_si256()), vec);\n        __m256i last_digit = _mm256_and_si256(abs_vec, _mm256_set1_epi32(9));\n        __m256i first_digit = _mm256_srli_epi32(abs_vec, 28);\n\n        // Check if first and last digits are odd\n        __m256i odd_first_last = _mm256_and_si256(first_digit, mask_odd_first_last);\n        odd_first_last = _mm256_and_si256(last_digit, mask_odd_first_last);\n\n        // Combine conditions\n        __m256i combined = _mm256_and_si256(gt_threshold, odd_first_last);\n\n        // Count true values\n        int local_count = _mm256_movemask_epi8(combined);\n        count += __builtin_popcount(local_count);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 8; i < nums.size(); ++i) {\n        if (nums[i] > 10 && ((abs(nums[i]) / 1000000000) % 2 != 0 && (abs(nums[i]) / 10) % 2 != 0)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    const int *data = nums.data();\n    int size = nums.size();\n\n    __m256i v_threshold = _mm256_set1_epi32(10);\n    __m256i v_mask_odd = _mm256_set1_epi32(0x55555555); // Binary: 01010101...\n\n    for (int i = 0; i <= size - 8; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i *)&data[i]);\n\n        // Check if numbers are greater than 10\n        __m256i v_greater_than_10 = _mm256_cmpgt_epi32(v_nums, v_threshold);\n\n        // Extract first and last digits\n        __m256i v_first_digit = _mm256_and_si256(_mm256_srai_epi32(v_nums, 28), v_mask_odd);\n        __m256i v_last_digit = _mm256_and_si256(_mm256_and_si256(v_nums, _mm256_set1_epi32(0xF)), v_mask_odd);\n\n        // Combine conditions\n        __m256i v_combined = _mm256_and_si256(v_greater_than_10, _mm256_and_si256(v_first_digit, v_last_digit));\n\n        // Count true values\n        int mask[8];\n        _mm256_storeu_si256((__m256i *)mask, v_combined);\n        for (int j = 0; j < 8; ++j) {\n            count += (mask[j] != 0);\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 8; i < size; ++i) {\n        if (data[i] > 10 && ((data[i] / 1000000000) % 2 != 0) && ((data[i] / 10) % 2 != 0)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t chunk_size = 8;\n    uint64_t count = 0;\n\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi64x(1);\n    __m256i v_three = _mm256_set1_epi64x(3);\n\n    for (uint64_t i = 0; i <= n - chunk_size; i += chunk_size) {\n        __m256i v_i = _mm256_add_epi64(_mm256_mullo_epi64(v_i, v_i), v_one);\n        __m256i v_j = _mm256_add_epi64(_mm256_mullo_epi64(v_j, v_j), v_one);\n        __m256i v_k = _mm256_add_epi64(_mm256_mullo_epi64(v_k, v_k), v_one);\n\n        __m256i v_sum = _mm256_add_epi64(_mm256_add_epi64(v_i, v_j), v_k);\n        __m256i v_mod = _mm256_and_epi64(_mm256_srai_epi64(_mm256_sub_epi64(v_sum, v_zero), 63), v_three);\n\n        __m256i v_mask = _mm256_cmpeq_epi64(v_mod, v_zero);\n        uint64_t mask_value = _mm256_movemask_epi8(v_mask);\n        count += __builtin_popcountll(mask_value);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t size = n / 8;\n    const uint64_t remainder = n % 8;\n\n    __m256i sum_mod_3 = _mm256_setzero_si256();\n    __m256i count_mod_3[3] = {_mm256_setzero_si256(), _mm256_setzero_si256(), _mm256_setzero_si256()};\n\n    for (uint64_t i = 0; i < size; ++i) {\n        __m256i indices = _mm256_setr_epi64x(i * 8 + 1, i * 8 + 2, i * 8 + 3, i * 8 + 4,\n                                            i * 8 + 5, i * 8 + 6, i * 8 + 7, i * 8 + 8);\n        __m256i values = _mm256_mullo_epi64(_mm256_sub_epi64(indices, _mm256_set1_epi64x(1)), indices);\n        values = _mm256_add_epi64(values, _mm256_set1_epi64x(1));\n\n        __m256i mod_3 = _mm256_and_epi64(values, _mm256_set1_epi64x(2));\n        mod_3 = _mm256_cmpgt_epi64(mod_3, _mm256_setzero_si256());\n        mod_3 = _mm256_and_si256(mod_3, _mm256_set1_epi64x(2));\n\n        sum_mod_3 = _mm256_add_epi64(sum_mod_3, mod_3);\n\n        for (int j = 0; j < 3; ++j) {\n            __m256i mask = _mm256_cmpeq_epi64(mod_3, _mm256_set1_epi64x(j));\n            count_mod_3[j] = _mm256_add_epi64(count_mod_3[j], mask);\n        }\n    }\n\n    uint64_t result = 0;\n    for (int j = 0; j < 3; ++j) {\n        result += _mm256_extract_epi64(count_mod_3[j], 0);\n        result += _mm256_extract_epi64(count_mod_3[j], 1);\n        result += _mm256_extract_epi64(count_mod_3[j], 2);\n        result += _mm256_extract_epi64(count_mod_3[j], 3);\n    }\n\n    for (uint64_t i = 0; i < remainder; ++i) {\n        uint64_t index = size * 8 + i + 1;\n        uint64_t value = index * index - index + 1;\n        uint64_t mod_3 = value % 3;\n        if (mod_3 == 0) {\n            result++;\n        } else if (mod_3 == 1) {\n            result += count_mod_3[2][0];\n        } else if (mod_3 == 2) {\n            result += count_mod_3[1][0];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t size = n / 8;\n    const uint64_t remainder = n % 8;\n\n    __m256i count = _mm256_setzero_si256();\n    __m256i sum_mod_3 = _mm256_setzero_si256();\n\n    for (uint64_t i = 0; i < size; ++i) {\n        __m256i indices = _mm256_setr_epi64x(i * 8 + 1, i * 8 + 2, i * 8 + 3, i * 8 + 4,\n                                            i * 8 + 5, i * 8 + 6, i * 8 + 7, i * 8 + 8);\n        __m256i values = _mm256_mullo_epi64(_mm256_sub_epi64(indices, _mm256_set1_epi64x(1)), indices);\n        values = _mm256_add_epi64(values, _mm256_set1_epi64x(1));\n\n        __m256i mod_3 = _mm256_and_si256(values, _mm256_set1_epi64x(2));\n        mod_3 = _mm256_add_epi64(mod_3, _mm256_srli_epi64(values, 1));\n        mod_3 = _mm256_and_si256(mod_3, _mm256_set1_epi64x(2));\n\n        sum_mod_3 = _mm256_add_epi64(sum_mod_3, mod_3);\n\n        __m256i mask = _mm256_cmpgt_epi64(mod_3, _mm256_setzero_si256());\n        count = _mm256_add_epi64(count, _mm256_and_si256(mask, _mm256_set1_epi64x(1)));\n    }\n\n    uint64_t result = _mm256_extract_epi64(count, 0) +\n                      _mm256_extract_epi64(count, 1) +\n                      _mm256_extract_epi64(count, 2) +\n                      _mm256_extract_epi64(count, 3);\n\n    for (uint64_t i = 0; i < remainder; ++i) {\n        uint64_t index = size * 8 + i + 1;\n        uint64_t value = index * index - index + 1;\n        uint64_t mod_3 = (value & 2) + ((value >> 1) & 2);\n        if (mod_3 > 0) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t chunk_size = 8;\n    uint64_t count = 0;\n\n    __m256i v_i = _mm256_setzero_si256();\n    __m256i v_j = _mm256_setzero_si256();\n    __m256i v_k = _mm256_setzero_si256();\n\n    for (uint64_t i = 0; i < n; i += chunk_size) {\n        __m256i v_a = _mm256_setzero_si256();\n        for (uint64_t j = 0; j < n; j += chunk_size) {\n            __m256i v_b = _mm256_setzero_si256();\n            for (uint64_t k = 0; k < n; k += chunk_size) {\n                __m256i v_c = _mm256_setzero_si256();\n                for (uint64_t ii = 0; ii < chunk_size && i + ii < n; ++ii) {\n                    v_a = _mm256_add_epi64(v_a, _mm256_set1_epi64x((i + ii) * (i + ii) - (i + ii) + 1));\n                }\n                for (uint64_t jj = 0; jj < chunk_size && j + jj < n; ++jj) {\n                    v_b = _mm256_add_epi64(v_b, _mm256_set1_epi64x((j + jj) * (j + jj) - (j + jj) + 1));\n                }\n                for (uint64_t kk = 0; kk < chunk_size && k + kk < n; ++kk) {\n                    v_c = _mm256_add_epi64(v_c, _mm256_set1_epi64x((k + kk) * (k + kk) - (k + kk) + 1));\n                }\n\n                __m256i v_sum = _mm256_add_epi64(_mm256_add_epi64(v_a, v_b), v_c);\n                __m256i v_mod = _mm256_and_si256(v_sum, _mm256_set1_epi64x(2));\n\n                __m256i v_mask = _mm256_cmpeq_epi64(v_mod, _mm256_setzero_si256());\n                uint64_t mask_value = _mm256_movemask_epi8(v_mask);\n\n                while (mask_value != 0) {\n                    if ((mask_value & 1) == 1) {\n                        count++;\n                    }\n                    mask_value >>= 1;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t size = n / 8;\n    const uint64_t remainder = n % 8;\n\n    __m256i sum_mod_3 = _mm256_setzero_si256();\n    __m256i count_mod_3[3] = {_mm256_setzero_si256(), _mm256_setzero_si256(), _mm256_setzero_si256()};\n\n    for (uint64_t i = 0; i < size; ++i) {\n        __m256i indices = _mm256_setr_epi64x(i * 8 + 1, i * 8 + 2, i * 8 + 3, i * 8 + 4,\n                                            i * 8 + 5, i * 8 + 6, i * 8 + 7, i * 8 + 8);\n        __m256i values = _mm256_mullo_epi64(indices, _mm256_sub_epi64(indices, _mm256_set1_epi64x(1)));\n        values = _mm256_add_epi64(values, _mm256_set1_epi64x(1));\n\n        __m256i mod_3 = _mm256_and_epi64(values, _mm256_set1_epi64x(2));\n        mod_3 = _mm256_add_epi64(mod_3, _mm256_and_epi64(values, _mm256_set1_epi64x(1)));\n\n        for (int j = 0; j < 3; ++j) {\n            __m256i mask = _mm256_cmpeq_epi64(mod_3, _mm256_set1_epi64x(j));\n            count_mod_3[j] = _mm256_add_epi64(count_mod_3[j], _mm256_and_si256(mask, _mm256_set1_epi64x(1)));\n        }\n\n        sum_mod_3 = _mm256_add_epi64(sum_mod_3, mod_3);\n    }\n\n    uint64_t total_count = 0;\n    for (int j = 0; j < 3; ++j) {\n        total_count += _mm256_reduce_add_epi64(count_mod_3[j]);\n    }\n\n    if (remainder > 0) {\n        __m256i indices = _mm256_setzero_si256();\n        for (uint64_t i = 0; i < remainder; ++i) {\n            indices = _mm256_insert_epi64(indices, i + size * 8 + 1, i);\n        }\n        __m256i values = _mm256_mullo_epi64(indices, _mm256_sub_epi64(indices, _mm256_set1_epi64x(1)));\n        values = _mm256_add_epi64(values, _mm256_set1_epi64x(1));\n\n        __m256i mod_3 = _mm256_and_epi64(values, _mm256_set1_epi64x(2));\n        mod_3 = _mm256_add_epi64(mod_3, _mm256_and_epi64(values, _mm256_set1_epi64x(1)));\n\n        for (int j = 0; j < 3; ++j) {\n            __m256i mask = _mm256_cmpeq_epi64(mod_3, _mm256_set1_epi64x(j));\n            total_count += _mm256_reduce_add_epi64(_mm256_and_si256(mask, _mm256_set1_epi64x(1)));\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_x = _mm256_set1_epi32(x);\n    __m256i v_y = _mm256_set1_epi32(y);\n\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n    __m256i v_four = _mm256_set1_epi32(4);\n    __m256i v_five = _mm256_set1_epi32(5);\n    __m256i v_six = _mm256_set1_epi32(6);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    __m256i v_eight = _mm256_set1_epi32(8);\n    __m256i v_nine = _mm256_set1_epi32(9);\n    __m256i v_ten = _mm256_set1_epi32(10);\n\n    __m256i v_is_prime = _mm256_cmpgt_epi32(v_n, v_one);\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmplt_epi32(v_n, v_ten));\n\n    __m256i v_not_prime = _mm256_andnot_si256(v_is_prime, _mm256_set1_epi32(-1));\n\n    __m256i v_result = _mm256_blendv_epi32(v_x, v_y, v_not_prime);\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_x = _mm256_set1_epi32(x);\n    __m256i v_y = _mm256_set1_epi32(y);\n\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n    __m256i v_four = _mm256_set1_epi32(4);\n    __m256i v_five = _mm256_set1_epi32(5);\n    __m256i v_six = _mm256_set1_epi32(6);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    __m256i v_eight = _mm256_set1_epi32(8);\n    __m256i v_nine = _mm256_set1_epi32(9);\n\n    __m256i v_mask = _mm256_cmpgt_epi32(v_n, v_zero);\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_one));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_two));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_three));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_four));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_five));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_six));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_seven));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_eight));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_nine));\n\n    __m256i v_result = _mm256_blendv_epi8(v_x, v_y, v_mask);\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n\n    for (int i = 0; i < 8; ++i) {\n        if (result[i] != 0) {\n            return result[i];\n        }\n    }\n\n    return 0;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_x = _mm256_set1_epi32(x);\n    __m256i v_y = _mm256_set1_epi32(y);\n\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n    __m256i v_four = _mm256_set1_epi32(4);\n    __m256i v_five = _mm256_set1_epi32(5);\n    __m256i v_six = _mm256_set1_epi32(6);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    __m256i v_eight = _mm256_set1_epi32(8);\n    __m256i v_nine = _mm256_set1_epi32(9);\n\n    __m256i v_mask = _mm256_cmpgt_epi32(v_n, v_zero);\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_one));\n\n    for (int i = 0; i < 8; ++i) {\n        __m256i v_divisor = _mm256_set1_epi32(i + 2);\n        __m256i v_remainder = _mm256_rem_epi32(v_n, v_divisor);\n        v_mask = _mm256_and_si256(v_mask, _mm256_cmpeq_epi32(v_remainder, v_zero));\n    }\n\n    __m256i v_result = _mm256_blendv_epi32(v_x, v_y, v_mask);\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_x = _mm256_set1_epi32(x);\n    __m256i v_y = _mm256_set1_epi32(y);\n\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n    __m256i v_four = _mm256_set1_epi32(4);\n    __m256i v_five = _mm256_set1_epi32(5);\n    __m256i v_six = _mm256_set1_epi32(6);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    __m256i v_eight = _mm256_set1_epi32(8);\n    __m256i v_nine = _mm256_set1_epi32(9);\n    __m256i v_ten = _mm256_set1_epi32(10);\n\n    __m256i v_is_prime = _mm256_cmpgt_epi32(v_n, v_one);\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_two));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_three));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_four));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_five));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_six));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_seven));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_eight));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_nine));\n    v_is_prime = _mm256_and_si256(v_is_prime, _mm256_cmpgt_epi32(v_n, v_ten));\n\n    __m256i v_result = _mm256_blendv_epi8(v_x, v_y, v_is_prime);\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_x = _mm256_set1_epi32(x);\n    __m256i v_y = _mm256_set1_epi32(y);\n\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n    __m256i v_four = _mm256_set1_epi32(4);\n    __m256i v_five = _mm256_set1_epi32(5);\n    __m256i v_six = _mm256_set1_epi32(6);\n    __m256i v_seven = _mm256_set1_epi32(7);\n    __m256i v_eight = _mm256_set1_epi32(8);\n    __m256i v_nine = _mm256_set1_epi32(9);\n    __m256i v_ten = _mm256_set1_epi32(10);\n\n    __m256i v_mask = _mm256_cmpgt_epi32(v_n, v_zero);\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_one));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_two));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_three));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_four));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_five));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_six));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_seven));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_eight));\n    v_mask = _mm256_and_si256(v_mask, _mm256_cmpgt_epi32(v_n, v_nine));\n\n    __m256i v_result = _mm256_blendv_epi32(v_x, v_y, v_mask);\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, v_result);\n\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    __m256 zero = _mm256_setzero_ps();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF); // Mask to ignore negative numbers\n    uint64_t sum = 0;\n\n    size_t n = lst.size();\n    size_t i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 abs_vec = _mm256_and_ps(vec, mask);\n        __m256i int_vec = _mm256_cvtps_epi32(abs_vec);\n        __m256i mod_vec = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i eq_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n        __m256 mul_vec = _mm256_mul_ps(vec, vec);\n        __m256 masked_mul_vec = _mm256_and_ps(mul_vec, _mm256_castsi256_ps(eq_vec));\n        __m256 sum_vec = _mm256_add_ps(masked_mul_vec, zero);\n        float result[8];\n        _mm256_storeu_ps(result, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += static_cast<uint64_t>(result[j]);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] >= 0 && fmod(lst[i], 2.0f) == 0) {\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    __m256 vzero = _mm256_setzero_ps();\n    __m256 viota = _mm256_set_ps(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256 vsqr = _mm256_set_ps(1, 1, 1, 1, 1, 1, 1, 1);\n\n    size_t n = lst.size();\n    size_t i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256 va = _mm256_loadu_ps(&lst[i]);\n        __m256 vb = _mm256_cmp_ps(va, vzero, _CMP_GE_OQ); // va >= 0\n        __m256 vc = _mm256_and_ps(va, vb); // va >= 0\n        __m256 vd = _mm256_cmp_ps(vc, viota, _CMP_LE_OQ); // va >= 0 && va <= 7\n        __m256 ve = _mm256_and_ps(vc, vd); // va >= 0 && va <= 7\n        __m256 vf = _mm256_floor_ps(ve); // floor(va)\n        __m256 vg = _mm256_cmp_ps(vf, ve, _CMP_EQ_OQ); // va == floor(va)\n        __m256 vh = _mm256_and_ps(ve, vg); // va > 0 && va == floor(va)\n        __m256 vi = _mm256_add_ps(vh, viota); // va % 2 != 0\n        __m256 vj = _mm256_and_ps(vi, vh); // va > 0 && va % 2 != 0\n        __m256 vk = _mm256_mul_ps(vj, vsqr); // va * va\n        __m256 vl = _mm256_hadd_ps(vk, vk);\n        __m256 vm = _mm256_hadd_ps(vl, vl);\n        float sum = vm[0] + vm[1];\n        result += static_cast<uint64_t>(sum);\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] >= 0 && lst[i] <= 7 && fmod(lst[i], 2.0) != 0) {\n            result += static_cast<uint64_t>(pow(lst[i], 2));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8;\n    uint64_t result = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_odd = _mm256_set1_epi32(0x55555555); // Odd number mask\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 abs_vec = _mm256_and_ps(vec, _mm256_set1_ps(0x7FFFFFFF)); // Absolute value\n        __m256i abs_int_vec = _mm256_cvtps_epi32(abs_vec);\n        __m256i odd_mask = _mm256_and_si256(abs_int_vec, mask_odd);\n\n        __m256i even_indices = _mm256_setr_epi32(0, 2, 4, 6, 0, 2, 4, 6);\n        __m256i odd_indices = _mm256_setr_epi32(1, 3, 5, 7, 1, 3, 5, 7);\n        __m256i indices = _mm256_or_si256(even_indices, odd_indices);\n\n        __m256i selected_values = _mm256_blendv_epi8(zero, abs_int_vec, odd_mask);\n        __m256i squared_values = _mm256_mullo_epi32(selected_values, selected_values);\n\n        __m256i sum = _mm256_reduce_add_epi32(squared_values);\n        result += _mm256_extract_epi32(sum, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8;\n    __m256 zero = _mm256_setzero_ps();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF); // Mask to ignore negative numbers\n    uint64_t sum = 0;\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 abs_vec = _mm256_and_ps(vec, mask);\n        __m256i cmp_vec = _mm256_cmp_ps(abs_vec, zero, _CMP_NEQ_OQ);\n        __m256i int_vec = _mm256_cvtps_epi32(abs_vec);\n        __m256i mod_vec = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i odd_mask = _mm256_cmpeq_epi32(mod_vec, _mm256_set1_epi32(1));\n        __m256 square_vec = _mm256_mul_ps(abs_vec, abs_vec);\n        __m256 result_vec = _mm256_blendv_ps(zero, square_vec, _mm256_castsi256_ps(odd_mask));\n        __m256i result_int = _mm256_cvtps_epi32(result_vec);\n        __m256i sum_vec = _mm256_sad_epu8(_mm256_castsi256_si128(result_int), _mm256_setzero_si128());\n        sum += _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 1);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    __m256 zero = _mm256_setzero_ps();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF); // Mask to ignore negative numbers\n    uint64_t sum = 0;\n\n    size_t n = lst.size();\n    size_t i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 abs_vec = _mm256_and_ps(vec, mask);\n        __m256i int_vec = _mm256_cvtps_epi32(abs_vec);\n        __m256i mod_vec = _mm256_and_si256(int_vec, _mm256_set1_epi32(1));\n        __m256i eq_vec = _mm256_cmpeq_epi32(mod_vec, _mm256_setzero_si256());\n        __m256 mul_vec = _mm256_mul_ps(vec, vec);\n        __m256 masked_mul_vec = _mm256_and_ps(mul_vec, _mm256_castsi256_ps(eq_vec));\n        __m256 sum_vec = _mm256_add_ps(masked_mul_vec, zero);\n        float result[8];\n        _mm256_storeu_ps(result, sum_vec);\n        for (int j = 0; j < 8; ++j) {\n            if (result[j] > 0) {\n                sum += static_cast<uint64_t>(result[j]);\n            }\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] >= 0 && fmod(lst[i], 2.0f) == 0) {\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i v_game = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i v_guess = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i v_diff = _mm256_abs_epi32(_mm256_sub_epi32(v_game, v_guess));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i v_game = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i v_guess = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i v_diff = _mm256_abs_epi32(_mm256_sub_epi32(v_game, v_guess));\n\n        _mm256_storeu_si256((__m256i*)&result[i], v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i v_game = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i v_guess = _mm256_loadu_si256((__m256i*)&guess[i]);\n\n        __m256i v_diff = _mm256_abs_epi32(_mm256_sub_epi32(v_game, v_guess));\n        _mm256_storeu_si256((__m256i*)&result[i], v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i v_game = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i v_guess = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i v_diff = _mm256_abs_epi32(_mm256_sub_epi32(v_game, v_guess));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i v_game = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i v_guess = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i v_diff = _mm256_abs_epi32(_mm256_sub_epi32(v_game, v_guess));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 8; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
